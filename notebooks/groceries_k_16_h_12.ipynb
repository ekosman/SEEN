{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "tree_depth = 12\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.197713851928711 | KNN Loss: 6.226964950561523 | BCE Loss: 1.9707493782043457\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.225666999816895 | KNN Loss: 6.226774215698242 | BCE Loss: 1.9988924264907837\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.174530029296875 | KNN Loss: 6.2269487380981445 | BCE Loss: 1.94758141040802\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.190489768981934 | KNN Loss: 6.226901054382324 | BCE Loss: 1.9635889530181885\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.177996635437012 | KNN Loss: 6.226274013519287 | BCE Loss: 1.9517223834991455\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.151252746582031 | KNN Loss: 6.225853443145752 | BCE Loss: 1.9253997802734375\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.15381145477295 | KNN Loss: 6.225696086883545 | BCE Loss: 1.9281151294708252\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.157215118408203 | KNN Loss: 6.225313186645508 | BCE Loss: 1.9319019317626953\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.17013931274414 | KNN Loss: 6.225063323974609 | BCE Loss: 1.9450758695602417\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.162270545959473 | KNN Loss: 6.225269794464111 | BCE Loss: 1.9370003938674927\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.128458023071289 | KNN Loss: 6.2248454093933105 | BCE Loss: 1.9036129713058472\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.140369415283203 | KNN Loss: 6.224599838256836 | BCE Loss: 1.9157692193984985\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.094426155090332 | KNN Loss: 6.224309921264648 | BCE Loss: 1.8701159954071045\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.116353034973145 | KNN Loss: 6.223930358886719 | BCE Loss: 1.8924224376678467\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.121856689453125 | KNN Loss: 6.223227024078369 | BCE Loss: 1.8986297845840454\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.119118690490723 | KNN Loss: 6.2236328125 | BCE Loss: 1.895485520362854\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.033690452575684 | KNN Loss: 6.22279167175293 | BCE Loss: 1.8108989000320435\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.060834884643555 | KNN Loss: 6.222347736358643 | BCE Loss: 1.838486671447754\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.04267692565918 | KNN Loss: 6.222245693206787 | BCE Loss: 1.8204307556152344\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.132131576538086 | KNN Loss: 6.221044540405273 | BCE Loss: 1.9110875129699707\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.049570083618164 | KNN Loss: 6.221299171447754 | BCE Loss: 1.8282711505889893\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.042625427246094 | KNN Loss: 6.220523834228516 | BCE Loss: 1.8221019506454468\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.995305061340332 | KNN Loss: 6.220292091369629 | BCE Loss: 1.775012731552124\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.013995170593262 | KNN Loss: 6.2195143699646 | BCE Loss: 1.7944804430007935\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.9423508644104 | KNN Loss: 6.218791484832764 | BCE Loss: 1.7235592603683472\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.995211601257324 | KNN Loss: 6.219021797180176 | BCE Loss: 1.7761898040771484\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.956683158874512 | KNN Loss: 6.2184953689575195 | BCE Loss: 1.7381880283355713\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.9880218505859375 | KNN Loss: 6.217051029205322 | BCE Loss: 1.7709710597991943\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.991548538208008 | KNN Loss: 6.216484546661377 | BCE Loss: 1.7750641107559204\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.975669860839844 | KNN Loss: 6.215714931488037 | BCE Loss: 1.759954810142517\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.962701797485352 | KNN Loss: 6.214874267578125 | BCE Loss: 1.7478276491165161\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.953072547912598 | KNN Loss: 6.214326858520508 | BCE Loss: 1.7387456893920898\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.902618885040283 | KNN Loss: 6.212411403656006 | BCE Loss: 1.6902074813842773\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.877246856689453 | KNN Loss: 6.210323333740234 | BCE Loss: 1.6669237613677979\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.935442924499512 | KNN Loss: 6.2106099128723145 | BCE Loss: 1.7248327732086182\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.871626853942871 | KNN Loss: 6.208729267120361 | BCE Loss: 1.6628977060317993\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.837599754333496 | KNN Loss: 6.207370281219482 | BCE Loss: 1.6302297115325928\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.8350419998168945 | KNN Loss: 6.205111503601074 | BCE Loss: 1.6299307346343994\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.847776412963867 | KNN Loss: 6.204318046569824 | BCE Loss: 1.6434582471847534\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.81761360168457 | KNN Loss: 6.202980041503906 | BCE Loss: 1.614633321762085\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.772114276885986 | KNN Loss: 6.200944900512695 | BCE Loss: 1.571169376373291\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.78367805480957 | KNN Loss: 6.197037696838379 | BCE Loss: 1.5866405963897705\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.800829887390137 | KNN Loss: 6.19512414932251 | BCE Loss: 1.6057054996490479\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.672560691833496 | KNN Loss: 6.191096782684326 | BCE Loss: 1.48146390914917\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.70494270324707 | KNN Loss: 6.189577102661133 | BCE Loss: 1.5153656005859375\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.656286239624023 | KNN Loss: 6.184513568878174 | BCE Loss: 1.4717729091644287\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.669437408447266 | KNN Loss: 6.1810197830200195 | BCE Loss: 1.488417625427246\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.6024017333984375 | KNN Loss: 6.173814296722412 | BCE Loss: 1.4285876750946045\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.627922534942627 | KNN Loss: 6.163259506225586 | BCE Loss: 1.464663028717041\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.557344436645508 | KNN Loss: 6.16418981552124 | BCE Loss: 1.3931543827056885\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.529733657836914 | KNN Loss: 6.157681465148926 | BCE Loss: 1.3720521926879883\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.48341703414917 | KNN Loss: 6.144045829772949 | BCE Loss: 1.3393710851669312\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.50022029876709 | KNN Loss: 6.137210369110107 | BCE Loss: 1.3630098104476929\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.4612298011779785 | KNN Loss: 6.129876613616943 | BCE Loss: 1.3313531875610352\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.411696434020996 | KNN Loss: 6.1187005043029785 | BCE Loss: 1.2929960489273071\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.376839637756348 | KNN Loss: 6.090620517730713 | BCE Loss: 1.2862188816070557\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.328512191772461 | KNN Loss: 6.080214977264404 | BCE Loss: 1.2482969760894775\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.318872451782227 | KNN Loss: 6.053486347198486 | BCE Loss: 1.2653858661651611\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.232066631317139 | KNN Loss: 6.036315441131592 | BCE Loss: 1.1957511901855469\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.237528324127197 | KNN Loss: 6.01469612121582 | BCE Loss: 1.2228320837020874\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.114624500274658 | KNN Loss: 5.952121257781982 | BCE Loss: 1.1625031232833862\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.140646457672119 | KNN Loss: 5.957969665527344 | BCE Loss: 1.1826766729354858\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.053061485290527 | KNN Loss: 5.90261173248291 | BCE Loss: 1.1504499912261963\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.966248035430908 | KNN Loss: 5.8401780128479 | BCE Loss: 1.1260699033737183\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.916252613067627 | KNN Loss: 5.797469139099121 | BCE Loss: 1.1187834739685059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 6.8714399337768555 | KNN Loss: 5.739673137664795 | BCE Loss: 1.13176691532135\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 6.798647880554199 | KNN Loss: 5.666825771331787 | BCE Loss: 1.131821870803833\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 6.681827545166016 | KNN Loss: 5.56065034866333 | BCE Loss: 1.1211771965026855\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.610039710998535 | KNN Loss: 5.499035835266113 | BCE Loss: 1.1110038757324219\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 6.4639201164245605 | KNN Loss: 5.373536109924316 | BCE Loss: 1.0903841257095337\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 6.341156959533691 | KNN Loss: 5.257620811462402 | BCE Loss: 1.0835363864898682\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 6.213466644287109 | KNN Loss: 5.108219623565674 | BCE Loss: 1.1052470207214355\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 6.144377708435059 | KNN Loss: 5.05210018157959 | BCE Loss: 1.0922777652740479\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 6.023323059082031 | KNN Loss: 4.922109603881836 | BCE Loss: 1.1012135744094849\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.818591117858887 | KNN Loss: 4.754240036010742 | BCE Loss: 1.0643508434295654\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.808452606201172 | KNN Loss: 4.718047618865967 | BCE Loss: 1.090404987335205\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.678877353668213 | KNN Loss: 4.563394546508789 | BCE Loss: 1.1154826879501343\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.546747207641602 | KNN Loss: 4.4287519454956055 | BCE Loss: 1.1179955005645752\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.381897449493408 | KNN Loss: 4.307407855987549 | BCE Loss: 1.0744895935058594\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.387628555297852 | KNN Loss: 4.269027233123779 | BCE Loss: 1.1186015605926514\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.239087104797363 | KNN Loss: 4.147558689117432 | BCE Loss: 1.091528296470642\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.226921081542969 | KNN Loss: 4.111876487731934 | BCE Loss: 1.115044355392456\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.095717430114746 | KNN Loss: 3.999485731124878 | BCE Loss: 1.0962316989898682\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 4.984212875366211 | KNN Loss: 3.9039294719696045 | BCE Loss: 1.0802836418151855\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 4.961940765380859 | KNN Loss: 3.8788769245147705 | BCE Loss: 1.0830638408660889\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 4.861355304718018 | KNN Loss: 3.8064115047454834 | BCE Loss: 1.0549437999725342\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.815768241882324 | KNN Loss: 3.7051167488098145 | BCE Loss: 1.1106513738632202\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.737639427185059 | KNN Loss: 3.68221378326416 | BCE Loss: 1.0554256439208984\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.717460632324219 | KNN Loss: 3.6292221546173096 | BCE Loss: 1.08823823928833\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 4.695599555969238 | KNN Loss: 3.6260435581207275 | BCE Loss: 1.0695562362670898\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 4.663774013519287 | KNN Loss: 3.5870683193206787 | BCE Loss: 1.076705813407898\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 4.605324745178223 | KNN Loss: 3.5194506645202637 | BCE Loss: 1.085874319076538\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.608543395996094 | KNN Loss: 3.541128158569336 | BCE Loss: 1.0674149990081787\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.585909843444824 | KNN Loss: 3.5002574920654297 | BCE Loss: 1.0856525897979736\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 4.552001953125 | KNN Loss: 3.478006601333618 | BCE Loss: 1.073995590209961\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.558280944824219 | KNN Loss: 3.4745945930480957 | BCE Loss: 1.083686113357544\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.504372596740723 | KNN Loss: 3.4396653175354004 | BCE Loss: 1.0647075176239014\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.477609634399414 | KNN Loss: 3.392810583114624 | BCE Loss: 1.0847992897033691\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.473552703857422 | KNN Loss: 3.4058492183685303 | BCE Loss: 1.067703366279602\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.430399417877197 | KNN Loss: 3.378270387649536 | BCE Loss: 1.0521291494369507\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.426433086395264 | KNN Loss: 3.371846914291382 | BCE Loss: 1.0545862913131714\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.468149185180664 | KNN Loss: 3.3891220092773438 | BCE Loss: 1.0790269374847412\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.491838455200195 | KNN Loss: 3.4182958602905273 | BCE Loss: 1.073542833328247\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.4743499755859375 | KNN Loss: 3.4043757915496826 | BCE Loss: 1.069974422454834\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.449147701263428 | KNN Loss: 3.382331132888794 | BCE Loss: 1.0668166875839233\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.40773344039917 | KNN Loss: 3.3340818881988525 | BCE Loss: 1.0736515522003174\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.430332183837891 | KNN Loss: 3.3580477237701416 | BCE Loss: 1.072284460067749\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.347248077392578 | KNN Loss: 3.2925047874450684 | BCE Loss: 1.0547435283660889\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.431025981903076 | KNN Loss: 3.350611925125122 | BCE Loss: 1.080414056777954\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.3861894607543945 | KNN Loss: 3.336557149887085 | BCE Loss: 1.04963219165802\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.416990756988525 | KNN Loss: 3.3393197059631348 | BCE Loss: 1.0776710510253906\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.413096904754639 | KNN Loss: 3.3575663566589355 | BCE Loss: 1.0555305480957031\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.338977813720703 | KNN Loss: 3.2763893604278564 | BCE Loss: 1.0625884532928467\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.323884010314941 | KNN Loss: 3.2828967571258545 | BCE Loss: 1.040987253189087\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.4094414710998535 | KNN Loss: 3.3333892822265625 | BCE Loss: 1.076052188873291\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.353055953979492 | KNN Loss: 3.3178346157073975 | BCE Loss: 1.0352210998535156\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.359452724456787 | KNN Loss: 3.30582594871521 | BCE Loss: 1.0536268949508667\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.354121208190918 | KNN Loss: 3.2715818881988525 | BCE Loss: 1.0825395584106445\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.305818557739258 | KNN Loss: 3.265906810760498 | BCE Loss: 1.0399115085601807\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.348756790161133 | KNN Loss: 3.313821315765381 | BCE Loss: 1.034935712814331\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.280819416046143 | KNN Loss: 3.2423481941223145 | BCE Loss: 1.0384713411331177\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.380071640014648 | KNN Loss: 3.31563138961792 | BCE Loss: 1.0644400119781494\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.335780143737793 | KNN Loss: 3.2761120796203613 | BCE Loss: 1.0596683025360107\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.349532604217529 | KNN Loss: 3.2902369499206543 | BCE Loss: 1.0592955350875854\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.316974639892578 | KNN Loss: 3.2715885639190674 | BCE Loss: 1.0453860759735107\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.281845569610596 | KNN Loss: 3.248410224914551 | BCE Loss: 1.033435344696045\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.316841125488281 | KNN Loss: 3.253685712814331 | BCE Loss: 1.0631556510925293\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.359936714172363 | KNN Loss: 3.325864791870117 | BCE Loss: 1.0340721607208252\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.3365631103515625 | KNN Loss: 3.276308536529541 | BCE Loss: 1.0602545738220215\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.281401634216309 | KNN Loss: 3.2491116523742676 | BCE Loss: 1.032289981842041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.320324897766113 | KNN Loss: 3.2698252201080322 | BCE Loss: 1.050499439239502\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.358704090118408 | KNN Loss: 3.313469409942627 | BCE Loss: 1.0452346801757812\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.304399013519287 | KNN Loss: 3.252042055130005 | BCE Loss: 1.0523569583892822\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.2353925704956055 | KNN Loss: 3.204836130142212 | BCE Loss: 1.030556559562683\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.277061462402344 | KNN Loss: 3.227719306945801 | BCE Loss: 1.0493419170379639\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.300971031188965 | KNN Loss: 3.247703790664673 | BCE Loss: 1.053267240524292\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.378009796142578 | KNN Loss: 3.311530113220215 | BCE Loss: 1.0664796829223633\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.334157943725586 | KNN Loss: 3.281982421875 | BCE Loss: 1.052175760269165\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.343212604522705 | KNN Loss: 3.3001716136932373 | BCE Loss: 1.0430411100387573\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.333344459533691 | KNN Loss: 3.2738122940063477 | BCE Loss: 1.0595322847366333\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.3236494064331055 | KNN Loss: 3.278771162033081 | BCE Loss: 1.0448782444000244\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.376358985900879 | KNN Loss: 3.3129358291625977 | BCE Loss: 1.0634233951568604\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.296698093414307 | KNN Loss: 3.2599329948425293 | BCE Loss: 1.036765217781067\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.255084991455078 | KNN Loss: 3.227306842803955 | BCE Loss: 1.0277783870697021\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.295279026031494 | KNN Loss: 3.25142240524292 | BCE Loss: 1.0438566207885742\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.349978446960449 | KNN Loss: 3.284661054611206 | BCE Loss: 1.065317153930664\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.266499996185303 | KNN Loss: 3.218426465988159 | BCE Loss: 1.0480735301971436\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.227800369262695 | KNN Loss: 3.2120306491851807 | BCE Loss: 1.0157699584960938\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.232898712158203 | KNN Loss: 3.1879732608795166 | BCE Loss: 1.0449256896972656\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.294610500335693 | KNN Loss: 3.251333475112915 | BCE Loss: 1.0432770252227783\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.30021333694458 | KNN Loss: 3.2317655086517334 | BCE Loss: 1.0684478282928467\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.288026332855225 | KNN Loss: 3.2582972049713135 | BCE Loss: 1.0297290086746216\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.272748947143555 | KNN Loss: 3.232762336730957 | BCE Loss: 1.039986491203308\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.234295845031738 | KNN Loss: 3.1799023151397705 | BCE Loss: 1.0543937683105469\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.252018928527832 | KNN Loss: 3.2081191539764404 | BCE Loss: 1.043899655342102\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.237099647521973 | KNN Loss: 3.2096095085144043 | BCE Loss: 1.0274901390075684\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.2251434326171875 | KNN Loss: 3.1812915802001953 | BCE Loss: 1.0438518524169922\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.222833633422852 | KNN Loss: 3.1708874702453613 | BCE Loss: 1.0519459247589111\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.248469352722168 | KNN Loss: 3.198542356491089 | BCE Loss: 1.049926996231079\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.2936248779296875 | KNN Loss: 3.2299344539642334 | BCE Loss: 1.063690423965454\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.225943088531494 | KNN Loss: 3.20556902885437 | BCE Loss: 1.0203741788864136\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.248984336853027 | KNN Loss: 3.187687635421753 | BCE Loss: 1.0612969398498535\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.224423408508301 | KNN Loss: 3.191924810409546 | BCE Loss: 1.032498836517334\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.230697154998779 | KNN Loss: 3.1951465606689453 | BCE Loss: 1.035550594329834\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.241079330444336 | KNN Loss: 3.1923816204071045 | BCE Loss: 1.0486977100372314\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.334011077880859 | KNN Loss: 3.270695209503174 | BCE Loss: 1.0633158683776855\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.196161270141602 | KNN Loss: 3.1666321754455566 | BCE Loss: 1.0295288562774658\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.256528854370117 | KNN Loss: 3.188145875930786 | BCE Loss: 1.068382740020752\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.25757360458374 | KNN Loss: 3.2248926162719727 | BCE Loss: 1.0326811075210571\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.16155481338501 | KNN Loss: 3.1704020500183105 | BCE Loss: 0.9911526441574097\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.208100318908691 | KNN Loss: 3.1597349643707275 | BCE Loss: 1.0483651161193848\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.2193169593811035 | KNN Loss: 3.173078775405884 | BCE Loss: 1.0462381839752197\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.2537689208984375 | KNN Loss: 3.225806713104248 | BCE Loss: 1.0279624462127686\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.205681800842285 | KNN Loss: 3.179267168045044 | BCE Loss: 1.026414394378662\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.21053409576416 | KNN Loss: 3.1605894565582275 | BCE Loss: 1.0499447584152222\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.326358795166016 | KNN Loss: 3.26931095123291 | BCE Loss: 1.057047724723816\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.2453179359436035 | KNN Loss: 3.2006165981292725 | BCE Loss: 1.044701337814331\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.241757869720459 | KNN Loss: 3.2014176845550537 | BCE Loss: 1.0403401851654053\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.177884101867676 | KNN Loss: 3.148977279663086 | BCE Loss: 1.028907060623169\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.188023567199707 | KNN Loss: 3.1600427627563477 | BCE Loss: 1.0279808044433594\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.169320106506348 | KNN Loss: 3.1475963592529297 | BCE Loss: 1.0217238664627075\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.263911247253418 | KNN Loss: 3.2190895080566406 | BCE Loss: 1.0448219776153564\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.202779769897461 | KNN Loss: 3.149186134338379 | BCE Loss: 1.053593635559082\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.309950828552246 | KNN Loss: 3.2675585746765137 | BCE Loss: 1.0423922538757324\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.246152877807617 | KNN Loss: 3.2036850452423096 | BCE Loss: 1.0424680709838867\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.215479373931885 | KNN Loss: 3.1870205402374268 | BCE Loss: 1.028458833694458\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.2052459716796875 | KNN Loss: 3.1522955894470215 | BCE Loss: 1.052950143814087\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.255475044250488 | KNN Loss: 3.2126071453094482 | BCE Loss: 1.0428681373596191\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.250580787658691 | KNN Loss: 3.2037997245788574 | BCE Loss: 1.046781301498413\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.24806022644043 | KNN Loss: 3.219736337661743 | BCE Loss: 1.0283241271972656\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.22972297668457 | KNN Loss: 3.1785683631896973 | BCE Loss: 1.0511547327041626\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.1865620613098145 | KNN Loss: 3.1711971759796143 | BCE Loss: 1.0153648853302002\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.234342575073242 | KNN Loss: 3.1999948024749756 | BCE Loss: 1.0343480110168457\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.239810943603516 | KNN Loss: 3.184938907623291 | BCE Loss: 1.0548720359802246\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.252976417541504 | KNN Loss: 3.207627296447754 | BCE Loss: 1.04534912109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.237036228179932 | KNN Loss: 3.198315143585205 | BCE Loss: 1.038720965385437\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.232545852661133 | KNN Loss: 3.216519832611084 | BCE Loss: 1.016026258468628\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.190826892852783 | KNN Loss: 3.1581554412841797 | BCE Loss: 1.0326714515686035\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.197216987609863 | KNN Loss: 3.1687138080596924 | BCE Loss: 1.02850341796875\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.229626655578613 | KNN Loss: 3.184157609939575 | BCE Loss: 1.045469045639038\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.245745658874512 | KNN Loss: 3.210498571395874 | BCE Loss: 1.0352468490600586\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.27773380279541 | KNN Loss: 3.2555296421051025 | BCE Loss: 1.0222043991088867\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.198144435882568 | KNN Loss: 3.184840440750122 | BCE Loss: 1.0133039951324463\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.213648796081543 | KNN Loss: 3.192018985748291 | BCE Loss: 1.0216295719146729\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.217341899871826 | KNN Loss: 3.1768229007720947 | BCE Loss: 1.0405189990997314\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.214892387390137 | KNN Loss: 3.147299289703369 | BCE Loss: 1.0675933361053467\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.232607841491699 | KNN Loss: 3.189728021621704 | BCE Loss: 1.0428800582885742\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.238494396209717 | KNN Loss: 3.205350875854492 | BCE Loss: 1.033143401145935\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.22067928314209 | KNN Loss: 3.16251540184021 | BCE Loss: 1.058164119720459\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.270280361175537 | KNN Loss: 3.2111313343048096 | BCE Loss: 1.0591490268707275\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.196482181549072 | KNN Loss: 3.158324956893921 | BCE Loss: 1.0381572246551514\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.226273059844971 | KNN Loss: 3.185058116912842 | BCE Loss: 1.041214942932129\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.251420974731445 | KNN Loss: 3.203127384185791 | BCE Loss: 1.0482933521270752\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.1927971839904785 | KNN Loss: 3.1332786083221436 | BCE Loss: 1.059518575668335\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.218031883239746 | KNN Loss: 3.1760103702545166 | BCE Loss: 1.0420215129852295\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.187705039978027 | KNN Loss: 3.1601998805999756 | BCE Loss: 1.0275051593780518\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.225257873535156 | KNN Loss: 3.1693196296691895 | BCE Loss: 1.0559380054473877\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.2093000411987305 | KNN Loss: 3.191985845565796 | BCE Loss: 1.0173144340515137\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.233650207519531 | KNN Loss: 3.1946797370910645 | BCE Loss: 1.038970708847046\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.180352687835693 | KNN Loss: 3.1321709156036377 | BCE Loss: 1.0481817722320557\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.155333518981934 | KNN Loss: 3.135601282119751 | BCE Loss: 1.0197324752807617\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.183539867401123 | KNN Loss: 3.171323299407959 | BCE Loss: 1.012216567993164\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.191794395446777 | KNN Loss: 3.159935712814331 | BCE Loss: 1.0318586826324463\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.2346343994140625 | KNN Loss: 3.1845734119415283 | BCE Loss: 1.0500609874725342\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.198605060577393 | KNN Loss: 3.138075590133667 | BCE Loss: 1.0605295896530151\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.236931800842285 | KNN Loss: 3.2051827907562256 | BCE Loss: 1.0317490100860596\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.141631126403809 | KNN Loss: 3.1229562759399414 | BCE Loss: 1.0186750888824463\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.174376487731934 | KNN Loss: 3.139186143875122 | BCE Loss: 1.0351903438568115\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.188948631286621 | KNN Loss: 3.154014825820923 | BCE Loss: 1.0349338054656982\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.147553443908691 | KNN Loss: 3.1574525833129883 | BCE Loss: 0.9901007413864136\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.172196865081787 | KNN Loss: 3.1616616249084473 | BCE Loss: 1.0105352401733398\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.23963737487793 | KNN Loss: 3.1800973415374756 | BCE Loss: 1.0595401525497437\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.211736679077148 | KNN Loss: 3.1622838973999023 | BCE Loss: 1.0494530200958252\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.216599464416504 | KNN Loss: 3.1712453365325928 | BCE Loss: 1.0453541278839111\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.149685382843018 | KNN Loss: 3.1242165565490723 | BCE Loss: 1.0254689455032349\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.26222038269043 | KNN Loss: 3.195894479751587 | BCE Loss: 1.0663257837295532\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.203490257263184 | KNN Loss: 3.1701836585998535 | BCE Loss: 1.03330659866333\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.211862087249756 | KNN Loss: 3.1817216873168945 | BCE Loss: 1.0301405191421509\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.208394527435303 | KNN Loss: 3.134927749633789 | BCE Loss: 1.0734667778015137\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.212136745452881 | KNN Loss: 3.1669304370880127 | BCE Loss: 1.0452064275741577\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.166525840759277 | KNN Loss: 3.115020513534546 | BCE Loss: 1.051505208015442\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.170592308044434 | KNN Loss: 3.125808000564575 | BCE Loss: 1.0447843074798584\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.185828685760498 | KNN Loss: 3.1656839847564697 | BCE Loss: 1.0201447010040283\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.180117607116699 | KNN Loss: 3.1532797813415527 | BCE Loss: 1.0268378257751465\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.18394660949707 | KNN Loss: 3.124687910079956 | BCE Loss: 1.0592586994171143\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.164650917053223 | KNN Loss: 3.1496264934539795 | BCE Loss: 1.015024185180664\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.200405120849609 | KNN Loss: 3.168018102645874 | BCE Loss: 1.0323867797851562\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.1875505447387695 | KNN Loss: 3.1359150409698486 | BCE Loss: 1.0516357421875\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.16585636138916 | KNN Loss: 3.149226188659668 | BCE Loss: 1.016629934310913\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.191017150878906 | KNN Loss: 3.146636962890625 | BCE Loss: 1.0443800687789917\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.145312309265137 | KNN Loss: 3.1064348220825195 | BCE Loss: 1.038877248764038\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.122809410095215 | KNN Loss: 3.1168265342712402 | BCE Loss: 1.0059831142425537\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.165801048278809 | KNN Loss: 3.1021602153778076 | BCE Loss: 1.0636407136917114\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.162237167358398 | KNN Loss: 3.1416032314300537 | BCE Loss: 1.0206340551376343\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.143564224243164 | KNN Loss: 3.1353042125701904 | BCE Loss: 1.0082600116729736\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.138756275177002 | KNN Loss: 3.1315290927886963 | BCE Loss: 1.0072270631790161\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.152440071105957 | KNN Loss: 3.1286864280700684 | BCE Loss: 1.0237534046173096\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.199978828430176 | KNN Loss: 3.14288592338562 | BCE Loss: 1.0570929050445557\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.175878047943115 | KNN Loss: 3.148725748062134 | BCE Loss: 1.027152180671692\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.165946960449219 | KNN Loss: 3.1172449588775635 | BCE Loss: 1.0487021207809448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.156114101409912 | KNN Loss: 3.1511781215667725 | BCE Loss: 1.0049360990524292\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.241332530975342 | KNN Loss: 3.19590163230896 | BCE Loss: 1.0454308986663818\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.158176422119141 | KNN Loss: 3.1401865482330322 | BCE Loss: 1.0179896354675293\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.214005470275879 | KNN Loss: 3.1279385089874268 | BCE Loss: 1.0860671997070312\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.188873291015625 | KNN Loss: 3.1579582691192627 | BCE Loss: 1.0309152603149414\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.160538196563721 | KNN Loss: 3.1324663162231445 | BCE Loss: 1.0280717611312866\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.171976089477539 | KNN Loss: 3.175236225128174 | BCE Loss: 0.9967400431632996\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.1338043212890625 | KNN Loss: 3.134368419647217 | BCE Loss: 0.9994357824325562\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.137375354766846 | KNN Loss: 3.134611129760742 | BCE Loss: 1.0027642250061035\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.139971733093262 | KNN Loss: 3.1333506107330322 | BCE Loss: 1.0066208839416504\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.1785478591918945 | KNN Loss: 3.1485726833343506 | BCE Loss: 1.029975175857544\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.196710586547852 | KNN Loss: 3.167452335357666 | BCE Loss: 1.0292582511901855\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.214498043060303 | KNN Loss: 3.165250301361084 | BCE Loss: 1.0492477416992188\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.1431379318237305 | KNN Loss: 3.1326537132263184 | BCE Loss: 1.010483980178833\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.1728196144104 | KNN Loss: 3.1476950645446777 | BCE Loss: 1.025124430656433\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.161022186279297 | KNN Loss: 3.138828754425049 | BCE Loss: 1.0221936702728271\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.172065258026123 | KNN Loss: 3.1256704330444336 | BCE Loss: 1.0463948249816895\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.182197570800781 | KNN Loss: 3.164438486099243 | BCE Loss: 1.0177592039108276\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.173239707946777 | KNN Loss: 3.137972116470337 | BCE Loss: 1.0352675914764404\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.170786380767822 | KNN Loss: 3.130098819732666 | BCE Loss: 1.0406875610351562\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.17589807510376 | KNN Loss: 3.16141414642334 | BCE Loss: 1.01448392868042\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.198225498199463 | KNN Loss: 3.1558494567871094 | BCE Loss: 1.042376160621643\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.146492958068848 | KNN Loss: 3.1437184810638428 | BCE Loss: 1.0027742385864258\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.188907146453857 | KNN Loss: 3.14953351020813 | BCE Loss: 1.0393736362457275\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.168091773986816 | KNN Loss: 3.1254098415374756 | BCE Loss: 1.0426816940307617\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.173099517822266 | KNN Loss: 3.132460594177246 | BCE Loss: 1.0406391620635986\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.100970268249512 | KNN Loss: 3.120108127593994 | BCE Loss: 0.9808619022369385\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.174448013305664 | KNN Loss: 3.128059148788452 | BCE Loss: 1.0463889837265015\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.1414995193481445 | KNN Loss: 3.108715295791626 | BCE Loss: 1.032784342765808\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.194491386413574 | KNN Loss: 3.1502645015716553 | BCE Loss: 1.0442266464233398\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.138998985290527 | KNN Loss: 3.1269006729125977 | BCE Loss: 1.0120981931686401\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.163512706756592 | KNN Loss: 3.1241748332977295 | BCE Loss: 1.0393379926681519\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.116588115692139 | KNN Loss: 3.100956678390503 | BCE Loss: 1.0156314373016357\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.134001731872559 | KNN Loss: 3.123182773590088 | BCE Loss: 1.0108191967010498\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.200250625610352 | KNN Loss: 3.1714494228363037 | BCE Loss: 1.028801441192627\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.189549446105957 | KNN Loss: 3.123218297958374 | BCE Loss: 1.066331148147583\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.182651042938232 | KNN Loss: 3.145660877227783 | BCE Loss: 1.0369902849197388\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.179614067077637 | KNN Loss: 3.137213945388794 | BCE Loss: 1.0424002408981323\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.151610374450684 | KNN Loss: 3.1356685161590576 | BCE Loss: 1.015942096710205\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.175879001617432 | KNN Loss: 3.1429295539855957 | BCE Loss: 1.032949447631836\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.148000240325928 | KNN Loss: 3.128270149230957 | BCE Loss: 1.0197300910949707\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.188948631286621 | KNN Loss: 3.1370580196380615 | BCE Loss: 1.0518908500671387\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.204568862915039 | KNN Loss: 3.1470766067504883 | BCE Loss: 1.0574922561645508\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.162321090698242 | KNN Loss: 3.1453583240509033 | BCE Loss: 1.016963005065918\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.209349632263184 | KNN Loss: 3.150902271270752 | BCE Loss: 1.0584474802017212\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.128668308258057 | KNN Loss: 3.1214513778686523 | BCE Loss: 1.0072170495986938\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.11993932723999 | KNN Loss: 3.106536865234375 | BCE Loss: 1.0134024620056152\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.151019096374512 | KNN Loss: 3.124516725540161 | BCE Loss: 1.0265023708343506\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.173909664154053 | KNN Loss: 3.135349750518799 | BCE Loss: 1.038559913635254\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.1538004875183105 | KNN Loss: 3.1263718605041504 | BCE Loss: 1.0274286270141602\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.140790939331055 | KNN Loss: 3.1413004398345947 | BCE Loss: 0.9994905591011047\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.1296844482421875 | KNN Loss: 3.0994327068328857 | BCE Loss: 1.0302516222000122\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.10517692565918 | KNN Loss: 3.0951743125915527 | BCE Loss: 1.0100024938583374\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.16845703125 | KNN Loss: 3.139760971069336 | BCE Loss: 1.0286959409713745\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.17723274230957 | KNN Loss: 3.144563674926758 | BCE Loss: 1.0326693058013916\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.162502765655518 | KNN Loss: 3.130002498626709 | BCE Loss: 1.0325002670288086\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.139603137969971 | KNN Loss: 3.1003031730651855 | BCE Loss: 1.0392999649047852\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.173985481262207 | KNN Loss: 3.1494903564453125 | BCE Loss: 1.0244951248168945\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.167777061462402 | KNN Loss: 3.1346349716186523 | BCE Loss: 1.0331419706344604\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.1721367835998535 | KNN Loss: 3.121305465698242 | BCE Loss: 1.0508314371109009\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.187870502471924 | KNN Loss: 3.144280195236206 | BCE Loss: 1.0435903072357178\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.182434558868408 | KNN Loss: 3.1536285877227783 | BCE Loss: 1.0288059711456299\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.145234107971191 | KNN Loss: 3.110692024230957 | BCE Loss: 1.0345420837402344\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.145651340484619 | KNN Loss: 3.1350972652435303 | BCE Loss: 1.0105540752410889\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.130266189575195 | KNN Loss: 3.112724542617798 | BCE Loss: 1.0175418853759766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.209314346313477 | KNN Loss: 3.1803693771362305 | BCE Loss: 1.028944969177246\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.224369049072266 | KNN Loss: 3.1429290771484375 | BCE Loss: 1.0814399719238281\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.168971538543701 | KNN Loss: 3.137782573699951 | BCE Loss: 1.03118896484375\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.174190521240234 | KNN Loss: 3.1545588970184326 | BCE Loss: 1.0196316242218018\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.156680107116699 | KNN Loss: 3.1302263736724854 | BCE Loss: 1.0264536142349243\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.129638671875 | KNN Loss: 3.118009567260742 | BCE Loss: 1.0116291046142578\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.117300987243652 | KNN Loss: 3.1140735149383545 | BCE Loss: 1.003227710723877\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.158773422241211 | KNN Loss: 3.143685817718506 | BCE Loss: 1.0150878429412842\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.146100997924805 | KNN Loss: 3.1444032192230225 | BCE Loss: 1.0016975402832031\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.183587074279785 | KNN Loss: 3.1323587894439697 | BCE Loss: 1.0512282848358154\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.145571708679199 | KNN Loss: 3.143209218978882 | BCE Loss: 1.002362608909607\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.151961326599121 | KNN Loss: 3.1205592155456543 | BCE Loss: 1.0314021110534668\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.1358537673950195 | KNN Loss: 3.121680974960327 | BCE Loss: 1.0141725540161133\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.153314590454102 | KNN Loss: 3.1424450874328613 | BCE Loss: 1.0108692646026611\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.164581298828125 | KNN Loss: 3.1362502574920654 | BCE Loss: 1.0283308029174805\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.15463924407959 | KNN Loss: 3.114492177963257 | BCE Loss: 1.040147304534912\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.167469024658203 | KNN Loss: 3.118985891342163 | BCE Loss: 1.0484830141067505\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.134795665740967 | KNN Loss: 3.104600429534912 | BCE Loss: 1.0301952362060547\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.171120643615723 | KNN Loss: 3.123692750930786 | BCE Loss: 1.0474276542663574\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.153903484344482 | KNN Loss: 3.1339993476867676 | BCE Loss: 1.0199041366577148\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.1580915451049805 | KNN Loss: 3.1144566535949707 | BCE Loss: 1.0436351299285889\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.200814247131348 | KNN Loss: 3.145193099975586 | BCE Loss: 1.0556209087371826\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.122020721435547 | KNN Loss: 3.1018660068511963 | BCE Loss: 1.0201549530029297\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.154869079589844 | KNN Loss: 3.1317079067230225 | BCE Loss: 1.0231614112854004\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.197943687438965 | KNN Loss: 3.172044515609741 | BCE Loss: 1.025899052619934\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.132657051086426 | KNN Loss: 3.123135566711426 | BCE Loss: 1.009521484375\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.151833534240723 | KNN Loss: 3.1370925903320312 | BCE Loss: 1.0147411823272705\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.183797836303711 | KNN Loss: 3.1297659873962402 | BCE Loss: 1.0540320873260498\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.121939182281494 | KNN Loss: 3.1105239391326904 | BCE Loss: 1.0114151239395142\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.124284744262695 | KNN Loss: 3.107980728149414 | BCE Loss: 1.0163042545318604\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.146272659301758 | KNN Loss: 3.1137783527374268 | BCE Loss: 1.0324945449829102\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.174709796905518 | KNN Loss: 3.1317012310028076 | BCE Loss: 1.04300856590271\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.178679943084717 | KNN Loss: 3.1341543197631836 | BCE Loss: 1.0445257425308228\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.13115119934082 | KNN Loss: 3.1126646995544434 | BCE Loss: 1.018486499786377\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.182015419006348 | KNN Loss: 3.124457597732544 | BCE Loss: 1.0575579404830933\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.153738021850586 | KNN Loss: 3.137357234954834 | BCE Loss: 1.0163805484771729\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.192445278167725 | KNN Loss: 3.1370322704315186 | BCE Loss: 1.055413007736206\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.172771453857422 | KNN Loss: 3.1210904121398926 | BCE Loss: 1.0516812801361084\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.153866767883301 | KNN Loss: 3.1148626804351807 | BCE Loss: 1.039003849029541\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.108899116516113 | KNN Loss: 3.0978024005889893 | BCE Loss: 1.011096477508545\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.165398597717285 | KNN Loss: 3.1220762729644775 | BCE Loss: 1.0433223247528076\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.176396369934082 | KNN Loss: 3.135174036026001 | BCE Loss: 1.041222333908081\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.1090593338012695 | KNN Loss: 3.1122066974639893 | BCE Loss: 0.996852457523346\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.17357063293457 | KNN Loss: 3.1413238048553467 | BCE Loss: 1.0322468280792236\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.118698596954346 | KNN Loss: 3.12381911277771 | BCE Loss: 0.994879424571991\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.14640998840332 | KNN Loss: 3.122008800506592 | BCE Loss: 1.0244009494781494\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.152680397033691 | KNN Loss: 3.117435932159424 | BCE Loss: 1.0352447032928467\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.163664817810059 | KNN Loss: 3.115929365158081 | BCE Loss: 1.0477354526519775\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.165346622467041 | KNN Loss: 3.129316568374634 | BCE Loss: 1.0360301733016968\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.170958518981934 | KNN Loss: 3.130983352661133 | BCE Loss: 1.0399754047393799\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.17216682434082 | KNN Loss: 3.1368753910064697 | BCE Loss: 1.0352916717529297\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.144650459289551 | KNN Loss: 3.1176886558532715 | BCE Loss: 1.0269616842269897\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.158921718597412 | KNN Loss: 3.1330628395080566 | BCE Loss: 1.025858998298645\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.163959980010986 | KNN Loss: 3.114511013031006 | BCE Loss: 1.0494489669799805\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.163463592529297 | KNN Loss: 3.134920358657837 | BCE Loss: 1.028543472290039\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.1746039390563965 | KNN Loss: 3.1435787677764893 | BCE Loss: 1.0310252904891968\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.180405616760254 | KNN Loss: 3.1735005378723145 | BCE Loss: 1.006905198097229\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.151301860809326 | KNN Loss: 3.1175832748413086 | BCE Loss: 1.0337185859680176\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.165752410888672 | KNN Loss: 3.1220498085021973 | BCE Loss: 1.0437028408050537\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.131718635559082 | KNN Loss: 3.1106107234954834 | BCE Loss: 1.021107792854309\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.155148506164551 | KNN Loss: 3.1186511516571045 | BCE Loss: 1.0364972352981567\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.190499305725098 | KNN Loss: 3.1549072265625 | BCE Loss: 1.0355918407440186\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.171123504638672 | KNN Loss: 3.1426005363464355 | BCE Loss: 1.0285228490829468\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.140419006347656 | KNN Loss: 3.138965368270874 | BCE Loss: 1.0014533996582031\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.1178789138793945 | KNN Loss: 3.118237257003784 | BCE Loss: 0.9996416568756104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.167259216308594 | KNN Loss: 3.107916831970215 | BCE Loss: 1.059342384338379\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.157262802124023 | KNN Loss: 3.1473915576934814 | BCE Loss: 1.009871482849121\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.100696563720703 | KNN Loss: 3.0978970527648926 | BCE Loss: 1.0027992725372314\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.148861408233643 | KNN Loss: 3.1161396503448486 | BCE Loss: 1.0327218770980835\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.183882236480713 | KNN Loss: 3.1561295986175537 | BCE Loss: 1.0277525186538696\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.107870101928711 | KNN Loss: 3.0830211639404297 | BCE Loss: 1.0248491764068604\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.134270668029785 | KNN Loss: 3.1169486045837402 | BCE Loss: 1.017322301864624\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.132001876831055 | KNN Loss: 3.119136333465576 | BCE Loss: 1.0128657817840576\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.143418788909912 | KNN Loss: 3.130800724029541 | BCE Loss: 1.012618064880371\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.1090898513793945 | KNN Loss: 3.1170248985290527 | BCE Loss: 0.9920651912689209\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.15354061126709 | KNN Loss: 3.1240768432617188 | BCE Loss: 1.029463768005371\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.178540229797363 | KNN Loss: 3.1481034755706787 | BCE Loss: 1.0304367542266846\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.156121253967285 | KNN Loss: 3.12995982170105 | BCE Loss: 1.0261611938476562\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.146542072296143 | KNN Loss: 3.1326217651367188 | BCE Loss: 1.0139204263687134\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.138724327087402 | KNN Loss: 3.112057685852051 | BCE Loss: 1.0266668796539307\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.141578197479248 | KNN Loss: 3.11350154876709 | BCE Loss: 1.0280767679214478\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.131897449493408 | KNN Loss: 3.0988941192626953 | BCE Loss: 1.033003330230713\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.15712833404541 | KNN Loss: 3.1338815689086914 | BCE Loss: 1.0232465267181396\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.131765842437744 | KNN Loss: 3.1196234226226807 | BCE Loss: 1.012142300605774\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.141421318054199 | KNN Loss: 3.1190030574798584 | BCE Loss: 1.0224181413650513\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.171223163604736 | KNN Loss: 3.1425037384033203 | BCE Loss: 1.0287193059921265\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.133718967437744 | KNN Loss: 3.1000754833221436 | BCE Loss: 1.033643364906311\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.117942810058594 | KNN Loss: 3.1187288761138916 | BCE Loss: 0.9992140531539917\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.104007244110107 | KNN Loss: 3.0800280570983887 | BCE Loss: 1.0239793062210083\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.118684768676758 | KNN Loss: 3.095494270324707 | BCE Loss: 1.0231906175613403\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.171657562255859 | KNN Loss: 3.133230209350586 | BCE Loss: 1.0384272336959839\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.187855243682861 | KNN Loss: 3.131134510040283 | BCE Loss: 1.0567206144332886\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.146730899810791 | KNN Loss: 3.1023149490356445 | BCE Loss: 1.044416069984436\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.150821685791016 | KNN Loss: 3.116853952407837 | BCE Loss: 1.0339674949645996\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.142541885375977 | KNN Loss: 3.142674446105957 | BCE Loss: 0.9998676776885986\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.150135040283203 | KNN Loss: 3.123643159866333 | BCE Loss: 1.0264919996261597\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.145071983337402 | KNN Loss: 3.120795488357544 | BCE Loss: 1.024276614189148\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.1893391609191895 | KNN Loss: 3.1253576278686523 | BCE Loss: 1.063981533050537\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.171380996704102 | KNN Loss: 3.1348156929016113 | BCE Loss: 1.0365651845932007\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.159345626831055 | KNN Loss: 3.125871419906616 | BCE Loss: 1.033474326133728\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.144892692565918 | KNN Loss: 3.129352331161499 | BCE Loss: 1.0155404806137085\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.152587890625 | KNN Loss: 3.119138240814209 | BCE Loss: 1.033449411392212\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.153998374938965 | KNN Loss: 3.12368106842041 | BCE Loss: 1.0303170680999756\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.121102333068848 | KNN Loss: 3.102138042449951 | BCE Loss: 1.0189645290374756\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.161403179168701 | KNN Loss: 3.1233601570129395 | BCE Loss: 1.0380430221557617\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.1746416091918945 | KNN Loss: 3.1268248558044434 | BCE Loss: 1.0478167533874512\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.099620342254639 | KNN Loss: 3.107266426086426 | BCE Loss: 0.9923537969589233\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.142516613006592 | KNN Loss: 3.108915328979492 | BCE Loss: 1.0336014032363892\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.160552978515625 | KNN Loss: 3.129897356033325 | BCE Loss: 1.0306553840637207\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.1838226318359375 | KNN Loss: 3.1308815479278564 | BCE Loss: 1.0529413223266602\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.078983306884766 | KNN Loss: 3.0873570442199707 | BCE Loss: 0.9916261434555054\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.160275459289551 | KNN Loss: 3.113992691040039 | BCE Loss: 1.0462828874588013\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.1378302574157715 | KNN Loss: 3.120258092880249 | BCE Loss: 1.0175721645355225\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.166376113891602 | KNN Loss: 3.1130385398864746 | BCE Loss: 1.0533373355865479\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.1550164222717285 | KNN Loss: 3.101132869720459 | BCE Loss: 1.05388343334198\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.166348457336426 | KNN Loss: 3.1383965015411377 | BCE Loss: 1.0279520750045776\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.123964786529541 | KNN Loss: 3.10290789604187 | BCE Loss: 1.0210570096969604\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.136883735656738 | KNN Loss: 3.117769479751587 | BCE Loss: 1.0191144943237305\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.134468078613281 | KNN Loss: 3.1113622188568115 | BCE Loss: 1.0231056213378906\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.112310409545898 | KNN Loss: 3.1149301528930664 | BCE Loss: 0.9973800182342529\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.1376953125 | KNN Loss: 3.124284505844116 | BCE Loss: 1.0134108066558838\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.148678779602051 | KNN Loss: 3.1317970752716064 | BCE Loss: 1.0168814659118652\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.163610458374023 | KNN Loss: 3.1299567222595215 | BCE Loss: 1.033653974533081\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.198831081390381 | KNN Loss: 3.167419910430908 | BCE Loss: 1.0314112901687622\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.120426654815674 | KNN Loss: 3.108400821685791 | BCE Loss: 1.0120257139205933\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.121323585510254 | KNN Loss: 3.1032958030700684 | BCE Loss: 1.018027663230896\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.169245719909668 | KNN Loss: 3.1368563175201416 | BCE Loss: 1.0323894023895264\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.206722259521484 | KNN Loss: 3.1419832706451416 | BCE Loss: 1.0647389888763428\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.172428131103516 | KNN Loss: 3.1505088806152344 | BCE Loss: 1.0219194889068604\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.142020225524902 | KNN Loss: 3.122386932373047 | BCE Loss: 1.019633412361145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.115258693695068 | KNN Loss: 3.0931437015533447 | BCE Loss: 1.022114872932434\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.11326789855957 | KNN Loss: 3.0994009971618652 | BCE Loss: 1.013866662979126\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.157391548156738 | KNN Loss: 3.124852180480957 | BCE Loss: 1.0325392484664917\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.120510101318359 | KNN Loss: 3.1057159900665283 | BCE Loss: 1.014794111251831\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.126264572143555 | KNN Loss: 3.100647449493408 | BCE Loss: 1.025617241859436\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.141498565673828 | KNN Loss: 3.118583917617798 | BCE Loss: 1.0229146480560303\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.108288764953613 | KNN Loss: 3.081177234649658 | BCE Loss: 1.027111291885376\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.127962112426758 | KNN Loss: 3.1061434745788574 | BCE Loss: 1.0218188762664795\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.129003047943115 | KNN Loss: 3.116664171218872 | BCE Loss: 1.0123388767242432\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.155221939086914 | KNN Loss: 3.1256508827209473 | BCE Loss: 1.0295709371566772\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.156233310699463 | KNN Loss: 3.105391502380371 | BCE Loss: 1.0508418083190918\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.187452793121338 | KNN Loss: 3.158482074737549 | BCE Loss: 1.028970718383789\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.128354072570801 | KNN Loss: 3.0691306591033936 | BCE Loss: 1.0592232942581177\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.112771034240723 | KNN Loss: 3.0700016021728516 | BCE Loss: 1.042769432067871\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.165120601654053 | KNN Loss: 3.127678155899048 | BCE Loss: 1.0374423265457153\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.168612480163574 | KNN Loss: 3.138080596923828 | BCE Loss: 1.0305321216583252\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.142250061035156 | KNN Loss: 3.112232208251953 | BCE Loss: 1.0300179719924927\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.159778594970703 | KNN Loss: 3.1268930435180664 | BCE Loss: 1.0328853130340576\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.122684955596924 | KNN Loss: 3.109189748764038 | BCE Loss: 1.0134952068328857\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.134079456329346 | KNN Loss: 3.091043710708618 | BCE Loss: 1.0430357456207275\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.157553195953369 | KNN Loss: 3.1255006790161133 | BCE Loss: 1.0320525169372559\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.1454548835754395 | KNN Loss: 3.1446173191070557 | BCE Loss: 1.0008375644683838\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.138288497924805 | KNN Loss: 3.115442991256714 | BCE Loss: 1.0228455066680908\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.173046588897705 | KNN Loss: 3.1319167613983154 | BCE Loss: 1.0411298274993896\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.140775680541992 | KNN Loss: 3.120685577392578 | BCE Loss: 1.0200902223587036\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.114048957824707 | KNN Loss: 3.082857370376587 | BCE Loss: 1.031191349029541\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.118395805358887 | KNN Loss: 3.104952096939087 | BCE Loss: 1.0134435892105103\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.154117107391357 | KNN Loss: 3.1240768432617188 | BCE Loss: 1.0300402641296387\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.155770778656006 | KNN Loss: 3.0968129634857178 | BCE Loss: 1.058957815170288\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.133825778961182 | KNN Loss: 3.1253700256347656 | BCE Loss: 1.0084556341171265\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.127586841583252 | KNN Loss: 3.0970962047576904 | BCE Loss: 1.0304906368255615\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.099329948425293 | KNN Loss: 3.0687339305877686 | BCE Loss: 1.0305960178375244\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.134278774261475 | KNN Loss: 3.098986864089966 | BCE Loss: 1.0352919101715088\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.095175743103027 | KNN Loss: 3.094437599182129 | BCE Loss: 1.0007383823394775\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.108109951019287 | KNN Loss: 3.0817344188690186 | BCE Loss: 1.0263755321502686\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.110071182250977 | KNN Loss: 3.082023859024048 | BCE Loss: 1.0280473232269287\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.151644706726074 | KNN Loss: 3.1039788722991943 | BCE Loss: 1.047666072845459\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.102887153625488 | KNN Loss: 3.0956554412841797 | BCE Loss: 1.0072319507598877\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.144104957580566 | KNN Loss: 3.1181864738464355 | BCE Loss: 1.02591872215271\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.107831001281738 | KNN Loss: 3.121718168258667 | BCE Loss: 0.9861130714416504\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.146435260772705 | KNN Loss: 3.113206624984741 | BCE Loss: 1.0332286357879639\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.1086320877075195 | KNN Loss: 3.058518171310425 | BCE Loss: 1.0501137971878052\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.11958122253418 | KNN Loss: 3.0835957527160645 | BCE Loss: 1.0359854698181152\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.096726417541504 | KNN Loss: 3.0754199028015137 | BCE Loss: 1.0213062763214111\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.109808921813965 | KNN Loss: 3.0995500087738037 | BCE Loss: 1.0102590322494507\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.145301342010498 | KNN Loss: 3.1171538829803467 | BCE Loss: 1.0281474590301514\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.170252799987793 | KNN Loss: 3.1414334774017334 | BCE Loss: 1.0288194417953491\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.107884407043457 | KNN Loss: 3.0966744422912598 | BCE Loss: 1.0112099647521973\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.146808624267578 | KNN Loss: 3.121607542037964 | BCE Loss: 1.0252013206481934\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.153985977172852 | KNN Loss: 3.1168274879455566 | BCE Loss: 1.037158727645874\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.125375747680664 | KNN Loss: 3.111236810684204 | BCE Loss: 1.01413893699646\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.147686004638672 | KNN Loss: 3.1198277473449707 | BCE Loss: 1.027858018875122\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.158393859863281 | KNN Loss: 3.1264710426330566 | BCE Loss: 1.0319230556488037\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.13590145111084 | KNN Loss: 3.12457013130188 | BCE Loss: 1.01133131980896\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.101227760314941 | KNN Loss: 3.087554931640625 | BCE Loss: 1.0136730670928955\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.103795051574707 | KNN Loss: 3.0810089111328125 | BCE Loss: 1.0227863788604736\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.104944229125977 | KNN Loss: 3.1140897274017334 | BCE Loss: 0.9908542633056641\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.180955410003662 | KNN Loss: 3.141470193862915 | BCE Loss: 1.039485216140747\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.140551567077637 | KNN Loss: 3.114614486694336 | BCE Loss: 1.0259368419647217\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.16110897064209 | KNN Loss: 3.124110698699951 | BCE Loss: 1.0369985103607178\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.146585464477539 | KNN Loss: 3.1274123191833496 | BCE Loss: 1.0191733837127686\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.121313095092773 | KNN Loss: 3.10327410697937 | BCE Loss: 1.0180388689041138\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.165305137634277 | KNN Loss: 3.1312484741210938 | BCE Loss: 1.034056544303894\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.131392478942871 | KNN Loss: 3.1076788902282715 | BCE Loss: 1.0237133502960205\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.140472888946533 | KNN Loss: 3.112828016281128 | BCE Loss: 1.0276449918746948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.145899295806885 | KNN Loss: 3.097020387649536 | BCE Loss: 1.048878788948059\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.130449295043945 | KNN Loss: 3.128154754638672 | BCE Loss: 1.0022944211959839\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.140462875366211 | KNN Loss: 3.108340263366699 | BCE Loss: 1.0321226119995117\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.137731075286865 | KNN Loss: 3.1127171516418457 | BCE Loss: 1.0250139236450195\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.126206398010254 | KNN Loss: 3.1050164699554443 | BCE Loss: 1.0211896896362305\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.109657287597656 | KNN Loss: 3.0867059230804443 | BCE Loss: 1.022951602935791\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.141723155975342 | KNN Loss: 3.1193838119506836 | BCE Loss: 1.0223392248153687\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.126799583435059 | KNN Loss: 3.098886728286743 | BCE Loss: 1.0279128551483154\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.172996520996094 | KNN Loss: 3.1499288082122803 | BCE Loss: 1.0230674743652344\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.174786567687988 | KNN Loss: 3.14642071723938 | BCE Loss: 1.0283658504486084\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.115275859832764 | KNN Loss: 3.1231162548065186 | BCE Loss: 0.9921597242355347\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.172733783721924 | KNN Loss: 3.136305570602417 | BCE Loss: 1.0364282131195068\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.170037269592285 | KNN Loss: 3.1181845664978027 | BCE Loss: 1.0518527030944824\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.143270492553711 | KNN Loss: 3.12160587310791 | BCE Loss: 1.0216648578643799\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.161749839782715 | KNN Loss: 3.133488893508911 | BCE Loss: 1.0282611846923828\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.151297569274902 | KNN Loss: 3.139146566390991 | BCE Loss: 1.0121512413024902\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.152029037475586 | KNN Loss: 3.1174447536468506 | BCE Loss: 1.0345841646194458\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.1618499755859375 | KNN Loss: 3.1160919666290283 | BCE Loss: 1.0457581281661987\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.127784252166748 | KNN Loss: 3.1073012351989746 | BCE Loss: 1.0204830169677734\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.127083778381348 | KNN Loss: 3.130927801132202 | BCE Loss: 0.9961560964584351\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.11466121673584 | KNN Loss: 3.099125385284424 | BCE Loss: 1.015535593032837\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.161171913146973 | KNN Loss: 3.1304924488067627 | BCE Loss: 1.03067946434021\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.161367416381836 | KNN Loss: 3.12515926361084 | BCE Loss: 1.0362083911895752\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.142358779907227 | KNN Loss: 3.1207752227783203 | BCE Loss: 1.0215837955474854\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.1407470703125 | KNN Loss: 3.0892138481140137 | BCE Loss: 1.0515334606170654\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.123649597167969 | KNN Loss: 3.1064698696136475 | BCE Loss: 1.0171797275543213\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.157649993896484 | KNN Loss: 3.1050193309783936 | BCE Loss: 1.05263090133667\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.156599998474121 | KNN Loss: 3.160491704940796 | BCE Loss: 0.9961081147193909\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.150498390197754 | KNN Loss: 3.1229803562164307 | BCE Loss: 1.0275177955627441\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.081324577331543 | KNN Loss: 3.102436065673828 | BCE Loss: 0.9788882732391357\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.16532039642334 | KNN Loss: 3.143739938735962 | BCE Loss: 1.021580457687378\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.139232635498047 | KNN Loss: 3.1034610271453857 | BCE Loss: 1.035771369934082\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.132726669311523 | KNN Loss: 3.1025755405426025 | BCE Loss: 1.0301513671875\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.221207618713379 | KNN Loss: 3.1583728790283203 | BCE Loss: 1.0628349781036377\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.143229007720947 | KNN Loss: 3.108887195587158 | BCE Loss: 1.034341812133789\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.100690841674805 | KNN Loss: 3.072528123855591 | BCE Loss: 1.028162956237793\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.099936008453369 | KNN Loss: 3.0853054523468018 | BCE Loss: 1.0146305561065674\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.103567123413086 | KNN Loss: 3.062105417251587 | BCE Loss: 1.041461706161499\n",
      "Epoch    93: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.092093467712402 | KNN Loss: 3.078876495361328 | BCE Loss: 1.0132172107696533\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.146086692810059 | KNN Loss: 3.082451343536377 | BCE Loss: 1.063635230064392\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.072591781616211 | KNN Loss: 3.0910065174102783 | BCE Loss: 0.9815854430198669\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.1561279296875 | KNN Loss: 3.107525110244751 | BCE Loss: 1.0486030578613281\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.137121200561523 | KNN Loss: 3.080869436264038 | BCE Loss: 1.0562520027160645\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.089364051818848 | KNN Loss: 3.0518898963928223 | BCE Loss: 1.0374741554260254\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.142514705657959 | KNN Loss: 3.0982210636138916 | BCE Loss: 1.0442936420440674\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.170291423797607 | KNN Loss: 3.127824068069458 | BCE Loss: 1.0424672365188599\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.115642547607422 | KNN Loss: 3.087975025177002 | BCE Loss: 1.0276672840118408\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.095634460449219 | KNN Loss: 3.0706591606140137 | BCE Loss: 1.024975061416626\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.119872093200684 | KNN Loss: 3.072209119796753 | BCE Loss: 1.0476630926132202\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.151908874511719 | KNN Loss: 3.1160409450531006 | BCE Loss: 1.0358680486679077\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.098055839538574 | KNN Loss: 3.090871572494507 | BCE Loss: 1.0071840286254883\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.1264543533325195 | KNN Loss: 3.112440824508667 | BCE Loss: 1.0140137672424316\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.101718902587891 | KNN Loss: 3.0743091106414795 | BCE Loss: 1.0274100303649902\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.12082052230835 | KNN Loss: 3.090635061264038 | BCE Loss: 1.030185341835022\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.10845947265625 | KNN Loss: 3.0933799743652344 | BCE Loss: 1.0150797367095947\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.127254486083984 | KNN Loss: 3.1091091632843018 | BCE Loss: 1.0181455612182617\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.116282939910889 | KNN Loss: 3.0767979621887207 | BCE Loss: 1.0394848585128784\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.092904090881348 | KNN Loss: 3.0936806201934814 | BCE Loss: 0.9992232322692871\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.1502227783203125 | KNN Loss: 3.123213291168213 | BCE Loss: 1.0270092487335205\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.119515419006348 | KNN Loss: 3.0921852588653564 | BCE Loss: 1.027329921722412\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.108421325683594 | KNN Loss: 3.0822346210479736 | BCE Loss: 1.026186466217041\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.09645938873291 | KNN Loss: 3.0808017253875732 | BCE Loss: 1.015657663345337\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.0936384201049805 | KNN Loss: 3.0618412494659424 | BCE Loss: 1.031796932220459\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.103933334350586 | KNN Loss: 3.0940940380096436 | BCE Loss: 1.009839415550232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.104032516479492 | KNN Loss: 3.079009532928467 | BCE Loss: 1.025023102760315\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.056861400604248 | KNN Loss: 3.0668978691101074 | BCE Loss: 0.9899635910987854\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.093735694885254 | KNN Loss: 3.072929859161377 | BCE Loss: 1.0208059549331665\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.113432884216309 | KNN Loss: 3.0902647972106934 | BCE Loss: 1.0231680870056152\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.096861362457275 | KNN Loss: 3.0889532566070557 | BCE Loss: 1.0079081058502197\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.11738395690918 | KNN Loss: 3.1147632598876953 | BCE Loss: 1.0026204586029053\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.152283668518066 | KNN Loss: 3.1218464374542236 | BCE Loss: 1.0304373502731323\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.1653218269348145 | KNN Loss: 3.1318163871765137 | BCE Loss: 1.0335053205490112\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.106590270996094 | KNN Loss: 3.0906856060028076 | BCE Loss: 1.0159049034118652\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.122062683105469 | KNN Loss: 3.09881329536438 | BCE Loss: 1.0232491493225098\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.093642234802246 | KNN Loss: 3.080503463745117 | BCE Loss: 1.013139009475708\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.139915466308594 | KNN Loss: 3.1033642292022705 | BCE Loss: 1.0365512371063232\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.102616786956787 | KNN Loss: 3.07749080657959 | BCE Loss: 1.0251259803771973\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.162604808807373 | KNN Loss: 3.1378495693206787 | BCE Loss: 1.0247553586959839\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.109631061553955 | KNN Loss: 3.086477041244507 | BCE Loss: 1.0231539011001587\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.139397621154785 | KNN Loss: 3.130786895751953 | BCE Loss: 1.008610725402832\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.104957580566406 | KNN Loss: 3.0883209705352783 | BCE Loss: 1.016636848449707\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.120379447937012 | KNN Loss: 3.1180105209350586 | BCE Loss: 1.0023688077926636\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.106242656707764 | KNN Loss: 3.0999574661254883 | BCE Loss: 1.0062851905822754\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.112921237945557 | KNN Loss: 3.088047742843628 | BCE Loss: 1.0248736143112183\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.061980247497559 | KNN Loss: 3.0765602588653564 | BCE Loss: 0.9854202270507812\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.103573799133301 | KNN Loss: 3.0868005752563477 | BCE Loss: 1.0167733430862427\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.124030590057373 | KNN Loss: 3.0890138149261475 | BCE Loss: 1.0350168943405151\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.133228778839111 | KNN Loss: 3.131633996963501 | BCE Loss: 1.0015946626663208\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.078483581542969 | KNN Loss: 3.0863373279571533 | BCE Loss: 0.9921461343765259\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.061346054077148 | KNN Loss: 3.0560338497161865 | BCE Loss: 1.0053120851516724\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.0614914894104 | KNN Loss: 3.0656044483184814 | BCE Loss: 0.995887041091919\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.159035682678223 | KNN Loss: 3.1199567317962646 | BCE Loss: 1.039078712463379\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.12591552734375 | KNN Loss: 3.1202123165130615 | BCE Loss: 1.0057034492492676\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.164953231811523 | KNN Loss: 3.1376891136169434 | BCE Loss: 1.0272639989852905\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.130054473876953 | KNN Loss: 3.112330913543701 | BCE Loss: 1.017723560333252\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.103949546813965 | KNN Loss: 3.0904898643493652 | BCE Loss: 1.0134599208831787\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.084375381469727 | KNN Loss: 3.094822645187378 | BCE Loss: 0.9895526170730591\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.111220836639404 | KNN Loss: 3.0737850666046143 | BCE Loss: 1.0374358892440796\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.1925835609436035 | KNN Loss: 3.129422426223755 | BCE Loss: 1.0631611347198486\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.158529281616211 | KNN Loss: 3.130483388900757 | BCE Loss: 1.0280461311340332\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.103245735168457 | KNN Loss: 3.1016287803649902 | BCE Loss: 1.001617193222046\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.106075286865234 | KNN Loss: 3.0894064903259277 | BCE Loss: 1.0166685581207275\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.133934020996094 | KNN Loss: 3.1283395290374756 | BCE Loss: 1.0055943727493286\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.10173225402832 | KNN Loss: 3.0907673835754395 | BCE Loss: 1.0109648704528809\n",
      "Epoch   104: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.1593780517578125 | KNN Loss: 3.1258294582366943 | BCE Loss: 1.0335485935211182\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.137977600097656 | KNN Loss: 3.097792387008667 | BCE Loss: 1.0401854515075684\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.162158012390137 | KNN Loss: 3.1186718940734863 | BCE Loss: 1.0434861183166504\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.119133472442627 | KNN Loss: 3.0882749557495117 | BCE Loss: 1.0308585166931152\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.088789463043213 | KNN Loss: 3.0827598571777344 | BCE Loss: 1.0060296058654785\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.06822395324707 | KNN Loss: 3.0623459815979004 | BCE Loss: 1.00587797164917\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.103761672973633 | KNN Loss: 3.1206717491149902 | BCE Loss: 0.9830901026725769\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.106727600097656 | KNN Loss: 3.0587263107299805 | BCE Loss: 1.0480010509490967\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.087395191192627 | KNN Loss: 3.0616650581359863 | BCE Loss: 1.0257301330566406\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.1403656005859375 | KNN Loss: 3.080176591873169 | BCE Loss: 1.0601887702941895\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.148181438446045 | KNN Loss: 3.113429069519043 | BCE Loss: 1.034752368927002\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.14381742477417 | KNN Loss: 3.1011743545532227 | BCE Loss: 1.0426430702209473\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.114577293395996 | KNN Loss: 3.1007497310638428 | BCE Loss: 1.0138274431228638\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.167791843414307 | KNN Loss: 3.1056015491485596 | BCE Loss: 1.062190294265747\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.089503765106201 | KNN Loss: 3.0634617805480957 | BCE Loss: 1.026041865348816\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.097376346588135 | KNN Loss: 3.103576421737671 | BCE Loss: 0.9938000440597534\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.104855537414551 | KNN Loss: 3.0771265029907227 | BCE Loss: 1.0277290344238281\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.087920665740967 | KNN Loss: 3.0803802013397217 | BCE Loss: 1.0075405836105347\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.1056623458862305 | KNN Loss: 3.0684332847595215 | BCE Loss: 1.037229299545288\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.147719383239746 | KNN Loss: 3.107567548751831 | BCE Loss: 1.040151834487915\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.095623970031738 | KNN Loss: 3.084049940109253 | BCE Loss: 1.0115739107131958\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.099976062774658 | KNN Loss: 3.077075719833374 | BCE Loss: 1.0229003429412842\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.138650417327881 | KNN Loss: 3.089653253555298 | BCE Loss: 1.048997163772583\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.115743637084961 | KNN Loss: 3.1106514930725098 | BCE Loss: 1.0050923824310303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.064780235290527 | KNN Loss: 3.083144426345825 | BCE Loss: 0.981635570526123\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.112067222595215 | KNN Loss: 3.080502510070801 | BCE Loss: 1.0315649509429932\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.134092807769775 | KNN Loss: 3.1187565326690674 | BCE Loss: 1.015336275100708\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.138516902923584 | KNN Loss: 3.1067519187927246 | BCE Loss: 1.0317649841308594\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.074275016784668 | KNN Loss: 3.0694282054901123 | BCE Loss: 1.0048470497131348\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.129911422729492 | KNN Loss: 3.101876974105835 | BCE Loss: 1.0280343294143677\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.121542453765869 | KNN Loss: 3.0900018215179443 | BCE Loss: 1.0315407514572144\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.131997108459473 | KNN Loss: 3.0987720489501953 | BCE Loss: 1.033225178718567\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.054078578948975 | KNN Loss: 3.0642080307006836 | BCE Loss: 0.989870548248291\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.133016586303711 | KNN Loss: 3.113412857055664 | BCE Loss: 1.0196034908294678\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.0950608253479 | KNN Loss: 3.0658769607543945 | BCE Loss: 1.0291837453842163\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.09065580368042 | KNN Loss: 3.075101852416992 | BCE Loss: 1.0155538320541382\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.116103172302246 | KNN Loss: 3.0671746730804443 | BCE Loss: 1.0489284992218018\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.080331802368164 | KNN Loss: 3.092562675476074 | BCE Loss: 0.9877688884735107\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.112213134765625 | KNN Loss: 3.072420120239258 | BCE Loss: 1.0397931337356567\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.086193084716797 | KNN Loss: 3.081220865249634 | BCE Loss: 1.004972219467163\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.150248050689697 | KNN Loss: 3.115619421005249 | BCE Loss: 1.0346286296844482\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.102518558502197 | KNN Loss: 3.0780932903289795 | BCE Loss: 1.0244251489639282\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.124632835388184 | KNN Loss: 3.0839033126831055 | BCE Loss: 1.0407297611236572\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.134347915649414 | KNN Loss: 3.0797860622406006 | BCE Loss: 1.0545616149902344\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.10134220123291 | KNN Loss: 3.08622670173645 | BCE Loss: 1.0151152610778809\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.094012260437012 | KNN Loss: 3.0900819301605225 | BCE Loss: 1.0039303302764893\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.120583534240723 | KNN Loss: 3.086347818374634 | BCE Loss: 1.0342358350753784\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.063814640045166 | KNN Loss: 3.068697452545166 | BCE Loss: 0.9951173663139343\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.082372665405273 | KNN Loss: 3.070154905319214 | BCE Loss: 1.0122179985046387\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.0889763832092285 | KNN Loss: 3.0745949745178223 | BCE Loss: 1.0143812894821167\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.104575157165527 | KNN Loss: 3.1096999645233154 | BCE Loss: 0.9948751926422119\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.126456260681152 | KNN Loss: 3.1032373905181885 | BCE Loss: 1.0232187509536743\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.100619792938232 | KNN Loss: 3.079373598098755 | BCE Loss: 1.021246075630188\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.13410758972168 | KNN Loss: 3.111515760421753 | BCE Loss: 1.0225919485092163\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.063152313232422 | KNN Loss: 3.082536220550537 | BCE Loss: 0.98061603307724\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.123319149017334 | KNN Loss: 3.0938656330108643 | BCE Loss: 1.0294535160064697\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.07828426361084 | KNN Loss: 3.0838983058929443 | BCE Loss: 0.9943861961364746\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.057398796081543 | KNN Loss: 3.0502724647521973 | BCE Loss: 1.0071264505386353\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.088582515716553 | KNN Loss: 3.051412343978882 | BCE Loss: 1.0371700525283813\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.1497697830200195 | KNN Loss: 3.113147735595703 | BCE Loss: 1.0366220474243164\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.085953235626221 | KNN Loss: 3.1020472049713135 | BCE Loss: 0.983906090259552\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.130338668823242 | KNN Loss: 3.1072444915771484 | BCE Loss: 1.0230942964553833\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.141500949859619 | KNN Loss: 3.1160941123962402 | BCE Loss: 1.025406837463379\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.1203718185424805 | KNN Loss: 3.1065070629119873 | BCE Loss: 1.0138646364212036\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.138078689575195 | KNN Loss: 3.098672866821289 | BCE Loss: 1.0394055843353271\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.138117790222168 | KNN Loss: 3.11722469329834 | BCE Loss: 1.020892858505249\n",
      "Epoch   115: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.1226806640625 | KNN Loss: 3.1200151443481445 | BCE Loss: 1.0026655197143555\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.122961044311523 | KNN Loss: 3.077707290649414 | BCE Loss: 1.045253872871399\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.091440677642822 | KNN Loss: 3.080371618270874 | BCE Loss: 1.0110691785812378\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.107378959655762 | KNN Loss: 3.0608575344085693 | BCE Loss: 1.0465211868286133\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.126880168914795 | KNN Loss: 3.0940608978271484 | BCE Loss: 1.0328192710876465\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.09055757522583 | KNN Loss: 3.075054168701172 | BCE Loss: 1.0155032873153687\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.081151485443115 | KNN Loss: 3.059767484664917 | BCE Loss: 1.0213841199874878\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.123422622680664 | KNN Loss: 3.0914344787597656 | BCE Loss: 1.0319880247116089\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.132971286773682 | KNN Loss: 3.103435516357422 | BCE Loss: 1.0295356512069702\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.13895320892334 | KNN Loss: 3.1007816791534424 | BCE Loss: 1.0381712913513184\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.103271007537842 | KNN Loss: 3.0871777534484863 | BCE Loss: 1.016093134880066\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.094387531280518 | KNN Loss: 3.0739190578460693 | BCE Loss: 1.0204684734344482\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.079946994781494 | KNN Loss: 3.0652077198028564 | BCE Loss: 1.0147391557693481\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.108972549438477 | KNN Loss: 3.0949599742889404 | BCE Loss: 1.0140125751495361\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.11259651184082 | KNN Loss: 3.0844457149505615 | BCE Loss: 1.0281505584716797\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.086333274841309 | KNN Loss: 3.091726303100586 | BCE Loss: 0.9946067333221436\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.098484039306641 | KNN Loss: 3.0795915126800537 | BCE Loss: 1.0188922882080078\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.068745136260986 | KNN Loss: 3.0773332118988037 | BCE Loss: 0.9914119243621826\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.092626094818115 | KNN Loss: 3.054166078567505 | BCE Loss: 1.0384598970413208\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.072190284729004 | KNN Loss: 3.0818068981170654 | BCE Loss: 0.9903834462165833\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.115083694458008 | KNN Loss: 3.074307680130005 | BCE Loss: 1.040776252746582\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.132133960723877 | KNN Loss: 3.0912115573883057 | BCE Loss: 1.0409222841262817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.07534122467041 | KNN Loss: 3.061506748199463 | BCE Loss: 1.0138345956802368\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.095791339874268 | KNN Loss: 3.0686893463134766 | BCE Loss: 1.0271021127700806\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.0810699462890625 | KNN Loss: 3.0565662384033203 | BCE Loss: 1.0245038270950317\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.141618728637695 | KNN Loss: 3.1109230518341064 | BCE Loss: 1.0306955575942993\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.101593017578125 | KNN Loss: 3.0896291732788086 | BCE Loss: 1.0119637250900269\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.140707015991211 | KNN Loss: 3.0879030227661133 | BCE Loss: 1.0528037548065186\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.098947048187256 | KNN Loss: 3.0683906078338623 | BCE Loss: 1.0305564403533936\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.074775218963623 | KNN Loss: 3.066349744796753 | BCE Loss: 1.0084253549575806\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.1276750564575195 | KNN Loss: 3.111483097076416 | BCE Loss: 1.0161917209625244\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.138374328613281 | KNN Loss: 3.1079516410827637 | BCE Loss: 1.0304224491119385\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.124128818511963 | KNN Loss: 3.0856566429138184 | BCE Loss: 1.038472056388855\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.128664970397949 | KNN Loss: 3.087709665298462 | BCE Loss: 1.0409553050994873\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.0562944412231445 | KNN Loss: 3.072986602783203 | BCE Loss: 0.983307957649231\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.0987067222595215 | KNN Loss: 3.080070972442627 | BCE Loss: 1.0186357498168945\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.127054214477539 | KNN Loss: 3.101184368133545 | BCE Loss: 1.025869607925415\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.109304904937744 | KNN Loss: 3.0786807537078857 | BCE Loss: 1.0306241512298584\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.090303897857666 | KNN Loss: 3.0759007930755615 | BCE Loss: 1.0144031047821045\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.13494873046875 | KNN Loss: 3.1165378093719482 | BCE Loss: 1.0184106826782227\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.105926990509033 | KNN Loss: 3.103576183319092 | BCE Loss: 1.002350926399231\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.10070276260376 | KNN Loss: 3.084541082382202 | BCE Loss: 1.0161617994308472\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.074118137359619 | KNN Loss: 3.0619447231292725 | BCE Loss: 1.0121734142303467\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.062966823577881 | KNN Loss: 3.0880637168884277 | BCE Loss: 0.9749032855033875\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.093179702758789 | KNN Loss: 3.078004837036133 | BCE Loss: 1.0151747465133667\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.118403434753418 | KNN Loss: 3.0834178924560547 | BCE Loss: 1.0349856615066528\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.099020481109619 | KNN Loss: 3.0646636486053467 | BCE Loss: 1.034356951713562\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.1243791580200195 | KNN Loss: 3.078279972076416 | BCE Loss: 1.0460991859436035\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.117826461791992 | KNN Loss: 3.089160680770874 | BCE Loss: 1.0286657810211182\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.129535675048828 | KNN Loss: 3.0905370712280273 | BCE Loss: 1.0389988422393799\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.122626304626465 | KNN Loss: 3.109027624130249 | BCE Loss: 1.013598918914795\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.108891487121582 | KNN Loss: 3.107219934463501 | BCE Loss: 1.001671552658081\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.1133270263671875 | KNN Loss: 3.0968878269195557 | BCE Loss: 1.016439437866211\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.087143421173096 | KNN Loss: 3.0471413135528564 | BCE Loss: 1.0400021076202393\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.106581687927246 | KNN Loss: 3.0883994102478027 | BCE Loss: 1.0181822776794434\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.063238143920898 | KNN Loss: 3.06451153755188 | BCE Loss: 0.9987268447875977\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.111319541931152 | KNN Loss: 3.1007261276245117 | BCE Loss: 1.0105936527252197\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.089081764221191 | KNN Loss: 3.082918882369995 | BCE Loss: 1.0061631202697754\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.0951128005981445 | KNN Loss: 3.0908942222595215 | BCE Loss: 1.004218339920044\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.092551231384277 | KNN Loss: 3.077331304550171 | BCE Loss: 1.0152199268341064\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.079834461212158 | KNN Loss: 3.050854206085205 | BCE Loss: 1.0289802551269531\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.090903282165527 | KNN Loss: 3.0868642330169678 | BCE Loss: 1.0040390491485596\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.139348983764648 | KNN Loss: 3.096921920776367 | BCE Loss: 1.0424268245697021\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.08206844329834 | KNN Loss: 3.071648120880127 | BCE Loss: 1.010420560836792\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.119138717651367 | KNN Loss: 3.0740432739257812 | BCE Loss: 1.0450952053070068\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.105831146240234 | KNN Loss: 3.0794923305511475 | BCE Loss: 1.0263385772705078\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.1431756019592285 | KNN Loss: 3.0634851455688477 | BCE Loss: 1.0796905755996704\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.12220573425293 | KNN Loss: 3.094724655151367 | BCE Loss: 1.0274813175201416\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.114368915557861 | KNN Loss: 3.080449342727661 | BCE Loss: 1.0339195728302002\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.069101333618164 | KNN Loss: 3.0681233406066895 | BCE Loss: 1.0009779930114746\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.102047443389893 | KNN Loss: 3.0994131565093994 | BCE Loss: 1.0026344060897827\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.073290824890137 | KNN Loss: 3.0611586570739746 | BCE Loss: 1.012131929397583\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.070361614227295 | KNN Loss: 3.0821971893310547 | BCE Loss: 0.9881642460823059\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.096688270568848 | KNN Loss: 3.0787482261657715 | BCE Loss: 1.0179400444030762\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.099038600921631 | KNN Loss: 3.0728650093078613 | BCE Loss: 1.02617347240448\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.133237361907959 | KNN Loss: 3.0976877212524414 | BCE Loss: 1.0355496406555176\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.094547271728516 | KNN Loss: 3.075575828552246 | BCE Loss: 1.0189716815948486\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.06716775894165 | KNN Loss: 3.040541410446167 | BCE Loss: 1.026626467704773\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.082248687744141 | KNN Loss: 3.0630712509155273 | BCE Loss: 1.0191774368286133\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.128900527954102 | KNN Loss: 3.0796475410461426 | BCE Loss: 1.0492527484893799\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.1021223068237305 | KNN Loss: 3.0716495513916016 | BCE Loss: 1.030472755432129\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.079208850860596 | KNN Loss: 3.069499969482422 | BCE Loss: 1.0097087621688843\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.104379653930664 | KNN Loss: 3.092106819152832 | BCE Loss: 1.012272596359253\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.102404594421387 | KNN Loss: 3.0794155597686768 | BCE Loss: 1.0229889154434204\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.121589660644531 | KNN Loss: 3.087071180343628 | BCE Loss: 1.0345185995101929\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.12397575378418 | KNN Loss: 3.1147799491882324 | BCE Loss: 1.0091958045959473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.127488136291504 | KNN Loss: 3.0967600345611572 | BCE Loss: 1.0307281017303467\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.08768892288208 | KNN Loss: 3.0845706462860107 | BCE Loss: 1.0031182765960693\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.069705486297607 | KNN Loss: 3.0448086261749268 | BCE Loss: 1.0248967409133911\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.103206634521484 | KNN Loss: 3.0649547576904297 | BCE Loss: 1.0382518768310547\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.0718159675598145 | KNN Loss: 3.0555579662323 | BCE Loss: 1.0162581205368042\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.059959411621094 | KNN Loss: 3.064202308654785 | BCE Loss: 0.9957571029663086\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.129817008972168 | KNN Loss: 3.095857620239258 | BCE Loss: 1.0339596271514893\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.087767601013184 | KNN Loss: 3.0970308780670166 | BCE Loss: 0.990736722946167\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.101686477661133 | KNN Loss: 3.0845937728881836 | BCE Loss: 1.0170925855636597\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.141853332519531 | KNN Loss: 3.0954084396362305 | BCE Loss: 1.0464451313018799\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.083495140075684 | KNN Loss: 3.037245512008667 | BCE Loss: 1.0462493896484375\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.089140892028809 | KNN Loss: 3.0665812492370605 | BCE Loss: 1.0225595235824585\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.097863674163818 | KNN Loss: 3.0788798332214355 | BCE Loss: 1.0189838409423828\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.12852668762207 | KNN Loss: 3.106820821762085 | BCE Loss: 1.021705985069275\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.127093315124512 | KNN Loss: 3.1024575233459473 | BCE Loss: 1.0246355533599854\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.055227756500244 | KNN Loss: 3.059257745742798 | BCE Loss: 0.9959699511528015\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.058526992797852 | KNN Loss: 3.05428409576416 | BCE Loss: 1.0042427778244019\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.133588790893555 | KNN Loss: 3.119093418121338 | BCE Loss: 1.014495611190796\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.122338771820068 | KNN Loss: 3.0895473957061768 | BCE Loss: 1.032791256904602\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.065807819366455 | KNN Loss: 3.053805112838745 | BCE Loss: 1.01200270652771\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.102359771728516 | KNN Loss: 3.0691487789154053 | BCE Loss: 1.0332109928131104\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.089064598083496 | KNN Loss: 3.0651845932006836 | BCE Loss: 1.023880124092102\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.0611371994018555 | KNN Loss: 3.0566205978393555 | BCE Loss: 1.0045164823532104\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.141523361206055 | KNN Loss: 3.102508068084717 | BCE Loss: 1.039015531539917\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.063045024871826 | KNN Loss: 3.0629289150238037 | BCE Loss: 1.0001161098480225\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.133578777313232 | KNN Loss: 3.0826613903045654 | BCE Loss: 1.050917387008667\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.069950103759766 | KNN Loss: 3.0662145614624023 | BCE Loss: 1.0037353038787842\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.097970962524414 | KNN Loss: 3.0688626766204834 | BCE Loss: 1.0291084051132202\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.078769683837891 | KNN Loss: 3.0609893798828125 | BCE Loss: 1.0177805423736572\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.107918739318848 | KNN Loss: 3.096190929412842 | BCE Loss: 1.0117276906967163\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.07745361328125 | KNN Loss: 3.0661799907684326 | BCE Loss: 1.0112736225128174\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.075768947601318 | KNN Loss: 3.0682239532470703 | BCE Loss: 1.0075451135635376\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.115834712982178 | KNN Loss: 3.1024169921875 | BCE Loss: 1.0134178400039673\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.137330532073975 | KNN Loss: 3.118650436401367 | BCE Loss: 1.0186799764633179\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.124544620513916 | KNN Loss: 3.097022294998169 | BCE Loss: 1.0275222063064575\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.111073017120361 | KNN Loss: 3.1064884662628174 | BCE Loss: 1.0045846700668335\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.154467582702637 | KNN Loss: 3.0917606353759766 | BCE Loss: 1.062706708908081\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.178175449371338 | KNN Loss: 3.1411192417144775 | BCE Loss: 1.0370562076568604\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.118966102600098 | KNN Loss: 3.1023263931274414 | BCE Loss: 1.0166397094726562\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.105552673339844 | KNN Loss: 3.0835156440734863 | BCE Loss: 1.022037148475647\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.089565277099609 | KNN Loss: 3.0699377059936523 | BCE Loss: 1.0196278095245361\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.120550632476807 | KNN Loss: 3.093111753463745 | BCE Loss: 1.0274388790130615\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.111502647399902 | KNN Loss: 3.079922914505005 | BCE Loss: 1.0315794944763184\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.1198930740356445 | KNN Loss: 3.069288969039917 | BCE Loss: 1.050604224205017\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.060025215148926 | KNN Loss: 3.0637989044189453 | BCE Loss: 0.9962260723114014\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.057122230529785 | KNN Loss: 3.0427331924438477 | BCE Loss: 1.0143887996673584\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.079120635986328 | KNN Loss: 3.054537296295166 | BCE Loss: 1.024583339691162\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.092001438140869 | KNN Loss: 3.0923492908477783 | BCE Loss: 0.9996520280838013\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.111323356628418 | KNN Loss: 3.0856266021728516 | BCE Loss: 1.0256965160369873\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.074100494384766 | KNN Loss: 3.0651347637176514 | BCE Loss: 1.0089654922485352\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.060486316680908 | KNN Loss: 3.0336077213287354 | BCE Loss: 1.0268785953521729\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.1104207038879395 | KNN Loss: 3.073603868484497 | BCE Loss: 1.0368168354034424\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.061634540557861 | KNN Loss: 3.0640604496002197 | BCE Loss: 0.997573971748352\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.13240385055542 | KNN Loss: 3.101921558380127 | BCE Loss: 1.030482292175293\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.1014814376831055 | KNN Loss: 3.0771396160125732 | BCE Loss: 1.0243415832519531\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.102010250091553 | KNN Loss: 3.0918776988983154 | BCE Loss: 1.0101325511932373\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.092040061950684 | KNN Loss: 3.0583999156951904 | BCE Loss: 1.0336401462554932\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.149859428405762 | KNN Loss: 3.1104354858398438 | BCE Loss: 1.039423942565918\n",
      "Epoch   139: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.106810569763184 | KNN Loss: 3.066753387451172 | BCE Loss: 1.0400574207305908\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.104601860046387 | KNN Loss: 3.0947296619415283 | BCE Loss: 1.0098719596862793\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.104220390319824 | KNN Loss: 3.0689024925231934 | BCE Loss: 1.0353178977966309\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.037744045257568 | KNN Loss: 3.0522873401641846 | BCE Loss: 0.9854568839073181\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.097679138183594 | KNN Loss: 3.082350730895996 | BCE Loss: 1.0153281688690186\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.124380111694336 | KNN Loss: 3.0941309928894043 | BCE Loss: 1.0302493572235107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.087669849395752 | KNN Loss: 3.072147846221924 | BCE Loss: 1.0155220031738281\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.066332817077637 | KNN Loss: 3.042588710784912 | BCE Loss: 1.0237441062927246\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.075277328491211 | KNN Loss: 3.0483484268188477 | BCE Loss: 1.0269291400909424\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.139771938323975 | KNN Loss: 3.120537757873535 | BCE Loss: 1.019234299659729\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.068439960479736 | KNN Loss: 3.0506632328033447 | BCE Loss: 1.0177768468856812\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.093867301940918 | KNN Loss: 3.0898005962371826 | BCE Loss: 1.0040664672851562\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.105763912200928 | KNN Loss: 3.1022231578826904 | BCE Loss: 1.0035407543182373\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.129912376403809 | KNN Loss: 3.090496301651001 | BCE Loss: 1.039415955543518\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.0583343505859375 | KNN Loss: 3.0662429332733154 | BCE Loss: 0.992091178894043\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.078575134277344 | KNN Loss: 3.085658550262451 | BCE Loss: 0.9929165244102478\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.114650726318359 | KNN Loss: 3.0936951637268066 | BCE Loss: 1.0209558010101318\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.123038291931152 | KNN Loss: 3.0920989513397217 | BCE Loss: 1.0309391021728516\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.080216407775879 | KNN Loss: 3.0397534370422363 | BCE Loss: 1.0404632091522217\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.094881534576416 | KNN Loss: 3.08246111869812 | BCE Loss: 1.012420415878296\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.09539270401001 | KNN Loss: 3.06400990486145 | BCE Loss: 1.0313827991485596\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.072375774383545 | KNN Loss: 3.0644586086273193 | BCE Loss: 1.007917046546936\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.099637031555176 | KNN Loss: 3.0765256881713867 | BCE Loss: 1.02311110496521\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.119312763214111 | KNN Loss: 3.0846617221832275 | BCE Loss: 1.0346510410308838\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.114593029022217 | KNN Loss: 3.084592819213867 | BCE Loss: 1.03000009059906\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.111494541168213 | KNN Loss: 3.0905215740203857 | BCE Loss: 1.0209728479385376\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.115993499755859 | KNN Loss: 3.1103949546813965 | BCE Loss: 1.0055983066558838\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.174829959869385 | KNN Loss: 3.1107699871063232 | BCE Loss: 1.064059853553772\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.180305480957031 | KNN Loss: 3.1278562545776367 | BCE Loss: 1.0524494647979736\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.093786239624023 | KNN Loss: 3.0770392417907715 | BCE Loss: 1.016747236251831\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.087906360626221 | KNN Loss: 3.0743603706359863 | BCE Loss: 1.0135459899902344\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.089468479156494 | KNN Loss: 3.0736286640167236 | BCE Loss: 1.0158398151397705\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.075672626495361 | KNN Loss: 3.061246156692505 | BCE Loss: 1.0144264698028564\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.106490135192871 | KNN Loss: 3.0558087825775146 | BCE Loss: 1.0506813526153564\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.075932025909424 | KNN Loss: 3.0696139335632324 | BCE Loss: 1.006318211555481\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.0401811599731445 | KNN Loss: 3.0482017993927 | BCE Loss: 0.9919793605804443\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.059477806091309 | KNN Loss: 3.0520179271698 | BCE Loss: 1.0074596405029297\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.118844985961914 | KNN Loss: 3.0881600379943848 | BCE Loss: 1.0306849479675293\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.062121868133545 | KNN Loss: 3.0721278190612793 | BCE Loss: 0.9899941682815552\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.13490629196167 | KNN Loss: 3.0996451377868652 | BCE Loss: 1.0352611541748047\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.086126327514648 | KNN Loss: 3.0685651302337646 | BCE Loss: 1.0175613164901733\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.119764804840088 | KNN Loss: 3.0678577423095703 | BCE Loss: 1.0519070625305176\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.053651332855225 | KNN Loss: 3.039422035217285 | BCE Loss: 1.0142292976379395\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.068671703338623 | KNN Loss: 3.078733205795288 | BCE Loss: 0.9899386763572693\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.100432395935059 | KNN Loss: 3.080308198928833 | BCE Loss: 1.0201241970062256\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.123544692993164 | KNN Loss: 3.0748560428619385 | BCE Loss: 1.048688530921936\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.108071327209473 | KNN Loss: 3.097754955291748 | BCE Loss: 1.0103163719177246\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.1089019775390625 | KNN Loss: 3.0761654376983643 | BCE Loss: 1.0327366590499878\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.110245227813721 | KNN Loss: 3.0859720706939697 | BCE Loss: 1.024273157119751\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.073606014251709 | KNN Loss: 3.0586979389190674 | BCE Loss: 1.0149080753326416\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.064077377319336 | KNN Loss: 3.0692758560180664 | BCE Loss: 0.99480140209198\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.099051475524902 | KNN Loss: 3.049293041229248 | BCE Loss: 1.0497586727142334\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.045696258544922 | KNN Loss: 3.053191661834717 | BCE Loss: 0.992504358291626\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.118853569030762 | KNN Loss: 3.0787911415100098 | BCE Loss: 1.0400621891021729\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.168028831481934 | KNN Loss: 3.121155023574829 | BCE Loss: 1.046873688697815\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.120718002319336 | KNN Loss: 3.100461959838867 | BCE Loss: 1.0202560424804688\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.106878280639648 | KNN Loss: 3.0624942779541016 | BCE Loss: 1.044384241104126\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.093140125274658 | KNN Loss: 3.091675043106079 | BCE Loss: 1.0014652013778687\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.081537246704102 | KNN Loss: 3.0660905838012695 | BCE Loss: 1.015446424484253\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.092538833618164 | KNN Loss: 3.0844686031341553 | BCE Loss: 1.0080699920654297\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.117199897766113 | KNN Loss: 3.105389356613159 | BCE Loss: 1.011810541152954\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.104750156402588 | KNN Loss: 3.0883615016937256 | BCE Loss: 1.0163886547088623\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.069896697998047 | KNN Loss: 3.094017267227173 | BCE Loss: 0.9758795499801636\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.083604335784912 | KNN Loss: 3.051377296447754 | BCE Loss: 1.0322270393371582\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.130793571472168 | KNN Loss: 3.0995566844940186 | BCE Loss: 1.0312368869781494\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.1153154373168945 | KNN Loss: 3.0910587310791016 | BCE Loss: 1.024256944656372\n",
      "Epoch   150: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.078058242797852 | KNN Loss: 3.0750362873077393 | BCE Loss: 1.0030219554901123\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.039530277252197 | KNN Loss: 3.0533671379089355 | BCE Loss: 0.9861632585525513\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.1282548904418945 | KNN Loss: 3.077017068862915 | BCE Loss: 1.0512378215789795\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.0878682136535645 | KNN Loss: 3.050147294998169 | BCE Loss: 1.037720799446106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.112130641937256 | KNN Loss: 3.090085983276367 | BCE Loss: 1.0220447778701782\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.1304144859313965 | KNN Loss: 3.0965538024902344 | BCE Loss: 1.033860683441162\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.101290225982666 | KNN Loss: 3.096068859100342 | BCE Loss: 1.0052212476730347\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.12487268447876 | KNN Loss: 3.096296787261963 | BCE Loss: 1.0285760164260864\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.109187126159668 | KNN Loss: 3.067141056060791 | BCE Loss: 1.042046070098877\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.133791923522949 | KNN Loss: 3.114473342895508 | BCE Loss: 1.0193188190460205\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.106403827667236 | KNN Loss: 3.0850844383239746 | BCE Loss: 1.0213193893432617\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.1623640060424805 | KNN Loss: 3.130061626434326 | BCE Loss: 1.0323021411895752\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.094564437866211 | KNN Loss: 3.0464060306549072 | BCE Loss: 1.0481581687927246\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.064935684204102 | KNN Loss: 3.062439441680908 | BCE Loss: 1.0024964809417725\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.113201141357422 | KNN Loss: 3.08925724029541 | BCE Loss: 1.0239437818527222\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.1020331382751465 | KNN Loss: 3.0770130157470703 | BCE Loss: 1.0250201225280762\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.124013900756836 | KNN Loss: 3.096221923828125 | BCE Loss: 1.027791976928711\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.065561294555664 | KNN Loss: 3.0554754734039307 | BCE Loss: 1.0100858211517334\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.108784198760986 | KNN Loss: 3.109776020050049 | BCE Loss: 0.9990082383155823\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.081185340881348 | KNN Loss: 3.070295810699463 | BCE Loss: 1.0108897686004639\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.034372806549072 | KNN Loss: 3.0425240993499756 | BCE Loss: 0.9918486475944519\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.079654693603516 | KNN Loss: 3.073918104171753 | BCE Loss: 1.0057368278503418\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.076851844787598 | KNN Loss: 3.059577703475952 | BCE Loss: 1.0172739028930664\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.060913562774658 | KNN Loss: 3.046538829803467 | BCE Loss: 1.0143747329711914\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.111506938934326 | KNN Loss: 3.09712815284729 | BCE Loss: 1.0143787860870361\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.061617374420166 | KNN Loss: 3.057915687561035 | BCE Loss: 1.0037016868591309\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.104217052459717 | KNN Loss: 3.0728652477264404 | BCE Loss: 1.031351923942566\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.122730731964111 | KNN Loss: 3.1167502403259277 | BCE Loss: 1.0059804916381836\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.114020824432373 | KNN Loss: 3.08697772026062 | BCE Loss: 1.0270432233810425\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.0752716064453125 | KNN Loss: 3.0845985412597656 | BCE Loss: 0.990673303604126\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.092009544372559 | KNN Loss: 3.0805482864379883 | BCE Loss: 1.0114612579345703\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.075252532958984 | KNN Loss: 3.0635054111480713 | BCE Loss: 1.0117470026016235\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.105033874511719 | KNN Loss: 3.102050304412842 | BCE Loss: 1.0029836893081665\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.121181488037109 | KNN Loss: 3.105184316635132 | BCE Loss: 1.015997052192688\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.091915607452393 | KNN Loss: 3.083219289779663 | BCE Loss: 1.008696436882019\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.092464447021484 | KNN Loss: 3.0703136920928955 | BCE Loss: 1.0221505165100098\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.102769374847412 | KNN Loss: 3.0675671100616455 | BCE Loss: 1.0352023839950562\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.0670976638793945 | KNN Loss: 3.06714129447937 | BCE Loss: 0.9999563097953796\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.060269832611084 | KNN Loss: 3.040336847305298 | BCE Loss: 1.0199331045150757\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.066873550415039 | KNN Loss: 3.0596587657928467 | BCE Loss: 1.0072145462036133\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.070475101470947 | KNN Loss: 3.05415940284729 | BCE Loss: 1.0163158178329468\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.080414772033691 | KNN Loss: 3.043698310852051 | BCE Loss: 1.0367166996002197\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.126765251159668 | KNN Loss: 3.0830745697021484 | BCE Loss: 1.0436909198760986\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.129153251647949 | KNN Loss: 3.1078202724456787 | BCE Loss: 1.0213327407836914\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.059679985046387 | KNN Loss: 3.0491652488708496 | BCE Loss: 1.0105148553848267\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.119926452636719 | KNN Loss: 3.0858542919158936 | BCE Loss: 1.034071922302246\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.107084274291992 | KNN Loss: 3.077418327331543 | BCE Loss: 1.0296657085418701\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.085546970367432 | KNN Loss: 3.0857529640197754 | BCE Loss: 0.9997938275337219\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.090877532958984 | KNN Loss: 3.092149496078491 | BCE Loss: 0.9987282752990723\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.0618896484375 | KNN Loss: 3.0462324619293213 | BCE Loss: 1.0156571865081787\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.078204154968262 | KNN Loss: 3.059433937072754 | BCE Loss: 1.0187702178955078\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.089054107666016 | KNN Loss: 3.0770702362060547 | BCE Loss: 1.0119836330413818\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.07375431060791 | KNN Loss: 3.063380718231201 | BCE Loss: 1.010373830795288\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.056997776031494 | KNN Loss: 3.0635530948638916 | BCE Loss: 0.993444561958313\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.144087791442871 | KNN Loss: 3.1045451164245605 | BCE Loss: 1.0395427942276\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.1113362312316895 | KNN Loss: 3.1229305267333984 | BCE Loss: 0.9884058833122253\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.077600479125977 | KNN Loss: 3.0615718364715576 | BCE Loss: 1.016028642654419\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.039431571960449 | KNN Loss: 3.0481743812561035 | BCE Loss: 0.9912571907043457\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.085260391235352 | KNN Loss: 3.040313720703125 | BCE Loss: 1.0449464321136475\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.077971458435059 | KNN Loss: 3.0492026805877686 | BCE Loss: 1.028768539428711\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.101261138916016 | KNN Loss: 3.0801122188568115 | BCE Loss: 1.0211488008499146\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.0475921630859375 | KNN Loss: 3.059504508972168 | BCE Loss: 0.9880876541137695\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.059505462646484 | KNN Loss: 3.0429282188415527 | BCE Loss: 1.0165772438049316\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.124044418334961 | KNN Loss: 3.0883629322052 | BCE Loss: 1.0356812477111816\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.069693088531494 | KNN Loss: 3.060696601867676 | BCE Loss: 1.0089963674545288\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.091493129730225 | KNN Loss: 3.0761866569519043 | BCE Loss: 1.0153064727783203\n",
      "Epoch   161: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.075501441955566 | KNN Loss: 3.0731537342071533 | BCE Loss: 1.002347469329834\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.144737243652344 | KNN Loss: 3.113762140274048 | BCE Loss: 1.0309748649597168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.118540287017822 | KNN Loss: 3.0823333263397217 | BCE Loss: 1.036206841468811\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.107219696044922 | KNN Loss: 3.086304187774658 | BCE Loss: 1.0209152698516846\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.0495100021362305 | KNN Loss: 3.0572421550750732 | BCE Loss: 0.9922676682472229\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.108197212219238 | KNN Loss: 3.083733320236206 | BCE Loss: 1.0244641304016113\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.090916633605957 | KNN Loss: 3.1008825302124023 | BCE Loss: 0.9900342226028442\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.107399940490723 | KNN Loss: 3.077390670776367 | BCE Loss: 1.0300090312957764\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.107733726501465 | KNN Loss: 3.115868091583252 | BCE Loss: 0.991865873336792\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.129289627075195 | KNN Loss: 3.0804851055145264 | BCE Loss: 1.048804759979248\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.1046271324157715 | KNN Loss: 3.0545899868011475 | BCE Loss: 1.0500370264053345\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.069736957550049 | KNN Loss: 3.0663866996765137 | BCE Loss: 1.0033502578735352\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.067991733551025 | KNN Loss: 3.0671768188476562 | BCE Loss: 1.0008147954940796\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.1247453689575195 | KNN Loss: 3.1125292778015137 | BCE Loss: 1.0122158527374268\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.072656154632568 | KNN Loss: 3.0851082801818848 | BCE Loss: 0.9875480532646179\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.05361795425415 | KNN Loss: 3.0515146255493164 | BCE Loss: 1.0021032094955444\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.077356338500977 | KNN Loss: 3.055128574371338 | BCE Loss: 1.0222276449203491\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.094130039215088 | KNN Loss: 3.0838277339935303 | BCE Loss: 1.0103024244308472\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.108787536621094 | KNN Loss: 3.0923874378204346 | BCE Loss: 1.0164002180099487\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.08348274230957 | KNN Loss: 3.0463056564331055 | BCE Loss: 1.0371768474578857\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.052166938781738 | KNN Loss: 3.057494878768921 | BCE Loss: 0.9946718811988831\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.1102213859558105 | KNN Loss: 3.0909478664398193 | BCE Loss: 1.0192735195159912\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.062554836273193 | KNN Loss: 3.068025588989258 | BCE Loss: 0.9945293664932251\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.1092305183410645 | KNN Loss: 3.0832512378692627 | BCE Loss: 1.0259793996810913\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.091647148132324 | KNN Loss: 3.0764331817626953 | BCE Loss: 1.015213966369629\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.082140922546387 | KNN Loss: 3.062467098236084 | BCE Loss: 1.0196737051010132\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.1072516441345215 | KNN Loss: 3.0544583797454834 | BCE Loss: 1.052793264389038\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.082780838012695 | KNN Loss: 3.0853171348571777 | BCE Loss: 0.9974639415740967\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.082523345947266 | KNN Loss: 3.0683505535125732 | BCE Loss: 1.0141727924346924\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.136075973510742 | KNN Loss: 3.0886662006378174 | BCE Loss: 1.0474095344543457\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.119636535644531 | KNN Loss: 3.094550609588623 | BCE Loss: 1.0250859260559082\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.109213352203369 | KNN Loss: 3.0735607147216797 | BCE Loss: 1.0356525182724\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.105236053466797 | KNN Loss: 3.0620198249816895 | BCE Loss: 1.043216347694397\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.0765252113342285 | KNN Loss: 3.0959908962249756 | BCE Loss: 0.9805344939231873\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.117323398590088 | KNN Loss: 3.0902624130249023 | BCE Loss: 1.027061104774475\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.081907272338867 | KNN Loss: 3.0720901489257812 | BCE Loss: 1.0098170042037964\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.064197540283203 | KNN Loss: 3.009199380874634 | BCE Loss: 1.0549979209899902\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.066941738128662 | KNN Loss: 3.065065383911133 | BCE Loss: 1.0018762350082397\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.121404647827148 | KNN Loss: 3.0879809856414795 | BCE Loss: 1.0334234237670898\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.027047157287598 | KNN Loss: 3.026837110519409 | BCE Loss: 1.0002102851867676\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.118059158325195 | KNN Loss: 3.1060705184936523 | BCE Loss: 1.011988878250122\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.103525161743164 | KNN Loss: 3.087958335876465 | BCE Loss: 1.0155665874481201\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.080321788787842 | KNN Loss: 3.0516841411590576 | BCE Loss: 1.0286376476287842\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.089905261993408 | KNN Loss: 3.0576999187469482 | BCE Loss: 1.03220534324646\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.077371597290039 | KNN Loss: 3.0630674362182617 | BCE Loss: 1.014304280281067\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.130002021789551 | KNN Loss: 3.0818560123443604 | BCE Loss: 1.0481462478637695\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.115448474884033 | KNN Loss: 3.0692965984344482 | BCE Loss: 1.046151876449585\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.068513870239258 | KNN Loss: 3.078260898590088 | BCE Loss: 0.9902529716491699\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.136139392852783 | KNN Loss: 3.107964038848877 | BCE Loss: 1.0281754732131958\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.097422122955322 | KNN Loss: 3.0467774868011475 | BCE Loss: 1.0506445169448853\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.078642845153809 | KNN Loss: 3.047464370727539 | BCE Loss: 1.0311784744262695\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.078097343444824 | KNN Loss: 3.061519145965576 | BCE Loss: 1.016577959060669\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.073700904846191 | KNN Loss: 3.0449180603027344 | BCE Loss: 1.028782844543457\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.070143699645996 | KNN Loss: 3.0467514991760254 | BCE Loss: 1.0233923196792603\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.069155693054199 | KNN Loss: 3.0433530807495117 | BCE Loss: 1.0258023738861084\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.035120964050293 | KNN Loss: 3.024198532104492 | BCE Loss: 1.0109226703643799\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.100074291229248 | KNN Loss: 3.0625948905944824 | BCE Loss: 1.0374795198440552\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.047728538513184 | KNN Loss: 3.04451847076416 | BCE Loss: 1.003210186958313\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.100439071655273 | KNN Loss: 3.081085681915283 | BCE Loss: 1.0193532705307007\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.085180282592773 | KNN Loss: 3.076230764389038 | BCE Loss: 1.0089492797851562\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.104283809661865 | KNN Loss: 3.0884411334991455 | BCE Loss: 1.0158426761627197\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.038134574890137 | KNN Loss: 3.0284502506256104 | BCE Loss: 1.0096843242645264\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.107959747314453 | KNN Loss: 3.076669931411743 | BCE Loss: 1.03128981590271\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.117793083190918 | KNN Loss: 3.077775001525879 | BCE Loss: 1.04001784324646\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.079574108123779 | KNN Loss: 3.0629801750183105 | BCE Loss: 1.0165939331054688\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.1116485595703125 | KNN Loss: 3.088862895965576 | BCE Loss: 1.0227856636047363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.082667350769043 | KNN Loss: 3.070828437805176 | BCE Loss: 1.011838674545288\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.105261325836182 | KNN Loss: 3.081597089767456 | BCE Loss: 1.0236642360687256\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.077622413635254 | KNN Loss: 3.059946298599243 | BCE Loss: 1.0176758766174316\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.094514846801758 | KNN Loss: 3.0599417686462402 | BCE Loss: 1.0345730781555176\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.062402248382568 | KNN Loss: 3.0536224842071533 | BCE Loss: 1.008779764175415\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.062097072601318 | KNN Loss: 3.0508034229278564 | BCE Loss: 1.0112935304641724\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.069813251495361 | KNN Loss: 3.0651464462280273 | BCE Loss: 1.0046666860580444\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.118173122406006 | KNN Loss: 3.06518816947937 | BCE Loss: 1.0529850721359253\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.046236038208008 | KNN Loss: 3.0253818035125732 | BCE Loss: 1.0208543539047241\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.124035835266113 | KNN Loss: 3.0855085849761963 | BCE Loss: 1.038527250289917\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.030778884887695 | KNN Loss: 3.02659010887146 | BCE Loss: 1.004188895225525\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.093733787536621 | KNN Loss: 3.074582099914551 | BCE Loss: 1.0191519260406494\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.1119914054870605 | KNN Loss: 3.092667818069458 | BCE Loss: 1.019323468208313\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.122837543487549 | KNN Loss: 3.1037070751190186 | BCE Loss: 1.0191304683685303\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.068526744842529 | KNN Loss: 3.058655261993408 | BCE Loss: 1.009871482849121\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.0812788009643555 | KNN Loss: 3.047417640686035 | BCE Loss: 1.0338611602783203\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.09511661529541 | KNN Loss: 3.0535855293273926 | BCE Loss: 1.041530966758728\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.105859756469727 | KNN Loss: 3.0597214698791504 | BCE Loss: 1.046138048171997\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.109580039978027 | KNN Loss: 3.068399667739868 | BCE Loss: 1.0411803722381592\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.098834991455078 | KNN Loss: 3.0657272338867188 | BCE Loss: 1.0331077575683594\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.099823951721191 | KNN Loss: 3.089709758758545 | BCE Loss: 1.0101139545440674\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.064115524291992 | KNN Loss: 3.0539584159851074 | BCE Loss: 1.0101569890975952\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.046360015869141 | KNN Loss: 3.0643694400787354 | BCE Loss: 0.9819908142089844\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.086812973022461 | KNN Loss: 3.0687918663024902 | BCE Loss: 1.0180212259292603\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.104085922241211 | KNN Loss: 3.0769641399383545 | BCE Loss: 1.0271220207214355\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.078062534332275 | KNN Loss: 3.0468297004699707 | BCE Loss: 1.0312329530715942\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.070468902587891 | KNN Loss: 3.074625253677368 | BCE Loss: 0.9958436489105225\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.072931289672852 | KNN Loss: 3.058418035507202 | BCE Loss: 1.0145130157470703\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.091376304626465 | KNN Loss: 3.089632749557495 | BCE Loss: 1.0017433166503906\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.116249084472656 | KNN Loss: 3.087334394454956 | BCE Loss: 1.0289149284362793\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.0947465896606445 | KNN Loss: 3.0739476680755615 | BCE Loss: 1.020799160003662\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.121952056884766 | KNN Loss: 3.1073529720306396 | BCE Loss: 1.0145988464355469\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.100870132446289 | KNN Loss: 3.0761353969573975 | BCE Loss: 1.0247347354888916\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.1157546043396 | KNN Loss: 3.0933728218078613 | BCE Loss: 1.0223819017410278\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.071937084197998 | KNN Loss: 3.05944561958313 | BCE Loss: 1.0124914646148682\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.095044136047363 | KNN Loss: 3.07572603225708 | BCE Loss: 1.019317865371704\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.088765621185303 | KNN Loss: 3.068511724472046 | BCE Loss: 1.0202538967132568\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.120061874389648 | KNN Loss: 3.0942482948303223 | BCE Loss: 1.0258138179779053\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.121364593505859 | KNN Loss: 3.1023199558258057 | BCE Loss: 1.0190445184707642\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.081279277801514 | KNN Loss: 3.0552687644958496 | BCE Loss: 1.026010513305664\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.074581146240234 | KNN Loss: 3.0592124462127686 | BCE Loss: 1.0153688192367554\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.116114139556885 | KNN Loss: 3.094223976135254 | BCE Loss: 1.0218901634216309\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.079472541809082 | KNN Loss: 3.041288137435913 | BCE Loss: 1.0381845235824585\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.092764854431152 | KNN Loss: 3.065382719039917 | BCE Loss: 1.0273821353912354\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.056825637817383 | KNN Loss: 3.042863607406616 | BCE Loss: 1.0139622688293457\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.077861785888672 | KNN Loss: 3.070918321609497 | BCE Loss: 1.0069434642791748\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.053165435791016 | KNN Loss: 3.0457262992858887 | BCE Loss: 1.0074388980865479\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.078835964202881 | KNN Loss: 3.0587503910064697 | BCE Loss: 1.0200855731964111\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.121243953704834 | KNN Loss: 3.091641664505005 | BCE Loss: 1.029602289199829\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.038063049316406 | KNN Loss: 3.0461525917053223 | BCE Loss: 0.9919105768203735\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.08220100402832 | KNN Loss: 3.0509872436523438 | BCE Loss: 1.031213641166687\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.093421936035156 | KNN Loss: 3.063014268875122 | BCE Loss: 1.0304077863693237\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.076871395111084 | KNN Loss: 3.042840003967285 | BCE Loss: 1.0340315103530884\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.047827243804932 | KNN Loss: 3.0498037338256836 | BCE Loss: 0.9980234503746033\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.084273338317871 | KNN Loss: 3.0666697025299072 | BCE Loss: 1.0176037549972534\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.071932792663574 | KNN Loss: 3.0707509517669678 | BCE Loss: 1.0011816024780273\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.1121063232421875 | KNN Loss: 3.0813376903533936 | BCE Loss: 1.0307683944702148\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.0869951248168945 | KNN Loss: 3.0654258728027344 | BCE Loss: 1.0215692520141602\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.077192783355713 | KNN Loss: 3.0663347244262695 | BCE Loss: 1.010858178138733\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.048794269561768 | KNN Loss: 3.0453898906707764 | BCE Loss: 1.0034044981002808\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.063722133636475 | KNN Loss: 3.0656919479370117 | BCE Loss: 0.9980300664901733\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.144311904907227 | KNN Loss: 3.0757336616516113 | BCE Loss: 1.0685780048370361\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.107780456542969 | KNN Loss: 3.0724711418151855 | BCE Loss: 1.0353094339370728\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.091283321380615 | KNN Loss: 3.0625178813934326 | BCE Loss: 1.0287654399871826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.077742576599121 | KNN Loss: 3.089813470840454 | BCE Loss: 0.9879289269447327\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.156537055969238 | KNN Loss: 3.1254093647003174 | BCE Loss: 1.031127691268921\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.09780740737915 | KNN Loss: 3.0823864936828613 | BCE Loss: 1.015420913696289\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.075613021850586 | KNN Loss: 3.0519683361053467 | BCE Loss: 1.0236444473266602\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.097132205963135 | KNN Loss: 3.0830821990966797 | BCE Loss: 1.0140501260757446\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.087172508239746 | KNN Loss: 3.086259603500366 | BCE Loss: 1.000913143157959\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.071048736572266 | KNN Loss: 3.057013750076294 | BCE Loss: 1.0140352249145508\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.075319766998291 | KNN Loss: 3.0735678672790527 | BCE Loss: 1.0017518997192383\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.078009605407715 | KNN Loss: 3.0544352531433105 | BCE Loss: 1.0235744714736938\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.107465744018555 | KNN Loss: 3.058743953704834 | BCE Loss: 1.0487220287322998\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.08625602722168 | KNN Loss: 3.0711586475372314 | BCE Loss: 1.0150971412658691\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.152568817138672 | KNN Loss: 3.087693214416504 | BCE Loss: 1.0648753643035889\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.0865478515625 | KNN Loss: 3.06483793258667 | BCE Loss: 1.0217097997665405\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.079710960388184 | KNN Loss: 3.0483996868133545 | BCE Loss: 1.03131103515625\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.06971549987793 | KNN Loss: 3.058682680130005 | BCE Loss: 1.0110329389572144\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.062817096710205 | KNN Loss: 3.04529070854187 | BCE Loss: 1.017526388168335\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.083728313446045 | KNN Loss: 3.069622755050659 | BCE Loss: 1.0141054391860962\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.0942158699035645 | KNN Loss: 3.0791962146759033 | BCE Loss: 1.0150195360183716\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.14109992980957 | KNN Loss: 3.100973606109619 | BCE Loss: 1.0401264429092407\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.141382217407227 | KNN Loss: 3.125946283340454 | BCE Loss: 1.0154361724853516\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.068420886993408 | KNN Loss: 3.0425033569335938 | BCE Loss: 1.0259175300598145\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.105245113372803 | KNN Loss: 3.071138858795166 | BCE Loss: 1.0341061353683472\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.02547025680542 | KNN Loss: 3.0395162105560303 | BCE Loss: 0.9859539866447449\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.102379322052002 | KNN Loss: 3.078657865524292 | BCE Loss: 1.02372145652771\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.076295852661133 | KNN Loss: 3.0623252391815186 | BCE Loss: 1.0139708518981934\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.127626419067383 | KNN Loss: 3.101123094558716 | BCE Loss: 1.026503324508667\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.094979286193848 | KNN Loss: 3.056042432785034 | BCE Loss: 1.038936972618103\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.077389240264893 | KNN Loss: 3.0640108585357666 | BCE Loss: 1.0133785009384155\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.0663838386535645 | KNN Loss: 3.0563650131225586 | BCE Loss: 1.0100188255310059\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.076340675354004 | KNN Loss: 3.0479462146759033 | BCE Loss: 1.0283942222595215\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.126237392425537 | KNN Loss: 3.094850778579712 | BCE Loss: 1.0313866138458252\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.124837398529053 | KNN Loss: 3.116755962371826 | BCE Loss: 1.0080815553665161\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.0599799156188965 | KNN Loss: 3.0536129474639893 | BCE Loss: 1.0063670873641968\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.0743303298950195 | KNN Loss: 3.0682120323181152 | BCE Loss: 1.0061185359954834\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.051959037780762 | KNN Loss: 3.0489695072174072 | BCE Loss: 1.0029897689819336\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.0414137840271 | KNN Loss: 3.047985553741455 | BCE Loss: 0.9934283494949341\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.05509090423584 | KNN Loss: 3.0421621799468994 | BCE Loss: 1.0129286050796509\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.12801456451416 | KNN Loss: 3.1133782863616943 | BCE Loss: 1.0146360397338867\n",
      "Epoch   189: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.0882368087768555 | KNN Loss: 3.046267032623291 | BCE Loss: 1.0419697761535645\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.103619575500488 | KNN Loss: 3.084043025970459 | BCE Loss: 1.0195767879486084\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.07193660736084 | KNN Loss: 3.0592761039733887 | BCE Loss: 1.0126607418060303\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.0991926193237305 | KNN Loss: 3.0738022327423096 | BCE Loss: 1.025390625\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.09941291809082 | KNN Loss: 3.078097343444824 | BCE Loss: 1.021315574645996\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.095178604125977 | KNN Loss: 3.0570428371429443 | BCE Loss: 1.0381355285644531\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.095347881317139 | KNN Loss: 3.082587957382202 | BCE Loss: 1.0127599239349365\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.1028313636779785 | KNN Loss: 3.0654187202453613 | BCE Loss: 1.0374125242233276\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.124856948852539 | KNN Loss: 3.1030921936035156 | BCE Loss: 1.0217647552490234\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.079474449157715 | KNN Loss: 3.049644947052002 | BCE Loss: 1.0298292636871338\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.076555252075195 | KNN Loss: 3.0558652877807617 | BCE Loss: 1.0206899642944336\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.035842418670654 | KNN Loss: 3.0511958599090576 | BCE Loss: 0.9846464395523071\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.0659308433532715 | KNN Loss: 3.0504565238952637 | BCE Loss: 1.0154743194580078\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.123385429382324 | KNN Loss: 3.0913989543914795 | BCE Loss: 1.0319864749908447\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.064446449279785 | KNN Loss: 3.0443687438964844 | BCE Loss: 1.0200777053833008\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.122173309326172 | KNN Loss: 3.103736639022827 | BCE Loss: 1.0184365510940552\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.0306901931762695 | KNN Loss: 3.0334229469299316 | BCE Loss: 0.9972671866416931\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.109009742736816 | KNN Loss: 3.093348503112793 | BCE Loss: 1.0156614780426025\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.13450813293457 | KNN Loss: 3.092679023742676 | BCE Loss: 1.0418293476104736\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.1038007736206055 | KNN Loss: 3.0726280212402344 | BCE Loss: 1.0311729907989502\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.036776542663574 | KNN Loss: 3.03905987739563 | BCE Loss: 0.9977165460586548\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.07887077331543 | KNN Loss: 3.0481951236724854 | BCE Loss: 1.0306756496429443\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.138763904571533 | KNN Loss: 3.099912643432617 | BCE Loss: 1.0388513803482056\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.063536167144775 | KNN Loss: 3.064321994781494 | BCE Loss: 0.9992139935493469\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.13008975982666 | KNN Loss: 3.0784342288970947 | BCE Loss: 1.0516554117202759\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.09354305267334 | KNN Loss: 3.0788824558258057 | BCE Loss: 1.0146607160568237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.075530052185059 | KNN Loss: 3.041816234588623 | BCE Loss: 1.0337138175964355\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.071645259857178 | KNN Loss: 3.048696517944336 | BCE Loss: 1.0229487419128418\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.098415374755859 | KNN Loss: 3.069889545440674 | BCE Loss: 1.0285258293151855\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.090778350830078 | KNN Loss: 3.0762500762939453 | BCE Loss: 1.0145280361175537\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.054068565368652 | KNN Loss: 3.038609027862549 | BCE Loss: 1.015459656715393\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.079634666442871 | KNN Loss: 3.064450979232788 | BCE Loss: 1.015183687210083\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.129920959472656 | KNN Loss: 3.093766927719116 | BCE Loss: 1.036153793334961\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.096051216125488 | KNN Loss: 3.101250410079956 | BCE Loss: 0.9948009252548218\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.077669143676758 | KNN Loss: 3.0416135787963867 | BCE Loss: 1.0360556840896606\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.0680155754089355 | KNN Loss: 3.0411698818206787 | BCE Loss: 1.0268456935882568\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.0677032470703125 | KNN Loss: 3.0442514419555664 | BCE Loss: 1.0234520435333252\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.042126655578613 | KNN Loss: 3.053006887435913 | BCE Loss: 0.9891197681427002\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.034143924713135 | KNN Loss: 3.0470733642578125 | BCE Loss: 0.9870705604553223\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.075216770172119 | KNN Loss: 3.0834968090057373 | BCE Loss: 0.9917201399803162\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.14241361618042 | KNN Loss: 3.0882434844970703 | BCE Loss: 1.0541702508926392\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.118171691894531 | KNN Loss: 3.0931458473205566 | BCE Loss: 1.025025725364685\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.071891784667969 | KNN Loss: 3.0425095558166504 | BCE Loss: 1.0293822288513184\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.052936553955078 | KNN Loss: 3.034996271133423 | BCE Loss: 1.0179401636123657\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.092537879943848 | KNN Loss: 3.046323537826538 | BCE Loss: 1.0462143421173096\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.029966831207275 | KNN Loss: 3.0196480751037598 | BCE Loss: 1.0103187561035156\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.048590660095215 | KNN Loss: 3.0569944381713867 | BCE Loss: 0.9915964603424072\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.080895900726318 | KNN Loss: 3.05289888381958 | BCE Loss: 1.0279970169067383\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.060917854309082 | KNN Loss: 3.024324655532837 | BCE Loss: 1.0365934371948242\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.07744026184082 | KNN Loss: 3.0544092655181885 | BCE Loss: 1.0230311155319214\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.115051746368408 | KNN Loss: 3.0706374645233154 | BCE Loss: 1.0444142818450928\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.117629528045654 | KNN Loss: 3.1071994304656982 | BCE Loss: 1.010430097579956\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.0741729736328125 | KNN Loss: 3.0567538738250732 | BCE Loss: 1.0174193382263184\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.036233425140381 | KNN Loss: 3.0213820934295654 | BCE Loss: 1.0148513317108154\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.08922004699707 | KNN Loss: 3.060615062713623 | BCE Loss: 1.0286047458648682\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.1133503913879395 | KNN Loss: 3.095534563064575 | BCE Loss: 1.0178159475326538\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.088320732116699 | KNN Loss: 3.06667423248291 | BCE Loss: 1.021646499633789\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.095494270324707 | KNN Loss: 3.066138982772827 | BCE Loss: 1.0293552875518799\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.161056995391846 | KNN Loss: 3.0959932804107666 | BCE Loss: 1.0650635957717896\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.068474292755127 | KNN Loss: 3.0549213886260986 | BCE Loss: 1.0135530233383179\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.075254440307617 | KNN Loss: 3.0508334636688232 | BCE Loss: 1.024421215057373\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.054610729217529 | KNN Loss: 3.0130553245544434 | BCE Loss: 1.0415555238723755\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.097127914428711 | KNN Loss: 3.0889854431152344 | BCE Loss: 1.008142352104187\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.0801239013671875 | KNN Loss: 3.0542240142822266 | BCE Loss: 1.0259000062942505\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.095818996429443 | KNN Loss: 3.0774803161621094 | BCE Loss: 1.018338680267334\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.107885360717773 | KNN Loss: 3.0638840198516846 | BCE Loss: 1.0440011024475098\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.060002326965332 | KNN Loss: 3.065075397491455 | BCE Loss: 0.9949268102645874\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.062738418579102 | KNN Loss: 3.054994821548462 | BCE Loss: 1.0077433586120605\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.037722110748291 | KNN Loss: 3.0449564456939697 | BCE Loss: 0.9927656650543213\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.0601806640625 | KNN Loss: 3.0429611206054688 | BCE Loss: 1.0172197818756104\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.068870544433594 | KNN Loss: 3.077662229537964 | BCE Loss: 0.9912083148956299\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.094247817993164 | KNN Loss: 3.073256254196167 | BCE Loss: 1.0209914445877075\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.065189838409424 | KNN Loss: 3.0485641956329346 | BCE Loss: 1.0166257619857788\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.105338096618652 | KNN Loss: 3.095008611679077 | BCE Loss: 1.0103297233581543\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.118339538574219 | KNN Loss: 3.0664806365966797 | BCE Loss: 1.051858901977539\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.091296195983887 | KNN Loss: 3.0763609409332275 | BCE Loss: 1.0149354934692383\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.146548271179199 | KNN Loss: 3.0984466075897217 | BCE Loss: 1.048101544380188\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.129698276519775 | KNN Loss: 3.1032745838165283 | BCE Loss: 1.0264238119125366\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.004479885101318 | KNN Loss: 3.0225658416748047 | BCE Loss: 0.9819141626358032\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.1031012535095215 | KNN Loss: 3.055356740951538 | BCE Loss: 1.0477445125579834\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.098062992095947 | KNN Loss: 3.032467842102051 | BCE Loss: 1.065595030784607\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.071893215179443 | KNN Loss: 3.0924606323242188 | BCE Loss: 0.9794327020645142\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.051606178283691 | KNN Loss: 3.0486397743225098 | BCE Loss: 1.0029664039611816\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.070425033569336 | KNN Loss: 3.0733261108398438 | BCE Loss: 0.9970986843109131\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.045373916625977 | KNN Loss: 3.0276246070861816 | BCE Loss: 1.0177490711212158\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.103926658630371 | KNN Loss: 3.1098530292510986 | BCE Loss: 0.9940733909606934\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.058189868927002 | KNN Loss: 3.0525035858154297 | BCE Loss: 1.0056862831115723\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.073138236999512 | KNN Loss: 3.0458688735961914 | BCE Loss: 1.0272691249847412\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.061585426330566 | KNN Loss: 3.063150405883789 | BCE Loss: 0.9984350800514221\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.105666637420654 | KNN Loss: 3.081306219100952 | BCE Loss: 1.0243605375289917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   204: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.125419616699219 | KNN Loss: 3.086040496826172 | BCE Loss: 1.0393792390823364\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.092495441436768 | KNN Loss: 3.0590319633483887 | BCE Loss: 1.0334633588790894\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.099667549133301 | KNN Loss: 3.0697333812713623 | BCE Loss: 1.0299339294433594\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.077828884124756 | KNN Loss: 3.0676543712615967 | BCE Loss: 1.0101745128631592\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.065883636474609 | KNN Loss: 3.0601091384887695 | BCE Loss: 1.005774736404419\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.1445441246032715 | KNN Loss: 3.111586332321167 | BCE Loss: 1.0329577922821045\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.07792854309082 | KNN Loss: 3.0669896602630615 | BCE Loss: 1.010939121246338\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.082094192504883 | KNN Loss: 3.057875394821167 | BCE Loss: 1.0242185592651367\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.10637092590332 | KNN Loss: 3.085144281387329 | BCE Loss: 1.0212266445159912\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.082806587219238 | KNN Loss: 3.0543220043182373 | BCE Loss: 1.0284844636917114\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.093701362609863 | KNN Loss: 3.077256679534912 | BCE Loss: 1.0164446830749512\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.048333168029785 | KNN Loss: 3.0443854331970215 | BCE Loss: 1.0039479732513428\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.092010498046875 | KNN Loss: 3.054612636566162 | BCE Loss: 1.0373976230621338\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.031421661376953 | KNN Loss: 3.035964012145996 | BCE Loss: 0.9954575300216675\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.085231781005859 | KNN Loss: 3.0661356449127197 | BCE Loss: 1.0190961360931396\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.099717140197754 | KNN Loss: 3.0498154163360596 | BCE Loss: 1.0499016046524048\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.094058036804199 | KNN Loss: 3.0754082202911377 | BCE Loss: 1.018649935722351\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.128835201263428 | KNN Loss: 3.091590642929077 | BCE Loss: 1.0372446775436401\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.104136943817139 | KNN Loss: 3.067706823348999 | BCE Loss: 1.0364301204681396\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.093796730041504 | KNN Loss: 3.0865278244018555 | BCE Loss: 1.0072689056396484\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.095526695251465 | KNN Loss: 3.061067819595337 | BCE Loss: 1.034458875656128\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.125130653381348 | KNN Loss: 3.0794870853424072 | BCE Loss: 1.0456433296203613\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.136502265930176 | KNN Loss: 3.109160900115967 | BCE Loss: 1.027341604232788\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.0570526123046875 | KNN Loss: 3.052224636077881 | BCE Loss: 1.0048282146453857\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.062700271606445 | KNN Loss: 3.050645589828491 | BCE Loss: 1.0120548009872437\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.095649719238281 | KNN Loss: 3.0532798767089844 | BCE Loss: 1.0423696041107178\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.075447082519531 | KNN Loss: 3.07021164894104 | BCE Loss: 1.0052354335784912\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.088537216186523 | KNN Loss: 3.0582494735717773 | BCE Loss: 1.030287742614746\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.101407051086426 | KNN Loss: 3.0779881477355957 | BCE Loss: 1.023418664932251\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.1386003494262695 | KNN Loss: 3.1063854694366455 | BCE Loss: 1.032214879989624\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.040971279144287 | KNN Loss: 3.027325391769409 | BCE Loss: 1.0136457681655884\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.103099822998047 | KNN Loss: 3.079342842102051 | BCE Loss: 1.023756980895996\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.074416160583496 | KNN Loss: 3.0555408000946045 | BCE Loss: 1.0188755989074707\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.104207992553711 | KNN Loss: 3.0798661708831787 | BCE Loss: 1.0243418216705322\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.110160827636719 | KNN Loss: 3.0888469219207764 | BCE Loss: 1.0213139057159424\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.077888011932373 | KNN Loss: 3.0612740516662598 | BCE Loss: 1.0166139602661133\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.052988052368164 | KNN Loss: 3.0512471199035645 | BCE Loss: 1.0017410516738892\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.068911075592041 | KNN Loss: 3.043553590774536 | BCE Loss: 1.0253574848175049\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.0673065185546875 | KNN Loss: 3.037041664123535 | BCE Loss: 1.030264973640442\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.110345840454102 | KNN Loss: 3.0994224548339844 | BCE Loss: 1.010923147201538\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.095449447631836 | KNN Loss: 3.0823371410369873 | BCE Loss: 1.013112187385559\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.082311153411865 | KNN Loss: 3.0676651000976562 | BCE Loss: 1.014646053314209\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.09450626373291 | KNN Loss: 3.085930109024048 | BCE Loss: 1.0085761547088623\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.148555278778076 | KNN Loss: 3.07352876663208 | BCE Loss: 1.075026512145996\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.054447174072266 | KNN Loss: 3.0575034618377686 | BCE Loss: 0.9969439506530762\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.076835632324219 | KNN Loss: 3.099621295928955 | BCE Loss: 0.9772140979766846\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.068584442138672 | KNN Loss: 3.0699844360351562 | BCE Loss: 0.9985997676849365\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.096904754638672 | KNN Loss: 3.0705251693725586 | BCE Loss: 1.0263798236846924\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.038991928100586 | KNN Loss: 3.0567097663879395 | BCE Loss: 0.9822821617126465\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.061514854431152 | KNN Loss: 3.0520689487457275 | BCE Loss: 1.0094459056854248\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.10699462890625 | KNN Loss: 3.0851938724517822 | BCE Loss: 1.0218007564544678\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.12255334854126 | KNN Loss: 3.109623908996582 | BCE Loss: 1.0129294395446777\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.057788372039795 | KNN Loss: 3.039475440979004 | BCE Loss: 1.018312931060791\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.104794025421143 | KNN Loss: 3.0686886310577393 | BCE Loss: 1.0361052751541138\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.111114501953125 | KNN Loss: 3.0605461597442627 | BCE Loss: 1.0505685806274414\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.079436779022217 | KNN Loss: 3.0513875484466553 | BCE Loss: 1.028049349784851\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.03297758102417 | KNN Loss: 3.0352249145507812 | BCE Loss: 0.9977526664733887\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.084821701049805 | KNN Loss: 3.0719408988952637 | BCE Loss: 1.012880802154541\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.0951128005981445 | KNN Loss: 3.078768014907837 | BCE Loss: 1.0163449048995972\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.019710540771484 | KNN Loss: 3.053849220275879 | BCE Loss: 0.9658612012863159\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.039882659912109 | KNN Loss: 3.02117919921875 | BCE Loss: 1.0187036991119385\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.106931209564209 | KNN Loss: 3.081763744354248 | BCE Loss: 1.025167465209961\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.097427845001221 | KNN Loss: 3.074312925338745 | BCE Loss: 1.0231149196624756\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.047089099884033 | KNN Loss: 3.05031156539917 | BCE Loss: 0.9967774152755737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.0959672927856445 | KNN Loss: 3.0575151443481445 | BCE Loss: 1.0384521484375\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.0950775146484375 | KNN Loss: 3.080497980117798 | BCE Loss: 1.0145795345306396\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.116948127746582 | KNN Loss: 3.110846519470215 | BCE Loss: 1.0061014890670776\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.124973297119141 | KNN Loss: 3.1150460243225098 | BCE Loss: 1.0099270343780518\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.1122636795043945 | KNN Loss: 3.088801622390747 | BCE Loss: 1.0234622955322266\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.1079816818237305 | KNN Loss: 3.0836756229400635 | BCE Loss: 1.024305820465088\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.08434534072876 | KNN Loss: 3.064077377319336 | BCE Loss: 1.0202678442001343\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.054769515991211 | KNN Loss: 3.0494039058685303 | BCE Loss: 1.0053656101226807\n",
      "Epoch   216: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.105307579040527 | KNN Loss: 3.080852746963501 | BCE Loss: 1.0244548320770264\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.106836795806885 | KNN Loss: 3.094806432723999 | BCE Loss: 1.0120304822921753\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.078766822814941 | KNN Loss: 3.0813181400299072 | BCE Loss: 0.9974488615989685\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.140710830688477 | KNN Loss: 3.1330068111419678 | BCE Loss: 1.0077039003372192\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.113020896911621 | KNN Loss: 3.1024723052978516 | BCE Loss: 1.0105488300323486\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.061238765716553 | KNN Loss: 3.054556369781494 | BCE Loss: 1.006682276725769\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.121109485626221 | KNN Loss: 3.0949015617370605 | BCE Loss: 1.0262080430984497\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.075584411621094 | KNN Loss: 3.0533409118652344 | BCE Loss: 1.0222437381744385\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.115729331970215 | KNN Loss: 3.058150053024292 | BCE Loss: 1.057579517364502\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.095641136169434 | KNN Loss: 3.0876731872558594 | BCE Loss: 1.0079680681228638\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.090144157409668 | KNN Loss: 3.064160108566284 | BCE Loss: 1.0259840488433838\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.077268123626709 | KNN Loss: 3.0747079849243164 | BCE Loss: 1.0025601387023926\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.050370693206787 | KNN Loss: 3.057882070541382 | BCE Loss: 0.9924888014793396\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.10098934173584 | KNN Loss: 3.0996339321136475 | BCE Loss: 1.0013554096221924\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.104458808898926 | KNN Loss: 3.0870418548583984 | BCE Loss: 1.0174171924591064\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.13030481338501 | KNN Loss: 3.1075918674468994 | BCE Loss: 1.0227129459381104\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.076565265655518 | KNN Loss: 3.0521178245544434 | BCE Loss: 1.0244475603103638\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.042455196380615 | KNN Loss: 3.055114984512329 | BCE Loss: 0.9873403906822205\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.086015224456787 | KNN Loss: 3.049683094024658 | BCE Loss: 1.0363320112228394\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.039918899536133 | KNN Loss: 3.052011489868164 | BCE Loss: 0.9879072904586792\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.020931243896484 | KNN Loss: 3.0236976146698 | BCE Loss: 0.9972335696220398\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.0729851722717285 | KNN Loss: 3.0440046787261963 | BCE Loss: 1.0289803743362427\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.029807090759277 | KNN Loss: 3.0371923446655273 | BCE Loss: 0.9926146268844604\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.09177303314209 | KNN Loss: 3.0586166381835938 | BCE Loss: 1.033156156539917\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.129268646240234 | KNN Loss: 3.08772611618042 | BCE Loss: 1.0415427684783936\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.078707695007324 | KNN Loss: 3.057441234588623 | BCE Loss: 1.021266222000122\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.0808281898498535 | KNN Loss: 3.068819284439087 | BCE Loss: 1.0120089054107666\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.07089376449585 | KNN Loss: 3.0395500659942627 | BCE Loss: 1.031343698501587\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.089799404144287 | KNN Loss: 3.0623064041137695 | BCE Loss: 1.0274931192398071\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.106102466583252 | KNN Loss: 3.0514867305755615 | BCE Loss: 1.0546156167984009\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.079615116119385 | KNN Loss: 3.0646848678588867 | BCE Loss: 1.0149301290512085\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.088750839233398 | KNN Loss: 3.050842761993408 | BCE Loss: 1.0379083156585693\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.080165863037109 | KNN Loss: 3.0609006881713867 | BCE Loss: 1.0192654132843018\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.16267728805542 | KNN Loss: 3.1089141368865967 | BCE Loss: 1.0537632703781128\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.095650672912598 | KNN Loss: 3.083969831466675 | BCE Loss: 1.0116808414459229\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.12531042098999 | KNN Loss: 3.1056387424468994 | BCE Loss: 1.0196716785430908\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.105215072631836 | KNN Loss: 3.0860049724578857 | BCE Loss: 1.019209861755371\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.078849792480469 | KNN Loss: 3.058929920196533 | BCE Loss: 1.019919753074646\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.109244346618652 | KNN Loss: 3.0949063301086426 | BCE Loss: 1.0143377780914307\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.121005058288574 | KNN Loss: 3.104414463043213 | BCE Loss: 1.0165903568267822\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.074466705322266 | KNN Loss: 3.090252637863159 | BCE Loss: 0.9842138886451721\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.094627857208252 | KNN Loss: 3.0909132957458496 | BCE Loss: 1.0037144422531128\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.07249641418457 | KNN Loss: 3.0461533069610596 | BCE Loss: 1.0263432264328003\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.073533535003662 | KNN Loss: 3.0405004024505615 | BCE Loss: 1.0330332517623901\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.085797309875488 | KNN Loss: 3.056135654449463 | BCE Loss: 1.0296616554260254\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.131344795227051 | KNN Loss: 3.1037557125091553 | BCE Loss: 1.0275888442993164\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.094263076782227 | KNN Loss: 3.0916500091552734 | BCE Loss: 1.0026131868362427\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.076669216156006 | KNN Loss: 3.065784454345703 | BCE Loss: 1.0108847618103027\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.03245735168457 | KNN Loss: 3.02939510345459 | BCE Loss: 1.003062129020691\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.104059219360352 | KNN Loss: 3.0571155548095703 | BCE Loss: 1.0469437837600708\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.059433937072754 | KNN Loss: 3.0676109790802 | BCE Loss: 0.9918229579925537\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.070285320281982 | KNN Loss: 3.0748753547668457 | BCE Loss: 0.9954098463058472\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.070868492126465 | KNN Loss: 3.0513417720794678 | BCE Loss: 1.0195269584655762\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.088807106018066 | KNN Loss: 3.0382542610168457 | BCE Loss: 1.0505526065826416\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.122923851013184 | KNN Loss: 3.093653917312622 | BCE Loss: 1.0292696952819824\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.081073760986328 | KNN Loss: 3.0468688011169434 | BCE Loss: 1.0342051982879639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.079341411590576 | KNN Loss: 3.073218822479248 | BCE Loss: 1.0061225891113281\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.080904006958008 | KNN Loss: 3.080845594406128 | BCE Loss: 1.0000581741333008\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.0407538414001465 | KNN Loss: 3.0450000762939453 | BCE Loss: 0.9957536458969116\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.089763164520264 | KNN Loss: 3.0705320835113525 | BCE Loss: 1.0192310810089111\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.060108184814453 | KNN Loss: 3.058717966079712 | BCE Loss: 1.0013902187347412\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.033927917480469 | KNN Loss: 3.0242152214050293 | BCE Loss: 1.0097126960754395\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.05644416809082 | KNN Loss: 3.0364699363708496 | BCE Loss: 1.0199744701385498\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.069771766662598 | KNN Loss: 3.068101167678833 | BCE Loss: 1.0016705989837646\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.049211502075195 | KNN Loss: 3.0478808879852295 | BCE Loss: 1.0013303756713867\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.10850191116333 | KNN Loss: 3.091449737548828 | BCE Loss: 1.017052173614502\n",
      "Epoch   227: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.043623924255371 | KNN Loss: 3.038170337677002 | BCE Loss: 1.0054534673690796\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.106261253356934 | KNN Loss: 3.0861923694610596 | BCE Loss: 1.0200690031051636\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.114163398742676 | KNN Loss: 3.0775983333587646 | BCE Loss: 1.0365650653839111\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.041813850402832 | KNN Loss: 3.034749746322632 | BCE Loss: 1.0070641040802002\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.035434722900391 | KNN Loss: 3.0449414253234863 | BCE Loss: 0.9904934167861938\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.058946132659912 | KNN Loss: 3.055466413497925 | BCE Loss: 1.0034797191619873\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.036693572998047 | KNN Loss: 3.0343875885009766 | BCE Loss: 1.0023058652877808\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.090935230255127 | KNN Loss: 3.0838847160339355 | BCE Loss: 1.007050633430481\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.14293909072876 | KNN Loss: 3.1136062145233154 | BCE Loss: 1.0293328762054443\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.106525897979736 | KNN Loss: 3.081007719039917 | BCE Loss: 1.0255181789398193\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.111752510070801 | KNN Loss: 3.0904173851013184 | BCE Loss: 1.0213353633880615\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.092343807220459 | KNN Loss: 3.073256254196167 | BCE Loss: 1.0190874338150024\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.089779853820801 | KNN Loss: 3.055386543273926 | BCE Loss: 1.034393310546875\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.0750041007995605 | KNN Loss: 3.0703423023223877 | BCE Loss: 1.0046616792678833\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.055402755737305 | KNN Loss: 3.069077491760254 | BCE Loss: 0.9863253235816956\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.076910018920898 | KNN Loss: 3.057541847229004 | BCE Loss: 1.019368290901184\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.010165214538574 | KNN Loss: 3.0153326988220215 | BCE Loss: 0.9948326349258423\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.089071273803711 | KNN Loss: 3.0542421340942383 | BCE Loss: 1.0348293781280518\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.072028160095215 | KNN Loss: 3.056684732437134 | BCE Loss: 1.0153433084487915\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.0784993171691895 | KNN Loss: 3.054525375366211 | BCE Loss: 1.023973822593689\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.017056465148926 | KNN Loss: 3.0143234729766846 | BCE Loss: 1.002732753753662\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.062111854553223 | KNN Loss: 3.063490152359009 | BCE Loss: 0.998621940612793\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.098535060882568 | KNN Loss: 3.0585362911224365 | BCE Loss: 1.0399987697601318\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.088186264038086 | KNN Loss: 3.0746068954467773 | BCE Loss: 1.013579249382019\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.022157669067383 | KNN Loss: 3.0226035118103027 | BCE Loss: 0.9995542764663696\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.067914009094238 | KNN Loss: 3.0650174617767334 | BCE Loss: 1.002896785736084\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.119686603546143 | KNN Loss: 3.08385968208313 | BCE Loss: 1.0358269214630127\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.102776527404785 | KNN Loss: 3.0759518146514893 | BCE Loss: 1.0268244743347168\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.1411309242248535 | KNN Loss: 3.122239828109741 | BCE Loss: 1.0188912153244019\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.091590881347656 | KNN Loss: 3.076500654220581 | BCE Loss: 1.0150902271270752\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.0762481689453125 | KNN Loss: 3.06373929977417 | BCE Loss: 1.0125086307525635\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.071645259857178 | KNN Loss: 3.0586578845977783 | BCE Loss: 1.0129872560501099\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.070774078369141 | KNN Loss: 3.080389976501465 | BCE Loss: 0.9903842210769653\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.052475929260254 | KNN Loss: 3.034344434738159 | BCE Loss: 1.0181312561035156\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.097189903259277 | KNN Loss: 3.0861504077911377 | BCE Loss: 1.0110397338867188\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.083629131317139 | KNN Loss: 3.068174362182617 | BCE Loss: 1.015454649925232\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.125965118408203 | KNN Loss: 3.0951266288757324 | BCE Loss: 1.0308386087417603\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.0964860916137695 | KNN Loss: 3.0801215171813965 | BCE Loss: 1.0163646936416626\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.130882740020752 | KNN Loss: 3.074951171875 | BCE Loss: 1.055931568145752\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.103276252746582 | KNN Loss: 3.0795376300811768 | BCE Loss: 1.0237383842468262\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.042843341827393 | KNN Loss: 3.03539776802063 | BCE Loss: 1.0074454545974731\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.0973358154296875 | KNN Loss: 3.071793794631958 | BCE Loss: 1.0255417823791504\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.1000800132751465 | KNN Loss: 3.0787949562072754 | BCE Loss: 1.0212849378585815\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.134647846221924 | KNN Loss: 3.1080877780914307 | BCE Loss: 1.0265600681304932\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.051621437072754 | KNN Loss: 3.022003412246704 | BCE Loss: 1.0296179056167603\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.074189186096191 | KNN Loss: 3.0442535877227783 | BCE Loss: 1.029935598373413\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.051397800445557 | KNN Loss: 3.067115545272827 | BCE Loss: 0.984282374382019\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.108102321624756 | KNN Loss: 3.0625803470611572 | BCE Loss: 1.0455219745635986\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.157458305358887 | KNN Loss: 3.102977991104126 | BCE Loss: 1.0544805526733398\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.060815811157227 | KNN Loss: 3.0530307292938232 | BCE Loss: 1.0077850818634033\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.10190486907959 | KNN Loss: 3.069361925125122 | BCE Loss: 1.0325427055358887\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.092702865600586 | KNN Loss: 3.07968807220459 | BCE Loss: 1.0130150318145752\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.078649520874023 | KNN Loss: 3.0516951084136963 | BCE Loss: 1.0269544124603271\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.131404876708984 | KNN Loss: 3.0720527172088623 | BCE Loss: 1.059352159500122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.123483657836914 | KNN Loss: 3.0767765045166016 | BCE Loss: 1.0467071533203125\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.099625587463379 | KNN Loss: 3.077615261077881 | BCE Loss: 1.0220104455947876\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.054172515869141 | KNN Loss: 3.0681746006011963 | BCE Loss: 0.9859981536865234\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.105992794036865 | KNN Loss: 3.090855598449707 | BCE Loss: 1.0151371955871582\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.063653945922852 | KNN Loss: 3.048473596572876 | BCE Loss: 1.0151804685592651\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.1444244384765625 | KNN Loss: 3.1009156703948975 | BCE Loss: 1.0435090065002441\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.147364616394043 | KNN Loss: 3.0983364582061768 | BCE Loss: 1.0490283966064453\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.0580525398254395 | KNN Loss: 3.064051389694214 | BCE Loss: 0.994001030921936\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.062767505645752 | KNN Loss: 3.0448155403137207 | BCE Loss: 1.0179519653320312\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.141945838928223 | KNN Loss: 3.117342948913574 | BCE Loss: 1.0246028900146484\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.077663898468018 | KNN Loss: 3.0541462898254395 | BCE Loss: 1.0235177278518677\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.053114414215088 | KNN Loss: 3.0286011695861816 | BCE Loss: 1.0245132446289062\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.093267440795898 | KNN Loss: 3.064702272415161 | BCE Loss: 1.0285654067993164\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.059968948364258 | KNN Loss: 3.0631608963012695 | BCE Loss: 0.9968078136444092\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.107998847961426 | KNN Loss: 3.0687339305877686 | BCE Loss: 1.0392646789550781\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.084572792053223 | KNN Loss: 3.0804443359375 | BCE Loss: 1.0041282176971436\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.078858375549316 | KNN Loss: 3.049582004547119 | BCE Loss: 1.0292766094207764\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.118075370788574 | KNN Loss: 3.0802125930786133 | BCE Loss: 1.03786301612854\n",
      "Epoch   239: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.1030168533325195 | KNN Loss: 3.0787110328674316 | BCE Loss: 1.0243055820465088\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.071511268615723 | KNN Loss: 3.0723671913146973 | BCE Loss: 0.9991440176963806\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.065710067749023 | KNN Loss: 3.0714330673217773 | BCE Loss: 0.9942769408226013\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.127533912658691 | KNN Loss: 3.0799007415771484 | BCE Loss: 1.0476330518722534\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.051877021789551 | KNN Loss: 3.0512166023254395 | BCE Loss: 1.0006606578826904\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.115382671356201 | KNN Loss: 3.079012393951416 | BCE Loss: 1.0363703966140747\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.064896583557129 | KNN Loss: 3.0522749423980713 | BCE Loss: 1.0126216411590576\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.043026924133301 | KNN Loss: 3.044602394104004 | BCE Loss: 0.9984244704246521\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.04822301864624 | KNN Loss: 3.044438600540161 | BCE Loss: 1.0037845373153687\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.075918197631836 | KNN Loss: 3.0702872276306152 | BCE Loss: 1.0056312084197998\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.070429801940918 | KNN Loss: 3.0481765270233154 | BCE Loss: 1.0222532749176025\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.126808166503906 | KNN Loss: 3.070453643798828 | BCE Loss: 1.056354284286499\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.087098121643066 | KNN Loss: 3.0834500789642334 | BCE Loss: 1.003648042678833\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.08980655670166 | KNN Loss: 3.070021152496338 | BCE Loss: 1.0197856426239014\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.052122592926025 | KNN Loss: 3.0539164543151855 | BCE Loss: 0.9982062578201294\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.101240634918213 | KNN Loss: 3.0922343730926514 | BCE Loss: 1.0090062618255615\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.138337135314941 | KNN Loss: 3.0666308403015137 | BCE Loss: 1.0717062950134277\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.140268325805664 | KNN Loss: 3.099740743637085 | BCE Loss: 1.0405277013778687\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.065343856811523 | KNN Loss: 3.058892250061035 | BCE Loss: 1.0064513683319092\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.121232986450195 | KNN Loss: 3.0789902210235596 | BCE Loss: 1.0422426462173462\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.095390796661377 | KNN Loss: 3.060202121734619 | BCE Loss: 1.0351885557174683\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.117950439453125 | KNN Loss: 3.074814796447754 | BCE Loss: 1.0431358814239502\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.050439834594727 | KNN Loss: 3.0431125164031982 | BCE Loss: 1.0073270797729492\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.085214138031006 | KNN Loss: 3.0628814697265625 | BCE Loss: 1.0223326683044434\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.092092037200928 | KNN Loss: 3.10079026222229 | BCE Loss: 0.9913016557693481\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.119431495666504 | KNN Loss: 3.086134672164917 | BCE Loss: 1.0332965850830078\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.113537311553955 | KNN Loss: 3.0932021141052246 | BCE Loss: 1.0203351974487305\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.099176406860352 | KNN Loss: 3.0553791522979736 | BCE Loss: 1.0437973737716675\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.165475845336914 | KNN Loss: 3.1261932849884033 | BCE Loss: 1.0392827987670898\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.091030597686768 | KNN Loss: 3.087505340576172 | BCE Loss: 1.0035252571105957\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.081180572509766 | KNN Loss: 3.0572824478149414 | BCE Loss: 1.0238978862762451\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.058933258056641 | KNN Loss: 3.043041944503784 | BCE Loss: 1.015891432762146\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.071538925170898 | KNN Loss: 3.065732479095459 | BCE Loss: 1.00580632686615\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.080375671386719 | KNN Loss: 3.063971519470215 | BCE Loss: 1.0164042711257935\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.07759952545166 | KNN Loss: 3.041118621826172 | BCE Loss: 1.0364806652069092\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.075285911560059 | KNN Loss: 3.058758020401001 | BCE Loss: 1.0165276527404785\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.083486557006836 | KNN Loss: 3.0837550163269043 | BCE Loss: 0.9997314214706421\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.084197998046875 | KNN Loss: 3.034552812576294 | BCE Loss: 1.049644947052002\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.05455207824707 | KNN Loss: 3.033313512802124 | BCE Loss: 1.0212383270263672\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.086745262145996 | KNN Loss: 3.0695364475250244 | BCE Loss: 1.0172085762023926\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.0984721183776855 | KNN Loss: 3.075526714324951 | BCE Loss: 1.0229454040527344\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.096776962280273 | KNN Loss: 3.0718369483947754 | BCE Loss: 1.024939775466919\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.075431823730469 | KNN Loss: 3.058690071105957 | BCE Loss: 1.0167419910430908\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.068369388580322 | KNN Loss: 3.0644335746765137 | BCE Loss: 1.0039359331130981\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.057580947875977 | KNN Loss: 3.0429141521453857 | BCE Loss: 1.0146667957305908\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.066112518310547 | KNN Loss: 3.046844005584717 | BCE Loss: 1.0192683935165405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.095270156860352 | KNN Loss: 3.0891337394714355 | BCE Loss: 1.006136417388916\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.084768772125244 | KNN Loss: 3.071667432785034 | BCE Loss: 1.0131014585494995\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.1899213790893555 | KNN Loss: 3.1274044513702393 | BCE Loss: 1.062516689300537\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.106707572937012 | KNN Loss: 3.070157527923584 | BCE Loss: 1.0365499258041382\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.08260440826416 | KNN Loss: 3.062119722366333 | BCE Loss: 1.0204845666885376\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.152750492095947 | KNN Loss: 3.099821090698242 | BCE Loss: 1.0529295206069946\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.082065105438232 | KNN Loss: 3.078385829925537 | BCE Loss: 1.0036792755126953\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.065840721130371 | KNN Loss: 3.0683465003967285 | BCE Loss: 0.9974943995475769\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.0916852951049805 | KNN Loss: 3.0840165615081787 | BCE Loss: 1.0076689720153809\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.0446014404296875 | KNN Loss: 3.027634382247925 | BCE Loss: 1.0169672966003418\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.065647602081299 | KNN Loss: 3.0674140453338623 | BCE Loss: 0.9982336759567261\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.089150428771973 | KNN Loss: 3.085926055908203 | BCE Loss: 1.003224492073059\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.071218490600586 | KNN Loss: 3.049431324005127 | BCE Loss: 1.021787166595459\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.115805625915527 | KNN Loss: 3.073232889175415 | BCE Loss: 1.0425727367401123\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.08204460144043 | KNN Loss: 3.036073684692383 | BCE Loss: 1.0459707975387573\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.084318161010742 | KNN Loss: 3.0658352375030518 | BCE Loss: 1.0184826850891113\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.076032638549805 | KNN Loss: 3.0526745319366455 | BCE Loss: 1.0233581066131592\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.066649913787842 | KNN Loss: 3.066035747528076 | BCE Loss: 1.0006141662597656\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.053898811340332 | KNN Loss: 3.056788444519043 | BCE Loss: 0.9971103668212891\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.085623264312744 | KNN Loss: 3.0665667057037354 | BCE Loss: 1.0190565586090088\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.065360069274902 | KNN Loss: 3.069711685180664 | BCE Loss: 0.9956482648849487\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.096949577331543 | KNN Loss: 3.0662875175476074 | BCE Loss: 1.0306622982025146\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.083530902862549 | KNN Loss: 3.0715584754943848 | BCE Loss: 1.011972427368164\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.051676273345947 | KNN Loss: 3.05810546875 | BCE Loss: 0.9935706853866577\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.050833225250244 | KNN Loss: 3.042741537094116 | BCE Loss: 1.008091688156128\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.117761611938477 | KNN Loss: 3.0728676319122314 | BCE Loss: 1.044893741607666\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.087479114532471 | KNN Loss: 3.064113140106201 | BCE Loss: 1.02336585521698\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.11830472946167 | KNN Loss: 3.0894978046417236 | BCE Loss: 1.0288068056106567\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.146787643432617 | KNN Loss: 3.11344575881958 | BCE Loss: 1.033341646194458\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.054798126220703 | KNN Loss: 3.0448153018951416 | BCE Loss: 1.009982705116272\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.083458423614502 | KNN Loss: 3.0883264541625977 | BCE Loss: 0.9951320886611938\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.083200454711914 | KNN Loss: 3.0600650310516357 | BCE Loss: 1.0231353044509888\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.065755844116211 | KNN Loss: 3.079145908355713 | BCE Loss: 0.986609697341919\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.079182147979736 | KNN Loss: 3.0651285648345947 | BCE Loss: 1.0140535831451416\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.104389190673828 | KNN Loss: 3.0760748386383057 | BCE Loss: 1.0283141136169434\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.050043106079102 | KNN Loss: 3.0414316654205322 | BCE Loss: 1.0086114406585693\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.1127729415893555 | KNN Loss: 3.0743367671966553 | BCE Loss: 1.0384362936019897\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.1173882484436035 | KNN Loss: 3.098505735397339 | BCE Loss: 1.018882393836975\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.095731258392334 | KNN Loss: 3.053825855255127 | BCE Loss: 1.0419055223464966\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.109505653381348 | KNN Loss: 3.0909411907196045 | BCE Loss: 1.018564224243164\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.072967529296875 | KNN Loss: 3.0581893920898438 | BCE Loss: 1.0147778987884521\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.087696075439453 | KNN Loss: 3.0529062747955322 | BCE Loss: 1.0347900390625\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.126059532165527 | KNN Loss: 3.073833465576172 | BCE Loss: 1.0522260665893555\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.084403038024902 | KNN Loss: 3.0730535984039307 | BCE Loss: 1.0113494396209717\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.130606651306152 | KNN Loss: 3.0812621116638184 | BCE Loss: 1.049344778060913\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.117915630340576 | KNN Loss: 3.075031042098999 | BCE Loss: 1.0428847074508667\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.08714485168457 | KNN Loss: 3.0692169666290283 | BCE Loss: 1.017928123474121\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.081311225891113 | KNN Loss: 3.066924810409546 | BCE Loss: 1.0143864154815674\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.089888095855713 | KNN Loss: 3.061772584915161 | BCE Loss: 1.0281156301498413\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.109966278076172 | KNN Loss: 3.0709543228149414 | BCE Loss: 1.0390117168426514\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.1090569496154785 | KNN Loss: 3.080442428588867 | BCE Loss: 1.0286145210266113\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.1260294914245605 | KNN Loss: 3.0744125843048096 | BCE Loss: 1.051616907119751\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.0984907150268555 | KNN Loss: 3.075871229171753 | BCE Loss: 1.022619605064392\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.030218124389648 | KNN Loss: 3.0518834590911865 | BCE Loss: 0.9783345460891724\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.085103511810303 | KNN Loss: 3.0850179195404053 | BCE Loss: 1.0000855922698975\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.056693077087402 | KNN Loss: 3.0279083251953125 | BCE Loss: 1.0287845134735107\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.085488319396973 | KNN Loss: 3.0689902305603027 | BCE Loss: 1.0164982080459595\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.0977888107299805 | KNN Loss: 3.0672547817230225 | BCE Loss: 1.030534029006958\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.117343425750732 | KNN Loss: 3.100832939147949 | BCE Loss: 1.0165104866027832\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.034633159637451 | KNN Loss: 3.0583062171936035 | BCE Loss: 0.9763270616531372\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.122306823730469 | KNN Loss: 3.096306562423706 | BCE Loss: 1.0260001420974731\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.059369087219238 | KNN Loss: 3.055391311645508 | BCE Loss: 1.0039775371551514\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.061391353607178 | KNN Loss: 3.0449719429016113 | BCE Loss: 1.0164192914962769\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.099786758422852 | KNN Loss: 3.072101593017578 | BCE Loss: 1.0276850461959839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.070108413696289 | KNN Loss: 3.037182331085205 | BCE Loss: 1.0329259634017944\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.140316486358643 | KNN Loss: 3.114244222640991 | BCE Loss: 1.0260722637176514\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.135322570800781 | KNN Loss: 3.096376657485962 | BCE Loss: 1.0389459133148193\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.143105506896973 | KNN Loss: 3.0671355724334717 | BCE Loss: 1.07597017288208\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.090673446655273 | KNN Loss: 3.0576834678649902 | BCE Loss: 1.0329902172088623\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.109383583068848 | KNN Loss: 3.0917980670928955 | BCE Loss: 1.0175857543945312\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.052361011505127 | KNN Loss: 3.0545129776000977 | BCE Loss: 0.9978480339050293\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.101809024810791 | KNN Loss: 3.0766265392303467 | BCE Loss: 1.0251824855804443\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.097947120666504 | KNN Loss: 3.0601584911346436 | BCE Loss: 1.0377888679504395\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.07694149017334 | KNN Loss: 3.076695203781128 | BCE Loss: 1.000246524810791\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.039149761199951 | KNN Loss: 3.0243303775787354 | BCE Loss: 1.0148193836212158\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.054998397827148 | KNN Loss: 3.051403045654297 | BCE Loss: 1.0035955905914307\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.0906548500061035 | KNN Loss: 3.0487594604492188 | BCE Loss: 1.0418953895568848\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.128316879272461 | KNN Loss: 3.0771210193634033 | BCE Loss: 1.0511958599090576\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.057329177856445 | KNN Loss: 3.043850898742676 | BCE Loss: 1.0134782791137695\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.036995887756348 | KNN Loss: 3.0340616703033447 | BCE Loss: 1.0029340982437134\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.097744464874268 | KNN Loss: 3.068030834197998 | BCE Loss: 1.0297136306762695\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.059874057769775 | KNN Loss: 3.0688769817352295 | BCE Loss: 0.9909969568252563\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.099273681640625 | KNN Loss: 3.057441473007202 | BCE Loss: 1.041832447052002\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.078920364379883 | KNN Loss: 3.0459799766540527 | BCE Loss: 1.03294038772583\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.052299976348877 | KNN Loss: 3.05334734916687 | BCE Loss: 0.9989525675773621\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.068351745605469 | KNN Loss: 3.079010248184204 | BCE Loss: 0.9893413782119751\n",
      "Epoch   261: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.077639579772949 | KNN Loss: 3.038356304168701 | BCE Loss: 1.039283037185669\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.12484073638916 | KNN Loss: 3.102166175842285 | BCE Loss: 1.022674560546875\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.09182596206665 | KNN Loss: 3.0837295055389404 | BCE Loss: 1.0080963373184204\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.0445709228515625 | KNN Loss: 3.035978317260742 | BCE Loss: 1.0085927248001099\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.072731971740723 | KNN Loss: 3.0631723403930664 | BCE Loss: 1.0095598697662354\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.061276912689209 | KNN Loss: 3.0405826568603516 | BCE Loss: 1.020694375038147\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.040268898010254 | KNN Loss: 3.046752452850342 | BCE Loss: 0.9935164451599121\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.064079284667969 | KNN Loss: 3.0350358486175537 | BCE Loss: 1.029043436050415\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.101441860198975 | KNN Loss: 3.091306447982788 | BCE Loss: 1.010135531425476\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.080183982849121 | KNN Loss: 3.0446367263793945 | BCE Loss: 1.0355474948883057\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.040574073791504 | KNN Loss: 3.05035138130188 | BCE Loss: 0.9902229309082031\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.055734634399414 | KNN Loss: 3.047579288482666 | BCE Loss: 1.0081554651260376\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.0827131271362305 | KNN Loss: 3.0790021419525146 | BCE Loss: 1.0037111043930054\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.072824478149414 | KNN Loss: 3.0600898265838623 | BCE Loss: 1.0127348899841309\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.126506805419922 | KNN Loss: 3.0758979320526123 | BCE Loss: 1.0506086349487305\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.037229061126709 | KNN Loss: 3.034294366836548 | BCE Loss: 1.0029346942901611\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.035672664642334 | KNN Loss: 3.04731822013855 | BCE Loss: 0.9883542656898499\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.089466571807861 | KNN Loss: 3.0688154697418213 | BCE Loss: 1.0206512212753296\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.058733940124512 | KNN Loss: 3.0453431606292725 | BCE Loss: 1.0133905410766602\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.0796380043029785 | KNN Loss: 3.0514533519744873 | BCE Loss: 1.0281847715377808\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.061004638671875 | KNN Loss: 3.0332348346710205 | BCE Loss: 1.0277695655822754\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.048151016235352 | KNN Loss: 3.0360090732574463 | BCE Loss: 1.0121420621871948\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.099184989929199 | KNN Loss: 3.0690383911132812 | BCE Loss: 1.0301463603973389\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.101585388183594 | KNN Loss: 3.0673725605010986 | BCE Loss: 1.0342128276824951\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.096931457519531 | KNN Loss: 3.0662295818328857 | BCE Loss: 1.0307021141052246\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.109616279602051 | KNN Loss: 3.0399155616760254 | BCE Loss: 1.0697007179260254\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.0809526443481445 | KNN Loss: 3.0619306564331055 | BCE Loss: 1.019021987915039\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.10851526260376 | KNN Loss: 3.099942207336426 | BCE Loss: 1.008573055267334\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.095654010772705 | KNN Loss: 3.0850703716278076 | BCE Loss: 1.010583519935608\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.068352699279785 | KNN Loss: 3.070704936981201 | BCE Loss: 0.9976478219032288\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.06878662109375 | KNN Loss: 3.0471975803375244 | BCE Loss: 1.0215891599655151\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.088984489440918 | KNN Loss: 3.0906589031219482 | BCE Loss: 0.998325765132904\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.105234622955322 | KNN Loss: 3.081415891647339 | BCE Loss: 1.0238187313079834\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.066382884979248 | KNN Loss: 3.0358808040618896 | BCE Loss: 1.0305020809173584\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.1363091468811035 | KNN Loss: 3.0858497619628906 | BCE Loss: 1.0504595041275024\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.071953773498535 | KNN Loss: 3.0444934368133545 | BCE Loss: 1.0274604558944702\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.0916643142700195 | KNN Loss: 3.078138589859009 | BCE Loss: 1.0135258436203003\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.056639671325684 | KNN Loss: 3.033998727798462 | BCE Loss: 1.0226411819458008\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.035401344299316 | KNN Loss: 3.043654203414917 | BCE Loss: 0.9917471408843994\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.068296909332275 | KNN Loss: 3.062673330307007 | BCE Loss: 1.0056235790252686\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.11141300201416 | KNN Loss: 3.06756591796875 | BCE Loss: 1.0438472032546997\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.125275611877441 | KNN Loss: 3.0836329460144043 | BCE Loss: 1.041642665863037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.100186347961426 | KNN Loss: 3.05822491645813 | BCE Loss: 1.0419611930847168\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.088730812072754 | KNN Loss: 3.0611038208007812 | BCE Loss: 1.0276272296905518\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.037891864776611 | KNN Loss: 3.029721736907959 | BCE Loss: 1.0081701278686523\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.092559337615967 | KNN Loss: 3.08541202545166 | BCE Loss: 1.0071474313735962\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.059804916381836 | KNN Loss: 3.0516316890716553 | BCE Loss: 1.0081733465194702\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.051492691040039 | KNN Loss: 3.0716278553009033 | BCE Loss: 0.9798646569252014\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.081490516662598 | KNN Loss: 3.0276541709899902 | BCE Loss: 1.0538363456726074\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.041841506958008 | KNN Loss: 3.030160665512085 | BCE Loss: 1.0116806030273438\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.043788909912109 | KNN Loss: 3.0371205806732178 | BCE Loss: 1.0066683292388916\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.102051734924316 | KNN Loss: 3.081697940826416 | BCE Loss: 1.02035391330719\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.053033828735352 | KNN Loss: 3.0488486289978027 | BCE Loss: 1.0041851997375488\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.1143012046813965 | KNN Loss: 3.1026697158813477 | BCE Loss: 1.0116314888000488\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.029335975646973 | KNN Loss: 3.0291621685028076 | BCE Loss: 1.0001740455627441\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.050454139709473 | KNN Loss: 3.051192045211792 | BCE Loss: 0.9992622137069702\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.092020034790039 | KNN Loss: 3.0614514350891113 | BCE Loss: 1.0305683612823486\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.090059280395508 | KNN Loss: 3.0749006271362305 | BCE Loss: 1.0151586532592773\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.037717819213867 | KNN Loss: 3.026777982711792 | BCE Loss: 1.0109400749206543\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.074527263641357 | KNN Loss: 3.0932395458221436 | BCE Loss: 0.9812878966331482\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.045589923858643 | KNN Loss: 3.067228317260742 | BCE Loss: 0.9783615469932556\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.076993942260742 | KNN Loss: 3.0657849311828613 | BCE Loss: 1.01120924949646\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.066840648651123 | KNN Loss: 3.0512938499450684 | BCE Loss: 1.0155467987060547\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.080158233642578 | KNN Loss: 3.0401692390441895 | BCE Loss: 1.0399888753890991\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.077667236328125 | KNN Loss: 3.0638766288757324 | BCE Loss: 1.0137908458709717\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.105559349060059 | KNN Loss: 3.0717010498046875 | BCE Loss: 1.033858060836792\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.120582580566406 | KNN Loss: 3.0787549018859863 | BCE Loss: 1.04182767868042\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.086231231689453 | KNN Loss: 3.0631861686706543 | BCE Loss: 1.0230451822280884\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.097329139709473 | KNN Loss: 3.0502216815948486 | BCE Loss: 1.047107219696045\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.136021614074707 | KNN Loss: 3.0824034214019775 | BCE Loss: 1.053618311882019\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.114011764526367 | KNN Loss: 3.0898590087890625 | BCE Loss: 1.0241527557373047\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.134949684143066 | KNN Loss: 3.0726304054260254 | BCE Loss: 1.062319278717041\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.081060409545898 | KNN Loss: 3.0417191982269287 | BCE Loss: 1.0393412113189697\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.087814807891846 | KNN Loss: 3.0753016471862793 | BCE Loss: 1.0125131607055664\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.059502601623535 | KNN Loss: 3.050135850906372 | BCE Loss: 1.0093669891357422\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.103604793548584 | KNN Loss: 3.0807061195373535 | BCE Loss: 1.0228986740112305\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.044067859649658 | KNN Loss: 3.0602834224700928 | BCE Loss: 0.983784556388855\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.0592193603515625 | KNN Loss: 3.0495874881744385 | BCE Loss: 1.0096321105957031\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.056962013244629 | KNN Loss: 3.0232391357421875 | BCE Loss: 1.0337228775024414\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.105062484741211 | KNN Loss: 3.080080032348633 | BCE Loss: 1.0249826908111572\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.109981536865234 | KNN Loss: 3.086312770843506 | BCE Loss: 1.0236685276031494\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.04479455947876 | KNN Loss: 3.06834077835083 | BCE Loss: 0.9764537811279297\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.043567180633545 | KNN Loss: 3.0446321964263916 | BCE Loss: 0.998934805393219\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.128654479980469 | KNN Loss: 3.099024772644043 | BCE Loss: 1.0296297073364258\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.048610687255859 | KNN Loss: 3.0500051975250244 | BCE Loss: 0.9986057281494141\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.077118873596191 | KNN Loss: 3.0520360469818115 | BCE Loss: 1.0250828266143799\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.090837478637695 | KNN Loss: 3.0794432163238525 | BCE Loss: 1.0113940238952637\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.08801794052124 | KNN Loss: 3.067035675048828 | BCE Loss: 1.020982265472412\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.070698261260986 | KNN Loss: 3.0672237873077393 | BCE Loss: 1.003474473953247\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.089807510375977 | KNN Loss: 3.0491833686828613 | BCE Loss: 1.0406239032745361\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.025333404541016 | KNN Loss: 3.0373141765594482 | BCE Loss: 0.9880192279815674\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.084012031555176 | KNN Loss: 3.0380825996398926 | BCE Loss: 1.0459296703338623\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.0393500328063965 | KNN Loss: 3.0513994693756104 | BCE Loss: 0.9879505634307861\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.060324192047119 | KNN Loss: 3.0489869117736816 | BCE Loss: 1.0113372802734375\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.143658638000488 | KNN Loss: 3.1079177856445312 | BCE Loss: 1.035740613937378\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.052534103393555 | KNN Loss: 3.0511934757232666 | BCE Loss: 1.0013408660888672\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.029675483703613 | KNN Loss: 3.054583787918091 | BCE Loss: 0.9750916957855225\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.110340118408203 | KNN Loss: 3.0870401859283447 | BCE Loss: 1.0232999324798584\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.113345146179199 | KNN Loss: 3.0783019065856934 | BCE Loss: 1.0350430011749268\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.085846900939941 | KNN Loss: 3.0440902709960938 | BCE Loss: 1.0417563915252686\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.132952690124512 | KNN Loss: 3.097693681716919 | BCE Loss: 1.0352592468261719\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.081076622009277 | KNN Loss: 3.0738282203674316 | BCE Loss: 1.0072485208511353\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.146309852600098 | KNN Loss: 3.114306926727295 | BCE Loss: 1.0320031642913818\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.094837665557861 | KNN Loss: 3.0743801593780518 | BCE Loss: 1.0204575061798096\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.082507133483887 | KNN Loss: 3.0792698860168457 | BCE Loss: 1.003237247467041\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.081090450286865 | KNN Loss: 3.056272268295288 | BCE Loss: 1.0248181819915771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.07474422454834 | KNN Loss: 3.043532371520996 | BCE Loss: 1.0312120914459229\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.093918800354004 | KNN Loss: 3.05249285697937 | BCE Loss: 1.041426181793213\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.113008499145508 | KNN Loss: 3.0846848487854004 | BCE Loss: 1.0283238887786865\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.090731143951416 | KNN Loss: 3.051161527633667 | BCE Loss: 1.039569616317749\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.082348823547363 | KNN Loss: 3.055863857269287 | BCE Loss: 1.0264849662780762\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.068127632141113 | KNN Loss: 3.054783582687378 | BCE Loss: 1.0133440494537354\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.050754547119141 | KNN Loss: 3.0480031967163086 | BCE Loss: 1.0027514696121216\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.101797103881836 | KNN Loss: 3.0572683811187744 | BCE Loss: 1.0445287227630615\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.112560749053955 | KNN Loss: 3.057847499847412 | BCE Loss: 1.054713249206543\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.095093250274658 | KNN Loss: 3.0688107013702393 | BCE Loss: 1.026282548904419\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.070992469787598 | KNN Loss: 3.038309097290039 | BCE Loss: 1.0326833724975586\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.060155868530273 | KNN Loss: 3.059248447418213 | BCE Loss: 1.0009076595306396\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.090411186218262 | KNN Loss: 3.0648117065429688 | BCE Loss: 1.0255992412567139\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.06142520904541 | KNN Loss: 3.0458219051361084 | BCE Loss: 1.0156030654907227\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.087070465087891 | KNN Loss: 3.0785698890686035 | BCE Loss: 1.0085006952285767\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.095251083374023 | KNN Loss: 3.0717673301696777 | BCE Loss: 1.0234839916229248\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.085318565368652 | KNN Loss: 3.0712759494781494 | BCE Loss: 1.0140427350997925\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.072985649108887 | KNN Loss: 3.0694217681884766 | BCE Loss: 1.0035641193389893\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.086245059967041 | KNN Loss: 3.054624319076538 | BCE Loss: 1.031620740890503\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.062721252441406 | KNN Loss: 3.0462021827697754 | BCE Loss: 1.0165190696716309\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.107011795043945 | KNN Loss: 3.0742928981781006 | BCE Loss: 1.0327186584472656\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.030713081359863 | KNN Loss: 3.021446704864502 | BCE Loss: 1.0092666149139404\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.100675582885742 | KNN Loss: 3.0746021270751953 | BCE Loss: 1.0260732173919678\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.070102691650391 | KNN Loss: 3.047433376312256 | BCE Loss: 1.0226690769195557\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.098897933959961 | KNN Loss: 3.0635159015655518 | BCE Loss: 1.0353822708129883\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.104877948760986 | KNN Loss: 3.0715532302856445 | BCE Loss: 1.0333245992660522\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.104455471038818 | KNN Loss: 3.0778872966766357 | BCE Loss: 1.0265681743621826\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.106496810913086 | KNN Loss: 3.068612575531006 | BCE Loss: 1.0378841161727905\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.063315391540527 | KNN Loss: 3.075906991958618 | BCE Loss: 0.9874085187911987\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.060667514801025 | KNN Loss: 3.0514848232269287 | BCE Loss: 1.0091826915740967\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.050942420959473 | KNN Loss: 3.0404787063598633 | BCE Loss: 1.0104637145996094\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.10482931137085 | KNN Loss: 3.064084053039551 | BCE Loss: 1.0407452583312988\n",
      "Epoch   284: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.061263084411621 | KNN Loss: 3.0349254608154297 | BCE Loss: 1.0263376235961914\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.094839096069336 | KNN Loss: 3.08298659324646 | BCE Loss: 1.011852741241455\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.02628755569458 | KNN Loss: 3.0286200046539307 | BCE Loss: 0.9976674914360046\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.107051849365234 | KNN Loss: 3.082634449005127 | BCE Loss: 1.0244174003601074\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.068510055541992 | KNN Loss: 3.031611442565918 | BCE Loss: 1.0368983745574951\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.089955806732178 | KNN Loss: 3.0906732082366943 | BCE Loss: 0.9992825984954834\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.0837507247924805 | KNN Loss: 3.0611302852630615 | BCE Loss: 1.022620439529419\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.097970485687256 | KNN Loss: 3.076953649520874 | BCE Loss: 1.0210167169570923\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.061424732208252 | KNN Loss: 3.0681774616241455 | BCE Loss: 0.9932472109794617\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.068296432495117 | KNN Loss: 3.055068016052246 | BCE Loss: 1.013228178024292\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.032890319824219 | KNN Loss: 3.0175106525421143 | BCE Loss: 1.015379548072815\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.058706283569336 | KNN Loss: 3.0644867420196533 | BCE Loss: 0.9942197799682617\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.048798084259033 | KNN Loss: 3.037318229675293 | BCE Loss: 1.0114798545837402\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.12160062789917 | KNN Loss: 3.091217517852783 | BCE Loss: 1.0303831100463867\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.089123249053955 | KNN Loss: 3.0723607540130615 | BCE Loss: 1.016762614250183\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.082122802734375 | KNN Loss: 3.057570219039917 | BCE Loss: 1.024552583694458\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.104428291320801 | KNN Loss: 3.0614452362060547 | BCE Loss: 1.042983055114746\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.061045169830322 | KNN Loss: 3.056154727935791 | BCE Loss: 1.0048905611038208\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.105215549468994 | KNN Loss: 3.052901029586792 | BCE Loss: 1.0523144006729126\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.065278053283691 | KNN Loss: 3.087803363800049 | BCE Loss: 0.9774744510650635\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.030007839202881 | KNN Loss: 3.0417580604553223 | BCE Loss: 0.9882498979568481\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.096652030944824 | KNN Loss: 3.079369068145752 | BCE Loss: 1.0172829627990723\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.078104019165039 | KNN Loss: 3.061807632446289 | BCE Loss: 1.0162962675094604\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.052954196929932 | KNN Loss: 3.029589891433716 | BCE Loss: 1.0233643054962158\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.0685577392578125 | KNN Loss: 3.04705548286438 | BCE Loss: 1.021502137184143\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.04685640335083 | KNN Loss: 3.0478997230529785 | BCE Loss: 0.9989566206932068\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.094304084777832 | KNN Loss: 3.0934512615203857 | BCE Loss: 1.0008530616760254\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.12021541595459 | KNN Loss: 3.077887773513794 | BCE Loss: 1.0423275232315063\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.049753189086914 | KNN Loss: 3.042318105697632 | BCE Loss: 1.0074350833892822\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.084005832672119 | KNN Loss: 3.0473077297210693 | BCE Loss: 1.0366981029510498\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.058069705963135 | KNN Loss: 3.0370497703552246 | BCE Loss: 1.0210198163986206\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.0840864181518555 | KNN Loss: 3.0538294315338135 | BCE Loss: 1.030256986618042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.04249382019043 | KNN Loss: 3.031182050704956 | BCE Loss: 1.0113115310668945\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.05267333984375 | KNN Loss: 3.068523406982422 | BCE Loss: 0.984149694442749\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.067854881286621 | KNN Loss: 3.0508241653442383 | BCE Loss: 1.0170304775238037\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.091040134429932 | KNN Loss: 3.0988481044769287 | BCE Loss: 0.9921919703483582\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.085176467895508 | KNN Loss: 3.0615639686584473 | BCE Loss: 1.0236122608184814\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.060710906982422 | KNN Loss: 3.057844638824463 | BCE Loss: 1.0028663873672485\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.091562747955322 | KNN Loss: 3.087888717651367 | BCE Loss: 1.003674030303955\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.131926536560059 | KNN Loss: 3.0802345275878906 | BCE Loss: 1.051692247390747\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.0852484703063965 | KNN Loss: 3.0668485164642334 | BCE Loss: 1.0184000730514526\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.070593357086182 | KNN Loss: 3.028130054473877 | BCE Loss: 1.0424631834030151\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.050756454467773 | KNN Loss: 3.038046360015869 | BCE Loss: 1.0127100944519043\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.072765350341797 | KNN Loss: 3.066053867340088 | BCE Loss: 1.006711721420288\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.130159854888916 | KNN Loss: 3.1047494411468506 | BCE Loss: 1.0254104137420654\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.070572853088379 | KNN Loss: 3.0434765815734863 | BCE Loss: 1.0270963907241821\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.073239803314209 | KNN Loss: 3.053361654281616 | BCE Loss: 1.0198781490325928\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.053934097290039 | KNN Loss: 3.0358173847198486 | BCE Loss: 1.01811683177948\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.054612636566162 | KNN Loss: 3.0528440475463867 | BCE Loss: 1.0017685890197754\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.04939079284668 | KNN Loss: 3.039050579071045 | BCE Loss: 1.0103399753570557\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.0680623054504395 | KNN Loss: 3.0440664291381836 | BCE Loss: 1.0239958763122559\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.123755931854248 | KNN Loss: 3.1084086894989014 | BCE Loss: 1.0153473615646362\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.02843713760376 | KNN Loss: 3.03741192817688 | BCE Loss: 0.9910251498222351\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.1211137771606445 | KNN Loss: 3.0886006355285645 | BCE Loss: 1.03251314163208\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.040868759155273 | KNN Loss: 3.028052568435669 | BCE Loss: 1.0128164291381836\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.073884963989258 | KNN Loss: 3.0681426525115967 | BCE Loss: 1.0057425498962402\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.108734130859375 | KNN Loss: 3.082660675048828 | BCE Loss: 1.0260733366012573\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.082625389099121 | KNN Loss: 3.0561623573303223 | BCE Loss: 1.026463270187378\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.07114315032959 | KNN Loss: 3.068166494369507 | BCE Loss: 1.002976655960083\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.0972065925598145 | KNN Loss: 3.060103178024292 | BCE Loss: 1.037103533744812\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.117003440856934 | KNN Loss: 3.0762195587158203 | BCE Loss: 1.0407841205596924\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.073938369750977 | KNN Loss: 3.0796844959259033 | BCE Loss: 0.9942538738250732\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.0581231117248535 | KNN Loss: 3.0482699871063232 | BCE Loss: 1.0098531246185303\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.113640785217285 | KNN Loss: 3.0871241092681885 | BCE Loss: 1.0265164375305176\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.115653991699219 | KNN Loss: 3.085975170135498 | BCE Loss: 1.0296787023544312\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.094060897827148 | KNN Loss: 3.062840700149536 | BCE Loss: 1.0312203168869019\n",
      "Epoch   295: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.105286598205566 | KNN Loss: 3.0763261318206787 | BCE Loss: 1.0289607048034668\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.091334819793701 | KNN Loss: 3.0795669555664062 | BCE Loss: 1.011767864227295\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.085785865783691 | KNN Loss: 3.086376428604126 | BCE Loss: 0.999409556388855\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.105194091796875 | KNN Loss: 3.080294370651245 | BCE Loss: 1.0248997211456299\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.081637382507324 | KNN Loss: 3.0662500858306885 | BCE Loss: 1.0153870582580566\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.119926452636719 | KNN Loss: 3.093244791030884 | BCE Loss: 1.0266814231872559\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.0501298904418945 | KNN Loss: 3.0611767768859863 | BCE Loss: 0.9889529943466187\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.070455551147461 | KNN Loss: 3.0671706199645996 | BCE Loss: 1.0032846927642822\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.07610559463501 | KNN Loss: 3.044985055923462 | BCE Loss: 1.0311205387115479\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.098078727722168 | KNN Loss: 3.0793378353118896 | BCE Loss: 1.0187410116195679\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.041884422302246 | KNN Loss: 3.0247364044189453 | BCE Loss: 1.0171477794647217\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.06503438949585 | KNN Loss: 3.058576822280884 | BCE Loss: 1.0064575672149658\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.099273681640625 | KNN Loss: 3.0645763874053955 | BCE Loss: 1.034697413444519\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.046919345855713 | KNN Loss: 3.050811529159546 | BCE Loss: 0.9961076974868774\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.093907356262207 | KNN Loss: 3.068643569946289 | BCE Loss: 1.025263786315918\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.068865776062012 | KNN Loss: 3.0556418895721436 | BCE Loss: 1.0132238864898682\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.076990127563477 | KNN Loss: 3.0655159950256348 | BCE Loss: 1.0114738941192627\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.061450004577637 | KNN Loss: 3.0434043407440186 | BCE Loss: 1.0180459022521973\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.088558673858643 | KNN Loss: 3.055861711502075 | BCE Loss: 1.0326969623565674\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.1001176834106445 | KNN Loss: 3.0983223915100098 | BCE Loss: 1.0017950534820557\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.168041706085205 | KNN Loss: 3.1247832775115967 | BCE Loss: 1.0432583093643188\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.131255149841309 | KNN Loss: 3.077410936355591 | BCE Loss: 1.0538439750671387\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.017025470733643 | KNN Loss: 3.0362284183502197 | BCE Loss: 0.9807972311973572\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.089426040649414 | KNN Loss: 3.0833218097686768 | BCE Loss: 1.0061044692993164\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.067561149597168 | KNN Loss: 3.049734354019165 | BCE Loss: 1.017827033996582\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.096179008483887 | KNN Loss: 3.0563292503356934 | BCE Loss: 1.0398496389389038\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.107738494873047 | KNN Loss: 3.0889828205108643 | BCE Loss: 1.0187554359436035\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.100687026977539 | KNN Loss: 3.056962013244629 | BCE Loss: 1.043724775314331\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.064208030700684 | KNN Loss: 3.0448250770568848 | BCE Loss: 1.0193827152252197\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.046231746673584 | KNN Loss: 3.0611817836761475 | BCE Loss: 0.9850497841835022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.03791618347168 | KNN Loss: 3.0237276554107666 | BCE Loss: 1.0141886472702026\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.053903579711914 | KNN Loss: 3.036454916000366 | BCE Loss: 1.017448902130127\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.1013946533203125 | KNN Loss: 3.069600820541382 | BCE Loss: 1.0317939519882202\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.0716166496276855 | KNN Loss: 3.0533080101013184 | BCE Loss: 1.0183086395263672\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.098880767822266 | KNN Loss: 3.095897674560547 | BCE Loss: 1.0029830932617188\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.079333305358887 | KNN Loss: 3.0556344985961914 | BCE Loss: 1.0236989259719849\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.1141510009765625 | KNN Loss: 3.0491294860839844 | BCE Loss: 1.065021276473999\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.093747615814209 | KNN Loss: 3.0795960426330566 | BCE Loss: 1.0141514539718628\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.08111047744751 | KNN Loss: 3.0821452140808105 | BCE Loss: 0.9989653825759888\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.116953372955322 | KNN Loss: 3.0779852867126465 | BCE Loss: 1.0389680862426758\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.063409805297852 | KNN Loss: 3.059361457824707 | BCE Loss: 1.004048466682434\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.093693256378174 | KNN Loss: 3.0497117042541504 | BCE Loss: 1.0439815521240234\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.060399055480957 | KNN Loss: 3.0636487007141113 | BCE Loss: 0.9967502355575562\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.081482887268066 | KNN Loss: 3.071934223175049 | BCE Loss: 1.0095484256744385\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.043462753295898 | KNN Loss: 3.0588366985321045 | BCE Loss: 0.9846259355545044\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.067400932312012 | KNN Loss: 3.061981439590454 | BCE Loss: 1.0054197311401367\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.087338447570801 | KNN Loss: 3.0628395080566406 | BCE Loss: 1.0244990587234497\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.096700668334961 | KNN Loss: 3.0729942321777344 | BCE Loss: 1.0237061977386475\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.046706676483154 | KNN Loss: 3.0364725589752197 | BCE Loss: 1.0102342367172241\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.106432914733887 | KNN Loss: 3.059015989303589 | BCE Loss: 1.0474166870117188\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.093592166900635 | KNN Loss: 3.067453384399414 | BCE Loss: 1.0261386632919312\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.064369201660156 | KNN Loss: 3.048718214035034 | BCE Loss: 1.015650749206543\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.091972827911377 | KNN Loss: 3.0853075981140137 | BCE Loss: 1.0066653490066528\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.078187465667725 | KNN Loss: 3.0579183101654053 | BCE Loss: 1.0202691555023193\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.069014549255371 | KNN Loss: 3.0656752586364746 | BCE Loss: 1.0033390522003174\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.08659553527832 | KNN Loss: 3.0642876625061035 | BCE Loss: 1.0223078727722168\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.071050643920898 | KNN Loss: 3.0700736045837402 | BCE Loss: 1.0009770393371582\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.096175670623779 | KNN Loss: 3.1045572757720947 | BCE Loss: 0.9916183948516846\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.107905387878418 | KNN Loss: 3.078518867492676 | BCE Loss: 1.0293865203857422\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.131868839263916 | KNN Loss: 3.084108829498291 | BCE Loss: 1.047760009765625\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.040001392364502 | KNN Loss: 3.034492254257202 | BCE Loss: 1.0055092573165894\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.0967206954956055 | KNN Loss: 3.0933098793029785 | BCE Loss: 1.003411054611206\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.122825622558594 | KNN Loss: 3.089635133743286 | BCE Loss: 1.0331904888153076\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.094882965087891 | KNN Loss: 3.0647692680358887 | BCE Loss: 1.0301134586334229\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.0657854080200195 | KNN Loss: 3.056541681289673 | BCE Loss: 1.0092439651489258\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.07852840423584 | KNN Loss: 3.057356595993042 | BCE Loss: 1.021172046661377\n",
      "Epoch   306: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.144146919250488 | KNN Loss: 3.0713260173797607 | BCE Loss: 1.072821021080017\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.086235046386719 | KNN Loss: 3.052757501602173 | BCE Loss: 1.033477544784546\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.090248107910156 | KNN Loss: 3.066120147705078 | BCE Loss: 1.024127721786499\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.075974941253662 | KNN Loss: 3.0509731769561768 | BCE Loss: 1.0250017642974854\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.082821846008301 | KNN Loss: 3.0476701259613037 | BCE Loss: 1.0351519584655762\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.130521774291992 | KNN Loss: 3.0817935466766357 | BCE Loss: 1.0487282276153564\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.06788969039917 | KNN Loss: 3.051163673400879 | BCE Loss: 1.0167258977890015\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.070613861083984 | KNN Loss: 3.038186550140381 | BCE Loss: 1.0324273109436035\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.093064308166504 | KNN Loss: 3.0682387351989746 | BCE Loss: 1.0248258113861084\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.080303192138672 | KNN Loss: 3.077868938446045 | BCE Loss: 1.0024340152740479\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.117332458496094 | KNN Loss: 3.0566587448120117 | BCE Loss: 1.060673475265503\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.094309329986572 | KNN Loss: 3.105501174926758 | BCE Loss: 0.9888083338737488\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.099611282348633 | KNN Loss: 3.07645583152771 | BCE Loss: 1.0231554508209229\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.080989837646484 | KNN Loss: 3.0543251037597656 | BCE Loss: 1.0266646146774292\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.051932334899902 | KNN Loss: 3.0458664894104004 | BCE Loss: 1.006066083908081\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.110772132873535 | KNN Loss: 3.0865910053253174 | BCE Loss: 1.0241808891296387\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.105477809906006 | KNN Loss: 3.0708611011505127 | BCE Loss: 1.0346168279647827\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.031057834625244 | KNN Loss: 3.0256662368774414 | BCE Loss: 1.0053915977478027\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.097967624664307 | KNN Loss: 3.079822301864624 | BCE Loss: 1.018145203590393\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.098167419433594 | KNN Loss: 3.0862085819244385 | BCE Loss: 1.0119587182998657\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.089300155639648 | KNN Loss: 3.0632598400115967 | BCE Loss: 1.0260405540466309\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.025300025939941 | KNN Loss: 3.021214246749878 | BCE Loss: 1.0040855407714844\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.093263626098633 | KNN Loss: 3.059429407119751 | BCE Loss: 1.0338340997695923\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.130880355834961 | KNN Loss: 3.0990819931030273 | BCE Loss: 1.0317981243133545\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.132976531982422 | KNN Loss: 3.0661652088165283 | BCE Loss: 1.0668115615844727\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.082180976867676 | KNN Loss: 3.042874336242676 | BCE Loss: 1.0393067598342896\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.0667009353637695 | KNN Loss: 3.0678884983062744 | BCE Loss: 0.9988125562667847\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.065713405609131 | KNN Loss: 3.0539920330047607 | BCE Loss: 1.0117213726043701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.0688676834106445 | KNN Loss: 3.054885149002075 | BCE Loss: 1.0139825344085693\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.069663047790527 | KNN Loss: 3.0568366050720215 | BCE Loss: 1.0128264427185059\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.105458736419678 | KNN Loss: 3.0780348777770996 | BCE Loss: 1.0274237394332886\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.0656561851501465 | KNN Loss: 3.0675697326660156 | BCE Loss: 0.9980863332748413\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.098581314086914 | KNN Loss: 3.0619049072265625 | BCE Loss: 1.0366766452789307\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.042320251464844 | KNN Loss: 3.022893190383911 | BCE Loss: 1.0194268226623535\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.037567138671875 | KNN Loss: 3.0338921546936035 | BCE Loss: 1.0036749839782715\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.072630405426025 | KNN Loss: 3.0723655223846436 | BCE Loss: 1.0002648830413818\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.105406761169434 | KNN Loss: 3.074608087539673 | BCE Loss: 1.0307986736297607\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.077908992767334 | KNN Loss: 3.0665359497070312 | BCE Loss: 1.0113729238510132\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.071718215942383 | KNN Loss: 3.0561325550079346 | BCE Loss: 1.0155858993530273\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.041722297668457 | KNN Loss: 3.057616710662842 | BCE Loss: 0.9841055870056152\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.040818691253662 | KNN Loss: 3.052738666534424 | BCE Loss: 0.9880801439285278\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.078509330749512 | KNN Loss: 3.0439233779907227 | BCE Loss: 1.0345861911773682\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.1110429763793945 | KNN Loss: 3.077619791030884 | BCE Loss: 1.0334231853485107\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.06691312789917 | KNN Loss: 3.068657875061035 | BCE Loss: 0.9982554316520691\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.043191909790039 | KNN Loss: 3.038055419921875 | BCE Loss: 1.005136251449585\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.114171981811523 | KNN Loss: 3.1101930141448975 | BCE Loss: 1.0039787292480469\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.081106185913086 | KNN Loss: 3.0814578533172607 | BCE Loss: 0.9996482133865356\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.055537700653076 | KNN Loss: 3.056262493133545 | BCE Loss: 0.9992750883102417\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.107590198516846 | KNN Loss: 3.0834550857543945 | BCE Loss: 1.0241349935531616\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.0637898445129395 | KNN Loss: 3.042611837387085 | BCE Loss: 1.021177887916565\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.043553829193115 | KNN Loss: 3.0609028339385986 | BCE Loss: 0.9826510548591614\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.033832550048828 | KNN Loss: 3.0218617916107178 | BCE Loss: 1.0119705200195312\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.050381660461426 | KNN Loss: 3.0370657444000244 | BCE Loss: 1.0133159160614014\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.133174419403076 | KNN Loss: 3.0843143463134766 | BCE Loss: 1.04885995388031\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.034983158111572 | KNN Loss: 3.0472464561462402 | BCE Loss: 0.9877365827560425\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.170246601104736 | KNN Loss: 3.0995030403137207 | BCE Loss: 1.0707435607910156\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.072229862213135 | KNN Loss: 3.065624237060547 | BCE Loss: 1.0066057443618774\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.1095404624938965 | KNN Loss: 3.06166410446167 | BCE Loss: 1.0478763580322266\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.087187767028809 | KNN Loss: 3.0687389373779297 | BCE Loss: 1.0184485912322998\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.0993475914001465 | KNN Loss: 3.0656824111938477 | BCE Loss: 1.0336650609970093\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.060384750366211 | KNN Loss: 3.046015501022339 | BCE Loss: 1.014369249343872\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.075089454650879 | KNN Loss: 3.076164484024048 | BCE Loss: 0.9989252090454102\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.061179161071777 | KNN Loss: 3.0391438007354736 | BCE Loss: 1.0220355987548828\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.103432655334473 | KNN Loss: 3.0811285972595215 | BCE Loss: 1.0223039388656616\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.05368709564209 | KNN Loss: 3.023597478866577 | BCE Loss: 1.0300897359848022\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.094923973083496 | KNN Loss: 3.05696177482605 | BCE Loss: 1.0379620790481567\n",
      "Epoch   317: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.094298362731934 | KNN Loss: 3.087503433227539 | BCE Loss: 1.0067949295043945\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.104723930358887 | KNN Loss: 3.066946506500244 | BCE Loss: 1.0377771854400635\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.095955848693848 | KNN Loss: 3.063406467437744 | BCE Loss: 1.0325491428375244\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.088177680969238 | KNN Loss: 3.053563356399536 | BCE Loss: 1.0346143245697021\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.11818790435791 | KNN Loss: 3.0834460258483887 | BCE Loss: 1.0347421169281006\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.1418561935424805 | KNN Loss: 3.0955560207366943 | BCE Loss: 1.0463000535964966\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.053344249725342 | KNN Loss: 3.0183000564575195 | BCE Loss: 1.0350443124771118\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.0936479568481445 | KNN Loss: 3.0698819160461426 | BCE Loss: 1.023766279220581\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.051624298095703 | KNN Loss: 3.029890298843384 | BCE Loss: 1.0217339992523193\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.083716869354248 | KNN Loss: 3.054367780685425 | BCE Loss: 1.0293492078781128\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.0259690284729 | KNN Loss: 3.0412704944610596 | BCE Loss: 0.984698474407196\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.077916145324707 | KNN Loss: 3.060849666595459 | BCE Loss: 1.017066240310669\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.077747821807861 | KNN Loss: 3.078619956970215 | BCE Loss: 0.9991278052330017\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.101540565490723 | KNN Loss: 3.0725173950195312 | BCE Loss: 1.0290229320526123\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.100361347198486 | KNN Loss: 3.066894769668579 | BCE Loss: 1.0334664583206177\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.089942932128906 | KNN Loss: 3.0617117881774902 | BCE Loss: 1.028230905532837\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.044976711273193 | KNN Loss: 3.0516104698181152 | BCE Loss: 0.9933661222457886\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.13169002532959 | KNN Loss: 3.096984386444092 | BCE Loss: 1.034705400466919\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.089089393615723 | KNN Loss: 3.073478937149048 | BCE Loss: 1.015610694885254\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.112814426422119 | KNN Loss: 3.0858421325683594 | BCE Loss: 1.0269724130630493\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.098398208618164 | KNN Loss: 3.0556416511535645 | BCE Loss: 1.0427563190460205\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.08182954788208 | KNN Loss: 3.0536415576934814 | BCE Loss: 1.0281879901885986\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.097843170166016 | KNN Loss: 3.074516773223877 | BCE Loss: 1.0233261585235596\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.042205810546875 | KNN Loss: 3.061250925064087 | BCE Loss: 0.9809551239013672\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.0998687744140625 | KNN Loss: 3.057180166244507 | BCE Loss: 1.0426888465881348\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.02518367767334 | KNN Loss: 3.02506422996521 | BCE Loss: 1.0001193284988403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.089625358581543 | KNN Loss: 3.05407452583313 | BCE Loss: 1.0355510711669922\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.054434299468994 | KNN Loss: 3.027496099472046 | BCE Loss: 1.0269383192062378\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.070505142211914 | KNN Loss: 3.0712969303131104 | BCE Loss: 0.9992079734802246\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.0474348068237305 | KNN Loss: 3.0566258430480957 | BCE Loss: 0.9908088445663452\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.1109619140625 | KNN Loss: 3.073997735977173 | BCE Loss: 1.0369642972946167\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.1141533851623535 | KNN Loss: 3.076200246810913 | BCE Loss: 1.0379530191421509\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.134049892425537 | KNN Loss: 3.0792510509490967 | BCE Loss: 1.0547988414764404\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.055889129638672 | KNN Loss: 3.0512466430664062 | BCE Loss: 1.0046427249908447\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.04600715637207 | KNN Loss: 3.0204999446868896 | BCE Loss: 1.0255072116851807\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.090090274810791 | KNN Loss: 3.093430995941162 | BCE Loss: 0.9966592788696289\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.100192070007324 | KNN Loss: 3.0850484371185303 | BCE Loss: 1.0151437520980835\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.108229160308838 | KNN Loss: 3.070140838623047 | BCE Loss: 1.038088321685791\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.062702178955078 | KNN Loss: 3.0512170791625977 | BCE Loss: 1.0114853382110596\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.077690124511719 | KNN Loss: 3.052704334259033 | BCE Loss: 1.0249860286712646\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.074357509613037 | KNN Loss: 3.040855646133423 | BCE Loss: 1.0335019826889038\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.024364948272705 | KNN Loss: 3.033970594406128 | BCE Loss: 0.9903942346572876\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.126565456390381 | KNN Loss: 3.0857784748077393 | BCE Loss: 1.0407869815826416\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.122968673706055 | KNN Loss: 3.081404447555542 | BCE Loss: 1.0415642261505127\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.09348201751709 | KNN Loss: 3.071962833404541 | BCE Loss: 1.0215189456939697\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.102100372314453 | KNN Loss: 3.092954397201538 | BCE Loss: 1.009145736694336\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.069285869598389 | KNN Loss: 3.04764461517334 | BCE Loss: 1.0216412544250488\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.0940070152282715 | KNN Loss: 3.0681798458099365 | BCE Loss: 1.025827169418335\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.091634750366211 | KNN Loss: 3.0531554222106934 | BCE Loss: 1.0384790897369385\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.1002373695373535 | KNN Loss: 3.086409568786621 | BCE Loss: 1.0138278007507324\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.11049747467041 | KNN Loss: 3.103781223297119 | BCE Loss: 1.006716251373291\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.017167091369629 | KNN Loss: 3.0140867233276367 | BCE Loss: 1.0030803680419922\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.101556301116943 | KNN Loss: 3.061107873916626 | BCE Loss: 1.0404484272003174\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.065720558166504 | KNN Loss: 3.0263984203338623 | BCE Loss: 1.0393218994140625\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.041862487792969 | KNN Loss: 3.0233523845672607 | BCE Loss: 1.018510103225708\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.079646110534668 | KNN Loss: 3.0448670387268066 | BCE Loss: 1.0347793102264404\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.083942413330078 | KNN Loss: 3.0662572383880615 | BCE Loss: 1.0176854133605957\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.093459129333496 | KNN Loss: 3.0867507457733154 | BCE Loss: 1.0067083835601807\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.069943428039551 | KNN Loss: 3.0395333766937256 | BCE Loss: 1.0304102897644043\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.094910621643066 | KNN Loss: 3.085969924926758 | BCE Loss: 1.0089404582977295\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.056334972381592 | KNN Loss: 3.0629773139953613 | BCE Loss: 0.99335777759552\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.11021614074707 | KNN Loss: 3.0687737464904785 | BCE Loss: 1.0414421558380127\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.077210426330566 | KNN Loss: 3.0605812072753906 | BCE Loss: 1.0166290998458862\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.0604424476623535 | KNN Loss: 3.0497679710388184 | BCE Loss: 1.0106744766235352\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.039758682250977 | KNN Loss: 3.0361039638519287 | BCE Loss: 1.0036544799804688\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.046567916870117 | KNN Loss: 3.0376453399658203 | BCE Loss: 1.0089226961135864\n",
      "Epoch   328: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.079752445220947 | KNN Loss: 3.0594136714935303 | BCE Loss: 1.020338773727417\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.101800918579102 | KNN Loss: 3.0595450401306152 | BCE Loss: 1.0422559976577759\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.057865142822266 | KNN Loss: 3.0402543544769287 | BCE Loss: 1.017610788345337\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.065616130828857 | KNN Loss: 3.0667481422424316 | BCE Loss: 0.9988678693771362\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.1039862632751465 | KNN Loss: 3.101102352142334 | BCE Loss: 1.0028839111328125\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.047606468200684 | KNN Loss: 3.0550899505615234 | BCE Loss: 0.9925167560577393\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.019650936126709 | KNN Loss: 3.0228986740112305 | BCE Loss: 0.9967522621154785\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.125466346740723 | KNN Loss: 3.1024301052093506 | BCE Loss: 1.0230361223220825\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.061591148376465 | KNN Loss: 3.0430667400360107 | BCE Loss: 1.0185245275497437\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.113988876342773 | KNN Loss: 3.0750575065612793 | BCE Loss: 1.0389312505722046\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.100153923034668 | KNN Loss: 3.0534491539001465 | BCE Loss: 1.0467045307159424\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.025232791900635 | KNN Loss: 3.014974594116211 | BCE Loss: 1.0102580785751343\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.080255031585693 | KNN Loss: 3.063095808029175 | BCE Loss: 1.017159104347229\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.09661865234375 | KNN Loss: 3.0855886936187744 | BCE Loss: 1.0110299587249756\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.076501846313477 | KNN Loss: 3.0741586685180664 | BCE Loss: 1.0023431777954102\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.118589401245117 | KNN Loss: 3.083868980407715 | BCE Loss: 1.0347201824188232\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.113558292388916 | KNN Loss: 3.08115553855896 | BCE Loss: 1.032402753829956\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.116160869598389 | KNN Loss: 3.094620704650879 | BCE Loss: 1.0215401649475098\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.066565036773682 | KNN Loss: 3.031115770339966 | BCE Loss: 1.0354491472244263\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.084134578704834 | KNN Loss: 3.0631134510040283 | BCE Loss: 1.0210212469100952\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.11630916595459 | KNN Loss: 3.0885138511657715 | BCE Loss: 1.0277950763702393\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.065086364746094 | KNN Loss: 3.0494906902313232 | BCE Loss: 1.0155954360961914\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.085781097412109 | KNN Loss: 3.0800440311431885 | BCE Loss: 1.0057373046875\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.043702602386475 | KNN Loss: 3.0253424644470215 | BCE Loss: 1.0183600187301636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.0422868728637695 | KNN Loss: 3.037440061569214 | BCE Loss: 1.0048468112945557\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.054657936096191 | KNN Loss: 3.055222749710083 | BCE Loss: 0.9994350075721741\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.098073959350586 | KNN Loss: 3.0644638538360596 | BCE Loss: 1.0336103439331055\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.08916711807251 | KNN Loss: 3.07126522064209 | BCE Loss: 1.0179020166397095\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.099562644958496 | KNN Loss: 3.0796992778778076 | BCE Loss: 1.0198631286621094\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.133970260620117 | KNN Loss: 3.1102516651153564 | BCE Loss: 1.0237188339233398\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.1077070236206055 | KNN Loss: 3.0923643112182617 | BCE Loss: 1.0153424739837646\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.0902419090271 | KNN Loss: 3.0819711685180664 | BCE Loss: 1.0082707405090332\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.080503463745117 | KNN Loss: 3.0916783809661865 | BCE Loss: 0.9888253211975098\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.110635280609131 | KNN Loss: 3.0771801471710205 | BCE Loss: 1.0334551334381104\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.126503944396973 | KNN Loss: 3.098525285720825 | BCE Loss: 1.0279786586761475\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.096225738525391 | KNN Loss: 3.065767765045166 | BCE Loss: 1.0304577350616455\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.119688987731934 | KNN Loss: 3.0851850509643555 | BCE Loss: 1.034503698348999\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.075455665588379 | KNN Loss: 3.0564451217651367 | BCE Loss: 1.0190105438232422\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.112951278686523 | KNN Loss: 3.085357427597046 | BCE Loss: 1.0275940895080566\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.092654705047607 | KNN Loss: 3.062959671020508 | BCE Loss: 1.0296950340270996\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.0714850425720215 | KNN Loss: 3.056520700454712 | BCE Loss: 1.0149644613265991\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.05696964263916 | KNN Loss: 3.043963670730591 | BCE Loss: 1.0130062103271484\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.12766170501709 | KNN Loss: 3.102674961090088 | BCE Loss: 1.024986743927002\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.010015964508057 | KNN Loss: 3.0191078186035156 | BCE Loss: 0.9909082055091858\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.081827163696289 | KNN Loss: 3.0823848247528076 | BCE Loss: 0.9994421601295471\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.086280345916748 | KNN Loss: 3.076162815093994 | BCE Loss: 1.010117530822754\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.133861541748047 | KNN Loss: 3.094817638397217 | BCE Loss: 1.0390441417694092\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.095430374145508 | KNN Loss: 3.048906087875366 | BCE Loss: 1.0465244054794312\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.148241996765137 | KNN Loss: 3.096583843231201 | BCE Loss: 1.051658034324646\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.109536170959473 | KNN Loss: 3.053532838821411 | BCE Loss: 1.0560035705566406\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.071259021759033 | KNN Loss: 3.085231304168701 | BCE Loss: 0.9860276579856873\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.102243423461914 | KNN Loss: 3.079988956451416 | BCE Loss: 1.0222547054290771\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.125067710876465 | KNN Loss: 3.1028053760528564 | BCE Loss: 1.0222625732421875\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.084946155548096 | KNN Loss: 3.0867631435394287 | BCE Loss: 0.9981830716133118\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.0634355545043945 | KNN Loss: 3.0852906703948975 | BCE Loss: 0.978144645690918\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.090595245361328 | KNN Loss: 3.063105583190918 | BCE Loss: 1.027489423751831\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.091416358947754 | KNN Loss: 3.059345245361328 | BCE Loss: 1.0320709943771362\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.087248802185059 | KNN Loss: 3.0437819957733154 | BCE Loss: 1.0434669256210327\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.049206733703613 | KNN Loss: 3.041863441467285 | BCE Loss: 1.0073432922363281\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.150425910949707 | KNN Loss: 3.110839366912842 | BCE Loss: 1.0395867824554443\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.055732727050781 | KNN Loss: 3.0562198162078857 | BCE Loss: 0.9995129704475403\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.084317207336426 | KNN Loss: 3.0810368061065674 | BCE Loss: 1.0032801628112793\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.079793930053711 | KNN Loss: 3.042140483856201 | BCE Loss: 1.0376536846160889\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.081823825836182 | KNN Loss: 3.0702290534973145 | BCE Loss: 1.0115946531295776\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.151054859161377 | KNN Loss: 3.0813565254211426 | BCE Loss: 1.069698452949524\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.112566947937012 | KNN Loss: 3.0893290042877197 | BCE Loss: 1.023237705230713\n",
      "Epoch   339: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.124185085296631 | KNN Loss: 3.059752941131592 | BCE Loss: 1.064432144165039\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.06348991394043 | KNN Loss: 3.0525479316711426 | BCE Loss: 1.0109422206878662\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.071340560913086 | KNN Loss: 3.047281265258789 | BCE Loss: 1.0240592956542969\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.134452819824219 | KNN Loss: 3.099541187286377 | BCE Loss: 1.034911870956421\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.120959281921387 | KNN Loss: 3.1135969161987305 | BCE Loss: 1.0073623657226562\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.0844550132751465 | KNN Loss: 3.0660593509674072 | BCE Loss: 1.0183956623077393\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.043152809143066 | KNN Loss: 3.0428738594055176 | BCE Loss: 1.000279188156128\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.083372592926025 | KNN Loss: 3.065444231033325 | BCE Loss: 1.0179283618927002\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.064106464385986 | KNN Loss: 3.061356782913208 | BCE Loss: 1.0027496814727783\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.107431411743164 | KNN Loss: 3.0782649517059326 | BCE Loss: 1.0291664600372314\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.13279914855957 | KNN Loss: 3.1180260181427 | BCE Loss: 1.0147732496261597\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.096217632293701 | KNN Loss: 3.070917844772339 | BCE Loss: 1.0252996683120728\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.092897415161133 | KNN Loss: 3.08146071434021 | BCE Loss: 1.0114364624023438\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.077630996704102 | KNN Loss: 3.0644004344940186 | BCE Loss: 1.013230562210083\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.054853439331055 | KNN Loss: 3.058070659637451 | BCE Loss: 0.9967827796936035\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.0753560066223145 | KNN Loss: 3.05251407623291 | BCE Loss: 1.0228419303894043\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.075429439544678 | KNN Loss: 3.0700523853302 | BCE Loss: 1.0053770542144775\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.07018518447876 | KNN Loss: 3.0502357482910156 | BCE Loss: 1.0199494361877441\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.0619611740112305 | KNN Loss: 3.052924394607544 | BCE Loss: 1.0090370178222656\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.110703468322754 | KNN Loss: 3.066887378692627 | BCE Loss: 1.043816328048706\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.020269870758057 | KNN Loss: 3.0416219234466553 | BCE Loss: 0.9786480665206909\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.100069046020508 | KNN Loss: 3.080219030380249 | BCE Loss: 1.019850254058838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.122278690338135 | KNN Loss: 3.080533742904663 | BCE Loss: 1.0417449474334717\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.056013107299805 | KNN Loss: 3.0413880348205566 | BCE Loss: 1.0146253108978271\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.137190818786621 | KNN Loss: 3.083211660385132 | BCE Loss: 1.0539790391921997\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.1156158447265625 | KNN Loss: 3.0589075088500977 | BCE Loss: 1.056708574295044\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.043389797210693 | KNN Loss: 3.0465331077575684 | BCE Loss: 0.996856689453125\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.051224231719971 | KNN Loss: 3.0441367626190186 | BCE Loss: 1.0070874691009521\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.089864730834961 | KNN Loss: 3.0485119819641113 | BCE Loss: 1.04135262966156\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.054082870483398 | KNN Loss: 3.0389881134033203 | BCE Loss: 1.0150947570800781\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.119588851928711 | KNN Loss: 3.0936453342437744 | BCE Loss: 1.0259437561035156\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.112094879150391 | KNN Loss: 3.103184938430786 | BCE Loss: 1.0089097023010254\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.085316181182861 | KNN Loss: 3.074375867843628 | BCE Loss: 1.0109403133392334\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.099011421203613 | KNN Loss: 3.0866620540618896 | BCE Loss: 1.0123494863510132\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.061664581298828 | KNN Loss: 3.083730936050415 | BCE Loss: 0.977933406829834\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.057242393493652 | KNN Loss: 3.037534475326538 | BCE Loss: 1.0197081565856934\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.040943622589111 | KNN Loss: 3.0304081439971924 | BCE Loss: 1.010535478591919\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.0904693603515625 | KNN Loss: 3.031311273574829 | BCE Loss: 1.0591580867767334\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.052826881408691 | KNN Loss: 3.062525749206543 | BCE Loss: 0.9903010129928589\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.08417272567749 | KNN Loss: 3.0420467853546143 | BCE Loss: 1.0421258211135864\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.070598602294922 | KNN Loss: 3.0456509590148926 | BCE Loss: 1.0249477624893188\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.0722832679748535 | KNN Loss: 3.0580077171325684 | BCE Loss: 1.0142755508422852\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.124281883239746 | KNN Loss: 3.074585437774658 | BCE Loss: 1.049696445465088\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.115602016448975 | KNN Loss: 3.088859796524048 | BCE Loss: 1.0267422199249268\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.101258277893066 | KNN Loss: 3.060645818710327 | BCE Loss: 1.0406126976013184\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.070417404174805 | KNN Loss: 3.065403461456299 | BCE Loss: 1.005014181137085\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.112407684326172 | KNN Loss: 3.0916788578033447 | BCE Loss: 1.0207290649414062\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.112103462219238 | KNN Loss: 3.108597755432129 | BCE Loss: 1.0035054683685303\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.075289726257324 | KNN Loss: 3.0601649284362793 | BCE Loss: 1.0151246786117554\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.0654616355896 | KNN Loss: 3.052562713623047 | BCE Loss: 1.0128990411758423\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.088618278503418 | KNN Loss: 3.0706772804260254 | BCE Loss: 1.0179409980773926\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.026683807373047 | KNN Loss: 3.014124870300293 | BCE Loss: 1.0125588178634644\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.087262153625488 | KNN Loss: 3.051980495452881 | BCE Loss: 1.0352815389633179\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.127910614013672 | KNN Loss: 3.0652928352355957 | BCE Loss: 1.0626176595687866\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.071971893310547 | KNN Loss: 3.0682969093322754 | BCE Loss: 1.0036749839782715\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.026147365570068 | KNN Loss: 3.023117780685425 | BCE Loss: 1.003029465675354\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.073934555053711 | KNN Loss: 3.0560872554779053 | BCE Loss: 1.0178471803665161\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.093598365783691 | KNN Loss: 3.0517313480377197 | BCE Loss: 1.0418671369552612\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.086648941040039 | KNN Loss: 3.0492289066314697 | BCE Loss: 1.0374200344085693\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.067114353179932 | KNN Loss: 3.0491831302642822 | BCE Loss: 1.0179312229156494\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.087175369262695 | KNN Loss: 3.0499885082244873 | BCE Loss: 1.0371867418289185\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.070921897888184 | KNN Loss: 3.045746326446533 | BCE Loss: 1.0251754522323608\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.078334808349609 | KNN Loss: 3.047978401184082 | BCE Loss: 1.0303564071655273\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.0912604331970215 | KNN Loss: 3.0728328227996826 | BCE Loss: 1.0184276103973389\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.087762832641602 | KNN Loss: 3.0676333904266357 | BCE Loss: 1.0201292037963867\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.060242176055908 | KNN Loss: 3.0460994243621826 | BCE Loss: 1.014142632484436\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.078744888305664 | KNN Loss: 3.0655651092529297 | BCE Loss: 1.0131800174713135\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.106021881103516 | KNN Loss: 3.0738723278045654 | BCE Loss: 1.0321495532989502\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.03774881362915 | KNN Loss: 3.038597583770752 | BCE Loss: 0.9991511702537537\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.053072452545166 | KNN Loss: 3.0300629138946533 | BCE Loss: 1.0230094194412231\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.080169200897217 | KNN Loss: 3.0674068927764893 | BCE Loss: 1.012762427330017\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.074009895324707 | KNN Loss: 3.0770275592803955 | BCE Loss: 0.9969825148582458\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.075695514678955 | KNN Loss: 3.0561728477478027 | BCE Loss: 1.0195225477218628\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.102144241333008 | KNN Loss: 3.080597400665283 | BCE Loss: 1.021546721458435\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.097232818603516 | KNN Loss: 3.0666489601135254 | BCE Loss: 1.0305836200714111\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.094210147857666 | KNN Loss: 3.072355270385742 | BCE Loss: 1.0218547582626343\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.096097946166992 | KNN Loss: 3.091458320617676 | BCE Loss: 1.0046393871307373\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.01722526550293 | KNN Loss: 3.0231704711914062 | BCE Loss: 0.994054913520813\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.072824478149414 | KNN Loss: 3.0640439987182617 | BCE Loss: 1.0087802410125732\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.080664157867432 | KNN Loss: 3.074716806411743 | BCE Loss: 1.0059473514556885\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.0991010665893555 | KNN Loss: 3.0541207790374756 | BCE Loss: 1.044980525970459\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.066353797912598 | KNN Loss: 3.0515167713165283 | BCE Loss: 1.0148369073867798\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.080776214599609 | KNN Loss: 3.063917875289917 | BCE Loss: 1.0168585777282715\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.047828197479248 | KNN Loss: 3.0556843280792236 | BCE Loss: 0.9921437501907349\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.095082759857178 | KNN Loss: 3.07313871383667 | BCE Loss: 1.0219440460205078\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.0986433029174805 | KNN Loss: 3.072157621383667 | BCE Loss: 1.026485562324524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.097019195556641 | KNN Loss: 3.074462413787842 | BCE Loss: 1.0225566625595093\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.085872650146484 | KNN Loss: 3.069568634033203 | BCE Loss: 1.0163038969039917\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.0974225997924805 | KNN Loss: 3.0741922855377197 | BCE Loss: 1.0232301950454712\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.092455863952637 | KNN Loss: 3.0785021781921387 | BCE Loss: 1.0139538049697876\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.056356430053711 | KNN Loss: 3.047128200531006 | BCE Loss: 1.0092281103134155\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.112671852111816 | KNN Loss: 3.0718259811401367 | BCE Loss: 1.0408458709716797\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.09751558303833 | KNN Loss: 3.079820156097412 | BCE Loss: 1.017695426940918\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.140211582183838 | KNN Loss: 3.137942314147949 | BCE Loss: 1.0022692680358887\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.0633697509765625 | KNN Loss: 3.043325901031494 | BCE Loss: 1.0200440883636475\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.160961151123047 | KNN Loss: 3.1199088096618652 | BCE Loss: 1.0410523414611816\n",
      "Epoch   355: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.077047348022461 | KNN Loss: 3.0555262565612793 | BCE Loss: 1.0215213298797607\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.095935821533203 | KNN Loss: 3.0962109565734863 | BCE Loss: 0.9997247457504272\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.074289798736572 | KNN Loss: 3.057690143585205 | BCE Loss: 1.0165997743606567\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.116010665893555 | KNN Loss: 3.09192156791687 | BCE Loss: 1.0240892171859741\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.064218521118164 | KNN Loss: 3.065743923187256 | BCE Loss: 0.9984747171401978\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.112328052520752 | KNN Loss: 3.0693976879119873 | BCE Loss: 1.0429303646087646\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.040036678314209 | KNN Loss: 3.0336267948150635 | BCE Loss: 1.0064098834991455\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.077553749084473 | KNN Loss: 3.0563104152679443 | BCE Loss: 1.0212430953979492\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.053654670715332 | KNN Loss: 3.0522329807281494 | BCE Loss: 1.0014219284057617\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.018962860107422 | KNN Loss: 3.0261764526367188 | BCE Loss: 0.9927866458892822\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.102609634399414 | KNN Loss: 3.0683038234710693 | BCE Loss: 1.0343060493469238\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.092779636383057 | KNN Loss: 3.0699734687805176 | BCE Loss: 1.022806167602539\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.114910125732422 | KNN Loss: 3.0728213787078857 | BCE Loss: 1.042088508605957\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.054445266723633 | KNN Loss: 3.030637741088867 | BCE Loss: 1.0238075256347656\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.069398403167725 | KNN Loss: 3.057755947113037 | BCE Loss: 1.0116424560546875\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.07047176361084 | KNN Loss: 3.0563743114471436 | BCE Loss: 1.0140974521636963\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.0612311363220215 | KNN Loss: 3.0427777767181396 | BCE Loss: 1.0184533596038818\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.050487518310547 | KNN Loss: 3.0449979305267334 | BCE Loss: 1.0054895877838135\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.1290435791015625 | KNN Loss: 3.097918748855591 | BCE Loss: 1.0311249494552612\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.042882919311523 | KNN Loss: 3.042876958847046 | BCE Loss: 1.0000061988830566\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.09328556060791 | KNN Loss: 3.061835527420044 | BCE Loss: 1.0314500331878662\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.068772792816162 | KNN Loss: 3.0499789714813232 | BCE Loss: 1.0187938213348389\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.0503997802734375 | KNN Loss: 3.0417001247406006 | BCE Loss: 1.0086994171142578\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.06733512878418 | KNN Loss: 3.039231061935425 | BCE Loss: 1.0281040668487549\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.084247589111328 | KNN Loss: 3.0732147693634033 | BCE Loss: 1.011033058166504\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.081823348999023 | KNN Loss: 3.0449745655059814 | BCE Loss: 1.036849021911621\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.096722602844238 | KNN Loss: 3.079775333404541 | BCE Loss: 1.0169473886489868\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.061281681060791 | KNN Loss: 3.0664007663726807 | BCE Loss: 0.9948809146881104\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.056159973144531 | KNN Loss: 3.0332045555114746 | BCE Loss: 1.0229554176330566\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.107575416564941 | KNN Loss: 3.081726312637329 | BCE Loss: 1.0258492231369019\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.093562126159668 | KNN Loss: 3.050691604614258 | BCE Loss: 1.0428705215454102\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.075430393218994 | KNN Loss: 3.0321359634399414 | BCE Loss: 1.0432945489883423\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.069411277770996 | KNN Loss: 3.0688912868499756 | BCE Loss: 1.0005199909210205\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.1185102462768555 | KNN Loss: 3.0635929107666016 | BCE Loss: 1.0549170970916748\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.085238933563232 | KNN Loss: 3.0696403980255127 | BCE Loss: 1.0155984163284302\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.0802998542785645 | KNN Loss: 3.0665369033813477 | BCE Loss: 1.0137630701065063\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.087047576904297 | KNN Loss: 3.0731070041656494 | BCE Loss: 1.0139403343200684\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.094289779663086 | KNN Loss: 3.076918840408325 | BCE Loss: 1.0173709392547607\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.074138641357422 | KNN Loss: 3.0542304515838623 | BCE Loss: 1.0199079513549805\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.109307289123535 | KNN Loss: 3.0704119205474854 | BCE Loss: 1.0388953685760498\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.095770835876465 | KNN Loss: 3.0678319931030273 | BCE Loss: 1.0279390811920166\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.059652328491211 | KNN Loss: 3.0409371852874756 | BCE Loss: 1.0187153816223145\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.093822002410889 | KNN Loss: 3.072516441345215 | BCE Loss: 1.0213054418563843\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.076147079467773 | KNN Loss: 3.060702085494995 | BCE Loss: 1.0154451131820679\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.055277347564697 | KNN Loss: 3.04060697555542 | BCE Loss: 1.0146703720092773\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.073691368103027 | KNN Loss: 3.0558273792266846 | BCE Loss: 1.0178637504577637\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.052188873291016 | KNN Loss: 3.037391424179077 | BCE Loss: 1.0147974491119385\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.031529903411865 | KNN Loss: 3.0303471088409424 | BCE Loss: 1.0011827945709229\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.066670894622803 | KNN Loss: 3.044515371322632 | BCE Loss: 1.0221554040908813\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.077287197113037 | KNN Loss: 3.0666422843933105 | BCE Loss: 1.010644793510437\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.081265449523926 | KNN Loss: 3.078364610671997 | BCE Loss: 1.0029007196426392\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.106927394866943 | KNN Loss: 3.070326089859009 | BCE Loss: 1.0366013050079346\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.043821811676025 | KNN Loss: 3.036259651184082 | BCE Loss: 1.0075621604919434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.09181022644043 | KNN Loss: 3.063035249710083 | BCE Loss: 1.0287747383117676\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.050440788269043 | KNN Loss: 3.049584150314331 | BCE Loss: 1.0008567571640015\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.089094638824463 | KNN Loss: 3.0801994800567627 | BCE Loss: 1.0088951587677002\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.087156295776367 | KNN Loss: 3.0584588050842285 | BCE Loss: 1.0286972522735596\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.084129333496094 | KNN Loss: 3.0673892498016357 | BCE Loss: 1.016740322113037\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.062153339385986 | KNN Loss: 3.047149181365967 | BCE Loss: 1.0150041580200195\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.113218307495117 | KNN Loss: 3.107050657272339 | BCE Loss: 1.0061675310134888\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.04009485244751 | KNN Loss: 3.0605435371398926 | BCE Loss: 0.9795511960983276\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.077237129211426 | KNN Loss: 3.034209966659546 | BCE Loss: 1.0430272817611694\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.096449851989746 | KNN Loss: 3.0717906951904297 | BCE Loss: 1.0246593952178955\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.118378639221191 | KNN Loss: 3.0859572887420654 | BCE Loss: 1.0324214696884155\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.093484878540039 | KNN Loss: 3.087984085083008 | BCE Loss: 1.0055010318756104\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.132449150085449 | KNN Loss: 3.0936408042907715 | BCE Loss: 1.0388085842132568\n",
      "Epoch   366: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.0841546058654785 | KNN Loss: 3.083580493927002 | BCE Loss: 1.0005741119384766\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.077773094177246 | KNN Loss: 3.0519423484802246 | BCE Loss: 1.0258305072784424\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.096345901489258 | KNN Loss: 3.072417736053467 | BCE Loss: 1.0239284038543701\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.045171737670898 | KNN Loss: 3.045229434967041 | BCE Loss: 0.9999423027038574\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.065176963806152 | KNN Loss: 3.047354221343994 | BCE Loss: 1.0178226232528687\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.104384899139404 | KNN Loss: 3.087512493133545 | BCE Loss: 1.0168722867965698\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.077188014984131 | KNN Loss: 3.0577330589294434 | BCE Loss: 1.0194549560546875\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.055619239807129 | KNN Loss: 3.0381388664245605 | BCE Loss: 1.017480492591858\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.076633453369141 | KNN Loss: 3.033695697784424 | BCE Loss: 1.042937994003296\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.082880020141602 | KNN Loss: 3.0695927143096924 | BCE Loss: 1.0132875442504883\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.090322017669678 | KNN Loss: 3.042358636856079 | BCE Loss: 1.047963261604309\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.126614570617676 | KNN Loss: 3.0834920406341553 | BCE Loss: 1.0431222915649414\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.116824150085449 | KNN Loss: 3.085153579711914 | BCE Loss: 1.031670331954956\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.049419403076172 | KNN Loss: 3.0373330116271973 | BCE Loss: 1.0120861530303955\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.089557647705078 | KNN Loss: 3.0628719329833984 | BCE Loss: 1.0266857147216797\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.092635631561279 | KNN Loss: 3.075671911239624 | BCE Loss: 1.0169637203216553\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.0435590744018555 | KNN Loss: 3.0397329330444336 | BCE Loss: 1.003826379776001\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.0854291915893555 | KNN Loss: 3.0950441360473633 | BCE Loss: 0.9903850555419922\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.098811149597168 | KNN Loss: 3.063209295272827 | BCE Loss: 1.0356018543243408\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.1285400390625 | KNN Loss: 3.083184242248535 | BCE Loss: 1.0453559160232544\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.051645755767822 | KNN Loss: 3.058018922805786 | BCE Loss: 0.9936269521713257\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.095894813537598 | KNN Loss: 3.101313352584839 | BCE Loss: 0.9945813417434692\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.076715469360352 | KNN Loss: 3.0705626010894775 | BCE Loss: 1.0061529874801636\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.081101417541504 | KNN Loss: 3.0473930835723877 | BCE Loss: 1.0337085723876953\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.083943843841553 | KNN Loss: 3.057990550994873 | BCE Loss: 1.0259532928466797\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.080052375793457 | KNN Loss: 3.085871696472168 | BCE Loss: 0.9941806793212891\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.0940446853637695 | KNN Loss: 3.0695927143096924 | BCE Loss: 1.024451732635498\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.099632740020752 | KNN Loss: 3.0664210319519043 | BCE Loss: 1.0332117080688477\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.018693923950195 | KNN Loss: 3.0346808433532715 | BCE Loss: 0.9840131998062134\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.087713241577148 | KNN Loss: 3.0590999126434326 | BCE Loss: 1.028613567352295\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.063671588897705 | KNN Loss: 3.0691912174224854 | BCE Loss: 0.9944803714752197\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.1295671463012695 | KNN Loss: 3.099175214767456 | BCE Loss: 1.0303921699523926\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.086090087890625 | KNN Loss: 3.0491786003112793 | BCE Loss: 1.0369112491607666\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.04656457901001 | KNN Loss: 3.0564868450164795 | BCE Loss: 0.9900776147842407\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.136787414550781 | KNN Loss: 3.0868425369262695 | BCE Loss: 1.0499446392059326\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.0527801513671875 | KNN Loss: 3.041966676712036 | BCE Loss: 1.0108132362365723\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.092744827270508 | KNN Loss: 3.0764310359954834 | BCE Loss: 1.0163137912750244\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.055326461791992 | KNN Loss: 3.0510785579681396 | BCE Loss: 1.004248023033142\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.053414821624756 | KNN Loss: 3.0602047443389893 | BCE Loss: 0.993209958076477\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.030281066894531 | KNN Loss: 3.0223135948181152 | BCE Loss: 1.0079673528671265\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.08298397064209 | KNN Loss: 3.056914806365967 | BCE Loss: 1.0260694026947021\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.111701965332031 | KNN Loss: 3.046983242034912 | BCE Loss: 1.0647187232971191\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.0632123947143555 | KNN Loss: 3.0320770740509033 | BCE Loss: 1.0311355590820312\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.065691947937012 | KNN Loss: 3.065358877182007 | BCE Loss: 1.0003328323364258\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.101874351501465 | KNN Loss: 3.067582845687866 | BCE Loss: 1.0342915058135986\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.1440582275390625 | KNN Loss: 3.100882053375244 | BCE Loss: 1.0431761741638184\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.117488861083984 | KNN Loss: 3.0770347118377686 | BCE Loss: 1.0404539108276367\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.091571807861328 | KNN Loss: 3.0667505264282227 | BCE Loss: 1.0248212814331055\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.075823783874512 | KNN Loss: 3.062471628189087 | BCE Loss: 1.0133521556854248\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.06175422668457 | KNN Loss: 3.0260632038116455 | BCE Loss: 1.0356907844543457\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.109439373016357 | KNN Loss: 3.0812435150146484 | BCE Loss: 1.028195858001709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.126917839050293 | KNN Loss: 3.1130356788635254 | BCE Loss: 1.0138819217681885\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.104763031005859 | KNN Loss: 3.078443765640259 | BCE Loss: 1.0263195037841797\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.115032196044922 | KNN Loss: 3.07304310798645 | BCE Loss: 1.0419893264770508\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.061326026916504 | KNN Loss: 3.0720436573028564 | BCE Loss: 0.989282488822937\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.078152656555176 | KNN Loss: 3.0504777431488037 | BCE Loss: 1.027674913406372\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.116270065307617 | KNN Loss: 3.0732333660125732 | BCE Loss: 1.0430364608764648\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.064121246337891 | KNN Loss: 3.045297145843506 | BCE Loss: 1.0188243389129639\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.106471061706543 | KNN Loss: 3.063218832015991 | BCE Loss: 1.0432522296905518\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.090730667114258 | KNN Loss: 3.0616118907928467 | BCE Loss: 1.0291186571121216\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.10064697265625 | KNN Loss: 3.086435079574585 | BCE Loss: 1.0142121315002441\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.078676700592041 | KNN Loss: 3.045639991760254 | BCE Loss: 1.0330368280410767\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.059220790863037 | KNN Loss: 3.0298635959625244 | BCE Loss: 1.0293571949005127\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.0599470138549805 | KNN Loss: 3.0445659160614014 | BCE Loss: 1.0153809785842896\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.050874710083008 | KNN Loss: 3.0428519248962402 | BCE Loss: 1.0080230236053467\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.093532085418701 | KNN Loss: 3.060441493988037 | BCE Loss: 1.033090591430664\n",
      "Epoch   377: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.1002349853515625 | KNN Loss: 3.062330722808838 | BCE Loss: 1.0379040241241455\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.0702619552612305 | KNN Loss: 3.0709168910980225 | BCE Loss: 0.999345064163208\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.061426162719727 | KNN Loss: 3.0579586029052734 | BCE Loss: 1.0034675598144531\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.057396411895752 | KNN Loss: 3.065509557723999 | BCE Loss: 0.9918868541717529\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.144047737121582 | KNN Loss: 3.100459575653076 | BCE Loss: 1.0435882806777954\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.07584810256958 | KNN Loss: 3.0788114070892334 | BCE Loss: 0.9970365762710571\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.119790554046631 | KNN Loss: 3.0933525562286377 | BCE Loss: 1.0264379978179932\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.116123676300049 | KNN Loss: 3.0898547172546387 | BCE Loss: 1.0262689590454102\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.103682994842529 | KNN Loss: 3.0396203994750977 | BCE Loss: 1.0640627145767212\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.075079441070557 | KNN Loss: 3.0687062740325928 | BCE Loss: 1.0063730478286743\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.0888824462890625 | KNN Loss: 3.058831214904785 | BCE Loss: 1.0300514698028564\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.083426475524902 | KNN Loss: 3.0572879314422607 | BCE Loss: 1.026138424873352\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.073920726776123 | KNN Loss: 3.0790326595306396 | BCE Loss: 0.994888186454773\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.058513164520264 | KNN Loss: 3.0508177280426025 | BCE Loss: 1.0076953172683716\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.087664604187012 | KNN Loss: 3.049677848815918 | BCE Loss: 1.0379866361618042\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.1188483238220215 | KNN Loss: 3.130375862121582 | BCE Loss: 0.9884726405143738\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.0834059715271 | KNN Loss: 3.0647542476654053 | BCE Loss: 1.0186517238616943\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.0320258140563965 | KNN Loss: 3.020587682723999 | BCE Loss: 1.011438250541687\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.08642053604126 | KNN Loss: 3.075516939163208 | BCE Loss: 1.0109037160873413\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.09959602355957 | KNN Loss: 3.0874080657958984 | BCE Loss: 1.012188196182251\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.095083713531494 | KNN Loss: 3.0645456314086914 | BCE Loss: 1.0305379629135132\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.0333571434021 | KNN Loss: 3.0339407920837402 | BCE Loss: 0.9994163513183594\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.071808815002441 | KNN Loss: 3.0358073711395264 | BCE Loss: 1.0360015630722046\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.097868919372559 | KNN Loss: 3.070894479751587 | BCE Loss: 1.0269746780395508\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.054399490356445 | KNN Loss: 3.0429980754852295 | BCE Loss: 1.0114012956619263\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.075748443603516 | KNN Loss: 3.0578267574310303 | BCE Loss: 1.0179214477539062\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.081140518188477 | KNN Loss: 3.091468334197998 | BCE Loss: 0.9896721243858337\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.110766410827637 | KNN Loss: 3.0495426654815674 | BCE Loss: 1.0612236261367798\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.068269729614258 | KNN Loss: 3.0465199947357178 | BCE Loss: 1.0217498540878296\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.081460475921631 | KNN Loss: 3.087512254714966 | BCE Loss: 0.9939483404159546\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.117705821990967 | KNN Loss: 3.0607733726501465 | BCE Loss: 1.0569324493408203\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.102231025695801 | KNN Loss: 3.083664655685425 | BCE Loss: 1.018566370010376\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.112496376037598 | KNN Loss: 3.094966173171997 | BCE Loss: 1.0175302028656006\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.087874412536621 | KNN Loss: 3.0718159675598145 | BCE Loss: 1.0160584449768066\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.079984188079834 | KNN Loss: 3.07743239402771 | BCE Loss: 1.002551794052124\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.080318450927734 | KNN Loss: 3.0635323524475098 | BCE Loss: 1.0167863368988037\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.1124444007873535 | KNN Loss: 3.0835933685302734 | BCE Loss: 1.02885103225708\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.098723411560059 | KNN Loss: 3.0549323558807373 | BCE Loss: 1.0437909364700317\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.081150054931641 | KNN Loss: 3.076552152633667 | BCE Loss: 1.0045976638793945\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.101025104522705 | KNN Loss: 3.10819673538208 | BCE Loss: 0.9928282499313354\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.074553489685059 | KNN Loss: 3.0264995098114014 | BCE Loss: 1.0480538606643677\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.084949016571045 | KNN Loss: 3.06144642829895 | BCE Loss: 1.0235025882720947\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.080613136291504 | KNN Loss: 3.062566041946411 | BCE Loss: 1.0180468559265137\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.041009902954102 | KNN Loss: 3.0305111408233643 | BCE Loss: 1.0104985237121582\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.069458484649658 | KNN Loss: 3.0796573162078857 | BCE Loss: 0.9898010492324829\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.042519569396973 | KNN Loss: 3.0437276363372803 | BCE Loss: 0.9987920522689819\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.070551872253418 | KNN Loss: 3.045454978942871 | BCE Loss: 1.0250968933105469\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.105262756347656 | KNN Loss: 3.0564072132110596 | BCE Loss: 1.0488557815551758\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.132236957550049 | KNN Loss: 3.0913705825805664 | BCE Loss: 1.0408662557601929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.129976272583008 | KNN Loss: 3.1048688888549805 | BCE Loss: 1.0251073837280273\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.064586639404297 | KNN Loss: 3.055338144302368 | BCE Loss: 1.0092484951019287\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.046426296234131 | KNN Loss: 3.042177200317383 | BCE Loss: 1.0042489767074585\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.09766960144043 | KNN Loss: 3.0776567459106445 | BCE Loss: 1.0200127363204956\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.068721771240234 | KNN Loss: 3.0444581508636475 | BCE Loss: 1.024263858795166\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.124364376068115 | KNN Loss: 3.0850038528442383 | BCE Loss: 1.039360523223877\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.096399307250977 | KNN Loss: 3.065905809402466 | BCE Loss: 1.0304932594299316\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.106784343719482 | KNN Loss: 3.0762219429016113 | BCE Loss: 1.0305622816085815\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.100414276123047 | KNN Loss: 3.066403388977051 | BCE Loss: 1.034010887145996\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.0660200119018555 | KNN Loss: 3.048872470855713 | BCE Loss: 1.0171476602554321\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.080367565155029 | KNN Loss: 3.0863120555877686 | BCE Loss: 0.9940553903579712\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.064154624938965 | KNN Loss: 3.055119276046753 | BCE Loss: 1.009035587310791\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.066488265991211 | KNN Loss: 3.056755542755127 | BCE Loss: 1.0097328424453735\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.114213943481445 | KNN Loss: 3.0802600383758545 | BCE Loss: 1.0339537858963013\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.083151340484619 | KNN Loss: 3.0657641887664795 | BCE Loss: 1.01738703250885\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.072923183441162 | KNN Loss: 3.03082013130188 | BCE Loss: 1.0421029329299927\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.064265251159668 | KNN Loss: 3.0455498695373535 | BCE Loss: 1.0187156200408936\n",
      "Epoch   388: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.100571155548096 | KNN Loss: 3.0884673595428467 | BCE Loss: 1.012103796005249\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.057779788970947 | KNN Loss: 3.063748598098755 | BCE Loss: 0.9940312504768372\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.088604927062988 | KNN Loss: 3.0670275688171387 | BCE Loss: 1.0215771198272705\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.093044757843018 | KNN Loss: 3.094329357147217 | BCE Loss: 0.998715341091156\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.054740905761719 | KNN Loss: 3.042267322540283 | BCE Loss: 1.0124733448028564\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.097423076629639 | KNN Loss: 3.077733039855957 | BCE Loss: 1.019689917564392\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.066514015197754 | KNN Loss: 3.059650182723999 | BCE Loss: 1.006864070892334\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.107521057128906 | KNN Loss: 3.067343235015869 | BCE Loss: 1.040177583694458\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.074241638183594 | KNN Loss: 3.0518746376037598 | BCE Loss: 1.0223667621612549\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.057147026062012 | KNN Loss: 3.050102949142456 | BCE Loss: 1.0070440769195557\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.046354293823242 | KNN Loss: 3.0389866828918457 | BCE Loss: 1.007367491722107\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.095992088317871 | KNN Loss: 3.0330774784088135 | BCE Loss: 1.0629143714904785\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.079898834228516 | KNN Loss: 3.0768439769744873 | BCE Loss: 1.0030546188354492\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.032467842102051 | KNN Loss: 3.0227010250091553 | BCE Loss: 1.0097670555114746\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.090057373046875 | KNN Loss: 3.053738832473755 | BCE Loss: 1.036318302154541\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.070394515991211 | KNN Loss: 3.065007448196411 | BCE Loss: 1.0053869485855103\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.084863185882568 | KNN Loss: 3.0760512351989746 | BCE Loss: 1.0088119506835938\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.089170455932617 | KNN Loss: 3.045062780380249 | BCE Loss: 1.0441077947616577\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.084651947021484 | KNN Loss: 3.048079490661621 | BCE Loss: 1.0365722179412842\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.05662727355957 | KNN Loss: 3.043879508972168 | BCE Loss: 1.0127475261688232\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.0966477394104 | KNN Loss: 3.0602781772613525 | BCE Loss: 1.0363694429397583\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.107407569885254 | KNN Loss: 3.0916969776153564 | BCE Loss: 1.0157105922698975\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.0973687171936035 | KNN Loss: 3.0816142559051514 | BCE Loss: 1.0157544612884521\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.057415962219238 | KNN Loss: 3.047133207321167 | BCE Loss: 1.0102829933166504\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.050330638885498 | KNN Loss: 3.0454564094543457 | BCE Loss: 1.0048741102218628\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.098954200744629 | KNN Loss: 3.0948867797851562 | BCE Loss: 1.0040675401687622\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.074827194213867 | KNN Loss: 3.0476462841033936 | BCE Loss: 1.027180790901184\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.09832763671875 | KNN Loss: 3.0657081604003906 | BCE Loss: 1.0326194763183594\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.1142144203186035 | KNN Loss: 3.0635693073272705 | BCE Loss: 1.0506449937820435\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.107254981994629 | KNN Loss: 3.067405939102173 | BCE Loss: 1.0398492813110352\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.0890045166015625 | KNN Loss: 3.079291820526123 | BCE Loss: 1.0097126960754395\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.161900520324707 | KNN Loss: 3.111701488494873 | BCE Loss: 1.0501987934112549\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.134108543395996 | KNN Loss: 3.0968568325042725 | BCE Loss: 1.0372519493103027\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.059759140014648 | KNN Loss: 3.0457725524902344 | BCE Loss: 1.013986587524414\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.129261016845703 | KNN Loss: 3.097707986831665 | BCE Loss: 1.0315532684326172\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.078530311584473 | KNN Loss: 3.068432092666626 | BCE Loss: 1.0100983381271362\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.1170430183410645 | KNN Loss: 3.086826801300049 | BCE Loss: 1.0302162170410156\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.080145835876465 | KNN Loss: 3.0520741939544678 | BCE Loss: 1.0280718803405762\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.061809539794922 | KNN Loss: 3.0550782680511475 | BCE Loss: 1.0067312717437744\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.057559967041016 | KNN Loss: 3.056688070297241 | BCE Loss: 1.0008721351623535\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.0885539054870605 | KNN Loss: 3.0733931064605713 | BCE Loss: 1.0151606798171997\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.05739688873291 | KNN Loss: 3.0533764362335205 | BCE Loss: 1.0040204524993896\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.109424591064453 | KNN Loss: 3.0783021450042725 | BCE Loss: 1.0311224460601807\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.058302402496338 | KNN Loss: 3.034128189086914 | BCE Loss: 1.0241743326187134\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.071300506591797 | KNN Loss: 3.060051918029785 | BCE Loss: 1.0112484693527222\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.046464443206787 | KNN Loss: 3.044290781021118 | BCE Loss: 1.0021735429763794\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.122027397155762 | KNN Loss: 3.0971243381500244 | BCE Loss: 1.0249030590057373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.082831382751465 | KNN Loss: 3.0514848232269287 | BCE Loss: 1.0313466787338257\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.0340423583984375 | KNN Loss: 3.0247106552124023 | BCE Loss: 1.0093317031860352\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.067949295043945 | KNN Loss: 3.0632834434509277 | BCE Loss: 1.0046658515930176\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.078846454620361 | KNN Loss: 3.0822558403015137 | BCE Loss: 0.9965904355049133\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.064353942871094 | KNN Loss: 3.0487091541290283 | BCE Loss: 1.0156450271606445\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.141764163970947 | KNN Loss: 3.0865445137023926 | BCE Loss: 1.0552196502685547\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.078712463378906 | KNN Loss: 3.0742363929748535 | BCE Loss: 1.0044763088226318\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.101964473724365 | KNN Loss: 3.083935260772705 | BCE Loss: 1.0180292129516602\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.069404602050781 | KNN Loss: 3.0586414337158203 | BCE Loss: 1.0107632875442505\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.0702948570251465 | KNN Loss: 3.0914976596832275 | BCE Loss: 0.9787973165512085\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.0529303550720215 | KNN Loss: 3.0567405223846436 | BCE Loss: 0.9961897134780884\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.110841751098633 | KNN Loss: 3.059349775314331 | BCE Loss: 1.0514920949935913\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.062986373901367 | KNN Loss: 3.057051181793213 | BCE Loss: 1.0059350728988647\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.088318347930908 | KNN Loss: 3.0679306983947754 | BCE Loss: 1.0203877687454224\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.042303085327148 | KNN Loss: 3.041472911834717 | BCE Loss: 1.0008299350738525\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.043951034545898 | KNN Loss: 3.0638983249664307 | BCE Loss: 0.9800524711608887\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.057440757751465 | KNN Loss: 3.032632827758789 | BCE Loss: 1.0248076915740967\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.107796669006348 | KNN Loss: 3.0965116024017334 | BCE Loss: 1.0112850666046143\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.100895881652832 | KNN Loss: 3.1113319396972656 | BCE Loss: 0.9895641207695007\n",
      "Epoch   399: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.1052165031433105 | KNN Loss: 3.079270601272583 | BCE Loss: 1.025946021080017\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.085748195648193 | KNN Loss: 3.0796735286712646 | BCE Loss: 1.0060747861862183\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.071962356567383 | KNN Loss: 3.0554444789886475 | BCE Loss: 1.0165181159973145\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.090306282043457 | KNN Loss: 3.0708582401275635 | BCE Loss: 1.0194480419158936\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.101495265960693 | KNN Loss: 3.048252582550049 | BCE Loss: 1.053242802619934\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.0777997970581055 | KNN Loss: 3.071812391281128 | BCE Loss: 1.0059876441955566\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.101501941680908 | KNN Loss: 3.081488847732544 | BCE Loss: 1.0200130939483643\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.129975318908691 | KNN Loss: 3.1026837825775146 | BCE Loss: 1.0272917747497559\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.051115989685059 | KNN Loss: 3.040663957595825 | BCE Loss: 1.0104519128799438\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.071906089782715 | KNN Loss: 3.0521373748779297 | BCE Loss: 1.0197687149047852\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.125925540924072 | KNN Loss: 3.125013828277588 | BCE Loss: 1.0009115934371948\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.124556541442871 | KNN Loss: 3.0682287216186523 | BCE Loss: 1.0563275814056396\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.0526204109191895 | KNN Loss: 3.0472426414489746 | BCE Loss: 1.0053776502609253\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.08297061920166 | KNN Loss: 3.051917314529419 | BCE Loss: 1.0310531854629517\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.10830020904541 | KNN Loss: 3.074019432067871 | BCE Loss: 1.034280776977539\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.100569725036621 | KNN Loss: 3.0736401081085205 | BCE Loss: 1.0269293785095215\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.069883346557617 | KNN Loss: 3.058133602142334 | BCE Loss: 1.011749505996704\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.044233322143555 | KNN Loss: 3.045961618423462 | BCE Loss: 0.9982717633247375\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.060788154602051 | KNN Loss: 3.056027412414551 | BCE Loss: 1.0047607421875\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.064939975738525 | KNN Loss: 3.0827383995056152 | BCE Loss: 0.9822016954421997\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.067307472229004 | KNN Loss: 3.0641531944274902 | BCE Loss: 1.0031545162200928\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.087761878967285 | KNN Loss: 3.072413921356201 | BCE Loss: 1.015348196029663\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.070918083190918 | KNN Loss: 3.06002140045166 | BCE Loss: 1.010896921157837\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.055761337280273 | KNN Loss: 3.019090414047241 | BCE Loss: 1.0366709232330322\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.071542739868164 | KNN Loss: 3.0564370155334473 | BCE Loss: 1.0151054859161377\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.057218074798584 | KNN Loss: 3.0660934448242188 | BCE Loss: 0.99112468957901\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.12600040435791 | KNN Loss: 3.088963031768799 | BCE Loss: 1.0370371341705322\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.099324703216553 | KNN Loss: 3.060068130493164 | BCE Loss: 1.0392566919326782\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.103006839752197 | KNN Loss: 3.0791773796081543 | BCE Loss: 1.0238293409347534\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.117065906524658 | KNN Loss: 3.077486038208008 | BCE Loss: 1.03957998752594\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.12752103805542 | KNN Loss: 3.078843116760254 | BCE Loss: 1.048677921295166\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.105450630187988 | KNN Loss: 3.0737388134002686 | BCE Loss: 1.0317115783691406\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.063086032867432 | KNN Loss: 3.0292716026306152 | BCE Loss: 1.0338144302368164\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.099864482879639 | KNN Loss: 3.0971438884735107 | BCE Loss: 1.0027204751968384\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.135759353637695 | KNN Loss: 3.1547396183013916 | BCE Loss: 0.9810196161270142\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.132662773132324 | KNN Loss: 3.1036808490753174 | BCE Loss: 1.028982162475586\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.064579010009766 | KNN Loss: 3.031899929046631 | BCE Loss: 1.0326788425445557\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.092609405517578 | KNN Loss: 3.0714938640594482 | BCE Loss: 1.021115779876709\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.08905029296875 | KNN Loss: 3.0663065910339355 | BCE Loss: 1.0227439403533936\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.083540439605713 | KNN Loss: 3.0668559074401855 | BCE Loss: 1.0166844129562378\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.096591949462891 | KNN Loss: 3.0888419151306152 | BCE Loss: 1.0077502727508545\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.110827445983887 | KNN Loss: 3.048330307006836 | BCE Loss: 1.0624969005584717\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.069777965545654 | KNN Loss: 3.069944143295288 | BCE Loss: 0.9998339414596558\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.139870643615723 | KNN Loss: 3.10676908493042 | BCE Loss: 1.0331016778945923\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.0717854499816895 | KNN Loss: 3.058487892150879 | BCE Loss: 1.0132976770401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.090633869171143 | KNN Loss: 3.1064648628234863 | BCE Loss: 0.9841690063476562\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.092097282409668 | KNN Loss: 3.066734790802002 | BCE Loss: 1.0253627300262451\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.071992874145508 | KNN Loss: 3.0668017864227295 | BCE Loss: 1.0051910877227783\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.057834148406982 | KNN Loss: 3.0643906593322754 | BCE Loss: 0.9934436082839966\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.059449195861816 | KNN Loss: 3.0701775550842285 | BCE Loss: 0.9892714023590088\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.111280918121338 | KNN Loss: 3.0892813205718994 | BCE Loss: 1.021999478340149\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.080898761749268 | KNN Loss: 3.0628952980041504 | BCE Loss: 1.0180034637451172\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.090489387512207 | KNN Loss: 3.06177020072937 | BCE Loss: 1.0287190675735474\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.049799919128418 | KNN Loss: 3.0339741706848145 | BCE Loss: 1.0158259868621826\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.074530601501465 | KNN Loss: 3.0453052520751953 | BCE Loss: 1.0292253494262695\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.095885276794434 | KNN Loss: 3.0770161151885986 | BCE Loss: 1.018869161605835\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.099421501159668 | KNN Loss: 3.0936739444732666 | BCE Loss: 1.0057474374771118\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.066570281982422 | KNN Loss: 3.057077169418335 | BCE Loss: 1.009493350982666\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.08138370513916 | KNN Loss: 3.0790855884552 | BCE Loss: 1.00229811668396\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.061307907104492 | KNN Loss: 3.058055877685547 | BCE Loss: 1.0032517910003662\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.0680694580078125 | KNN Loss: 3.0767157077789307 | BCE Loss: 0.9913535714149475\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.061900615692139 | KNN Loss: 3.030860185623169 | BCE Loss: 1.0310404300689697\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.073143482208252 | KNN Loss: 3.0528335571289062 | BCE Loss: 1.0203100442886353\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.076767921447754 | KNN Loss: 3.0568859577178955 | BCE Loss: 1.0198818445205688\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.096112251281738 | KNN Loss: 3.065419912338257 | BCE Loss: 1.030692458152771\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.098153114318848 | KNN Loss: 3.091749429702759 | BCE Loss: 1.0064034461975098\n",
      "Epoch   410: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.093025207519531 | KNN Loss: 3.063594102859497 | BCE Loss: 1.0294309854507446\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.067315101623535 | KNN Loss: 3.079139232635498 | BCE Loss: 0.9881758689880371\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.052274703979492 | KNN Loss: 3.03548526763916 | BCE Loss: 1.0167896747589111\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.085078716278076 | KNN Loss: 3.077101469039917 | BCE Loss: 1.0079773664474487\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.1249189376831055 | KNN Loss: 3.0826003551483154 | BCE Loss: 1.042318344116211\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.108400821685791 | KNN Loss: 3.079911708831787 | BCE Loss: 1.028489112854004\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.071616172790527 | KNN Loss: 3.0482900142669678 | BCE Loss: 1.0233261585235596\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.106788635253906 | KNN Loss: 3.074985980987549 | BCE Loss: 1.0318028926849365\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.112720489501953 | KNN Loss: 3.0861427783966064 | BCE Loss: 1.0265778303146362\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.10247802734375 | KNN Loss: 3.0709691047668457 | BCE Loss: 1.0315086841583252\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.042076110839844 | KNN Loss: 3.039423942565918 | BCE Loss: 1.0026522874832153\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.061399459838867 | KNN Loss: 3.0450022220611572 | BCE Loss: 1.0163969993591309\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.090161323547363 | KNN Loss: 3.0624501705169678 | BCE Loss: 1.0277113914489746\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.121768951416016 | KNN Loss: 3.0927677154541016 | BCE Loss: 1.0290013551712036\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.159032821655273 | KNN Loss: 3.11104154586792 | BCE Loss: 1.0479912757873535\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.075262069702148 | KNN Loss: 3.075719118118286 | BCE Loss: 0.9995431900024414\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.152307987213135 | KNN Loss: 3.0954387187957764 | BCE Loss: 1.0568692684173584\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.114378929138184 | KNN Loss: 3.071073532104492 | BCE Loss: 1.0433053970336914\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.062346935272217 | KNN Loss: 3.053420066833496 | BCE Loss: 1.0089267492294312\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.118788242340088 | KNN Loss: 3.084670066833496 | BCE Loss: 1.0341182947158813\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.048357009887695 | KNN Loss: 3.0459272861480713 | BCE Loss: 1.002429485321045\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.09077262878418 | KNN Loss: 3.065467119216919 | BCE Loss: 1.0253052711486816\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.0732269287109375 | KNN Loss: 3.069441556930542 | BCE Loss: 1.0037853717803955\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.082173824310303 | KNN Loss: 3.0652287006378174 | BCE Loss: 1.016945242881775\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.088625907897949 | KNN Loss: 3.0796072483062744 | BCE Loss: 1.0090184211730957\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.076787948608398 | KNN Loss: 3.053790330886841 | BCE Loss: 1.0229978561401367\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.06110954284668 | KNN Loss: 3.0535430908203125 | BCE Loss: 1.0075666904449463\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.0759711265563965 | KNN Loss: 3.04829478263855 | BCE Loss: 1.0276763439178467\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.085825443267822 | KNN Loss: 3.0960144996643066 | BCE Loss: 0.9898110628128052\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.140877723693848 | KNN Loss: 3.0942394733428955 | BCE Loss: 1.0466384887695312\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.081093788146973 | KNN Loss: 3.0836901664733887 | BCE Loss: 0.9974038600921631\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.104780197143555 | KNN Loss: 3.082718849182129 | BCE Loss: 1.0220612287521362\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.097409725189209 | KNN Loss: 3.0832364559173584 | BCE Loss: 1.0141733884811401\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.085287094116211 | KNN Loss: 3.064393997192383 | BCE Loss: 1.0208933353424072\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.098480224609375 | KNN Loss: 3.0671603679656982 | BCE Loss: 1.0313196182250977\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.082562446594238 | KNN Loss: 3.067641019821167 | BCE Loss: 1.0149213075637817\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.089588642120361 | KNN Loss: 3.093311309814453 | BCE Loss: 0.9962773323059082\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.01115083694458 | KNN Loss: 3.020843267440796 | BCE Loss: 0.9903076887130737\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.114123344421387 | KNN Loss: 3.065791368484497 | BCE Loss: 1.0483320951461792\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.043292999267578 | KNN Loss: 3.0512475967407227 | BCE Loss: 0.9920451641082764\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.0675225257873535 | KNN Loss: 3.0344464778900146 | BCE Loss: 1.0330761671066284\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.084252834320068 | KNN Loss: 3.0453264713287354 | BCE Loss: 1.038926362991333\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.085824966430664 | KNN Loss: 3.0891783237457275 | BCE Loss: 0.9966468811035156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.050157070159912 | KNN Loss: 3.0386476516723633 | BCE Loss: 1.0115095376968384\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.112070083618164 | KNN Loss: 3.088057518005371 | BCE Loss: 1.0240126848220825\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.081592559814453 | KNN Loss: 3.0780200958251953 | BCE Loss: 1.0035722255706787\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.071453094482422 | KNN Loss: 3.0735833644866943 | BCE Loss: 0.9978697299957275\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.114165306091309 | KNN Loss: 3.045379161834717 | BCE Loss: 1.0687860250473022\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.12565803527832 | KNN Loss: 3.118760824203491 | BCE Loss: 1.0068974494934082\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.043552875518799 | KNN Loss: 3.0345213413238525 | BCE Loss: 1.0090315341949463\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.083132743835449 | KNN Loss: 3.0734004974365234 | BCE Loss: 1.0097322463989258\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.055978775024414 | KNN Loss: 3.047877073287964 | BCE Loss: 1.0081017017364502\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.12861967086792 | KNN Loss: 3.077070474624634 | BCE Loss: 1.0515491962432861\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.069136619567871 | KNN Loss: 3.0492453575134277 | BCE Loss: 1.0198915004730225\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.093857765197754 | KNN Loss: 3.0735549926757812 | BCE Loss: 1.0203030109405518\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.0375075340271 | KNN Loss: 3.055832624435425 | BCE Loss: 0.9816749095916748\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.109763145446777 | KNN Loss: 3.0780885219573975 | BCE Loss: 1.0316746234893799\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.044354438781738 | KNN Loss: 3.0642588138580322 | BCE Loss: 0.980095624923706\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.074959754943848 | KNN Loss: 3.0469107627868652 | BCE Loss: 1.0280487537384033\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.058813095092773 | KNN Loss: 3.052833080291748 | BCE Loss: 1.005980134010315\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.052619457244873 | KNN Loss: 3.0489487648010254 | BCE Loss: 1.0036706924438477\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.051161289215088 | KNN Loss: 3.067258358001709 | BCE Loss: 0.9839030504226685\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.13435173034668 | KNN Loss: 3.1153318881988525 | BCE Loss: 1.0190197229385376\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.092071056365967 | KNN Loss: 3.054626226425171 | BCE Loss: 1.037444829940796\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.068056106567383 | KNN Loss: 3.041252851486206 | BCE Loss: 1.0268033742904663\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.062716484069824 | KNN Loss: 3.049123764038086 | BCE Loss: 1.0135924816131592\n",
      "Epoch   421: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.08610725402832 | KNN Loss: 3.079408884048462 | BCE Loss: 1.0066986083984375\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.1404523849487305 | KNN Loss: 3.086622476577759 | BCE Loss: 1.0538300275802612\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.082275390625 | KNN Loss: 3.076162099838257 | BCE Loss: 1.006113052368164\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.062150001525879 | KNN Loss: 3.0380890369415283 | BCE Loss: 1.0240607261657715\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.0751776695251465 | KNN Loss: 3.079352378845215 | BCE Loss: 0.9958252906799316\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.085132598876953 | KNN Loss: 3.0960447788238525 | BCE Loss: 0.9890875816345215\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.078516483306885 | KNN Loss: 3.0743610858917236 | BCE Loss: 1.0041555166244507\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.116885662078857 | KNN Loss: 3.1045165061950684 | BCE Loss: 1.012369155883789\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.087127685546875 | KNN Loss: 3.0655317306518555 | BCE Loss: 1.0215961933135986\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.04154109954834 | KNN Loss: 3.068453788757324 | BCE Loss: 0.97308748960495\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.095550537109375 | KNN Loss: 3.096910238265991 | BCE Loss: 0.9986405372619629\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.1430792808532715 | KNN Loss: 3.101541042327881 | BCE Loss: 1.0415382385253906\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.08454704284668 | KNN Loss: 3.0588719844818115 | BCE Loss: 1.0256749391555786\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.090926647186279 | KNN Loss: 3.0642566680908203 | BCE Loss: 1.0266700983047485\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.055479049682617 | KNN Loss: 3.0348832607269287 | BCE Loss: 1.020595908164978\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.071577072143555 | KNN Loss: 3.0365471839904785 | BCE Loss: 1.0350300073623657\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.12325382232666 | KNN Loss: 3.059629201889038 | BCE Loss: 1.063624382019043\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.077186584472656 | KNN Loss: 3.067141532897949 | BCE Loss: 1.0100451707839966\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.0959014892578125 | KNN Loss: 3.076580047607422 | BCE Loss: 1.0193216800689697\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.090216636657715 | KNN Loss: 3.022611379623413 | BCE Loss: 1.0676050186157227\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.107756614685059 | KNN Loss: 3.080543041229248 | BCE Loss: 1.0272138118743896\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.0505146980285645 | KNN Loss: 3.046320915222168 | BCE Loss: 1.004193663597107\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.154321193695068 | KNN Loss: 3.1298446655273438 | BCE Loss: 1.024476408958435\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.067124366760254 | KNN Loss: 3.05782151222229 | BCE Loss: 1.0093029737472534\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.129168510437012 | KNN Loss: 3.081432580947876 | BCE Loss: 1.0477361679077148\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.133785724639893 | KNN Loss: 3.069272518157959 | BCE Loss: 1.0645133256912231\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.037384033203125 | KNN Loss: 3.035740375518799 | BCE Loss: 1.001643419265747\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.061730861663818 | KNN Loss: 3.0496182441711426 | BCE Loss: 1.0121127367019653\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.10952091217041 | KNN Loss: 3.04068660736084 | BCE Loss: 1.0688341856002808\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.082841873168945 | KNN Loss: 3.070234537124634 | BCE Loss: 1.0126075744628906\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.069427490234375 | KNN Loss: 3.0384950637817383 | BCE Loss: 1.0309323072433472\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.077003002166748 | KNN Loss: 3.0685181617736816 | BCE Loss: 1.0084848403930664\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.085816860198975 | KNN Loss: 3.08656907081604 | BCE Loss: 0.9992477893829346\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.099855422973633 | KNN Loss: 3.0841064453125 | BCE Loss: 1.0157489776611328\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.109721660614014 | KNN Loss: 3.0746002197265625 | BCE Loss: 1.0351214408874512\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.089357376098633 | KNN Loss: 3.0723390579223633 | BCE Loss: 1.0170183181762695\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.089106559753418 | KNN Loss: 3.017902135848999 | BCE Loss: 1.071204423904419\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.068227767944336 | KNN Loss: 3.0623037815093994 | BCE Loss: 1.0059239864349365\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.121592998504639 | KNN Loss: 3.091925621032715 | BCE Loss: 1.0296674966812134\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.039911270141602 | KNN Loss: 3.025791645050049 | BCE Loss: 1.0141197443008423\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.062186241149902 | KNN Loss: 3.0554144382476807 | BCE Loss: 1.0067719221115112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.063131809234619 | KNN Loss: 3.060817241668701 | BCE Loss: 1.0023144483566284\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.0294671058654785 | KNN Loss: 3.0181758403778076 | BCE Loss: 1.011291265487671\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.068187713623047 | KNN Loss: 3.0853111743927 | BCE Loss: 0.9828767776489258\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.115523338317871 | KNN Loss: 3.0644357204437256 | BCE Loss: 1.0510876178741455\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.01731014251709 | KNN Loss: 3.032503128051758 | BCE Loss: 0.9848072528839111\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.1256513595581055 | KNN Loss: 3.0785109996795654 | BCE Loss: 1.04714035987854\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.085188388824463 | KNN Loss: 3.0763819217681885 | BCE Loss: 1.0088064670562744\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.111865520477295 | KNN Loss: 3.09560227394104 | BCE Loss: 1.0162632465362549\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.05866813659668 | KNN Loss: 3.0456278324127197 | BCE Loss: 1.013040542602539\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.103799819946289 | KNN Loss: 3.111839532852173 | BCE Loss: 0.9919604063034058\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.1137800216674805 | KNN Loss: 3.0562126636505127 | BCE Loss: 1.0575673580169678\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.09940242767334 | KNN Loss: 3.0833964347839355 | BCE Loss: 1.0160058736801147\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.067156791687012 | KNN Loss: 3.075120449066162 | BCE Loss: 0.9920362830162048\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.073350429534912 | KNN Loss: 3.0614125728607178 | BCE Loss: 1.0119377374649048\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.065832138061523 | KNN Loss: 3.059563398361206 | BCE Loss: 1.0062685012817383\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.0790510177612305 | KNN Loss: 3.06657075881958 | BCE Loss: 1.0124804973602295\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.145817756652832 | KNN Loss: 3.119356870651245 | BCE Loss: 1.026461124420166\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.0905046463012695 | KNN Loss: 3.0531413555145264 | BCE Loss: 1.037363052368164\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.0680084228515625 | KNN Loss: 3.0442323684692383 | BCE Loss: 1.0237760543823242\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.096932411193848 | KNN Loss: 3.084355354309082 | BCE Loss: 1.0125770568847656\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.127187728881836 | KNN Loss: 3.0873143672943115 | BCE Loss: 1.0398731231689453\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.1081647872924805 | KNN Loss: 3.064478635787964 | BCE Loss: 1.0436863899230957\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.135392189025879 | KNN Loss: 3.1369526386260986 | BCE Loss: 0.9984395503997803\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.124929428100586 | KNN Loss: 3.10886812210083 | BCE Loss: 1.016061544418335\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.11693000793457 | KNN Loss: 3.0861761569976807 | BCE Loss: 1.0307536125183105\n",
      "Epoch   432: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.081880569458008 | KNN Loss: 3.0866081714630127 | BCE Loss: 0.9952722787857056\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.1353654861450195 | KNN Loss: 3.115565776824951 | BCE Loss: 1.019799828529358\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.1026716232299805 | KNN Loss: 3.060154914855957 | BCE Loss: 1.0425167083740234\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.03381872177124 | KNN Loss: 3.0477683544158936 | BCE Loss: 0.9860504269599915\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.083376884460449 | KNN Loss: 3.0645840167999268 | BCE Loss: 1.0187926292419434\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.088709831237793 | KNN Loss: 3.0563793182373047 | BCE Loss: 1.0323307514190674\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.09519100189209 | KNN Loss: 3.0843865871429443 | BCE Loss: 1.0108046531677246\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.1126532554626465 | KNN Loss: 3.1016242504119873 | BCE Loss: 1.0110291242599487\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.0813517570495605 | KNN Loss: 3.048069715499878 | BCE Loss: 1.0332820415496826\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.083126544952393 | KNN Loss: 3.0605669021606445 | BCE Loss: 1.022559642791748\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.026667594909668 | KNN Loss: 3.0226306915283203 | BCE Loss: 1.0040366649627686\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.112813949584961 | KNN Loss: 3.079347848892212 | BCE Loss: 1.033466100692749\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.058813095092773 | KNN Loss: 3.0568227767944336 | BCE Loss: 1.001990556716919\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.0589704513549805 | KNN Loss: 3.042020320892334 | BCE Loss: 1.016950249671936\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.1436567306518555 | KNN Loss: 3.0974924564361572 | BCE Loss: 1.0461643934249878\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.070579528808594 | KNN Loss: 3.0583832263946533 | BCE Loss: 1.0121960639953613\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.06062126159668 | KNN Loss: 3.0490517616271973 | BCE Loss: 1.0115697383880615\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.092315673828125 | KNN Loss: 3.0656208992004395 | BCE Loss: 1.0266945362091064\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.039244174957275 | KNN Loss: 3.0245628356933594 | BCE Loss: 1.0146814584732056\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.093697547912598 | KNN Loss: 3.061904191970825 | BCE Loss: 1.0317933559417725\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.057620525360107 | KNN Loss: 3.0481793880462646 | BCE Loss: 1.0094412565231323\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.051445007324219 | KNN Loss: 3.0364651679992676 | BCE Loss: 1.0149797201156616\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.072577476501465 | KNN Loss: 3.0771594047546387 | BCE Loss: 0.9954178929328918\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.1314496994018555 | KNN Loss: 3.097137212753296 | BCE Loss: 1.03431236743927\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.029932498931885 | KNN Loss: 3.022535800933838 | BCE Loss: 1.0073965787887573\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.137178421020508 | KNN Loss: 3.0833895206451416 | BCE Loss: 1.053788661956787\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.08632230758667 | KNN Loss: 3.064723491668701 | BCE Loss: 1.0215986967086792\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 3.997312545776367 | KNN Loss: 3.0168099403381348 | BCE Loss: 0.9805024862289429\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.102323055267334 | KNN Loss: 3.085550546646118 | BCE Loss: 1.0167725086212158\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.114526271820068 | KNN Loss: 3.0690441131591797 | BCE Loss: 1.0454822778701782\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.055334091186523 | KNN Loss: 3.052830457687378 | BCE Loss: 1.0025033950805664\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.062434196472168 | KNN Loss: 3.065364122390747 | BCE Loss: 0.9970700144767761\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.060884952545166 | KNN Loss: 3.0444297790527344 | BCE Loss: 1.0164552927017212\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.0750837326049805 | KNN Loss: 3.0705416202545166 | BCE Loss: 1.0045419931411743\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.122550964355469 | KNN Loss: 3.108525276184082 | BCE Loss: 1.0140258073806763\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.057277679443359 | KNN Loss: 3.0387871265411377 | BCE Loss: 1.0184907913208008\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.08354377746582 | KNN Loss: 3.050560474395752 | BCE Loss: 1.0329833030700684\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.069509506225586 | KNN Loss: 3.0359082221984863 | BCE Loss: 1.0336012840270996\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.089707851409912 | KNN Loss: 3.0813450813293457 | BCE Loss: 1.0083626508712769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.070826530456543 | KNN Loss: 3.068413496017456 | BCE Loss: 1.0024127960205078\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.064877986907959 | KNN Loss: 3.0348563194274902 | BCE Loss: 1.0300217866897583\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.051326751708984 | KNN Loss: 3.0578482151031494 | BCE Loss: 0.9934786558151245\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.058355331420898 | KNN Loss: 3.0422353744506836 | BCE Loss: 1.016120195388794\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.057433128356934 | KNN Loss: 3.0393223762512207 | BCE Loss: 1.018110752105713\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.105820655822754 | KNN Loss: 3.0920188426971436 | BCE Loss: 1.0138015747070312\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.0294389724731445 | KNN Loss: 3.038922071456909 | BCE Loss: 0.9905167818069458\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.088231086730957 | KNN Loss: 3.064784526824951 | BCE Loss: 1.0234465599060059\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.114225387573242 | KNN Loss: 3.0649960041046143 | BCE Loss: 1.049229621887207\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.057106018066406 | KNN Loss: 3.0513412952423096 | BCE Loss: 1.0057649612426758\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.114940166473389 | KNN Loss: 3.11053466796875 | BCE Loss: 1.0044054985046387\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.061862468719482 | KNN Loss: 3.0537538528442383 | BCE Loss: 1.0081087350845337\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.102632522583008 | KNN Loss: 3.07521653175354 | BCE Loss: 1.0274162292480469\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.042309761047363 | KNN Loss: 3.022747278213501 | BCE Loss: 1.0195623636245728\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.074913024902344 | KNN Loss: 3.0743439197540283 | BCE Loss: 1.0005688667297363\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.035913944244385 | KNN Loss: 3.046623706817627 | BCE Loss: 0.9892903566360474\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.073315620422363 | KNN Loss: 3.0735127925872803 | BCE Loss: 0.9998030066490173\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.083243370056152 | KNN Loss: 3.049550771713257 | BCE Loss: 1.033692479133606\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.074864387512207 | KNN Loss: 3.045618772506714 | BCE Loss: 1.0292456150054932\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.06197452545166 | KNN Loss: 3.049025058746338 | BCE Loss: 1.0129492282867432\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.037940502166748 | KNN Loss: 3.0492002964019775 | BCE Loss: 0.988740086555481\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.123966217041016 | KNN Loss: 3.1085469722747803 | BCE Loss: 1.0154190063476562\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.097357749938965 | KNN Loss: 3.061628818511963 | BCE Loss: 1.0357286930084229\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.078036785125732 | KNN Loss: 3.0380728244781494 | BCE Loss: 1.0399640798568726\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.0657958984375 | KNN Loss: 3.0528783798217773 | BCE Loss: 1.0129172801971436\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.108325004577637 | KNN Loss: 3.0680909156799316 | BCE Loss: 1.040234088897705\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.0954694747924805 | KNN Loss: 3.092688798904419 | BCE Loss: 1.0027804374694824\n",
      "Epoch   443: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.041805267333984 | KNN Loss: 3.0374152660369873 | BCE Loss: 1.0043902397155762\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.089840412139893 | KNN Loss: 3.0655934810638428 | BCE Loss: 1.0242468118667603\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.0934014320373535 | KNN Loss: 3.0671334266662598 | BCE Loss: 1.0262680053710938\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.083862781524658 | KNN Loss: 3.069704532623291 | BCE Loss: 1.0141583681106567\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.0923967361450195 | KNN Loss: 3.068103551864624 | BCE Loss: 1.0242931842803955\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.0538787841796875 | KNN Loss: 3.0291051864624023 | BCE Loss: 1.0247735977172852\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.0566511154174805 | KNN Loss: 3.048354387283325 | BCE Loss: 1.0082969665527344\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.060863494873047 | KNN Loss: 3.043558120727539 | BCE Loss: 1.0173054933547974\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.075613021850586 | KNN Loss: 3.0346932411193848 | BCE Loss: 1.0409200191497803\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.111721992492676 | KNN Loss: 3.0700340270996094 | BCE Loss: 1.0416882038116455\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.135952949523926 | KNN Loss: 3.0758018493652344 | BCE Loss: 1.0601508617401123\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.051970481872559 | KNN Loss: 3.0601820945739746 | BCE Loss: 0.9917885065078735\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.099981784820557 | KNN Loss: 3.061967134475708 | BCE Loss: 1.0380147695541382\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.114484786987305 | KNN Loss: 3.081979513168335 | BCE Loss: 1.0325050354003906\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.057609558105469 | KNN Loss: 3.050278902053833 | BCE Loss: 1.0073305368423462\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.107717514038086 | KNN Loss: 3.078172445297241 | BCE Loss: 1.0295449495315552\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.0731306076049805 | KNN Loss: 3.065861225128174 | BCE Loss: 1.007269263267517\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.096928596496582 | KNN Loss: 3.0893478393554688 | BCE Loss: 1.0075806379318237\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.109438896179199 | KNN Loss: 3.0938620567321777 | BCE Loss: 1.015576720237732\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.073586463928223 | KNN Loss: 3.0632448196411133 | BCE Loss: 1.0103418827056885\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.110472679138184 | KNN Loss: 3.0814075469970703 | BCE Loss: 1.0290653705596924\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.091095924377441 | KNN Loss: 3.0349643230438232 | BCE Loss: 1.0561316013336182\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.070370674133301 | KNN Loss: 3.062656879425049 | BCE Loss: 1.0077135562896729\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.0723066329956055 | KNN Loss: 3.0815038681030273 | BCE Loss: 0.990802526473999\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.091176986694336 | KNN Loss: 3.063387870788574 | BCE Loss: 1.0277893543243408\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.037964344024658 | KNN Loss: 3.0333974361419678 | BCE Loss: 1.0045669078826904\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.066986083984375 | KNN Loss: 3.0566015243530273 | BCE Loss: 1.0103843212127686\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.122687339782715 | KNN Loss: 3.0930349826812744 | BCE Loss: 1.0296523571014404\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.0722126960754395 | KNN Loss: 3.0477116107940674 | BCE Loss: 1.0245009660720825\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.086395263671875 | KNN Loss: 3.0766396522521973 | BCE Loss: 1.0097558498382568\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.098909854888916 | KNN Loss: 3.0971884727478027 | BCE Loss: 1.0017212629318237\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.1226348876953125 | KNN Loss: 3.096139669418335 | BCE Loss: 1.0264954566955566\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.065863609313965 | KNN Loss: 3.048163890838623 | BCE Loss: 1.0176998376846313\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.085695266723633 | KNN Loss: 3.056786298751831 | BCE Loss: 1.0289092063903809\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.077071666717529 | KNN Loss: 3.0493252277374268 | BCE Loss: 1.0277464389801025\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.115525245666504 | KNN Loss: 3.079699754714966 | BCE Loss: 1.035825490951538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.074820518493652 | KNN Loss: 3.059860944747925 | BCE Loss: 1.0149593353271484\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.077322959899902 | KNN Loss: 3.0536935329437256 | BCE Loss: 1.0236293077468872\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.085323333740234 | KNN Loss: 3.068756341934204 | BCE Loss: 1.0165669918060303\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.125814437866211 | KNN Loss: 3.0888099670410156 | BCE Loss: 1.0370047092437744\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.084348678588867 | KNN Loss: 3.0829086303710938 | BCE Loss: 1.0014402866363525\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.132314682006836 | KNN Loss: 3.0944225788116455 | BCE Loss: 1.0378918647766113\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.035597324371338 | KNN Loss: 3.0204126834869385 | BCE Loss: 1.0151846408843994\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.066457748413086 | KNN Loss: 3.0734572410583496 | BCE Loss: 0.9930007457733154\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.096076488494873 | KNN Loss: 3.072404384613037 | BCE Loss: 1.023672103881836\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.099808692932129 | KNN Loss: 3.0756490230560303 | BCE Loss: 1.0241596698760986\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.083139419555664 | KNN Loss: 3.0400454998016357 | BCE Loss: 1.0430938005447388\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.047996520996094 | KNN Loss: 3.035921335220337 | BCE Loss: 1.0120749473571777\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.073086738586426 | KNN Loss: 3.0278632640838623 | BCE Loss: 1.0452234745025635\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.12664794921875 | KNN Loss: 3.081038475036621 | BCE Loss: 1.0456095933914185\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.081787109375 | KNN Loss: 3.0715057849884033 | BCE Loss: 1.0102810859680176\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.039427757263184 | KNN Loss: 3.0522449016571045 | BCE Loss: 0.9871827960014343\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.062241554260254 | KNN Loss: 3.054391384124756 | BCE Loss: 1.007850170135498\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.0943379402160645 | KNN Loss: 3.073512077331543 | BCE Loss: 1.0208258628845215\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.07276725769043 | KNN Loss: 3.0618042945861816 | BCE Loss: 1.0109632015228271\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.12479829788208 | KNN Loss: 3.0783464908599854 | BCE Loss: 1.0464516878128052\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.0841064453125 | KNN Loss: 3.055516242980957 | BCE Loss: 1.0285900831222534\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.0665130615234375 | KNN Loss: 3.0789031982421875 | BCE Loss: 0.9876099824905396\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.117895603179932 | KNN Loss: 3.101728916168213 | BCE Loss: 1.0161668062210083\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.0960235595703125 | KNN Loss: 3.07421612739563 | BCE Loss: 1.0218076705932617\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.059506416320801 | KNN Loss: 3.054913282394409 | BCE Loss: 1.0045928955078125\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.092572212219238 | KNN Loss: 3.0713915824890137 | BCE Loss: 1.0211808681488037\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.042731761932373 | KNN Loss: 3.053889513015747 | BCE Loss: 0.9888423085212708\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.060280799865723 | KNN Loss: 3.0657806396484375 | BCE Loss: 0.9945001602172852\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.136589050292969 | KNN Loss: 3.0811262130737305 | BCE Loss: 1.0554629564285278\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.124005317687988 | KNN Loss: 3.100048065185547 | BCE Loss: 1.0239572525024414\n",
      "Epoch   454: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.062457084655762 | KNN Loss: 3.045180320739746 | BCE Loss: 1.0172770023345947\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.13716983795166 | KNN Loss: 3.138183116912842 | BCE Loss: 0.998986542224884\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.095168113708496 | KNN Loss: 3.07735538482666 | BCE Loss: 1.017812967300415\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.082646369934082 | KNN Loss: 3.057743549346924 | BCE Loss: 1.0249029397964478\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.064705848693848 | KNN Loss: 3.0633387565612793 | BCE Loss: 1.0013669729232788\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.075618743896484 | KNN Loss: 3.06138277053833 | BCE Loss: 1.0142357349395752\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.04379940032959 | KNN Loss: 3.0303163528442383 | BCE Loss: 1.0134832859039307\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.088746070861816 | KNN Loss: 3.057168483734131 | BCE Loss: 1.0315773487091064\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.0985026359558105 | KNN Loss: 3.060715913772583 | BCE Loss: 1.0377867221832275\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.078082084655762 | KNN Loss: 3.050729990005493 | BCE Loss: 1.0273523330688477\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.056832313537598 | KNN Loss: 3.047208547592163 | BCE Loss: 1.0096238851547241\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.07631254196167 | KNN Loss: 3.0675618648529053 | BCE Loss: 1.0087507963180542\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.0631890296936035 | KNN Loss: 3.0457992553710938 | BCE Loss: 1.0173896551132202\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.093404293060303 | KNN Loss: 3.0596401691436768 | BCE Loss: 1.0337642431259155\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.038647651672363 | KNN Loss: 3.033390998840332 | BCE Loss: 1.0052566528320312\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.096890449523926 | KNN Loss: 3.056889772415161 | BCE Loss: 1.0400006771087646\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.075019836425781 | KNN Loss: 3.0555968284606934 | BCE Loss: 1.0194231271743774\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.051873683929443 | KNN Loss: 3.038743734359741 | BCE Loss: 1.0131298303604126\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.092437744140625 | KNN Loss: 3.062016010284424 | BCE Loss: 1.030421495437622\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.0565080642700195 | KNN Loss: 3.0547471046447754 | BCE Loss: 1.0017611980438232\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.053645133972168 | KNN Loss: 3.0426764488220215 | BCE Loss: 1.0109689235687256\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.144444942474365 | KNN Loss: 3.094733238220215 | BCE Loss: 1.04971182346344\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.092068195343018 | KNN Loss: 3.064970016479492 | BCE Loss: 1.027098298072815\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.07090950012207 | KNN Loss: 3.059758424758911 | BCE Loss: 1.0111510753631592\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.129305839538574 | KNN Loss: 3.0902814865112305 | BCE Loss: 1.0390243530273438\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.062741279602051 | KNN Loss: 3.0401365756988525 | BCE Loss: 1.0226047039031982\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.090518474578857 | KNN Loss: 3.0828206539154053 | BCE Loss: 1.0076979398727417\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.0566816329956055 | KNN Loss: 3.0755650997161865 | BCE Loss: 0.981116533279419\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.086935520172119 | KNN Loss: 3.0675857067108154 | BCE Loss: 1.0193498134613037\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.063055992126465 | KNN Loss: 3.0231993198394775 | BCE Loss: 1.0398569107055664\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.078813076019287 | KNN Loss: 3.070713758468628 | BCE Loss: 1.0080994367599487\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.041468143463135 | KNN Loss: 3.0379831790924072 | BCE Loss: 1.0034849643707275\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.091338157653809 | KNN Loss: 3.0762689113616943 | BCE Loss: 1.0150693655014038\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.085440635681152 | KNN Loss: 3.084294557571411 | BCE Loss: 1.0011463165283203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.079638957977295 | KNN Loss: 3.042768955230713 | BCE Loss: 1.0368698835372925\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.109965801239014 | KNN Loss: 3.082084894180298 | BCE Loss: 1.0278807878494263\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.082767486572266 | KNN Loss: 3.072995662689209 | BCE Loss: 1.0097715854644775\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.054298400878906 | KNN Loss: 3.0378551483154297 | BCE Loss: 1.0164430141448975\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.093414783477783 | KNN Loss: 3.0742766857147217 | BCE Loss: 1.019137978553772\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.090819835662842 | KNN Loss: 3.07895827293396 | BCE Loss: 1.0118614435195923\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.145552158355713 | KNN Loss: 3.1148338317871094 | BCE Loss: 1.030718207359314\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.0599870681762695 | KNN Loss: 3.039482355117798 | BCE Loss: 1.0205047130584717\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.1084489822387695 | KNN Loss: 3.084365129470825 | BCE Loss: 1.0240836143493652\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.0600175857543945 | KNN Loss: 3.049102544784546 | BCE Loss: 1.0109148025512695\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.048827171325684 | KNN Loss: 3.0355379581451416 | BCE Loss: 1.013289213180542\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.055966377258301 | KNN Loss: 3.0482263565063477 | BCE Loss: 1.007739782333374\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.096357345581055 | KNN Loss: 3.056626558303833 | BCE Loss: 1.0397305488586426\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.075663089752197 | KNN Loss: 3.055366277694702 | BCE Loss: 1.0202966928482056\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.073024749755859 | KNN Loss: 3.0513417720794678 | BCE Loss: 1.0216829776763916\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.080478668212891 | KNN Loss: 3.0631794929504395 | BCE Loss: 1.0172994136810303\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.034893989562988 | KNN Loss: 3.048400640487671 | BCE Loss: 0.9864934682846069\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.108505725860596 | KNN Loss: 3.0663278102874756 | BCE Loss: 1.0421777963638306\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.119387626647949 | KNN Loss: 3.0925118923187256 | BCE Loss: 1.0268759727478027\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.07594633102417 | KNN Loss: 3.0799694061279297 | BCE Loss: 0.9959770441055298\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.102547645568848 | KNN Loss: 3.0492236614227295 | BCE Loss: 1.0533239841461182\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.12537956237793 | KNN Loss: 3.076303005218506 | BCE Loss: 1.0490766763687134\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.039581298828125 | KNN Loss: 3.0366907119750977 | BCE Loss: 1.0028908252716064\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.062807083129883 | KNN Loss: 3.044872760772705 | BCE Loss: 1.0179343223571777\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.081428527832031 | KNN Loss: 3.0754387378692627 | BCE Loss: 1.0059895515441895\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.105821132659912 | KNN Loss: 3.0583999156951904 | BCE Loss: 1.0474213361740112\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.090872764587402 | KNN Loss: 3.0832457542419434 | BCE Loss: 1.007627010345459\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.0814666748046875 | KNN Loss: 3.0643091201782227 | BCE Loss: 1.0171575546264648\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.076148509979248 | KNN Loss: 3.056689739227295 | BCE Loss: 1.0194587707519531\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.087710380554199 | KNN Loss: 3.053264856338501 | BCE Loss: 1.0344457626342773\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.087711334228516 | KNN Loss: 3.0760157108306885 | BCE Loss: 1.0116956233978271\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.055603504180908 | KNN Loss: 3.038787603378296 | BCE Loss: 1.0168159008026123\n",
      "Epoch   465: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.069713592529297 | KNN Loss: 3.046952962875366 | BCE Loss: 1.0227607488632202\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.0542144775390625 | KNN Loss: 3.0304903984069824 | BCE Loss: 1.0237241983413696\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.134840965270996 | KNN Loss: 3.1204798221588135 | BCE Loss: 1.0143609046936035\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.092568397521973 | KNN Loss: 3.068423271179199 | BCE Loss: 1.0241453647613525\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.089713096618652 | KNN Loss: 3.0799317359924316 | BCE Loss: 1.0097811222076416\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.021799564361572 | KNN Loss: 3.026965856552124 | BCE Loss: 0.9948335289955139\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.044360160827637 | KNN Loss: 3.053257465362549 | BCE Loss: 0.9911028146743774\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.108003616333008 | KNN Loss: 3.0849266052246094 | BCE Loss: 1.023077130317688\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.05999755859375 | KNN Loss: 3.0253777503967285 | BCE Loss: 1.0346200466156006\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.053948402404785 | KNN Loss: 3.0561728477478027 | BCE Loss: 0.9977757930755615\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.099889278411865 | KNN Loss: 3.07603120803833 | BCE Loss: 1.0238581895828247\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.105579853057861 | KNN Loss: 3.0703468322753906 | BCE Loss: 1.0352330207824707\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.0785908699035645 | KNN Loss: 3.0524089336395264 | BCE Loss: 1.026181936264038\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.083024978637695 | KNN Loss: 3.0723345279693604 | BCE Loss: 1.0106902122497559\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.093914985656738 | KNN Loss: 3.0743019580841064 | BCE Loss: 1.019613265991211\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.043997287750244 | KNN Loss: 3.0572714805603027 | BCE Loss: 0.9867256283760071\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.105363845825195 | KNN Loss: 3.0899479389190674 | BCE Loss: 1.0154160261154175\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.080977916717529 | KNN Loss: 3.069688081741333 | BCE Loss: 1.0112897157669067\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.070500373840332 | KNN Loss: 3.074465274810791 | BCE Loss: 0.9960348606109619\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.02470064163208 | KNN Loss: 3.0170137882232666 | BCE Loss: 1.007686734199524\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.05108118057251 | KNN Loss: 3.0374135971069336 | BCE Loss: 1.0136675834655762\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.105620384216309 | KNN Loss: 3.0844573974609375 | BCE Loss: 1.021162986755371\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.0450663566589355 | KNN Loss: 3.0235021114349365 | BCE Loss: 1.021564245223999\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.116008281707764 | KNN Loss: 3.0866341590881348 | BCE Loss: 1.0293742418289185\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.087351322174072 | KNN Loss: 3.056138515472412 | BCE Loss: 1.0312128067016602\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.103559494018555 | KNN Loss: 3.067444324493408 | BCE Loss: 1.036115288734436\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.089134693145752 | KNN Loss: 3.0777747631073 | BCE Loss: 1.0113599300384521\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.069748401641846 | KNN Loss: 3.060790538787842 | BCE Loss: 1.0089579820632935\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.054916858673096 | KNN Loss: 3.056535243988037 | BCE Loss: 0.9983817338943481\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.074951171875 | KNN Loss: 3.0571324825286865 | BCE Loss: 1.0178186893463135\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.049644470214844 | KNN Loss: 3.0387308597564697 | BCE Loss: 1.010913372039795\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.040771484375 | KNN Loss: 3.0418601036071777 | BCE Loss: 0.9989111423492432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.104984283447266 | KNN Loss: 3.056821823120117 | BCE Loss: 1.0481626987457275\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.079991340637207 | KNN Loss: 3.051877498626709 | BCE Loss: 1.0281140804290771\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.057466983795166 | KNN Loss: 3.0654470920562744 | BCE Loss: 0.9920198917388916\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.107001781463623 | KNN Loss: 3.0615482330322266 | BCE Loss: 1.045453429222107\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.0921549797058105 | KNN Loss: 3.0709269046783447 | BCE Loss: 1.0212280750274658\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.114859580993652 | KNN Loss: 3.079730749130249 | BCE Loss: 1.0351288318634033\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.035788059234619 | KNN Loss: 3.0500328540802 | BCE Loss: 0.985755205154419\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.117266654968262 | KNN Loss: 3.072608470916748 | BCE Loss: 1.0446584224700928\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.067497253417969 | KNN Loss: 3.066946268081665 | BCE Loss: 1.0005512237548828\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.059000015258789 | KNN Loss: 3.058501720428467 | BCE Loss: 1.0004984140396118\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.091802597045898 | KNN Loss: 3.0622687339782715 | BCE Loss: 1.0295337438583374\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.082554340362549 | KNN Loss: 3.0549111366271973 | BCE Loss: 1.0276432037353516\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.068475246429443 | KNN Loss: 3.0513360500335693 | BCE Loss: 1.017139196395874\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.074662208557129 | KNN Loss: 3.0421500205993652 | BCE Loss: 1.0325119495391846\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.13541316986084 | KNN Loss: 3.096423625946045 | BCE Loss: 1.038989782333374\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.0763726234436035 | KNN Loss: 3.0301759243011475 | BCE Loss: 1.046196699142456\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.089175224304199 | KNN Loss: 3.0768303871154785 | BCE Loss: 1.0123445987701416\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.052292346954346 | KNN Loss: 3.0312771797180176 | BCE Loss: 1.0210152864456177\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.071436882019043 | KNN Loss: 3.0516011714935303 | BCE Loss: 1.0198359489440918\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.087639808654785 | KNN Loss: 3.0840771198272705 | BCE Loss: 1.0035624504089355\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.068647861480713 | KNN Loss: 3.0342302322387695 | BCE Loss: 1.0344176292419434\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.084967136383057 | KNN Loss: 3.0630416870117188 | BCE Loss: 1.0219255685806274\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.103633880615234 | KNN Loss: 3.0639712810516357 | BCE Loss: 1.0396625995635986\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.108945369720459 | KNN Loss: 3.076725721359253 | BCE Loss: 1.0322197675704956\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.108969688415527 | KNN Loss: 3.0878500938415527 | BCE Loss: 1.0211198329925537\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.0560622215271 | KNN Loss: 3.054722309112549 | BCE Loss: 1.0013397932052612\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.048344612121582 | KNN Loss: 3.0394985675811768 | BCE Loss: 1.0088459253311157\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.09227991104126 | KNN Loss: 3.0658445358276367 | BCE Loss: 1.0264352560043335\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.068210124969482 | KNN Loss: 3.059657573699951 | BCE Loss: 1.0085524320602417\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.119998931884766 | KNN Loss: 3.0927927494049072 | BCE Loss: 1.027206301689148\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.11686372756958 | KNN Loss: 3.0738275051116943 | BCE Loss: 1.0430363416671753\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.069626331329346 | KNN Loss: 3.0605220794677734 | BCE Loss: 1.0091042518615723\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.081998825073242 | KNN Loss: 3.0821564197540283 | BCE Loss: 0.9998423457145691\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.102417469024658 | KNN Loss: 3.06320858001709 | BCE Loss: 1.0392087697982788\n",
      "Epoch   476: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.101633071899414 | KNN Loss: 3.081582546234131 | BCE Loss: 1.0200505256652832\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.103466510772705 | KNN Loss: 3.0870981216430664 | BCE Loss: 1.0163683891296387\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.132021903991699 | KNN Loss: 3.096170425415039 | BCE Loss: 1.0358513593673706\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.071478843688965 | KNN Loss: 3.0390777587890625 | BCE Loss: 1.0324008464813232\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.099553108215332 | KNN Loss: 3.083425521850586 | BCE Loss: 1.016127586364746\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.1026129722595215 | KNN Loss: 3.0812621116638184 | BCE Loss: 1.0213508605957031\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.068063259124756 | KNN Loss: 3.0557966232299805 | BCE Loss: 1.012266755104065\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.093392372131348 | KNN Loss: 3.058534622192383 | BCE Loss: 1.0348577499389648\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.107923984527588 | KNN Loss: 3.061931848526001 | BCE Loss: 1.045992136001587\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.113066673278809 | KNN Loss: 3.087263584136963 | BCE Loss: 1.0258028507232666\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.064636707305908 | KNN Loss: 3.061966896057129 | BCE Loss: 1.0026698112487793\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.092887878417969 | KNN Loss: 3.0511884689331055 | BCE Loss: 1.0416991710662842\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.081930160522461 | KNN Loss: 3.0718626976013184 | BCE Loss: 1.0100672245025635\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.047966480255127 | KNN Loss: 3.038562297821045 | BCE Loss: 1.009404182434082\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.116006851196289 | KNN Loss: 3.095890522003174 | BCE Loss: 1.0201160907745361\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.074474334716797 | KNN Loss: 3.0456061363220215 | BCE Loss: 1.0288679599761963\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.086394786834717 | KNN Loss: 3.0513997077941895 | BCE Loss: 1.0349950790405273\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.068232536315918 | KNN Loss: 3.0647225379943848 | BCE Loss: 1.0035102367401123\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.09183406829834 | KNN Loss: 3.0847787857055664 | BCE Loss: 1.007055401802063\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.119370460510254 | KNN Loss: 3.0797030925750732 | BCE Loss: 1.0396672487258911\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.082700729370117 | KNN Loss: 3.0744011402130127 | BCE Loss: 1.0082993507385254\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.0417914390563965 | KNN Loss: 3.0403804779052734 | BCE Loss: 1.001410961151123\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.111196517944336 | KNN Loss: 3.067711591720581 | BCE Loss: 1.0434846878051758\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.07599401473999 | KNN Loss: 3.0786190032958984 | BCE Loss: 0.9973750710487366\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.075516700744629 | KNN Loss: 3.0500643253326416 | BCE Loss: 1.0254521369934082\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.062270641326904 | KNN Loss: 3.0449156761169434 | BCE Loss: 1.0173550844192505\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.137340068817139 | KNN Loss: 3.0995051860809326 | BCE Loss: 1.0378350019454956\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.088932514190674 | KNN Loss: 3.0419042110443115 | BCE Loss: 1.0470281839370728\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.104728698730469 | KNN Loss: 3.0680460929870605 | BCE Loss: 1.0366824865341187\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.08463716506958 | KNN Loss: 3.0932137966156006 | BCE Loss: 0.9914232492446899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.073573589324951 | KNN Loss: 3.070242404937744 | BCE Loss: 1.003331184387207\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.109986782073975 | KNN Loss: 3.0789897441864014 | BCE Loss: 1.0309971570968628\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.059248924255371 | KNN Loss: 3.0500409603118896 | BCE Loss: 1.0092079639434814\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.138001918792725 | KNN Loss: 3.11407732963562 | BCE Loss: 1.023924708366394\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.1047563552856445 | KNN Loss: 3.072605609893799 | BCE Loss: 1.0321505069732666\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.089522838592529 | KNN Loss: 3.062883138656616 | BCE Loss: 1.026639699935913\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.08200740814209 | KNN Loss: 3.0660314559936523 | BCE Loss: 1.0159759521484375\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.118971824645996 | KNN Loss: 3.0805106163024902 | BCE Loss: 1.0384613275527954\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.102530002593994 | KNN Loss: 3.083993434906006 | BCE Loss: 1.0185365676879883\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.0630645751953125 | KNN Loss: 3.0707547664642334 | BCE Loss: 0.9923099279403687\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.091514587402344 | KNN Loss: 3.069321632385254 | BCE Loss: 1.0221929550170898\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.104617118835449 | KNN Loss: 3.0772206783294678 | BCE Loss: 1.027396559715271\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.072153091430664 | KNN Loss: 3.0300090312957764 | BCE Loss: 1.0421438217163086\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.06634521484375 | KNN Loss: 3.061676025390625 | BCE Loss: 1.0046693086624146\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.065055847167969 | KNN Loss: 3.0431833267211914 | BCE Loss: 1.0218727588653564\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.078495979309082 | KNN Loss: 3.069742441177368 | BCE Loss: 1.008753776550293\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.103762149810791 | KNN Loss: 3.0727741718292236 | BCE Loss: 1.0309879779815674\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.060235977172852 | KNN Loss: 3.049798011779785 | BCE Loss: 1.0104377269744873\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.053507328033447 | KNN Loss: 3.044738531112671 | BCE Loss: 1.008768916130066\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.097224235534668 | KNN Loss: 3.076110601425171 | BCE Loss: 1.021113634109497\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.048224925994873 | KNN Loss: 3.041841506958008 | BCE Loss: 1.0063834190368652\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.063117027282715 | KNN Loss: 3.0452489852905273 | BCE Loss: 1.0178678035736084\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.098830699920654 | KNN Loss: 3.037199020385742 | BCE Loss: 1.0616315603256226\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.12374210357666 | KNN Loss: 3.0916481018066406 | BCE Loss: 1.0320940017700195\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.0853424072265625 | KNN Loss: 3.060781240463257 | BCE Loss: 1.0245611667633057\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.070109844207764 | KNN Loss: 3.0591821670532227 | BCE Loss: 1.010927677154541\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.063137531280518 | KNN Loss: 3.0634589195251465 | BCE Loss: 0.9996784329414368\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.102687358856201 | KNN Loss: 3.095057487487793 | BCE Loss: 1.0076298713684082\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.052773475646973 | KNN Loss: 3.063026189804077 | BCE Loss: 0.9897474050521851\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.100528240203857 | KNN Loss: 3.0547866821289062 | BCE Loss: 1.0457416772842407\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.073174476623535 | KNN Loss: 3.058764696121216 | BCE Loss: 1.0144100189208984\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.05219841003418 | KNN Loss: 3.053356885910034 | BCE Loss: 0.9988414645195007\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.102510452270508 | KNN Loss: 3.054473638534546 | BCE Loss: 1.048036813735962\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.050754070281982 | KNN Loss: 3.033851146697998 | BCE Loss: 1.016903042793274\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.081915855407715 | KNN Loss: 3.083770751953125 | BCE Loss: 0.9981452822685242\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.093502998352051 | KNN Loss: 3.091773271560669 | BCE Loss: 1.0017298460006714\n",
      "Epoch   487: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.091635704040527 | KNN Loss: 3.0563416481018066 | BCE Loss: 1.0352940559387207\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.052386283874512 | KNN Loss: 3.0570249557495117 | BCE Loss: 0.9953613877296448\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.0936970710754395 | KNN Loss: 3.099628210067749 | BCE Loss: 0.9940689206123352\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.0731000900268555 | KNN Loss: 3.056164026260376 | BCE Loss: 1.0169358253479004\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.06526517868042 | KNN Loss: 3.0608811378479004 | BCE Loss: 1.0043840408325195\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.118596076965332 | KNN Loss: 3.10296368598938 | BCE Loss: 1.0156326293945312\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.120684623718262 | KNN Loss: 3.080193042755127 | BCE Loss: 1.0404913425445557\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.073242664337158 | KNN Loss: 3.050861120223999 | BCE Loss: 1.0223816633224487\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.111220836639404 | KNN Loss: 3.0842809677124023 | BCE Loss: 1.026939868927002\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.084561347961426 | KNN Loss: 3.063089370727539 | BCE Loss: 1.0214720964431763\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.032092571258545 | KNN Loss: 3.02211856842041 | BCE Loss: 1.0099740028381348\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.1534528732299805 | KNN Loss: 3.1230428218841553 | BCE Loss: 1.030409812927246\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.079388618469238 | KNN Loss: 3.060202121734619 | BCE Loss: 1.0191867351531982\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.038771152496338 | KNN Loss: 3.0258901119232178 | BCE Loss: 1.0128811597824097\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.0655107498168945 | KNN Loss: 3.057137966156006 | BCE Loss: 1.0083725452423096\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.05244255065918 | KNN Loss: 3.0468380451202393 | BCE Loss: 1.0056045055389404\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.026058673858643 | KNN Loss: 3.035064458847046 | BCE Loss: 0.9909941554069519\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.079550743103027 | KNN Loss: 3.0643551349639893 | BCE Loss: 1.015195608139038\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.1118927001953125 | KNN Loss: 3.0692086219787598 | BCE Loss: 1.0426838397979736\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.127462387084961 | KNN Loss: 3.121744155883789 | BCE Loss: 1.005718469619751\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.0709333419799805 | KNN Loss: 3.0409090518951416 | BCE Loss: 1.030024528503418\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.076575756072998 | KNN Loss: 3.0663580894470215 | BCE Loss: 1.0102176666259766\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.080793380737305 | KNN Loss: 3.0662384033203125 | BCE Loss: 1.014554738998413\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.075108528137207 | KNN Loss: 3.0516040325164795 | BCE Loss: 1.0235044956207275\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.079294681549072 | KNN Loss: 3.085462808609009 | BCE Loss: 0.9938316941261292\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.09316873550415 | KNN Loss: 3.083956241607666 | BCE Loss: 1.0092124938964844\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.070640563964844 | KNN Loss: 3.0655105113983154 | BCE Loss: 1.0051302909851074\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.097971439361572 | KNN Loss: 3.0799062252044678 | BCE Loss: 1.0180652141571045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.110325813293457 | KNN Loss: 3.0628061294555664 | BCE Loss: 1.0475199222564697\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.0865702629089355 | KNN Loss: 3.091723918914795 | BCE Loss: 0.9948463439941406\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.10693359375 | KNN Loss: 3.0725910663604736 | BCE Loss: 1.0343425273895264\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.1229658126831055 | KNN Loss: 3.0546011924743652 | BCE Loss: 1.0683645009994507\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.117815971374512 | KNN Loss: 3.0638058185577393 | BCE Loss: 1.0540101528167725\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.043185710906982 | KNN Loss: 3.0612683296203613 | BCE Loss: 0.9819175004959106\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.104577541351318 | KNN Loss: 3.0563957691192627 | BCE Loss: 1.0481818914413452\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.127537727355957 | KNN Loss: 3.0996971130371094 | BCE Loss: 1.0278406143188477\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.116753101348877 | KNN Loss: 3.0767242908477783 | BCE Loss: 1.0400288105010986\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.105172634124756 | KNN Loss: 3.059035062789917 | BCE Loss: 1.0461374521255493\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.08561897277832 | KNN Loss: 3.066194772720337 | BCE Loss: 1.0194239616394043\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.102329730987549 | KNN Loss: 3.0892255306243896 | BCE Loss: 1.0131042003631592\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.080841064453125 | KNN Loss: 3.0726163387298584 | BCE Loss: 1.0082247257232666\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.0537214279174805 | KNN Loss: 3.0495572090148926 | BCE Loss: 1.004164457321167\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.059905529022217 | KNN Loss: 3.0553929805755615 | BCE Loss: 1.0045125484466553\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.105450630187988 | KNN Loss: 3.0875439643859863 | BCE Loss: 1.0179064273834229\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.04086446762085 | KNN Loss: 3.0410964488983154 | BCE Loss: 0.9997681975364685\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.086911201477051 | KNN Loss: 3.0633420944213867 | BCE Loss: 1.0235693454742432\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.070191860198975 | KNN Loss: 3.064938545227051 | BCE Loss: 1.0052531957626343\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.081980228424072 | KNN Loss: 3.076439142227173 | BCE Loss: 1.0055410861968994\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.095874309539795 | KNN Loss: 3.071148157119751 | BCE Loss: 1.024726152420044\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.042704105377197 | KNN Loss: 3.0481410026550293 | BCE Loss: 0.9945630431175232\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.099803924560547 | KNN Loss: 3.0731239318847656 | BCE Loss: 1.0266802310943604\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.0469465255737305 | KNN Loss: 3.0389833450317383 | BCE Loss: 1.007962942123413\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.100769996643066 | KNN Loss: 3.060433864593506 | BCE Loss: 1.0403363704681396\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.057183265686035 | KNN Loss: 3.0576910972595215 | BCE Loss: 0.9994921684265137\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.120392799377441 | KNN Loss: 3.0788402557373047 | BCE Loss: 1.0415523052215576\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.129899978637695 | KNN Loss: 3.1084375381469727 | BCE Loss: 1.0214624404907227\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.098781585693359 | KNN Loss: 3.0549569129943848 | BCE Loss: 1.0438249111175537\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.0772905349731445 | KNN Loss: 3.0733208656311035 | BCE Loss: 1.003969669342041\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.083667755126953 | KNN Loss: 3.0567944049835205 | BCE Loss: 1.0268735885620117\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.0146355628967285 | KNN Loss: 3.0204131603240967 | BCE Loss: 0.9942225813865662\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.107223033905029 | KNN Loss: 3.0792880058288574 | BCE Loss: 1.0279351472854614\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.105769634246826 | KNN Loss: 3.085758686065674 | BCE Loss: 1.0200109481811523\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.1350884437561035 | KNN Loss: 3.1118557453155518 | BCE Loss: 1.0232325792312622\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.098967552185059 | KNN Loss: 3.099242925643921 | BCE Loss: 0.9997245669364929\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.132546424865723 | KNN Loss: 3.0952699184417725 | BCE Loss: 1.0372765064239502\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.130821228027344 | KNN Loss: 3.133678674697876 | BCE Loss: 0.9971423149108887\n",
      "Epoch   498: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.0681562423706055 | KNN Loss: 3.0677549839019775 | BCE Loss: 1.000401496887207\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.090031623840332 | KNN Loss: 3.063713550567627 | BCE Loss: 1.026318073272705\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.046810150146484 | KNN Loss: 3.040708541870117 | BCE Loss: 1.0061014890670776\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.082050800323486 | KNN Loss: 3.0886621475219727 | BCE Loss: 0.9933887124061584\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.06222677230835 | KNN Loss: 3.0577380657196045 | BCE Loss: 1.0044888257980347\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.122080326080322 | KNN Loss: 3.1117732524871826 | BCE Loss: 1.01030695438385\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.063828468322754 | KNN Loss: 3.055302858352661 | BCE Loss: 1.0085258483886719\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.058554649353027 | KNN Loss: 3.048766613006592 | BCE Loss: 1.0097877979278564\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.062343120574951 | KNN Loss: 3.0749599933624268 | BCE Loss: 0.9873831272125244\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.07295560836792 | KNN Loss: 3.0549237728118896 | BCE Loss: 1.0180318355560303\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.106734275817871 | KNN Loss: 3.0948827266693115 | BCE Loss: 1.0118515491485596\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.09742546081543 | KNN Loss: 3.0770740509033203 | BCE Loss: 1.020351529121399\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.9690,  3.3708,  2.5624,  3.2352,  3.2558,  0.8220,  2.5128,  2.0602,\n",
      "          2.2463,  2.0747,  2.2316,  2.1456,  0.8478,  1.8535,  1.3311,  1.4486,\n",
      "          2.7928,  2.8470,  2.7278,  2.3734,  1.8174,  2.6831,  2.3625,  2.3584,\n",
      "          2.3992,  1.5953,  1.8204,  1.4668,  1.5452,  0.4173, -0.2491,  1.0625,\n",
      "          0.1785,  0.9827,  1.6118,  1.5006,  0.9838,  2.9433,  0.8661,  1.3627,\n",
      "          1.0244, -0.8252, -0.2011,  2.2390,  2.2572,  0.7698, -0.1759,  0.1700,\n",
      "          1.4033,  2.3332,  1.8797,  0.0729,  1.3771,  0.5723, -0.6208,  1.2612,\n",
      "          0.9353,  1.3836,  1.3940,  1.8973,  0.7152,  0.9429,  0.3115,  1.7001,\n",
      "          1.3407,  1.5566, -1.8287,  0.3413,  2.3174,  2.1991,  2.4107,  0.5554,\n",
      "          1.3664,  2.2806,  2.0282,  1.3582,  0.1327,  0.7336,  0.2443,  1.6848,\n",
      "          0.0856,  0.4232,  1.6518, -0.3664,  0.2874, -1.0850, -2.3750, -0.1772,\n",
      "          0.6430, -1.8912,  0.5034, -0.2220, -0.5476, -0.9107,  0.6913,  1.2768,\n",
      "         -0.6644, -0.7303,  0.3754,  1.2550,  0.8147, -1.2918,  0.9324,  1.0119,\n",
      "         -1.2583, -1.1335, -0.1021,  0.1216, -1.0364, -1.5859, -0.4794, -2.9697,\n",
      "         -0.4861,  1.6649,  1.5344, -0.2045, -0.5631, -0.0632,  1.5267, -2.6700,\n",
      "          0.2098, -0.1450,  0.5617, -0.6962,  0.0381, -0.6818, -0.8821,  0.9209,\n",
      "          0.3896, -0.5802,  0.2980, -0.6189, -1.2597, -0.4022, -0.5503,  0.8757,\n",
      "         -0.4654,  0.1300, -1.9943, -0.9785, -1.5052,  0.6786, -1.7510, -0.9396,\n",
      "         -1.0395, -0.6753, -1.5595, -1.0924, -2.4122, -1.1390, -1.1943, -0.3986,\n",
      "         -1.7409,  0.5075, -1.5661, -0.5004, -3.9530,  0.2003, -0.1065, -0.6628,\n",
      "         -2.1686, -1.6289, -1.2109, -1.4497, -2.7959, -2.4128, -3.9073]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.9530, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.3708, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8ffa31e7b74012aeb642ce1659ff79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 81.65it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddeb2b0b1bf466bbfc397a3f8677c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2863fdd26214e388d3f7e6bcf13933c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b1b22d468f4264a47856d171356bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "layer 9: 0.0\n",
      "layer 10: 0.0\n",
      "Epoch: 00 | Batch: 000 / 018 | Total loss: 9.632 | Reg loss: 0.014 | Tree loss: 9.632 | Accuracy: 0.000000 | 4.111 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 018 | Total loss: 9.630 | Reg loss: 0.013 | Tree loss: 9.630 | Accuracy: 0.000000 | 4.112 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 018 | Total loss: 9.630 | Reg loss: 0.012 | Tree loss: 9.630 | Accuracy: 0.007812 | 4.084 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 018 | Total loss: 9.629 | Reg loss: 0.011 | Tree loss: 9.629 | Accuracy: 0.035156 | 4.089 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 018 | Total loss: 9.627 | Reg loss: 0.010 | Tree loss: 9.627 | Accuracy: 0.060547 | 4.094 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 018 | Total loss: 9.625 | Reg loss: 0.009 | Tree loss: 9.625 | Accuracy: 0.062500 | 4.102 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 018 | Total loss: 9.625 | Reg loss: 0.008 | Tree loss: 9.625 | Accuracy: 0.072266 | 4.112 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 018 | Total loss: 9.623 | Reg loss: 0.007 | Tree loss: 9.623 | Accuracy: 0.070312 | 4.121 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 018 | Total loss: 9.620 | Reg loss: 0.007 | Tree loss: 9.620 | Accuracy: 0.093750 | 4.128 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 018 | Total loss: 9.620 | Reg loss: 0.006 | Tree loss: 9.620 | Accuracy: 0.056641 | 4.302 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 018 | Total loss: 9.620 | Reg loss: 0.006 | Tree loss: 9.620 | Accuracy: 0.074219 | 4.429 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 018 | Total loss: 9.619 | Reg loss: 0.005 | Tree loss: 9.619 | Accuracy: 0.070312 | 4.519 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 018 | Total loss: 9.618 | Reg loss: 0.005 | Tree loss: 9.618 | Accuracy: 0.056641 | 4.587 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 018 | Total loss: 9.616 | Reg loss: 0.005 | Tree loss: 9.616 | Accuracy: 0.076172 | 4.634 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 018 | Total loss: 9.615 | Reg loss: 0.005 | Tree loss: 9.615 | Accuracy: 0.070312 | 4.671 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 018 | Total loss: 9.614 | Reg loss: 0.005 | Tree loss: 9.614 | Accuracy: 0.082031 | 4.698 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 018 | Total loss: 9.614 | Reg loss: 0.005 | Tree loss: 9.614 | Accuracy: 0.064453 | 4.722 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 018 | Total loss: 9.609 | Reg loss: 0.005 | Tree loss: 9.609 | Accuracy: 0.146341 | 4.658 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 018 | Total loss: 9.615 | Reg loss: 0.003 | Tree loss: 9.615 | Accuracy: 0.074219 | 4.846 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 018 | Total loss: 9.615 | Reg loss: 0.003 | Tree loss: 9.615 | Accuracy: 0.066406 | 4.854 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 018 | Total loss: 9.613 | Reg loss: 0.003 | Tree loss: 9.613 | Accuracy: 0.085938 | 4.853 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 018 | Total loss: 9.613 | Reg loss: 0.003 | Tree loss: 9.613 | Accuracy: 0.083984 | 4.845 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 018 | Total loss: 9.613 | Reg loss: 0.004 | Tree loss: 9.613 | Accuracy: 0.062500 | 4.834 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 018 | Total loss: 9.611 | Reg loss: 0.004 | Tree loss: 9.611 | Accuracy: 0.070312 | 4.826 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 018 | Total loss: 9.610 | Reg loss: 0.004 | Tree loss: 9.610 | Accuracy: 0.058594 | 4.822 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 018 | Total loss: 9.609 | Reg loss: 0.004 | Tree loss: 9.609 | Accuracy: 0.089844 | 4.818 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 018 | Total loss: 9.608 | Reg loss: 0.004 | Tree loss: 9.608 | Accuracy: 0.076172 | 4.815 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 018 | Total loss: 9.606 | Reg loss: 0.005 | Tree loss: 9.606 | Accuracy: 0.072266 | 4.812 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 018 | Total loss: 9.607 | Reg loss: 0.005 | Tree loss: 9.607 | Accuracy: 0.066406 | 4.81 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 018 | Total loss: 9.605 | Reg loss: 0.005 | Tree loss: 9.605 | Accuracy: 0.074219 | 4.808 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 018 | Total loss: 9.605 | Reg loss: 0.005 | Tree loss: 9.605 | Accuracy: 0.066406 | 4.806 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 018 | Total loss: 9.605 | Reg loss: 0.005 | Tree loss: 9.605 | Accuracy: 0.070312 | 4.805 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 018 | Total loss: 9.604 | Reg loss: 0.005 | Tree loss: 9.604 | Accuracy: 0.074219 | 4.803 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 018 | Total loss: 9.603 | Reg loss: 0.005 | Tree loss: 9.603 | Accuracy: 0.076172 | 4.8 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 018 | Total loss: 9.602 | Reg loss: 0.006 | Tree loss: 9.602 | Accuracy: 0.076172 | 4.78 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 018 | Total loss: 9.611 | Reg loss: 0.006 | Tree loss: 9.611 | Accuracy: 0.048780 | 4.746 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 018 | Total loss: 9.607 | Reg loss: 0.004 | Tree loss: 9.607 | Accuracy: 0.083984 | 4.813 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 018 | Total loss: 9.607 | Reg loss: 0.004 | Tree loss: 9.607 | Accuracy: 0.070312 | 4.827 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 018 | Total loss: 9.605 | Reg loss: 0.004 | Tree loss: 9.605 | Accuracy: 0.078125 | 4.84 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 018 | Total loss: 9.603 | Reg loss: 0.004 | Tree loss: 9.603 | Accuracy: 0.099609 | 4.852 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 018 | Total loss: 9.608 | Reg loss: 0.004 | Tree loss: 9.608 | Accuracy: 0.052734 | 4.863 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 018 | Total loss: 9.605 | Reg loss: 0.004 | Tree loss: 9.605 | Accuracy: 0.066406 | 4.873 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 018 | Total loss: 9.604 | Reg loss: 0.004 | Tree loss: 9.604 | Accuracy: 0.070312 | 4.882 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 018 | Total loss: 9.603 | Reg loss: 0.005 | Tree loss: 9.603 | Accuracy: 0.078125 | 4.89 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 018 | Total loss: 9.603 | Reg loss: 0.005 | Tree loss: 9.603 | Accuracy: 0.066406 | 4.897 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 018 | Total loss: 9.601 | Reg loss: 0.005 | Tree loss: 9.601 | Accuracy: 0.074219 | 4.902 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 018 | Total loss: 9.602 | Reg loss: 0.005 | Tree loss: 9.602 | Accuracy: 0.076172 | 4.906 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 018 | Total loss: 9.601 | Reg loss: 0.005 | Tree loss: 9.601 | Accuracy: 0.074219 | 4.909 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 018 | Total loss: 9.601 | Reg loss: 0.005 | Tree loss: 9.601 | Accuracy: 0.056641 | 4.911 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 018 | Total loss: 9.599 | Reg loss: 0.005 | Tree loss: 9.599 | Accuracy: 0.076172 | 4.91 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 018 | Total loss: 9.600 | Reg loss: 0.006 | Tree loss: 9.600 | Accuracy: 0.068359 | 4.908 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 018 | Total loss: 9.598 | Reg loss: 0.006 | Tree loss: 9.598 | Accuracy: 0.082031 | 4.906 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 018 | Total loss: 9.601 | Reg loss: 0.006 | Tree loss: 9.601 | Accuracy: 0.068359 | 4.903 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 018 | Total loss: 9.592 | Reg loss: 0.006 | Tree loss: 9.592 | Accuracy: 0.073171 | 4.879 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 018 | Total loss: 9.602 | Reg loss: 0.005 | Tree loss: 9.602 | Accuracy: 0.062500 | 4.951 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 018 | Total loss: 9.604 | Reg loss: 0.005 | Tree loss: 9.604 | Accuracy: 0.064453 | 4.962 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 018 | Total loss: 9.604 | Reg loss: 0.005 | Tree loss: 9.604 | Accuracy: 0.082031 | 4.968 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Batch: 003 / 018 | Total loss: 9.602 | Reg loss: 0.005 | Tree loss: 9.602 | Accuracy: 0.074219 | 4.972 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 018 | Total loss: 9.602 | Reg loss: 0.005 | Tree loss: 9.602 | Accuracy: 0.076172 | 4.972 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 018 | Total loss: 9.600 | Reg loss: 0.005 | Tree loss: 9.600 | Accuracy: 0.080078 | 4.967 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 018 | Total loss: 9.599 | Reg loss: 0.005 | Tree loss: 9.599 | Accuracy: 0.058594 | 4.966 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 018 | Total loss: 9.597 | Reg loss: 0.005 | Tree loss: 9.597 | Accuracy: 0.072266 | 4.965 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 018 | Total loss: 9.598 | Reg loss: 0.005 | Tree loss: 9.598 | Accuracy: 0.080078 | 4.964 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 018 | Total loss: 9.597 | Reg loss: 0.006 | Tree loss: 9.597 | Accuracy: 0.082031 | 4.962 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 018 | Total loss: 9.597 | Reg loss: 0.006 | Tree loss: 9.597 | Accuracy: 0.070312 | 4.958 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 018 | Total loss: 9.598 | Reg loss: 0.006 | Tree loss: 9.598 | Accuracy: 0.064453 | 4.954 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 018 | Total loss: 9.596 | Reg loss: 0.006 | Tree loss: 9.596 | Accuracy: 0.058594 | 4.95 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 018 | Total loss: 9.597 | Reg loss: 0.006 | Tree loss: 9.597 | Accuracy: 0.066406 | 4.946 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 018 | Total loss: 9.596 | Reg loss: 0.006 | Tree loss: 9.596 | Accuracy: 0.091797 | 4.943 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 018 | Total loss: 9.594 | Reg loss: 0.006 | Tree loss: 9.594 | Accuracy: 0.080078 | 4.94 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 018 | Total loss: 9.594 | Reg loss: 0.007 | Tree loss: 9.594 | Accuracy: 0.074219 | 4.937 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 018 | Total loss: 9.589 | Reg loss: 0.007 | Tree loss: 9.589 | Accuracy: 0.121951 | 4.918 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 018 | Total loss: 9.597 | Reg loss: 0.005 | Tree loss: 9.597 | Accuracy: 0.093750 | 4.971 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 018 | Total loss: 9.597 | Reg loss: 0.005 | Tree loss: 9.597 | Accuracy: 0.091797 | 4.981 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 018 | Total loss: 9.598 | Reg loss: 0.006 | Tree loss: 9.598 | Accuracy: 0.078125 | 4.991 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 018 | Total loss: 9.598 | Reg loss: 0.006 | Tree loss: 9.598 | Accuracy: 0.058594 | 5.0 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 018 | Total loss: 9.598 | Reg loss: 0.006 | Tree loss: 9.598 | Accuracy: 0.062500 | 5.006 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 018 | Total loss: 9.595 | Reg loss: 0.006 | Tree loss: 9.595 | Accuracy: 0.074219 | 5.011 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 018 | Total loss: 9.596 | Reg loss: 0.006 | Tree loss: 9.596 | Accuracy: 0.068359 | 5.013 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 018 | Total loss: 9.596 | Reg loss: 0.006 | Tree loss: 9.596 | Accuracy: 0.070312 | 5.014 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 018 | Total loss: 9.599 | Reg loss: 0.006 | Tree loss: 9.599 | Accuracy: 0.041016 | 5.013 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 018 | Total loss: 9.595 | Reg loss: 0.006 | Tree loss: 9.595 | Accuracy: 0.068359 | 5.01 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 018 | Total loss: 9.594 | Reg loss: 0.007 | Tree loss: 9.594 | Accuracy: 0.082031 | 5.005 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 018 | Total loss: 9.592 | Reg loss: 0.007 | Tree loss: 9.592 | Accuracy: 0.072266 | 5.001 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 018 | Total loss: 9.590 | Reg loss: 0.007 | Tree loss: 9.590 | Accuracy: 0.072266 | 4.998 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 018 | Total loss: 9.590 | Reg loss: 0.007 | Tree loss: 9.590 | Accuracy: 0.080078 | 4.994 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 018 | Total loss: 9.589 | Reg loss: 0.007 | Tree loss: 9.589 | Accuracy: 0.085938 | 4.984 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 018 | Total loss: 9.590 | Reg loss: 0.007 | Tree loss: 9.590 | Accuracy: 0.052734 | 4.991 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 018 | Total loss: 9.589 | Reg loss: 0.008 | Tree loss: 9.589 | Accuracy: 0.087891 | 4.997 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 018 | Total loss: 9.586 | Reg loss: 0.008 | Tree loss: 9.586 | Accuracy: 0.097561 | 4.981 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 018 | Total loss: 9.593 | Reg loss: 0.006 | Tree loss: 9.593 | Accuracy: 0.062500 | 5.005 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 018 | Total loss: 9.594 | Reg loss: 0.006 | Tree loss: 9.594 | Accuracy: 0.064453 | 5.01 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 018 | Total loss: 9.593 | Reg loss: 0.006 | Tree loss: 9.593 | Accuracy: 0.074219 | 5.015 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 018 | Total loss: 9.593 | Reg loss: 0.007 | Tree loss: 9.593 | Accuracy: 0.087891 | 5.021 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 018 | Total loss: 9.590 | Reg loss: 0.007 | Tree loss: 9.590 | Accuracy: 0.066406 | 5.027 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 018 | Total loss: 9.590 | Reg loss: 0.007 | Tree loss: 9.590 | Accuracy: 0.068359 | 5.032 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 018 | Total loss: 9.590 | Reg loss: 0.007 | Tree loss: 9.590 | Accuracy: 0.078125 | 5.037 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 018 | Total loss: 9.589 | Reg loss: 0.007 | Tree loss: 9.589 | Accuracy: 0.072266 | 5.04 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 018 | Total loss: 9.587 | Reg loss: 0.007 | Tree loss: 9.587 | Accuracy: 0.066406 | 5.044 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 018 | Total loss: 9.588 | Reg loss: 0.008 | Tree loss: 9.588 | Accuracy: 0.078125 | 5.046 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 018 | Total loss: 9.583 | Reg loss: 0.008 | Tree loss: 9.583 | Accuracy: 0.083984 | 5.048 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 018 | Total loss: 9.584 | Reg loss: 0.008 | Tree loss: 9.584 | Accuracy: 0.076172 | 5.05 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 018 | Total loss: 9.586 | Reg loss: 0.008 | Tree loss: 9.586 | Accuracy: 0.070312 | 5.05 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 018 | Total loss: 9.583 | Reg loss: 0.008 | Tree loss: 9.583 | Accuracy: 0.064453 | 5.051 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 018 | Total loss: 9.580 | Reg loss: 0.009 | Tree loss: 9.580 | Accuracy: 0.097656 | 5.05 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 018 | Total loss: 9.577 | Reg loss: 0.009 | Tree loss: 9.577 | Accuracy: 0.076172 | 5.049 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 018 | Total loss: 9.577 | Reg loss: 0.009 | Tree loss: 9.577 | Accuracy: 0.062500 | 5.047 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 018 | Total loss: 9.580 | Reg loss: 0.009 | Tree loss: 9.580 | Accuracy: 0.024390 | 5.034 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 018 | Total loss: 9.587 | Reg loss: 0.008 | Tree loss: 9.587 | Accuracy: 0.070312 | 5.067 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 018 | Total loss: 9.586 | Reg loss: 0.008 | Tree loss: 9.586 | Accuracy: 0.054688 | 5.068 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 018 | Total loss: 9.582 | Reg loss: 0.008 | Tree loss: 9.582 | Accuracy: 0.060547 | 5.068 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 018 | Total loss: 9.583 | Reg loss: 0.008 | Tree loss: 9.583 | Accuracy: 0.054688 | 5.064 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 018 | Total loss: 9.581 | Reg loss: 0.008 | Tree loss: 9.581 | Accuracy: 0.072266 | 5.064 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 018 | Total loss: 9.579 | Reg loss: 0.008 | Tree loss: 9.579 | Accuracy: 0.089844 | 5.064 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 018 | Total loss: 9.576 | Reg loss: 0.008 | Tree loss: 9.576 | Accuracy: 0.085938 | 5.064 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 007 / 018 | Total loss: 9.576 | Reg loss: 0.009 | Tree loss: 9.576 | Accuracy: 0.052734 | 5.064 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 018 | Total loss: 9.569 | Reg loss: 0.009 | Tree loss: 9.569 | Accuracy: 0.085938 | 5.063 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 018 | Total loss: 9.576 | Reg loss: 0.009 | Tree loss: 9.576 | Accuracy: 0.076172 | 5.061 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 018 | Total loss: 9.564 | Reg loss: 0.009 | Tree loss: 9.564 | Accuracy: 0.064453 | 5.058 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 018 | Total loss: 9.567 | Reg loss: 0.009 | Tree loss: 9.567 | Accuracy: 0.087891 | 5.055 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 018 | Total loss: 9.565 | Reg loss: 0.010 | Tree loss: 9.565 | Accuracy: 0.062500 | 5.052 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 018 | Total loss: 9.563 | Reg loss: 0.010 | Tree loss: 9.563 | Accuracy: 0.074219 | 5.049 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 018 | Total loss: 9.557 | Reg loss: 0.010 | Tree loss: 9.557 | Accuracy: 0.064453 | 5.046 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 018 | Total loss: 9.557 | Reg loss: 0.011 | Tree loss: 9.557 | Accuracy: 0.066406 | 5.044 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 018 | Total loss: 9.549 | Reg loss: 0.011 | Tree loss: 9.549 | Accuracy: 0.056641 | 5.041 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 018 | Total loss: 9.539 | Reg loss: 0.011 | Tree loss: 9.539 | Accuracy: 0.073171 | 5.029 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 018 | Total loss: 9.571 | Reg loss: 0.009 | Tree loss: 9.571 | Accuracy: 0.074219 | 5.059 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 018 | Total loss: 9.567 | Reg loss: 0.009 | Tree loss: 9.567 | Accuracy: 0.068359 | 5.064 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 018 | Total loss: 9.561 | Reg loss: 0.009 | Tree loss: 9.561 | Accuracy: 0.074219 | 5.067 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 018 | Total loss: 9.563 | Reg loss: 0.009 | Tree loss: 9.563 | Accuracy: 0.074219 | 5.069 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 018 | Total loss: 9.559 | Reg loss: 0.009 | Tree loss: 9.559 | Accuracy: 0.078125 | 5.069 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 018 | Total loss: 9.553 | Reg loss: 0.010 | Tree loss: 9.553 | Accuracy: 0.089844 | 5.069 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 018 | Total loss: 9.554 | Reg loss: 0.010 | Tree loss: 9.554 | Accuracy: 0.058594 | 5.067 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 018 | Total loss: 9.547 | Reg loss: 0.010 | Tree loss: 9.547 | Accuracy: 0.072266 | 5.065 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 018 | Total loss: 9.541 | Reg loss: 0.010 | Tree loss: 9.541 | Accuracy: 0.066406 | 5.062 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 018 | Total loss: 9.537 | Reg loss: 0.010 | Tree loss: 9.537 | Accuracy: 0.070312 | 5.059 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 018 | Total loss: 9.527 | Reg loss: 0.011 | Tree loss: 9.527 | Accuracy: 0.107422 | 5.056 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 018 | Total loss: 9.530 | Reg loss: 0.011 | Tree loss: 9.530 | Accuracy: 0.052734 | 5.053 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 018 | Total loss: 9.524 | Reg loss: 0.011 | Tree loss: 9.524 | Accuracy: 0.068359 | 5.046 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 018 | Total loss: 9.524 | Reg loss: 0.012 | Tree loss: 9.524 | Accuracy: 0.080078 | 5.05 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 018 | Total loss: 9.514 | Reg loss: 0.012 | Tree loss: 9.514 | Accuracy: 0.066406 | 5.053 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 018 | Total loss: 9.509 | Reg loss: 0.012 | Tree loss: 9.509 | Accuracy: 0.080078 | 5.057 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 018 | Total loss: 9.500 | Reg loss: 0.012 | Tree loss: 9.500 | Accuracy: 0.076172 | 5.059 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 018 | Total loss: 9.495 | Reg loss: 0.013 | Tree loss: 9.495 | Accuracy: 0.146341 | 5.049 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 018 | Total loss: 9.534 | Reg loss: 0.010 | Tree loss: 9.534 | Accuracy: 0.078125 | 5.065 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 018 | Total loss: 9.528 | Reg loss: 0.010 | Tree loss: 9.528 | Accuracy: 0.083984 | 5.068 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 018 | Total loss: 9.528 | Reg loss: 0.010 | Tree loss: 9.528 | Accuracy: 0.060547 | 5.071 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 018 | Total loss: 9.516 | Reg loss: 0.010 | Tree loss: 9.516 | Accuracy: 0.060547 | 5.074 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 018 | Total loss: 9.511 | Reg loss: 0.011 | Tree loss: 9.511 | Accuracy: 0.083984 | 5.078 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 018 | Total loss: 9.505 | Reg loss: 0.011 | Tree loss: 9.505 | Accuracy: 0.082031 | 5.08 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 018 | Total loss: 9.502 | Reg loss: 0.011 | Tree loss: 9.502 | Accuracy: 0.076172 | 5.083 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 018 | Total loss: 9.501 | Reg loss: 0.011 | Tree loss: 9.501 | Accuracy: 0.068359 | 5.085 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 018 | Total loss: 9.490 | Reg loss: 0.011 | Tree loss: 9.490 | Accuracy: 0.080078 | 5.087 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 018 | Total loss: 9.484 | Reg loss: 0.012 | Tree loss: 9.484 | Accuracy: 0.074219 | 5.088 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 018 | Total loss: 9.470 | Reg loss: 0.012 | Tree loss: 9.470 | Accuracy: 0.087891 | 5.089 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 018 | Total loss: 9.464 | Reg loss: 0.012 | Tree loss: 9.464 | Accuracy: 0.072266 | 5.09 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 018 | Total loss: 9.458 | Reg loss: 0.013 | Tree loss: 9.458 | Accuracy: 0.068359 | 5.09 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 018 | Total loss: 9.450 | Reg loss: 0.013 | Tree loss: 9.450 | Accuracy: 0.076172 | 5.089 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 018 | Total loss: 9.442 | Reg loss: 0.013 | Tree loss: 9.442 | Accuracy: 0.056641 | 5.089 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 018 | Total loss: 9.437 | Reg loss: 0.014 | Tree loss: 9.437 | Accuracy: 0.070312 | 5.087 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 018 | Total loss: 9.422 | Reg loss: 0.014 | Tree loss: 9.422 | Accuracy: 0.068359 | 5.086 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 018 | Total loss: 9.436 | Reg loss: 0.014 | Tree loss: 9.436 | Accuracy: 0.097561 | 5.076 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 018 | Total loss: 9.471 | Reg loss: 0.011 | Tree loss: 9.471 | Accuracy: 0.078125 | 5.099 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 018 | Total loss: 9.470 | Reg loss: 0.011 | Tree loss: 9.470 | Accuracy: 0.060547 | 5.095 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 018 | Total loss: 9.463 | Reg loss: 0.012 | Tree loss: 9.463 | Accuracy: 0.089844 | 5.095 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 018 | Total loss: 9.452 | Reg loss: 0.012 | Tree loss: 9.452 | Accuracy: 0.068359 | 5.096 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 018 | Total loss: 9.445 | Reg loss: 0.012 | Tree loss: 9.445 | Accuracy: 0.087891 | 5.096 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 018 | Total loss: 9.434 | Reg loss: 0.012 | Tree loss: 9.434 | Accuracy: 0.078125 | 5.096 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 018 | Total loss: 9.414 | Reg loss: 0.012 | Tree loss: 9.414 | Accuracy: 0.080078 | 5.095 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 018 | Total loss: 9.414 | Reg loss: 0.012 | Tree loss: 9.414 | Accuracy: 0.072266 | 5.094 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 018 | Total loss: 9.399 | Reg loss: 0.013 | Tree loss: 9.399 | Accuracy: 0.087891 | 5.092 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 018 | Total loss: 9.393 | Reg loss: 0.013 | Tree loss: 9.393 | Accuracy: 0.070312 | 5.09 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 018 | Total loss: 9.381 | Reg loss: 0.013 | Tree loss: 9.381 | Accuracy: 0.074219 | 5.087 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Batch: 011 / 018 | Total loss: 9.366 | Reg loss: 0.013 | Tree loss: 9.366 | Accuracy: 0.074219 | 5.085 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 018 | Total loss: 9.355 | Reg loss: 0.014 | Tree loss: 9.355 | Accuracy: 0.076172 | 5.082 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 018 | Total loss: 9.355 | Reg loss: 0.014 | Tree loss: 9.355 | Accuracy: 0.054688 | 5.08 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 018 | Total loss: 9.343 | Reg loss: 0.014 | Tree loss: 9.343 | Accuracy: 0.076172 | 5.078 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 018 | Total loss: 9.317 | Reg loss: 0.015 | Tree loss: 9.317 | Accuracy: 0.082031 | 5.076 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 018 | Total loss: 9.317 | Reg loss: 0.015 | Tree loss: 9.317 | Accuracy: 0.074219 | 5.075 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 018 | Total loss: 9.294 | Reg loss: 0.015 | Tree loss: 9.294 | Accuracy: 0.073171 | 5.066 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 018 | Total loss: 9.384 | Reg loss: 0.013 | Tree loss: 9.384 | Accuracy: 0.082031 | 5.087 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 018 | Total loss: 9.373 | Reg loss: 0.013 | Tree loss: 9.373 | Accuracy: 0.070312 | 5.09 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 018 | Total loss: 9.367 | Reg loss: 0.013 | Tree loss: 9.367 | Accuracy: 0.076172 | 5.092 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 018 | Total loss: 9.350 | Reg loss: 0.013 | Tree loss: 9.350 | Accuracy: 0.085938 | 5.093 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 018 | Total loss: 9.353 | Reg loss: 0.013 | Tree loss: 9.353 | Accuracy: 0.076172 | 5.094 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 018 | Total loss: 9.329 | Reg loss: 0.013 | Tree loss: 9.329 | Accuracy: 0.076172 | 5.093 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 018 | Total loss: 9.320 | Reg loss: 0.013 | Tree loss: 9.320 | Accuracy: 0.068359 | 5.092 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 018 | Total loss: 9.299 | Reg loss: 0.014 | Tree loss: 9.299 | Accuracy: 0.074219 | 5.09 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 018 | Total loss: 9.293 | Reg loss: 0.014 | Tree loss: 9.293 | Accuracy: 0.080078 | 5.088 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 018 | Total loss: 9.271 | Reg loss: 0.014 | Tree loss: 9.271 | Accuracy: 0.076172 | 5.085 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 018 | Total loss: 9.244 | Reg loss: 0.014 | Tree loss: 9.244 | Accuracy: 0.056641 | 5.08 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 018 | Total loss: 9.218 | Reg loss: 0.014 | Tree loss: 9.218 | Accuracy: 0.078125 | 5.084 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 018 | Total loss: 9.240 | Reg loss: 0.015 | Tree loss: 9.240 | Accuracy: 0.054688 | 5.087 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 018 | Total loss: 9.209 | Reg loss: 0.015 | Tree loss: 9.209 | Accuracy: 0.093750 | 5.09 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 018 | Total loss: 9.181 | Reg loss: 0.015 | Tree loss: 9.181 | Accuracy: 0.089844 | 5.092 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 018 | Total loss: 9.183 | Reg loss: 0.016 | Tree loss: 9.183 | Accuracy: 0.066406 | 5.093 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 018 | Total loss: 9.164 | Reg loss: 0.016 | Tree loss: 9.164 | Accuracy: 0.083984 | 5.094 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 018 | Total loss: 9.175 | Reg loss: 0.016 | Tree loss: 9.175 | Accuracy: 0.024390 | 5.086 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 018 | Total loss: 9.262 | Reg loss: 0.014 | Tree loss: 9.262 | Accuracy: 0.089844 | 5.101 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 018 | Total loss: 9.249 | Reg loss: 0.014 | Tree loss: 9.249 | Accuracy: 0.082031 | 5.099 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 018 | Total loss: 9.241 | Reg loss: 0.014 | Tree loss: 9.241 | Accuracy: 0.082031 | 5.096 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 018 | Total loss: 9.226 | Reg loss: 0.014 | Tree loss: 9.226 | Accuracy: 0.076172 | 5.093 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 018 | Total loss: 9.206 | Reg loss: 0.014 | Tree loss: 9.206 | Accuracy: 0.076172 | 5.091 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 018 | Total loss: 9.216 | Reg loss: 0.014 | Tree loss: 9.216 | Accuracy: 0.052734 | 5.089 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 018 | Total loss: 9.195 | Reg loss: 0.014 | Tree loss: 9.195 | Accuracy: 0.070312 | 5.087 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 018 | Total loss: 9.163 | Reg loss: 0.014 | Tree loss: 9.163 | Accuracy: 0.044922 | 5.085 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 018 | Total loss: 9.132 | Reg loss: 0.015 | Tree loss: 9.132 | Accuracy: 0.091797 | 5.084 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 018 | Total loss: 9.098 | Reg loss: 0.015 | Tree loss: 9.098 | Accuracy: 0.078125 | 5.082 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 018 | Total loss: 9.097 | Reg loss: 0.015 | Tree loss: 9.097 | Accuracy: 0.072266 | 5.08 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 018 | Total loss: 9.089 | Reg loss: 0.015 | Tree loss: 9.089 | Accuracy: 0.070312 | 5.079 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 018 | Total loss: 9.082 | Reg loss: 0.016 | Tree loss: 9.082 | Accuracy: 0.074219 | 5.077 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 018 | Total loss: 9.071 | Reg loss: 0.016 | Tree loss: 9.071 | Accuracy: 0.068359 | 5.075 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 018 | Total loss: 9.019 | Reg loss: 0.016 | Tree loss: 9.019 | Accuracy: 0.085938 | 5.074 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 018 | Total loss: 9.000 | Reg loss: 0.016 | Tree loss: 9.000 | Accuracy: 0.083984 | 5.072 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 018 | Total loss: 8.990 | Reg loss: 0.017 | Tree loss: 8.990 | Accuracy: 0.085938 | 5.071 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 018 | Total loss: 9.060 | Reg loss: 0.017 | Tree loss: 9.060 | Accuracy: 0.073171 | 5.064 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 018 | Total loss: 9.116 | Reg loss: 0.015 | Tree loss: 9.116 | Accuracy: 0.082031 | 5.066 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 018 | Total loss: 9.119 | Reg loss: 0.015 | Tree loss: 9.119 | Accuracy: 0.070312 | 5.068 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 018 | Total loss: 9.087 | Reg loss: 0.015 | Tree loss: 9.087 | Accuracy: 0.066406 | 5.069 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 018 | Total loss: 9.073 | Reg loss: 0.015 | Tree loss: 9.073 | Accuracy: 0.085938 | 5.07 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 018 | Total loss: 9.047 | Reg loss: 0.015 | Tree loss: 9.047 | Accuracy: 0.070312 | 5.071 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 018 | Total loss: 9.009 | Reg loss: 0.015 | Tree loss: 9.009 | Accuracy: 0.082031 | 5.072 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 018 | Total loss: 9.015 | Reg loss: 0.015 | Tree loss: 9.015 | Accuracy: 0.066406 | 5.072 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 018 | Total loss: 9.021 | Reg loss: 0.015 | Tree loss: 9.021 | Accuracy: 0.068359 | 5.071 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 018 | Total loss: 8.985 | Reg loss: 0.015 | Tree loss: 8.985 | Accuracy: 0.074219 | 5.071 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 018 | Total loss: 8.978 | Reg loss: 0.016 | Tree loss: 8.978 | Accuracy: 0.060547 | 5.069 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 018 | Total loss: 8.929 | Reg loss: 0.016 | Tree loss: 8.929 | Accuracy: 0.080078 | 5.067 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 018 | Total loss: 8.919 | Reg loss: 0.016 | Tree loss: 8.919 | Accuracy: 0.068359 | 5.065 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 018 | Total loss: 8.908 | Reg loss: 0.016 | Tree loss: 8.908 | Accuracy: 0.074219 | 5.064 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 018 | Total loss: 8.881 | Reg loss: 0.016 | Tree loss: 8.881 | Accuracy: 0.087891 | 5.062 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 018 | Total loss: 8.843 | Reg loss: 0.017 | Tree loss: 8.843 | Accuracy: 0.080078 | 5.061 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 015 / 018 | Total loss: 8.846 | Reg loss: 0.017 | Tree loss: 8.846 | Accuracy: 0.064453 | 5.059 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 018 | Total loss: 8.819 | Reg loss: 0.017 | Tree loss: 8.819 | Accuracy: 0.099609 | 5.058 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 018 | Total loss: 8.788 | Reg loss: 0.017 | Tree loss: 8.788 | Accuracy: 0.121951 | 5.052 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 018 | Total loss: 8.983 | Reg loss: 0.015 | Tree loss: 8.983 | Accuracy: 0.076172 | 5.068 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 018 | Total loss: 8.941 | Reg loss: 0.015 | Tree loss: 8.941 | Accuracy: 0.078125 | 5.07 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 018 | Total loss: 8.914 | Reg loss: 0.015 | Tree loss: 8.914 | Accuracy: 0.068359 | 5.073 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 018 | Total loss: 8.926 | Reg loss: 0.016 | Tree loss: 8.926 | Accuracy: 0.068359 | 5.076 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 018 | Total loss: 8.856 | Reg loss: 0.016 | Tree loss: 8.856 | Accuracy: 0.083984 | 5.078 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 018 | Total loss: 8.858 | Reg loss: 0.016 | Tree loss: 8.858 | Accuracy: 0.091797 | 5.08 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 018 | Total loss: 8.838 | Reg loss: 0.016 | Tree loss: 8.838 | Accuracy: 0.068359 | 5.081 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 018 | Total loss: 8.810 | Reg loss: 0.016 | Tree loss: 8.810 | Accuracy: 0.097656 | 5.082 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 018 | Total loss: 8.810 | Reg loss: 0.016 | Tree loss: 8.810 | Accuracy: 0.042969 | 5.08 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 018 | Total loss: 8.759 | Reg loss: 0.016 | Tree loss: 8.759 | Accuracy: 0.062500 | 5.081 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 018 | Total loss: 8.756 | Reg loss: 0.016 | Tree loss: 8.756 | Accuracy: 0.095703 | 5.081 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 018 | Total loss: 8.744 | Reg loss: 0.017 | Tree loss: 8.744 | Accuracy: 0.083984 | 5.082 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 018 | Total loss: 8.718 | Reg loss: 0.017 | Tree loss: 8.718 | Accuracy: 0.068359 | 5.082 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 018 | Total loss: 8.712 | Reg loss: 0.017 | Tree loss: 8.712 | Accuracy: 0.074219 | 5.082 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 018 | Total loss: 8.684 | Reg loss: 0.017 | Tree loss: 8.684 | Accuracy: 0.068359 | 5.082 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 018 | Total loss: 8.642 | Reg loss: 0.017 | Tree loss: 8.642 | Accuracy: 0.066406 | 5.081 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 018 | Total loss: 8.626 | Reg loss: 0.017 | Tree loss: 8.626 | Accuracy: 0.087891 | 5.08 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 018 | Total loss: 8.602 | Reg loss: 0.018 | Tree loss: 8.602 | Accuracy: 0.097561 | 5.074 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 018 | Total loss: 8.782 | Reg loss: 0.016 | Tree loss: 8.782 | Accuracy: 0.072266 | 5.087 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 018 | Total loss: 8.773 | Reg loss: 0.016 | Tree loss: 8.773 | Accuracy: 0.068359 | 5.089 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 018 | Total loss: 8.754 | Reg loss: 0.016 | Tree loss: 8.754 | Accuracy: 0.089844 | 5.091 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 018 | Total loss: 8.704 | Reg loss: 0.016 | Tree loss: 8.704 | Accuracy: 0.082031 | 5.094 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 018 | Total loss: 8.703 | Reg loss: 0.016 | Tree loss: 8.703 | Accuracy: 0.076172 | 5.096 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 018 | Total loss: 8.673 | Reg loss: 0.016 | Tree loss: 8.673 | Accuracy: 0.085938 | 5.098 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 018 | Total loss: 8.672 | Reg loss: 0.016 | Tree loss: 8.672 | Accuracy: 0.060547 | 5.1 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 018 | Total loss: 8.628 | Reg loss: 0.017 | Tree loss: 8.628 | Accuracy: 0.076172 | 5.102 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 018 | Total loss: 8.599 | Reg loss: 0.017 | Tree loss: 8.599 | Accuracy: 0.080078 | 5.104 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 018 | Total loss: 8.595 | Reg loss: 0.017 | Tree loss: 8.595 | Accuracy: 0.085938 | 5.105 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 018 | Total loss: 8.572 | Reg loss: 0.017 | Tree loss: 8.572 | Accuracy: 0.062500 | 5.106 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 018 | Total loss: 8.556 | Reg loss: 0.017 | Tree loss: 8.556 | Accuracy: 0.068359 | 5.107 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 018 | Total loss: 8.519 | Reg loss: 0.017 | Tree loss: 8.519 | Accuracy: 0.082031 | 5.107 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 018 | Total loss: 8.457 | Reg loss: 0.017 | Tree loss: 8.457 | Accuracy: 0.072266 | 5.107 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 018 | Total loss: 8.496 | Reg loss: 0.018 | Tree loss: 8.496 | Accuracy: 0.066406 | 5.107 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 018 | Total loss: 8.456 | Reg loss: 0.018 | Tree loss: 8.456 | Accuracy: 0.087891 | 5.107 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 018 | Total loss: 8.466 | Reg loss: 0.018 | Tree loss: 8.466 | Accuracy: 0.068359 | 5.103 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 018 | Total loss: 8.421 | Reg loss: 0.018 | Tree loss: 8.421 | Accuracy: 0.073171 | 5.097 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 018 | Total loss: 8.610 | Reg loss: 0.017 | Tree loss: 8.610 | Accuracy: 0.060547 | 5.105 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 018 | Total loss: 8.600 | Reg loss: 0.017 | Tree loss: 8.600 | Accuracy: 0.068359 | 5.106 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 018 | Total loss: 8.560 | Reg loss: 0.017 | Tree loss: 8.560 | Accuracy: 0.072266 | 5.106 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 018 | Total loss: 8.529 | Reg loss: 0.017 | Tree loss: 8.529 | Accuracy: 0.080078 | 5.107 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 018 | Total loss: 8.540 | Reg loss: 0.017 | Tree loss: 8.540 | Accuracy: 0.066406 | 5.108 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 018 | Total loss: 8.503 | Reg loss: 0.017 | Tree loss: 8.503 | Accuracy: 0.076172 | 5.109 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 018 | Total loss: 8.466 | Reg loss: 0.017 | Tree loss: 8.466 | Accuracy: 0.087891 | 5.109 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 018 | Total loss: 8.425 | Reg loss: 0.017 | Tree loss: 8.425 | Accuracy: 0.082031 | 5.11 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 018 | Total loss: 8.396 | Reg loss: 0.017 | Tree loss: 8.396 | Accuracy: 0.082031 | 5.11 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 018 | Total loss: 8.408 | Reg loss: 0.017 | Tree loss: 8.408 | Accuracy: 0.072266 | 5.11 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 018 | Total loss: 8.379 | Reg loss: 0.017 | Tree loss: 8.379 | Accuracy: 0.066406 | 5.11 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 018 | Total loss: 8.333 | Reg loss: 0.018 | Tree loss: 8.333 | Accuracy: 0.070312 | 5.109 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 018 | Total loss: 8.317 | Reg loss: 0.018 | Tree loss: 8.317 | Accuracy: 0.085938 | 5.108 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 018 | Total loss: 8.305 | Reg loss: 0.018 | Tree loss: 8.305 | Accuracy: 0.074219 | 5.107 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 018 | Total loss: 8.271 | Reg loss: 0.018 | Tree loss: 8.271 | Accuracy: 0.074219 | 5.106 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 018 | Total loss: 8.239 | Reg loss: 0.018 | Tree loss: 8.239 | Accuracy: 0.095703 | 5.105 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 018 | Total loss: 8.249 | Reg loss: 0.018 | Tree loss: 8.249 | Accuracy: 0.070312 | 5.103 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 018 | Total loss: 8.207 | Reg loss: 0.018 | Tree loss: 8.207 | Accuracy: 0.073171 | 5.098 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 000 / 018 | Total loss: 8.393 | Reg loss: 0.017 | Tree loss: 8.393 | Accuracy: 0.070312 | 5.111 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 018 | Total loss: 8.401 | Reg loss: 0.017 | Tree loss: 8.401 | Accuracy: 0.070312 | 5.112 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 018 | Total loss: 8.366 | Reg loss: 0.017 | Tree loss: 8.366 | Accuracy: 0.068359 | 5.112 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 018 | Total loss: 8.306 | Reg loss: 0.017 | Tree loss: 8.306 | Accuracy: 0.085938 | 5.112 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 018 | Total loss: 8.319 | Reg loss: 0.017 | Tree loss: 8.319 | Accuracy: 0.085938 | 5.112 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 018 | Total loss: 8.305 | Reg loss: 0.017 | Tree loss: 8.305 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 018 | Total loss: 8.251 | Reg loss: 0.017 | Tree loss: 8.251 | Accuracy: 0.080078 | 5.106 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 018 | Total loss: 8.246 | Reg loss: 0.018 | Tree loss: 8.246 | Accuracy: 0.072266 | 5.107 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 018 | Total loss: 8.228 | Reg loss: 0.018 | Tree loss: 8.228 | Accuracy: 0.072266 | 5.108 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 018 | Total loss: 8.196 | Reg loss: 0.018 | Tree loss: 8.196 | Accuracy: 0.082031 | 5.108 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 018 | Total loss: 8.175 | Reg loss: 0.018 | Tree loss: 8.175 | Accuracy: 0.070312 | 5.107 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 018 | Total loss: 8.166 | Reg loss: 0.018 | Tree loss: 8.166 | Accuracy: 0.076172 | 5.106 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 018 | Total loss: 8.115 | Reg loss: 0.018 | Tree loss: 8.115 | Accuracy: 0.087891 | 5.105 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 018 | Total loss: 8.108 | Reg loss: 0.018 | Tree loss: 8.108 | Accuracy: 0.068359 | 5.104 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 018 | Total loss: 8.080 | Reg loss: 0.018 | Tree loss: 8.080 | Accuracy: 0.076172 | 5.102 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 018 | Total loss: 8.081 | Reg loss: 0.018 | Tree loss: 8.081 | Accuracy: 0.074219 | 5.101 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 018 | Total loss: 8.051 | Reg loss: 0.019 | Tree loss: 8.051 | Accuracy: 0.068359 | 5.1 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 018 | Total loss: 8.083 | Reg loss: 0.019 | Tree loss: 8.083 | Accuracy: 0.048780 | 5.095 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 018 | Total loss: 8.246 | Reg loss: 0.018 | Tree loss: 8.246 | Accuracy: 0.080078 | 5.107 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 018 | Total loss: 8.177 | Reg loss: 0.018 | Tree loss: 8.177 | Accuracy: 0.058594 | 5.108 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 018 | Total loss: 8.149 | Reg loss: 0.018 | Tree loss: 8.149 | Accuracy: 0.074219 | 5.108 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 018 | Total loss: 8.123 | Reg loss: 0.018 | Tree loss: 8.123 | Accuracy: 0.093750 | 5.108 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 018 | Total loss: 8.092 | Reg loss: 0.018 | Tree loss: 8.092 | Accuracy: 0.072266 | 5.107 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 018 | Total loss: 8.098 | Reg loss: 0.018 | Tree loss: 8.098 | Accuracy: 0.082031 | 5.106 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 018 | Total loss: 8.048 | Reg loss: 0.018 | Tree loss: 8.048 | Accuracy: 0.080078 | 5.104 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 018 | Total loss: 8.069 | Reg loss: 0.018 | Tree loss: 8.069 | Accuracy: 0.083984 | 5.103 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 018 | Total loss: 8.046 | Reg loss: 0.018 | Tree loss: 8.046 | Accuracy: 0.066406 | 5.102 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 018 | Total loss: 8.031 | Reg loss: 0.018 | Tree loss: 8.031 | Accuracy: 0.078125 | 5.101 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 018 | Total loss: 7.969 | Reg loss: 0.018 | Tree loss: 7.969 | Accuracy: 0.074219 | 5.099 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 018 | Total loss: 7.914 | Reg loss: 0.018 | Tree loss: 7.914 | Accuracy: 0.099609 | 5.098 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 018 | Total loss: 7.927 | Reg loss: 0.018 | Tree loss: 7.927 | Accuracy: 0.078125 | 5.097 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 018 | Total loss: 7.937 | Reg loss: 0.018 | Tree loss: 7.937 | Accuracy: 0.062500 | 5.096 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 018 | Total loss: 7.873 | Reg loss: 0.019 | Tree loss: 7.873 | Accuracy: 0.095703 | 5.093 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 018 | Total loss: 7.891 | Reg loss: 0.019 | Tree loss: 7.891 | Accuracy: 0.054688 | 5.094 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 018 | Total loss: 7.896 | Reg loss: 0.019 | Tree loss: 7.896 | Accuracy: 0.052734 | 5.095 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 018 | Total loss: 7.835 | Reg loss: 0.019 | Tree loss: 7.835 | Accuracy: 0.048780 | 5.09 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 018 | Total loss: 7.986 | Reg loss: 0.018 | Tree loss: 7.986 | Accuracy: 0.085938 | 5.097 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 018 | Total loss: 7.983 | Reg loss: 0.018 | Tree loss: 7.983 | Accuracy: 0.060547 | 5.098 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 018 | Total loss: 7.965 | Reg loss: 0.018 | Tree loss: 7.965 | Accuracy: 0.093750 | 5.099 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 018 | Total loss: 7.963 | Reg loss: 0.018 | Tree loss: 7.963 | Accuracy: 0.066406 | 5.101 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 018 | Total loss: 7.891 | Reg loss: 0.018 | Tree loss: 7.891 | Accuracy: 0.087891 | 5.102 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 018 | Total loss: 7.898 | Reg loss: 0.018 | Tree loss: 7.898 | Accuracy: 0.068359 | 5.103 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 018 | Total loss: 7.856 | Reg loss: 0.018 | Tree loss: 7.856 | Accuracy: 0.076172 | 5.104 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 018 | Total loss: 7.889 | Reg loss: 0.018 | Tree loss: 7.889 | Accuracy: 0.072266 | 5.105 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 018 | Total loss: 7.824 | Reg loss: 0.018 | Tree loss: 7.824 | Accuracy: 0.078125 | 5.106 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 018 | Total loss: 7.808 | Reg loss: 0.018 | Tree loss: 7.808 | Accuracy: 0.066406 | 5.106 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 018 | Total loss: 7.787 | Reg loss: 0.018 | Tree loss: 7.787 | Accuracy: 0.083984 | 5.107 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 018 | Total loss: 7.784 | Reg loss: 0.019 | Tree loss: 7.784 | Accuracy: 0.068359 | 5.107 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 018 | Total loss: 7.737 | Reg loss: 0.019 | Tree loss: 7.737 | Accuracy: 0.087891 | 5.107 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 018 | Total loss: 7.723 | Reg loss: 0.019 | Tree loss: 7.723 | Accuracy: 0.074219 | 5.107 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 018 | Total loss: 7.728 | Reg loss: 0.019 | Tree loss: 7.728 | Accuracy: 0.076172 | 5.106 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 018 | Total loss: 7.695 | Reg loss: 0.019 | Tree loss: 7.695 | Accuracy: 0.072266 | 5.106 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 018 | Total loss: 7.682 | Reg loss: 0.019 | Tree loss: 7.682 | Accuracy: 0.070312 | 5.105 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 018 | Total loss: 7.716 | Reg loss: 0.019 | Tree loss: 7.716 | Accuracy: 0.024390 | 5.101 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 018 | Total loss: 7.793 | Reg loss: 0.018 | Tree loss: 7.793 | Accuracy: 0.080078 | 5.111 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 018 | Total loss: 7.815 | Reg loss: 0.018 | Tree loss: 7.815 | Accuracy: 0.070312 | 5.111 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 018 | Total loss: 7.780 | Reg loss: 0.018 | Tree loss: 7.780 | Accuracy: 0.074219 | 5.111 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 018 | Total loss: 7.751 | Reg loss: 0.018 | Tree loss: 7.751 | Accuracy: 0.076172 | 5.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 004 / 018 | Total loss: 7.723 | Reg loss: 0.018 | Tree loss: 7.723 | Accuracy: 0.060547 | 5.11 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 018 | Total loss: 7.737 | Reg loss: 0.018 | Tree loss: 7.737 | Accuracy: 0.062500 | 5.11 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 018 | Total loss: 7.710 | Reg loss: 0.019 | Tree loss: 7.710 | Accuracy: 0.068359 | 5.11 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 018 | Total loss: 7.669 | Reg loss: 0.019 | Tree loss: 7.669 | Accuracy: 0.062500 | 5.11 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 018 | Total loss: 7.620 | Reg loss: 0.019 | Tree loss: 7.620 | Accuracy: 0.076172 | 5.109 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 018 | Total loss: 7.615 | Reg loss: 0.019 | Tree loss: 7.615 | Accuracy: 0.078125 | 5.109 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 018 | Total loss: 7.593 | Reg loss: 0.019 | Tree loss: 7.593 | Accuracy: 0.099609 | 5.108 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 018 | Total loss: 7.588 | Reg loss: 0.019 | Tree loss: 7.588 | Accuracy: 0.082031 | 5.106 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 018 | Total loss: 7.547 | Reg loss: 0.019 | Tree loss: 7.547 | Accuracy: 0.085938 | 5.105 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 018 | Total loss: 7.536 | Reg loss: 0.019 | Tree loss: 7.536 | Accuracy: 0.072266 | 5.104 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 018 | Total loss: 7.532 | Reg loss: 0.019 | Tree loss: 7.532 | Accuracy: 0.060547 | 5.103 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 018 | Total loss: 7.479 | Reg loss: 0.019 | Tree loss: 7.479 | Accuracy: 0.083984 | 5.102 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 018 | Total loss: 7.504 | Reg loss: 0.019 | Tree loss: 7.504 | Accuracy: 0.085938 | 5.101 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 018 | Total loss: 7.562 | Reg loss: 0.019 | Tree loss: 7.562 | Accuracy: 0.146341 | 5.096 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 018 | Total loss: 7.630 | Reg loss: 0.019 | Tree loss: 7.630 | Accuracy: 0.076172 | 5.107 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 018 | Total loss: 7.613 | Reg loss: 0.019 | Tree loss: 7.613 | Accuracy: 0.078125 | 5.108 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 018 | Total loss: 7.561 | Reg loss: 0.019 | Tree loss: 7.561 | Accuracy: 0.072266 | 5.11 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 018 | Total loss: 7.554 | Reg loss: 0.019 | Tree loss: 7.554 | Accuracy: 0.070312 | 5.111 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 018 | Total loss: 7.518 | Reg loss: 0.019 | Tree loss: 7.518 | Accuracy: 0.091797 | 5.113 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 018 | Total loss: 7.496 | Reg loss: 0.019 | Tree loss: 7.496 | Accuracy: 0.083984 | 5.115 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 018 | Total loss: 7.505 | Reg loss: 0.019 | Tree loss: 7.505 | Accuracy: 0.093750 | 5.117 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 018 | Total loss: 7.465 | Reg loss: 0.019 | Tree loss: 7.465 | Accuracy: 0.085938 | 5.118 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 018 | Total loss: 7.480 | Reg loss: 0.019 | Tree loss: 7.480 | Accuracy: 0.070312 | 5.12 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 018 | Total loss: 7.414 | Reg loss: 0.019 | Tree loss: 7.414 | Accuracy: 0.070312 | 5.121 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 018 | Total loss: 7.423 | Reg loss: 0.019 | Tree loss: 7.423 | Accuracy: 0.082031 | 5.122 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 018 | Total loss: 7.450 | Reg loss: 0.019 | Tree loss: 7.450 | Accuracy: 0.054688 | 5.12 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 018 | Total loss: 7.384 | Reg loss: 0.019 | Tree loss: 7.384 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 018 | Total loss: 7.373 | Reg loss: 0.019 | Tree loss: 7.373 | Accuracy: 0.062500 | 5.122 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 018 | Total loss: 7.355 | Reg loss: 0.019 | Tree loss: 7.355 | Accuracy: 0.066406 | 5.123 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 018 | Total loss: 7.327 | Reg loss: 0.019 | Tree loss: 7.327 | Accuracy: 0.068359 | 5.123 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 018 | Total loss: 7.339 | Reg loss: 0.019 | Tree loss: 7.339 | Accuracy: 0.066406 | 5.124 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 018 | Total loss: 7.235 | Reg loss: 0.020 | Tree loss: 7.235 | Accuracy: 0.097561 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 018 | Total loss: 7.431 | Reg loss: 0.019 | Tree loss: 7.431 | Accuracy: 0.078125 | 5.126 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 018 | Total loss: 7.459 | Reg loss: 0.019 | Tree loss: 7.459 | Accuracy: 0.082031 | 5.127 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 018 | Total loss: 7.393 | Reg loss: 0.019 | Tree loss: 7.393 | Accuracy: 0.064453 | 5.129 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 018 | Total loss: 7.366 | Reg loss: 0.019 | Tree loss: 7.366 | Accuracy: 0.076172 | 5.13 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 018 | Total loss: 7.396 | Reg loss: 0.019 | Tree loss: 7.396 | Accuracy: 0.074219 | 5.131 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 018 | Total loss: 7.383 | Reg loss: 0.019 | Tree loss: 7.383 | Accuracy: 0.062500 | 5.133 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 018 | Total loss: 7.315 | Reg loss: 0.019 | Tree loss: 7.315 | Accuracy: 0.087891 | 5.134 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 018 | Total loss: 7.277 | Reg loss: 0.019 | Tree loss: 7.277 | Accuracy: 0.068359 | 5.135 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 018 | Total loss: 7.285 | Reg loss: 0.019 | Tree loss: 7.285 | Accuracy: 0.085938 | 5.135 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 018 | Total loss: 7.246 | Reg loss: 0.019 | Tree loss: 7.246 | Accuracy: 0.087891 | 5.136 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 018 | Total loss: 7.260 | Reg loss: 0.019 | Tree loss: 7.260 | Accuracy: 0.078125 | 5.136 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 018 | Total loss: 7.207 | Reg loss: 0.019 | Tree loss: 7.207 | Accuracy: 0.074219 | 5.137 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 018 | Total loss: 7.180 | Reg loss: 0.019 | Tree loss: 7.180 | Accuracy: 0.068359 | 5.137 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 018 | Total loss: 7.183 | Reg loss: 0.019 | Tree loss: 7.183 | Accuracy: 0.074219 | 5.137 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 018 | Total loss: 7.161 | Reg loss: 0.020 | Tree loss: 7.161 | Accuracy: 0.074219 | 5.136 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 018 | Total loss: 7.149 | Reg loss: 0.020 | Tree loss: 7.149 | Accuracy: 0.078125 | 5.136 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 018 | Total loss: 7.165 | Reg loss: 0.020 | Tree loss: 7.165 | Accuracy: 0.072266 | 5.135 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 018 | Total loss: 7.197 | Reg loss: 0.020 | Tree loss: 7.197 | Accuracy: 0.048780 | 5.131 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 018 | Total loss: 7.255 | Reg loss: 0.019 | Tree loss: 7.255 | Accuracy: 0.099609 | 5.137 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 018 | Total loss: 7.227 | Reg loss: 0.019 | Tree loss: 7.227 | Accuracy: 0.070312 | 5.137 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 018 | Total loss: 7.270 | Reg loss: 0.019 | Tree loss: 7.270 | Accuracy: 0.085938 | 5.137 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 018 | Total loss: 7.226 | Reg loss: 0.019 | Tree loss: 7.226 | Accuracy: 0.060547 | 5.136 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 018 | Total loss: 7.206 | Reg loss: 0.019 | Tree loss: 7.206 | Accuracy: 0.082031 | 5.136 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 018 | Total loss: 7.167 | Reg loss: 0.019 | Tree loss: 7.167 | Accuracy: 0.070312 | 5.135 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 018 | Total loss: 7.160 | Reg loss: 0.019 | Tree loss: 7.160 | Accuracy: 0.068359 | 5.134 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 018 | Total loss: 7.115 | Reg loss: 0.019 | Tree loss: 7.115 | Accuracy: 0.082031 | 5.133 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batch: 008 / 018 | Total loss: 7.117 | Reg loss: 0.019 | Tree loss: 7.117 | Accuracy: 0.078125 | 5.132 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 018 | Total loss: 7.103 | Reg loss: 0.019 | Tree loss: 7.103 | Accuracy: 0.076172 | 5.131 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 018 | Total loss: 7.103 | Reg loss: 0.020 | Tree loss: 7.103 | Accuracy: 0.052734 | 5.13 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 018 | Total loss: 7.040 | Reg loss: 0.020 | Tree loss: 7.040 | Accuracy: 0.056641 | 5.129 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 018 | Total loss: 7.058 | Reg loss: 0.020 | Tree loss: 7.058 | Accuracy: 0.054688 | 5.128 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 018 | Total loss: 7.021 | Reg loss: 0.020 | Tree loss: 7.021 | Accuracy: 0.080078 | 5.127 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 018 | Total loss: 6.988 | Reg loss: 0.020 | Tree loss: 6.988 | Accuracy: 0.083984 | 5.126 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 018 | Total loss: 6.950 | Reg loss: 0.020 | Tree loss: 6.950 | Accuracy: 0.087891 | 5.125 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 018 | Total loss: 6.928 | Reg loss: 0.020 | Tree loss: 6.928 | Accuracy: 0.093750 | 5.124 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 018 | Total loss: 6.926 | Reg loss: 0.020 | Tree loss: 6.926 | Accuracy: 0.097561 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 018 | Total loss: 7.108 | Reg loss: 0.019 | Tree loss: 7.108 | Accuracy: 0.060547 | 5.129 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 018 | Total loss: 7.086 | Reg loss: 0.019 | Tree loss: 7.086 | Accuracy: 0.064453 | 5.13 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 018 | Total loss: 7.021 | Reg loss: 0.019 | Tree loss: 7.021 | Accuracy: 0.068359 | 5.131 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 018 | Total loss: 7.032 | Reg loss: 0.019 | Tree loss: 7.032 | Accuracy: 0.064453 | 5.131 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 018 | Total loss: 7.069 | Reg loss: 0.019 | Tree loss: 7.069 | Accuracy: 0.060547 | 5.131 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 018 | Total loss: 7.007 | Reg loss: 0.020 | Tree loss: 7.007 | Accuracy: 0.074219 | 5.131 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 018 | Total loss: 6.967 | Reg loss: 0.020 | Tree loss: 6.967 | Accuracy: 0.083984 | 5.13 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 018 | Total loss: 6.966 | Reg loss: 0.020 | Tree loss: 6.966 | Accuracy: 0.107422 | 5.129 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 018 | Total loss: 6.915 | Reg loss: 0.020 | Tree loss: 6.915 | Accuracy: 0.087891 | 5.128 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 018 | Total loss: 6.924 | Reg loss: 0.020 | Tree loss: 6.924 | Accuracy: 0.072266 | 5.125 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 018 | Total loss: 6.877 | Reg loss: 0.020 | Tree loss: 6.877 | Accuracy: 0.095703 | 5.127 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 018 | Total loss: 6.909 | Reg loss: 0.020 | Tree loss: 6.909 | Accuracy: 0.066406 | 5.128 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 018 | Total loss: 6.916 | Reg loss: 0.020 | Tree loss: 6.916 | Accuracy: 0.080078 | 5.129 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 018 | Total loss: 6.860 | Reg loss: 0.020 | Tree loss: 6.860 | Accuracy: 0.060547 | 5.129 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 018 | Total loss: 6.860 | Reg loss: 0.020 | Tree loss: 6.860 | Accuracy: 0.080078 | 5.129 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 018 | Total loss: 6.814 | Reg loss: 0.020 | Tree loss: 6.814 | Accuracy: 0.080078 | 5.129 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 018 | Total loss: 6.785 | Reg loss: 0.020 | Tree loss: 6.785 | Accuracy: 0.078125 | 5.129 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 018 | Total loss: 6.699 | Reg loss: 0.020 | Tree loss: 6.699 | Accuracy: 0.073171 | 5.125 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 018 | Total loss: 6.908 | Reg loss: 0.020 | Tree loss: 6.908 | Accuracy: 0.087891 | 5.132 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 018 | Total loss: 6.881 | Reg loss: 0.020 | Tree loss: 6.881 | Accuracy: 0.083984 | 5.132 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 018 | Total loss: 6.912 | Reg loss: 0.020 | Tree loss: 6.912 | Accuracy: 0.072266 | 5.132 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 018 | Total loss: 6.872 | Reg loss: 0.020 | Tree loss: 6.872 | Accuracy: 0.070312 | 5.13 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 018 | Total loss: 6.884 | Reg loss: 0.020 | Tree loss: 6.884 | Accuracy: 0.080078 | 5.129 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 018 | Total loss: 6.818 | Reg loss: 0.020 | Tree loss: 6.818 | Accuracy: 0.074219 | 5.128 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 018 | Total loss: 6.828 | Reg loss: 0.020 | Tree loss: 6.828 | Accuracy: 0.089844 | 5.127 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 018 | Total loss: 6.813 | Reg loss: 0.020 | Tree loss: 6.813 | Accuracy: 0.080078 | 5.126 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 018 | Total loss: 6.745 | Reg loss: 0.020 | Tree loss: 6.745 | Accuracy: 0.091797 | 5.125 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 018 | Total loss: 6.742 | Reg loss: 0.020 | Tree loss: 6.742 | Accuracy: 0.054688 | 5.124 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 018 | Total loss: 6.758 | Reg loss: 0.020 | Tree loss: 6.758 | Accuracy: 0.060547 | 5.123 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 018 | Total loss: 6.740 | Reg loss: 0.020 | Tree loss: 6.740 | Accuracy: 0.078125 | 5.122 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 018 | Total loss: 6.740 | Reg loss: 0.020 | Tree loss: 6.740 | Accuracy: 0.066406 | 5.122 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 018 | Total loss: 6.694 | Reg loss: 0.020 | Tree loss: 6.694 | Accuracy: 0.060547 | 5.121 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 018 | Total loss: 6.678 | Reg loss: 0.020 | Tree loss: 6.678 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 018 | Total loss: 6.711 | Reg loss: 0.020 | Tree loss: 6.711 | Accuracy: 0.085938 | 5.119 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 018 | Total loss: 6.629 | Reg loss: 0.020 | Tree loss: 6.629 | Accuracy: 0.076172 | 5.118 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 018 | Total loss: 6.703 | Reg loss: 0.020 | Tree loss: 6.703 | Accuracy: 0.048780 | 5.115 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 018 | Total loss: 6.738 | Reg loss: 0.020 | Tree loss: 6.738 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 018 | Total loss: 6.742 | Reg loss: 0.020 | Tree loss: 6.742 | Accuracy: 0.072266 | 5.124 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 018 | Total loss: 6.696 | Reg loss: 0.020 | Tree loss: 6.696 | Accuracy: 0.083984 | 5.124 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 018 | Total loss: 6.688 | Reg loss: 0.020 | Tree loss: 6.688 | Accuracy: 0.062500 | 5.124 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 018 | Total loss: 6.668 | Reg loss: 0.020 | Tree loss: 6.668 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 018 | Total loss: 6.669 | Reg loss: 0.020 | Tree loss: 6.669 | Accuracy: 0.072266 | 5.124 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 018 | Total loss: 6.681 | Reg loss: 0.020 | Tree loss: 6.681 | Accuracy: 0.050781 | 5.124 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 018 | Total loss: 6.658 | Reg loss: 0.020 | Tree loss: 6.658 | Accuracy: 0.095703 | 5.123 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 018 | Total loss: 6.647 | Reg loss: 0.020 | Tree loss: 6.647 | Accuracy: 0.074219 | 5.122 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 018 | Total loss: 6.625 | Reg loss: 0.020 | Tree loss: 6.625 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 018 | Total loss: 6.598 | Reg loss: 0.020 | Tree loss: 6.598 | Accuracy: 0.078125 | 5.12 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 018 | Total loss: 6.571 | Reg loss: 0.020 | Tree loss: 6.571 | Accuracy: 0.095703 | 5.119 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 012 / 018 | Total loss: 6.531 | Reg loss: 0.020 | Tree loss: 6.531 | Accuracy: 0.083984 | 5.118 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 018 | Total loss: 6.555 | Reg loss: 0.020 | Tree loss: 6.555 | Accuracy: 0.082031 | 5.117 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 018 | Total loss: 6.537 | Reg loss: 0.020 | Tree loss: 6.537 | Accuracy: 0.070312 | 5.116 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 018 | Total loss: 6.593 | Reg loss: 0.020 | Tree loss: 6.593 | Accuracy: 0.058594 | 5.116 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 018 | Total loss: 6.535 | Reg loss: 0.020 | Tree loss: 6.535 | Accuracy: 0.060547 | 5.115 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 018 | Total loss: 6.514 | Reg loss: 0.020 | Tree loss: 6.514 | Accuracy: 0.073171 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 018 | Total loss: 6.588 | Reg loss: 0.020 | Tree loss: 6.588 | Accuracy: 0.066406 | 5.119 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 018 | Total loss: 6.589 | Reg loss: 0.020 | Tree loss: 6.589 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 018 | Total loss: 6.594 | Reg loss: 0.020 | Tree loss: 6.594 | Accuracy: 0.085938 | 5.121 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 018 | Total loss: 6.566 | Reg loss: 0.020 | Tree loss: 6.566 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 018 | Total loss: 6.549 | Reg loss: 0.020 | Tree loss: 6.549 | Accuracy: 0.072266 | 5.12 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 018 | Total loss: 6.529 | Reg loss: 0.020 | Tree loss: 6.529 | Accuracy: 0.064453 | 5.12 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 018 | Total loss: 6.517 | Reg loss: 0.020 | Tree loss: 6.517 | Accuracy: 0.068359 | 5.119 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 018 | Total loss: 6.513 | Reg loss: 0.020 | Tree loss: 6.513 | Accuracy: 0.054688 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 018 | Total loss: 6.493 | Reg loss: 0.020 | Tree loss: 6.493 | Accuracy: 0.078125 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 018 | Total loss: 6.455 | Reg loss: 0.020 | Tree loss: 6.455 | Accuracy: 0.085938 | 5.118 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 018 | Total loss: 6.436 | Reg loss: 0.020 | Tree loss: 6.436 | Accuracy: 0.066406 | 5.118 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 018 | Total loss: 6.443 | Reg loss: 0.020 | Tree loss: 6.443 | Accuracy: 0.072266 | 5.118 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 018 | Total loss: 6.369 | Reg loss: 0.020 | Tree loss: 6.369 | Accuracy: 0.109375 | 5.118 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 018 | Total loss: 6.434 | Reg loss: 0.020 | Tree loss: 6.434 | Accuracy: 0.064453 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 018 | Total loss: 6.386 | Reg loss: 0.020 | Tree loss: 6.386 | Accuracy: 0.076172 | 5.116 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 018 | Total loss: 6.362 | Reg loss: 0.020 | Tree loss: 6.362 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 018 | Total loss: 6.357 | Reg loss: 0.020 | Tree loss: 6.357 | Accuracy: 0.078125 | 5.115 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 018 | Total loss: 6.496 | Reg loss: 0.021 | Tree loss: 6.496 | Accuracy: 0.048780 | 5.112 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 018 | Total loss: 6.463 | Reg loss: 0.020 | Tree loss: 6.463 | Accuracy: 0.060547 | 5.119 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 018 | Total loss: 6.455 | Reg loss: 0.020 | Tree loss: 6.455 | Accuracy: 0.072266 | 5.12 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 018 | Total loss: 6.433 | Reg loss: 0.020 | Tree loss: 6.433 | Accuracy: 0.078125 | 5.12 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 018 | Total loss: 6.393 | Reg loss: 0.020 | Tree loss: 6.393 | Accuracy: 0.072266 | 5.12 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 018 | Total loss: 6.370 | Reg loss: 0.020 | Tree loss: 6.370 | Accuracy: 0.082031 | 5.119 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 018 | Total loss: 6.431 | Reg loss: 0.020 | Tree loss: 6.431 | Accuracy: 0.080078 | 5.119 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 018 | Total loss: 6.342 | Reg loss: 0.020 | Tree loss: 6.342 | Accuracy: 0.076172 | 5.118 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 018 | Total loss: 6.360 | Reg loss: 0.020 | Tree loss: 6.360 | Accuracy: 0.078125 | 5.117 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 018 | Total loss: 6.337 | Reg loss: 0.020 | Tree loss: 6.337 | Accuracy: 0.099609 | 5.116 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 018 | Total loss: 6.336 | Reg loss: 0.020 | Tree loss: 6.336 | Accuracy: 0.082031 | 5.115 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 018 | Total loss: 6.308 | Reg loss: 0.020 | Tree loss: 6.308 | Accuracy: 0.064453 | 5.114 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 018 | Total loss: 6.278 | Reg loss: 0.020 | Tree loss: 6.278 | Accuracy: 0.083984 | 5.114 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 018 | Total loss: 6.264 | Reg loss: 0.020 | Tree loss: 6.264 | Accuracy: 0.080078 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 018 | Total loss: 6.293 | Reg loss: 0.020 | Tree loss: 6.293 | Accuracy: 0.072266 | 5.112 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 018 | Total loss: 6.271 | Reg loss: 0.021 | Tree loss: 6.271 | Accuracy: 0.054688 | 5.111 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 018 | Total loss: 6.251 | Reg loss: 0.021 | Tree loss: 6.251 | Accuracy: 0.060547 | 5.109 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 018 | Total loss: 6.175 | Reg loss: 0.021 | Tree loss: 6.175 | Accuracy: 0.085938 | 5.11 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 018 | Total loss: 6.127 | Reg loss: 0.021 | Tree loss: 6.127 | Accuracy: 0.097561 | 5.107 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 018 | Total loss: 6.319 | Reg loss: 0.020 | Tree loss: 6.319 | Accuracy: 0.087891 | 5.111 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 018 | Total loss: 6.308 | Reg loss: 0.020 | Tree loss: 6.308 | Accuracy: 0.087891 | 5.112 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 018 | Total loss: 6.303 | Reg loss: 0.020 | Tree loss: 6.303 | Accuracy: 0.048828 | 5.112 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 018 | Total loss: 6.253 | Reg loss: 0.020 | Tree loss: 6.253 | Accuracy: 0.097656 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 018 | Total loss: 6.271 | Reg loss: 0.020 | Tree loss: 6.271 | Accuracy: 0.083984 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 018 | Total loss: 6.244 | Reg loss: 0.020 | Tree loss: 6.244 | Accuracy: 0.066406 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 018 | Total loss: 6.218 | Reg loss: 0.020 | Tree loss: 6.218 | Accuracy: 0.070312 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 018 | Total loss: 6.190 | Reg loss: 0.020 | Tree loss: 6.190 | Accuracy: 0.076172 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 018 | Total loss: 6.174 | Reg loss: 0.020 | Tree loss: 6.174 | Accuracy: 0.058594 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 018 | Total loss: 6.202 | Reg loss: 0.021 | Tree loss: 6.202 | Accuracy: 0.080078 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 018 | Total loss: 6.176 | Reg loss: 0.021 | Tree loss: 6.176 | Accuracy: 0.070312 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 018 | Total loss: 6.157 | Reg loss: 0.021 | Tree loss: 6.157 | Accuracy: 0.064453 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 018 | Total loss: 6.167 | Reg loss: 0.021 | Tree loss: 6.167 | Accuracy: 0.093750 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 018 | Total loss: 6.134 | Reg loss: 0.021 | Tree loss: 6.134 | Accuracy: 0.068359 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 018 | Total loss: 6.082 | Reg loss: 0.021 | Tree loss: 6.082 | Accuracy: 0.091797 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 018 | Total loss: 6.128 | Reg loss: 0.021 | Tree loss: 6.128 | Accuracy: 0.070312 | 5.114 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 016 / 018 | Total loss: 6.085 | Reg loss: 0.021 | Tree loss: 6.085 | Accuracy: 0.070312 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 018 | Total loss: 6.001 | Reg loss: 0.021 | Tree loss: 6.001 | Accuracy: 0.048780 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 018 | Total loss: 6.209 | Reg loss: 0.021 | Tree loss: 6.209 | Accuracy: 0.072266 | 5.118 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 018 | Total loss: 6.190 | Reg loss: 0.021 | Tree loss: 6.190 | Accuracy: 0.062500 | 5.118 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 018 | Total loss: 6.160 | Reg loss: 0.021 | Tree loss: 6.160 | Accuracy: 0.062500 | 5.118 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 018 | Total loss: 6.136 | Reg loss: 0.021 | Tree loss: 6.136 | Accuracy: 0.076172 | 5.118 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 018 | Total loss: 6.108 | Reg loss: 0.021 | Tree loss: 6.108 | Accuracy: 0.058594 | 5.117 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 018 | Total loss: 6.145 | Reg loss: 0.021 | Tree loss: 6.145 | Accuracy: 0.070312 | 5.115 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 018 | Total loss: 6.034 | Reg loss: 0.021 | Tree loss: 6.034 | Accuracy: 0.097656 | 5.114 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 018 | Total loss: 6.116 | Reg loss: 0.021 | Tree loss: 6.116 | Accuracy: 0.064453 | 5.114 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 018 | Total loss: 6.095 | Reg loss: 0.021 | Tree loss: 6.095 | Accuracy: 0.070312 | 5.113 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 018 | Total loss: 6.072 | Reg loss: 0.021 | Tree loss: 6.072 | Accuracy: 0.085938 | 5.113 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 018 | Total loss: 6.003 | Reg loss: 0.021 | Tree loss: 6.003 | Accuracy: 0.080078 | 5.112 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 018 | Total loss: 5.982 | Reg loss: 0.021 | Tree loss: 5.982 | Accuracy: 0.091797 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 018 | Total loss: 6.009 | Reg loss: 0.021 | Tree loss: 6.009 | Accuracy: 0.074219 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 018 | Total loss: 6.041 | Reg loss: 0.021 | Tree loss: 6.041 | Accuracy: 0.072266 | 5.11 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 018 | Total loss: 6.000 | Reg loss: 0.021 | Tree loss: 6.000 | Accuracy: 0.082031 | 5.109 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 018 | Total loss: 5.942 | Reg loss: 0.021 | Tree loss: 5.942 | Accuracy: 0.080078 | 5.108 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 018 | Total loss: 5.926 | Reg loss: 0.021 | Tree loss: 5.926 | Accuracy: 0.082031 | 5.108 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 018 | Total loss: 5.789 | Reg loss: 0.021 | Tree loss: 5.789 | Accuracy: 0.097561 | 5.105 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 018 | Total loss: 6.055 | Reg loss: 0.021 | Tree loss: 6.055 | Accuracy: 0.062500 | 5.112 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 018 | Total loss: 6.046 | Reg loss: 0.021 | Tree loss: 6.046 | Accuracy: 0.062500 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 018 | Total loss: 6.041 | Reg loss: 0.021 | Tree loss: 6.041 | Accuracy: 0.080078 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 018 | Total loss: 6.010 | Reg loss: 0.021 | Tree loss: 6.010 | Accuracy: 0.078125 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 018 | Total loss: 5.944 | Reg loss: 0.021 | Tree loss: 5.944 | Accuracy: 0.085938 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 018 | Total loss: 5.988 | Reg loss: 0.021 | Tree loss: 5.988 | Accuracy: 0.083984 | 5.112 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 018 | Total loss: 5.973 | Reg loss: 0.021 | Tree loss: 5.973 | Accuracy: 0.082031 | 5.112 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 018 | Total loss: 5.932 | Reg loss: 0.021 | Tree loss: 5.932 | Accuracy: 0.087891 | 5.111 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 018 | Total loss: 5.922 | Reg loss: 0.021 | Tree loss: 5.922 | Accuracy: 0.093750 | 5.11 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 018 | Total loss: 5.924 | Reg loss: 0.021 | Tree loss: 5.924 | Accuracy: 0.068359 | 5.109 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 018 | Total loss: 5.876 | Reg loss: 0.021 | Tree loss: 5.876 | Accuracy: 0.089844 | 5.109 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 018 | Total loss: 5.965 | Reg loss: 0.021 | Tree loss: 5.965 | Accuracy: 0.068359 | 5.108 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 018 | Total loss: 5.911 | Reg loss: 0.021 | Tree loss: 5.911 | Accuracy: 0.072266 | 5.107 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 018 | Total loss: 5.886 | Reg loss: 0.021 | Tree loss: 5.886 | Accuracy: 0.052734 | 5.105 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 018 | Total loss: 5.886 | Reg loss: 0.021 | Tree loss: 5.886 | Accuracy: 0.058594 | 5.106 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 018 | Total loss: 5.847 | Reg loss: 0.021 | Tree loss: 5.847 | Accuracy: 0.068359 | 5.107 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 018 | Total loss: 5.788 | Reg loss: 0.021 | Tree loss: 5.788 | Accuracy: 0.091797 | 5.108 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 018 | Total loss: 5.766 | Reg loss: 0.021 | Tree loss: 5.766 | Accuracy: 0.048780 | 5.105 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 018 | Total loss: 5.951 | Reg loss: 0.021 | Tree loss: 5.951 | Accuracy: 0.068359 | 5.109 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 018 | Total loss: 5.878 | Reg loss: 0.021 | Tree loss: 5.878 | Accuracy: 0.085938 | 5.109 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 018 | Total loss: 5.909 | Reg loss: 0.021 | Tree loss: 5.909 | Accuracy: 0.072266 | 5.11 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 018 | Total loss: 5.891 | Reg loss: 0.021 | Tree loss: 5.891 | Accuracy: 0.058594 | 5.111 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 018 | Total loss: 5.871 | Reg loss: 0.021 | Tree loss: 5.871 | Accuracy: 0.078125 | 5.112 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 018 | Total loss: 5.846 | Reg loss: 0.021 | Tree loss: 5.846 | Accuracy: 0.078125 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 018 | Total loss: 5.823 | Reg loss: 0.021 | Tree loss: 5.823 | Accuracy: 0.062500 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 018 | Total loss: 5.885 | Reg loss: 0.021 | Tree loss: 5.885 | Accuracy: 0.072266 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 018 | Total loss: 5.840 | Reg loss: 0.021 | Tree loss: 5.840 | Accuracy: 0.083984 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 018 | Total loss: 5.786 | Reg loss: 0.021 | Tree loss: 5.786 | Accuracy: 0.054688 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 018 | Total loss: 5.776 | Reg loss: 0.021 | Tree loss: 5.776 | Accuracy: 0.076172 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 018 | Total loss: 5.787 | Reg loss: 0.021 | Tree loss: 5.787 | Accuracy: 0.087891 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 018 | Total loss: 5.715 | Reg loss: 0.021 | Tree loss: 5.715 | Accuracy: 0.095703 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 018 | Total loss: 5.769 | Reg loss: 0.021 | Tree loss: 5.769 | Accuracy: 0.066406 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 018 | Total loss: 5.750 | Reg loss: 0.021 | Tree loss: 5.750 | Accuracy: 0.072266 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 018 | Total loss: 5.718 | Reg loss: 0.021 | Tree loss: 5.718 | Accuracy: 0.087891 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 018 | Total loss: 5.735 | Reg loss: 0.021 | Tree loss: 5.735 | Accuracy: 0.082031 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 018 | Total loss: 5.596 | Reg loss: 0.021 | Tree loss: 5.596 | Accuracy: 0.097561 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 000 / 018 | Total loss: 5.794 | Reg loss: 0.021 | Tree loss: 5.794 | Accuracy: 0.083984 | 5.117 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 018 | Total loss: 5.782 | Reg loss: 0.021 | Tree loss: 5.782 | Accuracy: 0.068359 | 5.117 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 018 | Total loss: 5.765 | Reg loss: 0.021 | Tree loss: 5.765 | Accuracy: 0.072266 | 5.116 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 018 | Total loss: 5.727 | Reg loss: 0.021 | Tree loss: 5.727 | Accuracy: 0.076172 | 5.116 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 018 | Total loss: 5.718 | Reg loss: 0.021 | Tree loss: 5.718 | Accuracy: 0.078125 | 5.116 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 018 | Total loss: 5.743 | Reg loss: 0.021 | Tree loss: 5.743 | Accuracy: 0.076172 | 5.117 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 018 | Total loss: 5.743 | Reg loss: 0.021 | Tree loss: 5.743 | Accuracy: 0.091797 | 5.116 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 018 | Total loss: 5.702 | Reg loss: 0.021 | Tree loss: 5.702 | Accuracy: 0.056641 | 5.116 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 018 | Total loss: 5.696 | Reg loss: 0.021 | Tree loss: 5.696 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 018 | Total loss: 5.654 | Reg loss: 0.021 | Tree loss: 5.654 | Accuracy: 0.074219 | 5.115 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 018 | Total loss: 5.698 | Reg loss: 0.021 | Tree loss: 5.698 | Accuracy: 0.085938 | 5.115 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 018 | Total loss: 5.697 | Reg loss: 0.021 | Tree loss: 5.697 | Accuracy: 0.054688 | 5.114 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 018 | Total loss: 5.637 | Reg loss: 0.021 | Tree loss: 5.637 | Accuracy: 0.074219 | 5.113 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 018 | Total loss: 5.609 | Reg loss: 0.021 | Tree loss: 5.609 | Accuracy: 0.093750 | 5.112 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 018 | Total loss: 5.710 | Reg loss: 0.021 | Tree loss: 5.710 | Accuracy: 0.058594 | 5.112 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 018 | Total loss: 5.658 | Reg loss: 0.021 | Tree loss: 5.658 | Accuracy: 0.085938 | 5.111 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 018 | Total loss: 5.602 | Reg loss: 0.021 | Tree loss: 5.602 | Accuracy: 0.076172 | 5.11 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 018 | Total loss: 5.581 | Reg loss: 0.021 | Tree loss: 5.581 | Accuracy: 0.048780 | 5.108 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 018 | Total loss: 5.682 | Reg loss: 0.021 | Tree loss: 5.682 | Accuracy: 0.070312 | 5.114 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 018 | Total loss: 5.650 | Reg loss: 0.021 | Tree loss: 5.650 | Accuracy: 0.076172 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 018 | Total loss: 5.650 | Reg loss: 0.021 | Tree loss: 5.650 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 018 | Total loss: 5.614 | Reg loss: 0.021 | Tree loss: 5.614 | Accuracy: 0.093750 | 5.117 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 018 | Total loss: 5.658 | Reg loss: 0.021 | Tree loss: 5.658 | Accuracy: 0.072266 | 5.117 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 018 | Total loss: 5.599 | Reg loss: 0.021 | Tree loss: 5.599 | Accuracy: 0.083984 | 5.118 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 018 | Total loss: 5.602 | Reg loss: 0.021 | Tree loss: 5.602 | Accuracy: 0.082031 | 5.118 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 018 | Total loss: 5.590 | Reg loss: 0.021 | Tree loss: 5.590 | Accuracy: 0.070312 | 5.117 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 018 | Total loss: 5.628 | Reg loss: 0.021 | Tree loss: 5.628 | Accuracy: 0.064453 | 5.117 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 018 | Total loss: 5.595 | Reg loss: 0.021 | Tree loss: 5.595 | Accuracy: 0.062500 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 018 | Total loss: 5.543 | Reg loss: 0.021 | Tree loss: 5.543 | Accuracy: 0.093750 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 018 | Total loss: 5.572 | Reg loss: 0.021 | Tree loss: 5.572 | Accuracy: 0.080078 | 5.114 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 018 | Total loss: 5.530 | Reg loss: 0.021 | Tree loss: 5.530 | Accuracy: 0.076172 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 018 | Total loss: 5.581 | Reg loss: 0.021 | Tree loss: 5.581 | Accuracy: 0.062500 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 018 | Total loss: 5.529 | Reg loss: 0.021 | Tree loss: 5.529 | Accuracy: 0.076172 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 018 | Total loss: 5.511 | Reg loss: 0.021 | Tree loss: 5.511 | Accuracy: 0.070312 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 018 | Total loss: 5.494 | Reg loss: 0.021 | Tree loss: 5.494 | Accuracy: 0.074219 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 018 | Total loss: 5.530 | Reg loss: 0.021 | Tree loss: 5.530 | Accuracy: 0.024390 | 5.114 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 018 | Total loss: 5.586 | Reg loss: 0.021 | Tree loss: 5.586 | Accuracy: 0.056641 | 5.119 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 018 | Total loss: 5.554 | Reg loss: 0.021 | Tree loss: 5.554 | Accuracy: 0.078125 | 5.118 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 018 | Total loss: 5.550 | Reg loss: 0.021 | Tree loss: 5.550 | Accuracy: 0.076172 | 5.118 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 018 | Total loss: 5.500 | Reg loss: 0.021 | Tree loss: 5.500 | Accuracy: 0.087891 | 5.117 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 018 | Total loss: 5.566 | Reg loss: 0.021 | Tree loss: 5.566 | Accuracy: 0.070312 | 5.116 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 018 | Total loss: 5.512 | Reg loss: 0.021 | Tree loss: 5.512 | Accuracy: 0.054688 | 5.115 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 018 | Total loss: 5.495 | Reg loss: 0.021 | Tree loss: 5.495 | Accuracy: 0.056641 | 5.115 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 018 | Total loss: 5.463 | Reg loss: 0.021 | Tree loss: 5.463 | Accuracy: 0.087891 | 5.114 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 018 | Total loss: 5.502 | Reg loss: 0.021 | Tree loss: 5.502 | Accuracy: 0.064453 | 5.113 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 018 | Total loss: 5.455 | Reg loss: 0.021 | Tree loss: 5.455 | Accuracy: 0.089844 | 5.113 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 018 | Total loss: 5.485 | Reg loss: 0.021 | Tree loss: 5.485 | Accuracy: 0.089844 | 5.112 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 018 | Total loss: 5.480 | Reg loss: 0.021 | Tree loss: 5.480 | Accuracy: 0.078125 | 5.112 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 018 | Total loss: 5.453 | Reg loss: 0.021 | Tree loss: 5.453 | Accuracy: 0.064453 | 5.111 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 018 | Total loss: 5.444 | Reg loss: 0.021 | Tree loss: 5.444 | Accuracy: 0.064453 | 5.11 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 018 | Total loss: 5.412 | Reg loss: 0.021 | Tree loss: 5.412 | Accuracy: 0.091797 | 5.11 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 018 | Total loss: 5.356 | Reg loss: 0.021 | Tree loss: 5.356 | Accuracy: 0.093750 | 5.109 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 018 | Total loss: 5.416 | Reg loss: 0.021 | Tree loss: 5.416 | Accuracy: 0.078125 | 5.109 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 018 | Total loss: 5.318 | Reg loss: 0.021 | Tree loss: 5.318 | Accuracy: 0.097561 | 5.106 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 018 | Total loss: 5.407 | Reg loss: 0.021 | Tree loss: 5.407 | Accuracy: 0.101562 | 5.11 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 018 | Total loss: 5.461 | Reg loss: 0.021 | Tree loss: 5.461 | Accuracy: 0.066406 | 5.11 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 018 | Total loss: 5.487 | Reg loss: 0.021 | Tree loss: 5.487 | Accuracy: 0.052734 | 5.11 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 018 | Total loss: 5.459 | Reg loss: 0.021 | Tree loss: 5.459 | Accuracy: 0.072266 | 5.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 004 / 018 | Total loss: 5.465 | Reg loss: 0.021 | Tree loss: 5.465 | Accuracy: 0.062500 | 5.109 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 018 | Total loss: 5.398 | Reg loss: 0.021 | Tree loss: 5.398 | Accuracy: 0.099609 | 5.109 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 018 | Total loss: 5.355 | Reg loss: 0.021 | Tree loss: 5.355 | Accuracy: 0.082031 | 5.108 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 018 | Total loss: 5.379 | Reg loss: 0.021 | Tree loss: 5.379 | Accuracy: 0.078125 | 5.108 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 018 | Total loss: 5.368 | Reg loss: 0.021 | Tree loss: 5.368 | Accuracy: 0.066406 | 5.107 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 018 | Total loss: 5.423 | Reg loss: 0.021 | Tree loss: 5.423 | Accuracy: 0.050781 | 5.106 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 018 | Total loss: 5.321 | Reg loss: 0.021 | Tree loss: 5.321 | Accuracy: 0.083984 | 5.106 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 018 | Total loss: 5.382 | Reg loss: 0.021 | Tree loss: 5.382 | Accuracy: 0.070312 | 5.105 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 018 | Total loss: 5.345 | Reg loss: 0.021 | Tree loss: 5.345 | Accuracy: 0.089844 | 5.104 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 018 | Total loss: 5.333 | Reg loss: 0.021 | Tree loss: 5.333 | Accuracy: 0.095703 | 5.104 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 018 | Total loss: 5.299 | Reg loss: 0.021 | Tree loss: 5.299 | Accuracy: 0.064453 | 5.103 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 018 | Total loss: 5.310 | Reg loss: 0.021 | Tree loss: 5.310 | Accuracy: 0.082031 | 5.103 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 018 | Total loss: 5.303 | Reg loss: 0.021 | Tree loss: 5.303 | Accuracy: 0.060547 | 5.102 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 018 | Total loss: 5.157 | Reg loss: 0.022 | Tree loss: 5.157 | Accuracy: 0.146341 | 5.1 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 018 | Total loss: 5.346 | Reg loss: 0.021 | Tree loss: 5.346 | Accuracy: 0.085938 | 5.105 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 018 | Total loss: 5.308 | Reg loss: 0.021 | Tree loss: 5.308 | Accuracy: 0.089844 | 5.105 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 018 | Total loss: 5.341 | Reg loss: 0.021 | Tree loss: 5.341 | Accuracy: 0.074219 | 5.105 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 018 | Total loss: 5.341 | Reg loss: 0.021 | Tree loss: 5.341 | Accuracy: 0.078125 | 5.105 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 018 | Total loss: 5.365 | Reg loss: 0.021 | Tree loss: 5.365 | Accuracy: 0.058594 | 5.104 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 018 | Total loss: 5.299 | Reg loss: 0.021 | Tree loss: 5.299 | Accuracy: 0.082031 | 5.104 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 018 | Total loss: 5.322 | Reg loss: 0.021 | Tree loss: 5.322 | Accuracy: 0.062500 | 5.103 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 018 | Total loss: 5.290 | Reg loss: 0.021 | Tree loss: 5.290 | Accuracy: 0.078125 | 5.102 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 018 | Total loss: 5.292 | Reg loss: 0.021 | Tree loss: 5.292 | Accuracy: 0.074219 | 5.102 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 018 | Total loss: 5.308 | Reg loss: 0.021 | Tree loss: 5.308 | Accuracy: 0.068359 | 5.1 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 018 | Total loss: 5.296 | Reg loss: 0.021 | Tree loss: 5.296 | Accuracy: 0.072266 | 5.101 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 018 | Total loss: 5.233 | Reg loss: 0.022 | Tree loss: 5.233 | Accuracy: 0.089844 | 5.101 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 018 | Total loss: 5.226 | Reg loss: 0.022 | Tree loss: 5.226 | Accuracy: 0.074219 | 5.102 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 018 | Total loss: 5.254 | Reg loss: 0.022 | Tree loss: 5.254 | Accuracy: 0.078125 | 5.102 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 018 | Total loss: 5.239 | Reg loss: 0.022 | Tree loss: 5.239 | Accuracy: 0.068359 | 5.103 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 018 | Total loss: 5.215 | Reg loss: 0.022 | Tree loss: 5.215 | Accuracy: 0.070312 | 5.103 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 018 | Total loss: 5.144 | Reg loss: 0.022 | Tree loss: 5.144 | Accuracy: 0.082031 | 5.103 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 018 | Total loss: 5.345 | Reg loss: 0.022 | Tree loss: 5.345 | Accuracy: 0.048780 | 5.101 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 018 | Total loss: 5.254 | Reg loss: 0.021 | Tree loss: 5.254 | Accuracy: 0.099609 | 5.105 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 018 | Total loss: 5.273 | Reg loss: 0.021 | Tree loss: 5.273 | Accuracy: 0.074219 | 5.105 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 018 | Total loss: 5.288 | Reg loss: 0.021 | Tree loss: 5.288 | Accuracy: 0.066406 | 5.106 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 018 | Total loss: 5.212 | Reg loss: 0.022 | Tree loss: 5.212 | Accuracy: 0.097656 | 5.107 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 018 | Total loss: 5.268 | Reg loss: 0.022 | Tree loss: 5.268 | Accuracy: 0.056641 | 5.108 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 018 | Total loss: 5.190 | Reg loss: 0.022 | Tree loss: 5.190 | Accuracy: 0.080078 | 5.109 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 018 | Total loss: 5.210 | Reg loss: 0.022 | Tree loss: 5.210 | Accuracy: 0.060547 | 5.11 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 018 | Total loss: 5.242 | Reg loss: 0.022 | Tree loss: 5.242 | Accuracy: 0.078125 | 5.111 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 018 | Total loss: 5.150 | Reg loss: 0.022 | Tree loss: 5.150 | Accuracy: 0.072266 | 5.111 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 018 | Total loss: 5.188 | Reg loss: 0.022 | Tree loss: 5.188 | Accuracy: 0.076172 | 5.112 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 018 | Total loss: 5.189 | Reg loss: 0.022 | Tree loss: 5.189 | Accuracy: 0.068359 | 5.113 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 018 | Total loss: 5.123 | Reg loss: 0.022 | Tree loss: 5.123 | Accuracy: 0.082031 | 5.113 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 018 | Total loss: 5.181 | Reg loss: 0.022 | Tree loss: 5.181 | Accuracy: 0.062500 | 5.113 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 018 | Total loss: 5.142 | Reg loss: 0.022 | Tree loss: 5.142 | Accuracy: 0.085938 | 5.113 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 018 | Total loss: 5.092 | Reg loss: 0.022 | Tree loss: 5.092 | Accuracy: 0.101562 | 5.113 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 018 | Total loss: 5.132 | Reg loss: 0.022 | Tree loss: 5.132 | Accuracy: 0.056641 | 5.113 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 018 | Total loss: 5.129 | Reg loss: 0.022 | Tree loss: 5.129 | Accuracy: 0.066406 | 5.113 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 018 | Total loss: 5.050 | Reg loss: 0.022 | Tree loss: 5.050 | Accuracy: 0.073171 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 018 | Total loss: 5.183 | Reg loss: 0.022 | Tree loss: 5.183 | Accuracy: 0.083984 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 018 | Total loss: 5.164 | Reg loss: 0.022 | Tree loss: 5.164 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 018 | Total loss: 5.146 | Reg loss: 0.022 | Tree loss: 5.146 | Accuracy: 0.093750 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 018 | Total loss: 5.142 | Reg loss: 0.022 | Tree loss: 5.142 | Accuracy: 0.076172 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 018 | Total loss: 5.114 | Reg loss: 0.022 | Tree loss: 5.114 | Accuracy: 0.080078 | 5.115 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 018 | Total loss: 5.161 | Reg loss: 0.022 | Tree loss: 5.161 | Accuracy: 0.062500 | 5.114 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 018 | Total loss: 5.077 | Reg loss: 0.022 | Tree loss: 5.077 | Accuracy: 0.078125 | 5.114 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 018 | Total loss: 5.108 | Reg loss: 0.022 | Tree loss: 5.108 | Accuracy: 0.072266 | 5.113 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Batch: 008 / 018 | Total loss: 5.092 | Reg loss: 0.022 | Tree loss: 5.092 | Accuracy: 0.091797 | 5.112 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 018 | Total loss: 5.098 | Reg loss: 0.022 | Tree loss: 5.098 | Accuracy: 0.066406 | 5.112 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 018 | Total loss: 5.107 | Reg loss: 0.022 | Tree loss: 5.107 | Accuracy: 0.070312 | 5.111 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 018 | Total loss: 5.060 | Reg loss: 0.022 | Tree loss: 5.060 | Accuracy: 0.078125 | 5.111 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 018 | Total loss: 5.079 | Reg loss: 0.022 | Tree loss: 5.079 | Accuracy: 0.074219 | 5.11 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 018 | Total loss: 5.059 | Reg loss: 0.022 | Tree loss: 5.059 | Accuracy: 0.064453 | 5.11 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 018 | Total loss: 5.065 | Reg loss: 0.022 | Tree loss: 5.065 | Accuracy: 0.068359 | 5.109 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 018 | Total loss: 5.032 | Reg loss: 0.022 | Tree loss: 5.032 | Accuracy: 0.072266 | 5.109 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 018 | Total loss: 5.068 | Reg loss: 0.022 | Tree loss: 5.068 | Accuracy: 0.072266 | 5.108 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 018 | Total loss: 5.153 | Reg loss: 0.022 | Tree loss: 5.153 | Accuracy: 0.073171 | 5.106 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 018 | Total loss: 5.037 | Reg loss: 0.022 | Tree loss: 5.037 | Accuracy: 0.064453 | 5.111 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 018 | Total loss: 5.092 | Reg loss: 0.022 | Tree loss: 5.092 | Accuracy: 0.060547 | 5.112 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 018 | Total loss: 5.046 | Reg loss: 0.022 | Tree loss: 5.046 | Accuracy: 0.082031 | 5.112 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 018 | Total loss: 5.099 | Reg loss: 0.022 | Tree loss: 5.099 | Accuracy: 0.080078 | 5.112 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 018 | Total loss: 5.048 | Reg loss: 0.022 | Tree loss: 5.048 | Accuracy: 0.076172 | 5.112 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 018 | Total loss: 5.034 | Reg loss: 0.022 | Tree loss: 5.034 | Accuracy: 0.064453 | 5.111 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 018 | Total loss: 5.056 | Reg loss: 0.022 | Tree loss: 5.056 | Accuracy: 0.066406 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 018 | Total loss: 4.995 | Reg loss: 0.022 | Tree loss: 4.995 | Accuracy: 0.068359 | 5.109 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 018 | Total loss: 5.059 | Reg loss: 0.022 | Tree loss: 5.059 | Accuracy: 0.072266 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 018 | Total loss: 5.024 | Reg loss: 0.022 | Tree loss: 5.024 | Accuracy: 0.082031 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 018 | Total loss: 4.993 | Reg loss: 0.022 | Tree loss: 4.993 | Accuracy: 0.095703 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 018 | Total loss: 4.992 | Reg loss: 0.022 | Tree loss: 4.992 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 018 | Total loss: 5.004 | Reg loss: 0.022 | Tree loss: 5.004 | Accuracy: 0.074219 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 018 | Total loss: 4.994 | Reg loss: 0.022 | Tree loss: 4.994 | Accuracy: 0.085938 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 018 | Total loss: 4.938 | Reg loss: 0.022 | Tree loss: 4.938 | Accuracy: 0.103516 | 5.11 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 018 | Total loss: 4.927 | Reg loss: 0.022 | Tree loss: 4.927 | Accuracy: 0.064453 | 5.109 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 018 | Total loss: 4.987 | Reg loss: 0.022 | Tree loss: 4.987 | Accuracy: 0.068359 | 5.109 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 018 | Total loss: 5.162 | Reg loss: 0.022 | Tree loss: 5.162 | Accuracy: 0.048780 | 5.106 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 018 | Total loss: 4.990 | Reg loss: 0.022 | Tree loss: 4.990 | Accuracy: 0.085938 | 5.112 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 018 | Total loss: 4.981 | Reg loss: 0.022 | Tree loss: 4.981 | Accuracy: 0.078125 | 5.112 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 018 | Total loss: 4.969 | Reg loss: 0.022 | Tree loss: 4.969 | Accuracy: 0.062500 | 5.112 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 018 | Total loss: 4.994 | Reg loss: 0.022 | Tree loss: 4.994 | Accuracy: 0.083984 | 5.112 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 018 | Total loss: 4.944 | Reg loss: 0.022 | Tree loss: 4.944 | Accuracy: 0.078125 | 5.112 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 018 | Total loss: 4.926 | Reg loss: 0.022 | Tree loss: 4.926 | Accuracy: 0.066406 | 5.111 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 018 | Total loss: 4.968 | Reg loss: 0.022 | Tree loss: 4.968 | Accuracy: 0.062500 | 5.11 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 018 | Total loss: 4.943 | Reg loss: 0.022 | Tree loss: 4.943 | Accuracy: 0.068359 | 5.11 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 018 | Total loss: 4.949 | Reg loss: 0.022 | Tree loss: 4.949 | Accuracy: 0.091797 | 5.109 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 018 | Total loss: 4.978 | Reg loss: 0.022 | Tree loss: 4.978 | Accuracy: 0.064453 | 5.109 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 018 | Total loss: 4.955 | Reg loss: 0.022 | Tree loss: 4.955 | Accuracy: 0.066406 | 5.108 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 018 | Total loss: 4.922 | Reg loss: 0.022 | Tree loss: 4.922 | Accuracy: 0.064453 | 5.108 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 018 | Total loss: 4.948 | Reg loss: 0.022 | Tree loss: 4.948 | Accuracy: 0.074219 | 5.107 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 018 | Total loss: 4.881 | Reg loss: 0.022 | Tree loss: 4.881 | Accuracy: 0.082031 | 5.107 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 018 | Total loss: 4.896 | Reg loss: 0.022 | Tree loss: 4.896 | Accuracy: 0.076172 | 5.106 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 018 | Total loss: 4.850 | Reg loss: 0.022 | Tree loss: 4.850 | Accuracy: 0.099609 | 5.105 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 018 | Total loss: 4.877 | Reg loss: 0.022 | Tree loss: 4.877 | Accuracy: 0.083984 | 5.105 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 018 | Total loss: 5.121 | Reg loss: 0.022 | Tree loss: 5.121 | Accuracy: 0.024390 | 5.103 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 018 | Total loss: 5.007 | Reg loss: 0.022 | Tree loss: 5.007 | Accuracy: 0.087891 | 5.106 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 018 | Total loss: 4.945 | Reg loss: 0.022 | Tree loss: 4.945 | Accuracy: 0.066406 | 5.106 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 018 | Total loss: 4.898 | Reg loss: 0.022 | Tree loss: 4.898 | Accuracy: 0.085938 | 5.107 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 018 | Total loss: 4.901 | Reg loss: 0.022 | Tree loss: 4.901 | Accuracy: 0.074219 | 5.107 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 018 | Total loss: 4.878 | Reg loss: 0.022 | Tree loss: 4.878 | Accuracy: 0.068359 | 5.108 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 018 | Total loss: 4.878 | Reg loss: 0.022 | Tree loss: 4.878 | Accuracy: 0.093750 | 5.108 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 018 | Total loss: 4.849 | Reg loss: 0.022 | Tree loss: 4.849 | Accuracy: 0.093750 | 5.109 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 018 | Total loss: 4.816 | Reg loss: 0.022 | Tree loss: 4.816 | Accuracy: 0.076172 | 5.109 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 018 | Total loss: 4.845 | Reg loss: 0.022 | Tree loss: 4.845 | Accuracy: 0.064453 | 5.109 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 018 | Total loss: 4.865 | Reg loss: 0.022 | Tree loss: 4.865 | Accuracy: 0.076172 | 5.11 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 018 | Total loss: 4.894 | Reg loss: 0.022 | Tree loss: 4.894 | Accuracy: 0.076172 | 5.11 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 018 | Total loss: 4.875 | Reg loss: 0.022 | Tree loss: 4.875 | Accuracy: 0.054688 | 5.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 012 / 018 | Total loss: 4.823 | Reg loss: 0.022 | Tree loss: 4.823 | Accuracy: 0.052734 | 5.11 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 018 | Total loss: 4.883 | Reg loss: 0.022 | Tree loss: 4.883 | Accuracy: 0.072266 | 5.11 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 018 | Total loss: 4.784 | Reg loss: 0.022 | Tree loss: 4.784 | Accuracy: 0.091797 | 5.11 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 018 | Total loss: 4.799 | Reg loss: 0.022 | Tree loss: 4.799 | Accuracy: 0.076172 | 5.109 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 018 | Total loss: 4.768 | Reg loss: 0.022 | Tree loss: 4.768 | Accuracy: 0.078125 | 5.109 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 018 | Total loss: 4.889 | Reg loss: 0.022 | Tree loss: 4.889 | Accuracy: 0.024390 | 5.107 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 018 | Total loss: 4.821 | Reg loss: 0.022 | Tree loss: 4.821 | Accuracy: 0.093750 | 5.112 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 018 | Total loss: 4.822 | Reg loss: 0.022 | Tree loss: 4.822 | Accuracy: 0.080078 | 5.112 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 018 | Total loss: 4.834 | Reg loss: 0.022 | Tree loss: 4.834 | Accuracy: 0.072266 | 5.112 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 018 | Total loss: 4.832 | Reg loss: 0.022 | Tree loss: 4.832 | Accuracy: 0.074219 | 5.112 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 018 | Total loss: 4.828 | Reg loss: 0.022 | Tree loss: 4.828 | Accuracy: 0.083984 | 5.111 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 018 | Total loss: 4.841 | Reg loss: 0.022 | Tree loss: 4.841 | Accuracy: 0.064453 | 5.11 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 018 | Total loss: 4.793 | Reg loss: 0.022 | Tree loss: 4.793 | Accuracy: 0.074219 | 5.11 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 018 | Total loss: 4.791 | Reg loss: 0.022 | Tree loss: 4.791 | Accuracy: 0.082031 | 5.11 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 018 | Total loss: 4.762 | Reg loss: 0.022 | Tree loss: 4.762 | Accuracy: 0.080078 | 5.109 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 018 | Total loss: 4.789 | Reg loss: 0.022 | Tree loss: 4.789 | Accuracy: 0.074219 | 5.109 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 018 | Total loss: 4.763 | Reg loss: 0.022 | Tree loss: 4.763 | Accuracy: 0.064453 | 5.109 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 018 | Total loss: 4.774 | Reg loss: 0.022 | Tree loss: 4.774 | Accuracy: 0.076172 | 5.108 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 018 | Total loss: 4.778 | Reg loss: 0.022 | Tree loss: 4.778 | Accuracy: 0.074219 | 5.108 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 018 | Total loss: 4.765 | Reg loss: 0.022 | Tree loss: 4.765 | Accuracy: 0.074219 | 5.107 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 018 | Total loss: 4.736 | Reg loss: 0.022 | Tree loss: 4.736 | Accuracy: 0.082031 | 5.107 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 018 | Total loss: 4.817 | Reg loss: 0.022 | Tree loss: 4.817 | Accuracy: 0.074219 | 5.106 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 018 | Total loss: 4.772 | Reg loss: 0.022 | Tree loss: 4.772 | Accuracy: 0.056641 | 5.106 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 018 | Total loss: 4.564 | Reg loss: 0.022 | Tree loss: 4.564 | Accuracy: 0.121951 | 5.104 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 018 | Total loss: 4.740 | Reg loss: 0.022 | Tree loss: 4.740 | Accuracy: 0.070312 | 5.109 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 018 | Total loss: 4.744 | Reg loss: 0.022 | Tree loss: 4.744 | Accuracy: 0.101562 | 5.109 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 018 | Total loss: 4.759 | Reg loss: 0.022 | Tree loss: 4.759 | Accuracy: 0.082031 | 5.11 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 018 | Total loss: 4.734 | Reg loss: 0.022 | Tree loss: 4.734 | Accuracy: 0.080078 | 5.11 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 018 | Total loss: 4.737 | Reg loss: 0.022 | Tree loss: 4.737 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 018 | Total loss: 4.787 | Reg loss: 0.022 | Tree loss: 4.787 | Accuracy: 0.068359 | 5.11 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 018 | Total loss: 4.773 | Reg loss: 0.022 | Tree loss: 4.773 | Accuracy: 0.066406 | 5.11 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 018 | Total loss: 4.741 | Reg loss: 0.022 | Tree loss: 4.741 | Accuracy: 0.056641 | 5.109 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 018 | Total loss: 4.720 | Reg loss: 0.022 | Tree loss: 4.720 | Accuracy: 0.058594 | 5.109 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 018 | Total loss: 4.747 | Reg loss: 0.022 | Tree loss: 4.747 | Accuracy: 0.076172 | 5.108 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 018 | Total loss: 4.667 | Reg loss: 0.022 | Tree loss: 4.667 | Accuracy: 0.085938 | 5.108 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 018 | Total loss: 4.717 | Reg loss: 0.022 | Tree loss: 4.717 | Accuracy: 0.058594 | 5.107 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 018 | Total loss: 4.703 | Reg loss: 0.022 | Tree loss: 4.703 | Accuracy: 0.068359 | 5.107 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 018 | Total loss: 4.703 | Reg loss: 0.022 | Tree loss: 4.703 | Accuracy: 0.083984 | 5.105 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 018 | Total loss: 4.718 | Reg loss: 0.022 | Tree loss: 4.718 | Accuracy: 0.093750 | 5.106 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 018 | Total loss: 4.697 | Reg loss: 0.022 | Tree loss: 4.697 | Accuracy: 0.080078 | 5.106 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 018 | Total loss: 4.663 | Reg loss: 0.022 | Tree loss: 4.663 | Accuracy: 0.078125 | 5.107 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 018 | Total loss: 4.744 | Reg loss: 0.022 | Tree loss: 4.744 | Accuracy: 0.048780 | 5.105 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 018 | Total loss: 4.727 | Reg loss: 0.022 | Tree loss: 4.727 | Accuracy: 0.082031 | 5.108 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 018 | Total loss: 4.746 | Reg loss: 0.022 | Tree loss: 4.746 | Accuracy: 0.066406 | 5.108 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 018 | Total loss: 4.726 | Reg loss: 0.022 | Tree loss: 4.726 | Accuracy: 0.089844 | 5.109 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 018 | Total loss: 4.735 | Reg loss: 0.022 | Tree loss: 4.735 | Accuracy: 0.060547 | 5.109 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 018 | Total loss: 4.610 | Reg loss: 0.022 | Tree loss: 4.610 | Accuracy: 0.105469 | 5.11 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 018 | Total loss: 4.665 | Reg loss: 0.022 | Tree loss: 4.665 | Accuracy: 0.085938 | 5.111 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 018 | Total loss: 4.666 | Reg loss: 0.022 | Tree loss: 4.666 | Accuracy: 0.105469 | 5.111 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 018 | Total loss: 4.643 | Reg loss: 0.022 | Tree loss: 4.643 | Accuracy: 0.072266 | 5.111 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 018 | Total loss: 4.596 | Reg loss: 0.022 | Tree loss: 4.596 | Accuracy: 0.087891 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 018 | Total loss: 4.646 | Reg loss: 0.022 | Tree loss: 4.646 | Accuracy: 0.052734 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 018 | Total loss: 4.619 | Reg loss: 0.022 | Tree loss: 4.619 | Accuracy: 0.066406 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 018 | Total loss: 4.655 | Reg loss: 0.022 | Tree loss: 4.655 | Accuracy: 0.060547 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 018 | Total loss: 4.683 | Reg loss: 0.022 | Tree loss: 4.683 | Accuracy: 0.064453 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 018 | Total loss: 4.693 | Reg loss: 0.022 | Tree loss: 4.693 | Accuracy: 0.070312 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 018 | Total loss: 4.632 | Reg loss: 0.022 | Tree loss: 4.632 | Accuracy: 0.072266 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 018 | Total loss: 4.612 | Reg loss: 0.022 | Tree loss: 4.612 | Accuracy: 0.066406 | 5.112 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 016 / 018 | Total loss: 4.627 | Reg loss: 0.022 | Tree loss: 4.627 | Accuracy: 0.076172 | 5.112 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 018 | Total loss: 4.467 | Reg loss: 0.022 | Tree loss: 4.467 | Accuracy: 0.073171 | 5.11 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 018 | Total loss: 4.645 | Reg loss: 0.022 | Tree loss: 4.645 | Accuracy: 0.095703 | 5.114 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 018 | Total loss: 4.636 | Reg loss: 0.022 | Tree loss: 4.636 | Accuracy: 0.080078 | 5.114 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 018 | Total loss: 4.625 | Reg loss: 0.022 | Tree loss: 4.625 | Accuracy: 0.082031 | 5.113 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 018 | Total loss: 4.619 | Reg loss: 0.022 | Tree loss: 4.619 | Accuracy: 0.060547 | 5.113 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 018 | Total loss: 4.599 | Reg loss: 0.022 | Tree loss: 4.599 | Accuracy: 0.066406 | 5.113 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 018 | Total loss: 4.585 | Reg loss: 0.022 | Tree loss: 4.585 | Accuracy: 0.095703 | 5.113 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 018 | Total loss: 4.620 | Reg loss: 0.022 | Tree loss: 4.620 | Accuracy: 0.072266 | 5.113 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 018 | Total loss: 4.617 | Reg loss: 0.022 | Tree loss: 4.617 | Accuracy: 0.068359 | 5.113 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 018 | Total loss: 4.584 | Reg loss: 0.022 | Tree loss: 4.584 | Accuracy: 0.091797 | 5.112 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 018 | Total loss: 4.624 | Reg loss: 0.022 | Tree loss: 4.624 | Accuracy: 0.064453 | 5.112 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 018 | Total loss: 4.573 | Reg loss: 0.022 | Tree loss: 4.573 | Accuracy: 0.097656 | 5.111 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 018 | Total loss: 4.577 | Reg loss: 0.022 | Tree loss: 4.577 | Accuracy: 0.083984 | 5.111 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 018 | Total loss: 4.610 | Reg loss: 0.022 | Tree loss: 4.610 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 018 | Total loss: 4.627 | Reg loss: 0.022 | Tree loss: 4.627 | Accuracy: 0.058594 | 5.11 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 018 | Total loss: 4.554 | Reg loss: 0.022 | Tree loss: 4.554 | Accuracy: 0.068359 | 5.109 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 018 | Total loss: 4.578 | Reg loss: 0.022 | Tree loss: 4.578 | Accuracy: 0.066406 | 5.109 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 018 | Total loss: 4.575 | Reg loss: 0.022 | Tree loss: 4.575 | Accuracy: 0.058594 | 5.108 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 018 | Total loss: 4.527 | Reg loss: 0.022 | Tree loss: 4.527 | Accuracy: 0.024390 | 5.106 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 018 | Total loss: 4.602 | Reg loss: 0.022 | Tree loss: 4.602 | Accuracy: 0.080078 | 5.111 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 018 | Total loss: 4.589 | Reg loss: 0.022 | Tree loss: 4.589 | Accuracy: 0.072266 | 5.112 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 018 | Total loss: 4.617 | Reg loss: 0.022 | Tree loss: 4.617 | Accuracy: 0.068359 | 5.112 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 018 | Total loss: 4.573 | Reg loss: 0.022 | Tree loss: 4.573 | Accuracy: 0.082031 | 5.112 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 018 | Total loss: 4.621 | Reg loss: 0.022 | Tree loss: 4.621 | Accuracy: 0.060547 | 5.112 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 018 | Total loss: 4.518 | Reg loss: 0.022 | Tree loss: 4.518 | Accuracy: 0.070312 | 5.112 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 018 | Total loss: 4.548 | Reg loss: 0.022 | Tree loss: 4.548 | Accuracy: 0.070312 | 5.112 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 018 | Total loss: 4.515 | Reg loss: 0.022 | Tree loss: 4.515 | Accuracy: 0.085938 | 5.111 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 018 | Total loss: 4.563 | Reg loss: 0.022 | Tree loss: 4.563 | Accuracy: 0.066406 | 5.11 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 018 | Total loss: 4.538 | Reg loss: 0.022 | Tree loss: 4.538 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 018 | Total loss: 4.495 | Reg loss: 0.022 | Tree loss: 4.495 | Accuracy: 0.078125 | 5.109 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 018 | Total loss: 4.546 | Reg loss: 0.022 | Tree loss: 4.546 | Accuracy: 0.066406 | 5.108 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 018 | Total loss: 4.554 | Reg loss: 0.022 | Tree loss: 4.554 | Accuracy: 0.066406 | 5.109 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 018 | Total loss: 4.560 | Reg loss: 0.022 | Tree loss: 4.560 | Accuracy: 0.076172 | 5.11 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 018 | Total loss: 4.521 | Reg loss: 0.022 | Tree loss: 4.521 | Accuracy: 0.082031 | 5.11 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 018 | Total loss: 4.447 | Reg loss: 0.022 | Tree loss: 4.447 | Accuracy: 0.099609 | 5.111 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 018 | Total loss: 4.472 | Reg loss: 0.022 | Tree loss: 4.472 | Accuracy: 0.082031 | 5.112 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 018 | Total loss: 4.461 | Reg loss: 0.022 | Tree loss: 4.461 | Accuracy: 0.073171 | 5.11 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 018 | Total loss: 4.536 | Reg loss: 0.022 | Tree loss: 4.536 | Accuracy: 0.080078 | 5.112 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 018 | Total loss: 4.512 | Reg loss: 0.022 | Tree loss: 4.512 | Accuracy: 0.072266 | 5.113 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 018 | Total loss: 4.527 | Reg loss: 0.022 | Tree loss: 4.527 | Accuracy: 0.085938 | 5.113 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 018 | Total loss: 4.533 | Reg loss: 0.022 | Tree loss: 4.533 | Accuracy: 0.060547 | 5.114 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 018 | Total loss: 4.565 | Reg loss: 0.022 | Tree loss: 4.565 | Accuracy: 0.082031 | 5.115 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 018 | Total loss: 4.470 | Reg loss: 0.022 | Tree loss: 4.470 | Accuracy: 0.068359 | 5.115 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 018 | Total loss: 4.535 | Reg loss: 0.022 | Tree loss: 4.535 | Accuracy: 0.062500 | 5.116 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 018 | Total loss: 4.520 | Reg loss: 0.022 | Tree loss: 4.520 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 018 | Total loss: 4.477 | Reg loss: 0.022 | Tree loss: 4.477 | Accuracy: 0.068359 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 018 | Total loss: 4.525 | Reg loss: 0.022 | Tree loss: 4.525 | Accuracy: 0.070312 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 018 | Total loss: 4.517 | Reg loss: 0.022 | Tree loss: 4.517 | Accuracy: 0.074219 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 018 | Total loss: 4.469 | Reg loss: 0.022 | Tree loss: 4.469 | Accuracy: 0.083984 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 018 | Total loss: 4.473 | Reg loss: 0.022 | Tree loss: 4.473 | Accuracy: 0.068359 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 018 | Total loss: 4.462 | Reg loss: 0.022 | Tree loss: 4.462 | Accuracy: 0.082031 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 018 | Total loss: 4.411 | Reg loss: 0.022 | Tree loss: 4.411 | Accuracy: 0.080078 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 018 | Total loss: 4.405 | Reg loss: 0.022 | Tree loss: 4.405 | Accuracy: 0.083984 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 018 | Total loss: 4.437 | Reg loss: 0.022 | Tree loss: 4.437 | Accuracy: 0.078125 | 5.117 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 018 | Total loss: 4.376 | Reg loss: 0.022 | Tree loss: 4.376 | Accuracy: 0.121951 | 5.115 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 000 / 018 | Total loss: 4.516 | Reg loss: 0.022 | Tree loss: 4.516 | Accuracy: 0.097656 | 5.118 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 018 | Total loss: 4.469 | Reg loss: 0.022 | Tree loss: 4.469 | Accuracy: 0.076172 | 5.118 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 018 | Total loss: 4.506 | Reg loss: 0.022 | Tree loss: 4.506 | Accuracy: 0.058594 | 5.118 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 018 | Total loss: 4.438 | Reg loss: 0.022 | Tree loss: 4.438 | Accuracy: 0.083984 | 5.118 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 018 | Total loss: 4.490 | Reg loss: 0.022 | Tree loss: 4.490 | Accuracy: 0.066406 | 5.117 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 018 | Total loss: 4.504 | Reg loss: 0.022 | Tree loss: 4.504 | Accuracy: 0.070312 | 5.117 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 018 | Total loss: 4.407 | Reg loss: 0.022 | Tree loss: 4.407 | Accuracy: 0.080078 | 5.117 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 018 | Total loss: 4.374 | Reg loss: 0.022 | Tree loss: 4.374 | Accuracy: 0.099609 | 5.116 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 018 | Total loss: 4.442 | Reg loss: 0.022 | Tree loss: 4.442 | Accuracy: 0.064453 | 5.115 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 018 | Total loss: 4.436 | Reg loss: 0.022 | Tree loss: 4.436 | Accuracy: 0.070312 | 5.115 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 018 | Total loss: 4.450 | Reg loss: 0.022 | Tree loss: 4.450 | Accuracy: 0.068359 | 5.115 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 018 | Total loss: 4.424 | Reg loss: 0.022 | Tree loss: 4.424 | Accuracy: 0.078125 | 5.114 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 018 | Total loss: 4.408 | Reg loss: 0.022 | Tree loss: 4.408 | Accuracy: 0.076172 | 5.114 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 018 | Total loss: 4.415 | Reg loss: 0.022 | Tree loss: 4.415 | Accuracy: 0.076172 | 5.113 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 018 | Total loss: 4.435 | Reg loss: 0.022 | Tree loss: 4.435 | Accuracy: 0.048828 | 5.113 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 018 | Total loss: 4.374 | Reg loss: 0.022 | Tree loss: 4.374 | Accuracy: 0.101562 | 5.112 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 018 | Total loss: 4.412 | Reg loss: 0.022 | Tree loss: 4.412 | Accuracy: 0.070312 | 5.112 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 018 | Total loss: 4.538 | Reg loss: 0.022 | Tree loss: 4.538 | Accuracy: 0.048780 | 5.11 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 018 | Total loss: 4.462 | Reg loss: 0.022 | Tree loss: 4.462 | Accuracy: 0.060547 | 5.114 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 018 | Total loss: 4.444 | Reg loss: 0.022 | Tree loss: 4.444 | Accuracy: 0.076172 | 5.114 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 018 | Total loss: 4.427 | Reg loss: 0.022 | Tree loss: 4.427 | Accuracy: 0.076172 | 5.114 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 018 | Total loss: 4.404 | Reg loss: 0.022 | Tree loss: 4.404 | Accuracy: 0.089844 | 5.114 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 018 | Total loss: 4.421 | Reg loss: 0.022 | Tree loss: 4.421 | Accuracy: 0.058594 | 5.114 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 018 | Total loss: 4.403 | Reg loss: 0.022 | Tree loss: 4.403 | Accuracy: 0.087891 | 5.113 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 018 | Total loss: 4.438 | Reg loss: 0.022 | Tree loss: 4.438 | Accuracy: 0.062500 | 5.113 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 018 | Total loss: 4.365 | Reg loss: 0.022 | Tree loss: 4.365 | Accuracy: 0.078125 | 5.112 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 018 | Total loss: 4.440 | Reg loss: 0.022 | Tree loss: 4.440 | Accuracy: 0.072266 | 5.112 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 018 | Total loss: 4.365 | Reg loss: 0.022 | Tree loss: 4.365 | Accuracy: 0.101562 | 5.111 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 018 | Total loss: 4.379 | Reg loss: 0.022 | Tree loss: 4.379 | Accuracy: 0.060547 | 5.111 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 018 | Total loss: 4.378 | Reg loss: 0.022 | Tree loss: 4.378 | Accuracy: 0.076172 | 5.112 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 018 | Total loss: 4.347 | Reg loss: 0.022 | Tree loss: 4.347 | Accuracy: 0.080078 | 5.112 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 018 | Total loss: 4.380 | Reg loss: 0.022 | Tree loss: 4.380 | Accuracy: 0.068359 | 5.112 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 018 | Total loss: 4.388 | Reg loss: 0.022 | Tree loss: 4.388 | Accuracy: 0.072266 | 5.113 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 018 | Total loss: 4.344 | Reg loss: 0.022 | Tree loss: 4.344 | Accuracy: 0.085938 | 5.113 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 018 | Total loss: 4.325 | Reg loss: 0.022 | Tree loss: 4.325 | Accuracy: 0.080078 | 5.113 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 018 | Total loss: 4.307 | Reg loss: 0.022 | Tree loss: 4.307 | Accuracy: 0.048780 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 018 | Total loss: 4.374 | Reg loss: 0.022 | Tree loss: 4.374 | Accuracy: 0.087891 | 5.114 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 018 | Total loss: 4.338 | Reg loss: 0.022 | Tree loss: 4.338 | Accuracy: 0.085938 | 5.115 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 018 | Total loss: 4.356 | Reg loss: 0.022 | Tree loss: 4.356 | Accuracy: 0.091797 | 5.115 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 018 | Total loss: 4.357 | Reg loss: 0.022 | Tree loss: 4.357 | Accuracy: 0.083984 | 5.116 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 018 | Total loss: 4.424 | Reg loss: 0.022 | Tree loss: 4.424 | Accuracy: 0.068359 | 5.116 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 018 | Total loss: 4.348 | Reg loss: 0.022 | Tree loss: 4.348 | Accuracy: 0.080078 | 5.117 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 018 | Total loss: 4.357 | Reg loss: 0.022 | Tree loss: 4.357 | Accuracy: 0.062500 | 5.117 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 018 | Total loss: 4.365 | Reg loss: 0.022 | Tree loss: 4.365 | Accuracy: 0.074219 | 5.118 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 018 | Total loss: 4.398 | Reg loss: 0.022 | Tree loss: 4.398 | Accuracy: 0.056641 | 5.118 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 018 | Total loss: 4.314 | Reg loss: 0.022 | Tree loss: 4.314 | Accuracy: 0.085938 | 5.118 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 018 | Total loss: 4.334 | Reg loss: 0.022 | Tree loss: 4.334 | Accuracy: 0.085938 | 5.119 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 018 | Total loss: 4.351 | Reg loss: 0.022 | Tree loss: 4.351 | Accuracy: 0.087891 | 5.119 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 018 | Total loss: 4.342 | Reg loss: 0.022 | Tree loss: 4.342 | Accuracy: 0.052734 | 5.119 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 018 | Total loss: 4.328 | Reg loss: 0.022 | Tree loss: 4.328 | Accuracy: 0.076172 | 5.119 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 018 | Total loss: 4.318 | Reg loss: 0.022 | Tree loss: 4.318 | Accuracy: 0.064453 | 5.119 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 018 | Total loss: 4.305 | Reg loss: 0.022 | Tree loss: 4.305 | Accuracy: 0.058594 | 5.118 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 018 | Total loss: 4.343 | Reg loss: 0.023 | Tree loss: 4.343 | Accuracy: 0.082031 | 5.118 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 018 | Total loss: 4.247 | Reg loss: 0.023 | Tree loss: 4.247 | Accuracy: 0.073171 | 5.116 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 018 | Total loss: 4.329 | Reg loss: 0.022 | Tree loss: 4.329 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 018 | Total loss: 4.337 | Reg loss: 0.022 | Tree loss: 4.337 | Accuracy: 0.078125 | 5.121 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 018 | Total loss: 4.314 | Reg loss: 0.022 | Tree loss: 4.314 | Accuracy: 0.105469 | 5.121 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 018 | Total loss: 4.303 | Reg loss: 0.022 | Tree loss: 4.303 | Accuracy: 0.074219 | 5.121 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 004 / 018 | Total loss: 4.360 | Reg loss: 0.022 | Tree loss: 4.360 | Accuracy: 0.072266 | 5.12 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 018 | Total loss: 4.332 | Reg loss: 0.022 | Tree loss: 4.332 | Accuracy: 0.064453 | 5.12 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 018 | Total loss: 4.333 | Reg loss: 0.022 | Tree loss: 4.333 | Accuracy: 0.062500 | 5.12 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 018 | Total loss: 4.266 | Reg loss: 0.022 | Tree loss: 4.266 | Accuracy: 0.074219 | 5.119 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 018 | Total loss: 4.278 | Reg loss: 0.022 | Tree loss: 4.278 | Accuracy: 0.076172 | 5.119 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 018 | Total loss: 4.342 | Reg loss: 0.023 | Tree loss: 4.342 | Accuracy: 0.062500 | 5.118 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 018 | Total loss: 4.276 | Reg loss: 0.023 | Tree loss: 4.276 | Accuracy: 0.080078 | 5.118 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 018 | Total loss: 4.278 | Reg loss: 0.023 | Tree loss: 4.278 | Accuracy: 0.068359 | 5.117 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 018 | Total loss: 4.306 | Reg loss: 0.023 | Tree loss: 4.306 | Accuracy: 0.058594 | 5.117 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 018 | Total loss: 4.298 | Reg loss: 0.023 | Tree loss: 4.298 | Accuracy: 0.072266 | 5.116 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 018 | Total loss: 4.289 | Reg loss: 0.023 | Tree loss: 4.289 | Accuracy: 0.083984 | 5.116 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 018 | Total loss: 4.303 | Reg loss: 0.023 | Tree loss: 4.303 | Accuracy: 0.087891 | 5.116 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 018 | Total loss: 4.272 | Reg loss: 0.023 | Tree loss: 4.272 | Accuracy: 0.082031 | 5.115 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 018 | Total loss: 4.447 | Reg loss: 0.023 | Tree loss: 4.447 | Accuracy: 0.073171 | 5.114 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 018 | Total loss: 4.304 | Reg loss: 0.023 | Tree loss: 4.304 | Accuracy: 0.078125 | 5.117 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 018 | Total loss: 4.308 | Reg loss: 0.023 | Tree loss: 4.308 | Accuracy: 0.089844 | 5.118 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 018 | Total loss: 4.322 | Reg loss: 0.023 | Tree loss: 4.322 | Accuracy: 0.058594 | 5.118 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 018 | Total loss: 4.292 | Reg loss: 0.023 | Tree loss: 4.292 | Accuracy: 0.080078 | 5.118 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 018 | Total loss: 4.327 | Reg loss: 0.023 | Tree loss: 4.327 | Accuracy: 0.058594 | 5.118 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 018 | Total loss: 4.280 | Reg loss: 0.023 | Tree loss: 4.280 | Accuracy: 0.078125 | 5.117 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 018 | Total loss: 4.207 | Reg loss: 0.023 | Tree loss: 4.207 | Accuracy: 0.113281 | 5.117 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 018 | Total loss: 4.263 | Reg loss: 0.023 | Tree loss: 4.263 | Accuracy: 0.060547 | 5.116 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 018 | Total loss: 4.222 | Reg loss: 0.023 | Tree loss: 4.222 | Accuracy: 0.091797 | 5.116 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 018 | Total loss: 4.235 | Reg loss: 0.023 | Tree loss: 4.235 | Accuracy: 0.089844 | 5.117 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 018 | Total loss: 4.255 | Reg loss: 0.023 | Tree loss: 4.255 | Accuracy: 0.082031 | 5.117 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 018 | Total loss: 4.241 | Reg loss: 0.023 | Tree loss: 4.241 | Accuracy: 0.076172 | 5.117 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 018 | Total loss: 4.304 | Reg loss: 0.023 | Tree loss: 4.304 | Accuracy: 0.058594 | 5.117 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 018 | Total loss: 4.330 | Reg loss: 0.023 | Tree loss: 4.330 | Accuracy: 0.060547 | 5.116 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 018 | Total loss: 4.210 | Reg loss: 0.023 | Tree loss: 4.210 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 018 | Total loss: 4.229 | Reg loss: 0.023 | Tree loss: 4.229 | Accuracy: 0.066406 | 5.116 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 018 | Total loss: 4.231 | Reg loss: 0.023 | Tree loss: 4.231 | Accuracy: 0.064453 | 5.115 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 018 | Total loss: 4.224 | Reg loss: 0.023 | Tree loss: 4.224 | Accuracy: 0.048780 | 5.114 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 018 | Total loss: 4.228 | Reg loss: 0.023 | Tree loss: 4.228 | Accuracy: 0.062500 | 5.118 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 018 | Total loss: 4.222 | Reg loss: 0.023 | Tree loss: 4.222 | Accuracy: 0.085938 | 5.118 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 018 | Total loss: 4.225 | Reg loss: 0.023 | Tree loss: 4.225 | Accuracy: 0.093750 | 5.118 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 018 | Total loss: 4.254 | Reg loss: 0.023 | Tree loss: 4.254 | Accuracy: 0.064453 | 5.118 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 018 | Total loss: 4.282 | Reg loss: 0.023 | Tree loss: 4.282 | Accuracy: 0.068359 | 5.118 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 018 | Total loss: 4.243 | Reg loss: 0.023 | Tree loss: 4.243 | Accuracy: 0.076172 | 5.118 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 018 | Total loss: 4.205 | Reg loss: 0.023 | Tree loss: 4.205 | Accuracy: 0.076172 | 5.118 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 018 | Total loss: 4.268 | Reg loss: 0.023 | Tree loss: 4.268 | Accuracy: 0.074219 | 5.117 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 018 | Total loss: 4.215 | Reg loss: 0.023 | Tree loss: 4.215 | Accuracy: 0.095703 | 5.117 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 018 | Total loss: 4.239 | Reg loss: 0.023 | Tree loss: 4.239 | Accuracy: 0.064453 | 5.116 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 018 | Total loss: 4.242 | Reg loss: 0.023 | Tree loss: 4.242 | Accuracy: 0.078125 | 5.116 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 018 | Total loss: 4.190 | Reg loss: 0.023 | Tree loss: 4.190 | Accuracy: 0.074219 | 5.115 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 018 | Total loss: 4.218 | Reg loss: 0.023 | Tree loss: 4.218 | Accuracy: 0.068359 | 5.115 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 018 | Total loss: 4.271 | Reg loss: 0.023 | Tree loss: 4.271 | Accuracy: 0.054688 | 5.115 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 018 | Total loss: 4.233 | Reg loss: 0.023 | Tree loss: 4.233 | Accuracy: 0.074219 | 5.114 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 018 | Total loss: 4.159 | Reg loss: 0.023 | Tree loss: 4.159 | Accuracy: 0.095703 | 5.113 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 018 | Total loss: 4.205 | Reg loss: 0.023 | Tree loss: 4.205 | Accuracy: 0.080078 | 5.114 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 018 | Total loss: 4.533 | Reg loss: 0.023 | Tree loss: 4.533 | Accuracy: 0.048780 | 5.112 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 018 | Total loss: 4.278 | Reg loss: 0.023 | Tree loss: 4.278 | Accuracy: 0.058594 | 5.114 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 018 | Total loss: 4.268 | Reg loss: 0.023 | Tree loss: 4.268 | Accuracy: 0.066406 | 5.114 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 018 | Total loss: 4.262 | Reg loss: 0.023 | Tree loss: 4.262 | Accuracy: 0.080078 | 5.115 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 018 | Total loss: 4.218 | Reg loss: 0.023 | Tree loss: 4.218 | Accuracy: 0.072266 | 5.115 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 018 | Total loss: 4.230 | Reg loss: 0.023 | Tree loss: 4.230 | Accuracy: 0.078125 | 5.115 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 018 | Total loss: 4.226 | Reg loss: 0.023 | Tree loss: 4.226 | Accuracy: 0.070312 | 5.116 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 018 | Total loss: 4.168 | Reg loss: 0.023 | Tree loss: 4.168 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 018 | Total loss: 4.176 | Reg loss: 0.023 | Tree loss: 4.176 | Accuracy: 0.099609 | 5.116 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 008 / 018 | Total loss: 4.164 | Reg loss: 0.023 | Tree loss: 4.164 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 018 | Total loss: 4.216 | Reg loss: 0.023 | Tree loss: 4.216 | Accuracy: 0.074219 | 5.117 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 018 | Total loss: 4.171 | Reg loss: 0.023 | Tree loss: 4.171 | Accuracy: 0.056641 | 5.117 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 018 | Total loss: 4.101 | Reg loss: 0.023 | Tree loss: 4.101 | Accuracy: 0.072266 | 5.117 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 018 | Total loss: 4.160 | Reg loss: 0.023 | Tree loss: 4.160 | Accuracy: 0.083984 | 5.116 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 018 | Total loss: 4.173 | Reg loss: 0.023 | Tree loss: 4.173 | Accuracy: 0.091797 | 5.116 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 018 | Total loss: 4.174 | Reg loss: 0.023 | Tree loss: 4.174 | Accuracy: 0.064453 | 5.116 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 018 | Total loss: 4.162 | Reg loss: 0.023 | Tree loss: 4.162 | Accuracy: 0.085938 | 5.116 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 018 | Total loss: 4.198 | Reg loss: 0.023 | Tree loss: 4.198 | Accuracy: 0.070312 | 5.115 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 018 | Total loss: 3.955 | Reg loss: 0.023 | Tree loss: 3.955 | Accuracy: 0.073171 | 5.114 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 018 | Total loss: 4.191 | Reg loss: 0.023 | Tree loss: 4.191 | Accuracy: 0.083984 | 5.118 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 018 | Total loss: 4.231 | Reg loss: 0.023 | Tree loss: 4.231 | Accuracy: 0.066406 | 5.118 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 018 | Total loss: 4.206 | Reg loss: 0.023 | Tree loss: 4.206 | Accuracy: 0.072266 | 5.118 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 018 | Total loss: 4.169 | Reg loss: 0.023 | Tree loss: 4.169 | Accuracy: 0.074219 | 5.118 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 018 | Total loss: 4.207 | Reg loss: 0.023 | Tree loss: 4.207 | Accuracy: 0.076172 | 5.117 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 018 | Total loss: 4.201 | Reg loss: 0.023 | Tree loss: 4.201 | Accuracy: 0.080078 | 5.117 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 018 | Total loss: 4.176 | Reg loss: 0.023 | Tree loss: 4.176 | Accuracy: 0.072266 | 5.116 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 018 | Total loss: 4.122 | Reg loss: 0.023 | Tree loss: 4.122 | Accuracy: 0.076172 | 5.116 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 018 | Total loss: 4.186 | Reg loss: 0.023 | Tree loss: 4.186 | Accuracy: 0.085938 | 5.116 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 018 | Total loss: 4.129 | Reg loss: 0.023 | Tree loss: 4.129 | Accuracy: 0.080078 | 5.115 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 018 | Total loss: 4.110 | Reg loss: 0.023 | Tree loss: 4.110 | Accuracy: 0.087891 | 5.115 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 018 | Total loss: 4.164 | Reg loss: 0.023 | Tree loss: 4.164 | Accuracy: 0.062500 | 5.114 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 018 | Total loss: 4.158 | Reg loss: 0.023 | Tree loss: 4.158 | Accuracy: 0.062500 | 5.114 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 018 | Total loss: 4.166 | Reg loss: 0.023 | Tree loss: 4.166 | Accuracy: 0.064453 | 5.114 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 018 | Total loss: 4.112 | Reg loss: 0.023 | Tree loss: 4.112 | Accuracy: 0.076172 | 5.113 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 018 | Total loss: 4.150 | Reg loss: 0.023 | Tree loss: 4.150 | Accuracy: 0.072266 | 5.113 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 018 | Total loss: 4.091 | Reg loss: 0.023 | Tree loss: 4.091 | Accuracy: 0.089844 | 5.113 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 018 | Total loss: 4.153 | Reg loss: 0.023 | Tree loss: 4.153 | Accuracy: 0.097561 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 018 | Total loss: 4.219 | Reg loss: 0.023 | Tree loss: 4.219 | Accuracy: 0.056641 | 5.115 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 018 | Total loss: 4.166 | Reg loss: 0.023 | Tree loss: 4.166 | Accuracy: 0.085938 | 5.115 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 018 | Total loss: 4.139 | Reg loss: 0.023 | Tree loss: 4.139 | Accuracy: 0.070312 | 5.116 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 018 | Total loss: 4.153 | Reg loss: 0.023 | Tree loss: 4.153 | Accuracy: 0.070312 | 5.116 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 018 | Total loss: 4.153 | Reg loss: 0.023 | Tree loss: 4.153 | Accuracy: 0.076172 | 5.117 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 018 | Total loss: 4.104 | Reg loss: 0.023 | Tree loss: 4.104 | Accuracy: 0.087891 | 5.117 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 018 | Total loss: 4.120 | Reg loss: 0.023 | Tree loss: 4.120 | Accuracy: 0.085938 | 5.118 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 018 | Total loss: 4.147 | Reg loss: 0.023 | Tree loss: 4.147 | Accuracy: 0.083984 | 5.118 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 018 | Total loss: 4.127 | Reg loss: 0.023 | Tree loss: 4.127 | Accuracy: 0.076172 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 018 | Total loss: 4.105 | Reg loss: 0.023 | Tree loss: 4.105 | Accuracy: 0.068359 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 018 | Total loss: 4.156 | Reg loss: 0.023 | Tree loss: 4.156 | Accuracy: 0.060547 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 018 | Total loss: 4.104 | Reg loss: 0.023 | Tree loss: 4.104 | Accuracy: 0.085938 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 018 | Total loss: 4.137 | Reg loss: 0.023 | Tree loss: 4.137 | Accuracy: 0.070312 | 5.118 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 018 | Total loss: 4.123 | Reg loss: 0.023 | Tree loss: 4.123 | Accuracy: 0.087891 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 018 | Total loss: 4.054 | Reg loss: 0.023 | Tree loss: 4.054 | Accuracy: 0.082031 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 018 | Total loss: 4.114 | Reg loss: 0.023 | Tree loss: 4.114 | Accuracy: 0.070312 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 018 | Total loss: 4.115 | Reg loss: 0.023 | Tree loss: 4.115 | Accuracy: 0.072266 | 5.119 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 018 | Total loss: 4.231 | Reg loss: 0.023 | Tree loss: 4.231 | Accuracy: 0.000000 | 5.118 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 018 | Total loss: 4.140 | Reg loss: 0.023 | Tree loss: 4.140 | Accuracy: 0.083984 | 5.12 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 018 | Total loss: 4.106 | Reg loss: 0.023 | Tree loss: 4.106 | Accuracy: 0.087891 | 5.121 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 018 | Total loss: 4.131 | Reg loss: 0.023 | Tree loss: 4.131 | Accuracy: 0.076172 | 5.121 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 018 | Total loss: 4.129 | Reg loss: 0.023 | Tree loss: 4.129 | Accuracy: 0.072266 | 5.122 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 018 | Total loss: 4.081 | Reg loss: 0.023 | Tree loss: 4.081 | Accuracy: 0.056641 | 5.122 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 018 | Total loss: 4.120 | Reg loss: 0.023 | Tree loss: 4.120 | Accuracy: 0.064453 | 5.123 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 018 | Total loss: 4.177 | Reg loss: 0.023 | Tree loss: 4.177 | Accuracy: 0.072266 | 5.123 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 018 | Total loss: 4.088 | Reg loss: 0.023 | Tree loss: 4.088 | Accuracy: 0.070312 | 5.123 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 018 | Total loss: 4.100 | Reg loss: 0.023 | Tree loss: 4.100 | Accuracy: 0.083984 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 018 | Total loss: 4.114 | Reg loss: 0.023 | Tree loss: 4.114 | Accuracy: 0.080078 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 018 | Total loss: 4.087 | Reg loss: 0.023 | Tree loss: 4.087 | Accuracy: 0.089844 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 018 | Total loss: 4.083 | Reg loss: 0.023 | Tree loss: 4.083 | Accuracy: 0.074219 | 5.124 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 012 / 018 | Total loss: 4.082 | Reg loss: 0.023 | Tree loss: 4.082 | Accuracy: 0.080078 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 018 | Total loss: 4.114 | Reg loss: 0.023 | Tree loss: 4.114 | Accuracy: 0.078125 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 018 | Total loss: 4.044 | Reg loss: 0.023 | Tree loss: 4.044 | Accuracy: 0.087891 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 018 | Total loss: 4.118 | Reg loss: 0.023 | Tree loss: 4.118 | Accuracy: 0.054688 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 018 | Total loss: 4.054 | Reg loss: 0.023 | Tree loss: 4.054 | Accuracy: 0.070312 | 5.124 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 018 | Total loss: 3.908 | Reg loss: 0.023 | Tree loss: 3.908 | Accuracy: 0.097561 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 018 | Total loss: 4.131 | Reg loss: 0.023 | Tree loss: 4.131 | Accuracy: 0.082031 | 5.125 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 018 | Total loss: 4.063 | Reg loss: 0.023 | Tree loss: 4.063 | Accuracy: 0.064453 | 5.125 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 018 | Total loss: 4.105 | Reg loss: 0.023 | Tree loss: 4.105 | Accuracy: 0.056641 | 5.125 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 018 | Total loss: 4.092 | Reg loss: 0.023 | Tree loss: 4.092 | Accuracy: 0.066406 | 5.125 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 018 | Total loss: 4.080 | Reg loss: 0.023 | Tree loss: 4.080 | Accuracy: 0.080078 | 5.125 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 018 | Total loss: 4.108 | Reg loss: 0.023 | Tree loss: 4.108 | Accuracy: 0.066406 | 5.125 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 018 | Total loss: 4.059 | Reg loss: 0.023 | Tree loss: 4.059 | Accuracy: 0.087891 | 5.124 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 018 | Total loss: 4.094 | Reg loss: 0.023 | Tree loss: 4.094 | Accuracy: 0.068359 | 5.124 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 018 | Total loss: 4.103 | Reg loss: 0.023 | Tree loss: 4.103 | Accuracy: 0.058594 | 5.124 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 018 | Total loss: 4.067 | Reg loss: 0.023 | Tree loss: 4.067 | Accuracy: 0.089844 | 5.123 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 018 | Total loss: 4.104 | Reg loss: 0.023 | Tree loss: 4.104 | Accuracy: 0.085938 | 5.123 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 018 | Total loss: 4.064 | Reg loss: 0.023 | Tree loss: 4.064 | Accuracy: 0.064453 | 5.122 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 018 | Total loss: 4.047 | Reg loss: 0.023 | Tree loss: 4.047 | Accuracy: 0.091797 | 5.122 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 018 | Total loss: 4.045 | Reg loss: 0.023 | Tree loss: 4.045 | Accuracy: 0.082031 | 5.122 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 018 | Total loss: 4.055 | Reg loss: 0.023 | Tree loss: 4.055 | Accuracy: 0.087891 | 5.121 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 018 | Total loss: 4.002 | Reg loss: 0.023 | Tree loss: 4.002 | Accuracy: 0.087891 | 5.121 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 018 | Total loss: 4.083 | Reg loss: 0.023 | Tree loss: 4.083 | Accuracy: 0.058594 | 5.121 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 018 | Total loss: 3.970 | Reg loss: 0.023 | Tree loss: 3.970 | Accuracy: 0.146341 | 5.119 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 018 | Total loss: 4.075 | Reg loss: 0.023 | Tree loss: 4.075 | Accuracy: 0.083984 | 5.123 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 018 | Total loss: 4.041 | Reg loss: 0.023 | Tree loss: 4.041 | Accuracy: 0.060547 | 5.123 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 018 | Total loss: 4.056 | Reg loss: 0.023 | Tree loss: 4.056 | Accuracy: 0.060547 | 5.123 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 018 | Total loss: 4.091 | Reg loss: 0.023 | Tree loss: 4.091 | Accuracy: 0.062500 | 5.123 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 018 | Total loss: 4.038 | Reg loss: 0.023 | Tree loss: 4.038 | Accuracy: 0.064453 | 5.123 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 018 | Total loss: 4.058 | Reg loss: 0.023 | Tree loss: 4.058 | Accuracy: 0.066406 | 5.122 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 018 | Total loss: 4.115 | Reg loss: 0.023 | Tree loss: 4.115 | Accuracy: 0.076172 | 5.122 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 018 | Total loss: 4.040 | Reg loss: 0.023 | Tree loss: 4.040 | Accuracy: 0.068359 | 5.122 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 018 | Total loss: 4.088 | Reg loss: 0.023 | Tree loss: 4.088 | Accuracy: 0.072266 | 5.121 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 018 | Total loss: 4.028 | Reg loss: 0.023 | Tree loss: 4.028 | Accuracy: 0.095703 | 5.121 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 018 | Total loss: 4.065 | Reg loss: 0.023 | Tree loss: 4.065 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 018 | Total loss: 4.086 | Reg loss: 0.023 | Tree loss: 4.086 | Accuracy: 0.089844 | 5.12 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 018 | Total loss: 4.034 | Reg loss: 0.023 | Tree loss: 4.034 | Accuracy: 0.101562 | 5.121 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 018 | Total loss: 4.048 | Reg loss: 0.023 | Tree loss: 4.048 | Accuracy: 0.072266 | 5.121 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 018 | Total loss: 3.983 | Reg loss: 0.023 | Tree loss: 3.983 | Accuracy: 0.072266 | 5.121 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 018 | Total loss: 4.025 | Reg loss: 0.023 | Tree loss: 4.025 | Accuracy: 0.074219 | 5.121 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 018 | Total loss: 4.008 | Reg loss: 0.023 | Tree loss: 4.008 | Accuracy: 0.072266 | 5.122 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 018 | Total loss: 3.817 | Reg loss: 0.023 | Tree loss: 3.817 | Accuracy: 0.195122 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 018 | Total loss: 4.044 | Reg loss: 0.023 | Tree loss: 4.044 | Accuracy: 0.062500 | 5.123 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 018 | Total loss: 4.075 | Reg loss: 0.023 | Tree loss: 4.075 | Accuracy: 0.060547 | 5.123 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 018 | Total loss: 4.044 | Reg loss: 0.023 | Tree loss: 4.044 | Accuracy: 0.080078 | 5.123 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 018 | Total loss: 4.052 | Reg loss: 0.023 | Tree loss: 4.052 | Accuracy: 0.082031 | 5.124 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 018 | Total loss: 4.077 | Reg loss: 0.023 | Tree loss: 4.077 | Accuracy: 0.058594 | 5.124 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 018 | Total loss: 4.048 | Reg loss: 0.023 | Tree loss: 4.048 | Accuracy: 0.068359 | 5.125 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 018 | Total loss: 4.034 | Reg loss: 0.023 | Tree loss: 4.034 | Accuracy: 0.074219 | 5.125 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 018 | Total loss: 3.930 | Reg loss: 0.023 | Tree loss: 3.930 | Accuracy: 0.093750 | 5.126 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 018 | Total loss: 4.015 | Reg loss: 0.023 | Tree loss: 4.015 | Accuracy: 0.076172 | 5.126 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 018 | Total loss: 4.012 | Reg loss: 0.023 | Tree loss: 4.012 | Accuracy: 0.089844 | 5.126 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 018 | Total loss: 4.060 | Reg loss: 0.023 | Tree loss: 4.060 | Accuracy: 0.082031 | 5.126 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 018 | Total loss: 3.973 | Reg loss: 0.023 | Tree loss: 3.973 | Accuracy: 0.080078 | 5.127 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 018 | Total loss: 4.067 | Reg loss: 0.023 | Tree loss: 4.067 | Accuracy: 0.068359 | 5.127 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 018 | Total loss: 4.000 | Reg loss: 0.023 | Tree loss: 4.000 | Accuracy: 0.066406 | 5.127 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 018 | Total loss: 4.018 | Reg loss: 0.023 | Tree loss: 4.018 | Accuracy: 0.074219 | 5.127 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 018 | Total loss: 3.998 | Reg loss: 0.023 | Tree loss: 3.998 | Accuracy: 0.083984 | 5.126 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 016 / 018 | Total loss: 4.008 | Reg loss: 0.023 | Tree loss: 4.008 | Accuracy: 0.087891 | 5.126 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 018 | Total loss: 3.972 | Reg loss: 0.023 | Tree loss: 3.972 | Accuracy: 0.024390 | 5.125 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 018 | Total loss: 4.006 | Reg loss: 0.023 | Tree loss: 4.006 | Accuracy: 0.070312 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 018 | Total loss: 4.004 | Reg loss: 0.023 | Tree loss: 4.004 | Accuracy: 0.074219 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 018 | Total loss: 4.024 | Reg loss: 0.023 | Tree loss: 4.024 | Accuracy: 0.078125 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 018 | Total loss: 4.008 | Reg loss: 0.023 | Tree loss: 4.008 | Accuracy: 0.087891 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 018 | Total loss: 4.033 | Reg loss: 0.023 | Tree loss: 4.033 | Accuracy: 0.068359 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 018 | Total loss: 4.020 | Reg loss: 0.023 | Tree loss: 4.020 | Accuracy: 0.083984 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 018 | Total loss: 4.020 | Reg loss: 0.023 | Tree loss: 4.020 | Accuracy: 0.074219 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 018 | Total loss: 3.988 | Reg loss: 0.023 | Tree loss: 3.988 | Accuracy: 0.085938 | 5.125 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 018 | Total loss: 4.076 | Reg loss: 0.023 | Tree loss: 4.076 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 018 | Total loss: 3.987 | Reg loss: 0.023 | Tree loss: 3.987 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 018 | Total loss: 4.014 | Reg loss: 0.023 | Tree loss: 4.014 | Accuracy: 0.072266 | 5.123 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 018 | Total loss: 3.987 | Reg loss: 0.023 | Tree loss: 3.987 | Accuracy: 0.066406 | 5.123 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 018 | Total loss: 4.009 | Reg loss: 0.023 | Tree loss: 4.009 | Accuracy: 0.085938 | 5.123 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 018 | Total loss: 3.953 | Reg loss: 0.023 | Tree loss: 3.953 | Accuracy: 0.078125 | 5.122 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 018 | Total loss: 3.990 | Reg loss: 0.023 | Tree loss: 3.990 | Accuracy: 0.072266 | 5.122 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 018 | Total loss: 4.009 | Reg loss: 0.023 | Tree loss: 4.009 | Accuracy: 0.070312 | 5.122 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 018 | Total loss: 3.951 | Reg loss: 0.023 | Tree loss: 3.951 | Accuracy: 0.068359 | 5.121 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 018 | Total loss: 3.852 | Reg loss: 0.023 | Tree loss: 3.852 | Accuracy: 0.024390 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 018 | Total loss: 4.009 | Reg loss: 0.023 | Tree loss: 4.009 | Accuracy: 0.085938 | 5.123 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 018 | Total loss: 4.041 | Reg loss: 0.023 | Tree loss: 4.041 | Accuracy: 0.064453 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 018 | Total loss: 3.954 | Reg loss: 0.023 | Tree loss: 3.954 | Accuracy: 0.099609 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 018 | Total loss: 4.054 | Reg loss: 0.023 | Tree loss: 4.054 | Accuracy: 0.070312 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 018 | Total loss: 3.981 | Reg loss: 0.023 | Tree loss: 3.981 | Accuracy: 0.097656 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 018 | Total loss: 3.969 | Reg loss: 0.023 | Tree loss: 3.969 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 018 | Total loss: 3.961 | Reg loss: 0.023 | Tree loss: 3.961 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 018 | Total loss: 4.019 | Reg loss: 0.023 | Tree loss: 4.019 | Accuracy: 0.070312 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 018 | Total loss: 4.022 | Reg loss: 0.023 | Tree loss: 4.022 | Accuracy: 0.060547 | 5.123 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 018 | Total loss: 4.030 | Reg loss: 0.023 | Tree loss: 4.030 | Accuracy: 0.062500 | 5.122 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 018 | Total loss: 3.948 | Reg loss: 0.023 | Tree loss: 3.948 | Accuracy: 0.072266 | 5.123 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 018 | Total loss: 3.897 | Reg loss: 0.023 | Tree loss: 3.897 | Accuracy: 0.083984 | 5.123 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 018 | Total loss: 3.956 | Reg loss: 0.023 | Tree loss: 3.956 | Accuracy: 0.074219 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 018 | Total loss: 3.973 | Reg loss: 0.023 | Tree loss: 3.973 | Accuracy: 0.062500 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 018 | Total loss: 3.951 | Reg loss: 0.023 | Tree loss: 3.951 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 018 | Total loss: 3.965 | Reg loss: 0.023 | Tree loss: 3.965 | Accuracy: 0.080078 | 5.124 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 018 | Total loss: 3.987 | Reg loss: 0.023 | Tree loss: 3.987 | Accuracy: 0.074219 | 5.123 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 018 | Total loss: 3.878 | Reg loss: 0.023 | Tree loss: 3.878 | Accuracy: 0.048780 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 018 | Total loss: 3.941 | Reg loss: 0.023 | Tree loss: 3.941 | Accuracy: 0.097656 | 5.125 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 018 | Total loss: 3.987 | Reg loss: 0.023 | Tree loss: 3.987 | Accuracy: 0.095703 | 5.125 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 018 | Total loss: 4.012 | Reg loss: 0.023 | Tree loss: 4.012 | Accuracy: 0.070312 | 5.125 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 018 | Total loss: 3.982 | Reg loss: 0.023 | Tree loss: 3.982 | Accuracy: 0.078125 | 5.125 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 018 | Total loss: 3.958 | Reg loss: 0.023 | Tree loss: 3.958 | Accuracy: 0.070312 | 5.124 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 018 | Total loss: 3.985 | Reg loss: 0.023 | Tree loss: 3.985 | Accuracy: 0.083984 | 5.124 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 018 | Total loss: 4.017 | Reg loss: 0.023 | Tree loss: 4.017 | Accuracy: 0.076172 | 5.123 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 018 | Total loss: 3.932 | Reg loss: 0.023 | Tree loss: 3.932 | Accuracy: 0.076172 | 5.123 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 018 | Total loss: 3.923 | Reg loss: 0.023 | Tree loss: 3.923 | Accuracy: 0.058594 | 5.123 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 018 | Total loss: 3.976 | Reg loss: 0.023 | Tree loss: 3.976 | Accuracy: 0.074219 | 5.122 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 018 | Total loss: 3.970 | Reg loss: 0.023 | Tree loss: 3.970 | Accuracy: 0.058594 | 5.122 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 018 | Total loss: 3.973 | Reg loss: 0.023 | Tree loss: 3.973 | Accuracy: 0.074219 | 5.122 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 018 | Total loss: 3.934 | Reg loss: 0.023 | Tree loss: 3.934 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 018 | Total loss: 3.912 | Reg loss: 0.023 | Tree loss: 3.912 | Accuracy: 0.058594 | 5.121 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 018 | Total loss: 3.942 | Reg loss: 0.023 | Tree loss: 3.942 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 018 | Total loss: 3.966 | Reg loss: 0.023 | Tree loss: 3.966 | Accuracy: 0.099609 | 5.12 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 018 | Total loss: 3.956 | Reg loss: 0.023 | Tree loss: 3.956 | Accuracy: 0.068359 | 5.12 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 018 | Total loss: 3.969 | Reg loss: 0.023 | Tree loss: 3.969 | Accuracy: 0.170732 | 5.119 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Batch: 000 / 018 | Total loss: 3.987 | Reg loss: 0.023 | Tree loss: 3.987 | Accuracy: 0.078125 | 5.122 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 018 | Total loss: 3.926 | Reg loss: 0.023 | Tree loss: 3.926 | Accuracy: 0.068359 | 5.122 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 018 | Total loss: 3.949 | Reg loss: 0.023 | Tree loss: 3.949 | Accuracy: 0.078125 | 5.122 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 018 | Total loss: 3.994 | Reg loss: 0.023 | Tree loss: 3.994 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 018 | Total loss: 3.986 | Reg loss: 0.023 | Tree loss: 3.986 | Accuracy: 0.072266 | 5.123 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 018 | Total loss: 3.958 | Reg loss: 0.023 | Tree loss: 3.958 | Accuracy: 0.089844 | 5.123 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 018 | Total loss: 3.937 | Reg loss: 0.023 | Tree loss: 3.937 | Accuracy: 0.091797 | 5.122 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 018 | Total loss: 3.882 | Reg loss: 0.023 | Tree loss: 3.882 | Accuracy: 0.087891 | 5.122 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 018 | Total loss: 3.937 | Reg loss: 0.023 | Tree loss: 3.937 | Accuracy: 0.082031 | 5.122 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 018 | Total loss: 3.942 | Reg loss: 0.023 | Tree loss: 3.942 | Accuracy: 0.072266 | 5.121 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 018 | Total loss: 3.963 | Reg loss: 0.023 | Tree loss: 3.963 | Accuracy: 0.068359 | 5.121 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 018 | Total loss: 3.901 | Reg loss: 0.023 | Tree loss: 3.901 | Accuracy: 0.080078 | 5.121 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 018 | Total loss: 3.978 | Reg loss: 0.023 | Tree loss: 3.978 | Accuracy: 0.060547 | 5.12 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 018 | Total loss: 3.905 | Reg loss: 0.023 | Tree loss: 3.905 | Accuracy: 0.068359 | 5.12 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 018 | Total loss: 3.937 | Reg loss: 0.023 | Tree loss: 3.937 | Accuracy: 0.060547 | 5.12 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 018 | Total loss: 3.942 | Reg loss: 0.023 | Tree loss: 3.942 | Accuracy: 0.074219 | 5.119 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 018 | Total loss: 3.943 | Reg loss: 0.023 | Tree loss: 3.943 | Accuracy: 0.060547 | 5.119 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 018 | Total loss: 3.721 | Reg loss: 0.023 | Tree loss: 3.721 | Accuracy: 0.048780 | 5.118 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 018 | Total loss: 3.916 | Reg loss: 0.023 | Tree loss: 3.916 | Accuracy: 0.093750 | 5.121 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 018 | Total loss: 3.933 | Reg loss: 0.023 | Tree loss: 3.933 | Accuracy: 0.076172 | 5.121 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 018 | Total loss: 3.987 | Reg loss: 0.023 | Tree loss: 3.987 | Accuracy: 0.083984 | 5.121 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 018 | Total loss: 3.927 | Reg loss: 0.023 | Tree loss: 3.927 | Accuracy: 0.083984 | 5.121 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 018 | Total loss: 3.945 | Reg loss: 0.023 | Tree loss: 3.945 | Accuracy: 0.066406 | 5.121 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 018 | Total loss: 3.962 | Reg loss: 0.023 | Tree loss: 3.962 | Accuracy: 0.080078 | 5.121 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 018 | Total loss: 3.897 | Reg loss: 0.023 | Tree loss: 3.897 | Accuracy: 0.093750 | 5.121 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 018 | Total loss: 3.953 | Reg loss: 0.023 | Tree loss: 3.953 | Accuracy: 0.080078 | 5.12 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 018 | Total loss: 3.954 | Reg loss: 0.023 | Tree loss: 3.954 | Accuracy: 0.080078 | 5.12 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 018 | Total loss: 3.908 | Reg loss: 0.023 | Tree loss: 3.908 | Accuracy: 0.078125 | 5.12 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 018 | Total loss: 3.915 | Reg loss: 0.023 | Tree loss: 3.915 | Accuracy: 0.068359 | 5.12 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 018 | Total loss: 3.925 | Reg loss: 0.023 | Tree loss: 3.925 | Accuracy: 0.093750 | 5.12 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 018 | Total loss: 3.896 | Reg loss: 0.023 | Tree loss: 3.896 | Accuracy: 0.076172 | 5.12 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 018 | Total loss: 3.930 | Reg loss: 0.023 | Tree loss: 3.930 | Accuracy: 0.052734 | 5.119 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 018 | Total loss: 3.905 | Reg loss: 0.023 | Tree loss: 3.905 | Accuracy: 0.074219 | 5.119 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 018 | Total loss: 3.874 | Reg loss: 0.023 | Tree loss: 3.874 | Accuracy: 0.078125 | 5.119 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 018 | Total loss: 3.929 | Reg loss: 0.023 | Tree loss: 3.929 | Accuracy: 0.056641 | 5.119 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 018 | Total loss: 3.891 | Reg loss: 0.023 | Tree loss: 3.891 | Accuracy: 0.097561 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 018 | Total loss: 4.014 | Reg loss: 0.023 | Tree loss: 4.014 | Accuracy: 0.099609 | 5.12 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 018 | Total loss: 3.903 | Reg loss: 0.023 | Tree loss: 3.903 | Accuracy: 0.085938 | 5.121 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 018 | Total loss: 3.948 | Reg loss: 0.023 | Tree loss: 3.948 | Accuracy: 0.074219 | 5.121 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 018 | Total loss: 3.891 | Reg loss: 0.023 | Tree loss: 3.891 | Accuracy: 0.091797 | 5.121 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 018 | Total loss: 3.932 | Reg loss: 0.023 | Tree loss: 3.932 | Accuracy: 0.058594 | 5.122 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 018 | Total loss: 3.932 | Reg loss: 0.023 | Tree loss: 3.932 | Accuracy: 0.082031 | 5.121 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 018 | Total loss: 3.857 | Reg loss: 0.023 | Tree loss: 3.857 | Accuracy: 0.085938 | 5.121 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 018 | Total loss: 3.897 | Reg loss: 0.023 | Tree loss: 3.897 | Accuracy: 0.105469 | 5.121 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 018 | Total loss: 3.920 | Reg loss: 0.023 | Tree loss: 3.920 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 018 | Total loss: 3.895 | Reg loss: 0.023 | Tree loss: 3.895 | Accuracy: 0.089844 | 5.12 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 018 | Total loss: 3.874 | Reg loss: 0.023 | Tree loss: 3.874 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 018 | Total loss: 3.895 | Reg loss: 0.023 | Tree loss: 3.895 | Accuracy: 0.078125 | 5.12 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 018 | Total loss: 3.920 | Reg loss: 0.023 | Tree loss: 3.920 | Accuracy: 0.064453 | 5.119 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 018 | Total loss: 3.894 | Reg loss: 0.023 | Tree loss: 3.894 | Accuracy: 0.076172 | 5.119 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 018 | Total loss: 3.903 | Reg loss: 0.023 | Tree loss: 3.903 | Accuracy: 0.064453 | 5.119 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 018 | Total loss: 3.858 | Reg loss: 0.023 | Tree loss: 3.858 | Accuracy: 0.072266 | 5.118 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 018 | Total loss: 3.953 | Reg loss: 0.023 | Tree loss: 3.953 | Accuracy: 0.070312 | 5.118 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 018 | Total loss: 3.698 | Reg loss: 0.023 | Tree loss: 3.698 | Accuracy: 0.097561 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 018 | Total loss: 3.908 | Reg loss: 0.023 | Tree loss: 3.908 | Accuracy: 0.078125 | 5.119 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 018 | Total loss: 3.938 | Reg loss: 0.023 | Tree loss: 3.938 | Accuracy: 0.082031 | 5.119 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 018 | Total loss: 3.904 | Reg loss: 0.023 | Tree loss: 3.904 | Accuracy: 0.091797 | 5.119 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 018 | Total loss: 3.938 | Reg loss: 0.023 | Tree loss: 3.938 | Accuracy: 0.095703 | 5.119 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 004 / 018 | Total loss: 3.934 | Reg loss: 0.023 | Tree loss: 3.934 | Accuracy: 0.080078 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 018 | Total loss: 3.909 | Reg loss: 0.023 | Tree loss: 3.909 | Accuracy: 0.076172 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 018 | Total loss: 3.891 | Reg loss: 0.023 | Tree loss: 3.891 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 018 | Total loss: 3.896 | Reg loss: 0.023 | Tree loss: 3.896 | Accuracy: 0.089844 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 018 | Total loss: 3.907 | Reg loss: 0.023 | Tree loss: 3.907 | Accuracy: 0.089844 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 018 | Total loss: 3.859 | Reg loss: 0.023 | Tree loss: 3.859 | Accuracy: 0.078125 | 5.121 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 018 | Total loss: 3.841 | Reg loss: 0.023 | Tree loss: 3.841 | Accuracy: 0.070312 | 5.121 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 018 | Total loss: 3.866 | Reg loss: 0.023 | Tree loss: 3.866 | Accuracy: 0.083984 | 5.121 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 018 | Total loss: 3.872 | Reg loss: 0.023 | Tree loss: 3.872 | Accuracy: 0.074219 | 5.121 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 018 | Total loss: 3.851 | Reg loss: 0.023 | Tree loss: 3.851 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 018 | Total loss: 3.900 | Reg loss: 0.023 | Tree loss: 3.900 | Accuracy: 0.085938 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 018 | Total loss: 3.882 | Reg loss: 0.023 | Tree loss: 3.882 | Accuracy: 0.085938 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 018 | Total loss: 3.919 | Reg loss: 0.023 | Tree loss: 3.919 | Accuracy: 0.060547 | 5.12 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 018 | Total loss: 3.704 | Reg loss: 0.023 | Tree loss: 3.704 | Accuracy: 0.073171 | 5.118 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 018 | Total loss: 3.930 | Reg loss: 0.023 | Tree loss: 3.930 | Accuracy: 0.070312 | 5.121 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 018 | Total loss: 3.894 | Reg loss: 0.023 | Tree loss: 3.894 | Accuracy: 0.083984 | 5.122 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 018 | Total loss: 3.838 | Reg loss: 0.023 | Tree loss: 3.838 | Accuracy: 0.105469 | 5.122 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 018 | Total loss: 3.862 | Reg loss: 0.023 | Tree loss: 3.862 | Accuracy: 0.087891 | 5.121 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 018 | Total loss: 3.934 | Reg loss: 0.023 | Tree loss: 3.934 | Accuracy: 0.052734 | 5.121 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 018 | Total loss: 3.918 | Reg loss: 0.023 | Tree loss: 3.918 | Accuracy: 0.083984 | 5.121 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 018 | Total loss: 3.872 | Reg loss: 0.023 | Tree loss: 3.872 | Accuracy: 0.087891 | 5.121 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 018 | Total loss: 3.848 | Reg loss: 0.023 | Tree loss: 3.848 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 018 | Total loss: 3.880 | Reg loss: 0.023 | Tree loss: 3.880 | Accuracy: 0.080078 | 5.12 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 018 | Total loss: 3.909 | Reg loss: 0.023 | Tree loss: 3.909 | Accuracy: 0.068359 | 5.12 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 018 | Total loss: 3.880 | Reg loss: 0.023 | Tree loss: 3.880 | Accuracy: 0.085938 | 5.119 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 018 | Total loss: 3.943 | Reg loss: 0.023 | Tree loss: 3.943 | Accuracy: 0.070312 | 5.119 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 018 | Total loss: 3.825 | Reg loss: 0.023 | Tree loss: 3.825 | Accuracy: 0.087891 | 5.119 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 018 | Total loss: 3.830 | Reg loss: 0.023 | Tree loss: 3.830 | Accuracy: 0.082031 | 5.118 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 018 | Total loss: 3.826 | Reg loss: 0.023 | Tree loss: 3.826 | Accuracy: 0.099609 | 5.118 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 018 | Total loss: 3.816 | Reg loss: 0.023 | Tree loss: 3.816 | Accuracy: 0.080078 | 5.118 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 018 | Total loss: 3.946 | Reg loss: 0.023 | Tree loss: 3.946 | Accuracy: 0.078125 | 5.118 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 018 | Total loss: 3.878 | Reg loss: 0.023 | Tree loss: 3.878 | Accuracy: 0.073171 | 5.116 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 018 | Total loss: 3.874 | Reg loss: 0.023 | Tree loss: 3.874 | Accuracy: 0.099609 | 5.119 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 018 | Total loss: 3.926 | Reg loss: 0.023 | Tree loss: 3.926 | Accuracy: 0.076172 | 5.12 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 018 | Total loss: 3.842 | Reg loss: 0.023 | Tree loss: 3.842 | Accuracy: 0.080078 | 5.12 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 018 | Total loss: 3.839 | Reg loss: 0.023 | Tree loss: 3.839 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 018 | Total loss: 3.869 | Reg loss: 0.023 | Tree loss: 3.869 | Accuracy: 0.062500 | 5.12 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 018 | Total loss: 3.870 | Reg loss: 0.023 | Tree loss: 3.870 | Accuracy: 0.089844 | 5.12 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 018 | Total loss: 3.868 | Reg loss: 0.023 | Tree loss: 3.868 | Accuracy: 0.083984 | 5.12 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 018 | Total loss: 3.817 | Reg loss: 0.023 | Tree loss: 3.817 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 018 | Total loss: 3.917 | Reg loss: 0.023 | Tree loss: 3.917 | Accuracy: 0.080078 | 5.119 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 018 | Total loss: 3.862 | Reg loss: 0.023 | Tree loss: 3.862 | Accuracy: 0.078125 | 5.119 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 018 | Total loss: 3.908 | Reg loss: 0.023 | Tree loss: 3.908 | Accuracy: 0.070312 | 5.119 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 018 | Total loss: 3.843 | Reg loss: 0.023 | Tree loss: 3.843 | Accuracy: 0.083984 | 5.118 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 018 | Total loss: 3.931 | Reg loss: 0.023 | Tree loss: 3.931 | Accuracy: 0.070312 | 5.118 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 018 | Total loss: 3.817 | Reg loss: 0.023 | Tree loss: 3.817 | Accuracy: 0.082031 | 5.117 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 018 | Total loss: 3.839 | Reg loss: 0.023 | Tree loss: 3.839 | Accuracy: 0.093750 | 5.117 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 018 | Total loss: 3.865 | Reg loss: 0.023 | Tree loss: 3.865 | Accuracy: 0.091797 | 5.118 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 018 | Total loss: 3.835 | Reg loss: 0.023 | Tree loss: 3.835 | Accuracy: 0.085938 | 5.118 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 018 | Total loss: 3.742 | Reg loss: 0.023 | Tree loss: 3.742 | Accuracy: 0.097561 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 018 | Total loss: 3.825 | Reg loss: 0.023 | Tree loss: 3.825 | Accuracy: 0.083984 | 5.119 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 018 | Total loss: 3.893 | Reg loss: 0.023 | Tree loss: 3.893 | Accuracy: 0.082031 | 5.119 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 018 | Total loss: 3.908 | Reg loss: 0.023 | Tree loss: 3.908 | Accuracy: 0.091797 | 5.119 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 018 | Total loss: 3.837 | Reg loss: 0.023 | Tree loss: 3.837 | Accuracy: 0.087891 | 5.12 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 018 | Total loss: 3.770 | Reg loss: 0.023 | Tree loss: 3.770 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 018 | Total loss: 3.872 | Reg loss: 0.023 | Tree loss: 3.872 | Accuracy: 0.068359 | 5.12 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 018 | Total loss: 3.819 | Reg loss: 0.023 | Tree loss: 3.819 | Accuracy: 0.093750 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 018 | Total loss: 3.860 | Reg loss: 0.023 | Tree loss: 3.860 | Accuracy: 0.070312 | 5.121 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 008 / 018 | Total loss: 3.911 | Reg loss: 0.023 | Tree loss: 3.911 | Accuracy: 0.060547 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 018 | Total loss: 3.801 | Reg loss: 0.023 | Tree loss: 3.801 | Accuracy: 0.085938 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 018 | Total loss: 3.830 | Reg loss: 0.023 | Tree loss: 3.830 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 018 | Total loss: 3.851 | Reg loss: 0.023 | Tree loss: 3.851 | Accuracy: 0.091797 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 018 | Total loss: 3.863 | Reg loss: 0.023 | Tree loss: 3.863 | Accuracy: 0.076172 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 018 | Total loss: 3.847 | Reg loss: 0.023 | Tree loss: 3.847 | Accuracy: 0.076172 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 018 | Total loss: 3.890 | Reg loss: 0.023 | Tree loss: 3.890 | Accuracy: 0.070312 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 018 | Total loss: 3.836 | Reg loss: 0.023 | Tree loss: 3.836 | Accuracy: 0.080078 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 018 | Total loss: 3.895 | Reg loss: 0.023 | Tree loss: 3.895 | Accuracy: 0.082031 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 018 | Total loss: 3.596 | Reg loss: 0.023 | Tree loss: 3.596 | Accuracy: 0.121951 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 018 | Total loss: 3.866 | Reg loss: 0.023 | Tree loss: 3.866 | Accuracy: 0.083984 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 018 | Total loss: 3.866 | Reg loss: 0.023 | Tree loss: 3.866 | Accuracy: 0.080078 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 018 | Total loss: 3.895 | Reg loss: 0.023 | Tree loss: 3.895 | Accuracy: 0.066406 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 018 | Total loss: 3.865 | Reg loss: 0.023 | Tree loss: 3.865 | Accuracy: 0.087891 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 018 | Total loss: 3.833 | Reg loss: 0.023 | Tree loss: 3.833 | Accuracy: 0.083984 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 018 | Total loss: 3.836 | Reg loss: 0.023 | Tree loss: 3.836 | Accuracy: 0.087891 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 018 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.082031 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 018 | Total loss: 3.821 | Reg loss: 0.023 | Tree loss: 3.821 | Accuracy: 0.082031 | 5.122 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 018 | Total loss: 3.790 | Reg loss: 0.023 | Tree loss: 3.790 | Accuracy: 0.082031 | 5.121 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 018 | Total loss: 3.814 | Reg loss: 0.023 | Tree loss: 3.814 | Accuracy: 0.095703 | 5.121 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 018 | Total loss: 3.803 | Reg loss: 0.023 | Tree loss: 3.803 | Accuracy: 0.082031 | 5.121 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 018 | Total loss: 3.896 | Reg loss: 0.023 | Tree loss: 3.896 | Accuracy: 0.080078 | 5.12 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 018 | Total loss: 3.807 | Reg loss: 0.023 | Tree loss: 3.807 | Accuracy: 0.070312 | 5.12 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 018 | Total loss: 3.900 | Reg loss: 0.023 | Tree loss: 3.900 | Accuracy: 0.060547 | 5.12 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 018 | Total loss: 3.780 | Reg loss: 0.023 | Tree loss: 3.780 | Accuracy: 0.083984 | 5.119 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 018 | Total loss: 3.857 | Reg loss: 0.023 | Tree loss: 3.857 | Accuracy: 0.091797 | 5.119 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 018 | Total loss: 3.830 | Reg loss: 0.023 | Tree loss: 3.830 | Accuracy: 0.080078 | 5.119 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 018 | Total loss: 4.014 | Reg loss: 0.023 | Tree loss: 4.014 | Accuracy: 0.048780 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 018 | Total loss: 3.838 | Reg loss: 0.023 | Tree loss: 3.838 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 018 | Total loss: 3.878 | Reg loss: 0.023 | Tree loss: 3.878 | Accuracy: 0.085938 | 5.121 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 018 | Total loss: 3.827 | Reg loss: 0.023 | Tree loss: 3.827 | Accuracy: 0.074219 | 5.121 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 018 | Total loss: 3.870 | Reg loss: 0.023 | Tree loss: 3.870 | Accuracy: 0.076172 | 5.121 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 018 | Total loss: 3.828 | Reg loss: 0.023 | Tree loss: 3.828 | Accuracy: 0.082031 | 5.121 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 018 | Total loss: 3.844 | Reg loss: 0.023 | Tree loss: 3.844 | Accuracy: 0.060547 | 5.121 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 018 | Total loss: 3.830 | Reg loss: 0.023 | Tree loss: 3.830 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 018 | Total loss: 3.809 | Reg loss: 0.023 | Tree loss: 3.809 | Accuracy: 0.091797 | 5.12 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 018 | Total loss: 3.828 | Reg loss: 0.023 | Tree loss: 3.828 | Accuracy: 0.068359 | 5.119 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 018 | Total loss: 3.846 | Reg loss: 0.023 | Tree loss: 3.846 | Accuracy: 0.074219 | 5.119 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 018 | Total loss: 3.891 | Reg loss: 0.023 | Tree loss: 3.891 | Accuracy: 0.072266 | 5.119 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 018 | Total loss: 3.786 | Reg loss: 0.023 | Tree loss: 3.786 | Accuracy: 0.095703 | 5.118 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 018 | Total loss: 3.787 | Reg loss: 0.023 | Tree loss: 3.787 | Accuracy: 0.095703 | 5.118 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 018 | Total loss: 3.808 | Reg loss: 0.023 | Tree loss: 3.808 | Accuracy: 0.101562 | 5.119 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 018 | Total loss: 3.775 | Reg loss: 0.023 | Tree loss: 3.775 | Accuracy: 0.080078 | 5.119 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 018 | Total loss: 3.763 | Reg loss: 0.023 | Tree loss: 3.763 | Accuracy: 0.083984 | 5.119 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 018 | Total loss: 3.855 | Reg loss: 0.023 | Tree loss: 3.855 | Accuracy: 0.064453 | 5.119 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 018 | Total loss: 3.951 | Reg loss: 0.023 | Tree loss: 3.951 | Accuracy: 0.146341 | 5.118 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 018 | Total loss: 3.770 | Reg loss: 0.023 | Tree loss: 3.770 | Accuracy: 0.089844 | 5.12 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 018 | Total loss: 3.832 | Reg loss: 0.023 | Tree loss: 3.832 | Accuracy: 0.082031 | 5.12 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 018 | Total loss: 3.841 | Reg loss: 0.023 | Tree loss: 3.841 | Accuracy: 0.068359 | 5.121 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 018 | Total loss: 3.876 | Reg loss: 0.023 | Tree loss: 3.876 | Accuracy: 0.062500 | 5.121 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 018 | Total loss: 3.798 | Reg loss: 0.023 | Tree loss: 3.798 | Accuracy: 0.093750 | 5.121 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 018 | Total loss: 3.822 | Reg loss: 0.023 | Tree loss: 3.822 | Accuracy: 0.078125 | 5.122 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 018 | Total loss: 3.832 | Reg loss: 0.023 | Tree loss: 3.832 | Accuracy: 0.085938 | 5.122 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 018 | Total loss: 3.782 | Reg loss: 0.023 | Tree loss: 3.782 | Accuracy: 0.095703 | 5.122 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 018 | Total loss: 3.826 | Reg loss: 0.023 | Tree loss: 3.826 | Accuracy: 0.068359 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 018 | Total loss: 3.821 | Reg loss: 0.023 | Tree loss: 3.821 | Accuracy: 0.083984 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 018 | Total loss: 3.826 | Reg loss: 0.023 | Tree loss: 3.826 | Accuracy: 0.093750 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 018 | Total loss: 3.863 | Reg loss: 0.023 | Tree loss: 3.863 | Accuracy: 0.064453 | 5.123 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 012 / 018 | Total loss: 3.816 | Reg loss: 0.023 | Tree loss: 3.816 | Accuracy: 0.080078 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 018 | Total loss: 3.795 | Reg loss: 0.023 | Tree loss: 3.795 | Accuracy: 0.093750 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 018 | Total loss: 3.796 | Reg loss: 0.023 | Tree loss: 3.796 | Accuracy: 0.076172 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 018 | Total loss: 3.788 | Reg loss: 0.023 | Tree loss: 3.788 | Accuracy: 0.082031 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 018 | Total loss: 3.792 | Reg loss: 0.023 | Tree loss: 3.792 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 018 | Total loss: 3.930 | Reg loss: 0.023 | Tree loss: 3.930 | Accuracy: 0.097561 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 018 | Total loss: 3.832 | Reg loss: 0.023 | Tree loss: 3.832 | Accuracy: 0.082031 | 5.124 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 018 | Total loss: 3.812 | Reg loss: 0.023 | Tree loss: 3.812 | Accuracy: 0.082031 | 5.124 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 018 | Total loss: 3.807 | Reg loss: 0.023 | Tree loss: 3.807 | Accuracy: 0.085938 | 5.124 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 018 | Total loss: 3.804 | Reg loss: 0.023 | Tree loss: 3.804 | Accuracy: 0.080078 | 5.124 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 018 | Total loss: 3.776 | Reg loss: 0.023 | Tree loss: 3.776 | Accuracy: 0.085938 | 5.123 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 018 | Total loss: 3.793 | Reg loss: 0.023 | Tree loss: 3.793 | Accuracy: 0.091797 | 5.123 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 018 | Total loss: 3.825 | Reg loss: 0.023 | Tree loss: 3.825 | Accuracy: 0.101562 | 5.123 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 018 | Total loss: 3.751 | Reg loss: 0.023 | Tree loss: 3.751 | Accuracy: 0.070312 | 5.122 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 018 | Total loss: 3.790 | Reg loss: 0.023 | Tree loss: 3.790 | Accuracy: 0.085938 | 5.122 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 018 | Total loss: 3.816 | Reg loss: 0.023 | Tree loss: 3.816 | Accuracy: 0.080078 | 5.122 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 018 | Total loss: 3.816 | Reg loss: 0.023 | Tree loss: 3.816 | Accuracy: 0.082031 | 5.121 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 018 | Total loss: 3.755 | Reg loss: 0.023 | Tree loss: 3.755 | Accuracy: 0.080078 | 5.121 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 018 | Total loss: 3.797 | Reg loss: 0.023 | Tree loss: 3.797 | Accuracy: 0.074219 | 5.121 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 018 | Total loss: 3.813 | Reg loss: 0.023 | Tree loss: 3.813 | Accuracy: 0.103516 | 5.12 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 018 | Total loss: 3.855 | Reg loss: 0.023 | Tree loss: 3.855 | Accuracy: 0.062500 | 5.12 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 018 | Total loss: 3.810 | Reg loss: 0.023 | Tree loss: 3.810 | Accuracy: 0.070312 | 5.12 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 018 | Total loss: 3.845 | Reg loss: 0.023 | Tree loss: 3.845 | Accuracy: 0.066406 | 5.12 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 018 | Total loss: 3.934 | Reg loss: 0.023 | Tree loss: 3.934 | Accuracy: 0.000000 | 5.118 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 018 | Total loss: 3.818 | Reg loss: 0.023 | Tree loss: 3.818 | Accuracy: 0.070312 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 018 | Total loss: 3.781 | Reg loss: 0.023 | Tree loss: 3.781 | Accuracy: 0.060547 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 018 | Total loss: 3.807 | Reg loss: 0.023 | Tree loss: 3.807 | Accuracy: 0.085938 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 018 | Total loss: 3.761 | Reg loss: 0.023 | Tree loss: 3.761 | Accuracy: 0.076172 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 018 | Total loss: 3.804 | Reg loss: 0.023 | Tree loss: 3.804 | Accuracy: 0.095703 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 018 | Total loss: 3.810 | Reg loss: 0.023 | Tree loss: 3.810 | Accuracy: 0.087891 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 018 | Total loss: 3.777 | Reg loss: 0.023 | Tree loss: 3.777 | Accuracy: 0.083984 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 018 | Total loss: 3.723 | Reg loss: 0.023 | Tree loss: 3.723 | Accuracy: 0.093750 | 5.12 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 018 | Total loss: 3.797 | Reg loss: 0.023 | Tree loss: 3.797 | Accuracy: 0.097656 | 5.12 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 018 | Total loss: 3.844 | Reg loss: 0.023 | Tree loss: 3.844 | Accuracy: 0.078125 | 5.119 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 018 | Total loss: 3.816 | Reg loss: 0.023 | Tree loss: 3.816 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 018 | Total loss: 3.824 | Reg loss: 0.023 | Tree loss: 3.824 | Accuracy: 0.080078 | 5.12 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 018 | Total loss: 3.822 | Reg loss: 0.023 | Tree loss: 3.822 | Accuracy: 0.093750 | 5.12 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 018 | Total loss: 3.777 | Reg loss: 0.023 | Tree loss: 3.777 | Accuracy: 0.074219 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 018 | Total loss: 3.835 | Reg loss: 0.023 | Tree loss: 3.835 | Accuracy: 0.062500 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 018 | Total loss: 3.737 | Reg loss: 0.023 | Tree loss: 3.737 | Accuracy: 0.089844 | 5.121 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 018 | Total loss: 3.792 | Reg loss: 0.023 | Tree loss: 3.792 | Accuracy: 0.074219 | 5.122 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 018 | Total loss: 3.860 | Reg loss: 0.023 | Tree loss: 3.860 | Accuracy: 0.073171 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 018 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.097656 | 5.122 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 018 | Total loss: 3.735 | Reg loss: 0.023 | Tree loss: 3.735 | Accuracy: 0.093750 | 5.123 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 018 | Total loss: 3.790 | Reg loss: 0.023 | Tree loss: 3.790 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 018 | Total loss: 3.819 | Reg loss: 0.023 | Tree loss: 3.819 | Accuracy: 0.076172 | 5.123 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 018 | Total loss: 3.783 | Reg loss: 0.023 | Tree loss: 3.783 | Accuracy: 0.083984 | 5.124 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 018 | Total loss: 3.749 | Reg loss: 0.023 | Tree loss: 3.749 | Accuracy: 0.083984 | 5.124 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 018 | Total loss: 3.796 | Reg loss: 0.023 | Tree loss: 3.796 | Accuracy: 0.091797 | 5.125 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 018 | Total loss: 3.817 | Reg loss: 0.023 | Tree loss: 3.817 | Accuracy: 0.066406 | 5.125 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 018 | Total loss: 3.799 | Reg loss: 0.023 | Tree loss: 3.799 | Accuracy: 0.074219 | 5.125 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 018 | Total loss: 3.809 | Reg loss: 0.023 | Tree loss: 3.809 | Accuracy: 0.083984 | 5.126 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 018 | Total loss: 3.787 | Reg loss: 0.023 | Tree loss: 3.787 | Accuracy: 0.068359 | 5.126 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 018 | Total loss: 3.761 | Reg loss: 0.023 | Tree loss: 3.761 | Accuracy: 0.078125 | 5.126 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 018 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.080078 | 5.126 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 018 | Total loss: 3.823 | Reg loss: 0.023 | Tree loss: 3.823 | Accuracy: 0.080078 | 5.126 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 018 | Total loss: 3.747 | Reg loss: 0.023 | Tree loss: 3.747 | Accuracy: 0.087891 | 5.126 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 018 | Total loss: 3.780 | Reg loss: 0.023 | Tree loss: 3.780 | Accuracy: 0.072266 | 5.126 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 016 / 018 | Total loss: 3.763 | Reg loss: 0.023 | Tree loss: 3.763 | Accuracy: 0.085938 | 5.126 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 018 | Total loss: 3.851 | Reg loss: 0.023 | Tree loss: 3.851 | Accuracy: 0.024390 | 5.125 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 018 | Total loss: 3.793 | Reg loss: 0.023 | Tree loss: 3.793 | Accuracy: 0.099609 | 5.128 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 018 | Total loss: 3.812 | Reg loss: 0.023 | Tree loss: 3.812 | Accuracy: 0.068359 | 5.127 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 018 | Total loss: 3.816 | Reg loss: 0.023 | Tree loss: 3.816 | Accuracy: 0.060547 | 5.127 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 018 | Total loss: 3.839 | Reg loss: 0.023 | Tree loss: 3.839 | Accuracy: 0.076172 | 5.127 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 018 | Total loss: 3.813 | Reg loss: 0.023 | Tree loss: 3.813 | Accuracy: 0.082031 | 5.127 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 018 | Total loss: 3.795 | Reg loss: 0.023 | Tree loss: 3.795 | Accuracy: 0.078125 | 5.126 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 018 | Total loss: 3.739 | Reg loss: 0.023 | Tree loss: 3.739 | Accuracy: 0.082031 | 5.126 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 018 | Total loss: 3.794 | Reg loss: 0.023 | Tree loss: 3.794 | Accuracy: 0.076172 | 5.126 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 018 | Total loss: 3.789 | Reg loss: 0.023 | Tree loss: 3.789 | Accuracy: 0.082031 | 5.125 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 018 | Total loss: 3.792 | Reg loss: 0.023 | Tree loss: 3.792 | Accuracy: 0.095703 | 5.125 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 018 | Total loss: 3.704 | Reg loss: 0.023 | Tree loss: 3.704 | Accuracy: 0.097656 | 5.125 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 018 | Total loss: 3.794 | Reg loss: 0.023 | Tree loss: 3.794 | Accuracy: 0.074219 | 5.124 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 018 | Total loss: 3.755 | Reg loss: 0.023 | Tree loss: 3.755 | Accuracy: 0.107422 | 5.124 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 018 | Total loss: 3.720 | Reg loss: 0.023 | Tree loss: 3.720 | Accuracy: 0.068359 | 5.124 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 018 | Total loss: 3.769 | Reg loss: 0.023 | Tree loss: 3.769 | Accuracy: 0.080078 | 5.124 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 018 | Total loss: 3.763 | Reg loss: 0.023 | Tree loss: 3.763 | Accuracy: 0.082031 | 5.123 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 018 | Total loss: 3.725 | Reg loss: 0.023 | Tree loss: 3.725 | Accuracy: 0.072266 | 5.123 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 018 | Total loss: 3.800 | Reg loss: 0.023 | Tree loss: 3.800 | Accuracy: 0.024390 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 018 | Total loss: 3.740 | Reg loss: 0.023 | Tree loss: 3.740 | Accuracy: 0.083984 | 5.124 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 018 | Total loss: 3.735 | Reg loss: 0.023 | Tree loss: 3.735 | Accuracy: 0.091797 | 5.125 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 018 | Total loss: 3.796 | Reg loss: 0.023 | Tree loss: 3.796 | Accuracy: 0.074219 | 5.125 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 018 | Total loss: 3.846 | Reg loss: 0.023 | Tree loss: 3.846 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 018 | Total loss: 3.760 | Reg loss: 0.023 | Tree loss: 3.760 | Accuracy: 0.091797 | 5.124 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 018 | Total loss: 3.693 | Reg loss: 0.023 | Tree loss: 3.693 | Accuracy: 0.085938 | 5.124 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 018 | Total loss: 3.823 | Reg loss: 0.023 | Tree loss: 3.823 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 018 | Total loss: 3.818 | Reg loss: 0.023 | Tree loss: 3.818 | Accuracy: 0.066406 | 5.123 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 018 | Total loss: 3.806 | Reg loss: 0.023 | Tree loss: 3.806 | Accuracy: 0.070312 | 5.123 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 018 | Total loss: 3.737 | Reg loss: 0.023 | Tree loss: 3.737 | Accuracy: 0.095703 | 5.124 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 018 | Total loss: 3.747 | Reg loss: 0.023 | Tree loss: 3.747 | Accuracy: 0.095703 | 5.124 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 018 | Total loss: 3.717 | Reg loss: 0.023 | Tree loss: 3.717 | Accuracy: 0.101562 | 5.124 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 018 | Total loss: 3.728 | Reg loss: 0.023 | Tree loss: 3.728 | Accuracy: 0.070312 | 5.125 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 018 | Total loss: 3.841 | Reg loss: 0.023 | Tree loss: 3.841 | Accuracy: 0.074219 | 5.125 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 018 | Total loss: 3.793 | Reg loss: 0.023 | Tree loss: 3.793 | Accuracy: 0.080078 | 5.125 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 018 | Total loss: 3.759 | Reg loss: 0.023 | Tree loss: 3.759 | Accuracy: 0.083984 | 5.125 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 018 | Total loss: 3.735 | Reg loss: 0.023 | Tree loss: 3.735 | Accuracy: 0.056641 | 5.125 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 018 | Total loss: 3.704 | Reg loss: 0.023 | Tree loss: 3.704 | Accuracy: 0.097561 | 5.124 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 018 | Total loss: 3.713 | Reg loss: 0.023 | Tree loss: 3.713 | Accuracy: 0.089844 | 5.126 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 018 | Total loss: 3.787 | Reg loss: 0.023 | Tree loss: 3.787 | Accuracy: 0.074219 | 5.126 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 018 | Total loss: 3.777 | Reg loss: 0.023 | Tree loss: 3.777 | Accuracy: 0.082031 | 5.125 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 018 | Total loss: 3.756 | Reg loss: 0.023 | Tree loss: 3.756 | Accuracy: 0.082031 | 5.125 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 018 | Total loss: 3.740 | Reg loss: 0.023 | Tree loss: 3.740 | Accuracy: 0.082031 | 5.124 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 018 | Total loss: 3.800 | Reg loss: 0.023 | Tree loss: 3.800 | Accuracy: 0.089844 | 5.124 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 018 | Total loss: 3.741 | Reg loss: 0.023 | Tree loss: 3.741 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 018 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.066406 | 5.124 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 018 | Total loss: 3.710 | Reg loss: 0.023 | Tree loss: 3.710 | Accuracy: 0.109375 | 5.123 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 018 | Total loss: 3.838 | Reg loss: 0.023 | Tree loss: 3.838 | Accuracy: 0.089844 | 5.123 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 018 | Total loss: 3.832 | Reg loss: 0.023 | Tree loss: 3.832 | Accuracy: 0.074219 | 5.123 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 018 | Total loss: 3.726 | Reg loss: 0.023 | Tree loss: 3.726 | Accuracy: 0.074219 | 5.122 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 018 | Total loss: 3.714 | Reg loss: 0.023 | Tree loss: 3.714 | Accuracy: 0.076172 | 5.122 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 018 | Total loss: 3.748 | Reg loss: 0.023 | Tree loss: 3.748 | Accuracy: 0.064453 | 5.122 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 018 | Total loss: 3.733 | Reg loss: 0.023 | Tree loss: 3.733 | Accuracy: 0.083984 | 5.122 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 018 | Total loss: 3.785 | Reg loss: 0.023 | Tree loss: 3.785 | Accuracy: 0.087891 | 5.121 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 018 | Total loss: 3.735 | Reg loss: 0.023 | Tree loss: 3.735 | Accuracy: 0.082031 | 5.121 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 018 | Total loss: 3.664 | Reg loss: 0.023 | Tree loss: 3.664 | Accuracy: 0.000000 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 000 / 018 | Total loss: 3.784 | Reg loss: 0.023 | Tree loss: 3.784 | Accuracy: 0.074219 | 5.122 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 018 | Total loss: 3.780 | Reg loss: 0.023 | Tree loss: 3.780 | Accuracy: 0.078125 | 5.122 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 018 | Total loss: 3.783 | Reg loss: 0.023 | Tree loss: 3.783 | Accuracy: 0.082031 | 5.122 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 018 | Total loss: 3.723 | Reg loss: 0.023 | Tree loss: 3.723 | Accuracy: 0.085938 | 5.122 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 018 | Total loss: 3.729 | Reg loss: 0.023 | Tree loss: 3.729 | Accuracy: 0.064453 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 018 | Total loss: 3.734 | Reg loss: 0.023 | Tree loss: 3.734 | Accuracy: 0.080078 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 018 | Total loss: 3.785 | Reg loss: 0.023 | Tree loss: 3.785 | Accuracy: 0.085938 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 018 | Total loss: 3.739 | Reg loss: 0.023 | Tree loss: 3.739 | Accuracy: 0.080078 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 018 | Total loss: 3.688 | Reg loss: 0.023 | Tree loss: 3.688 | Accuracy: 0.087891 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 018 | Total loss: 3.723 | Reg loss: 0.023 | Tree loss: 3.723 | Accuracy: 0.105469 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 018 | Total loss: 3.810 | Reg loss: 0.023 | Tree loss: 3.810 | Accuracy: 0.082031 | 5.124 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 018 | Total loss: 3.745 | Reg loss: 0.023 | Tree loss: 3.745 | Accuracy: 0.080078 | 5.124 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 018 | Total loss: 3.768 | Reg loss: 0.023 | Tree loss: 3.768 | Accuracy: 0.080078 | 5.124 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 018 | Total loss: 3.780 | Reg loss: 0.023 | Tree loss: 3.780 | Accuracy: 0.072266 | 5.124 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 018 | Total loss: 3.760 | Reg loss: 0.023 | Tree loss: 3.760 | Accuracy: 0.076172 | 5.124 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 018 | Total loss: 3.744 | Reg loss: 0.023 | Tree loss: 3.744 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 018 | Total loss: 3.736 | Reg loss: 0.023 | Tree loss: 3.736 | Accuracy: 0.080078 | 5.123 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 018 | Total loss: 3.681 | Reg loss: 0.023 | Tree loss: 3.681 | Accuracy: 0.146341 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 018 | Total loss: 3.780 | Reg loss: 0.023 | Tree loss: 3.780 | Accuracy: 0.078125 | 5.124 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 018 | Total loss: 3.834 | Reg loss: 0.023 | Tree loss: 3.834 | Accuracy: 0.050781 | 5.124 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 018 | Total loss: 3.746 | Reg loss: 0.023 | Tree loss: 3.746 | Accuracy: 0.083984 | 5.124 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 018 | Total loss: 3.780 | Reg loss: 0.023 | Tree loss: 3.780 | Accuracy: 0.070312 | 5.124 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 018 | Total loss: 3.728 | Reg loss: 0.023 | Tree loss: 3.728 | Accuracy: 0.080078 | 5.123 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 018 | Total loss: 3.724 | Reg loss: 0.023 | Tree loss: 3.724 | Accuracy: 0.087891 | 5.123 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 018 | Total loss: 3.745 | Reg loss: 0.023 | Tree loss: 3.745 | Accuracy: 0.076172 | 5.123 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 018 | Total loss: 3.769 | Reg loss: 0.023 | Tree loss: 3.769 | Accuracy: 0.082031 | 5.124 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 018 | Total loss: 3.787 | Reg loss: 0.023 | Tree loss: 3.787 | Accuracy: 0.089844 | 5.124 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 018 | Total loss: 3.716 | Reg loss: 0.023 | Tree loss: 3.716 | Accuracy: 0.089844 | 5.125 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 018 | Total loss: 3.728 | Reg loss: 0.023 | Tree loss: 3.728 | Accuracy: 0.085938 | 5.125 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 018 | Total loss: 3.742 | Reg loss: 0.023 | Tree loss: 3.742 | Accuracy: 0.078125 | 5.125 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 018 | Total loss: 3.680 | Reg loss: 0.023 | Tree loss: 3.680 | Accuracy: 0.101562 | 5.126 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 018 | Total loss: 3.730 | Reg loss: 0.023 | Tree loss: 3.730 | Accuracy: 0.097656 | 5.126 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 018 | Total loss: 3.733 | Reg loss: 0.023 | Tree loss: 3.733 | Accuracy: 0.074219 | 5.126 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 018 | Total loss: 3.716 | Reg loss: 0.023 | Tree loss: 3.716 | Accuracy: 0.083984 | 5.127 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 018 | Total loss: 3.737 | Reg loss: 0.023 | Tree loss: 3.737 | Accuracy: 0.072266 | 5.127 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 018 | Total loss: 3.854 | Reg loss: 0.023 | Tree loss: 3.854 | Accuracy: 0.024390 | 5.126 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 018 | Total loss: 3.752 | Reg loss: 0.023 | Tree loss: 3.752 | Accuracy: 0.083984 | 5.126 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 018 | Total loss: 3.782 | Reg loss: 0.023 | Tree loss: 3.782 | Accuracy: 0.083984 | 5.126 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 018 | Total loss: 3.769 | Reg loss: 0.023 | Tree loss: 3.769 | Accuracy: 0.082031 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 018 | Total loss: 3.757 | Reg loss: 0.023 | Tree loss: 3.757 | Accuracy: 0.066406 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 018 | Total loss: 3.695 | Reg loss: 0.023 | Tree loss: 3.695 | Accuracy: 0.083984 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 018 | Total loss: 3.686 | Reg loss: 0.023 | Tree loss: 3.686 | Accuracy: 0.097656 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 018 | Total loss: 3.732 | Reg loss: 0.023 | Tree loss: 3.732 | Accuracy: 0.076172 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 018 | Total loss: 3.783 | Reg loss: 0.023 | Tree loss: 3.783 | Accuracy: 0.091797 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 018 | Total loss: 3.717 | Reg loss: 0.023 | Tree loss: 3.717 | Accuracy: 0.076172 | 5.128 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 018 | Total loss: 3.779 | Reg loss: 0.023 | Tree loss: 3.779 | Accuracy: 0.080078 | 5.128 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 018 | Total loss: 3.741 | Reg loss: 0.023 | Tree loss: 3.741 | Accuracy: 0.078125 | 5.128 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 018 | Total loss: 3.704 | Reg loss: 0.023 | Tree loss: 3.704 | Accuracy: 0.080078 | 5.128 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 018 | Total loss: 3.725 | Reg loss: 0.023 | Tree loss: 3.725 | Accuracy: 0.105469 | 5.128 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 018 | Total loss: 3.803 | Reg loss: 0.023 | Tree loss: 3.803 | Accuracy: 0.072266 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 018 | Total loss: 3.715 | Reg loss: 0.023 | Tree loss: 3.715 | Accuracy: 0.089844 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 018 | Total loss: 3.732 | Reg loss: 0.023 | Tree loss: 3.732 | Accuracy: 0.056641 | 5.127 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 018 | Total loss: 3.690 | Reg loss: 0.023 | Tree loss: 3.690 | Accuracy: 0.074219 | 5.128 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 018 | Total loss: 3.729 | Reg loss: 0.023 | Tree loss: 3.729 | Accuracy: 0.073171 | 5.127 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 018 | Total loss: 3.786 | Reg loss: 0.023 | Tree loss: 3.786 | Accuracy: 0.087891 | 5.128 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 018 | Total loss: 3.717 | Reg loss: 0.023 | Tree loss: 3.717 | Accuracy: 0.074219 | 5.128 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 018 | Total loss: 3.763 | Reg loss: 0.023 | Tree loss: 3.763 | Accuracy: 0.076172 | 5.129 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 018 | Total loss: 3.697 | Reg loss: 0.023 | Tree loss: 3.697 | Accuracy: 0.083984 | 5.129 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 004 / 018 | Total loss: 3.721 | Reg loss: 0.023 | Tree loss: 3.721 | Accuracy: 0.082031 | 5.129 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 018 | Total loss: 3.717 | Reg loss: 0.023 | Tree loss: 3.717 | Accuracy: 0.091797 | 5.13 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 018 | Total loss: 3.736 | Reg loss: 0.023 | Tree loss: 3.736 | Accuracy: 0.062500 | 5.13 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 018 | Total loss: 3.795 | Reg loss: 0.023 | Tree loss: 3.795 | Accuracy: 0.070312 | 5.13 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 018 | Total loss: 3.662 | Reg loss: 0.023 | Tree loss: 3.662 | Accuracy: 0.091797 | 5.13 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 018 | Total loss: 3.740 | Reg loss: 0.023 | Tree loss: 3.740 | Accuracy: 0.082031 | 5.13 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 018 | Total loss: 3.772 | Reg loss: 0.023 | Tree loss: 3.772 | Accuracy: 0.085938 | 5.131 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 018 | Total loss: 3.770 | Reg loss: 0.023 | Tree loss: 3.770 | Accuracy: 0.070312 | 5.131 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 018 | Total loss: 3.705 | Reg loss: 0.023 | Tree loss: 3.705 | Accuracy: 0.089844 | 5.131 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 018 | Total loss: 3.746 | Reg loss: 0.023 | Tree loss: 3.746 | Accuracy: 0.064453 | 5.131 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 018 | Total loss: 3.650 | Reg loss: 0.023 | Tree loss: 3.650 | Accuracy: 0.093750 | 5.131 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 018 | Total loss: 3.730 | Reg loss: 0.023 | Tree loss: 3.730 | Accuracy: 0.087891 | 5.131 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 018 | Total loss: 3.724 | Reg loss: 0.023 | Tree loss: 3.724 | Accuracy: 0.085938 | 5.13 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 018 | Total loss: 3.916 | Reg loss: 0.023 | Tree loss: 3.916 | Accuracy: 0.048780 | 5.129 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 018 | Total loss: 3.734 | Reg loss: 0.023 | Tree loss: 3.734 | Accuracy: 0.091797 | 5.132 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 018 | Total loss: 3.762 | Reg loss: 0.023 | Tree loss: 3.762 | Accuracy: 0.078125 | 5.132 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 018 | Total loss: 3.696 | Reg loss: 0.023 | Tree loss: 3.696 | Accuracy: 0.080078 | 5.131 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 018 | Total loss: 3.693 | Reg loss: 0.023 | Tree loss: 3.693 | Accuracy: 0.103516 | 5.131 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 018 | Total loss: 3.717 | Reg loss: 0.023 | Tree loss: 3.717 | Accuracy: 0.062500 | 5.131 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 018 | Total loss: 3.731 | Reg loss: 0.023 | Tree loss: 3.731 | Accuracy: 0.093750 | 5.131 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 018 | Total loss: 3.716 | Reg loss: 0.023 | Tree loss: 3.716 | Accuracy: 0.083984 | 5.131 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 018 | Total loss: 3.784 | Reg loss: 0.023 | Tree loss: 3.784 | Accuracy: 0.064453 | 5.131 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 018 | Total loss: 3.744 | Reg loss: 0.023 | Tree loss: 3.744 | Accuracy: 0.115234 | 5.13 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 018 | Total loss: 3.755 | Reg loss: 0.023 | Tree loss: 3.755 | Accuracy: 0.070312 | 5.13 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 018 | Total loss: 3.709 | Reg loss: 0.023 | Tree loss: 3.709 | Accuracy: 0.080078 | 5.13 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 018 | Total loss: 3.689 | Reg loss: 0.023 | Tree loss: 3.689 | Accuracy: 0.066406 | 5.129 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 018 | Total loss: 3.764 | Reg loss: 0.023 | Tree loss: 3.764 | Accuracy: 0.083984 | 5.129 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 018 | Total loss: 3.696 | Reg loss: 0.023 | Tree loss: 3.696 | Accuracy: 0.091797 | 5.129 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 018 | Total loss: 3.734 | Reg loss: 0.023 | Tree loss: 3.734 | Accuracy: 0.089844 | 5.129 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 018 | Total loss: 3.689 | Reg loss: 0.023 | Tree loss: 3.689 | Accuracy: 0.072266 | 5.128 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 018 | Total loss: 3.728 | Reg loss: 0.023 | Tree loss: 3.728 | Accuracy: 0.052734 | 5.128 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 018 | Total loss: 3.705 | Reg loss: 0.023 | Tree loss: 3.705 | Accuracy: 0.048780 | 5.127 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 018 | Total loss: 3.686 | Reg loss: 0.023 | Tree loss: 3.686 | Accuracy: 0.087891 | 5.13 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 018 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.093750 | 5.13 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 018 | Total loss: 3.700 | Reg loss: 0.023 | Tree loss: 3.700 | Accuracy: 0.091797 | 5.13 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 018 | Total loss: 3.696 | Reg loss: 0.023 | Tree loss: 3.696 | Accuracy: 0.082031 | 5.13 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 018 | Total loss: 3.736 | Reg loss: 0.023 | Tree loss: 3.736 | Accuracy: 0.082031 | 5.13 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 018 | Total loss: 3.762 | Reg loss: 0.023 | Tree loss: 3.762 | Accuracy: 0.080078 | 5.129 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 018 | Total loss: 3.743 | Reg loss: 0.023 | Tree loss: 3.743 | Accuracy: 0.082031 | 5.129 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 018 | Total loss: 3.757 | Reg loss: 0.023 | Tree loss: 3.757 | Accuracy: 0.068359 | 5.129 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 018 | Total loss: 3.698 | Reg loss: 0.023 | Tree loss: 3.698 | Accuracy: 0.093750 | 5.128 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 018 | Total loss: 3.714 | Reg loss: 0.023 | Tree loss: 3.714 | Accuracy: 0.074219 | 5.128 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 018 | Total loss: 3.682 | Reg loss: 0.023 | Tree loss: 3.682 | Accuracy: 0.074219 | 5.128 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 018 | Total loss: 3.762 | Reg loss: 0.023 | Tree loss: 3.762 | Accuracy: 0.060547 | 5.127 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 018 | Total loss: 3.761 | Reg loss: 0.023 | Tree loss: 3.761 | Accuracy: 0.064453 | 5.127 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 018 | Total loss: 3.710 | Reg loss: 0.023 | Tree loss: 3.710 | Accuracy: 0.091797 | 5.128 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 018 | Total loss: 3.668 | Reg loss: 0.023 | Tree loss: 3.668 | Accuracy: 0.066406 | 5.128 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 018 | Total loss: 3.685 | Reg loss: 0.023 | Tree loss: 3.685 | Accuracy: 0.068359 | 5.128 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 018 | Total loss: 3.767 | Reg loss: 0.023 | Tree loss: 3.767 | Accuracy: 0.117188 | 5.128 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 018 | Total loss: 3.810 | Reg loss: 0.023 | Tree loss: 3.810 | Accuracy: 0.073171 | 5.127 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 018 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.074219 | 5.129 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 018 | Total loss: 3.705 | Reg loss: 0.023 | Tree loss: 3.705 | Accuracy: 0.093750 | 5.129 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 018 | Total loss: 3.725 | Reg loss: 0.023 | Tree loss: 3.725 | Accuracy: 0.103516 | 5.129 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 018 | Total loss: 3.729 | Reg loss: 0.023 | Tree loss: 3.729 | Accuracy: 0.048828 | 5.13 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 018 | Total loss: 3.686 | Reg loss: 0.023 | Tree loss: 3.686 | Accuracy: 0.089844 | 5.13 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 018 | Total loss: 3.770 | Reg loss: 0.023 | Tree loss: 3.770 | Accuracy: 0.074219 | 5.13 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 018 | Total loss: 3.760 | Reg loss: 0.023 | Tree loss: 3.760 | Accuracy: 0.068359 | 5.131 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 018 | Total loss: 3.702 | Reg loss: 0.023 | Tree loss: 3.702 | Accuracy: 0.085938 | 5.131 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 008 / 018 | Total loss: 3.694 | Reg loss: 0.023 | Tree loss: 3.694 | Accuracy: 0.070312 | 5.131 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 018 | Total loss: 3.767 | Reg loss: 0.023 | Tree loss: 3.767 | Accuracy: 0.085938 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 018 | Total loss: 3.633 | Reg loss: 0.023 | Tree loss: 3.633 | Accuracy: 0.103516 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 018 | Total loss: 3.693 | Reg loss: 0.023 | Tree loss: 3.693 | Accuracy: 0.068359 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 018 | Total loss: 3.641 | Reg loss: 0.023 | Tree loss: 3.641 | Accuracy: 0.082031 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 018 | Total loss: 3.758 | Reg loss: 0.023 | Tree loss: 3.758 | Accuracy: 0.076172 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 018 | Total loss: 3.727 | Reg loss: 0.023 | Tree loss: 3.727 | Accuracy: 0.076172 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 018 | Total loss: 3.695 | Reg loss: 0.023 | Tree loss: 3.695 | Accuracy: 0.091797 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 018 | Total loss: 3.749 | Reg loss: 0.023 | Tree loss: 3.749 | Accuracy: 0.089844 | 5.132 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 018 | Total loss: 3.687 | Reg loss: 0.023 | Tree loss: 3.687 | Accuracy: 0.024390 | 5.131 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 018 | Total loss: 3.762 | Reg loss: 0.023 | Tree loss: 3.762 | Accuracy: 0.072266 | 5.133 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 018 | Total loss: 3.775 | Reg loss: 0.023 | Tree loss: 3.775 | Accuracy: 0.072266 | 5.133 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 018 | Total loss: 3.690 | Reg loss: 0.023 | Tree loss: 3.690 | Accuracy: 0.076172 | 5.133 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 018 | Total loss: 3.695 | Reg loss: 0.023 | Tree loss: 3.695 | Accuracy: 0.091797 | 5.132 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 018 | Total loss: 3.699 | Reg loss: 0.023 | Tree loss: 3.699 | Accuracy: 0.062500 | 5.132 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 018 | Total loss: 3.675 | Reg loss: 0.023 | Tree loss: 3.675 | Accuracy: 0.095703 | 5.132 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 018 | Total loss: 3.661 | Reg loss: 0.023 | Tree loss: 3.661 | Accuracy: 0.083984 | 5.132 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 018 | Total loss: 3.676 | Reg loss: 0.023 | Tree loss: 3.676 | Accuracy: 0.093750 | 5.131 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 018 | Total loss: 3.738 | Reg loss: 0.023 | Tree loss: 3.738 | Accuracy: 0.091797 | 5.131 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 018 | Total loss: 3.704 | Reg loss: 0.023 | Tree loss: 3.704 | Accuracy: 0.080078 | 5.131 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 018 | Total loss: 3.732 | Reg loss: 0.024 | Tree loss: 3.732 | Accuracy: 0.095703 | 5.131 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 018 | Total loss: 3.696 | Reg loss: 0.024 | Tree loss: 3.696 | Accuracy: 0.083984 | 5.13 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 018 | Total loss: 3.705 | Reg loss: 0.024 | Tree loss: 3.705 | Accuracy: 0.074219 | 5.13 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 018 | Total loss: 3.671 | Reg loss: 0.024 | Tree loss: 3.671 | Accuracy: 0.083984 | 5.13 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 018 | Total loss: 3.696 | Reg loss: 0.024 | Tree loss: 3.696 | Accuracy: 0.078125 | 5.13 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 018 | Total loss: 3.771 | Reg loss: 0.024 | Tree loss: 3.771 | Accuracy: 0.066406 | 5.129 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 018 | Total loss: 3.704 | Reg loss: 0.024 | Tree loss: 3.704 | Accuracy: 0.078125 | 5.129 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 018 | Total loss: 3.732 | Reg loss: 0.024 | Tree loss: 3.732 | Accuracy: 0.048780 | 5.128 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 018 | Total loss: 3.723 | Reg loss: 0.024 | Tree loss: 3.723 | Accuracy: 0.076172 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 018 | Total loss: 3.740 | Reg loss: 0.024 | Tree loss: 3.740 | Accuracy: 0.056641 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 018 | Total loss: 3.688 | Reg loss: 0.024 | Tree loss: 3.688 | Accuracy: 0.097656 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 018 | Total loss: 3.731 | Reg loss: 0.024 | Tree loss: 3.731 | Accuracy: 0.074219 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 018 | Total loss: 3.653 | Reg loss: 0.024 | Tree loss: 3.653 | Accuracy: 0.093750 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 018 | Total loss: 3.708 | Reg loss: 0.024 | Tree loss: 3.708 | Accuracy: 0.066406 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 018 | Total loss: 3.685 | Reg loss: 0.024 | Tree loss: 3.685 | Accuracy: 0.089844 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 018 | Total loss: 3.669 | Reg loss: 0.024 | Tree loss: 3.669 | Accuracy: 0.083984 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 018 | Total loss: 3.705 | Reg loss: 0.024 | Tree loss: 3.705 | Accuracy: 0.095703 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 018 | Total loss: 3.679 | Reg loss: 0.024 | Tree loss: 3.679 | Accuracy: 0.080078 | 5.13 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 018 | Total loss: 3.737 | Reg loss: 0.024 | Tree loss: 3.737 | Accuracy: 0.087891 | 5.13 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 018 | Total loss: 3.667 | Reg loss: 0.024 | Tree loss: 3.667 | Accuracy: 0.093750 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 018 | Total loss: 3.730 | Reg loss: 0.024 | Tree loss: 3.730 | Accuracy: 0.074219 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 018 | Total loss: 3.734 | Reg loss: 0.024 | Tree loss: 3.734 | Accuracy: 0.072266 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 018 | Total loss: 3.733 | Reg loss: 0.024 | Tree loss: 3.733 | Accuracy: 0.078125 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 018 | Total loss: 3.662 | Reg loss: 0.024 | Tree loss: 3.662 | Accuracy: 0.080078 | 5.131 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 018 | Total loss: 3.718 | Reg loss: 0.024 | Tree loss: 3.718 | Accuracy: 0.078125 | 5.13 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 018 | Total loss: 3.715 | Reg loss: 0.024 | Tree loss: 3.715 | Accuracy: 0.073171 | 5.129 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 018 | Total loss: 3.714 | Reg loss: 0.024 | Tree loss: 3.714 | Accuracy: 0.078125 | 5.132 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 018 | Total loss: 3.662 | Reg loss: 0.024 | Tree loss: 3.662 | Accuracy: 0.076172 | 5.132 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 018 | Total loss: 3.734 | Reg loss: 0.024 | Tree loss: 3.734 | Accuracy: 0.080078 | 5.132 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 018 | Total loss: 3.699 | Reg loss: 0.024 | Tree loss: 3.699 | Accuracy: 0.095703 | 5.132 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 018 | Total loss: 3.705 | Reg loss: 0.024 | Tree loss: 3.705 | Accuracy: 0.078125 | 5.132 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 018 | Total loss: 3.738 | Reg loss: 0.024 | Tree loss: 3.738 | Accuracy: 0.082031 | 5.132 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 018 | Total loss: 3.721 | Reg loss: 0.024 | Tree loss: 3.721 | Accuracy: 0.076172 | 5.131 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 018 | Total loss: 3.649 | Reg loss: 0.024 | Tree loss: 3.649 | Accuracy: 0.091797 | 5.131 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 018 | Total loss: 3.734 | Reg loss: 0.024 | Tree loss: 3.734 | Accuracy: 0.062500 | 5.131 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 018 | Total loss: 3.761 | Reg loss: 0.024 | Tree loss: 3.761 | Accuracy: 0.070312 | 5.131 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 018 | Total loss: 3.648 | Reg loss: 0.024 | Tree loss: 3.648 | Accuracy: 0.082031 | 5.13 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 018 | Total loss: 3.721 | Reg loss: 0.024 | Tree loss: 3.721 | Accuracy: 0.076172 | 5.13 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 012 / 018 | Total loss: 3.689 | Reg loss: 0.024 | Tree loss: 3.689 | Accuracy: 0.085938 | 5.13 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 018 | Total loss: 3.696 | Reg loss: 0.024 | Tree loss: 3.696 | Accuracy: 0.085938 | 5.13 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 018 | Total loss: 3.656 | Reg loss: 0.024 | Tree loss: 3.656 | Accuracy: 0.083984 | 5.129 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 018 | Total loss: 3.639 | Reg loss: 0.024 | Tree loss: 3.639 | Accuracy: 0.082031 | 5.129 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 018 | Total loss: 3.719 | Reg loss: 0.024 | Tree loss: 3.719 | Accuracy: 0.082031 | 5.129 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 018 | Total loss: 3.680 | Reg loss: 0.024 | Tree loss: 3.680 | Accuracy: 0.195122 | 5.128 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 018 | Total loss: 3.697 | Reg loss: 0.024 | Tree loss: 3.697 | Accuracy: 0.083984 | 5.13 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 018 | Total loss: 3.703 | Reg loss: 0.024 | Tree loss: 3.703 | Accuracy: 0.080078 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 018 | Total loss: 3.756 | Reg loss: 0.024 | Tree loss: 3.756 | Accuracy: 0.080078 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 018 | Total loss: 3.714 | Reg loss: 0.024 | Tree loss: 3.714 | Accuracy: 0.052734 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 018 | Total loss: 3.630 | Reg loss: 0.024 | Tree loss: 3.630 | Accuracy: 0.087891 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 018 | Total loss: 3.709 | Reg loss: 0.024 | Tree loss: 3.709 | Accuracy: 0.076172 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 018 | Total loss: 3.657 | Reg loss: 0.024 | Tree loss: 3.657 | Accuracy: 0.109375 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 018 | Total loss: 3.659 | Reg loss: 0.024 | Tree loss: 3.659 | Accuracy: 0.093750 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 018 | Total loss: 3.684 | Reg loss: 0.024 | Tree loss: 3.684 | Accuracy: 0.089844 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 018 | Total loss: 3.708 | Reg loss: 0.024 | Tree loss: 3.708 | Accuracy: 0.072266 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 018 | Total loss: 3.655 | Reg loss: 0.024 | Tree loss: 3.655 | Accuracy: 0.093750 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 018 | Total loss: 3.733 | Reg loss: 0.024 | Tree loss: 3.733 | Accuracy: 0.082031 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 018 | Total loss: 3.620 | Reg loss: 0.024 | Tree loss: 3.620 | Accuracy: 0.078125 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 018 | Total loss: 3.747 | Reg loss: 0.024 | Tree loss: 3.747 | Accuracy: 0.080078 | 5.132 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 018 | Total loss: 3.673 | Reg loss: 0.024 | Tree loss: 3.673 | Accuracy: 0.089844 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 018 | Total loss: 3.705 | Reg loss: 0.024 | Tree loss: 3.705 | Accuracy: 0.068359 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 018 | Total loss: 3.750 | Reg loss: 0.024 | Tree loss: 3.750 | Accuracy: 0.060547 | 5.131 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 018 | Total loss: 3.751 | Reg loss: 0.024 | Tree loss: 3.751 | Accuracy: 0.073171 | 5.13 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 018 | Total loss: 3.708 | Reg loss: 0.024 | Tree loss: 3.708 | Accuracy: 0.064453 | 5.132 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 018 | Total loss: 3.698 | Reg loss: 0.024 | Tree loss: 3.698 | Accuracy: 0.082031 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 018 | Total loss: 3.631 | Reg loss: 0.024 | Tree loss: 3.631 | Accuracy: 0.078125 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 018 | Total loss: 3.708 | Reg loss: 0.024 | Tree loss: 3.708 | Accuracy: 0.076172 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 018 | Total loss: 3.729 | Reg loss: 0.024 | Tree loss: 3.729 | Accuracy: 0.066406 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 018 | Total loss: 3.738 | Reg loss: 0.024 | Tree loss: 3.738 | Accuracy: 0.087891 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 018 | Total loss: 3.660 | Reg loss: 0.024 | Tree loss: 3.660 | Accuracy: 0.099609 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 018 | Total loss: 3.725 | Reg loss: 0.024 | Tree loss: 3.725 | Accuracy: 0.064453 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 018 | Total loss: 3.730 | Reg loss: 0.024 | Tree loss: 3.730 | Accuracy: 0.097656 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 018 | Total loss: 3.695 | Reg loss: 0.024 | Tree loss: 3.695 | Accuracy: 0.080078 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 018 | Total loss: 3.702 | Reg loss: 0.024 | Tree loss: 3.702 | Accuracy: 0.089844 | 5.133 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 018 | Total loss: 3.652 | Reg loss: 0.024 | Tree loss: 3.652 | Accuracy: 0.089844 | 5.132 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 018 | Total loss: 3.673 | Reg loss: 0.024 | Tree loss: 3.673 | Accuracy: 0.068359 | 5.132 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 018 | Total loss: 3.671 | Reg loss: 0.024 | Tree loss: 3.671 | Accuracy: 0.085938 | 5.132 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 018 | Total loss: 3.639 | Reg loss: 0.024 | Tree loss: 3.639 | Accuracy: 0.078125 | 5.132 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 018 | Total loss: 3.664 | Reg loss: 0.024 | Tree loss: 3.664 | Accuracy: 0.082031 | 5.131 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 018 | Total loss: 3.693 | Reg loss: 0.024 | Tree loss: 3.693 | Accuracy: 0.089844 | 5.131 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 018 | Total loss: 3.891 | Reg loss: 0.024 | Tree loss: 3.891 | Accuracy: 0.048780 | 5.13 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 018 | Total loss: 3.669 | Reg loss: 0.024 | Tree loss: 3.669 | Accuracy: 0.089844 | 5.132 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 018 | Total loss: 3.662 | Reg loss: 0.024 | Tree loss: 3.662 | Accuracy: 0.076172 | 5.133 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 018 | Total loss: 3.652 | Reg loss: 0.024 | Tree loss: 3.652 | Accuracy: 0.099609 | 5.133 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 018 | Total loss: 3.736 | Reg loss: 0.024 | Tree loss: 3.736 | Accuracy: 0.062500 | 5.133 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 018 | Total loss: 3.707 | Reg loss: 0.024 | Tree loss: 3.707 | Accuracy: 0.068359 | 5.134 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 018 | Total loss: 3.681 | Reg loss: 0.024 | Tree loss: 3.681 | Accuracy: 0.083984 | 5.134 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 018 | Total loss: 3.727 | Reg loss: 0.024 | Tree loss: 3.727 | Accuracy: 0.085938 | 5.134 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 018 | Total loss: 3.698 | Reg loss: 0.024 | Tree loss: 3.698 | Accuracy: 0.080078 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 018 | Total loss: 3.745 | Reg loss: 0.024 | Tree loss: 3.745 | Accuracy: 0.072266 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 018 | Total loss: 3.636 | Reg loss: 0.024 | Tree loss: 3.636 | Accuracy: 0.082031 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 018 | Total loss: 3.707 | Reg loss: 0.024 | Tree loss: 3.707 | Accuracy: 0.083984 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 018 | Total loss: 3.698 | Reg loss: 0.024 | Tree loss: 3.698 | Accuracy: 0.089844 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 018 | Total loss: 3.645 | Reg loss: 0.024 | Tree loss: 3.645 | Accuracy: 0.074219 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 018 | Total loss: 3.678 | Reg loss: 0.024 | Tree loss: 3.678 | Accuracy: 0.085938 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 018 | Total loss: 3.644 | Reg loss: 0.024 | Tree loss: 3.644 | Accuracy: 0.097656 | 5.135 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 018 | Total loss: 3.704 | Reg loss: 0.024 | Tree loss: 3.704 | Accuracy: 0.072266 | 5.135 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 016 / 018 | Total loss: 3.661 | Reg loss: 0.024 | Tree loss: 3.661 | Accuracy: 0.078125 | 5.136 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 018 | Total loss: 3.719 | Reg loss: 0.024 | Tree loss: 3.719 | Accuracy: 0.024390 | 5.135 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 018 | Total loss: 3.696 | Reg loss: 0.024 | Tree loss: 3.696 | Accuracy: 0.076172 | 5.136 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 018 | Total loss: 3.717 | Reg loss: 0.024 | Tree loss: 3.717 | Accuracy: 0.093750 | 5.136 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 018 | Total loss: 3.706 | Reg loss: 0.024 | Tree loss: 3.706 | Accuracy: 0.085938 | 5.136 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 018 | Total loss: 3.686 | Reg loss: 0.024 | Tree loss: 3.686 | Accuracy: 0.083984 | 5.137 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 018 | Total loss: 3.695 | Reg loss: 0.024 | Tree loss: 3.695 | Accuracy: 0.099609 | 5.137 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 018 | Total loss: 3.645 | Reg loss: 0.024 | Tree loss: 3.645 | Accuracy: 0.099609 | 5.137 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 018 | Total loss: 3.692 | Reg loss: 0.024 | Tree loss: 3.692 | Accuracy: 0.087891 | 5.137 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 018 | Total loss: 3.647 | Reg loss: 0.024 | Tree loss: 3.647 | Accuracy: 0.099609 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 018 | Total loss: 3.751 | Reg loss: 0.024 | Tree loss: 3.751 | Accuracy: 0.042969 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 018 | Total loss: 3.696 | Reg loss: 0.024 | Tree loss: 3.696 | Accuracy: 0.087891 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 018 | Total loss: 3.651 | Reg loss: 0.024 | Tree loss: 3.651 | Accuracy: 0.068359 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 018 | Total loss: 3.675 | Reg loss: 0.024 | Tree loss: 3.675 | Accuracy: 0.064453 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 018 | Total loss: 3.652 | Reg loss: 0.024 | Tree loss: 3.652 | Accuracy: 0.095703 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 018 | Total loss: 3.622 | Reg loss: 0.024 | Tree loss: 3.622 | Accuracy: 0.089844 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 018 | Total loss: 3.694 | Reg loss: 0.024 | Tree loss: 3.694 | Accuracy: 0.062500 | 5.138 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 018 | Total loss: 3.660 | Reg loss: 0.024 | Tree loss: 3.660 | Accuracy: 0.056641 | 5.137 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 018 | Total loss: 3.712 | Reg loss: 0.024 | Tree loss: 3.712 | Accuracy: 0.083984 | 5.137 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 018 | Total loss: 3.509 | Reg loss: 0.024 | Tree loss: 3.509 | Accuracy: 0.073171 | 5.136 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 018 | Total loss: 3.768 | Reg loss: 0.024 | Tree loss: 3.768 | Accuracy: 0.070312 | 5.138 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 018 | Total loss: 3.630 | Reg loss: 0.024 | Tree loss: 3.630 | Accuracy: 0.085938 | 5.139 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 018 | Total loss: 3.689 | Reg loss: 0.024 | Tree loss: 3.689 | Accuracy: 0.078125 | 5.138 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 018 | Total loss: 3.681 | Reg loss: 0.024 | Tree loss: 3.681 | Accuracy: 0.082031 | 5.138 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 018 | Total loss: 3.641 | Reg loss: 0.024 | Tree loss: 3.641 | Accuracy: 0.076172 | 5.139 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 018 | Total loss: 3.678 | Reg loss: 0.024 | Tree loss: 3.678 | Accuracy: 0.085938 | 5.139 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 018 | Total loss: 3.658 | Reg loss: 0.024 | Tree loss: 3.658 | Accuracy: 0.066406 | 5.139 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 018 | Total loss: 3.666 | Reg loss: 0.024 | Tree loss: 3.666 | Accuracy: 0.089844 | 5.139 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 018 | Total loss: 3.681 | Reg loss: 0.024 | Tree loss: 3.681 | Accuracy: 0.087891 | 5.139 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 018 | Total loss: 3.640 | Reg loss: 0.024 | Tree loss: 3.640 | Accuracy: 0.091797 | 5.138 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 018 | Total loss: 3.646 | Reg loss: 0.024 | Tree loss: 3.646 | Accuracy: 0.093750 | 5.138 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 018 | Total loss: 3.690 | Reg loss: 0.024 | Tree loss: 3.690 | Accuracy: 0.062500 | 5.138 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 018 | Total loss: 3.724 | Reg loss: 0.024 | Tree loss: 3.724 | Accuracy: 0.076172 | 5.138 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 018 | Total loss: 3.699 | Reg loss: 0.024 | Tree loss: 3.699 | Accuracy: 0.087891 | 5.137 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 018 | Total loss: 3.668 | Reg loss: 0.024 | Tree loss: 3.668 | Accuracy: 0.093750 | 5.137 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 018 | Total loss: 3.737 | Reg loss: 0.024 | Tree loss: 3.737 | Accuracy: 0.066406 | 5.137 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 018 | Total loss: 3.631 | Reg loss: 0.024 | Tree loss: 3.631 | Accuracy: 0.082031 | 5.137 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 018 | Total loss: 3.558 | Reg loss: 0.024 | Tree loss: 3.558 | Accuracy: 0.097561 | 5.136 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 018 | Total loss: 3.741 | Reg loss: 0.024 | Tree loss: 3.741 | Accuracy: 0.068359 | 5.138 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 018 | Total loss: 3.696 | Reg loss: 0.024 | Tree loss: 3.696 | Accuracy: 0.062500 | 5.138 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 018 | Total loss: 3.654 | Reg loss: 0.024 | Tree loss: 3.654 | Accuracy: 0.097656 | 5.138 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 018 | Total loss: 3.710 | Reg loss: 0.024 | Tree loss: 3.710 | Accuracy: 0.072266 | 5.139 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 018 | Total loss: 3.639 | Reg loss: 0.024 | Tree loss: 3.639 | Accuracy: 0.097656 | 5.139 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 018 | Total loss: 3.707 | Reg loss: 0.024 | Tree loss: 3.707 | Accuracy: 0.078125 | 5.14 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 018 | Total loss: 3.670 | Reg loss: 0.024 | Tree loss: 3.670 | Accuracy: 0.083984 | 5.14 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 018 | Total loss: 3.645 | Reg loss: 0.024 | Tree loss: 3.645 | Accuracy: 0.101562 | 5.14 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 018 | Total loss: 3.672 | Reg loss: 0.024 | Tree loss: 3.672 | Accuracy: 0.064453 | 5.14 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 018 | Total loss: 3.720 | Reg loss: 0.024 | Tree loss: 3.720 | Accuracy: 0.076172 | 5.141 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 018 | Total loss: 3.655 | Reg loss: 0.024 | Tree loss: 3.655 | Accuracy: 0.097656 | 5.14 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 018 | Total loss: 3.695 | Reg loss: 0.024 | Tree loss: 3.695 | Accuracy: 0.087891 | 5.14 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 018 | Total loss: 3.643 | Reg loss: 0.024 | Tree loss: 3.643 | Accuracy: 0.070312 | 5.141 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 018 | Total loss: 3.676 | Reg loss: 0.024 | Tree loss: 3.676 | Accuracy: 0.058594 | 5.141 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 018 | Total loss: 3.650 | Reg loss: 0.024 | Tree loss: 3.650 | Accuracy: 0.074219 | 5.141 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 018 | Total loss: 3.649 | Reg loss: 0.024 | Tree loss: 3.649 | Accuracy: 0.087891 | 5.141 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 018 | Total loss: 3.642 | Reg loss: 0.024 | Tree loss: 3.642 | Accuracy: 0.099609 | 5.141 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 018 | Total loss: 3.678 | Reg loss: 0.024 | Tree loss: 3.678 | Accuracy: 0.073171 | 5.14 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Batch: 000 / 018 | Total loss: 3.744 | Reg loss: 0.024 | Tree loss: 3.744 | Accuracy: 0.080078 | 5.142 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 018 | Total loss: 3.639 | Reg loss: 0.024 | Tree loss: 3.639 | Accuracy: 0.105469 | 5.142 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 018 | Total loss: 3.680 | Reg loss: 0.024 | Tree loss: 3.680 | Accuracy: 0.083984 | 5.142 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 018 | Total loss: 3.692 | Reg loss: 0.024 | Tree loss: 3.692 | Accuracy: 0.074219 | 5.143 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 018 | Total loss: 3.705 | Reg loss: 0.024 | Tree loss: 3.705 | Accuracy: 0.072266 | 5.143 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 018 | Total loss: 3.701 | Reg loss: 0.024 | Tree loss: 3.701 | Accuracy: 0.072266 | 5.143 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 018 | Total loss: 3.624 | Reg loss: 0.024 | Tree loss: 3.624 | Accuracy: 0.093750 | 5.143 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 018 | Total loss: 3.628 | Reg loss: 0.024 | Tree loss: 3.628 | Accuracy: 0.078125 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 018 | Total loss: 3.694 | Reg loss: 0.024 | Tree loss: 3.694 | Accuracy: 0.064453 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 018 | Total loss: 3.679 | Reg loss: 0.024 | Tree loss: 3.679 | Accuracy: 0.087891 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 018 | Total loss: 3.716 | Reg loss: 0.024 | Tree loss: 3.716 | Accuracy: 0.072266 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 018 | Total loss: 3.668 | Reg loss: 0.024 | Tree loss: 3.668 | Accuracy: 0.078125 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 018 | Total loss: 3.614 | Reg loss: 0.024 | Tree loss: 3.614 | Accuracy: 0.093750 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 018 | Total loss: 3.612 | Reg loss: 0.024 | Tree loss: 3.612 | Accuracy: 0.093750 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 018 | Total loss: 3.716 | Reg loss: 0.024 | Tree loss: 3.716 | Accuracy: 0.060547 | 5.144 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 018 | Total loss: 3.638 | Reg loss: 0.024 | Tree loss: 3.638 | Accuracy: 0.083984 | 5.143 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 018 | Total loss: 3.669 | Reg loss: 0.024 | Tree loss: 3.669 | Accuracy: 0.078125 | 5.143 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 018 | Total loss: 3.485 | Reg loss: 0.024 | Tree loss: 3.485 | Accuracy: 0.146341 | 5.142 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 018 | Total loss: 3.682 | Reg loss: 0.024 | Tree loss: 3.682 | Accuracy: 0.082031 | 5.142 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 018 | Total loss: 3.698 | Reg loss: 0.024 | Tree loss: 3.698 | Accuracy: 0.082031 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 018 | Total loss: 3.666 | Reg loss: 0.024 | Tree loss: 3.666 | Accuracy: 0.074219 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 018 | Total loss: 3.687 | Reg loss: 0.024 | Tree loss: 3.687 | Accuracy: 0.087891 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 018 | Total loss: 3.702 | Reg loss: 0.024 | Tree loss: 3.702 | Accuracy: 0.085938 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 018 | Total loss: 3.689 | Reg loss: 0.024 | Tree loss: 3.689 | Accuracy: 0.080078 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 018 | Total loss: 3.693 | Reg loss: 0.024 | Tree loss: 3.693 | Accuracy: 0.068359 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 018 | Total loss: 3.689 | Reg loss: 0.024 | Tree loss: 3.689 | Accuracy: 0.070312 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 018 | Total loss: 3.639 | Reg loss: 0.024 | Tree loss: 3.639 | Accuracy: 0.089844 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 018 | Total loss: 3.657 | Reg loss: 0.024 | Tree loss: 3.657 | Accuracy: 0.095703 | 5.143 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 018 | Total loss: 3.657 | Reg loss: 0.024 | Tree loss: 3.657 | Accuracy: 0.087891 | 5.142 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 018 | Total loss: 3.613 | Reg loss: 0.024 | Tree loss: 3.613 | Accuracy: 0.093750 | 5.142 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 018 | Total loss: 3.668 | Reg loss: 0.024 | Tree loss: 3.668 | Accuracy: 0.058594 | 5.142 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 018 | Total loss: 3.627 | Reg loss: 0.024 | Tree loss: 3.627 | Accuracy: 0.076172 | 5.142 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 018 | Total loss: 3.627 | Reg loss: 0.024 | Tree loss: 3.627 | Accuracy: 0.089844 | 5.141 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 018 | Total loss: 3.646 | Reg loss: 0.024 | Tree loss: 3.646 | Accuracy: 0.082031 | 5.141 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 018 | Total loss: 3.702 | Reg loss: 0.024 | Tree loss: 3.702 | Accuracy: 0.072266 | 5.141 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 018 | Total loss: 3.705 | Reg loss: 0.024 | Tree loss: 3.705 | Accuracy: 0.097561 | 5.14 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 018 | Total loss: 3.718 | Reg loss: 0.024 | Tree loss: 3.718 | Accuracy: 0.070312 | 5.142 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 018 | Total loss: 3.638 | Reg loss: 0.024 | Tree loss: 3.638 | Accuracy: 0.076172 | 5.142 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 018 | Total loss: 3.688 | Reg loss: 0.024 | Tree loss: 3.688 | Accuracy: 0.074219 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 018 | Total loss: 3.735 | Reg loss: 0.024 | Tree loss: 3.735 | Accuracy: 0.080078 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 018 | Total loss: 3.700 | Reg loss: 0.024 | Tree loss: 3.700 | Accuracy: 0.089844 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 018 | Total loss: 3.638 | Reg loss: 0.024 | Tree loss: 3.638 | Accuracy: 0.070312 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 018 | Total loss: 3.626 | Reg loss: 0.024 | Tree loss: 3.626 | Accuracy: 0.091797 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 018 | Total loss: 3.683 | Reg loss: 0.024 | Tree loss: 3.683 | Accuracy: 0.072266 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 018 | Total loss: 3.633 | Reg loss: 0.024 | Tree loss: 3.633 | Accuracy: 0.083984 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 018 | Total loss: 3.659 | Reg loss: 0.024 | Tree loss: 3.659 | Accuracy: 0.066406 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 018 | Total loss: 3.612 | Reg loss: 0.024 | Tree loss: 3.612 | Accuracy: 0.087891 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 018 | Total loss: 3.699 | Reg loss: 0.024 | Tree loss: 3.699 | Accuracy: 0.080078 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 018 | Total loss: 3.713 | Reg loss: 0.024 | Tree loss: 3.713 | Accuracy: 0.082031 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 018 | Total loss: 3.659 | Reg loss: 0.024 | Tree loss: 3.659 | Accuracy: 0.072266 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 018 | Total loss: 3.623 | Reg loss: 0.024 | Tree loss: 3.623 | Accuracy: 0.089844 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 018 | Total loss: 3.623 | Reg loss: 0.024 | Tree loss: 3.623 | Accuracy: 0.099609 | 5.143 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 018 | Total loss: 3.638 | Reg loss: 0.024 | Tree loss: 3.638 | Accuracy: 0.093750 | 5.142 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 018 | Total loss: 3.737 | Reg loss: 0.024 | Tree loss: 3.737 | Accuracy: 0.048780 | 5.141 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 018 | Total loss: 3.661 | Reg loss: 0.024 | Tree loss: 3.661 | Accuracy: 0.082031 | 5.143 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 018 | Total loss: 3.652 | Reg loss: 0.024 | Tree loss: 3.652 | Accuracy: 0.068359 | 5.144 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 018 | Total loss: 3.640 | Reg loss: 0.024 | Tree loss: 3.640 | Accuracy: 0.082031 | 5.144 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 018 | Total loss: 3.688 | Reg loss: 0.024 | Tree loss: 3.688 | Accuracy: 0.070312 | 5.144 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 004 / 018 | Total loss: 3.672 | Reg loss: 0.024 | Tree loss: 3.672 | Accuracy: 0.080078 | 5.145 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 018 | Total loss: 3.619 | Reg loss: 0.024 | Tree loss: 3.619 | Accuracy: 0.072266 | 5.145 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 018 | Total loss: 3.718 | Reg loss: 0.024 | Tree loss: 3.718 | Accuracy: 0.089844 | 5.145 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 018 | Total loss: 3.681 | Reg loss: 0.024 | Tree loss: 3.681 | Accuracy: 0.082031 | 5.145 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 018 | Total loss: 3.689 | Reg loss: 0.024 | Tree loss: 3.689 | Accuracy: 0.085938 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 018 | Total loss: 3.727 | Reg loss: 0.024 | Tree loss: 3.727 | Accuracy: 0.085938 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 018 | Total loss: 3.691 | Reg loss: 0.024 | Tree loss: 3.691 | Accuracy: 0.064453 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 018 | Total loss: 3.631 | Reg loss: 0.024 | Tree loss: 3.631 | Accuracy: 0.091797 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 018 | Total loss: 3.597 | Reg loss: 0.024 | Tree loss: 3.597 | Accuracy: 0.085938 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 018 | Total loss: 3.663 | Reg loss: 0.024 | Tree loss: 3.663 | Accuracy: 0.080078 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 018 | Total loss: 3.657 | Reg loss: 0.024 | Tree loss: 3.657 | Accuracy: 0.091797 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 018 | Total loss: 3.660 | Reg loss: 0.024 | Tree loss: 3.660 | Accuracy: 0.076172 | 5.146 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 018 | Total loss: 3.598 | Reg loss: 0.024 | Tree loss: 3.598 | Accuracy: 0.085938 | 5.145 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 018 | Total loss: 3.522 | Reg loss: 0.024 | Tree loss: 3.522 | Accuracy: 0.121951 | 5.144 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490c405471594d27bd2c28d1dac5b408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5126ce9fd184f948d1323d4ac9d8570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab4e4f836cf4079afc260068527ddf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2bfe9df3754e099affa0131318f1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 12.0\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 4096\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "5662\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "3083\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "============== Pattern 1025 ==============\n",
      "============== Pattern 1026 ==============\n",
      "============== Pattern 1027 ==============\n",
      "============== Pattern 1028 ==============\n",
      "============== Pattern 1029 ==============\n",
      "============== Pattern 1030 ==============\n",
      "============== Pattern 1031 ==============\n",
      "============== Pattern 1032 ==============\n",
      "============== Pattern 1033 ==============\n",
      "============== Pattern 1034 ==============\n",
      "============== Pattern 1035 ==============\n",
      "============== Pattern 1036 ==============\n",
      "============== Pattern 1037 ==============\n",
      "============== Pattern 1038 ==============\n",
      "============== Pattern 1039 ==============\n",
      "============== Pattern 1040 ==============\n",
      "============== Pattern 1041 ==============\n",
      "============== Pattern 1042 ==============\n",
      "============== Pattern 1043 ==============\n",
      "============== Pattern 1044 ==============\n",
      "============== Pattern 1045 ==============\n",
      "============== Pattern 1046 ==============\n",
      "============== Pattern 1047 ==============\n",
      "============== Pattern 1048 ==============\n",
      "============== Pattern 1049 ==============\n",
      "============== Pattern 1050 ==============\n",
      "============== Pattern 1051 ==============\n",
      "============== Pattern 1052 ==============\n",
      "============== Pattern 1053 ==============\n",
      "============== Pattern 1054 ==============\n",
      "============== Pattern 1055 ==============\n",
      "============== Pattern 1056 ==============\n",
      "============== Pattern 1057 ==============\n",
      "============== Pattern 1058 ==============\n",
      "============== Pattern 1059 ==============\n",
      "============== Pattern 1060 ==============\n",
      "============== Pattern 1061 ==============\n",
      "============== Pattern 1062 ==============\n",
      "============== Pattern 1063 ==============\n",
      "============== Pattern 1064 ==============\n",
      "============== Pattern 1065 ==============\n",
      "============== Pattern 1066 ==============\n",
      "============== Pattern 1067 ==============\n",
      "============== Pattern 1068 ==============\n",
      "============== Pattern 1069 ==============\n",
      "============== Pattern 1070 ==============\n",
      "============== Pattern 1071 ==============\n",
      "============== Pattern 1072 ==============\n",
      "============== Pattern 1073 ==============\n",
      "============== Pattern 1074 ==============\n",
      "============== Pattern 1075 ==============\n",
      "============== Pattern 1076 ==============\n",
      "============== Pattern 1077 ==============\n",
      "============== Pattern 1078 ==============\n",
      "============== Pattern 1079 ==============\n",
      "============== Pattern 1080 ==============\n",
      "============== Pattern 1081 ==============\n",
      "============== Pattern 1082 ==============\n",
      "============== Pattern 1083 ==============\n",
      "============== Pattern 1084 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1085 ==============\n",
      "============== Pattern 1086 ==============\n",
      "============== Pattern 1087 ==============\n",
      "============== Pattern 1088 ==============\n",
      "============== Pattern 1089 ==============\n",
      "============== Pattern 1090 ==============\n",
      "============== Pattern 1091 ==============\n",
      "============== Pattern 1092 ==============\n",
      "============== Pattern 1093 ==============\n",
      "============== Pattern 1094 ==============\n",
      "============== Pattern 1095 ==============\n",
      "============== Pattern 1096 ==============\n",
      "============== Pattern 1097 ==============\n",
      "============== Pattern 1098 ==============\n",
      "============== Pattern 1099 ==============\n",
      "============== Pattern 1100 ==============\n",
      "============== Pattern 1101 ==============\n",
      "============== Pattern 1102 ==============\n",
      "============== Pattern 1103 ==============\n",
      "============== Pattern 1104 ==============\n",
      "============== Pattern 1105 ==============\n",
      "============== Pattern 1106 ==============\n",
      "============== Pattern 1107 ==============\n",
      "============== Pattern 1108 ==============\n",
      "============== Pattern 1109 ==============\n",
      "============== Pattern 1110 ==============\n",
      "============== Pattern 1111 ==============\n",
      "============== Pattern 1112 ==============\n",
      "============== Pattern 1113 ==============\n",
      "============== Pattern 1114 ==============\n",
      "============== Pattern 1115 ==============\n",
      "============== Pattern 1116 ==============\n",
      "============== Pattern 1117 ==============\n",
      "============== Pattern 1118 ==============\n",
      "============== Pattern 1119 ==============\n",
      "============== Pattern 1120 ==============\n",
      "============== Pattern 1121 ==============\n",
      "============== Pattern 1122 ==============\n",
      "============== Pattern 1123 ==============\n",
      "============== Pattern 1124 ==============\n",
      "============== Pattern 1125 ==============\n",
      "============== Pattern 1126 ==============\n",
      "============== Pattern 1127 ==============\n",
      "============== Pattern 1128 ==============\n",
      "============== Pattern 1129 ==============\n",
      "============== Pattern 1130 ==============\n",
      "============== Pattern 1131 ==============\n",
      "============== Pattern 1132 ==============\n",
      "============== Pattern 1133 ==============\n",
      "============== Pattern 1134 ==============\n",
      "============== Pattern 1135 ==============\n",
      "============== Pattern 1136 ==============\n",
      "============== Pattern 1137 ==============\n",
      "============== Pattern 1138 ==============\n",
      "============== Pattern 1139 ==============\n",
      "============== Pattern 1140 ==============\n",
      "============== Pattern 1141 ==============\n",
      "============== Pattern 1142 ==============\n",
      "============== Pattern 1143 ==============\n",
      "============== Pattern 1144 ==============\n",
      "============== Pattern 1145 ==============\n",
      "============== Pattern 1146 ==============\n",
      "============== Pattern 1147 ==============\n",
      "============== Pattern 1148 ==============\n",
      "============== Pattern 1149 ==============\n",
      "============== Pattern 1150 ==============\n",
      "============== Pattern 1151 ==============\n",
      "============== Pattern 1152 ==============\n",
      "============== Pattern 1153 ==============\n",
      "============== Pattern 1154 ==============\n",
      "============== Pattern 1155 ==============\n",
      "============== Pattern 1156 ==============\n",
      "============== Pattern 1157 ==============\n",
      "============== Pattern 1158 ==============\n",
      "============== Pattern 1159 ==============\n",
      "============== Pattern 1160 ==============\n",
      "============== Pattern 1161 ==============\n",
      "============== Pattern 1162 ==============\n",
      "============== Pattern 1163 ==============\n",
      "============== Pattern 1164 ==============\n",
      "============== Pattern 1165 ==============\n",
      "============== Pattern 1166 ==============\n",
      "============== Pattern 1167 ==============\n",
      "============== Pattern 1168 ==============\n",
      "============== Pattern 1169 ==============\n",
      "============== Pattern 1170 ==============\n",
      "============== Pattern 1171 ==============\n",
      "============== Pattern 1172 ==============\n",
      "============== Pattern 1173 ==============\n",
      "============== Pattern 1174 ==============\n",
      "============== Pattern 1175 ==============\n",
      "============== Pattern 1176 ==============\n",
      "============== Pattern 1177 ==============\n",
      "============== Pattern 1178 ==============\n",
      "============== Pattern 1179 ==============\n",
      "============== Pattern 1180 ==============\n",
      "============== Pattern 1181 ==============\n",
      "============== Pattern 1182 ==============\n",
      "============== Pattern 1183 ==============\n",
      "============== Pattern 1184 ==============\n",
      "============== Pattern 1185 ==============\n",
      "============== Pattern 1186 ==============\n",
      "============== Pattern 1187 ==============\n",
      "============== Pattern 1188 ==============\n",
      "============== Pattern 1189 ==============\n",
      "============== Pattern 1190 ==============\n",
      "============== Pattern 1191 ==============\n",
      "============== Pattern 1192 ==============\n",
      "============== Pattern 1193 ==============\n",
      "============== Pattern 1194 ==============\n",
      "============== Pattern 1195 ==============\n",
      "============== Pattern 1196 ==============\n",
      "============== Pattern 1197 ==============\n",
      "============== Pattern 1198 ==============\n",
      "============== Pattern 1199 ==============\n",
      "============== Pattern 1200 ==============\n",
      "============== Pattern 1201 ==============\n",
      "============== Pattern 1202 ==============\n",
      "============== Pattern 1203 ==============\n",
      "============== Pattern 1204 ==============\n",
      "============== Pattern 1205 ==============\n",
      "============== Pattern 1206 ==============\n",
      "============== Pattern 1207 ==============\n",
      "============== Pattern 1208 ==============\n",
      "============== Pattern 1209 ==============\n",
      "============== Pattern 1210 ==============\n",
      "============== Pattern 1211 ==============\n",
      "============== Pattern 1212 ==============\n",
      "============== Pattern 1213 ==============\n",
      "============== Pattern 1214 ==============\n",
      "============== Pattern 1215 ==============\n",
      "============== Pattern 1216 ==============\n",
      "============== Pattern 1217 ==============\n",
      "============== Pattern 1218 ==============\n",
      "============== Pattern 1219 ==============\n",
      "============== Pattern 1220 ==============\n",
      "============== Pattern 1221 ==============\n",
      "============== Pattern 1222 ==============\n",
      "============== Pattern 1223 ==============\n",
      "============== Pattern 1224 ==============\n",
      "============== Pattern 1225 ==============\n",
      "============== Pattern 1226 ==============\n",
      "============== Pattern 1227 ==============\n",
      "============== Pattern 1228 ==============\n",
      "============== Pattern 1229 ==============\n",
      "============== Pattern 1230 ==============\n",
      "============== Pattern 1231 ==============\n",
      "============== Pattern 1232 ==============\n",
      "============== Pattern 1233 ==============\n",
      "============== Pattern 1234 ==============\n",
      "============== Pattern 1235 ==============\n",
      "============== Pattern 1236 ==============\n",
      "============== Pattern 1237 ==============\n",
      "============== Pattern 1238 ==============\n",
      "============== Pattern 1239 ==============\n",
      "============== Pattern 1240 ==============\n",
      "============== Pattern 1241 ==============\n",
      "============== Pattern 1242 ==============\n",
      "============== Pattern 1243 ==============\n",
      "============== Pattern 1244 ==============\n",
      "============== Pattern 1245 ==============\n",
      "============== Pattern 1246 ==============\n",
      "============== Pattern 1247 ==============\n",
      "============== Pattern 1248 ==============\n",
      "============== Pattern 1249 ==============\n",
      "============== Pattern 1250 ==============\n",
      "============== Pattern 1251 ==============\n",
      "============== Pattern 1252 ==============\n",
      "============== Pattern 1253 ==============\n",
      "============== Pattern 1254 ==============\n",
      "============== Pattern 1255 ==============\n",
      "============== Pattern 1256 ==============\n",
      "============== Pattern 1257 ==============\n",
      "============== Pattern 1258 ==============\n",
      "============== Pattern 1259 ==============\n",
      "============== Pattern 1260 ==============\n",
      "============== Pattern 1261 ==============\n",
      "============== Pattern 1262 ==============\n",
      "============== Pattern 1263 ==============\n",
      "============== Pattern 1264 ==============\n",
      "============== Pattern 1265 ==============\n",
      "============== Pattern 1266 ==============\n",
      "============== Pattern 1267 ==============\n",
      "============== Pattern 1268 ==============\n",
      "============== Pattern 1269 ==============\n",
      "============== Pattern 1270 ==============\n",
      "============== Pattern 1271 ==============\n",
      "============== Pattern 1272 ==============\n",
      "============== Pattern 1273 ==============\n",
      "============== Pattern 1274 ==============\n",
      "============== Pattern 1275 ==============\n",
      "============== Pattern 1276 ==============\n",
      "============== Pattern 1277 ==============\n",
      "============== Pattern 1278 ==============\n",
      "============== Pattern 1279 ==============\n",
      "============== Pattern 1280 ==============\n",
      "============== Pattern 1281 ==============\n",
      "============== Pattern 1282 ==============\n",
      "============== Pattern 1283 ==============\n",
      "============== Pattern 1284 ==============\n",
      "============== Pattern 1285 ==============\n",
      "============== Pattern 1286 ==============\n",
      "============== Pattern 1287 ==============\n",
      "============== Pattern 1288 ==============\n",
      "============== Pattern 1289 ==============\n",
      "============== Pattern 1290 ==============\n",
      "============== Pattern 1291 ==============\n",
      "============== Pattern 1292 ==============\n",
      "============== Pattern 1293 ==============\n",
      "============== Pattern 1294 ==============\n",
      "============== Pattern 1295 ==============\n",
      "============== Pattern 1296 ==============\n",
      "============== Pattern 1297 ==============\n",
      "============== Pattern 1298 ==============\n",
      "============== Pattern 1299 ==============\n",
      "============== Pattern 1300 ==============\n",
      "============== Pattern 1301 ==============\n",
      "============== Pattern 1302 ==============\n",
      "============== Pattern 1303 ==============\n",
      "============== Pattern 1304 ==============\n",
      "============== Pattern 1305 ==============\n",
      "============== Pattern 1306 ==============\n",
      "============== Pattern 1307 ==============\n",
      "============== Pattern 1308 ==============\n",
      "============== Pattern 1309 ==============\n",
      "============== Pattern 1310 ==============\n",
      "============== Pattern 1311 ==============\n",
      "============== Pattern 1312 ==============\n",
      "============== Pattern 1313 ==============\n",
      "============== Pattern 1314 ==============\n",
      "============== Pattern 1315 ==============\n",
      "============== Pattern 1316 ==============\n",
      "============== Pattern 1317 ==============\n",
      "============== Pattern 1318 ==============\n",
      "============== Pattern 1319 ==============\n",
      "============== Pattern 1320 ==============\n",
      "============== Pattern 1321 ==============\n",
      "============== Pattern 1322 ==============\n",
      "============== Pattern 1323 ==============\n",
      "============== Pattern 1324 ==============\n",
      "============== Pattern 1325 ==============\n",
      "============== Pattern 1326 ==============\n",
      "============== Pattern 1327 ==============\n",
      "============== Pattern 1328 ==============\n",
      "============== Pattern 1329 ==============\n",
      "============== Pattern 1330 ==============\n",
      "============== Pattern 1331 ==============\n",
      "============== Pattern 1332 ==============\n",
      "============== Pattern 1333 ==============\n",
      "============== Pattern 1334 ==============\n",
      "============== Pattern 1335 ==============\n",
      "============== Pattern 1336 ==============\n",
      "============== Pattern 1337 ==============\n",
      "============== Pattern 1338 ==============\n",
      "============== Pattern 1339 ==============\n",
      "============== Pattern 1340 ==============\n",
      "============== Pattern 1341 ==============\n",
      "============== Pattern 1342 ==============\n",
      "============== Pattern 1343 ==============\n",
      "============== Pattern 1344 ==============\n",
      "============== Pattern 1345 ==============\n",
      "============== Pattern 1346 ==============\n",
      "============== Pattern 1347 ==============\n",
      "============== Pattern 1348 ==============\n",
      "============== Pattern 1349 ==============\n",
      "============== Pattern 1350 ==============\n",
      "============== Pattern 1351 ==============\n",
      "============== Pattern 1352 ==============\n",
      "============== Pattern 1353 ==============\n",
      "============== Pattern 1354 ==============\n",
      "============== Pattern 1355 ==============\n",
      "============== Pattern 1356 ==============\n",
      "============== Pattern 1357 ==============\n",
      "============== Pattern 1358 ==============\n",
      "============== Pattern 1359 ==============\n",
      "============== Pattern 1360 ==============\n",
      "============== Pattern 1361 ==============\n",
      "============== Pattern 1362 ==============\n",
      "============== Pattern 1363 ==============\n",
      "============== Pattern 1364 ==============\n",
      "============== Pattern 1365 ==============\n",
      "============== Pattern 1366 ==============\n",
      "============== Pattern 1367 ==============\n",
      "============== Pattern 1368 ==============\n",
      "============== Pattern 1369 ==============\n",
      "============== Pattern 1370 ==============\n",
      "============== Pattern 1371 ==============\n",
      "============== Pattern 1372 ==============\n",
      "============== Pattern 1373 ==============\n",
      "============== Pattern 1374 ==============\n",
      "============== Pattern 1375 ==============\n",
      "============== Pattern 1376 ==============\n",
      "============== Pattern 1377 ==============\n",
      "============== Pattern 1378 ==============\n",
      "============== Pattern 1379 ==============\n",
      "============== Pattern 1380 ==============\n",
      "============== Pattern 1381 ==============\n",
      "============== Pattern 1382 ==============\n",
      "============== Pattern 1383 ==============\n",
      "============== Pattern 1384 ==============\n",
      "============== Pattern 1385 ==============\n",
      "============== Pattern 1386 ==============\n",
      "============== Pattern 1387 ==============\n",
      "============== Pattern 1388 ==============\n",
      "============== Pattern 1389 ==============\n",
      "============== Pattern 1390 ==============\n",
      "============== Pattern 1391 ==============\n",
      "============== Pattern 1392 ==============\n",
      "============== Pattern 1393 ==============\n",
      "============== Pattern 1394 ==============\n",
      "============== Pattern 1395 ==============\n",
      "============== Pattern 1396 ==============\n",
      "============== Pattern 1397 ==============\n",
      "============== Pattern 1398 ==============\n",
      "============== Pattern 1399 ==============\n",
      "============== Pattern 1400 ==============\n",
      "============== Pattern 1401 ==============\n",
      "============== Pattern 1402 ==============\n",
      "============== Pattern 1403 ==============\n",
      "============== Pattern 1404 ==============\n",
      "============== Pattern 1405 ==============\n",
      "============== Pattern 1406 ==============\n",
      "============== Pattern 1407 ==============\n",
      "============== Pattern 1408 ==============\n",
      "============== Pattern 1409 ==============\n",
      "============== Pattern 1410 ==============\n",
      "============== Pattern 1411 ==============\n",
      "============== Pattern 1412 ==============\n",
      "============== Pattern 1413 ==============\n",
      "============== Pattern 1414 ==============\n",
      "============== Pattern 1415 ==============\n",
      "============== Pattern 1416 ==============\n",
      "============== Pattern 1417 ==============\n",
      "============== Pattern 1418 ==============\n",
      "============== Pattern 1419 ==============\n",
      "============== Pattern 1420 ==============\n",
      "============== Pattern 1421 ==============\n",
      "============== Pattern 1422 ==============\n",
      "============== Pattern 1423 ==============\n",
      "============== Pattern 1424 ==============\n",
      "============== Pattern 1425 ==============\n",
      "============== Pattern 1426 ==============\n",
      "============== Pattern 1427 ==============\n",
      "============== Pattern 1428 ==============\n",
      "============== Pattern 1429 ==============\n",
      "============== Pattern 1430 ==============\n",
      "============== Pattern 1431 ==============\n",
      "============== Pattern 1432 ==============\n",
      "============== Pattern 1433 ==============\n",
      "============== Pattern 1434 ==============\n",
      "============== Pattern 1435 ==============\n",
      "============== Pattern 1436 ==============\n",
      "============== Pattern 1437 ==============\n",
      "============== Pattern 1438 ==============\n",
      "============== Pattern 1439 ==============\n",
      "============== Pattern 1440 ==============\n",
      "============== Pattern 1441 ==============\n",
      "============== Pattern 1442 ==============\n",
      "============== Pattern 1443 ==============\n",
      "============== Pattern 1444 ==============\n",
      "============== Pattern 1445 ==============\n",
      "============== Pattern 1446 ==============\n",
      "============== Pattern 1447 ==============\n",
      "============== Pattern 1448 ==============\n",
      "============== Pattern 1449 ==============\n",
      "============== Pattern 1450 ==============\n",
      "============== Pattern 1451 ==============\n",
      "============== Pattern 1452 ==============\n",
      "============== Pattern 1453 ==============\n",
      "============== Pattern 1454 ==============\n",
      "============== Pattern 1455 ==============\n",
      "============== Pattern 1456 ==============\n",
      "============== Pattern 1457 ==============\n",
      "============== Pattern 1458 ==============\n",
      "============== Pattern 1459 ==============\n",
      "============== Pattern 1460 ==============\n",
      "============== Pattern 1461 ==============\n",
      "============== Pattern 1462 ==============\n",
      "============== Pattern 1463 ==============\n",
      "============== Pattern 1464 ==============\n",
      "============== Pattern 1465 ==============\n",
      "============== Pattern 1466 ==============\n",
      "============== Pattern 1467 ==============\n",
      "============== Pattern 1468 ==============\n",
      "============== Pattern 1469 ==============\n",
      "============== Pattern 1470 ==============\n",
      "============== Pattern 1471 ==============\n",
      "============== Pattern 1472 ==============\n",
      "============== Pattern 1473 ==============\n",
      "============== Pattern 1474 ==============\n",
      "============== Pattern 1475 ==============\n",
      "============== Pattern 1476 ==============\n",
      "============== Pattern 1477 ==============\n",
      "============== Pattern 1478 ==============\n",
      "============== Pattern 1479 ==============\n",
      "============== Pattern 1480 ==============\n",
      "============== Pattern 1481 ==============\n",
      "============== Pattern 1482 ==============\n",
      "============== Pattern 1483 ==============\n",
      "============== Pattern 1484 ==============\n",
      "============== Pattern 1485 ==============\n",
      "============== Pattern 1486 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1487 ==============\n",
      "============== Pattern 1488 ==============\n",
      "============== Pattern 1489 ==============\n",
      "============== Pattern 1490 ==============\n",
      "============== Pattern 1491 ==============\n",
      "============== Pattern 1492 ==============\n",
      "============== Pattern 1493 ==============\n",
      "============== Pattern 1494 ==============\n",
      "============== Pattern 1495 ==============\n",
      "============== Pattern 1496 ==============\n",
      "============== Pattern 1497 ==============\n",
      "============== Pattern 1498 ==============\n",
      "============== Pattern 1499 ==============\n",
      "============== Pattern 1500 ==============\n",
      "============== Pattern 1501 ==============\n",
      "============== Pattern 1502 ==============\n",
      "============== Pattern 1503 ==============\n",
      "============== Pattern 1504 ==============\n",
      "============== Pattern 1505 ==============\n",
      "============== Pattern 1506 ==============\n",
      "============== Pattern 1507 ==============\n",
      "============== Pattern 1508 ==============\n",
      "============== Pattern 1509 ==============\n",
      "============== Pattern 1510 ==============\n",
      "============== Pattern 1511 ==============\n",
      "============== Pattern 1512 ==============\n",
      "============== Pattern 1513 ==============\n",
      "============== Pattern 1514 ==============\n",
      "============== Pattern 1515 ==============\n",
      "============== Pattern 1516 ==============\n",
      "============== Pattern 1517 ==============\n",
      "============== Pattern 1518 ==============\n",
      "============== Pattern 1519 ==============\n",
      "============== Pattern 1520 ==============\n",
      "============== Pattern 1521 ==============\n",
      "============== Pattern 1522 ==============\n",
      "============== Pattern 1523 ==============\n",
      "============== Pattern 1524 ==============\n",
      "============== Pattern 1525 ==============\n",
      "============== Pattern 1526 ==============\n",
      "============== Pattern 1527 ==============\n",
      "============== Pattern 1528 ==============\n",
      "============== Pattern 1529 ==============\n",
      "============== Pattern 1530 ==============\n",
      "============== Pattern 1531 ==============\n",
      "============== Pattern 1532 ==============\n",
      "============== Pattern 1533 ==============\n",
      "============== Pattern 1534 ==============\n",
      "============== Pattern 1535 ==============\n",
      "============== Pattern 1536 ==============\n",
      "============== Pattern 1537 ==============\n",
      "============== Pattern 1538 ==============\n",
      "============== Pattern 1539 ==============\n",
      "============== Pattern 1540 ==============\n",
      "============== Pattern 1541 ==============\n",
      "============== Pattern 1542 ==============\n",
      "============== Pattern 1543 ==============\n",
      "============== Pattern 1544 ==============\n",
      "============== Pattern 1545 ==============\n",
      "============== Pattern 1546 ==============\n",
      "============== Pattern 1547 ==============\n",
      "============== Pattern 1548 ==============\n",
      "============== Pattern 1549 ==============\n",
      "============== Pattern 1550 ==============\n",
      "============== Pattern 1551 ==============\n",
      "============== Pattern 1552 ==============\n",
      "============== Pattern 1553 ==============\n",
      "============== Pattern 1554 ==============\n",
      "============== Pattern 1555 ==============\n",
      "============== Pattern 1556 ==============\n",
      "============== Pattern 1557 ==============\n",
      "============== Pattern 1558 ==============\n",
      "============== Pattern 1559 ==============\n",
      "============== Pattern 1560 ==============\n",
      "============== Pattern 1561 ==============\n",
      "============== Pattern 1562 ==============\n",
      "============== Pattern 1563 ==============\n",
      "============== Pattern 1564 ==============\n",
      "============== Pattern 1565 ==============\n",
      "============== Pattern 1566 ==============\n",
      "============== Pattern 1567 ==============\n",
      "============== Pattern 1568 ==============\n",
      "============== Pattern 1569 ==============\n",
      "============== Pattern 1570 ==============\n",
      "============== Pattern 1571 ==============\n",
      "============== Pattern 1572 ==============\n",
      "============== Pattern 1573 ==============\n",
      "============== Pattern 1574 ==============\n",
      "============== Pattern 1575 ==============\n",
      "============== Pattern 1576 ==============\n",
      "============== Pattern 1577 ==============\n",
      "============== Pattern 1578 ==============\n",
      "============== Pattern 1579 ==============\n",
      "============== Pattern 1580 ==============\n",
      "============== Pattern 1581 ==============\n",
      "============== Pattern 1582 ==============\n",
      "============== Pattern 1583 ==============\n",
      "============== Pattern 1584 ==============\n",
      "============== Pattern 1585 ==============\n",
      "============== Pattern 1586 ==============\n",
      "============== Pattern 1587 ==============\n",
      "============== Pattern 1588 ==============\n",
      "============== Pattern 1589 ==============\n",
      "============== Pattern 1590 ==============\n",
      "============== Pattern 1591 ==============\n",
      "============== Pattern 1592 ==============\n",
      "============== Pattern 1593 ==============\n",
      "============== Pattern 1594 ==============\n",
      "============== Pattern 1595 ==============\n",
      "============== Pattern 1596 ==============\n",
      "============== Pattern 1597 ==============\n",
      "============== Pattern 1598 ==============\n",
      "============== Pattern 1599 ==============\n",
      "============== Pattern 1600 ==============\n",
      "============== Pattern 1601 ==============\n",
      "============== Pattern 1602 ==============\n",
      "============== Pattern 1603 ==============\n",
      "============== Pattern 1604 ==============\n",
      "============== Pattern 1605 ==============\n",
      "============== Pattern 1606 ==============\n",
      "============== Pattern 1607 ==============\n",
      "============== Pattern 1608 ==============\n",
      "============== Pattern 1609 ==============\n",
      "============== Pattern 1610 ==============\n",
      "============== Pattern 1611 ==============\n",
      "============== Pattern 1612 ==============\n",
      "============== Pattern 1613 ==============\n",
      "============== Pattern 1614 ==============\n",
      "============== Pattern 1615 ==============\n",
      "============== Pattern 1616 ==============\n",
      "============== Pattern 1617 ==============\n",
      "============== Pattern 1618 ==============\n",
      "============== Pattern 1619 ==============\n",
      "============== Pattern 1620 ==============\n",
      "============== Pattern 1621 ==============\n",
      "============== Pattern 1622 ==============\n",
      "============== Pattern 1623 ==============\n",
      "============== Pattern 1624 ==============\n",
      "============== Pattern 1625 ==============\n",
      "============== Pattern 1626 ==============\n",
      "============== Pattern 1627 ==============\n",
      "============== Pattern 1628 ==============\n",
      "============== Pattern 1629 ==============\n",
      "============== Pattern 1630 ==============\n",
      "============== Pattern 1631 ==============\n",
      "============== Pattern 1632 ==============\n",
      "============== Pattern 1633 ==============\n",
      "============== Pattern 1634 ==============\n",
      "============== Pattern 1635 ==============\n",
      "============== Pattern 1636 ==============\n",
      "============== Pattern 1637 ==============\n",
      "============== Pattern 1638 ==============\n",
      "============== Pattern 1639 ==============\n",
      "============== Pattern 1640 ==============\n",
      "============== Pattern 1641 ==============\n",
      "============== Pattern 1642 ==============\n",
      "============== Pattern 1643 ==============\n",
      "============== Pattern 1644 ==============\n",
      "============== Pattern 1645 ==============\n",
      "============== Pattern 1646 ==============\n",
      "============== Pattern 1647 ==============\n",
      "============== Pattern 1648 ==============\n",
      "============== Pattern 1649 ==============\n",
      "============== Pattern 1650 ==============\n",
      "============== Pattern 1651 ==============\n",
      "============== Pattern 1652 ==============\n",
      "============== Pattern 1653 ==============\n",
      "============== Pattern 1654 ==============\n",
      "============== Pattern 1655 ==============\n",
      "============== Pattern 1656 ==============\n",
      "============== Pattern 1657 ==============\n",
      "============== Pattern 1658 ==============\n",
      "============== Pattern 1659 ==============\n",
      "============== Pattern 1660 ==============\n",
      "============== Pattern 1661 ==============\n",
      "============== Pattern 1662 ==============\n",
      "============== Pattern 1663 ==============\n",
      "============== Pattern 1664 ==============\n",
      "============== Pattern 1665 ==============\n",
      "============== Pattern 1666 ==============\n",
      "============== Pattern 1667 ==============\n",
      "============== Pattern 1668 ==============\n",
      "============== Pattern 1669 ==============\n",
      "============== Pattern 1670 ==============\n",
      "============== Pattern 1671 ==============\n",
      "============== Pattern 1672 ==============\n",
      "============== Pattern 1673 ==============\n",
      "============== Pattern 1674 ==============\n",
      "============== Pattern 1675 ==============\n",
      "============== Pattern 1676 ==============\n",
      "============== Pattern 1677 ==============\n",
      "============== Pattern 1678 ==============\n",
      "============== Pattern 1679 ==============\n",
      "============== Pattern 1680 ==============\n",
      "============== Pattern 1681 ==============\n",
      "============== Pattern 1682 ==============\n",
      "============== Pattern 1683 ==============\n",
      "============== Pattern 1684 ==============\n",
      "============== Pattern 1685 ==============\n",
      "============== Pattern 1686 ==============\n",
      "============== Pattern 1687 ==============\n",
      "============== Pattern 1688 ==============\n",
      "============== Pattern 1689 ==============\n",
      "============== Pattern 1690 ==============\n",
      "============== Pattern 1691 ==============\n",
      "============== Pattern 1692 ==============\n",
      "============== Pattern 1693 ==============\n",
      "============== Pattern 1694 ==============\n",
      "============== Pattern 1695 ==============\n",
      "============== Pattern 1696 ==============\n",
      "============== Pattern 1697 ==============\n",
      "============== Pattern 1698 ==============\n",
      "============== Pattern 1699 ==============\n",
      "============== Pattern 1700 ==============\n",
      "============== Pattern 1701 ==============\n",
      "============== Pattern 1702 ==============\n",
      "============== Pattern 1703 ==============\n",
      "============== Pattern 1704 ==============\n",
      "============== Pattern 1705 ==============\n",
      "============== Pattern 1706 ==============\n",
      "============== Pattern 1707 ==============\n",
      "============== Pattern 1708 ==============\n",
      "============== Pattern 1709 ==============\n",
      "============== Pattern 1710 ==============\n",
      "============== Pattern 1711 ==============\n",
      "============== Pattern 1712 ==============\n",
      "============== Pattern 1713 ==============\n",
      "============== Pattern 1714 ==============\n",
      "============== Pattern 1715 ==============\n",
      "============== Pattern 1716 ==============\n",
      "============== Pattern 1717 ==============\n",
      "============== Pattern 1718 ==============\n",
      "============== Pattern 1719 ==============\n",
      "============== Pattern 1720 ==============\n",
      "============== Pattern 1721 ==============\n",
      "============== Pattern 1722 ==============\n",
      "============== Pattern 1723 ==============\n",
      "============== Pattern 1724 ==============\n",
      "============== Pattern 1725 ==============\n",
      "============== Pattern 1726 ==============\n",
      "============== Pattern 1727 ==============\n",
      "============== Pattern 1728 ==============\n",
      "============== Pattern 1729 ==============\n",
      "============== Pattern 1730 ==============\n",
      "============== Pattern 1731 ==============\n",
      "============== Pattern 1732 ==============\n",
      "============== Pattern 1733 ==============\n",
      "============== Pattern 1734 ==============\n",
      "============== Pattern 1735 ==============\n",
      "============== Pattern 1736 ==============\n",
      "============== Pattern 1737 ==============\n",
      "============== Pattern 1738 ==============\n",
      "============== Pattern 1739 ==============\n",
      "============== Pattern 1740 ==============\n",
      "============== Pattern 1741 ==============\n",
      "============== Pattern 1742 ==============\n",
      "============== Pattern 1743 ==============\n",
      "============== Pattern 1744 ==============\n",
      "============== Pattern 1745 ==============\n",
      "============== Pattern 1746 ==============\n",
      "============== Pattern 1747 ==============\n",
      "============== Pattern 1748 ==============\n",
      "============== Pattern 1749 ==============\n",
      "============== Pattern 1750 ==============\n",
      "============== Pattern 1751 ==============\n",
      "============== Pattern 1752 ==============\n",
      "============== Pattern 1753 ==============\n",
      "============== Pattern 1754 ==============\n",
      "============== Pattern 1755 ==============\n",
      "============== Pattern 1756 ==============\n",
      "============== Pattern 1757 ==============\n",
      "============== Pattern 1758 ==============\n",
      "============== Pattern 1759 ==============\n",
      "============== Pattern 1760 ==============\n",
      "============== Pattern 1761 ==============\n",
      "============== Pattern 1762 ==============\n",
      "============== Pattern 1763 ==============\n",
      "============== Pattern 1764 ==============\n",
      "============== Pattern 1765 ==============\n",
      "============== Pattern 1766 ==============\n",
      "============== Pattern 1767 ==============\n",
      "============== Pattern 1768 ==============\n",
      "============== Pattern 1769 ==============\n",
      "============== Pattern 1770 ==============\n",
      "============== Pattern 1771 ==============\n",
      "============== Pattern 1772 ==============\n",
      "============== Pattern 1773 ==============\n",
      "============== Pattern 1774 ==============\n",
      "============== Pattern 1775 ==============\n",
      "============== Pattern 1776 ==============\n",
      "============== Pattern 1777 ==============\n",
      "============== Pattern 1778 ==============\n",
      "============== Pattern 1779 ==============\n",
      "============== Pattern 1780 ==============\n",
      "============== Pattern 1781 ==============\n",
      "============== Pattern 1782 ==============\n",
      "============== Pattern 1783 ==============\n",
      "============== Pattern 1784 ==============\n",
      "============== Pattern 1785 ==============\n",
      "============== Pattern 1786 ==============\n",
      "============== Pattern 1787 ==============\n",
      "============== Pattern 1788 ==============\n",
      "============== Pattern 1789 ==============\n",
      "============== Pattern 1790 ==============\n",
      "============== Pattern 1791 ==============\n",
      "============== Pattern 1792 ==============\n",
      "============== Pattern 1793 ==============\n",
      "============== Pattern 1794 ==============\n",
      "============== Pattern 1795 ==============\n",
      "============== Pattern 1796 ==============\n",
      "============== Pattern 1797 ==============\n",
      "============== Pattern 1798 ==============\n",
      "============== Pattern 1799 ==============\n",
      "============== Pattern 1800 ==============\n",
      "============== Pattern 1801 ==============\n",
      "============== Pattern 1802 ==============\n",
      "============== Pattern 1803 ==============\n",
      "============== Pattern 1804 ==============\n",
      "============== Pattern 1805 ==============\n",
      "============== Pattern 1806 ==============\n",
      "============== Pattern 1807 ==============\n",
      "============== Pattern 1808 ==============\n",
      "============== Pattern 1809 ==============\n",
      "============== Pattern 1810 ==============\n",
      "============== Pattern 1811 ==============\n",
      "============== Pattern 1812 ==============\n",
      "============== Pattern 1813 ==============\n",
      "============== Pattern 1814 ==============\n",
      "============== Pattern 1815 ==============\n",
      "============== Pattern 1816 ==============\n",
      "============== Pattern 1817 ==============\n",
      "============== Pattern 1818 ==============\n",
      "============== Pattern 1819 ==============\n",
      "============== Pattern 1820 ==============\n",
      "============== Pattern 1821 ==============\n",
      "============== Pattern 1822 ==============\n",
      "============== Pattern 1823 ==============\n",
      "============== Pattern 1824 ==============\n",
      "============== Pattern 1825 ==============\n",
      "============== Pattern 1826 ==============\n",
      "============== Pattern 1827 ==============\n",
      "============== Pattern 1828 ==============\n",
      "============== Pattern 1829 ==============\n",
      "============== Pattern 1830 ==============\n",
      "============== Pattern 1831 ==============\n",
      "============== Pattern 1832 ==============\n",
      "============== Pattern 1833 ==============\n",
      "============== Pattern 1834 ==============\n",
      "============== Pattern 1835 ==============\n",
      "============== Pattern 1836 ==============\n",
      "============== Pattern 1837 ==============\n",
      "============== Pattern 1838 ==============\n",
      "============== Pattern 1839 ==============\n",
      "============== Pattern 1840 ==============\n",
      "============== Pattern 1841 ==============\n",
      "============== Pattern 1842 ==============\n",
      "============== Pattern 1843 ==============\n",
      "============== Pattern 1844 ==============\n",
      "============== Pattern 1845 ==============\n",
      "============== Pattern 1846 ==============\n",
      "============== Pattern 1847 ==============\n",
      "============== Pattern 1848 ==============\n",
      "============== Pattern 1849 ==============\n",
      "============== Pattern 1850 ==============\n",
      "============== Pattern 1851 ==============\n",
      "============== Pattern 1852 ==============\n",
      "============== Pattern 1853 ==============\n",
      "============== Pattern 1854 ==============\n",
      "============== Pattern 1855 ==============\n",
      "============== Pattern 1856 ==============\n",
      "============== Pattern 1857 ==============\n",
      "============== Pattern 1858 ==============\n",
      "============== Pattern 1859 ==============\n",
      "============== Pattern 1860 ==============\n",
      "============== Pattern 1861 ==============\n",
      "============== Pattern 1862 ==============\n",
      "============== Pattern 1863 ==============\n",
      "============== Pattern 1864 ==============\n",
      "============== Pattern 1865 ==============\n",
      "============== Pattern 1866 ==============\n",
      "============== Pattern 1867 ==============\n",
      "============== Pattern 1868 ==============\n",
      "============== Pattern 1869 ==============\n",
      "============== Pattern 1870 ==============\n",
      "============== Pattern 1871 ==============\n",
      "============== Pattern 1872 ==============\n",
      "============== Pattern 1873 ==============\n",
      "============== Pattern 1874 ==============\n",
      "============== Pattern 1875 ==============\n",
      "============== Pattern 1876 ==============\n",
      "============== Pattern 1877 ==============\n",
      "============== Pattern 1878 ==============\n",
      "============== Pattern 1879 ==============\n",
      "============== Pattern 1880 ==============\n",
      "============== Pattern 1881 ==============\n",
      "============== Pattern 1882 ==============\n",
      "============== Pattern 1883 ==============\n",
      "============== Pattern 1884 ==============\n",
      "============== Pattern 1885 ==============\n",
      "============== Pattern 1886 ==============\n",
      "============== Pattern 1887 ==============\n",
      "============== Pattern 1888 ==============\n",
      "============== Pattern 1889 ==============\n",
      "============== Pattern 1890 ==============\n",
      "============== Pattern 1891 ==============\n",
      "============== Pattern 1892 ==============\n",
      "============== Pattern 1893 ==============\n",
      "============== Pattern 1894 ==============\n",
      "============== Pattern 1895 ==============\n",
      "============== Pattern 1896 ==============\n",
      "============== Pattern 1897 ==============\n",
      "============== Pattern 1898 ==============\n",
      "============== Pattern 1899 ==============\n",
      "============== Pattern 1900 ==============\n",
      "============== Pattern 1901 ==============\n",
      "============== Pattern 1902 ==============\n",
      "============== Pattern 1903 ==============\n",
      "============== Pattern 1904 ==============\n",
      "============== Pattern 1905 ==============\n",
      "============== Pattern 1906 ==============\n",
      "============== Pattern 1907 ==============\n",
      "============== Pattern 1908 ==============\n",
      "============== Pattern 1909 ==============\n",
      "============== Pattern 1910 ==============\n",
      "============== Pattern 1911 ==============\n",
      "============== Pattern 1912 ==============\n",
      "============== Pattern 1913 ==============\n",
      "============== Pattern 1914 ==============\n",
      "============== Pattern 1915 ==============\n",
      "============== Pattern 1916 ==============\n",
      "============== Pattern 1917 ==============\n",
      "============== Pattern 1918 ==============\n",
      "============== Pattern 1919 ==============\n",
      "============== Pattern 1920 ==============\n",
      "============== Pattern 1921 ==============\n",
      "============== Pattern 1922 ==============\n",
      "============== Pattern 1923 ==============\n",
      "============== Pattern 1924 ==============\n",
      "============== Pattern 1925 ==============\n",
      "============== Pattern 1926 ==============\n",
      "============== Pattern 1927 ==============\n",
      "============== Pattern 1928 ==============\n",
      "============== Pattern 1929 ==============\n",
      "============== Pattern 1930 ==============\n",
      "============== Pattern 1931 ==============\n",
      "============== Pattern 1932 ==============\n",
      "============== Pattern 1933 ==============\n",
      "============== Pattern 1934 ==============\n",
      "============== Pattern 1935 ==============\n",
      "============== Pattern 1936 ==============\n",
      "============== Pattern 1937 ==============\n",
      "============== Pattern 1938 ==============\n",
      "============== Pattern 1939 ==============\n",
      "============== Pattern 1940 ==============\n",
      "============== Pattern 1941 ==============\n",
      "============== Pattern 1942 ==============\n",
      "============== Pattern 1943 ==============\n",
      "============== Pattern 1944 ==============\n",
      "============== Pattern 1945 ==============\n",
      "============== Pattern 1946 ==============\n",
      "============== Pattern 1947 ==============\n",
      "============== Pattern 1948 ==============\n",
      "============== Pattern 1949 ==============\n",
      "============== Pattern 1950 ==============\n",
      "============== Pattern 1951 ==============\n",
      "============== Pattern 1952 ==============\n",
      "============== Pattern 1953 ==============\n",
      "============== Pattern 1954 ==============\n",
      "============== Pattern 1955 ==============\n",
      "============== Pattern 1956 ==============\n",
      "============== Pattern 1957 ==============\n",
      "============== Pattern 1958 ==============\n",
      "============== Pattern 1959 ==============\n",
      "============== Pattern 1960 ==============\n",
      "============== Pattern 1961 ==============\n",
      "============== Pattern 1962 ==============\n",
      "============== Pattern 1963 ==============\n",
      "============== Pattern 1964 ==============\n",
      "============== Pattern 1965 ==============\n",
      "============== Pattern 1966 ==============\n",
      "============== Pattern 1967 ==============\n",
      "============== Pattern 1968 ==============\n",
      "============== Pattern 1969 ==============\n",
      "============== Pattern 1970 ==============\n",
      "============== Pattern 1971 ==============\n",
      "============== Pattern 1972 ==============\n",
      "============== Pattern 1973 ==============\n",
      "============== Pattern 1974 ==============\n",
      "============== Pattern 1975 ==============\n",
      "============== Pattern 1976 ==============\n",
      "============== Pattern 1977 ==============\n",
      "============== Pattern 1978 ==============\n",
      "============== Pattern 1979 ==============\n",
      "============== Pattern 1980 ==============\n",
      "============== Pattern 1981 ==============\n",
      "============== Pattern 1982 ==============\n",
      "============== Pattern 1983 ==============\n",
      "============== Pattern 1984 ==============\n",
      "============== Pattern 1985 ==============\n",
      "============== Pattern 1986 ==============\n",
      "============== Pattern 1987 ==============\n",
      "============== Pattern 1988 ==============\n",
      "============== Pattern 1989 ==============\n",
      "============== Pattern 1990 ==============\n",
      "============== Pattern 1991 ==============\n",
      "============== Pattern 1992 ==============\n",
      "============== Pattern 1993 ==============\n",
      "============== Pattern 1994 ==============\n",
      "============== Pattern 1995 ==============\n",
      "============== Pattern 1996 ==============\n",
      "============== Pattern 1997 ==============\n",
      "============== Pattern 1998 ==============\n",
      "============== Pattern 1999 ==============\n",
      "============== Pattern 2000 ==============\n",
      "============== Pattern 2001 ==============\n",
      "============== Pattern 2002 ==============\n",
      "============== Pattern 2003 ==============\n",
      "============== Pattern 2004 ==============\n",
      "============== Pattern 2005 ==============\n",
      "============== Pattern 2006 ==============\n",
      "============== Pattern 2007 ==============\n",
      "============== Pattern 2008 ==============\n",
      "============== Pattern 2009 ==============\n",
      "============== Pattern 2010 ==============\n",
      "============== Pattern 2011 ==============\n",
      "============== Pattern 2012 ==============\n",
      "============== Pattern 2013 ==============\n",
      "============== Pattern 2014 ==============\n",
      "============== Pattern 2015 ==============\n",
      "============== Pattern 2016 ==============\n",
      "============== Pattern 2017 ==============\n",
      "============== Pattern 2018 ==============\n",
      "============== Pattern 2019 ==============\n",
      "============== Pattern 2020 ==============\n",
      "============== Pattern 2021 ==============\n",
      "============== Pattern 2022 ==============\n",
      "============== Pattern 2023 ==============\n",
      "============== Pattern 2024 ==============\n",
      "============== Pattern 2025 ==============\n",
      "============== Pattern 2026 ==============\n",
      "============== Pattern 2027 ==============\n",
      "============== Pattern 2028 ==============\n",
      "============== Pattern 2029 ==============\n",
      "============== Pattern 2030 ==============\n",
      "============== Pattern 2031 ==============\n",
      "============== Pattern 2032 ==============\n",
      "============== Pattern 2033 ==============\n",
      "============== Pattern 2034 ==============\n",
      "============== Pattern 2035 ==============\n",
      "============== Pattern 2036 ==============\n",
      "============== Pattern 2037 ==============\n",
      "============== Pattern 2038 ==============\n",
      "============== Pattern 2039 ==============\n",
      "============== Pattern 2040 ==============\n",
      "============== Pattern 2041 ==============\n",
      "============== Pattern 2042 ==============\n",
      "============== Pattern 2043 ==============\n",
      "============== Pattern 2044 ==============\n",
      "============== Pattern 2045 ==============\n",
      "============== Pattern 2046 ==============\n",
      "============== Pattern 2047 ==============\n",
      "============== Pattern 2048 ==============\n",
      "============== Pattern 2049 ==============\n",
      "============== Pattern 2050 ==============\n",
      "============== Pattern 2051 ==============\n",
      "============== Pattern 2052 ==============\n",
      "============== Pattern 2053 ==============\n",
      "============== Pattern 2054 ==============\n",
      "============== Pattern 2055 ==============\n",
      "============== Pattern 2056 ==============\n",
      "============== Pattern 2057 ==============\n",
      "============== Pattern 2058 ==============\n",
      "============== Pattern 2059 ==============\n",
      "============== Pattern 2060 ==============\n",
      "============== Pattern 2061 ==============\n",
      "============== Pattern 2062 ==============\n",
      "============== Pattern 2063 ==============\n",
      "============== Pattern 2064 ==============\n",
      "============== Pattern 2065 ==============\n",
      "============== Pattern 2066 ==============\n",
      "============== Pattern 2067 ==============\n",
      "============== Pattern 2068 ==============\n",
      "============== Pattern 2069 ==============\n",
      "============== Pattern 2070 ==============\n",
      "============== Pattern 2071 ==============\n",
      "============== Pattern 2072 ==============\n",
      "============== Pattern 2073 ==============\n",
      "============== Pattern 2074 ==============\n",
      "============== Pattern 2075 ==============\n",
      "============== Pattern 2076 ==============\n",
      "============== Pattern 2077 ==============\n",
      "============== Pattern 2078 ==============\n",
      "============== Pattern 2079 ==============\n",
      "============== Pattern 2080 ==============\n",
      "============== Pattern 2081 ==============\n",
      "============== Pattern 2082 ==============\n",
      "============== Pattern 2083 ==============\n",
      "============== Pattern 2084 ==============\n",
      "============== Pattern 2085 ==============\n",
      "============== Pattern 2086 ==============\n",
      "============== Pattern 2087 ==============\n",
      "============== Pattern 2088 ==============\n",
      "============== Pattern 2089 ==============\n",
      "============== Pattern 2090 ==============\n",
      "============== Pattern 2091 ==============\n",
      "============== Pattern 2092 ==============\n",
      "============== Pattern 2093 ==============\n",
      "============== Pattern 2094 ==============\n",
      "============== Pattern 2095 ==============\n",
      "============== Pattern 2096 ==============\n",
      "============== Pattern 2097 ==============\n",
      "============== Pattern 2098 ==============\n",
      "============== Pattern 2099 ==============\n",
      "============== Pattern 2100 ==============\n",
      "============== Pattern 2101 ==============\n",
      "============== Pattern 2102 ==============\n",
      "============== Pattern 2103 ==============\n",
      "============== Pattern 2104 ==============\n",
      "============== Pattern 2105 ==============\n",
      "============== Pattern 2106 ==============\n",
      "============== Pattern 2107 ==============\n",
      "============== Pattern 2108 ==============\n",
      "============== Pattern 2109 ==============\n",
      "============== Pattern 2110 ==============\n",
      "============== Pattern 2111 ==============\n",
      "============== Pattern 2112 ==============\n",
      "============== Pattern 2113 ==============\n",
      "============== Pattern 2114 ==============\n",
      "============== Pattern 2115 ==============\n",
      "============== Pattern 2116 ==============\n",
      "============== Pattern 2117 ==============\n",
      "============== Pattern 2118 ==============\n",
      "============== Pattern 2119 ==============\n",
      "============== Pattern 2120 ==============\n",
      "============== Pattern 2121 ==============\n",
      "============== Pattern 2122 ==============\n",
      "============== Pattern 2123 ==============\n",
      "============== Pattern 2124 ==============\n",
      "============== Pattern 2125 ==============\n",
      "============== Pattern 2126 ==============\n",
      "============== Pattern 2127 ==============\n",
      "============== Pattern 2128 ==============\n",
      "============== Pattern 2129 ==============\n",
      "============== Pattern 2130 ==============\n",
      "============== Pattern 2131 ==============\n",
      "============== Pattern 2132 ==============\n",
      "============== Pattern 2133 ==============\n",
      "============== Pattern 2134 ==============\n",
      "============== Pattern 2135 ==============\n",
      "============== Pattern 2136 ==============\n",
      "============== Pattern 2137 ==============\n",
      "============== Pattern 2138 ==============\n",
      "============== Pattern 2139 ==============\n",
      "============== Pattern 2140 ==============\n",
      "============== Pattern 2141 ==============\n",
      "============== Pattern 2142 ==============\n",
      "============== Pattern 2143 ==============\n",
      "============== Pattern 2144 ==============\n",
      "============== Pattern 2145 ==============\n",
      "============== Pattern 2146 ==============\n",
      "============== Pattern 2147 ==============\n",
      "============== Pattern 2148 ==============\n",
      "============== Pattern 2149 ==============\n",
      "============== Pattern 2150 ==============\n",
      "============== Pattern 2151 ==============\n",
      "============== Pattern 2152 ==============\n",
      "============== Pattern 2153 ==============\n",
      "============== Pattern 2154 ==============\n",
      "============== Pattern 2155 ==============\n",
      "============== Pattern 2156 ==============\n",
      "============== Pattern 2157 ==============\n",
      "============== Pattern 2158 ==============\n",
      "============== Pattern 2159 ==============\n",
      "============== Pattern 2160 ==============\n",
      "============== Pattern 2161 ==============\n",
      "============== Pattern 2162 ==============\n",
      "============== Pattern 2163 ==============\n",
      "============== Pattern 2164 ==============\n",
      "============== Pattern 2165 ==============\n",
      "============== Pattern 2166 ==============\n",
      "============== Pattern 2167 ==============\n",
      "============== Pattern 2168 ==============\n",
      "============== Pattern 2169 ==============\n",
      "============== Pattern 2170 ==============\n",
      "============== Pattern 2171 ==============\n",
      "============== Pattern 2172 ==============\n",
      "============== Pattern 2173 ==============\n",
      "============== Pattern 2174 ==============\n",
      "============== Pattern 2175 ==============\n",
      "============== Pattern 2176 ==============\n",
      "============== Pattern 2177 ==============\n",
      "============== Pattern 2178 ==============\n",
      "============== Pattern 2179 ==============\n",
      "============== Pattern 2180 ==============\n",
      "============== Pattern 2181 ==============\n",
      "============== Pattern 2182 ==============\n",
      "============== Pattern 2183 ==============\n",
      "============== Pattern 2184 ==============\n",
      "============== Pattern 2185 ==============\n",
      "============== Pattern 2186 ==============\n",
      "============== Pattern 2187 ==============\n",
      "============== Pattern 2188 ==============\n",
      "============== Pattern 2189 ==============\n",
      "============== Pattern 2190 ==============\n",
      "============== Pattern 2191 ==============\n",
      "============== Pattern 2192 ==============\n",
      "============== Pattern 2193 ==============\n",
      "============== Pattern 2194 ==============\n",
      "============== Pattern 2195 ==============\n",
      "============== Pattern 2196 ==============\n",
      "============== Pattern 2197 ==============\n",
      "============== Pattern 2198 ==============\n",
      "============== Pattern 2199 ==============\n",
      "============== Pattern 2200 ==============\n",
      "============== Pattern 2201 ==============\n",
      "============== Pattern 2202 ==============\n",
      "============== Pattern 2203 ==============\n",
      "============== Pattern 2204 ==============\n",
      "============== Pattern 2205 ==============\n",
      "============== Pattern 2206 ==============\n",
      "============== Pattern 2207 ==============\n",
      "============== Pattern 2208 ==============\n",
      "============== Pattern 2209 ==============\n",
      "============== Pattern 2210 ==============\n",
      "============== Pattern 2211 ==============\n",
      "============== Pattern 2212 ==============\n",
      "============== Pattern 2213 ==============\n",
      "============== Pattern 2214 ==============\n",
      "============== Pattern 2215 ==============\n",
      "============== Pattern 2216 ==============\n",
      "============== Pattern 2217 ==============\n",
      "============== Pattern 2218 ==============\n",
      "============== Pattern 2219 ==============\n",
      "============== Pattern 2220 ==============\n",
      "============== Pattern 2221 ==============\n",
      "============== Pattern 2222 ==============\n",
      "============== Pattern 2223 ==============\n",
      "============== Pattern 2224 ==============\n",
      "============== Pattern 2225 ==============\n",
      "============== Pattern 2226 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2227 ==============\n",
      "============== Pattern 2228 ==============\n",
      "============== Pattern 2229 ==============\n",
      "============== Pattern 2230 ==============\n",
      "============== Pattern 2231 ==============\n",
      "============== Pattern 2232 ==============\n",
      "============== Pattern 2233 ==============\n",
      "============== Pattern 2234 ==============\n",
      "============== Pattern 2235 ==============\n",
      "============== Pattern 2236 ==============\n",
      "============== Pattern 2237 ==============\n",
      "============== Pattern 2238 ==============\n",
      "============== Pattern 2239 ==============\n",
      "============== Pattern 2240 ==============\n",
      "============== Pattern 2241 ==============\n",
      "============== Pattern 2242 ==============\n",
      "============== Pattern 2243 ==============\n",
      "============== Pattern 2244 ==============\n",
      "============== Pattern 2245 ==============\n",
      "============== Pattern 2246 ==============\n",
      "============== Pattern 2247 ==============\n",
      "============== Pattern 2248 ==============\n",
      "============== Pattern 2249 ==============\n",
      "============== Pattern 2250 ==============\n",
      "============== Pattern 2251 ==============\n",
      "============== Pattern 2252 ==============\n",
      "============== Pattern 2253 ==============\n",
      "============== Pattern 2254 ==============\n",
      "============== Pattern 2255 ==============\n",
      "============== Pattern 2256 ==============\n",
      "============== Pattern 2257 ==============\n",
      "============== Pattern 2258 ==============\n",
      "============== Pattern 2259 ==============\n",
      "============== Pattern 2260 ==============\n",
      "============== Pattern 2261 ==============\n",
      "============== Pattern 2262 ==============\n",
      "============== Pattern 2263 ==============\n",
      "============== Pattern 2264 ==============\n",
      "============== Pattern 2265 ==============\n",
      "============== Pattern 2266 ==============\n",
      "============== Pattern 2267 ==============\n",
      "============== Pattern 2268 ==============\n",
      "============== Pattern 2269 ==============\n",
      "============== Pattern 2270 ==============\n",
      "============== Pattern 2271 ==============\n",
      "============== Pattern 2272 ==============\n",
      "============== Pattern 2273 ==============\n",
      "============== Pattern 2274 ==============\n",
      "============== Pattern 2275 ==============\n",
      "============== Pattern 2276 ==============\n",
      "============== Pattern 2277 ==============\n",
      "============== Pattern 2278 ==============\n",
      "============== Pattern 2279 ==============\n",
      "============== Pattern 2280 ==============\n",
      "============== Pattern 2281 ==============\n",
      "============== Pattern 2282 ==============\n",
      "============== Pattern 2283 ==============\n",
      "============== Pattern 2284 ==============\n",
      "============== Pattern 2285 ==============\n",
      "============== Pattern 2286 ==============\n",
      "============== Pattern 2287 ==============\n",
      "============== Pattern 2288 ==============\n",
      "============== Pattern 2289 ==============\n",
      "============== Pattern 2290 ==============\n",
      "============== Pattern 2291 ==============\n",
      "============== Pattern 2292 ==============\n",
      "============== Pattern 2293 ==============\n",
      "============== Pattern 2294 ==============\n",
      "============== Pattern 2295 ==============\n",
      "============== Pattern 2296 ==============\n",
      "============== Pattern 2297 ==============\n",
      "============== Pattern 2298 ==============\n",
      "============== Pattern 2299 ==============\n",
      "============== Pattern 2300 ==============\n",
      "============== Pattern 2301 ==============\n",
      "============== Pattern 2302 ==============\n",
      "============== Pattern 2303 ==============\n",
      "============== Pattern 2304 ==============\n",
      "============== Pattern 2305 ==============\n",
      "============== Pattern 2306 ==============\n",
      "============== Pattern 2307 ==============\n",
      "============== Pattern 2308 ==============\n",
      "============== Pattern 2309 ==============\n",
      "============== Pattern 2310 ==============\n",
      "============== Pattern 2311 ==============\n",
      "============== Pattern 2312 ==============\n",
      "============== Pattern 2313 ==============\n",
      "============== Pattern 2314 ==============\n",
      "============== Pattern 2315 ==============\n",
      "============== Pattern 2316 ==============\n",
      "============== Pattern 2317 ==============\n",
      "============== Pattern 2318 ==============\n",
      "============== Pattern 2319 ==============\n",
      "============== Pattern 2320 ==============\n",
      "============== Pattern 2321 ==============\n",
      "============== Pattern 2322 ==============\n",
      "============== Pattern 2323 ==============\n",
      "============== Pattern 2324 ==============\n",
      "============== Pattern 2325 ==============\n",
      "============== Pattern 2326 ==============\n",
      "============== Pattern 2327 ==============\n",
      "============== Pattern 2328 ==============\n",
      "============== Pattern 2329 ==============\n",
      "============== Pattern 2330 ==============\n",
      "============== Pattern 2331 ==============\n",
      "============== Pattern 2332 ==============\n",
      "============== Pattern 2333 ==============\n",
      "============== Pattern 2334 ==============\n",
      "============== Pattern 2335 ==============\n",
      "============== Pattern 2336 ==============\n",
      "============== Pattern 2337 ==============\n",
      "============== Pattern 2338 ==============\n",
      "============== Pattern 2339 ==============\n",
      "============== Pattern 2340 ==============\n",
      "============== Pattern 2341 ==============\n",
      "============== Pattern 2342 ==============\n",
      "============== Pattern 2343 ==============\n",
      "============== Pattern 2344 ==============\n",
      "============== Pattern 2345 ==============\n",
      "============== Pattern 2346 ==============\n",
      "============== Pattern 2347 ==============\n",
      "============== Pattern 2348 ==============\n",
      "============== Pattern 2349 ==============\n",
      "============== Pattern 2350 ==============\n",
      "============== Pattern 2351 ==============\n",
      "============== Pattern 2352 ==============\n",
      "============== Pattern 2353 ==============\n",
      "============== Pattern 2354 ==============\n",
      "============== Pattern 2355 ==============\n",
      "============== Pattern 2356 ==============\n",
      "============== Pattern 2357 ==============\n",
      "============== Pattern 2358 ==============\n",
      "============== Pattern 2359 ==============\n",
      "============== Pattern 2360 ==============\n",
      "============== Pattern 2361 ==============\n",
      "============== Pattern 2362 ==============\n",
      "============== Pattern 2363 ==============\n",
      "============== Pattern 2364 ==============\n",
      "============== Pattern 2365 ==============\n",
      "============== Pattern 2366 ==============\n",
      "============== Pattern 2367 ==============\n",
      "============== Pattern 2368 ==============\n",
      "============== Pattern 2369 ==============\n",
      "============== Pattern 2370 ==============\n",
      "============== Pattern 2371 ==============\n",
      "============== Pattern 2372 ==============\n",
      "============== Pattern 2373 ==============\n",
      "============== Pattern 2374 ==============\n",
      "============== Pattern 2375 ==============\n",
      "============== Pattern 2376 ==============\n",
      "============== Pattern 2377 ==============\n",
      "============== Pattern 2378 ==============\n",
      "============== Pattern 2379 ==============\n",
      "============== Pattern 2380 ==============\n",
      "============== Pattern 2381 ==============\n",
      "============== Pattern 2382 ==============\n",
      "============== Pattern 2383 ==============\n",
      "============== Pattern 2384 ==============\n",
      "============== Pattern 2385 ==============\n",
      "============== Pattern 2386 ==============\n",
      "============== Pattern 2387 ==============\n",
      "============== Pattern 2388 ==============\n",
      "============== Pattern 2389 ==============\n",
      "============== Pattern 2390 ==============\n",
      "============== Pattern 2391 ==============\n",
      "============== Pattern 2392 ==============\n",
      "============== Pattern 2393 ==============\n",
      "============== Pattern 2394 ==============\n",
      "============== Pattern 2395 ==============\n",
      "============== Pattern 2396 ==============\n",
      "============== Pattern 2397 ==============\n",
      "============== Pattern 2398 ==============\n",
      "============== Pattern 2399 ==============\n",
      "============== Pattern 2400 ==============\n",
      "============== Pattern 2401 ==============\n",
      "============== Pattern 2402 ==============\n",
      "============== Pattern 2403 ==============\n",
      "============== Pattern 2404 ==============\n",
      "============== Pattern 2405 ==============\n",
      "============== Pattern 2406 ==============\n",
      "============== Pattern 2407 ==============\n",
      "============== Pattern 2408 ==============\n",
      "============== Pattern 2409 ==============\n",
      "============== Pattern 2410 ==============\n",
      "============== Pattern 2411 ==============\n",
      "============== Pattern 2412 ==============\n",
      "============== Pattern 2413 ==============\n",
      "============== Pattern 2414 ==============\n",
      "============== Pattern 2415 ==============\n",
      "============== Pattern 2416 ==============\n",
      "============== Pattern 2417 ==============\n",
      "============== Pattern 2418 ==============\n",
      "============== Pattern 2419 ==============\n",
      "============== Pattern 2420 ==============\n",
      "============== Pattern 2421 ==============\n",
      "============== Pattern 2422 ==============\n",
      "============== Pattern 2423 ==============\n",
      "============== Pattern 2424 ==============\n",
      "============== Pattern 2425 ==============\n",
      "============== Pattern 2426 ==============\n",
      "============== Pattern 2427 ==============\n",
      "============== Pattern 2428 ==============\n",
      "============== Pattern 2429 ==============\n",
      "============== Pattern 2430 ==============\n",
      "============== Pattern 2431 ==============\n",
      "============== Pattern 2432 ==============\n",
      "============== Pattern 2433 ==============\n",
      "============== Pattern 2434 ==============\n",
      "============== Pattern 2435 ==============\n",
      "============== Pattern 2436 ==============\n",
      "============== Pattern 2437 ==============\n",
      "============== Pattern 2438 ==============\n",
      "============== Pattern 2439 ==============\n",
      "============== Pattern 2440 ==============\n",
      "============== Pattern 2441 ==============\n",
      "============== Pattern 2442 ==============\n",
      "============== Pattern 2443 ==============\n",
      "============== Pattern 2444 ==============\n",
      "============== Pattern 2445 ==============\n",
      "============== Pattern 2446 ==============\n",
      "============== Pattern 2447 ==============\n",
      "============== Pattern 2448 ==============\n",
      "============== Pattern 2449 ==============\n",
      "============== Pattern 2450 ==============\n",
      "============== Pattern 2451 ==============\n",
      "============== Pattern 2452 ==============\n",
      "============== Pattern 2453 ==============\n",
      "============== Pattern 2454 ==============\n",
      "============== Pattern 2455 ==============\n",
      "============== Pattern 2456 ==============\n",
      "============== Pattern 2457 ==============\n",
      "============== Pattern 2458 ==============\n",
      "============== Pattern 2459 ==============\n",
      "============== Pattern 2460 ==============\n",
      "============== Pattern 2461 ==============\n",
      "============== Pattern 2462 ==============\n",
      "============== Pattern 2463 ==============\n",
      "============== Pattern 2464 ==============\n",
      "============== Pattern 2465 ==============\n",
      "============== Pattern 2466 ==============\n",
      "============== Pattern 2467 ==============\n",
      "============== Pattern 2468 ==============\n",
      "============== Pattern 2469 ==============\n",
      "============== Pattern 2470 ==============\n",
      "============== Pattern 2471 ==============\n",
      "============== Pattern 2472 ==============\n",
      "============== Pattern 2473 ==============\n",
      "============== Pattern 2474 ==============\n",
      "============== Pattern 2475 ==============\n",
      "============== Pattern 2476 ==============\n",
      "============== Pattern 2477 ==============\n",
      "============== Pattern 2478 ==============\n",
      "============== Pattern 2479 ==============\n",
      "============== Pattern 2480 ==============\n",
      "============== Pattern 2481 ==============\n",
      "============== Pattern 2482 ==============\n",
      "============== Pattern 2483 ==============\n",
      "============== Pattern 2484 ==============\n",
      "============== Pattern 2485 ==============\n",
      "============== Pattern 2486 ==============\n",
      "============== Pattern 2487 ==============\n",
      "============== Pattern 2488 ==============\n",
      "============== Pattern 2489 ==============\n",
      "============== Pattern 2490 ==============\n",
      "============== Pattern 2491 ==============\n",
      "============== Pattern 2492 ==============\n",
      "============== Pattern 2493 ==============\n",
      "============== Pattern 2494 ==============\n",
      "============== Pattern 2495 ==============\n",
      "============== Pattern 2496 ==============\n",
      "============== Pattern 2497 ==============\n",
      "============== Pattern 2498 ==============\n",
      "============== Pattern 2499 ==============\n",
      "============== Pattern 2500 ==============\n",
      "============== Pattern 2501 ==============\n",
      "============== Pattern 2502 ==============\n",
      "============== Pattern 2503 ==============\n",
      "============== Pattern 2504 ==============\n",
      "============== Pattern 2505 ==============\n",
      "============== Pattern 2506 ==============\n",
      "============== Pattern 2507 ==============\n",
      "============== Pattern 2508 ==============\n",
      "============== Pattern 2509 ==============\n",
      "============== Pattern 2510 ==============\n",
      "============== Pattern 2511 ==============\n",
      "============== Pattern 2512 ==============\n",
      "============== Pattern 2513 ==============\n",
      "============== Pattern 2514 ==============\n",
      "============== Pattern 2515 ==============\n",
      "============== Pattern 2516 ==============\n",
      "============== Pattern 2517 ==============\n",
      "============== Pattern 2518 ==============\n",
      "============== Pattern 2519 ==============\n",
      "============== Pattern 2520 ==============\n",
      "============== Pattern 2521 ==============\n",
      "============== Pattern 2522 ==============\n",
      "============== Pattern 2523 ==============\n",
      "============== Pattern 2524 ==============\n",
      "============== Pattern 2525 ==============\n",
      "============== Pattern 2526 ==============\n",
      "============== Pattern 2527 ==============\n",
      "============== Pattern 2528 ==============\n",
      "============== Pattern 2529 ==============\n",
      "============== Pattern 2530 ==============\n",
      "============== Pattern 2531 ==============\n",
      "============== Pattern 2532 ==============\n",
      "============== Pattern 2533 ==============\n",
      "============== Pattern 2534 ==============\n",
      "============== Pattern 2535 ==============\n",
      "============== Pattern 2536 ==============\n",
      "============== Pattern 2537 ==============\n",
      "============== Pattern 2538 ==============\n",
      "============== Pattern 2539 ==============\n",
      "============== Pattern 2540 ==============\n",
      "============== Pattern 2541 ==============\n",
      "============== Pattern 2542 ==============\n",
      "============== Pattern 2543 ==============\n",
      "============== Pattern 2544 ==============\n",
      "============== Pattern 2545 ==============\n",
      "============== Pattern 2546 ==============\n",
      "============== Pattern 2547 ==============\n",
      "============== Pattern 2548 ==============\n",
      "============== Pattern 2549 ==============\n",
      "============== Pattern 2550 ==============\n",
      "============== Pattern 2551 ==============\n",
      "============== Pattern 2552 ==============\n",
      "============== Pattern 2553 ==============\n",
      "============== Pattern 2554 ==============\n",
      "============== Pattern 2555 ==============\n",
      "============== Pattern 2556 ==============\n",
      "============== Pattern 2557 ==============\n",
      "============== Pattern 2558 ==============\n",
      "============== Pattern 2559 ==============\n",
      "============== Pattern 2560 ==============\n",
      "============== Pattern 2561 ==============\n",
      "============== Pattern 2562 ==============\n",
      "============== Pattern 2563 ==============\n",
      "============== Pattern 2564 ==============\n",
      "============== Pattern 2565 ==============\n",
      "============== Pattern 2566 ==============\n",
      "============== Pattern 2567 ==============\n",
      "============== Pattern 2568 ==============\n",
      "============== Pattern 2569 ==============\n",
      "============== Pattern 2570 ==============\n",
      "============== Pattern 2571 ==============\n",
      "============== Pattern 2572 ==============\n",
      "============== Pattern 2573 ==============\n",
      "============== Pattern 2574 ==============\n",
      "============== Pattern 2575 ==============\n",
      "============== Pattern 2576 ==============\n",
      "============== Pattern 2577 ==============\n",
      "============== Pattern 2578 ==============\n",
      "============== Pattern 2579 ==============\n",
      "============== Pattern 2580 ==============\n",
      "============== Pattern 2581 ==============\n",
      "============== Pattern 2582 ==============\n",
      "============== Pattern 2583 ==============\n",
      "============== Pattern 2584 ==============\n",
      "============== Pattern 2585 ==============\n",
      "============== Pattern 2586 ==============\n",
      "============== Pattern 2587 ==============\n",
      "============== Pattern 2588 ==============\n",
      "============== Pattern 2589 ==============\n",
      "============== Pattern 2590 ==============\n",
      "============== Pattern 2591 ==============\n",
      "============== Pattern 2592 ==============\n",
      "============== Pattern 2593 ==============\n",
      "============== Pattern 2594 ==============\n",
      "============== Pattern 2595 ==============\n",
      "============== Pattern 2596 ==============\n",
      "============== Pattern 2597 ==============\n",
      "============== Pattern 2598 ==============\n",
      "============== Pattern 2599 ==============\n",
      "============== Pattern 2600 ==============\n",
      "============== Pattern 2601 ==============\n",
      "============== Pattern 2602 ==============\n",
      "============== Pattern 2603 ==============\n",
      "============== Pattern 2604 ==============\n",
      "============== Pattern 2605 ==============\n",
      "============== Pattern 2606 ==============\n",
      "============== Pattern 2607 ==============\n",
      "============== Pattern 2608 ==============\n",
      "============== Pattern 2609 ==============\n",
      "============== Pattern 2610 ==============\n",
      "============== Pattern 2611 ==============\n",
      "============== Pattern 2612 ==============\n",
      "============== Pattern 2613 ==============\n",
      "============== Pattern 2614 ==============\n",
      "============== Pattern 2615 ==============\n",
      "============== Pattern 2616 ==============\n",
      "============== Pattern 2617 ==============\n",
      "============== Pattern 2618 ==============\n",
      "============== Pattern 2619 ==============\n",
      "============== Pattern 2620 ==============\n",
      "============== Pattern 2621 ==============\n",
      "============== Pattern 2622 ==============\n",
      "============== Pattern 2623 ==============\n",
      "============== Pattern 2624 ==============\n",
      "============== Pattern 2625 ==============\n",
      "============== Pattern 2626 ==============\n",
      "============== Pattern 2627 ==============\n",
      "============== Pattern 2628 ==============\n",
      "============== Pattern 2629 ==============\n",
      "============== Pattern 2630 ==============\n",
      "============== Pattern 2631 ==============\n",
      "============== Pattern 2632 ==============\n",
      "============== Pattern 2633 ==============\n",
      "============== Pattern 2634 ==============\n",
      "============== Pattern 2635 ==============\n",
      "============== Pattern 2636 ==============\n",
      "============== Pattern 2637 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2638 ==============\n",
      "============== Pattern 2639 ==============\n",
      "============== Pattern 2640 ==============\n",
      "============== Pattern 2641 ==============\n",
      "============== Pattern 2642 ==============\n",
      "============== Pattern 2643 ==============\n",
      "============== Pattern 2644 ==============\n",
      "============== Pattern 2645 ==============\n",
      "============== Pattern 2646 ==============\n",
      "============== Pattern 2647 ==============\n",
      "============== Pattern 2648 ==============\n",
      "============== Pattern 2649 ==============\n",
      "============== Pattern 2650 ==============\n",
      "============== Pattern 2651 ==============\n",
      "============== Pattern 2652 ==============\n",
      "============== Pattern 2653 ==============\n",
      "============== Pattern 2654 ==============\n",
      "============== Pattern 2655 ==============\n",
      "============== Pattern 2656 ==============\n",
      "============== Pattern 2657 ==============\n",
      "============== Pattern 2658 ==============\n",
      "============== Pattern 2659 ==============\n",
      "============== Pattern 2660 ==============\n",
      "============== Pattern 2661 ==============\n",
      "============== Pattern 2662 ==============\n",
      "============== Pattern 2663 ==============\n",
      "============== Pattern 2664 ==============\n",
      "============== Pattern 2665 ==============\n",
      "============== Pattern 2666 ==============\n",
      "============== Pattern 2667 ==============\n",
      "============== Pattern 2668 ==============\n",
      "============== Pattern 2669 ==============\n",
      "============== Pattern 2670 ==============\n",
      "============== Pattern 2671 ==============\n",
      "============== Pattern 2672 ==============\n",
      "============== Pattern 2673 ==============\n",
      "============== Pattern 2674 ==============\n",
      "============== Pattern 2675 ==============\n",
      "============== Pattern 2676 ==============\n",
      "============== Pattern 2677 ==============\n",
      "============== Pattern 2678 ==============\n",
      "============== Pattern 2679 ==============\n",
      "============== Pattern 2680 ==============\n",
      "============== Pattern 2681 ==============\n",
      "============== Pattern 2682 ==============\n",
      "============== Pattern 2683 ==============\n",
      "============== Pattern 2684 ==============\n",
      "============== Pattern 2685 ==============\n",
      "============== Pattern 2686 ==============\n",
      "============== Pattern 2687 ==============\n",
      "============== Pattern 2688 ==============\n",
      "============== Pattern 2689 ==============\n",
      "============== Pattern 2690 ==============\n",
      "============== Pattern 2691 ==============\n",
      "============== Pattern 2692 ==============\n",
      "============== Pattern 2693 ==============\n",
      "============== Pattern 2694 ==============\n",
      "============== Pattern 2695 ==============\n",
      "============== Pattern 2696 ==============\n",
      "============== Pattern 2697 ==============\n",
      "============== Pattern 2698 ==============\n",
      "============== Pattern 2699 ==============\n",
      "============== Pattern 2700 ==============\n",
      "============== Pattern 2701 ==============\n",
      "============== Pattern 2702 ==============\n",
      "============== Pattern 2703 ==============\n",
      "============== Pattern 2704 ==============\n",
      "============== Pattern 2705 ==============\n",
      "============== Pattern 2706 ==============\n",
      "============== Pattern 2707 ==============\n",
      "============== Pattern 2708 ==============\n",
      "============== Pattern 2709 ==============\n",
      "============== Pattern 2710 ==============\n",
      "============== Pattern 2711 ==============\n",
      "============== Pattern 2712 ==============\n",
      "============== Pattern 2713 ==============\n",
      "============== Pattern 2714 ==============\n",
      "============== Pattern 2715 ==============\n",
      "============== Pattern 2716 ==============\n",
      "============== Pattern 2717 ==============\n",
      "============== Pattern 2718 ==============\n",
      "============== Pattern 2719 ==============\n",
      "============== Pattern 2720 ==============\n",
      "============== Pattern 2721 ==============\n",
      "============== Pattern 2722 ==============\n",
      "============== Pattern 2723 ==============\n",
      "============== Pattern 2724 ==============\n",
      "============== Pattern 2725 ==============\n",
      "============== Pattern 2726 ==============\n",
      "============== Pattern 2727 ==============\n",
      "============== Pattern 2728 ==============\n",
      "============== Pattern 2729 ==============\n",
      "============== Pattern 2730 ==============\n",
      "============== Pattern 2731 ==============\n",
      "============== Pattern 2732 ==============\n",
      "============== Pattern 2733 ==============\n",
      "============== Pattern 2734 ==============\n",
      "============== Pattern 2735 ==============\n",
      "============== Pattern 2736 ==============\n",
      "============== Pattern 2737 ==============\n",
      "============== Pattern 2738 ==============\n",
      "============== Pattern 2739 ==============\n",
      "============== Pattern 2740 ==============\n",
      "============== Pattern 2741 ==============\n",
      "============== Pattern 2742 ==============\n",
      "============== Pattern 2743 ==============\n",
      "============== Pattern 2744 ==============\n",
      "============== Pattern 2745 ==============\n",
      "============== Pattern 2746 ==============\n",
      "============== Pattern 2747 ==============\n",
      "============== Pattern 2748 ==============\n",
      "============== Pattern 2749 ==============\n",
      "============== Pattern 2750 ==============\n",
      "============== Pattern 2751 ==============\n",
      "============== Pattern 2752 ==============\n",
      "============== Pattern 2753 ==============\n",
      "============== Pattern 2754 ==============\n",
      "============== Pattern 2755 ==============\n",
      "============== Pattern 2756 ==============\n",
      "============== Pattern 2757 ==============\n",
      "============== Pattern 2758 ==============\n",
      "============== Pattern 2759 ==============\n",
      "============== Pattern 2760 ==============\n",
      "============== Pattern 2761 ==============\n",
      "============== Pattern 2762 ==============\n",
      "============== Pattern 2763 ==============\n",
      "============== Pattern 2764 ==============\n",
      "============== Pattern 2765 ==============\n",
      "============== Pattern 2766 ==============\n",
      "============== Pattern 2767 ==============\n",
      "============== Pattern 2768 ==============\n",
      "============== Pattern 2769 ==============\n",
      "============== Pattern 2770 ==============\n",
      "============== Pattern 2771 ==============\n",
      "============== Pattern 2772 ==============\n",
      "============== Pattern 2773 ==============\n",
      "============== Pattern 2774 ==============\n",
      "============== Pattern 2775 ==============\n",
      "============== Pattern 2776 ==============\n",
      "============== Pattern 2777 ==============\n",
      "============== Pattern 2778 ==============\n",
      "============== Pattern 2779 ==============\n",
      "============== Pattern 2780 ==============\n",
      "============== Pattern 2781 ==============\n",
      "============== Pattern 2782 ==============\n",
      "============== Pattern 2783 ==============\n",
      "============== Pattern 2784 ==============\n",
      "============== Pattern 2785 ==============\n",
      "============== Pattern 2786 ==============\n",
      "============== Pattern 2787 ==============\n",
      "============== Pattern 2788 ==============\n",
      "============== Pattern 2789 ==============\n",
      "============== Pattern 2790 ==============\n",
      "============== Pattern 2791 ==============\n",
      "============== Pattern 2792 ==============\n",
      "============== Pattern 2793 ==============\n",
      "============== Pattern 2794 ==============\n",
      "============== Pattern 2795 ==============\n",
      "============== Pattern 2796 ==============\n",
      "============== Pattern 2797 ==============\n",
      "============== Pattern 2798 ==============\n",
      "============== Pattern 2799 ==============\n",
      "============== Pattern 2800 ==============\n",
      "============== Pattern 2801 ==============\n",
      "============== Pattern 2802 ==============\n",
      "============== Pattern 2803 ==============\n",
      "============== Pattern 2804 ==============\n",
      "============== Pattern 2805 ==============\n",
      "============== Pattern 2806 ==============\n",
      "============== Pattern 2807 ==============\n",
      "============== Pattern 2808 ==============\n",
      "============== Pattern 2809 ==============\n",
      "============== Pattern 2810 ==============\n",
      "============== Pattern 2811 ==============\n",
      "============== Pattern 2812 ==============\n",
      "============== Pattern 2813 ==============\n",
      "============== Pattern 2814 ==============\n",
      "============== Pattern 2815 ==============\n",
      "============== Pattern 2816 ==============\n",
      "============== Pattern 2817 ==============\n",
      "============== Pattern 2818 ==============\n",
      "============== Pattern 2819 ==============\n",
      "============== Pattern 2820 ==============\n",
      "============== Pattern 2821 ==============\n",
      "============== Pattern 2822 ==============\n",
      "============== Pattern 2823 ==============\n",
      "============== Pattern 2824 ==============\n",
      "============== Pattern 2825 ==============\n",
      "============== Pattern 2826 ==============\n",
      "============== Pattern 2827 ==============\n",
      "============== Pattern 2828 ==============\n",
      "============== Pattern 2829 ==============\n",
      "============== Pattern 2830 ==============\n",
      "============== Pattern 2831 ==============\n",
      "============== Pattern 2832 ==============\n",
      "============== Pattern 2833 ==============\n",
      "============== Pattern 2834 ==============\n",
      "============== Pattern 2835 ==============\n",
      "============== Pattern 2836 ==============\n",
      "============== Pattern 2837 ==============\n",
      "============== Pattern 2838 ==============\n",
      "============== Pattern 2839 ==============\n",
      "============== Pattern 2840 ==============\n",
      "============== Pattern 2841 ==============\n",
      "============== Pattern 2842 ==============\n",
      "============== Pattern 2843 ==============\n",
      "============== Pattern 2844 ==============\n",
      "============== Pattern 2845 ==============\n",
      "============== Pattern 2846 ==============\n",
      "============== Pattern 2847 ==============\n",
      "============== Pattern 2848 ==============\n",
      "============== Pattern 2849 ==============\n",
      "============== Pattern 2850 ==============\n",
      "============== Pattern 2851 ==============\n",
      "============== Pattern 2852 ==============\n",
      "============== Pattern 2853 ==============\n",
      "============== Pattern 2854 ==============\n",
      "============== Pattern 2855 ==============\n",
      "============== Pattern 2856 ==============\n",
      "============== Pattern 2857 ==============\n",
      "============== Pattern 2858 ==============\n",
      "============== Pattern 2859 ==============\n",
      "============== Pattern 2860 ==============\n",
      "============== Pattern 2861 ==============\n",
      "============== Pattern 2862 ==============\n",
      "============== Pattern 2863 ==============\n",
      "============== Pattern 2864 ==============\n",
      "============== Pattern 2865 ==============\n",
      "============== Pattern 2866 ==============\n",
      "============== Pattern 2867 ==============\n",
      "============== Pattern 2868 ==============\n",
      "============== Pattern 2869 ==============\n",
      "============== Pattern 2870 ==============\n",
      "============== Pattern 2871 ==============\n",
      "============== Pattern 2872 ==============\n",
      "============== Pattern 2873 ==============\n",
      "============== Pattern 2874 ==============\n",
      "============== Pattern 2875 ==============\n",
      "============== Pattern 2876 ==============\n",
      "============== Pattern 2877 ==============\n",
      "============== Pattern 2878 ==============\n",
      "============== Pattern 2879 ==============\n",
      "============== Pattern 2880 ==============\n",
      "============== Pattern 2881 ==============\n",
      "============== Pattern 2882 ==============\n",
      "============== Pattern 2883 ==============\n",
      "============== Pattern 2884 ==============\n",
      "============== Pattern 2885 ==============\n",
      "============== Pattern 2886 ==============\n",
      "============== Pattern 2887 ==============\n",
      "============== Pattern 2888 ==============\n",
      "============== Pattern 2889 ==============\n",
      "============== Pattern 2890 ==============\n",
      "============== Pattern 2891 ==============\n",
      "============== Pattern 2892 ==============\n",
      "============== Pattern 2893 ==============\n",
      "============== Pattern 2894 ==============\n",
      "============== Pattern 2895 ==============\n",
      "============== Pattern 2896 ==============\n",
      "============== Pattern 2897 ==============\n",
      "============== Pattern 2898 ==============\n",
      "============== Pattern 2899 ==============\n",
      "============== Pattern 2900 ==============\n",
      "============== Pattern 2901 ==============\n",
      "============== Pattern 2902 ==============\n",
      "============== Pattern 2903 ==============\n",
      "============== Pattern 2904 ==============\n",
      "============== Pattern 2905 ==============\n",
      "============== Pattern 2906 ==============\n",
      "============== Pattern 2907 ==============\n",
      "============== Pattern 2908 ==============\n",
      "============== Pattern 2909 ==============\n",
      "============== Pattern 2910 ==============\n",
      "============== Pattern 2911 ==============\n",
      "============== Pattern 2912 ==============\n",
      "============== Pattern 2913 ==============\n",
      "============== Pattern 2914 ==============\n",
      "============== Pattern 2915 ==============\n",
      "============== Pattern 2916 ==============\n",
      "============== Pattern 2917 ==============\n",
      "============== Pattern 2918 ==============\n",
      "============== Pattern 2919 ==============\n",
      "============== Pattern 2920 ==============\n",
      "============== Pattern 2921 ==============\n",
      "============== Pattern 2922 ==============\n",
      "============== Pattern 2923 ==============\n",
      "============== Pattern 2924 ==============\n",
      "============== Pattern 2925 ==============\n",
      "============== Pattern 2926 ==============\n",
      "============== Pattern 2927 ==============\n",
      "============== Pattern 2928 ==============\n",
      "============== Pattern 2929 ==============\n",
      "============== Pattern 2930 ==============\n",
      "============== Pattern 2931 ==============\n",
      "============== Pattern 2932 ==============\n",
      "============== Pattern 2933 ==============\n",
      "============== Pattern 2934 ==============\n",
      "============== Pattern 2935 ==============\n",
      "============== Pattern 2936 ==============\n",
      "============== Pattern 2937 ==============\n",
      "============== Pattern 2938 ==============\n",
      "============== Pattern 2939 ==============\n",
      "============== Pattern 2940 ==============\n",
      "============== Pattern 2941 ==============\n",
      "============== Pattern 2942 ==============\n",
      "============== Pattern 2943 ==============\n",
      "============== Pattern 2944 ==============\n",
      "============== Pattern 2945 ==============\n",
      "============== Pattern 2946 ==============\n",
      "============== Pattern 2947 ==============\n",
      "============== Pattern 2948 ==============\n",
      "============== Pattern 2949 ==============\n",
      "============== Pattern 2950 ==============\n",
      "============== Pattern 2951 ==============\n",
      "============== Pattern 2952 ==============\n",
      "============== Pattern 2953 ==============\n",
      "============== Pattern 2954 ==============\n",
      "============== Pattern 2955 ==============\n",
      "============== Pattern 2956 ==============\n",
      "============== Pattern 2957 ==============\n",
      "============== Pattern 2958 ==============\n",
      "============== Pattern 2959 ==============\n",
      "============== Pattern 2960 ==============\n",
      "============== Pattern 2961 ==============\n",
      "============== Pattern 2962 ==============\n",
      "============== Pattern 2963 ==============\n",
      "============== Pattern 2964 ==============\n",
      "============== Pattern 2965 ==============\n",
      "============== Pattern 2966 ==============\n",
      "============== Pattern 2967 ==============\n",
      "============== Pattern 2968 ==============\n",
      "============== Pattern 2969 ==============\n",
      "============== Pattern 2970 ==============\n",
      "============== Pattern 2971 ==============\n",
      "============== Pattern 2972 ==============\n",
      "============== Pattern 2973 ==============\n",
      "============== Pattern 2974 ==============\n",
      "============== Pattern 2975 ==============\n",
      "============== Pattern 2976 ==============\n",
      "============== Pattern 2977 ==============\n",
      "============== Pattern 2978 ==============\n",
      "============== Pattern 2979 ==============\n",
      "============== Pattern 2980 ==============\n",
      "============== Pattern 2981 ==============\n",
      "============== Pattern 2982 ==============\n",
      "============== Pattern 2983 ==============\n",
      "============== Pattern 2984 ==============\n",
      "============== Pattern 2985 ==============\n",
      "============== Pattern 2986 ==============\n",
      "============== Pattern 2987 ==============\n",
      "============== Pattern 2988 ==============\n",
      "============== Pattern 2989 ==============\n",
      "============== Pattern 2990 ==============\n",
      "============== Pattern 2991 ==============\n",
      "============== Pattern 2992 ==============\n",
      "============== Pattern 2993 ==============\n",
      "============== Pattern 2994 ==============\n",
      "============== Pattern 2995 ==============\n",
      "============== Pattern 2996 ==============\n",
      "============== Pattern 2997 ==============\n",
      "============== Pattern 2998 ==============\n",
      "============== Pattern 2999 ==============\n",
      "============== Pattern 3000 ==============\n",
      "============== Pattern 3001 ==============\n",
      "============== Pattern 3002 ==============\n",
      "============== Pattern 3003 ==============\n",
      "============== Pattern 3004 ==============\n",
      "============== Pattern 3005 ==============\n",
      "============== Pattern 3006 ==============\n",
      "============== Pattern 3007 ==============\n",
      "============== Pattern 3008 ==============\n",
      "============== Pattern 3009 ==============\n",
      "============== Pattern 3010 ==============\n",
      "============== Pattern 3011 ==============\n",
      "============== Pattern 3012 ==============\n",
      "============== Pattern 3013 ==============\n",
      "============== Pattern 3014 ==============\n",
      "============== Pattern 3015 ==============\n",
      "============== Pattern 3016 ==============\n",
      "============== Pattern 3017 ==============\n",
      "============== Pattern 3018 ==============\n",
      "============== Pattern 3019 ==============\n",
      "============== Pattern 3020 ==============\n",
      "============== Pattern 3021 ==============\n",
      "============== Pattern 3022 ==============\n",
      "============== Pattern 3023 ==============\n",
      "============== Pattern 3024 ==============\n",
      "============== Pattern 3025 ==============\n",
      "============== Pattern 3026 ==============\n",
      "============== Pattern 3027 ==============\n",
      "============== Pattern 3028 ==============\n",
      "============== Pattern 3029 ==============\n",
      "============== Pattern 3030 ==============\n",
      "============== Pattern 3031 ==============\n",
      "============== Pattern 3032 ==============\n",
      "============== Pattern 3033 ==============\n",
      "============== Pattern 3034 ==============\n",
      "============== Pattern 3035 ==============\n",
      "============== Pattern 3036 ==============\n",
      "============== Pattern 3037 ==============\n",
      "============== Pattern 3038 ==============\n",
      "============== Pattern 3039 ==============\n",
      "============== Pattern 3040 ==============\n",
      "============== Pattern 3041 ==============\n",
      "============== Pattern 3042 ==============\n",
      "============== Pattern 3043 ==============\n",
      "============== Pattern 3044 ==============\n",
      "============== Pattern 3045 ==============\n",
      "============== Pattern 3046 ==============\n",
      "============== Pattern 3047 ==============\n",
      "============== Pattern 3048 ==============\n",
      "============== Pattern 3049 ==============\n",
      "============== Pattern 3050 ==============\n",
      "============== Pattern 3051 ==============\n",
      "============== Pattern 3052 ==============\n",
      "============== Pattern 3053 ==============\n",
      "============== Pattern 3054 ==============\n",
      "============== Pattern 3055 ==============\n",
      "============== Pattern 3056 ==============\n",
      "============== Pattern 3057 ==============\n",
      "============== Pattern 3058 ==============\n",
      "============== Pattern 3059 ==============\n",
      "============== Pattern 3060 ==============\n",
      "============== Pattern 3061 ==============\n",
      "============== Pattern 3062 ==============\n",
      "============== Pattern 3063 ==============\n",
      "============== Pattern 3064 ==============\n",
      "============== Pattern 3065 ==============\n",
      "============== Pattern 3066 ==============\n",
      "============== Pattern 3067 ==============\n",
      "============== Pattern 3068 ==============\n",
      "============== Pattern 3069 ==============\n",
      "============== Pattern 3070 ==============\n",
      "============== Pattern 3071 ==============\n",
      "============== Pattern 3072 ==============\n",
      "============== Pattern 3073 ==============\n",
      "============== Pattern 3074 ==============\n",
      "============== Pattern 3075 ==============\n",
      "============== Pattern 3076 ==============\n",
      "============== Pattern 3077 ==============\n",
      "============== Pattern 3078 ==============\n",
      "============== Pattern 3079 ==============\n",
      "============== Pattern 3080 ==============\n",
      "============== Pattern 3081 ==============\n",
      "============== Pattern 3082 ==============\n",
      "============== Pattern 3083 ==============\n",
      "============== Pattern 3084 ==============\n",
      "============== Pattern 3085 ==============\n",
      "============== Pattern 3086 ==============\n",
      "============== Pattern 3087 ==============\n",
      "============== Pattern 3088 ==============\n",
      "============== Pattern 3089 ==============\n",
      "============== Pattern 3090 ==============\n",
      "============== Pattern 3091 ==============\n",
      "============== Pattern 3092 ==============\n",
      "============== Pattern 3093 ==============\n",
      "============== Pattern 3094 ==============\n",
      "============== Pattern 3095 ==============\n",
      "============== Pattern 3096 ==============\n",
      "============== Pattern 3097 ==============\n",
      "============== Pattern 3098 ==============\n",
      "============== Pattern 3099 ==============\n",
      "============== Pattern 3100 ==============\n",
      "============== Pattern 3101 ==============\n",
      "============== Pattern 3102 ==============\n",
      "============== Pattern 3103 ==============\n",
      "============== Pattern 3104 ==============\n",
      "============== Pattern 3105 ==============\n",
      "============== Pattern 3106 ==============\n",
      "============== Pattern 3107 ==============\n",
      "============== Pattern 3108 ==============\n",
      "============== Pattern 3109 ==============\n",
      "============== Pattern 3110 ==============\n",
      "============== Pattern 3111 ==============\n",
      "============== Pattern 3112 ==============\n",
      "============== Pattern 3113 ==============\n",
      "============== Pattern 3114 ==============\n",
      "============== Pattern 3115 ==============\n",
      "============== Pattern 3116 ==============\n",
      "============== Pattern 3117 ==============\n",
      "============== Pattern 3118 ==============\n",
      "============== Pattern 3119 ==============\n",
      "============== Pattern 3120 ==============\n",
      "============== Pattern 3121 ==============\n",
      "============== Pattern 3122 ==============\n",
      "============== Pattern 3123 ==============\n",
      "============== Pattern 3124 ==============\n",
      "============== Pattern 3125 ==============\n",
      "============== Pattern 3126 ==============\n",
      "============== Pattern 3127 ==============\n",
      "============== Pattern 3128 ==============\n",
      "============== Pattern 3129 ==============\n",
      "============== Pattern 3130 ==============\n",
      "============== Pattern 3131 ==============\n",
      "============== Pattern 3132 ==============\n",
      "============== Pattern 3133 ==============\n",
      "============== Pattern 3134 ==============\n",
      "============== Pattern 3135 ==============\n",
      "============== Pattern 3136 ==============\n",
      "============== Pattern 3137 ==============\n",
      "============== Pattern 3138 ==============\n",
      "============== Pattern 3139 ==============\n",
      "============== Pattern 3140 ==============\n",
      "============== Pattern 3141 ==============\n",
      "============== Pattern 3142 ==============\n",
      "============== Pattern 3143 ==============\n",
      "============== Pattern 3144 ==============\n",
      "============== Pattern 3145 ==============\n",
      "============== Pattern 3146 ==============\n",
      "============== Pattern 3147 ==============\n",
      "============== Pattern 3148 ==============\n",
      "============== Pattern 3149 ==============\n",
      "============== Pattern 3150 ==============\n",
      "============== Pattern 3151 ==============\n",
      "============== Pattern 3152 ==============\n",
      "============== Pattern 3153 ==============\n",
      "============== Pattern 3154 ==============\n",
      "============== Pattern 3155 ==============\n",
      "============== Pattern 3156 ==============\n",
      "============== Pattern 3157 ==============\n",
      "============== Pattern 3158 ==============\n",
      "============== Pattern 3159 ==============\n",
      "============== Pattern 3160 ==============\n",
      "============== Pattern 3161 ==============\n",
      "============== Pattern 3162 ==============\n",
      "============== Pattern 3163 ==============\n",
      "============== Pattern 3164 ==============\n",
      "============== Pattern 3165 ==============\n",
      "============== Pattern 3166 ==============\n",
      "============== Pattern 3167 ==============\n",
      "============== Pattern 3168 ==============\n",
      "============== Pattern 3169 ==============\n",
      "============== Pattern 3170 ==============\n",
      "============== Pattern 3171 ==============\n",
      "============== Pattern 3172 ==============\n",
      "============== Pattern 3173 ==============\n",
      "============== Pattern 3174 ==============\n",
      "============== Pattern 3175 ==============\n",
      "============== Pattern 3176 ==============\n",
      "============== Pattern 3177 ==============\n",
      "============== Pattern 3178 ==============\n",
      "============== Pattern 3179 ==============\n",
      "============== Pattern 3180 ==============\n",
      "============== Pattern 3181 ==============\n",
      "============== Pattern 3182 ==============\n",
      "============== Pattern 3183 ==============\n",
      "============== Pattern 3184 ==============\n",
      "============== Pattern 3185 ==============\n",
      "============== Pattern 3186 ==============\n",
      "============== Pattern 3187 ==============\n",
      "============== Pattern 3188 ==============\n",
      "============== Pattern 3189 ==============\n",
      "============== Pattern 3190 ==============\n",
      "============== Pattern 3191 ==============\n",
      "============== Pattern 3192 ==============\n",
      "============== Pattern 3193 ==============\n",
      "============== Pattern 3194 ==============\n",
      "============== Pattern 3195 ==============\n",
      "============== Pattern 3196 ==============\n",
      "============== Pattern 3197 ==============\n",
      "============== Pattern 3198 ==============\n",
      "============== Pattern 3199 ==============\n",
      "============== Pattern 3200 ==============\n",
      "============== Pattern 3201 ==============\n",
      "============== Pattern 3202 ==============\n",
      "============== Pattern 3203 ==============\n",
      "============== Pattern 3204 ==============\n",
      "============== Pattern 3205 ==============\n",
      "============== Pattern 3206 ==============\n",
      "============== Pattern 3207 ==============\n",
      "============== Pattern 3208 ==============\n",
      "============== Pattern 3209 ==============\n",
      "============== Pattern 3210 ==============\n",
      "============== Pattern 3211 ==============\n",
      "============== Pattern 3212 ==============\n",
      "============== Pattern 3213 ==============\n",
      "============== Pattern 3214 ==============\n",
      "============== Pattern 3215 ==============\n",
      "============== Pattern 3216 ==============\n",
      "============== Pattern 3217 ==============\n",
      "============== Pattern 3218 ==============\n",
      "============== Pattern 3219 ==============\n",
      "============== Pattern 3220 ==============\n",
      "============== Pattern 3221 ==============\n",
      "============== Pattern 3222 ==============\n",
      "============== Pattern 3223 ==============\n",
      "============== Pattern 3224 ==============\n",
      "============== Pattern 3225 ==============\n",
      "============== Pattern 3226 ==============\n",
      "============== Pattern 3227 ==============\n",
      "============== Pattern 3228 ==============\n",
      "============== Pattern 3229 ==============\n",
      "============== Pattern 3230 ==============\n",
      "============== Pattern 3231 ==============\n",
      "============== Pattern 3232 ==============\n",
      "============== Pattern 3233 ==============\n",
      "============== Pattern 3234 ==============\n",
      "============== Pattern 3235 ==============\n",
      "============== Pattern 3236 ==============\n",
      "============== Pattern 3237 ==============\n",
      "============== Pattern 3238 ==============\n",
      "============== Pattern 3239 ==============\n",
      "============== Pattern 3240 ==============\n",
      "============== Pattern 3241 ==============\n",
      "============== Pattern 3242 ==============\n",
      "============== Pattern 3243 ==============\n",
      "============== Pattern 3244 ==============\n",
      "============== Pattern 3245 ==============\n",
      "============== Pattern 3246 ==============\n",
      "============== Pattern 3247 ==============\n",
      "============== Pattern 3248 ==============\n",
      "============== Pattern 3249 ==============\n",
      "============== Pattern 3250 ==============\n",
      "============== Pattern 3251 ==============\n",
      "============== Pattern 3252 ==============\n",
      "============== Pattern 3253 ==============\n",
      "============== Pattern 3254 ==============\n",
      "============== Pattern 3255 ==============\n",
      "============== Pattern 3256 ==============\n",
      "============== Pattern 3257 ==============\n",
      "============== Pattern 3258 ==============\n",
      "============== Pattern 3259 ==============\n",
      "============== Pattern 3260 ==============\n",
      "============== Pattern 3261 ==============\n",
      "============== Pattern 3262 ==============\n",
      "============== Pattern 3263 ==============\n",
      "============== Pattern 3264 ==============\n",
      "============== Pattern 3265 ==============\n",
      "============== Pattern 3266 ==============\n",
      "============== Pattern 3267 ==============\n",
      "============== Pattern 3268 ==============\n",
      "============== Pattern 3269 ==============\n",
      "============== Pattern 3270 ==============\n",
      "============== Pattern 3271 ==============\n",
      "============== Pattern 3272 ==============\n",
      "============== Pattern 3273 ==============\n",
      "============== Pattern 3274 ==============\n",
      "============== Pattern 3275 ==============\n",
      "============== Pattern 3276 ==============\n",
      "============== Pattern 3277 ==============\n",
      "============== Pattern 3278 ==============\n",
      "============== Pattern 3279 ==============\n",
      "============== Pattern 3280 ==============\n",
      "============== Pattern 3281 ==============\n",
      "============== Pattern 3282 ==============\n",
      "============== Pattern 3283 ==============\n",
      "============== Pattern 3284 ==============\n",
      "============== Pattern 3285 ==============\n",
      "============== Pattern 3286 ==============\n",
      "============== Pattern 3287 ==============\n",
      "============== Pattern 3288 ==============\n",
      "============== Pattern 3289 ==============\n",
      "============== Pattern 3290 ==============\n",
      "============== Pattern 3291 ==============\n",
      "============== Pattern 3292 ==============\n",
      "============== Pattern 3293 ==============\n",
      "============== Pattern 3294 ==============\n",
      "============== Pattern 3295 ==============\n",
      "============== Pattern 3296 ==============\n",
      "============== Pattern 3297 ==============\n",
      "============== Pattern 3298 ==============\n",
      "============== Pattern 3299 ==============\n",
      "============== Pattern 3300 ==============\n",
      "============== Pattern 3301 ==============\n",
      "============== Pattern 3302 ==============\n",
      "============== Pattern 3303 ==============\n",
      "============== Pattern 3304 ==============\n",
      "============== Pattern 3305 ==============\n",
      "============== Pattern 3306 ==============\n",
      "============== Pattern 3307 ==============\n",
      "============== Pattern 3308 ==============\n",
      "============== Pattern 3309 ==============\n",
      "============== Pattern 3310 ==============\n",
      "============== Pattern 3311 ==============\n",
      "============== Pattern 3312 ==============\n",
      "============== Pattern 3313 ==============\n",
      "============== Pattern 3314 ==============\n",
      "============== Pattern 3315 ==============\n",
      "============== Pattern 3316 ==============\n",
      "============== Pattern 3317 ==============\n",
      "============== Pattern 3318 ==============\n",
      "============== Pattern 3319 ==============\n",
      "============== Pattern 3320 ==============\n",
      "============== Pattern 3321 ==============\n",
      "============== Pattern 3322 ==============\n",
      "============== Pattern 3323 ==============\n",
      "============== Pattern 3324 ==============\n",
      "============== Pattern 3325 ==============\n",
      "============== Pattern 3326 ==============\n",
      "============== Pattern 3327 ==============\n",
      "============== Pattern 3328 ==============\n",
      "============== Pattern 3329 ==============\n",
      "============== Pattern 3330 ==============\n",
      "============== Pattern 3331 ==============\n",
      "============== Pattern 3332 ==============\n",
      "============== Pattern 3333 ==============\n",
      "============== Pattern 3334 ==============\n",
      "============== Pattern 3335 ==============\n",
      "============== Pattern 3336 ==============\n",
      "============== Pattern 3337 ==============\n",
      "============== Pattern 3338 ==============\n",
      "============== Pattern 3339 ==============\n",
      "============== Pattern 3340 ==============\n",
      "============== Pattern 3341 ==============\n",
      "============== Pattern 3342 ==============\n",
      "============== Pattern 3343 ==============\n",
      "============== Pattern 3344 ==============\n",
      "============== Pattern 3345 ==============\n",
      "============== Pattern 3346 ==============\n",
      "============== Pattern 3347 ==============\n",
      "============== Pattern 3348 ==============\n",
      "============== Pattern 3349 ==============\n",
      "============== Pattern 3350 ==============\n",
      "============== Pattern 3351 ==============\n",
      "============== Pattern 3352 ==============\n",
      "============== Pattern 3353 ==============\n",
      "============== Pattern 3354 ==============\n",
      "============== Pattern 3355 ==============\n",
      "============== Pattern 3356 ==============\n",
      "============== Pattern 3357 ==============\n",
      "============== Pattern 3358 ==============\n",
      "============== Pattern 3359 ==============\n",
      "============== Pattern 3360 ==============\n",
      "============== Pattern 3361 ==============\n",
      "============== Pattern 3362 ==============\n",
      "============== Pattern 3363 ==============\n",
      "============== Pattern 3364 ==============\n",
      "============== Pattern 3365 ==============\n",
      "============== Pattern 3366 ==============\n",
      "============== Pattern 3367 ==============\n",
      "============== Pattern 3368 ==============\n",
      "============== Pattern 3369 ==============\n",
      "============== Pattern 3370 ==============\n",
      "============== Pattern 3371 ==============\n",
      "============== Pattern 3372 ==============\n",
      "============== Pattern 3373 ==============\n",
      "============== Pattern 3374 ==============\n",
      "============== Pattern 3375 ==============\n",
      "============== Pattern 3376 ==============\n",
      "============== Pattern 3377 ==============\n",
      "============== Pattern 3378 ==============\n",
      "============== Pattern 3379 ==============\n",
      "============== Pattern 3380 ==============\n",
      "============== Pattern 3381 ==============\n",
      "============== Pattern 3382 ==============\n",
      "============== Pattern 3383 ==============\n",
      "============== Pattern 3384 ==============\n",
      "============== Pattern 3385 ==============\n",
      "============== Pattern 3386 ==============\n",
      "============== Pattern 3387 ==============\n",
      "============== Pattern 3388 ==============\n",
      "============== Pattern 3389 ==============\n",
      "============== Pattern 3390 ==============\n",
      "============== Pattern 3391 ==============\n",
      "============== Pattern 3392 ==============\n",
      "============== Pattern 3393 ==============\n",
      "============== Pattern 3394 ==============\n",
      "============== Pattern 3395 ==============\n",
      "============== Pattern 3396 ==============\n",
      "============== Pattern 3397 ==============\n",
      "============== Pattern 3398 ==============\n",
      "============== Pattern 3399 ==============\n",
      "============== Pattern 3400 ==============\n",
      "============== Pattern 3401 ==============\n",
      "============== Pattern 3402 ==============\n",
      "============== Pattern 3403 ==============\n",
      "============== Pattern 3404 ==============\n",
      "============== Pattern 3405 ==============\n",
      "============== Pattern 3406 ==============\n",
      "============== Pattern 3407 ==============\n",
      "============== Pattern 3408 ==============\n",
      "============== Pattern 3409 ==============\n",
      "============== Pattern 3410 ==============\n",
      "============== Pattern 3411 ==============\n",
      "============== Pattern 3412 ==============\n",
      "============== Pattern 3413 ==============\n",
      "============== Pattern 3414 ==============\n",
      "============== Pattern 3415 ==============\n",
      "============== Pattern 3416 ==============\n",
      "============== Pattern 3417 ==============\n",
      "============== Pattern 3418 ==============\n",
      "============== Pattern 3419 ==============\n",
      "============== Pattern 3420 ==============\n",
      "============== Pattern 3421 ==============\n",
      "============== Pattern 3422 ==============\n",
      "============== Pattern 3423 ==============\n",
      "============== Pattern 3424 ==============\n",
      "============== Pattern 3425 ==============\n",
      "============== Pattern 3426 ==============\n",
      "============== Pattern 3427 ==============\n",
      "============== Pattern 3428 ==============\n",
      "============== Pattern 3429 ==============\n",
      "============== Pattern 3430 ==============\n",
      "============== Pattern 3431 ==============\n",
      "============== Pattern 3432 ==============\n",
      "============== Pattern 3433 ==============\n",
      "============== Pattern 3434 ==============\n",
      "============== Pattern 3435 ==============\n",
      "============== Pattern 3436 ==============\n",
      "============== Pattern 3437 ==============\n",
      "============== Pattern 3438 ==============\n",
      "============== Pattern 3439 ==============\n",
      "============== Pattern 3440 ==============\n",
      "============== Pattern 3441 ==============\n",
      "============== Pattern 3442 ==============\n",
      "============== Pattern 3443 ==============\n",
      "============== Pattern 3444 ==============\n",
      "============== Pattern 3445 ==============\n",
      "============== Pattern 3446 ==============\n",
      "============== Pattern 3447 ==============\n",
      "============== Pattern 3448 ==============\n",
      "============== Pattern 3449 ==============\n",
      "============== Pattern 3450 ==============\n",
      "============== Pattern 3451 ==============\n",
      "============== Pattern 3452 ==============\n",
      "============== Pattern 3453 ==============\n",
      "============== Pattern 3454 ==============\n",
      "============== Pattern 3455 ==============\n",
      "============== Pattern 3456 ==============\n",
      "============== Pattern 3457 ==============\n",
      "============== Pattern 3458 ==============\n",
      "============== Pattern 3459 ==============\n",
      "============== Pattern 3460 ==============\n",
      "============== Pattern 3461 ==============\n",
      "============== Pattern 3462 ==============\n",
      "============== Pattern 3463 ==============\n",
      "============== Pattern 3464 ==============\n",
      "============== Pattern 3465 ==============\n",
      "============== Pattern 3466 ==============\n",
      "============== Pattern 3467 ==============\n",
      "============== Pattern 3468 ==============\n",
      "============== Pattern 3469 ==============\n",
      "============== Pattern 3470 ==============\n",
      "============== Pattern 3471 ==============\n",
      "============== Pattern 3472 ==============\n",
      "============== Pattern 3473 ==============\n",
      "============== Pattern 3474 ==============\n",
      "============== Pattern 3475 ==============\n",
      "============== Pattern 3476 ==============\n",
      "============== Pattern 3477 ==============\n",
      "============== Pattern 3478 ==============\n",
      "============== Pattern 3479 ==============\n",
      "============== Pattern 3480 ==============\n",
      "============== Pattern 3481 ==============\n",
      "============== Pattern 3482 ==============\n",
      "============== Pattern 3483 ==============\n",
      "============== Pattern 3484 ==============\n",
      "============== Pattern 3485 ==============\n",
      "============== Pattern 3486 ==============\n",
      "============== Pattern 3487 ==============\n",
      "============== Pattern 3488 ==============\n",
      "============== Pattern 3489 ==============\n",
      "============== Pattern 3490 ==============\n",
      "============== Pattern 3491 ==============\n",
      "============== Pattern 3492 ==============\n",
      "============== Pattern 3493 ==============\n",
      "============== Pattern 3494 ==============\n",
      "============== Pattern 3495 ==============\n",
      "============== Pattern 3496 ==============\n",
      "============== Pattern 3497 ==============\n",
      "============== Pattern 3498 ==============\n",
      "============== Pattern 3499 ==============\n",
      "============== Pattern 3500 ==============\n",
      "============== Pattern 3501 ==============\n",
      "============== Pattern 3502 ==============\n",
      "============== Pattern 3503 ==============\n",
      "============== Pattern 3504 ==============\n",
      "============== Pattern 3505 ==============\n",
      "============== Pattern 3506 ==============\n",
      "============== Pattern 3507 ==============\n",
      "============== Pattern 3508 ==============\n",
      "============== Pattern 3509 ==============\n",
      "============== Pattern 3510 ==============\n",
      "============== Pattern 3511 ==============\n",
      "============== Pattern 3512 ==============\n",
      "============== Pattern 3513 ==============\n",
      "============== Pattern 3514 ==============\n",
      "============== Pattern 3515 ==============\n",
      "============== Pattern 3516 ==============\n",
      "============== Pattern 3517 ==============\n",
      "============== Pattern 3518 ==============\n",
      "============== Pattern 3519 ==============\n",
      "============== Pattern 3520 ==============\n",
      "============== Pattern 3521 ==============\n",
      "============== Pattern 3522 ==============\n",
      "============== Pattern 3523 ==============\n",
      "============== Pattern 3524 ==============\n",
      "============== Pattern 3525 ==============\n",
      "============== Pattern 3526 ==============\n",
      "============== Pattern 3527 ==============\n",
      "============== Pattern 3528 ==============\n",
      "============== Pattern 3529 ==============\n",
      "============== Pattern 3530 ==============\n",
      "============== Pattern 3531 ==============\n",
      "============== Pattern 3532 ==============\n",
      "============== Pattern 3533 ==============\n",
      "============== Pattern 3534 ==============\n",
      "============== Pattern 3535 ==============\n",
      "============== Pattern 3536 ==============\n",
      "============== Pattern 3537 ==============\n",
      "============== Pattern 3538 ==============\n",
      "============== Pattern 3539 ==============\n",
      "============== Pattern 3540 ==============\n",
      "============== Pattern 3541 ==============\n",
      "============== Pattern 3542 ==============\n",
      "============== Pattern 3543 ==============\n",
      "============== Pattern 3544 ==============\n",
      "============== Pattern 3545 ==============\n",
      "============== Pattern 3546 ==============\n",
      "============== Pattern 3547 ==============\n",
      "============== Pattern 3548 ==============\n",
      "============== Pattern 3549 ==============\n",
      "============== Pattern 3550 ==============\n",
      "============== Pattern 3551 ==============\n",
      "============== Pattern 3552 ==============\n",
      "============== Pattern 3553 ==============\n",
      "============== Pattern 3554 ==============\n",
      "============== Pattern 3555 ==============\n",
      "============== Pattern 3556 ==============\n",
      "============== Pattern 3557 ==============\n",
      "============== Pattern 3558 ==============\n",
      "============== Pattern 3559 ==============\n",
      "============== Pattern 3560 ==============\n",
      "============== Pattern 3561 ==============\n",
      "============== Pattern 3562 ==============\n",
      "============== Pattern 3563 ==============\n",
      "============== Pattern 3564 ==============\n",
      "============== Pattern 3565 ==============\n",
      "============== Pattern 3566 ==============\n",
      "============== Pattern 3567 ==============\n",
      "============== Pattern 3568 ==============\n",
      "============== Pattern 3569 ==============\n",
      "============== Pattern 3570 ==============\n",
      "============== Pattern 3571 ==============\n",
      "============== Pattern 3572 ==============\n",
      "============== Pattern 3573 ==============\n",
      "============== Pattern 3574 ==============\n",
      "============== Pattern 3575 ==============\n",
      "============== Pattern 3576 ==============\n",
      "============== Pattern 3577 ==============\n",
      "============== Pattern 3578 ==============\n",
      "============== Pattern 3579 ==============\n",
      "============== Pattern 3580 ==============\n",
      "============== Pattern 3581 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 3582 ==============\n",
      "============== Pattern 3583 ==============\n",
      "============== Pattern 3584 ==============\n",
      "============== Pattern 3585 ==============\n",
      "============== Pattern 3586 ==============\n",
      "============== Pattern 3587 ==============\n",
      "============== Pattern 3588 ==============\n",
      "============== Pattern 3589 ==============\n",
      "============== Pattern 3590 ==============\n",
      "============== Pattern 3591 ==============\n",
      "============== Pattern 3592 ==============\n",
      "============== Pattern 3593 ==============\n",
      "============== Pattern 3594 ==============\n",
      "============== Pattern 3595 ==============\n",
      "============== Pattern 3596 ==============\n",
      "============== Pattern 3597 ==============\n",
      "============== Pattern 3598 ==============\n",
      "============== Pattern 3599 ==============\n",
      "============== Pattern 3600 ==============\n",
      "============== Pattern 3601 ==============\n",
      "============== Pattern 3602 ==============\n",
      "============== Pattern 3603 ==============\n",
      "============== Pattern 3604 ==============\n",
      "============== Pattern 3605 ==============\n",
      "============== Pattern 3606 ==============\n",
      "============== Pattern 3607 ==============\n",
      "============== Pattern 3608 ==============\n",
      "============== Pattern 3609 ==============\n",
      "============== Pattern 3610 ==============\n",
      "============== Pattern 3611 ==============\n",
      "============== Pattern 3612 ==============\n",
      "============== Pattern 3613 ==============\n",
      "============== Pattern 3614 ==============\n",
      "============== Pattern 3615 ==============\n",
      "============== Pattern 3616 ==============\n",
      "============== Pattern 3617 ==============\n",
      "============== Pattern 3618 ==============\n",
      "============== Pattern 3619 ==============\n",
      "============== Pattern 3620 ==============\n",
      "============== Pattern 3621 ==============\n",
      "============== Pattern 3622 ==============\n",
      "============== Pattern 3623 ==============\n",
      "============== Pattern 3624 ==============\n",
      "============== Pattern 3625 ==============\n",
      "============== Pattern 3626 ==============\n",
      "============== Pattern 3627 ==============\n",
      "============== Pattern 3628 ==============\n",
      "============== Pattern 3629 ==============\n",
      "============== Pattern 3630 ==============\n",
      "============== Pattern 3631 ==============\n",
      "============== Pattern 3632 ==============\n",
      "============== Pattern 3633 ==============\n",
      "============== Pattern 3634 ==============\n",
      "============== Pattern 3635 ==============\n",
      "============== Pattern 3636 ==============\n",
      "============== Pattern 3637 ==============\n",
      "============== Pattern 3638 ==============\n",
      "============== Pattern 3639 ==============\n",
      "============== Pattern 3640 ==============\n",
      "============== Pattern 3641 ==============\n",
      "============== Pattern 3642 ==============\n",
      "============== Pattern 3643 ==============\n",
      "============== Pattern 3644 ==============\n",
      "============== Pattern 3645 ==============\n",
      "============== Pattern 3646 ==============\n",
      "============== Pattern 3647 ==============\n",
      "============== Pattern 3648 ==============\n",
      "============== Pattern 3649 ==============\n",
      "============== Pattern 3650 ==============\n",
      "============== Pattern 3651 ==============\n",
      "============== Pattern 3652 ==============\n",
      "============== Pattern 3653 ==============\n",
      "============== Pattern 3654 ==============\n",
      "============== Pattern 3655 ==============\n",
      "============== Pattern 3656 ==============\n",
      "============== Pattern 3657 ==============\n",
      "============== Pattern 3658 ==============\n",
      "============== Pattern 3659 ==============\n",
      "============== Pattern 3660 ==============\n",
      "============== Pattern 3661 ==============\n",
      "============== Pattern 3662 ==============\n",
      "============== Pattern 3663 ==============\n",
      "============== Pattern 3664 ==============\n",
      "============== Pattern 3665 ==============\n",
      "============== Pattern 3666 ==============\n",
      "============== Pattern 3667 ==============\n",
      "============== Pattern 3668 ==============\n",
      "============== Pattern 3669 ==============\n",
      "============== Pattern 3670 ==============\n",
      "============== Pattern 3671 ==============\n",
      "============== Pattern 3672 ==============\n",
      "============== Pattern 3673 ==============\n",
      "============== Pattern 3674 ==============\n",
      "============== Pattern 3675 ==============\n",
      "============== Pattern 3676 ==============\n",
      "============== Pattern 3677 ==============\n",
      "============== Pattern 3678 ==============\n",
      "============== Pattern 3679 ==============\n",
      "============== Pattern 3680 ==============\n",
      "============== Pattern 3681 ==============\n",
      "============== Pattern 3682 ==============\n",
      "============== Pattern 3683 ==============\n",
      "============== Pattern 3684 ==============\n",
      "============== Pattern 3685 ==============\n",
      "============== Pattern 3686 ==============\n",
      "============== Pattern 3687 ==============\n",
      "============== Pattern 3688 ==============\n",
      "============== Pattern 3689 ==============\n",
      "============== Pattern 3690 ==============\n",
      "============== Pattern 3691 ==============\n",
      "============== Pattern 3692 ==============\n",
      "============== Pattern 3693 ==============\n",
      "============== Pattern 3694 ==============\n",
      "============== Pattern 3695 ==============\n",
      "============== Pattern 3696 ==============\n",
      "============== Pattern 3697 ==============\n",
      "============== Pattern 3698 ==============\n",
      "============== Pattern 3699 ==============\n",
      "============== Pattern 3700 ==============\n",
      "============== Pattern 3701 ==============\n",
      "============== Pattern 3702 ==============\n",
      "============== Pattern 3703 ==============\n",
      "============== Pattern 3704 ==============\n",
      "============== Pattern 3705 ==============\n",
      "============== Pattern 3706 ==============\n",
      "============== Pattern 3707 ==============\n",
      "============== Pattern 3708 ==============\n",
      "============== Pattern 3709 ==============\n",
      "============== Pattern 3710 ==============\n",
      "============== Pattern 3711 ==============\n",
      "============== Pattern 3712 ==============\n",
      "============== Pattern 3713 ==============\n",
      "============== Pattern 3714 ==============\n",
      "============== Pattern 3715 ==============\n",
      "============== Pattern 3716 ==============\n",
      "============== Pattern 3717 ==============\n",
      "============== Pattern 3718 ==============\n",
      "============== Pattern 3719 ==============\n",
      "============== Pattern 3720 ==============\n",
      "============== Pattern 3721 ==============\n",
      "============== Pattern 3722 ==============\n",
      "============== Pattern 3723 ==============\n",
      "============== Pattern 3724 ==============\n",
      "============== Pattern 3725 ==============\n",
      "============== Pattern 3726 ==============\n",
      "============== Pattern 3727 ==============\n",
      "============== Pattern 3728 ==============\n",
      "============== Pattern 3729 ==============\n",
      "============== Pattern 3730 ==============\n",
      "============== Pattern 3731 ==============\n",
      "============== Pattern 3732 ==============\n",
      "============== Pattern 3733 ==============\n",
      "============== Pattern 3734 ==============\n",
      "============== Pattern 3735 ==============\n",
      "============== Pattern 3736 ==============\n",
      "============== Pattern 3737 ==============\n",
      "============== Pattern 3738 ==============\n",
      "============== Pattern 3739 ==============\n",
      "============== Pattern 3740 ==============\n",
      "============== Pattern 3741 ==============\n",
      "============== Pattern 3742 ==============\n",
      "============== Pattern 3743 ==============\n",
      "============== Pattern 3744 ==============\n",
      "============== Pattern 3745 ==============\n",
      "============== Pattern 3746 ==============\n",
      "============== Pattern 3747 ==============\n",
      "============== Pattern 3748 ==============\n",
      "============== Pattern 3749 ==============\n",
      "============== Pattern 3750 ==============\n",
      "============== Pattern 3751 ==============\n",
      "============== Pattern 3752 ==============\n",
      "============== Pattern 3753 ==============\n",
      "============== Pattern 3754 ==============\n",
      "============== Pattern 3755 ==============\n",
      "============== Pattern 3756 ==============\n",
      "============== Pattern 3757 ==============\n",
      "============== Pattern 3758 ==============\n",
      "============== Pattern 3759 ==============\n",
      "============== Pattern 3760 ==============\n",
      "============== Pattern 3761 ==============\n",
      "============== Pattern 3762 ==============\n",
      "============== Pattern 3763 ==============\n",
      "============== Pattern 3764 ==============\n",
      "============== Pattern 3765 ==============\n",
      "============== Pattern 3766 ==============\n",
      "============== Pattern 3767 ==============\n",
      "============== Pattern 3768 ==============\n",
      "============== Pattern 3769 ==============\n",
      "============== Pattern 3770 ==============\n",
      "============== Pattern 3771 ==============\n",
      "============== Pattern 3772 ==============\n",
      "============== Pattern 3773 ==============\n",
      "============== Pattern 3774 ==============\n",
      "============== Pattern 3775 ==============\n",
      "============== Pattern 3776 ==============\n",
      "============== Pattern 3777 ==============\n",
      "============== Pattern 3778 ==============\n",
      "============== Pattern 3779 ==============\n",
      "============== Pattern 3780 ==============\n",
      "============== Pattern 3781 ==============\n",
      "============== Pattern 3782 ==============\n",
      "============== Pattern 3783 ==============\n",
      "============== Pattern 3784 ==============\n",
      "============== Pattern 3785 ==============\n",
      "============== Pattern 3786 ==============\n",
      "============== Pattern 3787 ==============\n",
      "============== Pattern 3788 ==============\n",
      "============== Pattern 3789 ==============\n",
      "============== Pattern 3790 ==============\n",
      "============== Pattern 3791 ==============\n",
      "============== Pattern 3792 ==============\n",
      "============== Pattern 3793 ==============\n",
      "============== Pattern 3794 ==============\n",
      "============== Pattern 3795 ==============\n",
      "============== Pattern 3796 ==============\n",
      "============== Pattern 3797 ==============\n",
      "============== Pattern 3798 ==============\n",
      "============== Pattern 3799 ==============\n",
      "============== Pattern 3800 ==============\n",
      "============== Pattern 3801 ==============\n",
      "============== Pattern 3802 ==============\n",
      "============== Pattern 3803 ==============\n",
      "============== Pattern 3804 ==============\n",
      "============== Pattern 3805 ==============\n",
      "============== Pattern 3806 ==============\n",
      "============== Pattern 3807 ==============\n",
      "============== Pattern 3808 ==============\n",
      "============== Pattern 3809 ==============\n",
      "============== Pattern 3810 ==============\n",
      "============== Pattern 3811 ==============\n",
      "============== Pattern 3812 ==============\n",
      "============== Pattern 3813 ==============\n",
      "============== Pattern 3814 ==============\n",
      "============== Pattern 3815 ==============\n",
      "============== Pattern 3816 ==============\n",
      "============== Pattern 3817 ==============\n",
      "============== Pattern 3818 ==============\n",
      "============== Pattern 3819 ==============\n",
      "============== Pattern 3820 ==============\n",
      "============== Pattern 3821 ==============\n",
      "============== Pattern 3822 ==============\n",
      "============== Pattern 3823 ==============\n",
      "============== Pattern 3824 ==============\n",
      "============== Pattern 3825 ==============\n",
      "============== Pattern 3826 ==============\n",
      "============== Pattern 3827 ==============\n",
      "============== Pattern 3828 ==============\n",
      "============== Pattern 3829 ==============\n",
      "============== Pattern 3830 ==============\n",
      "============== Pattern 3831 ==============\n",
      "============== Pattern 3832 ==============\n",
      "============== Pattern 3833 ==============\n",
      "============== Pattern 3834 ==============\n",
      "============== Pattern 3835 ==============\n",
      "============== Pattern 3836 ==============\n",
      "============== Pattern 3837 ==============\n",
      "============== Pattern 3838 ==============\n",
      "============== Pattern 3839 ==============\n",
      "============== Pattern 3840 ==============\n",
      "============== Pattern 3841 ==============\n",
      "============== Pattern 3842 ==============\n",
      "============== Pattern 3843 ==============\n",
      "============== Pattern 3844 ==============\n",
      "============== Pattern 3845 ==============\n",
      "============== Pattern 3846 ==============\n",
      "============== Pattern 3847 ==============\n",
      "============== Pattern 3848 ==============\n",
      "============== Pattern 3849 ==============\n",
      "============== Pattern 3850 ==============\n",
      "============== Pattern 3851 ==============\n",
      "============== Pattern 3852 ==============\n",
      "============== Pattern 3853 ==============\n",
      "============== Pattern 3854 ==============\n",
      "============== Pattern 3855 ==============\n",
      "============== Pattern 3856 ==============\n",
      "============== Pattern 3857 ==============\n",
      "============== Pattern 3858 ==============\n",
      "============== Pattern 3859 ==============\n",
      "============== Pattern 3860 ==============\n",
      "============== Pattern 3861 ==============\n",
      "============== Pattern 3862 ==============\n",
      "============== Pattern 3863 ==============\n",
      "============== Pattern 3864 ==============\n",
      "============== Pattern 3865 ==============\n",
      "============== Pattern 3866 ==============\n",
      "============== Pattern 3867 ==============\n",
      "============== Pattern 3868 ==============\n",
      "============== Pattern 3869 ==============\n",
      "============== Pattern 3870 ==============\n",
      "============== Pattern 3871 ==============\n",
      "============== Pattern 3872 ==============\n",
      "============== Pattern 3873 ==============\n",
      "============== Pattern 3874 ==============\n",
      "============== Pattern 3875 ==============\n",
      "============== Pattern 3876 ==============\n",
      "============== Pattern 3877 ==============\n",
      "============== Pattern 3878 ==============\n",
      "============== Pattern 3879 ==============\n",
      "============== Pattern 3880 ==============\n",
      "============== Pattern 3881 ==============\n",
      "============== Pattern 3882 ==============\n",
      "============== Pattern 3883 ==============\n",
      "============== Pattern 3884 ==============\n",
      "============== Pattern 3885 ==============\n",
      "============== Pattern 3886 ==============\n",
      "============== Pattern 3887 ==============\n",
      "============== Pattern 3888 ==============\n",
      "============== Pattern 3889 ==============\n",
      "============== Pattern 3890 ==============\n",
      "============== Pattern 3891 ==============\n",
      "============== Pattern 3892 ==============\n",
      "============== Pattern 3893 ==============\n",
      "============== Pattern 3894 ==============\n",
      "============== Pattern 3895 ==============\n",
      "============== Pattern 3896 ==============\n",
      "============== Pattern 3897 ==============\n",
      "============== Pattern 3898 ==============\n",
      "============== Pattern 3899 ==============\n",
      "============== Pattern 3900 ==============\n",
      "============== Pattern 3901 ==============\n",
      "============== Pattern 3902 ==============\n",
      "============== Pattern 3903 ==============\n",
      "============== Pattern 3904 ==============\n",
      "============== Pattern 3905 ==============\n",
      "============== Pattern 3906 ==============\n",
      "============== Pattern 3907 ==============\n",
      "============== Pattern 3908 ==============\n",
      "============== Pattern 3909 ==============\n",
      "============== Pattern 3910 ==============\n",
      "============== Pattern 3911 ==============\n",
      "============== Pattern 3912 ==============\n",
      "============== Pattern 3913 ==============\n",
      "============== Pattern 3914 ==============\n",
      "============== Pattern 3915 ==============\n",
      "============== Pattern 3916 ==============\n",
      "============== Pattern 3917 ==============\n",
      "============== Pattern 3918 ==============\n",
      "============== Pattern 3919 ==============\n",
      "============== Pattern 3920 ==============\n",
      "============== Pattern 3921 ==============\n",
      "============== Pattern 3922 ==============\n",
      "============== Pattern 3923 ==============\n",
      "============== Pattern 3924 ==============\n",
      "============== Pattern 3925 ==============\n",
      "============== Pattern 3926 ==============\n",
      "============== Pattern 3927 ==============\n",
      "============== Pattern 3928 ==============\n",
      "============== Pattern 3929 ==============\n",
      "============== Pattern 3930 ==============\n",
      "============== Pattern 3931 ==============\n",
      "============== Pattern 3932 ==============\n",
      "============== Pattern 3933 ==============\n",
      "============== Pattern 3934 ==============\n",
      "============== Pattern 3935 ==============\n",
      "============== Pattern 3936 ==============\n",
      "============== Pattern 3937 ==============\n",
      "============== Pattern 3938 ==============\n",
      "============== Pattern 3939 ==============\n",
      "============== Pattern 3940 ==============\n",
      "============== Pattern 3941 ==============\n",
      "============== Pattern 3942 ==============\n",
      "============== Pattern 3943 ==============\n",
      "============== Pattern 3944 ==============\n",
      "============== Pattern 3945 ==============\n",
      "============== Pattern 3946 ==============\n",
      "============== Pattern 3947 ==============\n",
      "============== Pattern 3948 ==============\n",
      "============== Pattern 3949 ==============\n",
      "============== Pattern 3950 ==============\n",
      "============== Pattern 3951 ==============\n",
      "============== Pattern 3952 ==============\n",
      "============== Pattern 3953 ==============\n",
      "============== Pattern 3954 ==============\n",
      "============== Pattern 3955 ==============\n",
      "============== Pattern 3956 ==============\n",
      "============== Pattern 3957 ==============\n",
      "============== Pattern 3958 ==============\n",
      "============== Pattern 3959 ==============\n",
      "============== Pattern 3960 ==============\n",
      "============== Pattern 3961 ==============\n",
      "============== Pattern 3962 ==============\n",
      "============== Pattern 3963 ==============\n",
      "============== Pattern 3964 ==============\n",
      "============== Pattern 3965 ==============\n",
      "============== Pattern 3966 ==============\n",
      "============== Pattern 3967 ==============\n",
      "============== Pattern 3968 ==============\n",
      "============== Pattern 3969 ==============\n",
      "============== Pattern 3970 ==============\n",
      "============== Pattern 3971 ==============\n",
      "============== Pattern 3972 ==============\n",
      "============== Pattern 3973 ==============\n",
      "============== Pattern 3974 ==============\n",
      "============== Pattern 3975 ==============\n",
      "============== Pattern 3976 ==============\n",
      "============== Pattern 3977 ==============\n",
      "============== Pattern 3978 ==============\n",
      "============== Pattern 3979 ==============\n",
      "============== Pattern 3980 ==============\n",
      "============== Pattern 3981 ==============\n",
      "============== Pattern 3982 ==============\n",
      "============== Pattern 3983 ==============\n",
      "============== Pattern 3984 ==============\n",
      "============== Pattern 3985 ==============\n",
      "============== Pattern 3986 ==============\n",
      "============== Pattern 3987 ==============\n",
      "============== Pattern 3988 ==============\n",
      "============== Pattern 3989 ==============\n",
      "============== Pattern 3990 ==============\n",
      "============== Pattern 3991 ==============\n",
      "============== Pattern 3992 ==============\n",
      "============== Pattern 3993 ==============\n",
      "============== Pattern 3994 ==============\n",
      "============== Pattern 3995 ==============\n",
      "============== Pattern 3996 ==============\n",
      "============== Pattern 3997 ==============\n",
      "============== Pattern 3998 ==============\n",
      "============== Pattern 3999 ==============\n",
      "============== Pattern 4000 ==============\n",
      "============== Pattern 4001 ==============\n",
      "============== Pattern 4002 ==============\n",
      "============== Pattern 4003 ==============\n",
      "============== Pattern 4004 ==============\n",
      "============== Pattern 4005 ==============\n",
      "============== Pattern 4006 ==============\n",
      "============== Pattern 4007 ==============\n",
      "============== Pattern 4008 ==============\n",
      "============== Pattern 4009 ==============\n",
      "============== Pattern 4010 ==============\n",
      "============== Pattern 4011 ==============\n",
      "============== Pattern 4012 ==============\n",
      "============== Pattern 4013 ==============\n",
      "============== Pattern 4014 ==============\n",
      "============== Pattern 4015 ==============\n",
      "============== Pattern 4016 ==============\n",
      "============== Pattern 4017 ==============\n",
      "============== Pattern 4018 ==============\n",
      "============== Pattern 4019 ==============\n",
      "============== Pattern 4020 ==============\n",
      "============== Pattern 4021 ==============\n",
      "============== Pattern 4022 ==============\n",
      "============== Pattern 4023 ==============\n",
      "============== Pattern 4024 ==============\n",
      "============== Pattern 4025 ==============\n",
      "============== Pattern 4026 ==============\n",
      "============== Pattern 4027 ==============\n",
      "============== Pattern 4028 ==============\n",
      "============== Pattern 4029 ==============\n",
      "============== Pattern 4030 ==============\n",
      "============== Pattern 4031 ==============\n",
      "============== Pattern 4032 ==============\n",
      "============== Pattern 4033 ==============\n",
      "============== Pattern 4034 ==============\n",
      "============== Pattern 4035 ==============\n",
      "============== Pattern 4036 ==============\n",
      "============== Pattern 4037 ==============\n",
      "============== Pattern 4038 ==============\n",
      "============== Pattern 4039 ==============\n",
      "============== Pattern 4040 ==============\n",
      "============== Pattern 4041 ==============\n",
      "============== Pattern 4042 ==============\n",
      "============== Pattern 4043 ==============\n",
      "============== Pattern 4044 ==============\n",
      "============== Pattern 4045 ==============\n",
      "============== Pattern 4046 ==============\n",
      "============== Pattern 4047 ==============\n",
      "============== Pattern 4048 ==============\n",
      "============== Pattern 4049 ==============\n",
      "============== Pattern 4050 ==============\n",
      "============== Pattern 4051 ==============\n",
      "============== Pattern 4052 ==============\n",
      "============== Pattern 4053 ==============\n",
      "============== Pattern 4054 ==============\n",
      "============== Pattern 4055 ==============\n",
      "============== Pattern 4056 ==============\n",
      "============== Pattern 4057 ==============\n",
      "============== Pattern 4058 ==============\n",
      "============== Pattern 4059 ==============\n",
      "============== Pattern 4060 ==============\n",
      "============== Pattern 4061 ==============\n",
      "============== Pattern 4062 ==============\n",
      "============== Pattern 4063 ==============\n",
      "============== Pattern 4064 ==============\n",
      "============== Pattern 4065 ==============\n",
      "============== Pattern 4066 ==============\n",
      "============== Pattern 4067 ==============\n",
      "============== Pattern 4068 ==============\n",
      "============== Pattern 4069 ==============\n",
      "============== Pattern 4070 ==============\n",
      "============== Pattern 4071 ==============\n",
      "============== Pattern 4072 ==============\n",
      "============== Pattern 4073 ==============\n",
      "============== Pattern 4074 ==============\n",
      "============== Pattern 4075 ==============\n",
      "============== Pattern 4076 ==============\n",
      "============== Pattern 4077 ==============\n",
      "============== Pattern 4078 ==============\n",
      "============== Pattern 4079 ==============\n",
      "============== Pattern 4080 ==============\n",
      "============== Pattern 4081 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 4082 ==============\n",
      "============== Pattern 4083 ==============\n",
      "============== Pattern 4084 ==============\n",
      "============== Pattern 4085 ==============\n",
      "============== Pattern 4086 ==============\n",
      "============== Pattern 4087 ==============\n",
      "============== Pattern 4088 ==============\n",
      "============== Pattern 4089 ==============\n",
      "============== Pattern 4090 ==============\n",
      "============== Pattern 4091 ==============\n",
      "============== Pattern 4092 ==============\n",
      "============== Pattern 4093 ==============\n",
      "============== Pattern 4094 ==============\n",
      "============== Pattern 4095 ==============\n",
      "============== Pattern 4096 ==============\n",
      "Average comprehensibility: 54.7666015625\n",
      "std comprehensibility: 3.3709781843513253\n",
      "var comprehensibility: 11.363493919372559\n",
      "minimum comprehensibility: 48\n",
      "maximum comprehensibility: 68\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
