{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 32\n",
    "tree_depth = 8\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.180998802185059 | KNN Loss: 6.227673530578613 | BCE Loss: 1.9533252716064453\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.19716739654541 | KNN Loss: 6.2274298667907715 | BCE Loss: 1.9697372913360596\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.151121139526367 | KNN Loss: 6.227524757385254 | BCE Loss: 1.9235968589782715\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.188850402832031 | KNN Loss: 6.2274041175842285 | BCE Loss: 1.9614464044570923\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.152887344360352 | KNN Loss: 6.227034568786621 | BCE Loss: 1.9258532524108887\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.164999008178711 | KNN Loss: 6.226687431335449 | BCE Loss: 1.9383113384246826\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.155362129211426 | KNN Loss: 6.226534366607666 | BCE Loss: 1.9288280010223389\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.13614273071289 | KNN Loss: 6.226588726043701 | BCE Loss: 1.9095542430877686\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.08188247680664 | KNN Loss: 6.225865364074707 | BCE Loss: 1.856016993522644\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.10098934173584 | KNN Loss: 6.225793838500977 | BCE Loss: 1.8751952648162842\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.110013961791992 | KNN Loss: 6.225716590881348 | BCE Loss: 1.8842973709106445\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.081755638122559 | KNN Loss: 6.225494861602783 | BCE Loss: 1.856261134147644\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.075053215026855 | KNN Loss: 6.224700927734375 | BCE Loss: 1.8503526449203491\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.094630241394043 | KNN Loss: 6.224642276763916 | BCE Loss: 1.8699883222579956\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.125384330749512 | KNN Loss: 6.224071502685547 | BCE Loss: 1.9013127088546753\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.048147201538086 | KNN Loss: 6.22349214553833 | BCE Loss: 1.8246549367904663\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.087465286254883 | KNN Loss: 6.22311544418335 | BCE Loss: 1.864349603652954\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.023802757263184 | KNN Loss: 6.223113536834717 | BCE Loss: 1.8006892204284668\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.044709205627441 | KNN Loss: 6.222241401672363 | BCE Loss: 1.8224680423736572\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.032660484313965 | KNN Loss: 6.222500324249268 | BCE Loss: 1.8101598024368286\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.00028133392334 | KNN Loss: 6.221348285675049 | BCE Loss: 1.7789326906204224\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.003252029418945 | KNN Loss: 6.219770431518555 | BCE Loss: 1.7834811210632324\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.995131492614746 | KNN Loss: 6.220677852630615 | BCE Loss: 1.7744536399841309\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.9835968017578125 | KNN Loss: 6.218142986297607 | BCE Loss: 1.765453577041626\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.949454307556152 | KNN Loss: 6.216888904571533 | BCE Loss: 1.73256516456604\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.9313249588012695 | KNN Loss: 6.2171454429626465 | BCE Loss: 1.714179277420044\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.916287422180176 | KNN Loss: 6.216228008270264 | BCE Loss: 1.7000592947006226\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.931404113769531 | KNN Loss: 6.214888572692871 | BCE Loss: 1.716515302658081\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.889272212982178 | KNN Loss: 6.213715076446533 | BCE Loss: 1.6755571365356445\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.938895225524902 | KNN Loss: 6.213258266448975 | BCE Loss: 1.7256371974945068\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.849704742431641 | KNN Loss: 6.210962295532227 | BCE Loss: 1.638742446899414\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.878611087799072 | KNN Loss: 6.208024978637695 | BCE Loss: 1.6705862283706665\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.797740936279297 | KNN Loss: 6.208559989929199 | BCE Loss: 1.5891807079315186\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.813210487365723 | KNN Loss: 6.206655025482178 | BCE Loss: 1.606555461883545\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.826234817504883 | KNN Loss: 6.202889442443848 | BCE Loss: 1.623345136642456\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.836917400360107 | KNN Loss: 6.200315952301025 | BCE Loss: 1.636601448059082\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.769495487213135 | KNN Loss: 6.199506759643555 | BCE Loss: 1.56998872756958\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.790506362915039 | KNN Loss: 6.192752838134766 | BCE Loss: 1.5977537631988525\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.753209114074707 | KNN Loss: 6.192676544189453 | BCE Loss: 1.5605326890945435\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.724713325500488 | KNN Loss: 6.190271377563477 | BCE Loss: 1.5344417095184326\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.718528747558594 | KNN Loss: 6.184455394744873 | BCE Loss: 1.5340731143951416\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.667716026306152 | KNN Loss: 6.172842502593994 | BCE Loss: 1.4948736429214478\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.645930767059326 | KNN Loss: 6.173171520233154 | BCE Loss: 1.4727592468261719\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.5752410888671875 | KNN Loss: 6.165436744689941 | BCE Loss: 1.4098042249679565\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.532667636871338 | KNN Loss: 6.154752254486084 | BCE Loss: 1.3779152631759644\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.530046463012695 | KNN Loss: 6.144401550292969 | BCE Loss: 1.3856449127197266\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.501465797424316 | KNN Loss: 6.1321492195129395 | BCE Loss: 1.3693163394927979\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.45402193069458 | KNN Loss: 6.119963645935059 | BCE Loss: 1.334058165550232\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.430082321166992 | KNN Loss: 6.1028571128845215 | BCE Loss: 1.3272252082824707\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.423330307006836 | KNN Loss: 6.097252368927002 | BCE Loss: 1.326077938079834\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.34844446182251 | KNN Loss: 6.059509754180908 | BCE Loss: 1.2889348268508911\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.300991058349609 | KNN Loss: 6.038669109344482 | BCE Loss: 1.2623218297958374\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.248386859893799 | KNN Loss: 6.0105671882629395 | BCE Loss: 1.2378196716308594\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.217254638671875 | KNN Loss: 5.971220016479492 | BCE Loss: 1.246034860610962\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.1150689125061035 | KNN Loss: 5.939725399017334 | BCE Loss: 1.1753435134887695\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.094999313354492 | KNN Loss: 5.8769402503967285 | BCE Loss: 1.2180593013763428\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.966364860534668 | KNN Loss: 5.817079067230225 | BCE Loss: 1.1492857933044434\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.887422561645508 | KNN Loss: 5.745983123779297 | BCE Loss: 1.1414393186569214\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.823383331298828 | KNN Loss: 5.684483528137207 | BCE Loss: 1.138899564743042\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.740147590637207 | KNN Loss: 5.60922908782959 | BCE Loss: 1.130918264389038\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.59070348739624 | KNN Loss: 5.466651439666748 | BCE Loss: 1.1240519285202026\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.488285064697266 | KNN Loss: 5.369657039642334 | BCE Loss: 1.1186281442642212\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.4003005027771 | KNN Loss: 5.293198585510254 | BCE Loss: 1.1071020364761353\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.212427616119385 | KNN Loss: 5.094568729400635 | BCE Loss: 1.1178587675094604\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.0930938720703125 | KNN Loss: 4.9917893409729 | BCE Loss: 1.101304531097412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 5.934737205505371 | KNN Loss: 4.850061893463135 | BCE Loss: 1.0846753120422363\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.811098098754883 | KNN Loss: 4.706234455108643 | BCE Loss: 1.1048638820648193\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.732649326324463 | KNN Loss: 4.618710994720459 | BCE Loss: 1.113938331604004\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.6056976318359375 | KNN Loss: 4.51455545425415 | BCE Loss: 1.0911420583724976\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.611255645751953 | KNN Loss: 4.4998064041137695 | BCE Loss: 1.1114490032196045\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.560354709625244 | KNN Loss: 4.41553258895874 | BCE Loss: 1.1448222398757935\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.4366607666015625 | KNN Loss: 4.3242902755737305 | BCE Loss: 1.112370491027832\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.374316692352295 | KNN Loss: 4.267876148223877 | BCE Loss: 1.1064406633377075\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.318950653076172 | KNN Loss: 4.228024005889893 | BCE Loss: 1.0909264087677002\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.272088527679443 | KNN Loss: 4.17547607421875 | BCE Loss: 1.0966123342514038\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.2134857177734375 | KNN Loss: 4.132808208465576 | BCE Loss: 1.0806775093078613\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.158963680267334 | KNN Loss: 4.068126678466797 | BCE Loss: 1.090837001800537\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.209827899932861 | KNN Loss: 4.097484588623047 | BCE Loss: 1.112343430519104\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.140707492828369 | KNN Loss: 4.073565483093262 | BCE Loss: 1.0671418905258179\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.134883880615234 | KNN Loss: 4.0401787757873535 | BCE Loss: 1.0947051048278809\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.10191011428833 | KNN Loss: 4.000537395477295 | BCE Loss: 1.1013728380203247\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.095410346984863 | KNN Loss: 4.023692607879639 | BCE Loss: 1.0717178583145142\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.149527549743652 | KNN Loss: 4.075890064239502 | BCE Loss: 1.07363760471344\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 5.045210838317871 | KNN Loss: 3.974768877029419 | BCE Loss: 1.0704421997070312\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 5.016243934631348 | KNN Loss: 3.96158504486084 | BCE Loss: 1.054659128189087\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 5.101149559020996 | KNN Loss: 4.00448751449585 | BCE Loss: 1.0966620445251465\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 5.046907424926758 | KNN Loss: 4.0126848220825195 | BCE Loss: 1.0342228412628174\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 5.026793956756592 | KNN Loss: 3.945068597793579 | BCE Loss: 1.0817253589630127\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 5.016244888305664 | KNN Loss: 3.9467406272888184 | BCE Loss: 1.0695042610168457\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 5.036590099334717 | KNN Loss: 3.9546210765838623 | BCE Loss: 1.081968903541565\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.007389545440674 | KNN Loss: 3.9237070083618164 | BCE Loss: 1.0836824178695679\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 5.031628131866455 | KNN Loss: 3.963916540145874 | BCE Loss: 1.067711591720581\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.976742744445801 | KNN Loss: 3.9006998538970947 | BCE Loss: 1.076042652130127\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 5.042220115661621 | KNN Loss: 3.976642608642578 | BCE Loss: 1.0655772686004639\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 5.011645793914795 | KNN Loss: 3.9236693382263184 | BCE Loss: 1.087976336479187\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 5.029026985168457 | KNN Loss: 3.971355676651001 | BCE Loss: 1.057671308517456\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 5.05811882019043 | KNN Loss: 3.990887403488159 | BCE Loss: 1.0672316551208496\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 5.040426731109619 | KNN Loss: 3.9572882652282715 | BCE Loss: 1.0831384658813477\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.96382999420166 | KNN Loss: 3.90914249420166 | BCE Loss: 1.0546876192092896\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 5.065048694610596 | KNN Loss: 3.9891161918640137 | BCE Loss: 1.0759323835372925\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 5.093137741088867 | KNN Loss: 4.018045425415039 | BCE Loss: 1.0750925540924072\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 5.008720397949219 | KNN Loss: 3.9441561698913574 | BCE Loss: 1.0645643472671509\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 5.0079731941223145 | KNN Loss: 3.9554972648620605 | BCE Loss: 1.0524758100509644\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.939607620239258 | KNN Loss: 3.8967103958129883 | BCE Loss: 1.0428974628448486\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.9596757888793945 | KNN Loss: 3.901573657989502 | BCE Loss: 1.058102011680603\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.974268913269043 | KNN Loss: 3.928715229034424 | BCE Loss: 1.0455535650253296\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.9947333335876465 | KNN Loss: 3.927332878112793 | BCE Loss: 1.067400574684143\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.996039390563965 | KNN Loss: 3.9250426292419434 | BCE Loss: 1.0709965229034424\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.947136402130127 | KNN Loss: 3.870734691619873 | BCE Loss: 1.0764018297195435\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.97365140914917 | KNN Loss: 3.921173095703125 | BCE Loss: 1.0524781942367554\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.991316795349121 | KNN Loss: 3.951458215713501 | BCE Loss: 1.039858341217041\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 5.014837741851807 | KNN Loss: 3.9285082817077637 | BCE Loss: 1.086329460144043\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.948307037353516 | KNN Loss: 3.8764052391052246 | BCE Loss: 1.071901798248291\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.919206142425537 | KNN Loss: 3.8640730381011963 | BCE Loss: 1.0551329851150513\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.969013214111328 | KNN Loss: 3.882885456085205 | BCE Loss: 1.086127758026123\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.937113285064697 | KNN Loss: 3.8860931396484375 | BCE Loss: 1.0510201454162598\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.937741279602051 | KNN Loss: 3.883596181869507 | BCE Loss: 1.054145336151123\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.917222023010254 | KNN Loss: 3.8547680377960205 | BCE Loss: 1.0624537467956543\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.977052688598633 | KNN Loss: 3.903676986694336 | BCE Loss: 1.0733754634857178\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.97950553894043 | KNN Loss: 3.9015321731567383 | BCE Loss: 1.077973484992981\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.947636604309082 | KNN Loss: 3.8848605155944824 | BCE Loss: 1.0627760887145996\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.9544878005981445 | KNN Loss: 3.909450054168701 | BCE Loss: 1.0450375080108643\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.948420524597168 | KNN Loss: 3.888115167617798 | BCE Loss: 1.0603054761886597\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.9217023849487305 | KNN Loss: 3.850137233734131 | BCE Loss: 1.07156503200531\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.971907615661621 | KNN Loss: 3.9260590076446533 | BCE Loss: 1.0458486080169678\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.905514717102051 | KNN Loss: 3.878352165222168 | BCE Loss: 1.0271625518798828\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.921711444854736 | KNN Loss: 3.8787925243377686 | BCE Loss: 1.0429189205169678\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.9397125244140625 | KNN Loss: 3.8784093856811523 | BCE Loss: 1.061302900314331\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.934825897216797 | KNN Loss: 3.8814265727996826 | BCE Loss: 1.0533993244171143\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.912968635559082 | KNN Loss: 3.8466837406158447 | BCE Loss: 1.0662848949432373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.933305263519287 | KNN Loss: 3.8630523681640625 | BCE Loss: 1.070252776145935\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.93026876449585 | KNN Loss: 3.8572144508361816 | BCE Loss: 1.073054313659668\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.902609825134277 | KNN Loss: 3.868712902069092 | BCE Loss: 1.0338969230651855\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.924625396728516 | KNN Loss: 3.8884031772613525 | BCE Loss: 1.036222219467163\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.907341003417969 | KNN Loss: 3.8555479049682617 | BCE Loss: 1.0517933368682861\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.910784721374512 | KNN Loss: 3.857971429824829 | BCE Loss: 1.0528134107589722\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.968305587768555 | KNN Loss: 3.8832643032073975 | BCE Loss: 1.0850410461425781\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 5.030717372894287 | KNN Loss: 3.936378002166748 | BCE Loss: 1.0943392515182495\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.898096561431885 | KNN Loss: 3.8668580055236816 | BCE Loss: 1.0312384366989136\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.926985263824463 | KNN Loss: 3.873791217803955 | BCE Loss: 1.0531940460205078\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.964139461517334 | KNN Loss: 3.913585901260376 | BCE Loss: 1.050553560256958\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.940284252166748 | KNN Loss: 3.882974863052368 | BCE Loss: 1.0573093891143799\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.926647663116455 | KNN Loss: 3.8672585487365723 | BCE Loss: 1.0593892335891724\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.892554759979248 | KNN Loss: 3.8270163536071777 | BCE Loss: 1.0655384063720703\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.925342559814453 | KNN Loss: 3.857896566390991 | BCE Loss: 1.067445993423462\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.830578327178955 | KNN Loss: 3.82141375541687 | BCE Loss: 1.009164571762085\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.877864837646484 | KNN Loss: 3.8414409160614014 | BCE Loss: 1.036423683166504\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.88774299621582 | KNN Loss: 3.8392157554626465 | BCE Loss: 1.0485272407531738\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.902509689331055 | KNN Loss: 3.8300108909606934 | BCE Loss: 1.0724990367889404\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.947466850280762 | KNN Loss: 3.8566768169403076 | BCE Loss: 1.090789794921875\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.880709171295166 | KNN Loss: 3.841290235519409 | BCE Loss: 1.0394189357757568\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.939701080322266 | KNN Loss: 3.868863105773926 | BCE Loss: 1.070838212966919\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.89144229888916 | KNN Loss: 3.8615756034851074 | BCE Loss: 1.0298664569854736\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.856627941131592 | KNN Loss: 3.8362057209014893 | BCE Loss: 1.020422101020813\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.882235050201416 | KNN Loss: 3.8255937099456787 | BCE Loss: 1.0566413402557373\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.865353107452393 | KNN Loss: 3.8335063457489014 | BCE Loss: 1.0318467617034912\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.886120796203613 | KNN Loss: 3.831610918045044 | BCE Loss: 1.0545098781585693\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.85427713394165 | KNN Loss: 3.820143461227417 | BCE Loss: 1.0341335535049438\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.9658203125 | KNN Loss: 3.897322416305542 | BCE Loss: 1.068497657775879\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.930041313171387 | KNN Loss: 3.8895320892333984 | BCE Loss: 1.0405089855194092\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.918771266937256 | KNN Loss: 3.8449854850769043 | BCE Loss: 1.0737859010696411\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.821918487548828 | KNN Loss: 3.8011116981506348 | BCE Loss: 1.0208067893981934\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.891644477844238 | KNN Loss: 3.838136911392212 | BCE Loss: 1.0535075664520264\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.861059665679932 | KNN Loss: 3.8428854942321777 | BCE Loss: 1.0181740522384644\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.942452907562256 | KNN Loss: 3.8690361976623535 | BCE Loss: 1.0734167098999023\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.858937740325928 | KNN Loss: 3.820481300354004 | BCE Loss: 1.0384564399719238\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.904745101928711 | KNN Loss: 3.847698450088501 | BCE Loss: 1.057046890258789\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.868862152099609 | KNN Loss: 3.8213038444519043 | BCE Loss: 1.0475581884384155\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.813044548034668 | KNN Loss: 3.771466016769409 | BCE Loss: 1.0415784120559692\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.903059959411621 | KNN Loss: 3.8374011516571045 | BCE Loss: 1.0656585693359375\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.889355659484863 | KNN Loss: 3.841700315475464 | BCE Loss: 1.0476553440093994\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.841076850891113 | KNN Loss: 3.82545804977417 | BCE Loss: 1.0156190395355225\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.932696342468262 | KNN Loss: 3.894885540008545 | BCE Loss: 1.037811040878296\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.854651927947998 | KNN Loss: 3.813218832015991 | BCE Loss: 1.0414332151412964\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.928327560424805 | KNN Loss: 3.843050718307495 | BCE Loss: 1.0852770805358887\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.871226787567139 | KNN Loss: 3.8110289573669434 | BCE Loss: 1.0601979494094849\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.82417106628418 | KNN Loss: 3.784771203994751 | BCE Loss: 1.0394001007080078\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.862429618835449 | KNN Loss: 3.8151533603668213 | BCE Loss: 1.0472760200500488\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.860325813293457 | KNN Loss: 3.814117431640625 | BCE Loss: 1.046208143234253\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.840471267700195 | KNN Loss: 3.8060033321380615 | BCE Loss: 1.034468173980713\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.858821868896484 | KNN Loss: 3.8079004287719727 | BCE Loss: 1.0509212017059326\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.798576831817627 | KNN Loss: 3.7638113498687744 | BCE Loss: 1.034765601158142\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.841133117675781 | KNN Loss: 3.7940948009490967 | BCE Loss: 1.047038197517395\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.826558589935303 | KNN Loss: 3.7826521396636963 | BCE Loss: 1.0439064502716064\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.882635593414307 | KNN Loss: 3.817002296447754 | BCE Loss: 1.0656332969665527\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.878944396972656 | KNN Loss: 3.8310275077819824 | BCE Loss: 1.0479166507720947\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.84359884262085 | KNN Loss: 3.775885820388794 | BCE Loss: 1.0677130222320557\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.854462623596191 | KNN Loss: 3.8171074390411377 | BCE Loss: 1.0373554229736328\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.880521774291992 | KNN Loss: 3.826749324798584 | BCE Loss: 1.0537726879119873\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.830103874206543 | KNN Loss: 3.7702722549438477 | BCE Loss: 1.0598313808441162\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.861963272094727 | KNN Loss: 3.808035135269165 | BCE Loss: 1.0539281368255615\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.828592300415039 | KNN Loss: 3.778639793395996 | BCE Loss: 1.049952507019043\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.849973678588867 | KNN Loss: 3.819071054458618 | BCE Loss: 1.03090238571167\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.8218536376953125 | KNN Loss: 3.793252944946289 | BCE Loss: 1.0286009311676025\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.878349304199219 | KNN Loss: 3.8131890296936035 | BCE Loss: 1.0651605129241943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.861330032348633 | KNN Loss: 3.83554744720459 | BCE Loss: 1.0257823467254639\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.830349445343018 | KNN Loss: 3.784989833831787 | BCE Loss: 1.04535973072052\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.912267208099365 | KNN Loss: 3.822322368621826 | BCE Loss: 1.089944839477539\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.833408355712891 | KNN Loss: 3.7764010429382324 | BCE Loss: 1.057007074356079\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.839536666870117 | KNN Loss: 3.7879533767700195 | BCE Loss: 1.0515832901000977\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.895272254943848 | KNN Loss: 3.8180091381073 | BCE Loss: 1.0772629976272583\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.921525001525879 | KNN Loss: 3.8660085201263428 | BCE Loss: 1.055516242980957\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.872917175292969 | KNN Loss: 3.8399155139923096 | BCE Loss: 1.0330018997192383\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.812205791473389 | KNN Loss: 3.748788833618164 | BCE Loss: 1.063416838645935\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.871244430541992 | KNN Loss: 3.820513963699341 | BCE Loss: 1.0507304668426514\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.864165782928467 | KNN Loss: 3.8126542568206787 | BCE Loss: 1.0515116453170776\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.838603973388672 | KNN Loss: 3.7664437294006348 | BCE Loss: 1.072160005569458\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.888458251953125 | KNN Loss: 3.805989980697632 | BCE Loss: 1.0824685096740723\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.823233604431152 | KNN Loss: 3.780606985092163 | BCE Loss: 1.0426263809204102\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.810474872589111 | KNN Loss: 3.782343626022339 | BCE Loss: 1.0281312465667725\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.794855117797852 | KNN Loss: 3.779306411743164 | BCE Loss: 1.015548825263977\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.842050552368164 | KNN Loss: 3.7827281951904297 | BCE Loss: 1.0593225955963135\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.831208229064941 | KNN Loss: 3.799391508102417 | BCE Loss: 1.0318169593811035\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.790946006774902 | KNN Loss: 3.759922742843628 | BCE Loss: 1.0310230255126953\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.830029487609863 | KNN Loss: 3.797844171524048 | BCE Loss: 1.0321850776672363\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.823429107666016 | KNN Loss: 3.779146432876587 | BCE Loss: 1.0442825555801392\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.80543327331543 | KNN Loss: 3.7716994285583496 | BCE Loss: 1.033733606338501\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.876011848449707 | KNN Loss: 3.808189868927002 | BCE Loss: 1.0678220987319946\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.806085586547852 | KNN Loss: 3.7548084259033203 | BCE Loss: 1.0512769222259521\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.776662349700928 | KNN Loss: 3.749796152114868 | BCE Loss: 1.0268663167953491\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.865701675415039 | KNN Loss: 3.8403642177581787 | BCE Loss: 1.0253374576568604\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.783850193023682 | KNN Loss: 3.7609331607818604 | BCE Loss: 1.0229170322418213\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.7850542068481445 | KNN Loss: 3.757613182067871 | BCE Loss: 1.0274412631988525\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.830129623413086 | KNN Loss: 3.775477170944214 | BCE Loss: 1.0546526908874512\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.840869903564453 | KNN Loss: 3.8016879558563232 | BCE Loss: 1.0391817092895508\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.833288192749023 | KNN Loss: 3.7921431064605713 | BCE Loss: 1.0411452054977417\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.836660385131836 | KNN Loss: 3.803602933883667 | BCE Loss: 1.0330575704574585\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.782553672790527 | KNN Loss: 3.755586624145508 | BCE Loss: 1.0269668102264404\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.822566509246826 | KNN Loss: 3.762578010559082 | BCE Loss: 1.0599884986877441\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.772468566894531 | KNN Loss: 3.74180006980896 | BCE Loss: 1.0306684970855713\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.812697410583496 | KNN Loss: 3.7842941284179688 | BCE Loss: 1.0284035205841064\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.921133995056152 | KNN Loss: 3.8697986602783203 | BCE Loss: 1.0513355731964111\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.8519511222839355 | KNN Loss: 3.806425094604492 | BCE Loss: 1.045526146888733\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.83776330947876 | KNN Loss: 3.803500175476074 | BCE Loss: 1.034263253211975\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.785968780517578 | KNN Loss: 3.758033514022827 | BCE Loss: 1.0279350280761719\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.749476432800293 | KNN Loss: 3.7392451763153076 | BCE Loss: 1.0102310180664062\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.823490142822266 | KNN Loss: 3.779543876647949 | BCE Loss: 1.0439465045928955\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.795927047729492 | KNN Loss: 3.75771164894104 | BCE Loss: 1.0382152795791626\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.7992658615112305 | KNN Loss: 3.747178316116333 | BCE Loss: 1.0520877838134766\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.8048810958862305 | KNN Loss: 3.752307891845703 | BCE Loss: 1.0525729656219482\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.81404972076416 | KNN Loss: 3.783452272415161 | BCE Loss: 1.0305976867675781\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.807717800140381 | KNN Loss: 3.7719016075134277 | BCE Loss: 1.0358160734176636\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.849137306213379 | KNN Loss: 3.7821035385131836 | BCE Loss: 1.0670340061187744\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.828780174255371 | KNN Loss: 3.7981784343719482 | BCE Loss: 1.0306017398834229\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.818093299865723 | KNN Loss: 3.7870023250579834 | BCE Loss: 1.0310908555984497\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.8240742683410645 | KNN Loss: 3.7742574214935303 | BCE Loss: 1.0498169660568237\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.798896312713623 | KNN Loss: 3.7529091835021973 | BCE Loss: 1.0459871292114258\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.7996978759765625 | KNN Loss: 3.789360523223877 | BCE Loss: 1.0103373527526855\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.818656921386719 | KNN Loss: 3.7930517196655273 | BCE Loss: 1.0256054401397705\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.800846576690674 | KNN Loss: 3.7772786617279053 | BCE Loss: 1.0235679149627686\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.810214042663574 | KNN Loss: 3.7901670932769775 | BCE Loss: 1.0200468301773071\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.739071369171143 | KNN Loss: 3.7267708778381348 | BCE Loss: 1.0123006105422974\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.831313610076904 | KNN Loss: 3.7597506046295166 | BCE Loss: 1.0715630054473877\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.800768852233887 | KNN Loss: 3.753880739212036 | BCE Loss: 1.0468883514404297\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.820982933044434 | KNN Loss: 3.7925403118133545 | BCE Loss: 1.0284427404403687\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.789310932159424 | KNN Loss: 3.7686944007873535 | BCE Loss: 1.0206165313720703\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.786200046539307 | KNN Loss: 3.7647831439971924 | BCE Loss: 1.0214169025421143\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.814019203186035 | KNN Loss: 3.7628228664398193 | BCE Loss: 1.0511960983276367\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.820096015930176 | KNN Loss: 3.7549664974212646 | BCE Loss: 1.0651295185089111\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.8675312995910645 | KNN Loss: 3.789062738418579 | BCE Loss: 1.0784685611724854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.803697109222412 | KNN Loss: 3.750314950942993 | BCE Loss: 1.0533822774887085\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.808242321014404 | KNN Loss: 3.760650634765625 | BCE Loss: 1.0475915670394897\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.770204544067383 | KNN Loss: 3.7297310829162598 | BCE Loss: 1.040473461151123\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.796553611755371 | KNN Loss: 3.752020835876465 | BCE Loss: 1.0445325374603271\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.824166774749756 | KNN Loss: 3.776170492172241 | BCE Loss: 1.0479962825775146\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.785524845123291 | KNN Loss: 3.7503693103790283 | BCE Loss: 1.0351555347442627\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.7650604248046875 | KNN Loss: 3.7463302612304688 | BCE Loss: 1.0187301635742188\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.760532379150391 | KNN Loss: 3.742922306060791 | BCE Loss: 1.0176103115081787\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.809689521789551 | KNN Loss: 3.7730278968811035 | BCE Loss: 1.0366618633270264\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.807248115539551 | KNN Loss: 3.7513017654418945 | BCE Loss: 1.0559463500976562\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.766755104064941 | KNN Loss: 3.7557339668273926 | BCE Loss: 1.011021375656128\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.814160346984863 | KNN Loss: 3.764779806137085 | BCE Loss: 1.0493803024291992\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.788011074066162 | KNN Loss: 3.7633302211761475 | BCE Loss: 1.0246808528900146\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.788607597351074 | KNN Loss: 3.7403345108032227 | BCE Loss: 1.0482728481292725\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.747663497924805 | KNN Loss: 3.736968517303467 | BCE Loss: 1.0106947422027588\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.762115001678467 | KNN Loss: 3.7407753467559814 | BCE Loss: 1.0213395357131958\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.777207851409912 | KNN Loss: 3.736067295074463 | BCE Loss: 1.0411404371261597\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.759616851806641 | KNN Loss: 3.7432656288146973 | BCE Loss: 1.0163514614105225\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.7696533203125 | KNN Loss: 3.7458715438842773 | BCE Loss: 1.0237817764282227\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.818617820739746 | KNN Loss: 3.7870819568634033 | BCE Loss: 1.0315361022949219\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.816381931304932 | KNN Loss: 3.7621583938598633 | BCE Loss: 1.0542235374450684\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.7451887130737305 | KNN Loss: 3.730437994003296 | BCE Loss: 1.0147504806518555\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.764911651611328 | KNN Loss: 3.734951972961426 | BCE Loss: 1.0299599170684814\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.85669469833374 | KNN Loss: 3.782027006149292 | BCE Loss: 1.0746676921844482\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.831918716430664 | KNN Loss: 3.7812142372131348 | BCE Loss: 1.0507044792175293\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.802405834197998 | KNN Loss: 3.753803014755249 | BCE Loss: 1.0486027002334595\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.8498358726501465 | KNN Loss: 3.7911808490753174 | BCE Loss: 1.0586549043655396\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.742989540100098 | KNN Loss: 3.7539496421813965 | BCE Loss: 0.9890397191047668\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.806577205657959 | KNN Loss: 3.7596168518066406 | BCE Loss: 1.0469603538513184\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.766605854034424 | KNN Loss: 3.7320899963378906 | BCE Loss: 1.0345159769058228\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.822394371032715 | KNN Loss: 3.7588987350463867 | BCE Loss: 1.0634958744049072\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.761590003967285 | KNN Loss: 3.7428271770477295 | BCE Loss: 1.0187627077102661\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.800373554229736 | KNN Loss: 3.766934633255005 | BCE Loss: 1.0334389209747314\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.845642566680908 | KNN Loss: 3.7905077934265137 | BCE Loss: 1.0551347732543945\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.799254894256592 | KNN Loss: 3.748600482940674 | BCE Loss: 1.0506545305252075\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.819332122802734 | KNN Loss: 3.7953367233276367 | BCE Loss: 1.0239953994750977\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.811530113220215 | KNN Loss: 3.7872631549835205 | BCE Loss: 1.0242667198181152\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.768104076385498 | KNN Loss: 3.7498362064361572 | BCE Loss: 1.0182678699493408\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.720365047454834 | KNN Loss: 3.7254178524017334 | BCE Loss: 0.994947075843811\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.798872470855713 | KNN Loss: 3.7497830390930176 | BCE Loss: 1.0490895509719849\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.750449180603027 | KNN Loss: 3.7091948986053467 | BCE Loss: 1.0412544012069702\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.781874656677246 | KNN Loss: 3.7490079402923584 | BCE Loss: 1.0328664779663086\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.752344608306885 | KNN Loss: 3.7400104999542236 | BCE Loss: 1.0123341083526611\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.783691883087158 | KNN Loss: 3.736670732498169 | BCE Loss: 1.0470212697982788\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.781852722167969 | KNN Loss: 3.748645544052124 | BCE Loss: 1.0332069396972656\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.777322769165039 | KNN Loss: 3.7557497024536133 | BCE Loss: 1.0215730667114258\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.749117851257324 | KNN Loss: 3.7135190963745117 | BCE Loss: 1.035598874092102\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.818731784820557 | KNN Loss: 3.7694339752197266 | BCE Loss: 1.0492979288101196\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.7899088859558105 | KNN Loss: 3.743450164794922 | BCE Loss: 1.0464588403701782\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.792688846588135 | KNN Loss: 3.7766520977020264 | BCE Loss: 1.0160367488861084\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.775415420532227 | KNN Loss: 3.727161407470703 | BCE Loss: 1.048254132270813\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.786040782928467 | KNN Loss: 3.7444052696228027 | BCE Loss: 1.041635513305664\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.823451995849609 | KNN Loss: 3.7729249000549316 | BCE Loss: 1.0505270957946777\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.798760414123535 | KNN Loss: 3.772675037384033 | BCE Loss: 1.026085376739502\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.770959854125977 | KNN Loss: 3.7428154945373535 | BCE Loss: 1.0281445980072021\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.758718490600586 | KNN Loss: 3.7204575538635254 | BCE Loss: 1.0382609367370605\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.780066967010498 | KNN Loss: 3.756164073944092 | BCE Loss: 1.0239028930664062\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.758853912353516 | KNN Loss: 3.74872088432312 | BCE Loss: 1.0101332664489746\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.769329071044922 | KNN Loss: 3.749216318130493 | BCE Loss: 1.0201129913330078\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.775792121887207 | KNN Loss: 3.751737117767334 | BCE Loss: 1.0240548849105835\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.8086442947387695 | KNN Loss: 3.764371395111084 | BCE Loss: 1.0442726612091064\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.720922470092773 | KNN Loss: 3.7085421085357666 | BCE Loss: 1.0123803615570068\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.7758307456970215 | KNN Loss: 3.7442963123321533 | BCE Loss: 1.0315344333648682\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.740845203399658 | KNN Loss: 3.713683843612671 | BCE Loss: 1.0271612405776978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.719110012054443 | KNN Loss: 3.7152626514434814 | BCE Loss: 1.003847360610962\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.844790458679199 | KNN Loss: 3.7730729579925537 | BCE Loss: 1.0717175006866455\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.7858662605285645 | KNN Loss: 3.762510061264038 | BCE Loss: 1.0233561992645264\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.77788782119751 | KNN Loss: 3.7321507930755615 | BCE Loss: 1.0457369089126587\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.776598930358887 | KNN Loss: 3.746474266052246 | BCE Loss: 1.030124545097351\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.750310897827148 | KNN Loss: 3.7381091117858887 | BCE Loss: 1.0122017860412598\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.761354446411133 | KNN Loss: 3.742938995361328 | BCE Loss: 1.0184154510498047\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.790983200073242 | KNN Loss: 3.7687289714813232 | BCE Loss: 1.022254228591919\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.744828701019287 | KNN Loss: 3.7355432510375977 | BCE Loss: 1.0092854499816895\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.784183502197266 | KNN Loss: 3.7405948638916016 | BCE Loss: 1.0435888767242432\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.744748592376709 | KNN Loss: 3.7276294231414795 | BCE Loss: 1.017119288444519\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.786845684051514 | KNN Loss: 3.7559781074523926 | BCE Loss: 1.030867576599121\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.799429893493652 | KNN Loss: 3.7661213874816895 | BCE Loss: 1.0333082675933838\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.773677825927734 | KNN Loss: 3.7465226650238037 | BCE Loss: 1.0271553993225098\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.781399250030518 | KNN Loss: 3.7344226837158203 | BCE Loss: 1.0469764471054077\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.7718000411987305 | KNN Loss: 3.721432685852051 | BCE Loss: 1.0503675937652588\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.76235294342041 | KNN Loss: 3.7188894748687744 | BCE Loss: 1.0434637069702148\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.756904602050781 | KNN Loss: 3.7195093631744385 | BCE Loss: 1.0373950004577637\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.733508110046387 | KNN Loss: 3.6940977573394775 | BCE Loss: 1.0394103527069092\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.784161567687988 | KNN Loss: 3.758329153060913 | BCE Loss: 1.025832176208496\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.732662677764893 | KNN Loss: 3.7076590061187744 | BCE Loss: 1.0250036716461182\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.759725093841553 | KNN Loss: 3.7392635345458984 | BCE Loss: 1.0204615592956543\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.7813544273376465 | KNN Loss: 3.738646984100342 | BCE Loss: 1.0427074432373047\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.771055698394775 | KNN Loss: 3.732842206954956 | BCE Loss: 1.0382136106491089\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.7283501625061035 | KNN Loss: 3.719667673110962 | BCE Loss: 1.0086824893951416\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.790243148803711 | KNN Loss: 3.7505383491516113 | BCE Loss: 1.0397050380706787\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.806971073150635 | KNN Loss: 3.7430801391601562 | BCE Loss: 1.063890814781189\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.77934455871582 | KNN Loss: 3.7361860275268555 | BCE Loss: 1.043158769607544\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.7411274909973145 | KNN Loss: 3.726529598236084 | BCE Loss: 1.01459801197052\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.748086929321289 | KNN Loss: 3.73586106300354 | BCE Loss: 1.012225866317749\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.737608909606934 | KNN Loss: 3.7249462604522705 | BCE Loss: 1.0126628875732422\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.797447204589844 | KNN Loss: 3.745542049407959 | BCE Loss: 1.0519051551818848\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.7232489585876465 | KNN Loss: 3.7247154712677 | BCE Loss: 0.998533308506012\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.772099494934082 | KNN Loss: 3.7189512252807617 | BCE Loss: 1.0531480312347412\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.734217643737793 | KNN Loss: 3.7046501636505127 | BCE Loss: 1.0295673608779907\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.798318862915039 | KNN Loss: 3.7913050651550293 | BCE Loss: 1.0070140361785889\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.707260608673096 | KNN Loss: 3.7134764194488525 | BCE Loss: 0.9937843680381775\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.748085975646973 | KNN Loss: 3.7176716327667236 | BCE Loss: 1.030414342880249\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.762460708618164 | KNN Loss: 3.749098539352417 | BCE Loss: 1.0133620500564575\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.731298923492432 | KNN Loss: 3.71051287651062 | BCE Loss: 1.0207860469818115\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.748271942138672 | KNN Loss: 3.711214780807495 | BCE Loss: 1.0370571613311768\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.761805534362793 | KNN Loss: 3.739344596862793 | BCE Loss: 1.022460699081421\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.7397661209106445 | KNN Loss: 3.700619697570801 | BCE Loss: 1.0391466617584229\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.750893592834473 | KNN Loss: 3.7221896648406982 | BCE Loss: 1.0287041664123535\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.803823471069336 | KNN Loss: 3.753618001937866 | BCE Loss: 1.0502054691314697\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.805994033813477 | KNN Loss: 3.751751184463501 | BCE Loss: 1.0542429685592651\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.692304611206055 | KNN Loss: 3.6874892711639404 | BCE Loss: 1.0048155784606934\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.776352405548096 | KNN Loss: 3.750797986984253 | BCE Loss: 1.0255542993545532\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.740826606750488 | KNN Loss: 3.702280044555664 | BCE Loss: 1.0385464429855347\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.813535213470459 | KNN Loss: 3.7549943923950195 | BCE Loss: 1.0585408210754395\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.744578838348389 | KNN Loss: 3.7302629947662354 | BCE Loss: 1.0143157243728638\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.712682723999023 | KNN Loss: 3.7159245014190674 | BCE Loss: 0.9967581033706665\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.708960056304932 | KNN Loss: 3.6884665489196777 | BCE Loss: 1.020493507385254\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.7156877517700195 | KNN Loss: 3.6959056854248047 | BCE Loss: 1.0197818279266357\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.753771781921387 | KNN Loss: 3.7359719276428223 | BCE Loss: 1.0177998542785645\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.746413230895996 | KNN Loss: 3.714968204498291 | BCE Loss: 1.0314449071884155\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.7468791007995605 | KNN Loss: 3.713042974472046 | BCE Loss: 1.0338362455368042\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.764837265014648 | KNN Loss: 3.7329018115997314 | BCE Loss: 1.031935691833496\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.763463973999023 | KNN Loss: 3.726665735244751 | BCE Loss: 1.0367984771728516\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.768857002258301 | KNN Loss: 3.7370057106018066 | BCE Loss: 1.0318512916564941\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.739691734313965 | KNN Loss: 3.686941623687744 | BCE Loss: 1.0527498722076416\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.782871246337891 | KNN Loss: 3.7323648929595947 | BCE Loss: 1.050506591796875\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.802626609802246 | KNN Loss: 3.7486298084259033 | BCE Loss: 1.0539970397949219\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.771618366241455 | KNN Loss: 3.744004726409912 | BCE Loss: 1.027613639831543\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.731180191040039 | KNN Loss: 3.707200288772583 | BCE Loss: 1.0239800214767456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.760372161865234 | KNN Loss: 3.718369722366333 | BCE Loss: 1.0420024394989014\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.731297492980957 | KNN Loss: 3.6951308250427246 | BCE Loss: 1.036166787147522\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.7566752433776855 | KNN Loss: 3.7244739532470703 | BCE Loss: 1.0322014093399048\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.7477216720581055 | KNN Loss: 3.7371108531951904 | BCE Loss: 1.0106109380722046\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.768034934997559 | KNN Loss: 3.7284927368164062 | BCE Loss: 1.039542317390442\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.728218078613281 | KNN Loss: 3.709228754043579 | BCE Loss: 1.0189893245697021\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.718601226806641 | KNN Loss: 3.6989688873291016 | BCE Loss: 1.019632339477539\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.711813449859619 | KNN Loss: 3.712442636489868 | BCE Loss: 0.9993706345558167\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.745917320251465 | KNN Loss: 3.715261697769165 | BCE Loss: 1.0306556224822998\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.754389762878418 | KNN Loss: 3.711888551712036 | BCE Loss: 1.0425012111663818\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.785615921020508 | KNN Loss: 3.7303576469421387 | BCE Loss: 1.0552583932876587\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.728769302368164 | KNN Loss: 3.7051663398742676 | BCE Loss: 1.0236029624938965\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.770296096801758 | KNN Loss: 3.7394330501556396 | BCE Loss: 1.0308630466461182\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.741184234619141 | KNN Loss: 3.68831467628479 | BCE Loss: 1.0528693199157715\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.74479866027832 | KNN Loss: 3.726985216140747 | BCE Loss: 1.0178134441375732\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.737451553344727 | KNN Loss: 3.704495429992676 | BCE Loss: 1.0329558849334717\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.743626594543457 | KNN Loss: 3.7012710571289062 | BCE Loss: 1.0423557758331299\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.74393892288208 | KNN Loss: 3.7280991077423096 | BCE Loss: 1.0158398151397705\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.726369380950928 | KNN Loss: 3.7026565074920654 | BCE Loss: 1.0237128734588623\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.752669334411621 | KNN Loss: 3.7152085304260254 | BCE Loss: 1.0374610424041748\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.7266740798950195 | KNN Loss: 3.7232096195220947 | BCE Loss: 1.0034644603729248\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.771198272705078 | KNN Loss: 3.7498106956481934 | BCE Loss: 1.0213873386383057\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.726798057556152 | KNN Loss: 3.712202787399292 | BCE Loss: 1.0145951509475708\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.703149318695068 | KNN Loss: 3.698606252670288 | BCE Loss: 1.0045429468154907\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.702725887298584 | KNN Loss: 3.6863205432891846 | BCE Loss: 1.0164053440093994\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.782068252563477 | KNN Loss: 3.7208938598632812 | BCE Loss: 1.0611745119094849\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.759369373321533 | KNN Loss: 3.7434892654418945 | BCE Loss: 1.0158801078796387\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.760198593139648 | KNN Loss: 3.7134714126586914 | BCE Loss: 1.0467274188995361\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.796486854553223 | KNN Loss: 3.744856595993042 | BCE Loss: 1.0516302585601807\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.700514793395996 | KNN Loss: 3.7083749771118164 | BCE Loss: 0.9921398162841797\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.783079147338867 | KNN Loss: 3.7479805946350098 | BCE Loss: 1.0350985527038574\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.732858180999756 | KNN Loss: 3.7116551399230957 | BCE Loss: 1.0212029218673706\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.790432453155518 | KNN Loss: 3.7358179092407227 | BCE Loss: 1.054614543914795\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.756645202636719 | KNN Loss: 3.705543041229248 | BCE Loss: 1.0511019229888916\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.777304649353027 | KNN Loss: 3.7457234859466553 | BCE Loss: 1.031581163406372\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.772170543670654 | KNN Loss: 3.7384836673736572 | BCE Loss: 1.033686876296997\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.806424617767334 | KNN Loss: 3.733617067337036 | BCE Loss: 1.0728074312210083\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.7463154792785645 | KNN Loss: 3.737684726715088 | BCE Loss: 1.0086307525634766\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.755136966705322 | KNN Loss: 3.7119343280792236 | BCE Loss: 1.0432026386260986\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.74492073059082 | KNN Loss: 3.7131412029266357 | BCE Loss: 1.0317797660827637\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.7688798904418945 | KNN Loss: 3.7131593227386475 | BCE Loss: 1.055720329284668\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.813648223876953 | KNN Loss: 3.7673492431640625 | BCE Loss: 1.0462989807128906\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.735538005828857 | KNN Loss: 3.716566324234009 | BCE Loss: 1.0189718008041382\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.728283882141113 | KNN Loss: 3.706509590148926 | BCE Loss: 1.0217740535736084\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.759923458099365 | KNN Loss: 3.722691059112549 | BCE Loss: 1.0372323989868164\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.73433780670166 | KNN Loss: 3.7117810249328613 | BCE Loss: 1.0225567817687988\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.7801055908203125 | KNN Loss: 3.7430477142333984 | BCE Loss: 1.037057876586914\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.718092918395996 | KNN Loss: 3.717726469039917 | BCE Loss: 1.0003666877746582\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.719809055328369 | KNN Loss: 3.6830148696899414 | BCE Loss: 1.0367941856384277\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.78794527053833 | KNN Loss: 3.7236430644989014 | BCE Loss: 1.0643022060394287\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.708483695983887 | KNN Loss: 3.702299118041992 | BCE Loss: 1.0061848163604736\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.814203262329102 | KNN Loss: 3.7667312622070312 | BCE Loss: 1.0474721193313599\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.753294944763184 | KNN Loss: 3.732518434524536 | BCE Loss: 1.0207762718200684\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.763150215148926 | KNN Loss: 3.706136465072632 | BCE Loss: 1.057013988494873\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.747053146362305 | KNN Loss: 3.7287001609802246 | BCE Loss: 1.018352746963501\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.750405311584473 | KNN Loss: 3.7170393466949463 | BCE Loss: 1.0333662033081055\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.7625250816345215 | KNN Loss: 3.741729497909546 | BCE Loss: 1.020795464515686\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.7538838386535645 | KNN Loss: 3.7115280628204346 | BCE Loss: 1.0423556566238403\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.700865268707275 | KNN Loss: 3.6831936836242676 | BCE Loss: 1.0176715850830078\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.706638336181641 | KNN Loss: 3.699152708053589 | BCE Loss: 1.0074856281280518\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.737691879272461 | KNN Loss: 3.6963136196136475 | BCE Loss: 1.0413782596588135\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.742318153381348 | KNN Loss: 3.739877462387085 | BCE Loss: 1.0024408102035522\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.741828918457031 | KNN Loss: 3.7090580463409424 | BCE Loss: 1.0327707529067993\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.7764892578125 | KNN Loss: 3.710599184036255 | BCE Loss: 1.0658900737762451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.7230544090271 | KNN Loss: 3.6981046199798584 | BCE Loss: 1.0249499082565308\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.7285261154174805 | KNN Loss: 3.6937403678894043 | BCE Loss: 1.0347857475280762\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.7381086349487305 | KNN Loss: 3.6927576065063477 | BCE Loss: 1.0453510284423828\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.750196933746338 | KNN Loss: 3.7426750659942627 | BCE Loss: 1.0075218677520752\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.773016452789307 | KNN Loss: 3.740229368209839 | BCE Loss: 1.0327869653701782\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.744694709777832 | KNN Loss: 3.7445616722106934 | BCE Loss: 1.0001327991485596\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.742612361907959 | KNN Loss: 3.7221782207489014 | BCE Loss: 1.0204341411590576\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.666397571563721 | KNN Loss: 3.6742031574249268 | BCE Loss: 0.992194414138794\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.741629600524902 | KNN Loss: 3.7124435901641846 | BCE Loss: 1.0291862487792969\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.827464580535889 | KNN Loss: 3.7980830669403076 | BCE Loss: 1.0293816328048706\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.73424768447876 | KNN Loss: 3.700275182723999 | BCE Loss: 1.0339725017547607\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.773526668548584 | KNN Loss: 3.74218487739563 | BCE Loss: 1.0313419103622437\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.700256824493408 | KNN Loss: 3.6904540061950684 | BCE Loss: 1.0098026990890503\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.744726181030273 | KNN Loss: 3.722867488861084 | BCE Loss: 1.0218589305877686\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.70630407333374 | KNN Loss: 3.6817946434020996 | BCE Loss: 1.0245094299316406\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.785379409790039 | KNN Loss: 3.7361056804656982 | BCE Loss: 1.0492736101150513\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.759286880493164 | KNN Loss: 3.7418558597564697 | BCE Loss: 1.0174307823181152\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.755690574645996 | KNN Loss: 3.7377254962921143 | BCE Loss: 1.0179648399353027\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.740662574768066 | KNN Loss: 3.6984705924987793 | BCE Loss: 1.0421918630599976\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.701507568359375 | KNN Loss: 3.697857618331909 | BCE Loss: 1.0036498308181763\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.7224931716918945 | KNN Loss: 3.696810007095337 | BCE Loss: 1.0256834030151367\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.712121963500977 | KNN Loss: 3.6925604343414307 | BCE Loss: 1.019561529159546\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.753096580505371 | KNN Loss: 3.727449655532837 | BCE Loss: 1.0256471633911133\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.78753662109375 | KNN Loss: 3.754511833190918 | BCE Loss: 1.0330250263214111\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.757564544677734 | KNN Loss: 3.7471156120300293 | BCE Loss: 1.010448694229126\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.7466325759887695 | KNN Loss: 3.715129852294922 | BCE Loss: 1.0315029621124268\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.704987049102783 | KNN Loss: 3.6942405700683594 | BCE Loss: 1.0107464790344238\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.716325759887695 | KNN Loss: 3.6949098110198975 | BCE Loss: 1.021416187286377\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.769525527954102 | KNN Loss: 3.7338013648986816 | BCE Loss: 1.0357239246368408\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.768120288848877 | KNN Loss: 3.7524406909942627 | BCE Loss: 1.0156795978546143\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.705410480499268 | KNN Loss: 3.6792304515838623 | BCE Loss: 1.0261800289154053\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.737551212310791 | KNN Loss: 3.71781325340271 | BCE Loss: 1.019737958908081\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.723289489746094 | KNN Loss: 3.694448471069336 | BCE Loss: 1.0288410186767578\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.776552677154541 | KNN Loss: 3.739654541015625 | BCE Loss: 1.036898136138916\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.760614395141602 | KNN Loss: 3.7283055782318115 | BCE Loss: 1.0323090553283691\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.7688775062561035 | KNN Loss: 3.771568775177002 | BCE Loss: 0.997308611869812\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.770742416381836 | KNN Loss: 3.7288455963134766 | BCE Loss: 1.0418970584869385\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.705163955688477 | KNN Loss: 3.7086591720581055 | BCE Loss: 0.996504545211792\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.760536193847656 | KNN Loss: 3.7345306873321533 | BCE Loss: 1.026005506515503\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.749503135681152 | KNN Loss: 3.7167296409606934 | BCE Loss: 1.032773494720459\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.820051193237305 | KNN Loss: 3.7700116634368896 | BCE Loss: 1.0500397682189941\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.772387981414795 | KNN Loss: 3.700775623321533 | BCE Loss: 1.0716122388839722\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.750601291656494 | KNN Loss: 3.731321334838867 | BCE Loss: 1.019279956817627\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.711008071899414 | KNN Loss: 3.6864283084869385 | BCE Loss: 1.0245800018310547\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.733180999755859 | KNN Loss: 3.71810245513916 | BCE Loss: 1.0150783061981201\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.70475435256958 | KNN Loss: 3.6745660305023193 | BCE Loss: 1.0301883220672607\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.751481533050537 | KNN Loss: 3.745964288711548 | BCE Loss: 1.0055172443389893\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.808529376983643 | KNN Loss: 3.7631072998046875 | BCE Loss: 1.0454221963882446\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.763458728790283 | KNN Loss: 3.7256076335906982 | BCE Loss: 1.037851095199585\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.747475624084473 | KNN Loss: 3.7155706882476807 | BCE Loss: 1.031904697418213\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.724964618682861 | KNN Loss: 3.7216455936431885 | BCE Loss: 1.0033191442489624\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.720976829528809 | KNN Loss: 3.6908717155456543 | BCE Loss: 1.0301052331924438\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.771559715270996 | KNN Loss: 3.739821195602417 | BCE Loss: 1.0317387580871582\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.757297515869141 | KNN Loss: 3.720656156539917 | BCE Loss: 1.0366413593292236\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.716341018676758 | KNN Loss: 3.68585205078125 | BCE Loss: 1.0304889678955078\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.677887439727783 | KNN Loss: 3.6886327266693115 | BCE Loss: 0.9892545342445374\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.775781631469727 | KNN Loss: 3.7511324882507324 | BCE Loss: 1.024648904800415\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.774703025817871 | KNN Loss: 3.7572009563446045 | BCE Loss: 1.0175020694732666\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.74360466003418 | KNN Loss: 3.729881525039673 | BCE Loss: 1.0137232542037964\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.7750020027160645 | KNN Loss: 3.7420544624328613 | BCE Loss: 1.0329475402832031\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.759400367736816 | KNN Loss: 3.7301247119903564 | BCE Loss: 1.0292754173278809\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.691898822784424 | KNN Loss: 3.6963062286376953 | BCE Loss: 0.9955925941467285\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.74452543258667 | KNN Loss: 3.7189719676971436 | BCE Loss: 1.025553584098816\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.74730110168457 | KNN Loss: 3.71897029876709 | BCE Loss: 1.0283305644989014\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.78265380859375 | KNN Loss: 3.7143185138702393 | BCE Loss: 1.0683352947235107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.7436933517456055 | KNN Loss: 3.7249066829681396 | BCE Loss: 1.0187864303588867\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.7403717041015625 | KNN Loss: 3.7147579193115234 | BCE Loss: 1.025613784790039\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.728116989135742 | KNN Loss: 3.6967148780822754 | BCE Loss: 1.0314019918441772\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.7343878746032715 | KNN Loss: 3.707601308822632 | BCE Loss: 1.02678644657135\n",
      "Epoch    87: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.703076362609863 | KNN Loss: 3.6937506198883057 | BCE Loss: 1.009325623512268\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.7834577560424805 | KNN Loss: 3.723421812057495 | BCE Loss: 1.0600359439849854\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.7734904289245605 | KNN Loss: 3.7249653339385986 | BCE Loss: 1.048525094985962\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.6984734535217285 | KNN Loss: 3.710012435913086 | BCE Loss: 0.988460898399353\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.767815589904785 | KNN Loss: 3.7346043586730957 | BCE Loss: 1.0332111120224\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.737143039703369 | KNN Loss: 3.716907024383545 | BCE Loss: 1.0202361345291138\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.762968063354492 | KNN Loss: 3.7217729091644287 | BCE Loss: 1.041195034980774\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.685487270355225 | KNN Loss: 3.685101270675659 | BCE Loss: 1.0003858804702759\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.738883018493652 | KNN Loss: 3.7118146419525146 | BCE Loss: 1.0270686149597168\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.775045394897461 | KNN Loss: 3.7536182403564453 | BCE Loss: 1.0214273929595947\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.742150783538818 | KNN Loss: 3.682666301727295 | BCE Loss: 1.0594844818115234\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.772088050842285 | KNN Loss: 3.750138282775879 | BCE Loss: 1.0219496488571167\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.708107948303223 | KNN Loss: 3.6898131370544434 | BCE Loss: 1.0182948112487793\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.777263164520264 | KNN Loss: 3.725665807723999 | BCE Loss: 1.0515973567962646\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.765172004699707 | KNN Loss: 3.726905107498169 | BCE Loss: 1.0382670164108276\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.707460403442383 | KNN Loss: 3.692556619644165 | BCE Loss: 1.0149037837982178\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.749215126037598 | KNN Loss: 3.7464280128479004 | BCE Loss: 1.0027872323989868\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.7142415046691895 | KNN Loss: 3.6823315620422363 | BCE Loss: 1.0319099426269531\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.7190985679626465 | KNN Loss: 3.714181661605835 | BCE Loss: 1.004917025566101\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.772585868835449 | KNN Loss: 3.728471279144287 | BCE Loss: 1.044114589691162\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.740618705749512 | KNN Loss: 3.710996150970459 | BCE Loss: 1.0296223163604736\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.768457412719727 | KNN Loss: 3.7181591987609863 | BCE Loss: 1.0502984523773193\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.711895942687988 | KNN Loss: 3.6998543739318848 | BCE Loss: 1.0120418071746826\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.682734489440918 | KNN Loss: 3.6839890480041504 | BCE Loss: 0.9987455606460571\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.697726249694824 | KNN Loss: 3.692638635635376 | BCE Loss: 1.0050877332687378\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.817241668701172 | KNN Loss: 3.750826120376587 | BCE Loss: 1.0664153099060059\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.708442211151123 | KNN Loss: 3.6740846633911133 | BCE Loss: 1.0343575477600098\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.776666641235352 | KNN Loss: 3.7325236797332764 | BCE Loss: 1.044142723083496\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.6856231689453125 | KNN Loss: 3.6736507415771484 | BCE Loss: 1.0119723081588745\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.7225470542907715 | KNN Loss: 3.7121384143829346 | BCE Loss: 1.0104085206985474\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.764595985412598 | KNN Loss: 3.739546060562134 | BCE Loss: 1.0250496864318848\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.706676483154297 | KNN Loss: 3.69997501373291 | BCE Loss: 1.0067014694213867\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.755765438079834 | KNN Loss: 3.7155776023864746 | BCE Loss: 1.0401877164840698\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.714596271514893 | KNN Loss: 3.712188243865967 | BCE Loss: 1.0024080276489258\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.754003524780273 | KNN Loss: 3.7181856632232666 | BCE Loss: 1.0358176231384277\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.693951606750488 | KNN Loss: 3.6924896240234375 | BCE Loss: 1.0014619827270508\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.739747524261475 | KNN Loss: 3.7179293632507324 | BCE Loss: 1.0218180418014526\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.772552967071533 | KNN Loss: 3.761525869369507 | BCE Loss: 1.0110270977020264\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.728791236877441 | KNN Loss: 3.700150489807129 | BCE Loss: 1.0286405086517334\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.779572486877441 | KNN Loss: 3.756572961807251 | BCE Loss: 1.0229997634887695\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.747992992401123 | KNN Loss: 3.710731029510498 | BCE Loss: 1.0372618436813354\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.74246883392334 | KNN Loss: 3.70693039894104 | BCE Loss: 1.0355383157730103\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.7793354988098145 | KNN Loss: 3.7113699913024902 | BCE Loss: 1.0679653882980347\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.736790657043457 | KNN Loss: 3.748872756958008 | BCE Loss: 0.9879178404808044\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.721595287322998 | KNN Loss: 3.709956169128418 | BCE Loss: 1.0116392374038696\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.756772518157959 | KNN Loss: 3.725717067718506 | BCE Loss: 1.0310554504394531\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.763249397277832 | KNN Loss: 3.736719846725464 | BCE Loss: 1.0265295505523682\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.706437110900879 | KNN Loss: 3.693488359451294 | BCE Loss: 1.0129486322402954\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.654352188110352 | KNN Loss: 3.6607816219329834 | BCE Loss: 0.9935704469680786\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.76199197769165 | KNN Loss: 3.7002692222595215 | BCE Loss: 1.061722755432129\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.750970363616943 | KNN Loss: 3.7409913539886475 | BCE Loss: 1.009979009628296\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.753053665161133 | KNN Loss: 3.7419919967651367 | BCE Loss: 1.0110619068145752\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.753616809844971 | KNN Loss: 3.7049007415771484 | BCE Loss: 1.0487160682678223\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.719336986541748 | KNN Loss: 3.6895899772644043 | BCE Loss: 1.0297470092773438\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.734322547912598 | KNN Loss: 3.714796543121338 | BCE Loss: 1.0195260047912598\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.779774188995361 | KNN Loss: 3.7316043376922607 | BCE Loss: 1.048169732093811\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.733630180358887 | KNN Loss: 3.6963889598846436 | BCE Loss: 1.0372413396835327\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.750383377075195 | KNN Loss: 3.717480182647705 | BCE Loss: 1.0329031944274902\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.705642223358154 | KNN Loss: 3.703477144241333 | BCE Loss: 1.0021649599075317\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.742166519165039 | KNN Loss: 3.705970287322998 | BCE Loss: 1.0361964702606201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.7416276931762695 | KNN Loss: 3.710026502609253 | BCE Loss: 1.0316014289855957\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.699658393859863 | KNN Loss: 3.70951247215271 | BCE Loss: 0.9901460409164429\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.779877662658691 | KNN Loss: 3.74356746673584 | BCE Loss: 1.0363104343414307\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.720315933227539 | KNN Loss: 3.6791651248931885 | BCE Loss: 1.041150689125061\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.735020160675049 | KNN Loss: 3.7110471725463867 | BCE Loss: 1.0239731073379517\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.723799228668213 | KNN Loss: 3.68276047706604 | BCE Loss: 1.0410388708114624\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.699944972991943 | KNN Loss: 3.713561773300171 | BCE Loss: 0.986383318901062\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.738221168518066 | KNN Loss: 3.7093424797058105 | BCE Loss: 1.0288784503936768\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.736861228942871 | KNN Loss: 3.715123414993286 | BCE Loss: 1.0217379331588745\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.747337341308594 | KNN Loss: 3.7095038890838623 | BCE Loss: 1.0378332138061523\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.725837707519531 | KNN Loss: 3.706555128097534 | BCE Loss: 1.0192826986312866\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.731141090393066 | KNN Loss: 3.6983773708343506 | BCE Loss: 1.0327637195587158\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.70585823059082 | KNN Loss: 3.686251640319824 | BCE Loss: 1.019606351852417\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.781427383422852 | KNN Loss: 3.7530128955841064 | BCE Loss: 1.0284147262573242\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.697942733764648 | KNN Loss: 3.664170742034912 | BCE Loss: 1.0337721109390259\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.720436096191406 | KNN Loss: 3.707209348678589 | BCE Loss: 1.0132266283035278\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.706140041351318 | KNN Loss: 3.6932215690612793 | BCE Loss: 1.012918472290039\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.688277244567871 | KNN Loss: 3.6688008308410645 | BCE Loss: 1.019476294517517\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.763860702514648 | KNN Loss: 3.722412586212158 | BCE Loss: 1.0414478778839111\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.7851362228393555 | KNN Loss: 3.752542734146118 | BCE Loss: 1.0325934886932373\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.720911979675293 | KNN Loss: 3.7008190155029297 | BCE Loss: 1.0200929641723633\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.718796253204346 | KNN Loss: 3.7076523303985596 | BCE Loss: 1.0111438035964966\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.704699993133545 | KNN Loss: 3.6735751628875732 | BCE Loss: 1.0311248302459717\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.727688312530518 | KNN Loss: 3.709088087081909 | BCE Loss: 1.0186002254486084\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.760868072509766 | KNN Loss: 3.740158796310425 | BCE Loss: 1.02070951461792\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.786323547363281 | KNN Loss: 3.7440459728240967 | BCE Loss: 1.0422773361206055\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.720124244689941 | KNN Loss: 3.692699909210205 | BCE Loss: 1.0274245738983154\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.729432582855225 | KNN Loss: 3.7057206630706787 | BCE Loss: 1.023711919784546\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.682742118835449 | KNN Loss: 3.663004159927368 | BCE Loss: 1.0197380781173706\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.77331018447876 | KNN Loss: 3.7490110397338867 | BCE Loss: 1.0242992639541626\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.686912536621094 | KNN Loss: 3.678278684616089 | BCE Loss: 1.0086338520050049\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.78156042098999 | KNN Loss: 3.7589924335479736 | BCE Loss: 1.0225679874420166\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.772430896759033 | KNN Loss: 3.715151071548462 | BCE Loss: 1.0572798252105713\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.743108749389648 | KNN Loss: 3.7117772102355957 | BCE Loss: 1.0313313007354736\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.699695587158203 | KNN Loss: 3.675274610519409 | BCE Loss: 1.0244208574295044\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.703083038330078 | KNN Loss: 3.693312406539917 | BCE Loss: 1.009770393371582\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.716303825378418 | KNN Loss: 3.6921050548553467 | BCE Loss: 1.0241988897323608\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.79429292678833 | KNN Loss: 3.737281322479248 | BCE Loss: 1.057011604309082\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.7952799797058105 | KNN Loss: 3.7432265281677246 | BCE Loss: 1.052053451538086\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.774110317230225 | KNN Loss: 3.722001552581787 | BCE Loss: 1.0521087646484375\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.728331089019775 | KNN Loss: 3.701448917388916 | BCE Loss: 1.0268821716308594\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.738358974456787 | KNN Loss: 3.714662551879883 | BCE Loss: 1.0236963033676147\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.696983814239502 | KNN Loss: 3.6849365234375 | BCE Loss: 1.012047290802002\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.690514087677002 | KNN Loss: 3.688607692718506 | BCE Loss: 1.001906394958496\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.71708869934082 | KNN Loss: 3.6884868144989014 | BCE Loss: 1.0286016464233398\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.7355451583862305 | KNN Loss: 3.716468095779419 | BCE Loss: 1.0190773010253906\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.749020099639893 | KNN Loss: 3.721846342086792 | BCE Loss: 1.0271738767623901\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.779683589935303 | KNN Loss: 3.73831844329834 | BCE Loss: 1.0413650274276733\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.754240989685059 | KNN Loss: 3.7249391078948975 | BCE Loss: 1.0293021202087402\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.717880725860596 | KNN Loss: 3.709721803665161 | BCE Loss: 1.008158802986145\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.768200397491455 | KNN Loss: 3.7161638736724854 | BCE Loss: 1.0520364046096802\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.815973281860352 | KNN Loss: 3.7534077167510986 | BCE Loss: 1.0625653266906738\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.710272789001465 | KNN Loss: 3.6984827518463135 | BCE Loss: 1.0117899179458618\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.750524997711182 | KNN Loss: 3.71589732170105 | BCE Loss: 1.0346275568008423\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.732644081115723 | KNN Loss: 3.688141107559204 | BCE Loss: 1.0445027351379395\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.678097248077393 | KNN Loss: 3.6827738285064697 | BCE Loss: 0.9953234791755676\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.742300033569336 | KNN Loss: 3.7282378673553467 | BCE Loss: 1.0140619277954102\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.777262210845947 | KNN Loss: 3.726620674133301 | BCE Loss: 1.0506415367126465\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.765806198120117 | KNN Loss: 3.730055093765259 | BCE Loss: 1.0357511043548584\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.757411479949951 | KNN Loss: 3.7167506217956543 | BCE Loss: 1.0406607389450073\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.779087543487549 | KNN Loss: 3.7399308681488037 | BCE Loss: 1.0391565561294556\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.701274871826172 | KNN Loss: 3.693516492843628 | BCE Loss: 1.007758617401123\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.709797382354736 | KNN Loss: 3.6908323764801025 | BCE Loss: 1.0189650058746338\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.710344314575195 | KNN Loss: 3.704594612121582 | BCE Loss: 1.0057497024536133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.726795673370361 | KNN Loss: 3.7179653644561768 | BCE Loss: 1.0088303089141846\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.707529544830322 | KNN Loss: 3.6718239784240723 | BCE Loss: 1.03570556640625\n",
      "Epoch   108: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.67328405380249 | KNN Loss: 3.662954807281494 | BCE Loss: 1.010329246520996\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.7479166984558105 | KNN Loss: 3.7154738903045654 | BCE Loss: 1.0324428081512451\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.743028163909912 | KNN Loss: 3.768920421600342 | BCE Loss: 0.9741077423095703\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.756906986236572 | KNN Loss: 3.720902442932129 | BCE Loss: 1.0360045433044434\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.758960723876953 | KNN Loss: 3.736894130706787 | BCE Loss: 1.022066354751587\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.684635162353516 | KNN Loss: 3.6700241565704346 | BCE Loss: 1.0146111249923706\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.737757682800293 | KNN Loss: 3.6908674240112305 | BCE Loss: 1.0468902587890625\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.729635238647461 | KNN Loss: 3.722710371017456 | BCE Loss: 1.0069246292114258\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.730759620666504 | KNN Loss: 3.6857683658599854 | BCE Loss: 1.0449910163879395\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.689418792724609 | KNN Loss: 3.675647020339966 | BCE Loss: 1.0137717723846436\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.715081214904785 | KNN Loss: 3.694643497467041 | BCE Loss: 1.0204379558563232\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.734606742858887 | KNN Loss: 3.708660364151001 | BCE Loss: 1.0259461402893066\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.734374046325684 | KNN Loss: 3.749561071395874 | BCE Loss: 0.9848129749298096\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.75685453414917 | KNN Loss: 3.7261569499969482 | BCE Loss: 1.0306977033615112\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.793459415435791 | KNN Loss: 3.735548973083496 | BCE Loss: 1.0579103231430054\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.714578628540039 | KNN Loss: 3.701101303100586 | BCE Loss: 1.0134773254394531\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.694331645965576 | KNN Loss: 3.6885080337524414 | BCE Loss: 1.0058236122131348\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.7351226806640625 | KNN Loss: 3.729264736175537 | BCE Loss: 1.005858063697815\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.778634071350098 | KNN Loss: 3.7216763496398926 | BCE Loss: 1.056957721710205\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.738503456115723 | KNN Loss: 3.7170369625091553 | BCE Loss: 1.0214667320251465\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.730884075164795 | KNN Loss: 3.7108030319213867 | BCE Loss: 1.0200810432434082\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.730956077575684 | KNN Loss: 3.7054600715637207 | BCE Loss: 1.0254961252212524\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.736739158630371 | KNN Loss: 3.722975730895996 | BCE Loss: 1.013763189315796\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.718879699707031 | KNN Loss: 3.723419666290283 | BCE Loss: 0.9954602718353271\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.73314905166626 | KNN Loss: 3.714527130126953 | BCE Loss: 1.0186219215393066\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.773617267608643 | KNN Loss: 3.7250871658325195 | BCE Loss: 1.0485299825668335\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.723350524902344 | KNN Loss: 3.716010570526123 | BCE Loss: 1.0073399543762207\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.70182991027832 | KNN Loss: 3.6986687183380127 | BCE Loss: 1.0031611919403076\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.748634338378906 | KNN Loss: 3.715698003768921 | BCE Loss: 1.0329365730285645\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.714323043823242 | KNN Loss: 3.684890031814575 | BCE Loss: 1.0294331312179565\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.717191696166992 | KNN Loss: 3.69474720954895 | BCE Loss: 1.022444725036621\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.756631851196289 | KNN Loss: 3.737506151199341 | BCE Loss: 1.0191256999969482\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.695106506347656 | KNN Loss: 3.688650131225586 | BCE Loss: 1.0064563751220703\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.730734348297119 | KNN Loss: 3.6964871883392334 | BCE Loss: 1.0342471599578857\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.694174289703369 | KNN Loss: 3.6939051151275635 | BCE Loss: 1.0002691745758057\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.725170135498047 | KNN Loss: 3.7144768238067627 | BCE Loss: 1.010693073272705\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.695923805236816 | KNN Loss: 3.6794209480285645 | BCE Loss: 1.0165027379989624\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.73387336730957 | KNN Loss: 3.692376136779785 | BCE Loss: 1.0414974689483643\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.760202407836914 | KNN Loss: 3.7288057804107666 | BCE Loss: 1.0313963890075684\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.735323905944824 | KNN Loss: 3.7270708084106445 | BCE Loss: 1.0082528591156006\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.726576805114746 | KNN Loss: 3.703591823577881 | BCE Loss: 1.0229852199554443\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.72652006149292 | KNN Loss: 3.679389238357544 | BCE Loss: 1.047130823135376\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.69948673248291 | KNN Loss: 3.7046797275543213 | BCE Loss: 0.994807243347168\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.723547458648682 | KNN Loss: 3.6887269020080566 | BCE Loss: 1.034820556640625\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.761367321014404 | KNN Loss: 3.6869044303894043 | BCE Loss: 1.0744627714157104\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.736810684204102 | KNN Loss: 3.6917724609375 | BCE Loss: 1.045038104057312\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.683339595794678 | KNN Loss: 3.6653881072998047 | BCE Loss: 1.017951488494873\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.720826148986816 | KNN Loss: 3.697112798690796 | BCE Loss: 1.0237133502960205\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.715013027191162 | KNN Loss: 3.68477201461792 | BCE Loss: 1.0302410125732422\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.694859027862549 | KNN Loss: 3.6952245235443115 | BCE Loss: 0.999634325504303\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.736016750335693 | KNN Loss: 3.7033867835998535 | BCE Loss: 1.0326300859451294\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.777730941772461 | KNN Loss: 3.7490382194519043 | BCE Loss: 1.0286928415298462\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.665189266204834 | KNN Loss: 3.6633472442626953 | BCE Loss: 1.0018420219421387\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.704383850097656 | KNN Loss: 3.687004566192627 | BCE Loss: 1.0173792839050293\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.7121477127075195 | KNN Loss: 3.703859567642212 | BCE Loss: 1.0082881450653076\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.701695442199707 | KNN Loss: 3.667282819747925 | BCE Loss: 1.0344128608703613\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.7317376136779785 | KNN Loss: 3.712376594543457 | BCE Loss: 1.019360899925232\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.711165428161621 | KNN Loss: 3.7041854858398438 | BCE Loss: 1.0069801807403564\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.769054889678955 | KNN Loss: 3.745262384414673 | BCE Loss: 1.0237926244735718\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.790299892425537 | KNN Loss: 3.733217477798462 | BCE Loss: 1.0570824146270752\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.758863925933838 | KNN Loss: 3.7283904552459717 | BCE Loss: 1.0304735898971558\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.712499618530273 | KNN Loss: 3.7128708362579346 | BCE Loss: 0.9996286034584045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.735187530517578 | KNN Loss: 3.6942498683929443 | BCE Loss: 1.040937900543213\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.779535293579102 | KNN Loss: 3.7382898330688477 | BCE Loss: 1.041245698928833\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.699033737182617 | KNN Loss: 3.687662363052368 | BCE Loss: 1.0113712549209595\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.693668365478516 | KNN Loss: 3.6779465675354004 | BCE Loss: 1.0157215595245361\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.739509582519531 | KNN Loss: 3.710425615310669 | BCE Loss: 1.0290839672088623\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.727892875671387 | KNN Loss: 3.722181797027588 | BCE Loss: 1.0057108402252197\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.707941055297852 | KNN Loss: 3.680720567703247 | BCE Loss: 1.0272204875946045\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.722529888153076 | KNN Loss: 3.7007174491882324 | BCE Loss: 1.0218123197555542\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.715492248535156 | KNN Loss: 3.6975209712982178 | BCE Loss: 1.017971396446228\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.691281795501709 | KNN Loss: 3.686178207397461 | BCE Loss: 1.0051037073135376\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.716830253601074 | KNN Loss: 3.692519187927246 | BCE Loss: 1.024310827255249\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.7606306076049805 | KNN Loss: 3.6983087062835693 | BCE Loss: 1.0623221397399902\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.731312274932861 | KNN Loss: 3.7167961597442627 | BCE Loss: 1.0145161151885986\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.705309867858887 | KNN Loss: 3.6979482173919678 | BCE Loss: 1.007361650466919\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.733654499053955 | KNN Loss: 3.7173774242401123 | BCE Loss: 1.0162769556045532\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.748493194580078 | KNN Loss: 3.7209672927856445 | BCE Loss: 1.0275261402130127\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.727603912353516 | KNN Loss: 3.68955659866333 | BCE Loss: 1.0380473136901855\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.732509136199951 | KNN Loss: 3.6917073726654053 | BCE Loss: 1.0408016443252563\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.705596923828125 | KNN Loss: 3.7122199535369873 | BCE Loss: 0.9933767318725586\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.735307693481445 | KNN Loss: 3.69447660446167 | BCE Loss: 1.0408310890197754\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.711831569671631 | KNN Loss: 3.7008111476898193 | BCE Loss: 1.0110204219818115\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.7725019454956055 | KNN Loss: 3.717618465423584 | BCE Loss: 1.0548834800720215\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.700159549713135 | KNN Loss: 3.684032678604126 | BCE Loss: 1.0161268711090088\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.732918739318848 | KNN Loss: 3.722853899002075 | BCE Loss: 1.010064721107483\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.716444969177246 | KNN Loss: 3.693204402923584 | BCE Loss: 1.023240566253662\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.725566387176514 | KNN Loss: 3.6976516246795654 | BCE Loss: 1.0279148817062378\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.764181137084961 | KNN Loss: 3.715196371078491 | BCE Loss: 1.0489847660064697\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.705185890197754 | KNN Loss: 3.683472156524658 | BCE Loss: 1.0217137336730957\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.757648944854736 | KNN Loss: 3.7277419567108154 | BCE Loss: 1.029906988143921\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.712798118591309 | KNN Loss: 3.695526123046875 | BCE Loss: 1.017271876335144\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.720203399658203 | KNN Loss: 3.686164617538452 | BCE Loss: 1.03403902053833\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.733041286468506 | KNN Loss: 3.693822145462036 | BCE Loss: 1.0392191410064697\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.710738182067871 | KNN Loss: 3.6676909923553467 | BCE Loss: 1.0430474281311035\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.676644802093506 | KNN Loss: 3.6596763134002686 | BCE Loss: 1.0169686079025269\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.686390399932861 | KNN Loss: 3.676199197769165 | BCE Loss: 1.0101912021636963\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.708553314208984 | KNN Loss: 3.6865320205688477 | BCE Loss: 1.0220215320587158\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.736412048339844 | KNN Loss: 3.744232416152954 | BCE Loss: 0.9921795129776001\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.709301948547363 | KNN Loss: 3.680941104888916 | BCE Loss: 1.0283608436584473\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.727234840393066 | KNN Loss: 3.674471616744995 | BCE Loss: 1.0527631044387817\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.7630743980407715 | KNN Loss: 3.7049343585968018 | BCE Loss: 1.0581400394439697\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.697988986968994 | KNN Loss: 3.6840572357177734 | BCE Loss: 1.0139317512512207\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.761420249938965 | KNN Loss: 3.737362861633301 | BCE Loss: 1.0240576267242432\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.7459330558776855 | KNN Loss: 3.710847854614258 | BCE Loss: 1.0350852012634277\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.730820178985596 | KNN Loss: 3.708252191543579 | BCE Loss: 1.0225679874420166\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.721668720245361 | KNN Loss: 3.700221300125122 | BCE Loss: 1.0214475393295288\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.709528923034668 | KNN Loss: 3.675199508666992 | BCE Loss: 1.0343294143676758\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.683086395263672 | KNN Loss: 3.669170379638672 | BCE Loss: 1.013916015625\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.73738956451416 | KNN Loss: 3.705460548400879 | BCE Loss: 1.0319292545318604\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.732277870178223 | KNN Loss: 3.7030904293060303 | BCE Loss: 1.0291876792907715\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.678548812866211 | KNN Loss: 3.6902146339416504 | BCE Loss: 0.9883341193199158\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.708978652954102 | KNN Loss: 3.6816325187683105 | BCE Loss: 1.0273460149765015\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.755764961242676 | KNN Loss: 3.695596933364868 | BCE Loss: 1.0601682662963867\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.740387439727783 | KNN Loss: 3.713419198989868 | BCE Loss: 1.0269683599472046\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.677281856536865 | KNN Loss: 3.6744439601898193 | BCE Loss: 1.002837896347046\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.774524688720703 | KNN Loss: 3.715745449066162 | BCE Loss: 1.0587794780731201\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.724310398101807 | KNN Loss: 3.728039026260376 | BCE Loss: 0.9962711930274963\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.747586250305176 | KNN Loss: 3.7055423259735107 | BCE Loss: 1.042043685913086\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.698662757873535 | KNN Loss: 3.6540656089782715 | BCE Loss: 1.0445969104766846\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.72210693359375 | KNN Loss: 3.696895122528076 | BCE Loss: 1.0252118110656738\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.701692581176758 | KNN Loss: 3.695408821105957 | BCE Loss: 1.0062836408615112\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.7111053466796875 | KNN Loss: 3.6900551319122314 | BCE Loss: 1.021050214767456\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.711359024047852 | KNN Loss: 3.693186044692993 | BCE Loss: 1.018173098564148\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.7623796463012695 | KNN Loss: 3.7536439895629883 | BCE Loss: 1.0087355375289917\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.719788074493408 | KNN Loss: 3.6855742931365967 | BCE Loss: 1.0342137813568115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.675452709197998 | KNN Loss: 3.662527084350586 | BCE Loss: 1.012925624847412\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.748711109161377 | KNN Loss: 3.710463285446167 | BCE Loss: 1.03824782371521\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.781057357788086 | KNN Loss: 3.7316596508026123 | BCE Loss: 1.0493974685668945\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.707805633544922 | KNN Loss: 3.674666404724121 | BCE Loss: 1.0331394672393799\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.733466148376465 | KNN Loss: 3.7168054580688477 | BCE Loss: 1.0166606903076172\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.77614688873291 | KNN Loss: 3.7193779945373535 | BCE Loss: 1.0567686557769775\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.753278732299805 | KNN Loss: 3.6971356868743896 | BCE Loss: 1.056143045425415\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.717169284820557 | KNN Loss: 3.6845569610595703 | BCE Loss: 1.0326123237609863\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.718969345092773 | KNN Loss: 3.696303367614746 | BCE Loss: 1.0226658582687378\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.749822616577148 | KNN Loss: 3.739570140838623 | BCE Loss: 1.010252594947815\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.786495685577393 | KNN Loss: 3.752264976501465 | BCE Loss: 1.0342308282852173\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.738145351409912 | KNN Loss: 3.6980345249176025 | BCE Loss: 1.04011070728302\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.715919494628906 | KNN Loss: 3.7085072994232178 | BCE Loss: 1.0074119567871094\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.789697647094727 | KNN Loss: 3.7362656593322754 | BCE Loss: 1.0534322261810303\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.686561107635498 | KNN Loss: 3.6835179328918457 | BCE Loss: 1.003043293952942\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.723015785217285 | KNN Loss: 3.6836581230163574 | BCE Loss: 1.0393576622009277\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.729352951049805 | KNN Loss: 3.700666904449463 | BCE Loss: 1.028686285018921\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.75890588760376 | KNN Loss: 3.7202515602111816 | BCE Loss: 1.0386543273925781\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.7015767097473145 | KNN Loss: 3.6908016204833984 | BCE Loss: 1.0107752084732056\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.749692916870117 | KNN Loss: 3.7163097858428955 | BCE Loss: 1.0333832502365112\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.70941686630249 | KNN Loss: 3.6849164962768555 | BCE Loss: 1.0245002508163452\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.67831563949585 | KNN Loss: 3.649982452392578 | BCE Loss: 1.028333067893982\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.701556205749512 | KNN Loss: 3.681572437286377 | BCE Loss: 1.0199837684631348\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.711278915405273 | KNN Loss: 3.7148945331573486 | BCE Loss: 0.9963846206665039\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.687555313110352 | KNN Loss: 3.673222064971924 | BCE Loss: 1.0143334865570068\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.736857891082764 | KNN Loss: 3.721464157104492 | BCE Loss: 1.0153937339782715\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.756291389465332 | KNN Loss: 3.716134548187256 | BCE Loss: 1.040156602859497\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.78660774230957 | KNN Loss: 3.7430973052978516 | BCE Loss: 1.0435106754302979\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.712352752685547 | KNN Loss: 3.7109973430633545 | BCE Loss: 1.0013554096221924\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.7638468742370605 | KNN Loss: 3.7522048950195312 | BCE Loss: 1.0116419792175293\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.716296195983887 | KNN Loss: 3.685424327850342 | BCE Loss: 1.030872106552124\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.738762855529785 | KNN Loss: 3.707249641418457 | BCE Loss: 1.031512975692749\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.722101211547852 | KNN Loss: 3.7087206840515137 | BCE Loss: 1.013380765914917\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.703958511352539 | KNN Loss: 3.6990644931793213 | BCE Loss: 1.0048937797546387\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.741979122161865 | KNN Loss: 3.720914363861084 | BCE Loss: 1.0210646390914917\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.765757083892822 | KNN Loss: 3.739591121673584 | BCE Loss: 1.0261659622192383\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.788238525390625 | KNN Loss: 3.7396392822265625 | BCE Loss: 1.0485992431640625\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.726777076721191 | KNN Loss: 3.684962034225464 | BCE Loss: 1.0418150424957275\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.720884323120117 | KNN Loss: 3.686866044998169 | BCE Loss: 1.0340185165405273\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.715080261230469 | KNN Loss: 3.6877846717834473 | BCE Loss: 1.027295470237732\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.66697359085083 | KNN Loss: 3.666199207305908 | BCE Loss: 1.0007742643356323\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.709671497344971 | KNN Loss: 3.668813705444336 | BCE Loss: 1.0408576726913452\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.738775730133057 | KNN Loss: 3.725924015045166 | BCE Loss: 1.0128517150878906\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.729836463928223 | KNN Loss: 3.6991958618164062 | BCE Loss: 1.0306406021118164\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.758418083190918 | KNN Loss: 3.726439952850342 | BCE Loss: 1.0319781303405762\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.758440017700195 | KNN Loss: 3.73759388923645 | BCE Loss: 1.0208463668823242\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.781979560852051 | KNN Loss: 3.730412721633911 | BCE Loss: 1.0515668392181396\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.729196548461914 | KNN Loss: 3.701296806335449 | BCE Loss: 1.0278995037078857\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.713407039642334 | KNN Loss: 3.7016055583953857 | BCE Loss: 1.0118016004562378\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.7598066329956055 | KNN Loss: 3.7258315086364746 | BCE Loss: 1.0339748859405518\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.713578224182129 | KNN Loss: 3.6917784214019775 | BCE Loss: 1.0217996835708618\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.7202606201171875 | KNN Loss: 3.699516773223877 | BCE Loss: 1.0207439661026\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.7403669357299805 | KNN Loss: 3.714143753051758 | BCE Loss: 1.0262234210968018\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.658011436462402 | KNN Loss: 3.648160696029663 | BCE Loss: 1.0098507404327393\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.737648010253906 | KNN Loss: 3.688791513442993 | BCE Loss: 1.048856258392334\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.732473850250244 | KNN Loss: 3.7222564220428467 | BCE Loss: 1.010217547416687\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.800961017608643 | KNN Loss: 3.7444264888763428 | BCE Loss: 1.0565344095230103\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.741293907165527 | KNN Loss: 3.70426869392395 | BCE Loss: 1.0370252132415771\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.70927095413208 | KNN Loss: 3.669381856918335 | BCE Loss: 1.0398890972137451\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.6856913566589355 | KNN Loss: 3.6986865997314453 | BCE Loss: 0.9870047569274902\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.680459976196289 | KNN Loss: 3.6815075874328613 | BCE Loss: 0.9989525675773621\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.770964622497559 | KNN Loss: 3.752852439880371 | BCE Loss: 1.018112301826477\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.723743438720703 | KNN Loss: 3.7030868530273438 | BCE Loss: 1.0206565856933594\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.716290473937988 | KNN Loss: 3.685953378677368 | BCE Loss: 1.030336856842041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.736207485198975 | KNN Loss: 3.7151901721954346 | BCE Loss: 1.02101731300354\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.728896141052246 | KNN Loss: 3.708296060562134 | BCE Loss: 1.0206003189086914\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.690422058105469 | KNN Loss: 3.67264723777771 | BCE Loss: 1.0177748203277588\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.715219497680664 | KNN Loss: 3.70550537109375 | BCE Loss: 1.009713888168335\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.674323081970215 | KNN Loss: 3.682277202606201 | BCE Loss: 0.9920459389686584\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.7396674156188965 | KNN Loss: 3.706331491470337 | BCE Loss: 1.0333359241485596\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.705522060394287 | KNN Loss: 3.671377182006836 | BCE Loss: 1.0341448783874512\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.704563140869141 | KNN Loss: 3.6625020503997803 | BCE Loss: 1.0420610904693604\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.694877624511719 | KNN Loss: 3.684265613555908 | BCE Loss: 1.0106117725372314\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.7668023109436035 | KNN Loss: 3.7009167671203613 | BCE Loss: 1.0658855438232422\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.710743427276611 | KNN Loss: 3.708008289337158 | BCE Loss: 1.0027351379394531\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.71262264251709 | KNN Loss: 3.7230186462402344 | BCE Loss: 0.9896038770675659\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.724058151245117 | KNN Loss: 3.681427240371704 | BCE Loss: 1.042630910873413\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.695987701416016 | KNN Loss: 3.6956512928009033 | BCE Loss: 1.0003361701965332\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.766819000244141 | KNN Loss: 3.707709789276123 | BCE Loss: 1.059109091758728\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.71523380279541 | KNN Loss: 3.7086079120635986 | BCE Loss: 1.006625771522522\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.740180015563965 | KNN Loss: 3.6884851455688477 | BCE Loss: 1.0516951084136963\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.724549293518066 | KNN Loss: 3.6973860263824463 | BCE Loss: 1.0271632671356201\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.721599102020264 | KNN Loss: 3.708453416824341 | BCE Loss: 1.0131455659866333\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.703066825866699 | KNN Loss: 3.7027132511138916 | BCE Loss: 1.0003533363342285\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.700258731842041 | KNN Loss: 3.6570334434509277 | BCE Loss: 1.0432252883911133\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.738681793212891 | KNN Loss: 3.70483136177063 | BCE Loss: 1.0338506698608398\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.748852729797363 | KNN Loss: 3.7073538303375244 | BCE Loss: 1.0414986610412598\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.704438209533691 | KNN Loss: 3.6848111152648926 | BCE Loss: 1.0196272134780884\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.722278118133545 | KNN Loss: 3.7144250869750977 | BCE Loss: 1.0078530311584473\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.718664169311523 | KNN Loss: 3.692288875579834 | BCE Loss: 1.0263750553131104\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.725558757781982 | KNN Loss: 3.693340539932251 | BCE Loss: 1.032218337059021\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.742999076843262 | KNN Loss: 3.7351059913635254 | BCE Loss: 1.0078928470611572\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.740027904510498 | KNN Loss: 3.6869406700134277 | BCE Loss: 1.0530872344970703\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.726380825042725 | KNN Loss: 3.69109845161438 | BCE Loss: 1.0352824926376343\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.698657989501953 | KNN Loss: 3.6690526008605957 | BCE Loss: 1.0296056270599365\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.695779323577881 | KNN Loss: 3.6766860485076904 | BCE Loss: 1.01909339427948\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.709805011749268 | KNN Loss: 3.6789846420288086 | BCE Loss: 1.030820369720459\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.727297306060791 | KNN Loss: 3.698005199432373 | BCE Loss: 1.0292922258377075\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.749757289886475 | KNN Loss: 3.71913480758667 | BCE Loss: 1.0306224822998047\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.701992988586426 | KNN Loss: 3.6749556064605713 | BCE Loss: 1.027037501335144\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.714267253875732 | KNN Loss: 3.688539981842041 | BCE Loss: 1.0257271528244019\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.73000955581665 | KNN Loss: 3.6790084838867188 | BCE Loss: 1.0510010719299316\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.73475456237793 | KNN Loss: 3.720262050628662 | BCE Loss: 1.0144925117492676\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.738194465637207 | KNN Loss: 3.683607339859009 | BCE Loss: 1.0545871257781982\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.729358673095703 | KNN Loss: 3.7079482078552246 | BCE Loss: 1.0214107036590576\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.745842933654785 | KNN Loss: 3.704537868499756 | BCE Loss: 1.0413048267364502\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.724127769470215 | KNN Loss: 3.713656425476074 | BCE Loss: 1.0104713439941406\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.735151767730713 | KNN Loss: 3.7255425453186035 | BCE Loss: 1.009609341621399\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.705646514892578 | KNN Loss: 3.691699266433716 | BCE Loss: 1.0139472484588623\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.7603254318237305 | KNN Loss: 3.7182774543762207 | BCE Loss: 1.0420477390289307\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.779146194458008 | KNN Loss: 3.7364323139190674 | BCE Loss: 1.0427138805389404\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.778506278991699 | KNN Loss: 3.7530901432037354 | BCE Loss: 1.0254158973693848\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.7129740715026855 | KNN Loss: 3.6903200149536133 | BCE Loss: 1.0226540565490723\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.708043098449707 | KNN Loss: 3.6802029609680176 | BCE Loss: 1.0278403759002686\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.688645839691162 | KNN Loss: 3.6848182678222656 | BCE Loss: 1.0038275718688965\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.730602741241455 | KNN Loss: 3.708740711212158 | BCE Loss: 1.0218620300292969\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.697916030883789 | KNN Loss: 3.6825830936431885 | BCE Loss: 1.0153331756591797\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.72081995010376 | KNN Loss: 3.6946511268615723 | BCE Loss: 1.026168704032898\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.750575542449951 | KNN Loss: 3.6934609413146973 | BCE Loss: 1.0571147203445435\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.7262725830078125 | KNN Loss: 3.7432479858398438 | BCE Loss: 0.9830247163772583\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.696181297302246 | KNN Loss: 3.6739883422851562 | BCE Loss: 1.0221927165985107\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.782520294189453 | KNN Loss: 3.7149813175201416 | BCE Loss: 1.0675389766693115\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.73594856262207 | KNN Loss: 3.7130281925201416 | BCE Loss: 1.0229206085205078\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.721800327301025 | KNN Loss: 3.7300610542297363 | BCE Loss: 0.9917393326759338\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.674753665924072 | KNN Loss: 3.6648006439208984 | BCE Loss: 1.0099530220031738\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.704781532287598 | KNN Loss: 3.6728157997131348 | BCE Loss: 1.031965732574463\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.7140793800354 | KNN Loss: 3.6782100200653076 | BCE Loss: 1.0358694791793823\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.732824802398682 | KNN Loss: 3.7115044593811035 | BCE Loss: 1.0213202238082886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.725688457489014 | KNN Loss: 3.680338144302368 | BCE Loss: 1.0453503131866455\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.719351768493652 | KNN Loss: 3.665105104446411 | BCE Loss: 1.0542466640472412\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.748157978057861 | KNN Loss: 3.710408926010132 | BCE Loss: 1.0377490520477295\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.717916488647461 | KNN Loss: 3.6924781799316406 | BCE Loss: 1.0254383087158203\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.733489513397217 | KNN Loss: 3.7075681686401367 | BCE Loss: 1.0259212255477905\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.739500045776367 | KNN Loss: 3.6978673934936523 | BCE Loss: 1.0416326522827148\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.742783546447754 | KNN Loss: 3.6902246475219727 | BCE Loss: 1.0525590181350708\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.714107036590576 | KNN Loss: 3.6824774742126465 | BCE Loss: 1.0316295623779297\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.705515384674072 | KNN Loss: 3.7047903537750244 | BCE Loss: 1.0007250308990479\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.794427871704102 | KNN Loss: 3.7328150272369385 | BCE Loss: 1.061612844467163\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.7240166664123535 | KNN Loss: 3.704521894454956 | BCE Loss: 1.0194947719573975\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.778970718383789 | KNN Loss: 3.731968879699707 | BCE Loss: 1.047001838684082\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.753848075866699 | KNN Loss: 3.7225804328918457 | BCE Loss: 1.0312678813934326\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.729299545288086 | KNN Loss: 3.706981658935547 | BCE Loss: 1.0223177671432495\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.7845377922058105 | KNN Loss: 3.7638039588928223 | BCE Loss: 1.0207338333129883\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.706732273101807 | KNN Loss: 3.7086222171783447 | BCE Loss: 0.9981101751327515\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.698793888092041 | KNN Loss: 3.691805124282837 | BCE Loss: 1.0069888830184937\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.740988731384277 | KNN Loss: 3.6966323852539062 | BCE Loss: 1.044356107711792\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.690540313720703 | KNN Loss: 3.677029848098755 | BCE Loss: 1.0135107040405273\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.721696853637695 | KNN Loss: 3.700456380844116 | BCE Loss: 1.021240472793579\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.680284023284912 | KNN Loss: 3.663787364959717 | BCE Loss: 1.0164965391159058\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.723967552185059 | KNN Loss: 3.711101770401001 | BCE Loss: 1.0128660202026367\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.694097518920898 | KNN Loss: 3.6591548919677734 | BCE Loss: 1.0349427461624146\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.755997180938721 | KNN Loss: 3.716205358505249 | BCE Loss: 1.0397919416427612\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.706398010253906 | KNN Loss: 3.6773874759674072 | BCE Loss: 1.02901029586792\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.70751953125 | KNN Loss: 3.697262763977051 | BCE Loss: 1.0102570056915283\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.706707000732422 | KNN Loss: 3.6806960105895996 | BCE Loss: 1.0260112285614014\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.740745544433594 | KNN Loss: 3.7380340099334717 | BCE Loss: 1.002711296081543\n",
      "Epoch   155: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.725140571594238 | KNN Loss: 3.7079343795776367 | BCE Loss: 1.0172063112258911\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.726810455322266 | KNN Loss: 3.699158191680908 | BCE Loss: 1.0276520252227783\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.706686019897461 | KNN Loss: 3.6967389583587646 | BCE Loss: 1.0099468231201172\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.7136125564575195 | KNN Loss: 3.7073476314544678 | BCE Loss: 1.0062651634216309\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.6597580909729 | KNN Loss: 3.651362657546997 | BCE Loss: 1.0083954334259033\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.74681282043457 | KNN Loss: 3.701019287109375 | BCE Loss: 1.0457935333251953\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.734704971313477 | KNN Loss: 3.680911064147949 | BCE Loss: 1.053794026374817\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.7488017082214355 | KNN Loss: 3.6980419158935547 | BCE Loss: 1.0507597923278809\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.727190017700195 | KNN Loss: 3.6879513263702393 | BCE Loss: 1.039238691329956\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.711310386657715 | KNN Loss: 3.719571352005005 | BCE Loss: 0.9917389154434204\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.700907230377197 | KNN Loss: 3.6997547149658203 | BCE Loss: 1.001152515411377\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.762245178222656 | KNN Loss: 3.725116491317749 | BCE Loss: 1.0371289253234863\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.698200225830078 | KNN Loss: 3.680091142654419 | BCE Loss: 1.0181092023849487\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.732484340667725 | KNN Loss: 3.722865104675293 | BCE Loss: 1.0096193552017212\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.699064254760742 | KNN Loss: 3.6830809116363525 | BCE Loss: 1.0159835815429688\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.711051940917969 | KNN Loss: 3.6665890216827393 | BCE Loss: 1.04446280002594\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.710864067077637 | KNN Loss: 3.6782777309417725 | BCE Loss: 1.0325860977172852\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.716314315795898 | KNN Loss: 3.69201397895813 | BCE Loss: 1.024300456047058\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.741866111755371 | KNN Loss: 3.7087650299072266 | BCE Loss: 1.0331010818481445\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.769076824188232 | KNN Loss: 3.7222938537597656 | BCE Loss: 1.0467828512191772\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.724943161010742 | KNN Loss: 3.6885247230529785 | BCE Loss: 1.0364186763763428\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.76087760925293 | KNN Loss: 3.7280609607696533 | BCE Loss: 1.0328164100646973\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.733717918395996 | KNN Loss: 3.696051836013794 | BCE Loss: 1.0376660823822021\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.766138076782227 | KNN Loss: 3.7359507083892822 | BCE Loss: 1.0301874876022339\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.711358070373535 | KNN Loss: 3.6973888874053955 | BCE Loss: 1.0139689445495605\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.721076488494873 | KNN Loss: 3.701038122177124 | BCE Loss: 1.0200382471084595\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.68976354598999 | KNN Loss: 3.6738648414611816 | BCE Loss: 1.0158987045288086\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.783969879150391 | KNN Loss: 3.7381198406219482 | BCE Loss: 1.045850157737732\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.6826934814453125 | KNN Loss: 3.6651062965393066 | BCE Loss: 1.0175873041152954\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.707486152648926 | KNN Loss: 3.6969494819641113 | BCE Loss: 1.0105364322662354\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.682709217071533 | KNN Loss: 3.671400308609009 | BCE Loss: 1.0113089084625244\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.705103874206543 | KNN Loss: 3.6836705207824707 | BCE Loss: 1.0214331150054932\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.693584442138672 | KNN Loss: 3.6675302982330322 | BCE Loss: 1.0260539054870605\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.736865997314453 | KNN Loss: 3.6876473426818848 | BCE Loss: 1.0492184162139893\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.720085144042969 | KNN Loss: 3.6894097328186035 | BCE Loss: 1.0306756496429443\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.722993850708008 | KNN Loss: 3.7064478397369385 | BCE Loss: 1.0165457725524902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.769018173217773 | KNN Loss: 3.7405524253845215 | BCE Loss: 1.028465986251831\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.781330585479736 | KNN Loss: 3.7522664070129395 | BCE Loss: 1.0290642976760864\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.72381591796875 | KNN Loss: 3.6829776763916016 | BCE Loss: 1.040838360786438\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.712642192840576 | KNN Loss: 3.7055134773254395 | BCE Loss: 1.0071285963058472\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.76686429977417 | KNN Loss: 3.756225824356079 | BCE Loss: 1.0106384754180908\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.7425150871276855 | KNN Loss: 3.6990272998809814 | BCE Loss: 1.0434879064559937\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.705746650695801 | KNN Loss: 3.657306671142578 | BCE Loss: 1.0484399795532227\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.700399875640869 | KNN Loss: 3.6840932369232178 | BCE Loss: 1.016306757926941\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.718623161315918 | KNN Loss: 3.6828293800354004 | BCE Loss: 1.0357935428619385\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.668057441711426 | KNN Loss: 3.6716270446777344 | BCE Loss: 0.9964303970336914\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.689464092254639 | KNN Loss: 3.6924755573272705 | BCE Loss: 0.9969885349273682\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.73012113571167 | KNN Loss: 3.6877076625823975 | BCE Loss: 1.042413353919983\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.707531452178955 | KNN Loss: 3.6925864219665527 | BCE Loss: 1.0149449110031128\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.713555335998535 | KNN Loss: 3.703054428100586 | BCE Loss: 1.0105011463165283\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.723684310913086 | KNN Loss: 3.674177885055542 | BCE Loss: 1.0495065450668335\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.722746849060059 | KNN Loss: 3.6813690662384033 | BCE Loss: 1.0413780212402344\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.729126930236816 | KNN Loss: 3.685943365097046 | BCE Loss: 1.0431835651397705\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.7018585205078125 | KNN Loss: 3.6904163360595703 | BCE Loss: 1.0114421844482422\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.720516204833984 | KNN Loss: 3.6839113235473633 | BCE Loss: 1.036604881286621\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.720182418823242 | KNN Loss: 3.711427688598633 | BCE Loss: 1.008754849433899\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.7160797119140625 | KNN Loss: 3.6644396781921387 | BCE Loss: 1.0516400337219238\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.712467193603516 | KNN Loss: 3.7098186016082764 | BCE Loss: 1.0026483535766602\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.721474647521973 | KNN Loss: 3.6952624320983887 | BCE Loss: 1.026212215423584\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.734515190124512 | KNN Loss: 3.7157135009765625 | BCE Loss: 1.0188018083572388\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.700846195220947 | KNN Loss: 3.677938461303711 | BCE Loss: 1.0229076147079468\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.727668285369873 | KNN Loss: 3.681323528289795 | BCE Loss: 1.0463447570800781\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.721583366394043 | KNN Loss: 3.6978039741516113 | BCE Loss: 1.0237796306610107\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.708703517913818 | KNN Loss: 3.6863715648651123 | BCE Loss: 1.022331953048706\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.6976237297058105 | KNN Loss: 3.6798999309539795 | BCE Loss: 1.0177236795425415\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.7320942878723145 | KNN Loss: 3.7185583114624023 | BCE Loss: 1.013535976409912\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.729279518127441 | KNN Loss: 3.7060978412628174 | BCE Loss: 1.0231817960739136\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.708156108856201 | KNN Loss: 3.677797555923462 | BCE Loss: 1.0303585529327393\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.706172466278076 | KNN Loss: 3.6968743801116943 | BCE Loss: 1.0092980861663818\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.70884895324707 | KNN Loss: 3.67269229888916 | BCE Loss: 1.0361565351486206\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.708849906921387 | KNN Loss: 3.6902642250061035 | BCE Loss: 1.0185858011245728\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.728755474090576 | KNN Loss: 3.692477226257324 | BCE Loss: 1.036278247833252\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.681571960449219 | KNN Loss: 3.658174514770508 | BCE Loss: 1.023397445678711\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.6982316970825195 | KNN Loss: 3.6858646869659424 | BCE Loss: 1.0123668909072876\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.7168474197387695 | KNN Loss: 3.6929187774658203 | BCE Loss: 1.0239284038543701\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.748108863830566 | KNN Loss: 3.7253127098083496 | BCE Loss: 1.022796392440796\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.669761657714844 | KNN Loss: 3.6718952655792236 | BCE Loss: 0.9978666305541992\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.723265171051025 | KNN Loss: 3.7112514972686768 | BCE Loss: 1.0120137929916382\n",
      "Epoch   168: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.696269512176514 | KNN Loss: 3.670062303543091 | BCE Loss: 1.0262072086334229\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.743139266967773 | KNN Loss: 3.7254481315612793 | BCE Loss: 1.0176913738250732\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.701535701751709 | KNN Loss: 3.6759939193725586 | BCE Loss: 1.0255417823791504\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.730889320373535 | KNN Loss: 3.6762328147888184 | BCE Loss: 1.054656744003296\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.770730018615723 | KNN Loss: 3.7296788692474365 | BCE Loss: 1.041050910949707\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.686398029327393 | KNN Loss: 3.6673059463500977 | BCE Loss: 1.019092082977295\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.682734489440918 | KNN Loss: 3.679263114929199 | BCE Loss: 1.0034711360931396\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.754181861877441 | KNN Loss: 3.714733839035034 | BCE Loss: 1.0394482612609863\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.6847686767578125 | KNN Loss: 3.674838066101074 | BCE Loss: 1.0099303722381592\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.692711353302002 | KNN Loss: 3.6642558574676514 | BCE Loss: 1.0284554958343506\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.695301055908203 | KNN Loss: 3.6835336685180664 | BCE Loss: 1.0117673873901367\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.716082572937012 | KNN Loss: 3.7060160636901855 | BCE Loss: 1.0100667476654053\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.7418718338012695 | KNN Loss: 3.7182974815368652 | BCE Loss: 1.0235745906829834\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.705376625061035 | KNN Loss: 3.672321081161499 | BCE Loss: 1.033055305480957\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.705429553985596 | KNN Loss: 3.685242176055908 | BCE Loss: 1.020187258720398\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.753817558288574 | KNN Loss: 3.7206926345825195 | BCE Loss: 1.0331246852874756\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.762034893035889 | KNN Loss: 3.708287477493286 | BCE Loss: 1.053747296333313\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.700689315795898 | KNN Loss: 3.6885204315185547 | BCE Loss: 1.0121691226959229\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.730196952819824 | KNN Loss: 3.70953106880188 | BCE Loss: 1.0206656455993652\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.714105606079102 | KNN Loss: 3.6935088634490967 | BCE Loss: 1.0205967426300049\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.692677021026611 | KNN Loss: 3.6958255767822266 | BCE Loss: 0.9968514442443848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.714395999908447 | KNN Loss: 3.689777135848999 | BCE Loss: 1.0246188640594482\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.806906223297119 | KNN Loss: 3.7575912475585938 | BCE Loss: 1.0493149757385254\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.789076328277588 | KNN Loss: 3.7383737564086914 | BCE Loss: 1.0507025718688965\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.670455455780029 | KNN Loss: 3.677607297897339 | BCE Loss: 0.9928482174873352\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.700645446777344 | KNN Loss: 3.672260284423828 | BCE Loss: 1.0283854007720947\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.76609992980957 | KNN Loss: 3.7262001037597656 | BCE Loss: 1.0398995876312256\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.716073036193848 | KNN Loss: 3.688462495803833 | BCE Loss: 1.0276105403900146\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.71012020111084 | KNN Loss: 3.688098907470703 | BCE Loss: 1.0220215320587158\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.691267490386963 | KNN Loss: 3.6955647468566895 | BCE Loss: 0.9957026243209839\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.689486980438232 | KNN Loss: 3.672807455062866 | BCE Loss: 1.0166796445846558\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.720866680145264 | KNN Loss: 3.7166786193847656 | BCE Loss: 1.004188060760498\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.701574325561523 | KNN Loss: 3.6798312664031982 | BCE Loss: 1.0217432975769043\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.694222450256348 | KNN Loss: 3.6767003536224365 | BCE Loss: 1.0175220966339111\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.721037864685059 | KNN Loss: 3.667631149291992 | BCE Loss: 1.053406834602356\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.7253947257995605 | KNN Loss: 3.7070765495300293 | BCE Loss: 1.0183182954788208\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.721495628356934 | KNN Loss: 3.7105183601379395 | BCE Loss: 1.0109772682189941\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.696784973144531 | KNN Loss: 3.6672160625457764 | BCE Loss: 1.0295686721801758\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.716200828552246 | KNN Loss: 3.6838903427124023 | BCE Loss: 1.0323107242584229\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.7426300048828125 | KNN Loss: 3.6947522163391113 | BCE Loss: 1.0478779077529907\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.72532844543457 | KNN Loss: 3.679755449295044 | BCE Loss: 1.0455727577209473\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.750994682312012 | KNN Loss: 3.6901869773864746 | BCE Loss: 1.060807466506958\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.6984076499938965 | KNN Loss: 3.6683311462402344 | BCE Loss: 1.0300763845443726\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.714752197265625 | KNN Loss: 3.710824728012085 | BCE Loss: 1.0039275884628296\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.731956481933594 | KNN Loss: 3.7123353481292725 | BCE Loss: 1.0196208953857422\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.685022354125977 | KNN Loss: 3.6681485176086426 | BCE Loss: 1.016874074935913\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.769476890563965 | KNN Loss: 3.755969524383545 | BCE Loss: 1.0135071277618408\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.708042621612549 | KNN Loss: 3.6727638244628906 | BCE Loss: 1.0352789163589478\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.735912799835205 | KNN Loss: 3.708223819732666 | BCE Loss: 1.027688980102539\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.688492298126221 | KNN Loss: 3.691351890563965 | BCE Loss: 0.9971402287483215\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.7233686447143555 | KNN Loss: 3.7133800983428955 | BCE Loss: 1.00998854637146\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.747845649719238 | KNN Loss: 3.71040415763855 | BCE Loss: 1.0374412536621094\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.715814590454102 | KNN Loss: 3.6770689487457275 | BCE Loss: 1.038745403289795\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.708599090576172 | KNN Loss: 3.685222625732422 | BCE Loss: 1.0233765840530396\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.704245567321777 | KNN Loss: 3.6698107719421387 | BCE Loss: 1.0344350337982178\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.700376510620117 | KNN Loss: 3.6630141735076904 | BCE Loss: 1.0373620986938477\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.716725826263428 | KNN Loss: 3.6862926483154297 | BCE Loss: 1.0304330587387085\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.700101375579834 | KNN Loss: 3.678323268890381 | BCE Loss: 1.0217781066894531\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.756954193115234 | KNN Loss: 3.737210988998413 | BCE Loss: 1.0197432041168213\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.701416015625 | KNN Loss: 3.683764696121216 | BCE Loss: 1.0176515579223633\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.724205017089844 | KNN Loss: 3.6917850971221924 | BCE Loss: 1.0324201583862305\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.7175445556640625 | KNN Loss: 3.7030129432678223 | BCE Loss: 1.0145317316055298\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.733743667602539 | KNN Loss: 3.7229440212249756 | BCE Loss: 1.0107994079589844\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.700337886810303 | KNN Loss: 3.6669833660125732 | BCE Loss: 1.0333545207977295\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.743908405303955 | KNN Loss: 3.7314391136169434 | BCE Loss: 1.0124691724777222\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.736910343170166 | KNN Loss: 3.7162282466888428 | BCE Loss: 1.0206820964813232\n",
      "Epoch   179: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.65944766998291 | KNN Loss: 3.6531572341918945 | BCE Loss: 1.0062905550003052\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.736126899719238 | KNN Loss: 3.6891062259674072 | BCE Loss: 1.0470209121704102\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.689801216125488 | KNN Loss: 3.6704912185668945 | BCE Loss: 1.0193102359771729\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.6848344802856445 | KNN Loss: 3.660893678665161 | BCE Loss: 1.0239405632019043\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.706705093383789 | KNN Loss: 3.6847381591796875 | BCE Loss: 1.0219670534133911\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.725095748901367 | KNN Loss: 3.710843324661255 | BCE Loss: 1.0142524242401123\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.687917709350586 | KNN Loss: 3.6603124141693115 | BCE Loss: 1.0276052951812744\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.73722505569458 | KNN Loss: 3.7319014072418213 | BCE Loss: 1.0053236484527588\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.736299991607666 | KNN Loss: 3.7123706340789795 | BCE Loss: 1.0239293575286865\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.715254783630371 | KNN Loss: 3.692336082458496 | BCE Loss: 1.022918701171875\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.679602146148682 | KNN Loss: 3.644944906234741 | BCE Loss: 1.0346572399139404\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.722269058227539 | KNN Loss: 3.6870245933532715 | BCE Loss: 1.0352442264556885\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.689253330230713 | KNN Loss: 3.6635444164276123 | BCE Loss: 1.0257089138031006\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.6775312423706055 | KNN Loss: 3.6591134071350098 | BCE Loss: 1.0184178352355957\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.7417144775390625 | KNN Loss: 3.7238590717315674 | BCE Loss: 1.0178556442260742\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.669742584228516 | KNN Loss: 3.6644654273986816 | BCE Loss: 1.0052769184112549\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.751343727111816 | KNN Loss: 3.718928098678589 | BCE Loss: 1.0324158668518066\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.786281108856201 | KNN Loss: 3.7210922241210938 | BCE Loss: 1.0651888847351074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.739169120788574 | KNN Loss: 3.6828365325927734 | BCE Loss: 1.0563325881958008\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.734935760498047 | KNN Loss: 3.699075698852539 | BCE Loss: 1.0358598232269287\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.703526020050049 | KNN Loss: 3.672065019607544 | BCE Loss: 1.0314610004425049\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.665281295776367 | KNN Loss: 3.646109104156494 | BCE Loss: 1.019171953201294\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.724977970123291 | KNN Loss: 3.6787548065185547 | BCE Loss: 1.0462230443954468\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.695399284362793 | KNN Loss: 3.685948371887207 | BCE Loss: 1.0094510316848755\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.718674659729004 | KNN Loss: 3.6991236209869385 | BCE Loss: 1.0195512771606445\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.743906021118164 | KNN Loss: 3.715883493423462 | BCE Loss: 1.0280225276947021\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.730936527252197 | KNN Loss: 3.72735857963562 | BCE Loss: 1.0035780668258667\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.757972717285156 | KNN Loss: 3.717681646347046 | BCE Loss: 1.0402908325195312\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.77269172668457 | KNN Loss: 3.7111423015594482 | BCE Loss: 1.061549425125122\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.712185859680176 | KNN Loss: 3.6941397190093994 | BCE Loss: 1.0180461406707764\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.739250183105469 | KNN Loss: 3.7040560245513916 | BCE Loss: 1.035193920135498\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.808304309844971 | KNN Loss: 3.753457546234131 | BCE Loss: 1.0548468828201294\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.732479095458984 | KNN Loss: 3.716827154159546 | BCE Loss: 1.015651822090149\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.6885986328125 | KNN Loss: 3.6625356674194336 | BCE Loss: 1.0260627269744873\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.717050552368164 | KNN Loss: 3.698970317840576 | BCE Loss: 1.018080472946167\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.726202487945557 | KNN Loss: 3.6939430236816406 | BCE Loss: 1.0322595834732056\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.769132614135742 | KNN Loss: 3.723872184753418 | BCE Loss: 1.0452604293823242\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.718348503112793 | KNN Loss: 3.6989357471466064 | BCE Loss: 1.0194129943847656\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.6955060958862305 | KNN Loss: 3.6780457496643066 | BCE Loss: 1.0174601078033447\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.711648464202881 | KNN Loss: 3.6786811351776123 | BCE Loss: 1.0329673290252686\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.760410785675049 | KNN Loss: 3.7186756134033203 | BCE Loss: 1.041735053062439\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.726839065551758 | KNN Loss: 3.7002179622650146 | BCE Loss: 1.026620864868164\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.719427108764648 | KNN Loss: 3.693683385848999 | BCE Loss: 1.0257436037063599\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.759134292602539 | KNN Loss: 3.6929237842559814 | BCE Loss: 1.0662106275558472\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.7532525062561035 | KNN Loss: 3.6882896423339844 | BCE Loss: 1.0649628639221191\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.716705322265625 | KNN Loss: 3.6581428050994873 | BCE Loss: 1.0585622787475586\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.705319404602051 | KNN Loss: 3.7033746242523193 | BCE Loss: 1.0019445419311523\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.724671363830566 | KNN Loss: 3.7151870727539062 | BCE Loss: 1.0094845294952393\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.747042655944824 | KNN Loss: 3.7031636238098145 | BCE Loss: 1.0438792705535889\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.7186994552612305 | KNN Loss: 3.6994636058807373 | BCE Loss: 1.019235610961914\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.748543739318848 | KNN Loss: 3.71600604057312 | BCE Loss: 1.032537579536438\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.713451385498047 | KNN Loss: 3.6755361557006836 | BCE Loss: 1.0379151105880737\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.694146156311035 | KNN Loss: 3.6740875244140625 | BCE Loss: 1.0200588703155518\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.734618186950684 | KNN Loss: 3.7325799465179443 | BCE Loss: 1.0020380020141602\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.749256134033203 | KNN Loss: 3.738729953765869 | BCE Loss: 1.010526180267334\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.732855796813965 | KNN Loss: 3.6968464851379395 | BCE Loss: 1.036009430885315\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.714406967163086 | KNN Loss: 3.684140682220459 | BCE Loss: 1.030266523361206\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.711079120635986 | KNN Loss: 3.674753189086914 | BCE Loss: 1.0363259315490723\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.687859535217285 | KNN Loss: 3.671391487121582 | BCE Loss: 1.016467809677124\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.733326435089111 | KNN Loss: 3.704761266708374 | BCE Loss: 1.0285651683807373\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.6892242431640625 | KNN Loss: 3.676985740661621 | BCE Loss: 1.0122382640838623\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.735200881958008 | KNN Loss: 3.7087795734405518 | BCE Loss: 1.0264215469360352\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.700077056884766 | KNN Loss: 3.6850833892822266 | BCE Loss: 1.014993667602539\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.732423782348633 | KNN Loss: 3.7377264499664307 | BCE Loss: 0.9946972131729126\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.698517799377441 | KNN Loss: 3.6881439685821533 | BCE Loss: 1.0103737115859985\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.750097751617432 | KNN Loss: 3.692718982696533 | BCE Loss: 1.0573786497116089\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.748302459716797 | KNN Loss: 3.710082530975342 | BCE Loss: 1.038219928741455\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.697268486022949 | KNN Loss: 3.692950487136841 | BCE Loss: 1.0043177604675293\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.700869560241699 | KNN Loss: 3.663419485092163 | BCE Loss: 1.037449836730957\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.72178840637207 | KNN Loss: 3.69748592376709 | BCE Loss: 1.0243022441864014\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.726971626281738 | KNN Loss: 3.6732606887817383 | BCE Loss: 1.0537109375\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.683067798614502 | KNN Loss: 3.6788508892059326 | BCE Loss: 1.0042170286178589\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.703958511352539 | KNN Loss: 3.6827640533447266 | BCE Loss: 1.0211946964263916\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.699818134307861 | KNN Loss: 3.6937639713287354 | BCE Loss: 1.0060542821884155\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.694486141204834 | KNN Loss: 3.6838550567626953 | BCE Loss: 1.0106309652328491\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.6971964836120605 | KNN Loss: 3.693044424057007 | BCE Loss: 1.0041520595550537\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.687455177307129 | KNN Loss: 3.682121515274048 | BCE Loss: 1.0053339004516602\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.693199634552002 | KNN Loss: 3.6489169597625732 | BCE Loss: 1.0442826747894287\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.7006449699401855 | KNN Loss: 3.684206008911133 | BCE Loss: 1.0164389610290527\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.714912414550781 | KNN Loss: 3.706178903579712 | BCE Loss: 1.0087337493896484\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.728603363037109 | KNN Loss: 3.7094168663024902 | BCE Loss: 1.0191866159439087\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.7163166999816895 | KNN Loss: 3.681265115737915 | BCE Loss: 1.0350515842437744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.7211456298828125 | KNN Loss: 3.68992018699646 | BCE Loss: 1.031225562095642\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.700775146484375 | KNN Loss: 3.655952215194702 | BCE Loss: 1.0448229312896729\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.671133041381836 | KNN Loss: 3.652195453643799 | BCE Loss: 1.018937587738037\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.70466947555542 | KNN Loss: 3.697723150253296 | BCE Loss: 1.006946325302124\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.7451171875 | KNN Loss: 3.6882615089416504 | BCE Loss: 1.0568557977676392\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.671378135681152 | KNN Loss: 3.6710267066955566 | BCE Loss: 1.0003514289855957\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.720921993255615 | KNN Loss: 3.6986963748931885 | BCE Loss: 1.0222254991531372\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.698876857757568 | KNN Loss: 3.6827828884124756 | BCE Loss: 1.0160939693450928\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.688397407531738 | KNN Loss: 3.672787666320801 | BCE Loss: 1.0156097412109375\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.735464572906494 | KNN Loss: 3.687119483947754 | BCE Loss: 1.0483450889587402\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.747073173522949 | KNN Loss: 3.735769271850586 | BCE Loss: 1.0113041400909424\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.718112468719482 | KNN Loss: 3.7024781703948975 | BCE Loss: 1.0156341791152954\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.7107744216918945 | KNN Loss: 3.690790891647339 | BCE Loss: 1.0199837684631348\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.774502754211426 | KNN Loss: 3.744696617126465 | BCE Loss: 1.0298058986663818\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.677834987640381 | KNN Loss: 3.682497262954712 | BCE Loss: 0.995337724685669\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.727313995361328 | KNN Loss: 3.6921956539154053 | BCE Loss: 1.0351181030273438\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.729496002197266 | KNN Loss: 3.693230628967285 | BCE Loss: 1.036265254020691\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.704482555389404 | KNN Loss: 3.677966356277466 | BCE Loss: 1.026516079902649\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.733569622039795 | KNN Loss: 3.718179941177368 | BCE Loss: 1.0153896808624268\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.696125030517578 | KNN Loss: 3.676762819290161 | BCE Loss: 1.019362449645996\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.713521957397461 | KNN Loss: 3.6989920139312744 | BCE Loss: 1.0145297050476074\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.716286659240723 | KNN Loss: 3.678300380706787 | BCE Loss: 1.037986397743225\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.669152736663818 | KNN Loss: 3.66304612159729 | BCE Loss: 1.0061064958572388\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.755656719207764 | KNN Loss: 3.720721483230591 | BCE Loss: 1.0349352359771729\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.708260536193848 | KNN Loss: 3.6815996170043945 | BCE Loss: 1.0266607999801636\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.692162990570068 | KNN Loss: 3.6744325160980225 | BCE Loss: 1.0177303552627563\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.731330394744873 | KNN Loss: 3.7008001804351807 | BCE Loss: 1.0305302143096924\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.742866516113281 | KNN Loss: 3.7013630867004395 | BCE Loss: 1.0415034294128418\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.697815418243408 | KNN Loss: 3.660407781600952 | BCE Loss: 1.037407636642456\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.71088981628418 | KNN Loss: 3.6860098838806152 | BCE Loss: 1.0248796939849854\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.727991104125977 | KNN Loss: 3.71264910697937 | BCE Loss: 1.0153417587280273\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.761169910430908 | KNN Loss: 3.7134668827056885 | BCE Loss: 1.0477030277252197\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.6873908042907715 | KNN Loss: 3.670717477798462 | BCE Loss: 1.01667320728302\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.746696472167969 | KNN Loss: 3.7093141078948975 | BCE Loss: 1.0373821258544922\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.765613555908203 | KNN Loss: 3.7334320545196533 | BCE Loss: 1.032181739807129\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.8103485107421875 | KNN Loss: 3.801443338394165 | BCE Loss: 1.0089049339294434\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.719400882720947 | KNN Loss: 3.713609218597412 | BCE Loss: 1.0057915449142456\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.741249084472656 | KNN Loss: 3.6819286346435547 | BCE Loss: 1.0593202114105225\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.69831657409668 | KNN Loss: 3.680467128753662 | BCE Loss: 1.0178496837615967\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.695071697235107 | KNN Loss: 3.6544189453125 | BCE Loss: 1.040652871131897\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.724886417388916 | KNN Loss: 3.696314811706543 | BCE Loss: 1.0285714864730835\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.743441104888916 | KNN Loss: 3.6912643909454346 | BCE Loss: 1.0521767139434814\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.733839988708496 | KNN Loss: 3.7034969329833984 | BCE Loss: 1.030342936515808\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.684006690979004 | KNN Loss: 3.6656296253204346 | BCE Loss: 1.0183771848678589\n",
      "Epoch   200: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.687068939208984 | KNN Loss: 3.6706290245056152 | BCE Loss: 1.01643967628479\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.748058795928955 | KNN Loss: 3.713660478591919 | BCE Loss: 1.0343983173370361\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.688413619995117 | KNN Loss: 3.683852434158325 | BCE Loss: 1.004561185836792\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.736963272094727 | KNN Loss: 3.685830593109131 | BCE Loss: 1.0511325597763062\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.726797580718994 | KNN Loss: 3.687549114227295 | BCE Loss: 1.0392485857009888\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.744406700134277 | KNN Loss: 3.7085893154144287 | BCE Loss: 1.0358171463012695\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.648869514465332 | KNN Loss: 3.673529624938965 | BCE Loss: 0.9753400087356567\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.717653274536133 | KNN Loss: 3.6873371601104736 | BCE Loss: 1.0303162336349487\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.748469829559326 | KNN Loss: 3.691039800643921 | BCE Loss: 1.0574300289154053\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.763095855712891 | KNN Loss: 3.703373670578003 | BCE Loss: 1.0597221851348877\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.678696155548096 | KNN Loss: 3.663816213607788 | BCE Loss: 1.0148799419403076\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.807482719421387 | KNN Loss: 3.749752998352051 | BCE Loss: 1.0577298402786255\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.704416751861572 | KNN Loss: 3.6737582683563232 | BCE Loss: 1.0306583642959595\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.689401626586914 | KNN Loss: 3.6707522869110107 | BCE Loss: 1.0186491012573242\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.6949381828308105 | KNN Loss: 3.6956121921539307 | BCE Loss: 0.9993259310722351\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.722122669219971 | KNN Loss: 3.7135603427886963 | BCE Loss: 1.0085623264312744\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.759124755859375 | KNN Loss: 3.7185685634613037 | BCE Loss: 1.0405559539794922\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.704041004180908 | KNN Loss: 3.6854729652404785 | BCE Loss: 1.0185680389404297\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.70733642578125 | KNN Loss: 3.698906660079956 | BCE Loss: 1.008430004119873\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.691122055053711 | KNN Loss: 3.6526365280151367 | BCE Loss: 1.0384854078292847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.716550827026367 | KNN Loss: 3.6833183765411377 | BCE Loss: 1.033232569694519\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.729662895202637 | KNN Loss: 3.730487108230591 | BCE Loss: 0.9991757869720459\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.779178619384766 | KNN Loss: 3.722198486328125 | BCE Loss: 1.0569798946380615\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.687356948852539 | KNN Loss: 3.6964638233184814 | BCE Loss: 0.9908931255340576\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.743438720703125 | KNN Loss: 3.713141679763794 | BCE Loss: 1.030297040939331\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.700294494628906 | KNN Loss: 3.6910719871520996 | BCE Loss: 1.0092225074768066\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.732605457305908 | KNN Loss: 3.710179567337036 | BCE Loss: 1.022425889968872\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.719783782958984 | KNN Loss: 3.7004141807556152 | BCE Loss: 1.01936936378479\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.71121883392334 | KNN Loss: 3.6843490600585938 | BCE Loss: 1.026869535446167\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.7335662841796875 | KNN Loss: 3.696970224380493 | BCE Loss: 1.0365958213806152\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.738720893859863 | KNN Loss: 3.6805717945098877 | BCE Loss: 1.058148980140686\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.760019302368164 | KNN Loss: 3.728280782699585 | BCE Loss: 1.0317384004592896\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.695583343505859 | KNN Loss: 3.6630423069000244 | BCE Loss: 1.0325411558151245\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.714721202850342 | KNN Loss: 3.7103211879730225 | BCE Loss: 1.0043998956680298\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.715579986572266 | KNN Loss: 3.7151007652282715 | BCE Loss: 1.0004792213439941\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.7338690757751465 | KNN Loss: 3.690078020095825 | BCE Loss: 1.0437910556793213\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.72355318069458 | KNN Loss: 3.7030866146087646 | BCE Loss: 1.020466685295105\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.741974353790283 | KNN Loss: 3.705712080001831 | BCE Loss: 1.0362621545791626\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.700399398803711 | KNN Loss: 3.66935396194458 | BCE Loss: 1.03104567527771\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.775097846984863 | KNN Loss: 3.7313966751098633 | BCE Loss: 1.043701410293579\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.7658820152282715 | KNN Loss: 3.7140448093414307 | BCE Loss: 1.0518372058868408\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.75256872177124 | KNN Loss: 3.7386465072631836 | BCE Loss: 1.013922095298767\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.730391502380371 | KNN Loss: 3.7044565677642822 | BCE Loss: 1.025935173034668\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.709417343139648 | KNN Loss: 3.684990406036377 | BCE Loss: 1.0244269371032715\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.671347141265869 | KNN Loss: 3.6775524616241455 | BCE Loss: 0.9937946796417236\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.71458625793457 | KNN Loss: 3.679142713546753 | BCE Loss: 1.0354435443878174\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.679625034332275 | KNN Loss: 3.6694223880767822 | BCE Loss: 1.0102026462554932\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.712719917297363 | KNN Loss: 3.693328380584717 | BCE Loss: 1.0193912982940674\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.686549186706543 | KNN Loss: 3.663602590560913 | BCE Loss: 1.0229465961456299\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.715578079223633 | KNN Loss: 3.6799278259277344 | BCE Loss: 1.0356502532958984\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.652482986450195 | KNN Loss: 3.6392197608947754 | BCE Loss: 1.01326322555542\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.697291851043701 | KNN Loss: 3.671027183532715 | BCE Loss: 1.0262645483016968\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.758475303649902 | KNN Loss: 3.7104928493499756 | BCE Loss: 1.0479823350906372\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.790074825286865 | KNN Loss: 3.7636938095092773 | BCE Loss: 1.026381015777588\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.681755542755127 | KNN Loss: 3.6885759830474854 | BCE Loss: 0.9931796789169312\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.743884086608887 | KNN Loss: 3.7118453979492188 | BCE Loss: 1.032038688659668\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.747633457183838 | KNN Loss: 3.7256393432617188 | BCE Loss: 1.0219941139221191\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.711204528808594 | KNN Loss: 3.6969850063323975 | BCE Loss: 1.0142192840576172\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.691427707672119 | KNN Loss: 3.651879072189331 | BCE Loss: 1.0395487546920776\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.747595310211182 | KNN Loss: 3.7199320793151855 | BCE Loss: 1.027663230895996\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.704636573791504 | KNN Loss: 3.681661367416382 | BCE Loss: 1.0229750871658325\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.747029781341553 | KNN Loss: 3.726853370666504 | BCE Loss: 1.0201762914657593\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.7057905197143555 | KNN Loss: 3.697399616241455 | BCE Loss: 1.0083911418914795\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.674921035766602 | KNN Loss: 3.6798527240753174 | BCE Loss: 0.9950683116912842\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.732789039611816 | KNN Loss: 3.698707342147827 | BCE Loss: 1.0340816974639893\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.757260322570801 | KNN Loss: 3.7039029598236084 | BCE Loss: 1.0533573627471924\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.80025053024292 | KNN Loss: 3.7434561252593994 | BCE Loss: 1.056794285774231\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.6997809410095215 | KNN Loss: 3.687838077545166 | BCE Loss: 1.011942744255066\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.742804050445557 | KNN Loss: 3.733888626098633 | BCE Loss: 1.0089154243469238\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.693183422088623 | KNN Loss: 3.674436330795288 | BCE Loss: 1.018747091293335\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.7557878494262695 | KNN Loss: 3.6957366466522217 | BCE Loss: 1.0600510835647583\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.680727005004883 | KNN Loss: 3.6851181983947754 | BCE Loss: 0.9956085681915283\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.719409942626953 | KNN Loss: 3.6890594959259033 | BCE Loss: 1.0303502082824707\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.720599174499512 | KNN Loss: 3.71087384223938 | BCE Loss: 1.009725570678711\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.7350287437438965 | KNN Loss: 3.7243521213531494 | BCE Loss: 1.0106767416000366\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.740967750549316 | KNN Loss: 3.7005767822265625 | BCE Loss: 1.0403910875320435\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.740014553070068 | KNN Loss: 3.7071566581726074 | BCE Loss: 1.032857894897461\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.720832824707031 | KNN Loss: 3.716365337371826 | BCE Loss: 1.004467487335205\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.711477279663086 | KNN Loss: 3.6824557781219482 | BCE Loss: 1.0290212631225586\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.72499418258667 | KNN Loss: 3.711735963821411 | BCE Loss: 1.0132580995559692\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.698999404907227 | KNN Loss: 3.7092816829681396 | BCE Loss: 0.989717960357666\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.7485456466674805 | KNN Loss: 3.7120614051818848 | BCE Loss: 1.0364840030670166\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.713831424713135 | KNN Loss: 3.7031776905059814 | BCE Loss: 1.0106537342071533\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.679539680480957 | KNN Loss: 3.655820608139038 | BCE Loss: 1.023719072341919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.733996391296387 | KNN Loss: 3.722764730453491 | BCE Loss: 1.0112314224243164\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.68924617767334 | KNN Loss: 3.662137508392334 | BCE Loss: 1.0271084308624268\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.674508094787598 | KNN Loss: 3.669020891189575 | BCE Loss: 1.005487084388733\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.706817626953125 | KNN Loss: 3.6736128330230713 | BCE Loss: 1.0332046747207642\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.731132507324219 | KNN Loss: 3.7301411628723145 | BCE Loss: 1.0009913444519043\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.696763515472412 | KNN Loss: 3.6721482276916504 | BCE Loss: 1.0246152877807617\n",
      "Epoch   215: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.731341361999512 | KNN Loss: 3.7105159759521484 | BCE Loss: 1.0208253860473633\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.635898113250732 | KNN Loss: 3.6423869132995605 | BCE Loss: 0.9935113787651062\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.710059642791748 | KNN Loss: 3.69995379447937 | BCE Loss: 1.0101059675216675\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.724319934844971 | KNN Loss: 3.6872916221618652 | BCE Loss: 1.0370283126831055\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.713348388671875 | KNN Loss: 3.7026443481445312 | BCE Loss: 1.0107042789459229\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.699036121368408 | KNN Loss: 3.6694633960723877 | BCE Loss: 1.029572606086731\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.7126851081848145 | KNN Loss: 3.6974337100982666 | BCE Loss: 1.0152513980865479\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.671956539154053 | KNN Loss: 3.6783156394958496 | BCE Loss: 0.9936410188674927\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.698576927185059 | KNN Loss: 3.6796982288360596 | BCE Loss: 1.0188789367675781\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.724710464477539 | KNN Loss: 3.7238996028900146 | BCE Loss: 1.0008111000061035\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.77320671081543 | KNN Loss: 3.746026039123535 | BCE Loss: 1.0271804332733154\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.701299667358398 | KNN Loss: 3.6980834007263184 | BCE Loss: 1.003216028213501\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.698759078979492 | KNN Loss: 3.669624090194702 | BCE Loss: 1.02913498878479\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.726207733154297 | KNN Loss: 3.6935389041900635 | BCE Loss: 1.0326685905456543\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.702232837677002 | KNN Loss: 3.673532724380493 | BCE Loss: 1.0287001132965088\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.662473201751709 | KNN Loss: 3.6556482315063477 | BCE Loss: 1.0068249702453613\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.729022026062012 | KNN Loss: 3.707716941833496 | BCE Loss: 1.0213053226470947\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.766908645629883 | KNN Loss: 3.7417736053466797 | BCE Loss: 1.0251352787017822\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.717319488525391 | KNN Loss: 3.6677632331848145 | BCE Loss: 1.0495564937591553\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.7157769203186035 | KNN Loss: 3.709193706512451 | BCE Loss: 1.0065830945968628\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.731308460235596 | KNN Loss: 3.6935184001922607 | BCE Loss: 1.037790060043335\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.723527908325195 | KNN Loss: 3.6996915340423584 | BCE Loss: 1.023836374282837\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.6617302894592285 | KNN Loss: 3.6642167568206787 | BCE Loss: 0.9975134134292603\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.728252410888672 | KNN Loss: 3.680893659591675 | BCE Loss: 1.047358512878418\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.689613342285156 | KNN Loss: 3.6882612705230713 | BCE Loss: 1.0013518333435059\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.737667083740234 | KNN Loss: 3.6804144382476807 | BCE Loss: 1.0572528839111328\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.724254608154297 | KNN Loss: 3.6980526447296143 | BCE Loss: 1.0262019634246826\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.714702129364014 | KNN Loss: 3.708988904953003 | BCE Loss: 1.0057132244110107\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.69104528427124 | KNN Loss: 3.693784475326538 | BCE Loss: 0.9972609281539917\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.655745506286621 | KNN Loss: 3.6390371322631836 | BCE Loss: 1.0167083740234375\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.684483528137207 | KNN Loss: 3.6509158611297607 | BCE Loss: 1.0335676670074463\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.680159091949463 | KNN Loss: 3.6622023582458496 | BCE Loss: 1.0179568529129028\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.677414894104004 | KNN Loss: 3.649585485458374 | BCE Loss: 1.0278291702270508\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.692393779754639 | KNN Loss: 3.6656389236450195 | BCE Loss: 1.0267547369003296\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.7063093185424805 | KNN Loss: 3.677117109298706 | BCE Loss: 1.0291922092437744\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.732054710388184 | KNN Loss: 3.7101128101348877 | BCE Loss: 1.0219417810440063\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.708766937255859 | KNN Loss: 3.6790401935577393 | BCE Loss: 1.029726505279541\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.74635124206543 | KNN Loss: 3.7087502479553223 | BCE Loss: 1.0376009941101074\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.74485445022583 | KNN Loss: 3.6973512172698975 | BCE Loss: 1.047503113746643\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.7588958740234375 | KNN Loss: 3.7490246295928955 | BCE Loss: 1.0098713636398315\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.703167915344238 | KNN Loss: 3.6967411041259766 | BCE Loss: 1.0064269304275513\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.728949546813965 | KNN Loss: 3.6975014209747314 | BCE Loss: 1.0314481258392334\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.688833713531494 | KNN Loss: 3.668511390686035 | BCE Loss: 1.020322322845459\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.718369007110596 | KNN Loss: 3.713595390319824 | BCE Loss: 1.0047736167907715\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.711224555969238 | KNN Loss: 3.6926422119140625 | BCE Loss: 1.0185824632644653\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.721226215362549 | KNN Loss: 3.708895444869995 | BCE Loss: 1.0123308897018433\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.78446102142334 | KNN Loss: 3.75635027885437 | BCE Loss: 1.0281109809875488\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.7324604988098145 | KNN Loss: 3.7115156650543213 | BCE Loss: 1.0209449529647827\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.6820173263549805 | KNN Loss: 3.656820297241211 | BCE Loss: 1.0251970291137695\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.712006568908691 | KNN Loss: 3.696838855743408 | BCE Loss: 1.0151677131652832\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.699478626251221 | KNN Loss: 3.6862058639526367 | BCE Loss: 1.0132726430892944\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.771440505981445 | KNN Loss: 3.7369203567504883 | BCE Loss: 1.034520149230957\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.716131210327148 | KNN Loss: 3.6928975582122803 | BCE Loss: 1.0232336521148682\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.653761863708496 | KNN Loss: 3.6575284004211426 | BCE Loss: 0.996233344078064\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.737058639526367 | KNN Loss: 3.6894192695617676 | BCE Loss: 1.04763925075531\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.745533466339111 | KNN Loss: 3.734618902206421 | BCE Loss: 1.0109144449234009\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.692439556121826 | KNN Loss: 3.7037930488586426 | BCE Loss: 0.9886463284492493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.707334518432617 | KNN Loss: 3.6952459812164307 | BCE Loss: 1.0120882987976074\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.7220025062561035 | KNN Loss: 3.6695873737335205 | BCE Loss: 1.0524152517318726\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.783000469207764 | KNN Loss: 3.742971658706665 | BCE Loss: 1.040028691291809\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.696141719818115 | KNN Loss: 3.708183765411377 | BCE Loss: 0.9879578351974487\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.677399158477783 | KNN Loss: 3.652381420135498 | BCE Loss: 1.0250177383422852\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.723726749420166 | KNN Loss: 3.706437110900879 | BCE Loss: 1.017289638519287\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.670502662658691 | KNN Loss: 3.6692519187927246 | BCE Loss: 1.001250982284546\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.725288391113281 | KNN Loss: 3.6887595653533936 | BCE Loss: 1.0365289449691772\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.7332844734191895 | KNN Loss: 3.711793899536133 | BCE Loss: 1.0214906930923462\n",
      "Epoch   226: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.7726826667785645 | KNN Loss: 3.711449384689331 | BCE Loss: 1.0612331628799438\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.682016849517822 | KNN Loss: 3.666862726211548 | BCE Loss: 1.0151540040969849\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.691568851470947 | KNN Loss: 3.664447784423828 | BCE Loss: 1.0271209478378296\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.713265419006348 | KNN Loss: 3.6884405612945557 | BCE Loss: 1.024824857711792\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.702914714813232 | KNN Loss: 3.6553821563720703 | BCE Loss: 1.0475326776504517\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.7325944900512695 | KNN Loss: 3.711758613586426 | BCE Loss: 1.0208361148834229\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.780364036560059 | KNN Loss: 3.7503297328948975 | BCE Loss: 1.030034065246582\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.732141017913818 | KNN Loss: 3.6994097232818604 | BCE Loss: 1.0327311754226685\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.672695636749268 | KNN Loss: 3.689143657684326 | BCE Loss: 0.9835520386695862\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.768580436706543 | KNN Loss: 3.7179126739501953 | BCE Loss: 1.0506680011749268\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.763859272003174 | KNN Loss: 3.729832410812378 | BCE Loss: 1.034026861190796\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.660538196563721 | KNN Loss: 3.6666016578674316 | BCE Loss: 0.9939367175102234\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.779244422912598 | KNN Loss: 3.7217857837677 | BCE Loss: 1.0574584007263184\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.716647624969482 | KNN Loss: 3.7084155082702637 | BCE Loss: 1.0082321166992188\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.6704912185668945 | KNN Loss: 3.666412591934204 | BCE Loss: 1.0040786266326904\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.768556594848633 | KNN Loss: 3.730912446975708 | BCE Loss: 1.0376439094543457\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.701689720153809 | KNN Loss: 3.668757200241089 | BCE Loss: 1.0329325199127197\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.672017574310303 | KNN Loss: 3.6786251068115234 | BCE Loss: 0.993392288684845\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.679657459259033 | KNN Loss: 3.6759536266326904 | BCE Loss: 1.0037038326263428\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.719616889953613 | KNN Loss: 3.689113140106201 | BCE Loss: 1.0305039882659912\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.662005424499512 | KNN Loss: 3.6719300746917725 | BCE Loss: 0.9900755882263184\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.757977485656738 | KNN Loss: 3.718167781829834 | BCE Loss: 1.0398094654083252\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.71400785446167 | KNN Loss: 3.6863555908203125 | BCE Loss: 1.0276522636413574\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.787095546722412 | KNN Loss: 3.7369346618652344 | BCE Loss: 1.0501608848571777\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.765047073364258 | KNN Loss: 3.7106194496154785 | BCE Loss: 1.0544277429580688\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.688177108764648 | KNN Loss: 3.67828631401062 | BCE Loss: 1.0098909139633179\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.728500843048096 | KNN Loss: 3.686382532119751 | BCE Loss: 1.0421183109283447\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.700709819793701 | KNN Loss: 3.664808750152588 | BCE Loss: 1.0359010696411133\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.683211803436279 | KNN Loss: 3.674682140350342 | BCE Loss: 1.0085296630859375\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.684543609619141 | KNN Loss: 3.6709532737731934 | BCE Loss: 1.0135902166366577\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.741605758666992 | KNN Loss: 3.693795680999756 | BCE Loss: 1.0478100776672363\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.75732421875 | KNN Loss: 3.724384307861328 | BCE Loss: 1.0329400300979614\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.728069305419922 | KNN Loss: 3.686885118484497 | BCE Loss: 1.0411840677261353\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.748316287994385 | KNN Loss: 3.7414932250976562 | BCE Loss: 1.0068230628967285\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.690244197845459 | KNN Loss: 3.679731845855713 | BCE Loss: 1.010512351989746\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.719062805175781 | KNN Loss: 3.6865968704223633 | BCE Loss: 1.032465934753418\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.684996128082275 | KNN Loss: 3.673496961593628 | BCE Loss: 1.0114991664886475\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.719470024108887 | KNN Loss: 3.6879806518554688 | BCE Loss: 1.031489372253418\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.721846103668213 | KNN Loss: 3.6810414791107178 | BCE Loss: 1.0408046245574951\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.732378005981445 | KNN Loss: 3.6881792545318604 | BCE Loss: 1.044198751449585\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.731912612915039 | KNN Loss: 3.691641092300415 | BCE Loss: 1.0402714014053345\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.693932056427002 | KNN Loss: 3.6686654090881348 | BCE Loss: 1.0252665281295776\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.7056379318237305 | KNN Loss: 3.6792495250701904 | BCE Loss: 1.0263886451721191\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.679543495178223 | KNN Loss: 3.6815507411956787 | BCE Loss: 0.9979928731918335\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.689235687255859 | KNN Loss: 3.6786956787109375 | BCE Loss: 1.0105401277542114\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.77735710144043 | KNN Loss: 3.7262179851531982 | BCE Loss: 1.0511388778686523\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.6823883056640625 | KNN Loss: 3.674351692199707 | BCE Loss: 1.0080366134643555\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.711357593536377 | KNN Loss: 3.6838033199310303 | BCE Loss: 1.0275542736053467\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.706644535064697 | KNN Loss: 3.685814619064331 | BCE Loss: 1.0208299160003662\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.708915710449219 | KNN Loss: 3.6974525451660156 | BCE Loss: 1.0114631652832031\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.77641487121582 | KNN Loss: 3.7315967082977295 | BCE Loss: 1.04481840133667\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.729392051696777 | KNN Loss: 3.7086541652679443 | BCE Loss: 1.020738124847412\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.725487232208252 | KNN Loss: 3.682520866394043 | BCE Loss: 1.0429664850234985\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.720582962036133 | KNN Loss: 3.702072858810425 | BCE Loss: 1.018510103225708\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.723348140716553 | KNN Loss: 3.7111458778381348 | BCE Loss: 1.012202262878418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.6887922286987305 | KNN Loss: 3.6575610637664795 | BCE Loss: 1.03123140335083\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.759778022766113 | KNN Loss: 3.72322154045105 | BCE Loss: 1.0365564823150635\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.737240791320801 | KNN Loss: 3.7101662158966064 | BCE Loss: 1.0270744562149048\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.725982189178467 | KNN Loss: 3.7253103256225586 | BCE Loss: 1.0006718635559082\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.702294826507568 | KNN Loss: 3.6671929359436035 | BCE Loss: 1.0351020097732544\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.773587226867676 | KNN Loss: 3.7160604000091553 | BCE Loss: 1.0575265884399414\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.66157341003418 | KNN Loss: 3.6657235622406006 | BCE Loss: 0.9958497285842896\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.692432403564453 | KNN Loss: 3.6719021797180176 | BCE Loss: 1.0205302238464355\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.71645450592041 | KNN Loss: 3.6926333904266357 | BCE Loss: 1.0238213539123535\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.723043918609619 | KNN Loss: 3.70238995552063 | BCE Loss: 1.0206538438796997\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.779688358306885 | KNN Loss: 3.7452921867370605 | BCE Loss: 1.0343960523605347\n",
      "Epoch   237: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.736720085144043 | KNN Loss: 3.7007548809051514 | BCE Loss: 1.0359652042388916\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.715827465057373 | KNN Loss: 3.6760332584381104 | BCE Loss: 1.0397940874099731\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.739382743835449 | KNN Loss: 3.6845200061798096 | BCE Loss: 1.0548627376556396\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.687339782714844 | KNN Loss: 3.6971218585968018 | BCE Loss: 0.9902180433273315\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.663365840911865 | KNN Loss: 3.6602554321289062 | BCE Loss: 1.003110408782959\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.728304862976074 | KNN Loss: 3.694519519805908 | BCE Loss: 1.0337854623794556\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.685831069946289 | KNN Loss: 3.6791810989379883 | BCE Loss: 1.0066502094268799\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.739480972290039 | KNN Loss: 3.7039778232574463 | BCE Loss: 1.0355033874511719\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.728985786437988 | KNN Loss: 3.692800760269165 | BCE Loss: 1.0361847877502441\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.721130847930908 | KNN Loss: 3.7148780822753906 | BCE Loss: 1.0062528848648071\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.726844310760498 | KNN Loss: 3.697316884994507 | BCE Loss: 1.0295273065567017\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.702053070068359 | KNN Loss: 3.6729369163513184 | BCE Loss: 1.029115915298462\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.6938066482543945 | KNN Loss: 3.683007001876831 | BCE Loss: 1.0107996463775635\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.781618595123291 | KNN Loss: 3.7414908409118652 | BCE Loss: 1.0401277542114258\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.746432781219482 | KNN Loss: 3.6904265880584717 | BCE Loss: 1.0560060739517212\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.712973117828369 | KNN Loss: 3.6906070709228516 | BCE Loss: 1.0223660469055176\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.7376909255981445 | KNN Loss: 3.7243876457214355 | BCE Loss: 1.013303279876709\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.692465782165527 | KNN Loss: 3.654447078704834 | BCE Loss: 1.0380189418792725\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.72270393371582 | KNN Loss: 3.7138798236846924 | BCE Loss: 1.008824348449707\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.734498023986816 | KNN Loss: 3.6792004108428955 | BCE Loss: 1.0552973747253418\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.734143257141113 | KNN Loss: 3.723935127258301 | BCE Loss: 1.010208249092102\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.699728965759277 | KNN Loss: 3.674917459487915 | BCE Loss: 1.0248117446899414\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.781410217285156 | KNN Loss: 3.768221139907837 | BCE Loss: 1.0131889581680298\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.753751754760742 | KNN Loss: 3.7309718132019043 | BCE Loss: 1.0227800607681274\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.717958450317383 | KNN Loss: 3.7059412002563477 | BCE Loss: 1.0120173692703247\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.729951858520508 | KNN Loss: 3.7117185592651367 | BCE Loss: 1.018233299255371\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.72793436050415 | KNN Loss: 3.667282819747925 | BCE Loss: 1.0606516599655151\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.73267126083374 | KNN Loss: 3.6944973468780518 | BCE Loss: 1.038174033164978\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.666913032531738 | KNN Loss: 3.6530089378356934 | BCE Loss: 1.0139038562774658\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.708277702331543 | KNN Loss: 3.693211317062378 | BCE Loss: 1.015066385269165\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.748470783233643 | KNN Loss: 3.6881942749023438 | BCE Loss: 1.0602765083312988\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.733458518981934 | KNN Loss: 3.7041068077087402 | BCE Loss: 1.029351830482483\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.68009090423584 | KNN Loss: 3.6720213890075684 | BCE Loss: 1.0080697536468506\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.72145938873291 | KNN Loss: 3.715169668197632 | BCE Loss: 1.0062899589538574\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.707576751708984 | KNN Loss: 3.700928211212158 | BCE Loss: 1.006648302078247\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.718387603759766 | KNN Loss: 3.668121576309204 | BCE Loss: 1.050265908241272\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.701287269592285 | KNN Loss: 3.677793025970459 | BCE Loss: 1.0234942436218262\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.670580863952637 | KNN Loss: 3.673140287399292 | BCE Loss: 0.9974403381347656\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.727997303009033 | KNN Loss: 3.6981754302978516 | BCE Loss: 1.029821753501892\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.696610450744629 | KNN Loss: 3.688627004623413 | BCE Loss: 1.007983684539795\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.720837116241455 | KNN Loss: 3.725923538208008 | BCE Loss: 0.9949135184288025\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.684978485107422 | KNN Loss: 3.663651943206787 | BCE Loss: 1.0213263034820557\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.715694427490234 | KNN Loss: 3.6985082626342773 | BCE Loss: 1.0171864032745361\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.73319673538208 | KNN Loss: 3.705232858657837 | BCE Loss: 1.0279638767242432\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.684301376342773 | KNN Loss: 3.6578879356384277 | BCE Loss: 1.0264135599136353\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.706162929534912 | KNN Loss: 3.6908345222473145 | BCE Loss: 1.0153284072875977\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.723853588104248 | KNN Loss: 3.7054150104522705 | BCE Loss: 1.0184385776519775\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.695250034332275 | KNN Loss: 3.6804893016815186 | BCE Loss: 1.0147606134414673\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.668062686920166 | KNN Loss: 3.646322011947632 | BCE Loss: 1.0217406749725342\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.679224491119385 | KNN Loss: 3.6525886058807373 | BCE Loss: 1.0266358852386475\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.68284797668457 | KNN Loss: 3.6742491722106934 | BCE Loss: 1.0085985660552979\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.680904388427734 | KNN Loss: 3.6690542697906494 | BCE Loss: 1.011850357055664\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.691126823425293 | KNN Loss: 3.6832284927368164 | BCE Loss: 1.0078983306884766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.711059093475342 | KNN Loss: 3.6831564903259277 | BCE Loss: 1.0279024839401245\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.713857173919678 | KNN Loss: 3.69326114654541 | BCE Loss: 1.0205961465835571\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.673521995544434 | KNN Loss: 3.6751444339752197 | BCE Loss: 0.9983777403831482\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.6681318283081055 | KNN Loss: 3.6483070850372314 | BCE Loss: 1.0198249816894531\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.672610282897949 | KNN Loss: 3.6553807258605957 | BCE Loss: 1.0172297954559326\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.700257301330566 | KNN Loss: 3.6917738914489746 | BCE Loss: 1.0084832906723022\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.701848983764648 | KNN Loss: 3.659717082977295 | BCE Loss: 1.0421316623687744\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.737268924713135 | KNN Loss: 3.727158784866333 | BCE Loss: 1.0101100206375122\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.662837982177734 | KNN Loss: 3.6559736728668213 | BCE Loss: 1.006864309310913\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.740453720092773 | KNN Loss: 3.711226224899292 | BCE Loss: 1.0292277336120605\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.715251922607422 | KNN Loss: 3.6747663021087646 | BCE Loss: 1.0404855012893677\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.728118896484375 | KNN Loss: 3.705261468887329 | BCE Loss: 1.022857666015625\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.766341209411621 | KNN Loss: 3.7456700801849365 | BCE Loss: 1.0206712484359741\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.751182556152344 | KNN Loss: 3.728740692138672 | BCE Loss: 1.0224417448043823\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.715497016906738 | KNN Loss: 3.6993634700775146 | BCE Loss: 1.016133427619934\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.721538543701172 | KNN Loss: 3.690929889678955 | BCE Loss: 1.0306084156036377\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.728945255279541 | KNN Loss: 3.6996939182281494 | BCE Loss: 1.0292514562606812\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.758970260620117 | KNN Loss: 3.6852993965148926 | BCE Loss: 1.0736709833145142\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.710962295532227 | KNN Loss: 3.6864748001098633 | BCE Loss: 1.0244874954223633\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.679601669311523 | KNN Loss: 3.6398351192474365 | BCE Loss: 1.039766788482666\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.725420951843262 | KNN Loss: 3.7075204849243164 | BCE Loss: 1.0179004669189453\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.708163738250732 | KNN Loss: 3.7059192657470703 | BCE Loss: 1.0022445917129517\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.728426456451416 | KNN Loss: 3.7165472507476807 | BCE Loss: 1.0118792057037354\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.660391807556152 | KNN Loss: 3.654189109802246 | BCE Loss: 1.0062029361724854\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.677043914794922 | KNN Loss: 3.6648945808410645 | BCE Loss: 1.0121492147445679\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.708575248718262 | KNN Loss: 3.6726179122924805 | BCE Loss: 1.0359573364257812\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.745785713195801 | KNN Loss: 3.7549540996551514 | BCE Loss: 0.9908315539360046\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.736762523651123 | KNN Loss: 3.7115564346313477 | BCE Loss: 1.0252060890197754\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.716737270355225 | KNN Loss: 3.6983723640441895 | BCE Loss: 1.0183649063110352\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.717349052429199 | KNN Loss: 3.702289581298828 | BCE Loss: 1.0150595903396606\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.726902961730957 | KNN Loss: 3.6765236854553223 | BCE Loss: 1.0503791570663452\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.729038238525391 | KNN Loss: 3.6951792240142822 | BCE Loss: 1.033859133720398\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.689468860626221 | KNN Loss: 3.6816139221191406 | BCE Loss: 1.00785493850708\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.659032821655273 | KNN Loss: 3.6545684337615967 | BCE Loss: 1.0044646263122559\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.7082624435424805 | KNN Loss: 3.7008869647979736 | BCE Loss: 1.007375717163086\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.814143657684326 | KNN Loss: 3.7453463077545166 | BCE Loss: 1.0687973499298096\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.767792701721191 | KNN Loss: 3.7255444526672363 | BCE Loss: 1.0422481298446655\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.701064109802246 | KNN Loss: 3.6857948303222656 | BCE Loss: 1.01526939868927\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.733500003814697 | KNN Loss: 3.7107417583465576 | BCE Loss: 1.0227582454681396\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.728900909423828 | KNN Loss: 3.69836688041687 | BCE Loss: 1.030534029006958\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.759770393371582 | KNN Loss: 3.7008323669433594 | BCE Loss: 1.0589382648468018\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.707174777984619 | KNN Loss: 3.6994025707244873 | BCE Loss: 1.0077723264694214\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.700252532958984 | KNN Loss: 3.6966354846954346 | BCE Loss: 1.003617286682129\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.7458720207214355 | KNN Loss: 3.7267627716064453 | BCE Loss: 1.0191092491149902\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.755416393280029 | KNN Loss: 3.717959403991699 | BCE Loss: 1.0374568700790405\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.725362300872803 | KNN Loss: 3.6778788566589355 | BCE Loss: 1.0474834442138672\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.745695114135742 | KNN Loss: 3.7168800830841064 | BCE Loss: 1.0288147926330566\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.695772171020508 | KNN Loss: 3.6898720264434814 | BCE Loss: 1.0058999061584473\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.6956281661987305 | KNN Loss: 3.686950445175171 | BCE Loss: 1.0086774826049805\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.719705104827881 | KNN Loss: 3.709805727005005 | BCE Loss: 1.009899377822876\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.695444583892822 | KNN Loss: 3.6749584674835205 | BCE Loss: 1.0204861164093018\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.709456443786621 | KNN Loss: 3.6752195358276367 | BCE Loss: 1.034237027168274\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.724054336547852 | KNN Loss: 3.697326183319092 | BCE Loss: 1.0267281532287598\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.744611740112305 | KNN Loss: 3.719318389892578 | BCE Loss: 1.0252931118011475\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.720919132232666 | KNN Loss: 3.6733081340789795 | BCE Loss: 1.047610878944397\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.638860702514648 | KNN Loss: 3.637793779373169 | BCE Loss: 1.0010671615600586\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.652012825012207 | KNN Loss: 3.6715826988220215 | BCE Loss: 0.9804301857948303\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.660848617553711 | KNN Loss: 3.6529200077056885 | BCE Loss: 1.0079283714294434\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.744298458099365 | KNN Loss: 3.7074761390686035 | BCE Loss: 1.0368223190307617\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.7014007568359375 | KNN Loss: 3.665144443511963 | BCE Loss: 1.0362560749053955\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.702569961547852 | KNN Loss: 3.675187110900879 | BCE Loss: 1.0273830890655518\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.734739303588867 | KNN Loss: 3.6903512477874756 | BCE Loss: 1.0443882942199707\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.724958896636963 | KNN Loss: 3.7103333473205566 | BCE Loss: 1.0146254301071167\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.74755334854126 | KNN Loss: 3.742131471633911 | BCE Loss: 1.005421757698059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.703512191772461 | KNN Loss: 3.698789119720459 | BCE Loss: 1.0047228336334229\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.7712507247924805 | KNN Loss: 3.701223373413086 | BCE Loss: 1.0700273513793945\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.738153457641602 | KNN Loss: 3.6909780502319336 | BCE Loss: 1.047175407409668\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.703385353088379 | KNN Loss: 3.657529830932617 | BCE Loss: 1.0458554029464722\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.6957902908325195 | KNN Loss: 3.6990087032318115 | BCE Loss: 0.9967814683914185\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.7409515380859375 | KNN Loss: 3.6940677165985107 | BCE Loss: 1.0468838214874268\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.695403575897217 | KNN Loss: 3.6973190307617188 | BCE Loss: 0.998084545135498\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.68182373046875 | KNN Loss: 3.66194748878479 | BCE Loss: 1.0198761224746704\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.734711647033691 | KNN Loss: 3.7135653495788574 | BCE Loss: 1.021146535873413\n",
      "Epoch   258: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.6810760498046875 | KNN Loss: 3.673341751098633 | BCE Loss: 1.0077342987060547\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.70111083984375 | KNN Loss: 3.6841394901275635 | BCE Loss: 1.0169711112976074\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.726197719573975 | KNN Loss: 3.7116034030914307 | BCE Loss: 1.014594316482544\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.77302360534668 | KNN Loss: 3.7077255249023438 | BCE Loss: 1.065298318862915\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.703606605529785 | KNN Loss: 3.6828057765960693 | BCE Loss: 1.0208005905151367\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.702218055725098 | KNN Loss: 3.673952102661133 | BCE Loss: 1.0282658338546753\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.69891357421875 | KNN Loss: 3.698361873626709 | BCE Loss: 1.000551700592041\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.72424840927124 | KNN Loss: 3.702305793762207 | BCE Loss: 1.0219426155090332\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.726583003997803 | KNN Loss: 3.667433500289917 | BCE Loss: 1.0591495037078857\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.7092390060424805 | KNN Loss: 3.689397096633911 | BCE Loss: 1.0198419094085693\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.712799072265625 | KNN Loss: 3.6979191303253174 | BCE Loss: 1.0148801803588867\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.703385353088379 | KNN Loss: 3.67612361907959 | BCE Loss: 1.0272619724273682\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.741070747375488 | KNN Loss: 3.702800989151001 | BCE Loss: 1.0382695198059082\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.778044700622559 | KNN Loss: 3.751988410949707 | BCE Loss: 1.0260560512542725\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.695766925811768 | KNN Loss: 3.6693546772003174 | BCE Loss: 1.0264121294021606\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.737791061401367 | KNN Loss: 3.7041101455688477 | BCE Loss: 1.03368079662323\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.722956657409668 | KNN Loss: 3.6815807819366455 | BCE Loss: 1.041375756263733\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.7124409675598145 | KNN Loss: 3.6804211139678955 | BCE Loss: 1.0320197343826294\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.752612113952637 | KNN Loss: 3.7055583000183105 | BCE Loss: 1.0470540523529053\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.7283124923706055 | KNN Loss: 3.674049139022827 | BCE Loss: 1.0542633533477783\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.778436660766602 | KNN Loss: 3.7508788108825684 | BCE Loss: 1.0275580883026123\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.722630023956299 | KNN Loss: 3.700610876083374 | BCE Loss: 1.0220191478729248\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.750308036804199 | KNN Loss: 3.7260844707489014 | BCE Loss: 1.0242233276367188\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.696686744689941 | KNN Loss: 3.6891469955444336 | BCE Loss: 1.0075397491455078\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.6619648933410645 | KNN Loss: 3.653062582015991 | BCE Loss: 1.0089024305343628\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.76221227645874 | KNN Loss: 3.710322856903076 | BCE Loss: 1.0518895387649536\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.699010848999023 | KNN Loss: 3.689750909805298 | BCE Loss: 1.0092597007751465\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.731529235839844 | KNN Loss: 3.6882927417755127 | BCE Loss: 1.0432367324829102\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.7149152755737305 | KNN Loss: 3.7034590244293213 | BCE Loss: 1.0114564895629883\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.728587627410889 | KNN Loss: 3.723447799682617 | BCE Loss: 1.0051398277282715\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.688600063323975 | KNN Loss: 3.6909689903259277 | BCE Loss: 0.9976309537887573\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.701030731201172 | KNN Loss: 3.6953299045562744 | BCE Loss: 1.0057008266448975\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.713963985443115 | KNN Loss: 3.670203685760498 | BCE Loss: 1.0437604188919067\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.7160773277282715 | KNN Loss: 3.6915457248687744 | BCE Loss: 1.0245314836502075\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.769025802612305 | KNN Loss: 3.7322661876678467 | BCE Loss: 1.036759376525879\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.736728668212891 | KNN Loss: 3.7016193866729736 | BCE Loss: 1.035109519958496\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.703862190246582 | KNN Loss: 3.658337116241455 | BCE Loss: 1.045525312423706\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.7618021965026855 | KNN Loss: 3.719280481338501 | BCE Loss: 1.0425217151641846\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.730574607849121 | KNN Loss: 3.706052303314209 | BCE Loss: 1.024522066116333\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.735448360443115 | KNN Loss: 3.6992149353027344 | BCE Loss: 1.0362333059310913\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.689380645751953 | KNN Loss: 3.664933681488037 | BCE Loss: 1.024446725845337\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.682938575744629 | KNN Loss: 3.6819589138031006 | BCE Loss: 1.0009796619415283\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.766168594360352 | KNN Loss: 3.7476956844329834 | BCE Loss: 1.0184729099273682\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.7441935539245605 | KNN Loss: 3.6796741485595703 | BCE Loss: 1.0645195245742798\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.723049163818359 | KNN Loss: 3.7061643600463867 | BCE Loss: 1.0168848037719727\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.714327812194824 | KNN Loss: 3.7013001441955566 | BCE Loss: 1.0130279064178467\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.718459129333496 | KNN Loss: 3.6844615936279297 | BCE Loss: 1.0339975357055664\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.707885265350342 | KNN Loss: 3.662893056869507 | BCE Loss: 1.0449920892715454\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.707769393920898 | KNN Loss: 3.654902458190918 | BCE Loss: 1.0528669357299805\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.753316879272461 | KNN Loss: 3.7272632122039795 | BCE Loss: 1.0260536670684814\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.756069660186768 | KNN Loss: 3.726811170578003 | BCE Loss: 1.029258370399475\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.7766618728637695 | KNN Loss: 3.739067554473877 | BCE Loss: 1.037594199180603\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.672410011291504 | KNN Loss: 3.6685638427734375 | BCE Loss: 1.0038461685180664\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.722565650939941 | KNN Loss: 3.6871705055236816 | BCE Loss: 1.0353953838348389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.71190881729126 | KNN Loss: 3.6798558235168457 | BCE Loss: 1.032052993774414\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.698554992675781 | KNN Loss: 3.6765825748443604 | BCE Loss: 1.02197265625\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.718596458435059 | KNN Loss: 3.676518201828003 | BCE Loss: 1.0420780181884766\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.708215236663818 | KNN Loss: 3.6877501010894775 | BCE Loss: 1.0204652547836304\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.637581825256348 | KNN Loss: 3.64620304107666 | BCE Loss: 0.9913787245750427\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.704524517059326 | KNN Loss: 3.694326162338257 | BCE Loss: 1.0101983547210693\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.708281993865967 | KNN Loss: 3.7090396881103516 | BCE Loss: 0.9992421269416809\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.66422700881958 | KNN Loss: 3.6736865043640137 | BCE Loss: 0.990540623664856\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.704929828643799 | KNN Loss: 3.6964519023895264 | BCE Loss: 1.0084779262542725\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.691018104553223 | KNN Loss: 3.67446231842041 | BCE Loss: 1.016555666923523\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.743398666381836 | KNN Loss: 3.7026095390319824 | BCE Loss: 1.040789008140564\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.717308044433594 | KNN Loss: 3.7023210525512695 | BCE Loss: 1.0149872303009033\n",
      "Epoch   269: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.742262363433838 | KNN Loss: 3.709172487258911 | BCE Loss: 1.0330899953842163\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.741696834564209 | KNN Loss: 3.6745593547821045 | BCE Loss: 1.0671374797821045\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.734149932861328 | KNN Loss: 3.68119740486145 | BCE Loss: 1.052952766418457\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.705898761749268 | KNN Loss: 3.6774260997772217 | BCE Loss: 1.0284725427627563\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.778634071350098 | KNN Loss: 3.7420356273651123 | BCE Loss: 1.0365984439849854\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.687652111053467 | KNN Loss: 3.688483953475952 | BCE Loss: 0.9991679787635803\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.715334892272949 | KNN Loss: 3.6963119506835938 | BCE Loss: 1.0190229415893555\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.69532585144043 | KNN Loss: 3.661207437515259 | BCE Loss: 1.0341181755065918\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.660841464996338 | KNN Loss: 3.652881145477295 | BCE Loss: 1.0079604387283325\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.711897850036621 | KNN Loss: 3.682884693145752 | BCE Loss: 1.0290133953094482\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.662744045257568 | KNN Loss: 3.6675736904144287 | BCE Loss: 0.995170533657074\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.715662002563477 | KNN Loss: 3.698416233062744 | BCE Loss: 1.017245888710022\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.697397708892822 | KNN Loss: 3.653346300125122 | BCE Loss: 1.0440514087677002\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.7083024978637695 | KNN Loss: 3.688371419906616 | BCE Loss: 1.0199310779571533\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.75529670715332 | KNN Loss: 3.6963651180267334 | BCE Loss: 1.0589313507080078\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.7102556228637695 | KNN Loss: 3.697356700897217 | BCE Loss: 1.0128991603851318\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.701240539550781 | KNN Loss: 3.6825525760650635 | BCE Loss: 1.0186882019042969\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.724922180175781 | KNN Loss: 3.714625835418701 | BCE Loss: 1.010296106338501\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.692188739776611 | KNN Loss: 3.6894683837890625 | BCE Loss: 1.0027204751968384\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.662233829498291 | KNN Loss: 3.663902521133423 | BCE Loss: 0.9983313083648682\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.698896408081055 | KNN Loss: 3.6794989109039307 | BCE Loss: 1.019397497177124\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.711003303527832 | KNN Loss: 3.6745071411132812 | BCE Loss: 1.0364960432052612\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.729734420776367 | KNN Loss: 3.6989498138427734 | BCE Loss: 1.0307847261428833\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.679643154144287 | KNN Loss: 3.6656949520111084 | BCE Loss: 1.0139480829238892\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.683201313018799 | KNN Loss: 3.6704282760620117 | BCE Loss: 1.0127729177474976\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.687297821044922 | KNN Loss: 3.653139352798462 | BCE Loss: 1.0341585874557495\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.730349540710449 | KNN Loss: 3.687563419342041 | BCE Loss: 1.0427861213684082\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.700195789337158 | KNN Loss: 3.681654691696167 | BCE Loss: 1.0185410976409912\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.682410717010498 | KNN Loss: 3.690171957015991 | BCE Loss: 0.9922387003898621\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.689257621765137 | KNN Loss: 3.688211441040039 | BCE Loss: 1.0010464191436768\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.69633150100708 | KNN Loss: 3.7004220485687256 | BCE Loss: 0.9959096312522888\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.768181800842285 | KNN Loss: 3.7308249473571777 | BCE Loss: 1.0373566150665283\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.7273993492126465 | KNN Loss: 3.690401792526245 | BCE Loss: 1.0369975566864014\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.748480319976807 | KNN Loss: 3.708775520324707 | BCE Loss: 1.03970468044281\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.707326889038086 | KNN Loss: 3.696716070175171 | BCE Loss: 1.0106109380722046\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.698155403137207 | KNN Loss: 3.65923810005188 | BCE Loss: 1.0389175415039062\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.745431900024414 | KNN Loss: 3.7058019638061523 | BCE Loss: 1.0396299362182617\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.7632036209106445 | KNN Loss: 3.701721429824829 | BCE Loss: 1.0614819526672363\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.712459564208984 | KNN Loss: 3.7067058086395264 | BCE Loss: 1.005753517150879\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.747597694396973 | KNN Loss: 3.7206883430480957 | BCE Loss: 1.026909351348877\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.719913482666016 | KNN Loss: 3.699734687805176 | BCE Loss: 1.0201787948608398\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.7155609130859375 | KNN Loss: 3.676561117172241 | BCE Loss: 1.0389997959136963\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.734923362731934 | KNN Loss: 3.709071636199951 | BCE Loss: 1.0258519649505615\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.728795051574707 | KNN Loss: 3.7152976989746094 | BCE Loss: 1.0134971141815186\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.732328414916992 | KNN Loss: 3.7173924446105957 | BCE Loss: 1.0149359703063965\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.7259907722473145 | KNN Loss: 3.7084438800811768 | BCE Loss: 1.0175468921661377\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.715059280395508 | KNN Loss: 3.7015435695648193 | BCE Loss: 1.013515830039978\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.792494773864746 | KNN Loss: 3.765766143798828 | BCE Loss: 1.0267287492752075\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.668679237365723 | KNN Loss: 3.6611578464508057 | BCE Loss: 1.007521629333496\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.7534403800964355 | KNN Loss: 3.7404375076293945 | BCE Loss: 1.013002872467041\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.698676109313965 | KNN Loss: 3.684459686279297 | BCE Loss: 1.0142165422439575\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.736093997955322 | KNN Loss: 3.708427906036377 | BCE Loss: 1.0276660919189453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.755423545837402 | KNN Loss: 3.701122760772705 | BCE Loss: 1.0543010234832764\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.687032699584961 | KNN Loss: 3.692953586578369 | BCE Loss: 0.9940793514251709\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.7337141036987305 | KNN Loss: 3.730053186416626 | BCE Loss: 1.0036606788635254\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.704862117767334 | KNN Loss: 3.6801252365112305 | BCE Loss: 1.0247368812561035\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.7160868644714355 | KNN Loss: 3.6975960731506348 | BCE Loss: 1.0184907913208008\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.678962707519531 | KNN Loss: 3.666198968887329 | BCE Loss: 1.012763500213623\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.760176658630371 | KNN Loss: 3.705671787261963 | BCE Loss: 1.054504632949829\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.719836235046387 | KNN Loss: 3.676417112350464 | BCE Loss: 1.0434191226959229\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.688841819763184 | KNN Loss: 3.6670773029327393 | BCE Loss: 1.0217643976211548\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.724117279052734 | KNN Loss: 3.685182571411133 | BCE Loss: 1.0389347076416016\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.709796905517578 | KNN Loss: 3.679334878921509 | BCE Loss: 1.0304621458053589\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.705631256103516 | KNN Loss: 3.6861391067504883 | BCE Loss: 1.0194923877716064\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.732006072998047 | KNN Loss: 3.6869142055511475 | BCE Loss: 1.0450917482376099\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.725347518920898 | KNN Loss: 3.7030515670776367 | BCE Loss: 1.0222958326339722\n",
      "Epoch   280: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.674572944641113 | KNN Loss: 3.668884038925171 | BCE Loss: 1.0056891441345215\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.701163291931152 | KNN Loss: 3.6733150482177734 | BCE Loss: 1.027848243713379\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.721540451049805 | KNN Loss: 3.6697521209716797 | BCE Loss: 1.0517882108688354\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.738797664642334 | KNN Loss: 3.732266426086426 | BCE Loss: 1.0065312385559082\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.70097541809082 | KNN Loss: 3.684997320175171 | BCE Loss: 1.0159783363342285\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.690354347229004 | KNN Loss: 3.6725375652313232 | BCE Loss: 1.0178167819976807\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.698456287384033 | KNN Loss: 3.6542809009552 | BCE Loss: 1.0441752672195435\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.711498737335205 | KNN Loss: 3.668961763381958 | BCE Loss: 1.0425368547439575\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.721096038818359 | KNN Loss: 3.69319748878479 | BCE Loss: 1.0278984308242798\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.722332954406738 | KNN Loss: 3.717109441757202 | BCE Loss: 1.0052235126495361\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.727814674377441 | KNN Loss: 3.6870100498199463 | BCE Loss: 1.040804386138916\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.735790252685547 | KNN Loss: 3.721644163131714 | BCE Loss: 1.014146089553833\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.6569437980651855 | KNN Loss: 3.6619417667388916 | BCE Loss: 0.9950021505355835\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.7764892578125 | KNN Loss: 3.7330548763275146 | BCE Loss: 1.0434342622756958\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.735685348510742 | KNN Loss: 3.6958470344543457 | BCE Loss: 1.0398385524749756\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.740943908691406 | KNN Loss: 3.703756093978882 | BCE Loss: 1.0371878147125244\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.76363468170166 | KNN Loss: 3.758114814758301 | BCE Loss: 1.0055197477340698\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.739635467529297 | KNN Loss: 3.6957826614379883 | BCE Loss: 1.0438525676727295\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.709753036499023 | KNN Loss: 3.664720296859741 | BCE Loss: 1.0450327396392822\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.743817329406738 | KNN Loss: 3.707587480545044 | BCE Loss: 1.0362300872802734\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.740333557128906 | KNN Loss: 3.7179336547851562 | BCE Loss: 1.022399663925171\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.731553077697754 | KNN Loss: 3.6892786026000977 | BCE Loss: 1.0422743558883667\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.706554889678955 | KNN Loss: 3.6969282627105713 | BCE Loss: 1.0096265077590942\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.671272277832031 | KNN Loss: 3.664232015609741 | BCE Loss: 1.0070405006408691\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.741263389587402 | KNN Loss: 3.707265853881836 | BCE Loss: 1.0339977741241455\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.6366801261901855 | KNN Loss: 3.6479873657226562 | BCE Loss: 0.9886926412582397\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.680336952209473 | KNN Loss: 3.683548927307129 | BCE Loss: 0.9967881441116333\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.714261054992676 | KNN Loss: 3.6985156536102295 | BCE Loss: 1.0157451629638672\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.757938385009766 | KNN Loss: 3.714916229248047 | BCE Loss: 1.0430221557617188\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.683376312255859 | KNN Loss: 3.677401542663574 | BCE Loss: 1.0059750080108643\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.727667808532715 | KNN Loss: 3.7146430015563965 | BCE Loss: 1.0130250453948975\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.702397346496582 | KNN Loss: 3.6927404403686523 | BCE Loss: 1.0096567869186401\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.69327974319458 | KNN Loss: 3.663741111755371 | BCE Loss: 1.029538631439209\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.713715553283691 | KNN Loss: 3.699597120285034 | BCE Loss: 1.0141184329986572\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.738371849060059 | KNN Loss: 3.7126636505126953 | BCE Loss: 1.0257079601287842\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.699090957641602 | KNN Loss: 3.6759536266326904 | BCE Loss: 1.0231375694274902\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.6931562423706055 | KNN Loss: 3.6587252616882324 | BCE Loss: 1.0344312191009521\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.701528072357178 | KNN Loss: 3.6851587295532227 | BCE Loss: 1.016369342803955\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.704560279846191 | KNN Loss: 3.6833078861236572 | BCE Loss: 1.0212522745132446\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.703241348266602 | KNN Loss: 3.6799471378326416 | BCE Loss: 1.0232939720153809\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.655001640319824 | KNN Loss: 3.6429340839385986 | BCE Loss: 1.012067437171936\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.7092509269714355 | KNN Loss: 3.70166015625 | BCE Loss: 1.007590889930725\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.686801910400391 | KNN Loss: 3.661742687225342 | BCE Loss: 1.0250589847564697\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.720723628997803 | KNN Loss: 3.680262327194214 | BCE Loss: 1.0404611825942993\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.719355583190918 | KNN Loss: 3.709169626235962 | BCE Loss: 1.010185956954956\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.716975212097168 | KNN Loss: 3.692434310913086 | BCE Loss: 1.024540662765503\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.716311454772949 | KNN Loss: 3.6806745529174805 | BCE Loss: 1.0356366634368896\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.76419734954834 | KNN Loss: 3.714381456375122 | BCE Loss: 1.0498157739639282\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.678244590759277 | KNN Loss: 3.6510403156280518 | BCE Loss: 1.0272043943405151\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.724913597106934 | KNN Loss: 3.709183931350708 | BCE Loss: 1.0157294273376465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.728771209716797 | KNN Loss: 3.706209421157837 | BCE Loss: 1.0225619077682495\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.711973190307617 | KNN Loss: 3.70750093460083 | BCE Loss: 1.0044723749160767\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.727872848510742 | KNN Loss: 3.6813645362854004 | BCE Loss: 1.046508550643921\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.736748695373535 | KNN Loss: 3.698896884918213 | BCE Loss: 1.0378520488739014\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.6785383224487305 | KNN Loss: 3.659581422805786 | BCE Loss: 1.0189566612243652\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.711480140686035 | KNN Loss: 3.673100709915161 | BCE Loss: 1.038379192352295\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.695392608642578 | KNN Loss: 3.6869466304779053 | BCE Loss: 1.008446216583252\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.718842506408691 | KNN Loss: 3.691472053527832 | BCE Loss: 1.0273702144622803\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.737698554992676 | KNN Loss: 3.717385768890381 | BCE Loss: 1.020313024520874\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.674140930175781 | KNN Loss: 3.6628174781799316 | BCE Loss: 1.0113236904144287\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.722507476806641 | KNN Loss: 3.697690725326538 | BCE Loss: 1.024816870689392\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.685663223266602 | KNN Loss: 3.6831133365631104 | BCE Loss: 1.0025498867034912\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.741219520568848 | KNN Loss: 3.7275426387786865 | BCE Loss: 1.0136770009994507\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.749462604522705 | KNN Loss: 3.702718734741211 | BCE Loss: 1.0467438697814941\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.715262413024902 | KNN Loss: 3.6966605186462402 | BCE Loss: 1.0186021327972412\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.697351455688477 | KNN Loss: 3.673915147781372 | BCE Loss: 1.0234363079071045\n",
      "Epoch   291: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.680503845214844 | KNN Loss: 3.66489577293396 | BCE Loss: 1.0156080722808838\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.6847310066223145 | KNN Loss: 3.6855008602142334 | BCE Loss: 0.9992299675941467\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.753791332244873 | KNN Loss: 3.737361192703247 | BCE Loss: 1.016430139541626\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.755925178527832 | KNN Loss: 3.701172113418579 | BCE Loss: 1.054753303527832\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.753708839416504 | KNN Loss: 3.6974844932556152 | BCE Loss: 1.0562243461608887\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.699188232421875 | KNN Loss: 3.682556390762329 | BCE Loss: 1.016631841659546\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.728405952453613 | KNN Loss: 3.6961162090301514 | BCE Loss: 1.032289743423462\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.709782600402832 | KNN Loss: 3.6827805042266846 | BCE Loss: 1.027002215385437\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.693706035614014 | KNN Loss: 3.6886987686157227 | BCE Loss: 1.005007266998291\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.6872453689575195 | KNN Loss: 3.6704788208007812 | BCE Loss: 1.0167664289474487\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.733730316162109 | KNN Loss: 3.6973869800567627 | BCE Loss: 1.0363430976867676\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.719970703125 | KNN Loss: 3.689908266067505 | BCE Loss: 1.0300623178482056\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.735041618347168 | KNN Loss: 3.6954457759857178 | BCE Loss: 1.0395958423614502\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.65659236907959 | KNN Loss: 3.635939359664917 | BCE Loss: 1.0206530094146729\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.651105880737305 | KNN Loss: 3.648209571838379 | BCE Loss: 1.0028965473175049\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.6796040534973145 | KNN Loss: 3.671441078186035 | BCE Loss: 1.0081629753112793\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.729072570800781 | KNN Loss: 3.700673818588257 | BCE Loss: 1.0283987522125244\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.690934181213379 | KNN Loss: 3.668550729751587 | BCE Loss: 1.022383213043213\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.68332576751709 | KNN Loss: 3.655547857284546 | BCE Loss: 1.0277776718139648\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.74241304397583 | KNN Loss: 3.6773970127105713 | BCE Loss: 1.0650159120559692\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.739405155181885 | KNN Loss: 3.695005178451538 | BCE Loss: 1.0443999767303467\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.742722511291504 | KNN Loss: 3.726923704147339 | BCE Loss: 1.0157990455627441\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.703276634216309 | KNN Loss: 3.6824541091918945 | BCE Loss: 1.020822286605835\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.739617347717285 | KNN Loss: 3.69429612159729 | BCE Loss: 1.045320987701416\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.72391939163208 | KNN Loss: 3.6996936798095703 | BCE Loss: 1.0242257118225098\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.700665473937988 | KNN Loss: 3.6768131256103516 | BCE Loss: 1.0238523483276367\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.672863006591797 | KNN Loss: 3.6585631370544434 | BCE Loss: 1.0142996311187744\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.719906330108643 | KNN Loss: 3.6760334968566895 | BCE Loss: 1.0438728332519531\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.690402030944824 | KNN Loss: 3.693056106567383 | BCE Loss: 0.9973458051681519\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.690214157104492 | KNN Loss: 3.6753926277160645 | BCE Loss: 1.0148215293884277\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.759097099304199 | KNN Loss: 3.717567205429077 | BCE Loss: 1.0415301322937012\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.734095096588135 | KNN Loss: 3.7086193561553955 | BCE Loss: 1.0254756212234497\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.726254463195801 | KNN Loss: 3.677290678024292 | BCE Loss: 1.048964023590088\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.724567413330078 | KNN Loss: 3.672619104385376 | BCE Loss: 1.0519483089447021\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.720653057098389 | KNN Loss: 3.7088568210601807 | BCE Loss: 1.011796236038208\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.708671569824219 | KNN Loss: 3.6959800720214844 | BCE Loss: 1.0126914978027344\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.734489917755127 | KNN Loss: 3.697594165802002 | BCE Loss: 1.036895751953125\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.69516658782959 | KNN Loss: 3.678372383117676 | BCE Loss: 1.0167943239212036\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.692823886871338 | KNN Loss: 3.6820998191833496 | BCE Loss: 1.0107240676879883\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.770005226135254 | KNN Loss: 3.7162580490112305 | BCE Loss: 1.0537471771240234\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.701108932495117 | KNN Loss: 3.6849732398986816 | BCE Loss: 1.0161359310150146\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.710837364196777 | KNN Loss: 3.6900675296783447 | BCE Loss: 1.0207699537277222\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.764040946960449 | KNN Loss: 3.735928535461426 | BCE Loss: 1.0281124114990234\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.701989650726318 | KNN Loss: 3.6937334537506104 | BCE Loss: 1.008256196975708\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.7545647621154785 | KNN Loss: 3.6981749534606934 | BCE Loss: 1.0563896894454956\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.703001022338867 | KNN Loss: 3.6741995811462402 | BCE Loss: 1.0288015604019165\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.724474906921387 | KNN Loss: 3.704362154006958 | BCE Loss: 1.0201128721237183\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.784204959869385 | KNN Loss: 3.7323029041290283 | BCE Loss: 1.0519020557403564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.708618640899658 | KNN Loss: 3.7068488597869873 | BCE Loss: 1.0017696619033813\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.709845066070557 | KNN Loss: 3.7064685821533203 | BCE Loss: 1.0033764839172363\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.74594783782959 | KNN Loss: 3.708810567855835 | BCE Loss: 1.0371373891830444\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.681051254272461 | KNN Loss: 3.6460604667663574 | BCE Loss: 1.0349905490875244\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.736692428588867 | KNN Loss: 3.7289984226226807 | BCE Loss: 1.0076942443847656\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.689509868621826 | KNN Loss: 3.6938366889953613 | BCE Loss: 0.9956731200218201\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.740659236907959 | KNN Loss: 3.6989645957946777 | BCE Loss: 1.0416946411132812\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.747907638549805 | KNN Loss: 3.700430154800415 | BCE Loss: 1.0474774837493896\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.743620872497559 | KNN Loss: 3.712712049484253 | BCE Loss: 1.0309088230133057\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.7218475341796875 | KNN Loss: 3.688972234725952 | BCE Loss: 1.0328751802444458\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.688867568969727 | KNN Loss: 3.679605722427368 | BCE Loss: 1.0092620849609375\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.645279407501221 | KNN Loss: 3.6600968837738037 | BCE Loss: 0.9851827025413513\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.729984283447266 | KNN Loss: 3.686129570007324 | BCE Loss: 1.0438544750213623\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.733918190002441 | KNN Loss: 3.691835403442383 | BCE Loss: 1.0420830249786377\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.702991485595703 | KNN Loss: 3.6651451587677 | BCE Loss: 1.037846565246582\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.728194713592529 | KNN Loss: 3.7112228870391846 | BCE Loss: 1.0169717073440552\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.746651649475098 | KNN Loss: 3.7086093425750732 | BCE Loss: 1.0380420684814453\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.780153751373291 | KNN Loss: 3.75163197517395 | BCE Loss: 1.0285216569900513\n",
      "Epoch   302: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.778881549835205 | KNN Loss: 3.7484982013702393 | BCE Loss: 1.0303833484649658\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.751438140869141 | KNN Loss: 3.709442138671875 | BCE Loss: 1.0419957637786865\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.780723571777344 | KNN Loss: 3.7488622665405273 | BCE Loss: 1.0318613052368164\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.743746280670166 | KNN Loss: 3.7117276191711426 | BCE Loss: 1.0320185422897339\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.742491245269775 | KNN Loss: 3.713090658187866 | BCE Loss: 1.0294004678726196\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.676626205444336 | KNN Loss: 3.6737253665924072 | BCE Loss: 1.0029007196426392\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.718633651733398 | KNN Loss: 3.6819965839385986 | BCE Loss: 1.0366371870040894\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.7559099197387695 | KNN Loss: 3.7344043254852295 | BCE Loss: 1.0215058326721191\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.770166397094727 | KNN Loss: 3.74774169921875 | BCE Loss: 1.0224249362945557\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.7099995613098145 | KNN Loss: 3.6910691261291504 | BCE Loss: 1.0189303159713745\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.692450523376465 | KNN Loss: 3.681224822998047 | BCE Loss: 1.0112258195877075\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.663870811462402 | KNN Loss: 3.6485002040863037 | BCE Loss: 1.0153703689575195\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.698421955108643 | KNN Loss: 3.689133405685425 | BCE Loss: 1.0092886686325073\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.679720401763916 | KNN Loss: 3.6539039611816406 | BCE Loss: 1.0258164405822754\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.701748371124268 | KNN Loss: 3.677964210510254 | BCE Loss: 1.0237841606140137\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.709036350250244 | KNN Loss: 3.6733264923095703 | BCE Loss: 1.0357099771499634\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.730324745178223 | KNN Loss: 3.693190336227417 | BCE Loss: 1.0371341705322266\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.681332588195801 | KNN Loss: 3.691895008087158 | BCE Loss: 0.989437460899353\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.703447341918945 | KNN Loss: 3.6820218563079834 | BCE Loss: 1.021425485610962\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.7461442947387695 | KNN Loss: 3.6994576454162598 | BCE Loss: 1.0466866493225098\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.713869094848633 | KNN Loss: 3.6899521350860596 | BCE Loss: 1.0239169597625732\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.69811487197876 | KNN Loss: 3.676914691925049 | BCE Loss: 1.021200180053711\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.69651985168457 | KNN Loss: 3.6722300052642822 | BCE Loss: 1.024289608001709\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.698429584503174 | KNN Loss: 3.6743083000183105 | BCE Loss: 1.0241212844848633\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.754699230194092 | KNN Loss: 3.7149760723114014 | BCE Loss: 1.0397231578826904\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.690581798553467 | KNN Loss: 3.671617269515991 | BCE Loss: 1.0189645290374756\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.72836971282959 | KNN Loss: 3.6949353218078613 | BCE Loss: 1.0334343910217285\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.668229103088379 | KNN Loss: 3.6663973331451416 | BCE Loss: 1.0018316507339478\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.710281848907471 | KNN Loss: 3.6606061458587646 | BCE Loss: 1.0496755838394165\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.716853141784668 | KNN Loss: 3.6893208026885986 | BCE Loss: 1.0275321006774902\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.745248794555664 | KNN Loss: 3.7143778800964355 | BCE Loss: 1.0308706760406494\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.712753772735596 | KNN Loss: 3.6942358016967773 | BCE Loss: 1.0185179710388184\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.731350421905518 | KNN Loss: 3.6864237785339355 | BCE Loss: 1.044926643371582\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.766839981079102 | KNN Loss: 3.7181291580200195 | BCE Loss: 1.048710823059082\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.684333801269531 | KNN Loss: 3.6752641201019287 | BCE Loss: 1.0090694427490234\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.740184783935547 | KNN Loss: 3.708218574523926 | BCE Loss: 1.031965970993042\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.695789337158203 | KNN Loss: 3.677914619445801 | BCE Loss: 1.0178749561309814\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.78145170211792 | KNN Loss: 3.7110753059387207 | BCE Loss: 1.0703765153884888\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.728693962097168 | KNN Loss: 3.693552255630493 | BCE Loss: 1.0351417064666748\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.707521915435791 | KNN Loss: 3.6898579597473145 | BCE Loss: 1.0176639556884766\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.732602119445801 | KNN Loss: 3.691972255706787 | BCE Loss: 1.0406297445297241\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.674031734466553 | KNN Loss: 3.6646456718444824 | BCE Loss: 1.0093860626220703\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.700911045074463 | KNN Loss: 3.6836659908294678 | BCE Loss: 1.0172450542449951\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.748643398284912 | KNN Loss: 3.7193052768707275 | BCE Loss: 1.0293382406234741\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.731071949005127 | KNN Loss: 3.7138826847076416 | BCE Loss: 1.017189383506775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.73975133895874 | KNN Loss: 3.715670108795166 | BCE Loss: 1.0240812301635742\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.695099830627441 | KNN Loss: 3.692857027053833 | BCE Loss: 1.0022426843643188\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.7131476402282715 | KNN Loss: 3.6822092533111572 | BCE Loss: 1.0309383869171143\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.69252872467041 | KNN Loss: 3.669090747833252 | BCE Loss: 1.0234382152557373\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.71948766708374 | KNN Loss: 3.657866954803467 | BCE Loss: 1.061620831489563\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.772586822509766 | KNN Loss: 3.7172834873199463 | BCE Loss: 1.0553035736083984\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.681685447692871 | KNN Loss: 3.6680397987365723 | BCE Loss: 1.013645887374878\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.675919055938721 | KNN Loss: 3.6670525074005127 | BCE Loss: 1.0088664293289185\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.714100360870361 | KNN Loss: 3.6921467781066895 | BCE Loss: 1.0219535827636719\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.728029727935791 | KNN Loss: 3.7066874504089355 | BCE Loss: 1.021342396736145\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.686312675476074 | KNN Loss: 3.658320426940918 | BCE Loss: 1.0279923677444458\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.727357387542725 | KNN Loss: 3.687588691711426 | BCE Loss: 1.0397686958312988\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.70052433013916 | KNN Loss: 3.663046360015869 | BCE Loss: 1.037477970123291\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.709965229034424 | KNN Loss: 3.679234266281128 | BCE Loss: 1.030730962753296\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.749905586242676 | KNN Loss: 3.7357301712036133 | BCE Loss: 1.0141756534576416\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.717617034912109 | KNN Loss: 3.6978442668914795 | BCE Loss: 1.0197725296020508\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.731931686401367 | KNN Loss: 3.6871767044067383 | BCE Loss: 1.044755220413208\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.714461326599121 | KNN Loss: 3.699721097946167 | BCE Loss: 1.014739990234375\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.681892395019531 | KNN Loss: 3.684769630432129 | BCE Loss: 0.9971227049827576\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.630285739898682 | KNN Loss: 3.6556849479675293 | BCE Loss: 0.9746006727218628\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.711325645446777 | KNN Loss: 3.6820902824401855 | BCE Loss: 1.029235601425171\n",
      "Epoch   313: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.745567798614502 | KNN Loss: 3.714397430419922 | BCE Loss: 1.03117036819458\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.710567474365234 | KNN Loss: 3.701594114303589 | BCE Loss: 1.008973479270935\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.739859104156494 | KNN Loss: 3.697366237640381 | BCE Loss: 1.0424927473068237\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.734969139099121 | KNN Loss: 3.707379102706909 | BCE Loss: 1.0275897979736328\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.714310646057129 | KNN Loss: 3.69516658782959 | BCE Loss: 1.01914381980896\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.710072994232178 | KNN Loss: 3.7043309211730957 | BCE Loss: 1.0057419538497925\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.713751792907715 | KNN Loss: 3.690232038497925 | BCE Loss: 1.0235196352005005\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.655363082885742 | KNN Loss: 3.6529040336608887 | BCE Loss: 1.002458930015564\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.724796772003174 | KNN Loss: 3.69596791267395 | BCE Loss: 1.028828740119934\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.71298360824585 | KNN Loss: 3.6965177059173584 | BCE Loss: 1.0164659023284912\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.758661270141602 | KNN Loss: 3.7271761894226074 | BCE Loss: 1.0314850807189941\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.693098068237305 | KNN Loss: 3.6638944149017334 | BCE Loss: 1.0292034149169922\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.722812652587891 | KNN Loss: 3.708047866821289 | BCE Loss: 1.0147645473480225\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.7487897872924805 | KNN Loss: 3.7240028381347656 | BCE Loss: 1.024787187576294\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.742920875549316 | KNN Loss: 3.715444564819336 | BCE Loss: 1.0274765491485596\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.69447135925293 | KNN Loss: 3.683685064315796 | BCE Loss: 1.0107862949371338\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.754525184631348 | KNN Loss: 3.72001576423645 | BCE Loss: 1.034509301185608\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.71190881729126 | KNN Loss: 3.676098108291626 | BCE Loss: 1.0358108282089233\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.674280643463135 | KNN Loss: 3.669930934906006 | BCE Loss: 1.0043498277664185\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.710269451141357 | KNN Loss: 3.704118251800537 | BCE Loss: 1.0061510801315308\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.6964335441589355 | KNN Loss: 3.676543712615967 | BCE Loss: 1.0198897123336792\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.702666282653809 | KNN Loss: 3.671584129333496 | BCE Loss: 1.0310819149017334\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.690742492675781 | KNN Loss: 3.669076919555664 | BCE Loss: 1.0216658115386963\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.70709228515625 | KNN Loss: 3.678267002105713 | BCE Loss: 1.028825283050537\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.706165313720703 | KNN Loss: 3.677703857421875 | BCE Loss: 1.0284616947174072\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.738334655761719 | KNN Loss: 3.717017889022827 | BCE Loss: 1.0213165283203125\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.7196502685546875 | KNN Loss: 3.706041097640991 | BCE Loss: 1.0136094093322754\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.698028564453125 | KNN Loss: 3.6519153118133545 | BCE Loss: 1.0461134910583496\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.719095706939697 | KNN Loss: 3.6914377212524414 | BCE Loss: 1.0276581048965454\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.702053070068359 | KNN Loss: 3.680952548980713 | BCE Loss: 1.021100640296936\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.742079257965088 | KNN Loss: 3.7383806705474854 | BCE Loss: 1.003698706626892\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.720917701721191 | KNN Loss: 3.695619583129883 | BCE Loss: 1.0252978801727295\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.729252815246582 | KNN Loss: 3.705605983734131 | BCE Loss: 1.0236467123031616\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.722224235534668 | KNN Loss: 3.7002711296081543 | BCE Loss: 1.0219528675079346\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.730792999267578 | KNN Loss: 3.7080800533294678 | BCE Loss: 1.0227131843566895\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.7398176193237305 | KNN Loss: 3.6812334060668945 | BCE Loss: 1.0585840940475464\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.718488693237305 | KNN Loss: 3.6692252159118652 | BCE Loss: 1.049263596534729\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.647272109985352 | KNN Loss: 3.644531488418579 | BCE Loss: 1.002740740776062\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.711552619934082 | KNN Loss: 3.678676128387451 | BCE Loss: 1.0328764915466309\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.717213153839111 | KNN Loss: 3.704501152038574 | BCE Loss: 1.012712001800537\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.691740989685059 | KNN Loss: 3.6848440170288086 | BCE Loss: 1.00689697265625\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.774745941162109 | KNN Loss: 3.7575504779815674 | BCE Loss: 1.0171955823898315\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.720959663391113 | KNN Loss: 3.7076058387756348 | BCE Loss: 1.0133540630340576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.730607986450195 | KNN Loss: 3.6972365379333496 | BCE Loss: 1.0333714485168457\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.660962104797363 | KNN Loss: 3.6573567390441895 | BCE Loss: 1.0036052465438843\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.693453788757324 | KNN Loss: 3.6762590408325195 | BCE Loss: 1.0171947479248047\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.739112854003906 | KNN Loss: 3.7203009128570557 | BCE Loss: 1.0188121795654297\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.717229843139648 | KNN Loss: 3.6764776706695557 | BCE Loss: 1.0407520532608032\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.728982925415039 | KNN Loss: 3.686455249786377 | BCE Loss: 1.042527437210083\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.708450794219971 | KNN Loss: 3.6810147762298584 | BCE Loss: 1.0274360179901123\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.683122634887695 | KNN Loss: 3.6621155738830566 | BCE Loss: 1.0210069417953491\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.6815690994262695 | KNN Loss: 3.6751716136932373 | BCE Loss: 1.0063974857330322\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.671838283538818 | KNN Loss: 3.6647307872772217 | BCE Loss: 1.0071076154708862\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.7260541915893555 | KNN Loss: 3.7090258598327637 | BCE Loss: 1.0170282125473022\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.7350239753723145 | KNN Loss: 3.677800416946411 | BCE Loss: 1.0572236776351929\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.744457244873047 | KNN Loss: 3.7117507457733154 | BCE Loss: 1.0327064990997314\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.670694351196289 | KNN Loss: 3.668437957763672 | BCE Loss: 1.0022562742233276\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.6843767166137695 | KNN Loss: 3.678691864013672 | BCE Loss: 1.0056850910186768\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.743434906005859 | KNN Loss: 3.6958117485046387 | BCE Loss: 1.0476230382919312\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.686549186706543 | KNN Loss: 3.6696038246154785 | BCE Loss: 1.016945481300354\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.742914199829102 | KNN Loss: 3.7205610275268555 | BCE Loss: 1.0223534107208252\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.718011856079102 | KNN Loss: 3.6922101974487305 | BCE Loss: 1.0258018970489502\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.731418609619141 | KNN Loss: 3.682178258895874 | BCE Loss: 1.0492403507232666\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.716875076293945 | KNN Loss: 3.6892523765563965 | BCE Loss: 1.0276225805282593\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.751891613006592 | KNN Loss: 3.7279551029205322 | BCE Loss: 1.02393639087677\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.824697971343994 | KNN Loss: 3.766362190246582 | BCE Loss: 1.0583359003067017\n",
      "Epoch   324: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.744198799133301 | KNN Loss: 3.722590923309326 | BCE Loss: 1.0216076374053955\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.685129165649414 | KNN Loss: 3.6550345420837402 | BCE Loss: 1.030094861984253\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.7061567306518555 | KNN Loss: 3.698688268661499 | BCE Loss: 1.0074684619903564\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.68725061416626 | KNN Loss: 3.663102388381958 | BCE Loss: 1.0241482257843018\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.6873884201049805 | KNN Loss: 3.6498801708221436 | BCE Loss: 1.0375083684921265\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.701635837554932 | KNN Loss: 3.6922457218170166 | BCE Loss: 1.009390115737915\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.756589412689209 | KNN Loss: 3.7001006603240967 | BCE Loss: 1.0564886331558228\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.699018478393555 | KNN Loss: 3.681103467941284 | BCE Loss: 1.0179147720336914\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.724794387817383 | KNN Loss: 3.699972152709961 | BCE Loss: 1.0248223543167114\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.713018417358398 | KNN Loss: 3.687396764755249 | BCE Loss: 1.0256216526031494\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.696227073669434 | KNN Loss: 3.6651785373687744 | BCE Loss: 1.0310485363006592\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.751349925994873 | KNN Loss: 3.7219595909118652 | BCE Loss: 1.0293903350830078\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.725646018981934 | KNN Loss: 3.6856110095977783 | BCE Loss: 1.0400347709655762\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.711050987243652 | KNN Loss: 3.6970293521881104 | BCE Loss: 1.014021396636963\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.669294357299805 | KNN Loss: 3.6565322875976562 | BCE Loss: 1.012762188911438\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.744572639465332 | KNN Loss: 3.6889801025390625 | BCE Loss: 1.0555927753448486\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.72276496887207 | KNN Loss: 3.7141754627227783 | BCE Loss: 1.008589506149292\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.7646870613098145 | KNN Loss: 3.7362000942230225 | BCE Loss: 1.0284870862960815\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.68386173248291 | KNN Loss: 3.6570515632629395 | BCE Loss: 1.0268104076385498\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.721659183502197 | KNN Loss: 3.6884467601776123 | BCE Loss: 1.0332125425338745\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.786601543426514 | KNN Loss: 3.7549967765808105 | BCE Loss: 1.0316047668457031\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.73994779586792 | KNN Loss: 3.6927645206451416 | BCE Loss: 1.0471832752227783\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.726251602172852 | KNN Loss: 3.6790804862976074 | BCE Loss: 1.0471712350845337\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.694711685180664 | KNN Loss: 3.6641101837158203 | BCE Loss: 1.0306015014648438\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.708150863647461 | KNN Loss: 3.684501886367798 | BCE Loss: 1.0236492156982422\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.742892265319824 | KNN Loss: 3.6910359859466553 | BCE Loss: 1.051856517791748\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.710792064666748 | KNN Loss: 3.6694881916046143 | BCE Loss: 1.0413037538528442\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.700967311859131 | KNN Loss: 3.7130918502807617 | BCE Loss: 0.9878755807876587\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.7036919593811035 | KNN Loss: 3.698866128921509 | BCE Loss: 1.0048257112503052\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.706926345825195 | KNN Loss: 3.694754123687744 | BCE Loss: 1.0121721029281616\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.761902809143066 | KNN Loss: 3.7120978832244873 | BCE Loss: 1.0498046875\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.727996349334717 | KNN Loss: 3.705936908721924 | BCE Loss: 1.022059440612793\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.725430965423584 | KNN Loss: 3.7022533416748047 | BCE Loss: 1.0231776237487793\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.669250011444092 | KNN Loss: 3.6442394256591797 | BCE Loss: 1.0250104665756226\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.719451904296875 | KNN Loss: 3.6791744232177734 | BCE Loss: 1.0402776002883911\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.763448238372803 | KNN Loss: 3.7279205322265625 | BCE Loss: 1.0355277061462402\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.755765914916992 | KNN Loss: 3.717850685119629 | BCE Loss: 1.0379152297973633\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.683048725128174 | KNN Loss: 3.6584479808807373 | BCE Loss: 1.024600863456726\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.703629493713379 | KNN Loss: 3.685824155807495 | BCE Loss: 1.017805576324463\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.699366569519043 | KNN Loss: 3.6671931743621826 | BCE Loss: 1.0321731567382812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.647955417633057 | KNN Loss: 3.661982536315918 | BCE Loss: 0.985973060131073\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.687281608581543 | KNN Loss: 3.673563241958618 | BCE Loss: 1.0137184858322144\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.708765983581543 | KNN Loss: 3.6791622638702393 | BCE Loss: 1.0296039581298828\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.716582775115967 | KNN Loss: 3.6822543144226074 | BCE Loss: 1.034328579902649\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.731959342956543 | KNN Loss: 3.7215702533721924 | BCE Loss: 1.0103893280029297\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.6975274085998535 | KNN Loss: 3.707078218460083 | BCE Loss: 0.9904493093490601\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.734625339508057 | KNN Loss: 3.6975345611572266 | BCE Loss: 1.03709077835083\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.687020301818848 | KNN Loss: 3.676769495010376 | BCE Loss: 1.0102505683898926\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.711452484130859 | KNN Loss: 3.7012531757354736 | BCE Loss: 1.0101995468139648\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.670698642730713 | KNN Loss: 3.6558635234832764 | BCE Loss: 1.0148351192474365\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.688518047332764 | KNN Loss: 3.686915874481201 | BCE Loss: 1.0016021728515625\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.7627458572387695 | KNN Loss: 3.721562147140503 | BCE Loss: 1.0411839485168457\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.68719482421875 | KNN Loss: 3.664987564086914 | BCE Loss: 1.0222070217132568\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.73490571975708 | KNN Loss: 3.6801531314849854 | BCE Loss: 1.0547525882720947\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.754507064819336 | KNN Loss: 3.720823287963867 | BCE Loss: 1.0336840152740479\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.7319231033325195 | KNN Loss: 3.712956428527832 | BCE Loss: 1.018966555595398\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.682369232177734 | KNN Loss: 3.6676151752471924 | BCE Loss: 1.014753818511963\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.706571578979492 | KNN Loss: 3.6777830123901367 | BCE Loss: 1.0287883281707764\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.721384525299072 | KNN Loss: 3.7036898136138916 | BCE Loss: 1.0176947116851807\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.698451519012451 | KNN Loss: 3.6718077659606934 | BCE Loss: 1.0266437530517578\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.751967430114746 | KNN Loss: 3.728449583053589 | BCE Loss: 1.0235180854797363\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.739128589630127 | KNN Loss: 3.7002878189086914 | BCE Loss: 1.0388407707214355\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.703293323516846 | KNN Loss: 3.690690279006958 | BCE Loss: 1.0126030445098877\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.746461391448975 | KNN Loss: 3.7039878368377686 | BCE Loss: 1.0424734354019165\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.744848251342773 | KNN Loss: 3.699463367462158 | BCE Loss: 1.0453848838806152\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.792963981628418 | KNN Loss: 3.7584173679351807 | BCE Loss: 1.0345463752746582\n",
      "Epoch   335: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.7035627365112305 | KNN Loss: 3.671501636505127 | BCE Loss: 1.032061219215393\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.763479232788086 | KNN Loss: 3.7123029232025146 | BCE Loss: 1.0511760711669922\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.738240718841553 | KNN Loss: 3.6908130645751953 | BCE Loss: 1.0474276542663574\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.74021053314209 | KNN Loss: 3.717897415161133 | BCE Loss: 1.0223133563995361\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.685050010681152 | KNN Loss: 3.66169810295105 | BCE Loss: 1.0233521461486816\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.682466506958008 | KNN Loss: 3.695026159286499 | BCE Loss: 0.9874405264854431\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.7045512199401855 | KNN Loss: 3.690190315246582 | BCE Loss: 1.0143609046936035\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.740176200866699 | KNN Loss: 3.6970770359039307 | BCE Loss: 1.043099045753479\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.745790958404541 | KNN Loss: 3.7003326416015625 | BCE Loss: 1.045458436012268\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.737648963928223 | KNN Loss: 3.6944162845611572 | BCE Loss: 1.0432324409484863\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.694875240325928 | KNN Loss: 3.710905075073242 | BCE Loss: 0.9839701652526855\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.709067344665527 | KNN Loss: 3.6946651935577393 | BCE Loss: 1.0144020318984985\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.734421253204346 | KNN Loss: 3.7057993412017822 | BCE Loss: 1.0286219120025635\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.7040205001831055 | KNN Loss: 3.6631381511688232 | BCE Loss: 1.0408823490142822\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.813212871551514 | KNN Loss: 3.760953903198242 | BCE Loss: 1.0522589683532715\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.696361541748047 | KNN Loss: 3.6778669357299805 | BCE Loss: 1.018494725227356\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.723372459411621 | KNN Loss: 3.685621738433838 | BCE Loss: 1.037750482559204\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.6865339279174805 | KNN Loss: 3.6686809062957764 | BCE Loss: 1.017853021621704\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.683361530303955 | KNN Loss: 3.6769661903381348 | BCE Loss: 1.0063953399658203\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.693716049194336 | KNN Loss: 3.6830859184265137 | BCE Loss: 1.0106301307678223\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.702507019042969 | KNN Loss: 3.678823947906494 | BCE Loss: 1.0236828327178955\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.721190452575684 | KNN Loss: 3.704434394836426 | BCE Loss: 1.0167558193206787\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.710390090942383 | KNN Loss: 3.688013792037964 | BCE Loss: 1.0223760604858398\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.714847564697266 | KNN Loss: 3.6844840049743652 | BCE Loss: 1.0303633213043213\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.7066755294799805 | KNN Loss: 3.70137095451355 | BCE Loss: 1.0053045749664307\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.744655609130859 | KNN Loss: 3.710829734802246 | BCE Loss: 1.0338258743286133\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.7416791915893555 | KNN Loss: 3.7171452045440674 | BCE Loss: 1.024533748626709\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.71372127532959 | KNN Loss: 3.6978015899658203 | BCE Loss: 1.0159194469451904\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.72176456451416 | KNN Loss: 3.6863303184509277 | BCE Loss: 1.0354340076446533\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.69307804107666 | KNN Loss: 3.694866418838501 | BCE Loss: 0.9982118606567383\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.710583209991455 | KNN Loss: 3.704362630844116 | BCE Loss: 1.0062204599380493\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.691751480102539 | KNN Loss: 3.687784194946289 | BCE Loss: 1.00396728515625\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.714438438415527 | KNN Loss: 3.701167106628418 | BCE Loss: 1.0132715702056885\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.730813026428223 | KNN Loss: 3.674604892730713 | BCE Loss: 1.0562078952789307\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.751076698303223 | KNN Loss: 3.721578598022461 | BCE Loss: 1.0294978618621826\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.662981033325195 | KNN Loss: 3.6621651649475098 | BCE Loss: 1.0008156299591064\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.710829734802246 | KNN Loss: 3.7007858753204346 | BCE Loss: 1.0100438594818115\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.740065574645996 | KNN Loss: 3.683305025100708 | BCE Loss: 1.0567607879638672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.761072158813477 | KNN Loss: 3.7229440212249756 | BCE Loss: 1.0381282567977905\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.707817077636719 | KNN Loss: 3.6950271129608154 | BCE Loss: 1.0127899646759033\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.721379280090332 | KNN Loss: 3.6951565742492676 | BCE Loss: 1.0262227058410645\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.764959335327148 | KNN Loss: 3.731513261795044 | BCE Loss: 1.033445954322815\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.691410064697266 | KNN Loss: 3.687889575958252 | BCE Loss: 1.0035207271575928\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.759422302246094 | KNN Loss: 3.7196271419525146 | BCE Loss: 1.039794921875\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.680342674255371 | KNN Loss: 3.6862599849700928 | BCE Loss: 0.9940825700759888\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.707197189331055 | KNN Loss: 3.6826043128967285 | BCE Loss: 1.0245928764343262\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.748019218444824 | KNN Loss: 3.739941358566284 | BCE Loss: 1.00807785987854\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.673135757446289 | KNN Loss: 3.6608059406280518 | BCE Loss: 1.0123295783996582\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.700181007385254 | KNN Loss: 3.678351879119873 | BCE Loss: 1.0218288898468018\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.715324401855469 | KNN Loss: 3.6816389560699463 | BCE Loss: 1.0336854457855225\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.702658653259277 | KNN Loss: 3.6634860038757324 | BCE Loss: 1.039172887802124\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.725492477416992 | KNN Loss: 3.719020128250122 | BCE Loss: 1.006472110748291\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.75132417678833 | KNN Loss: 3.6900839805603027 | BCE Loss: 1.0612401962280273\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.718463897705078 | KNN Loss: 3.689547061920166 | BCE Loss: 1.028916835784912\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.707272529602051 | KNN Loss: 3.688122034072876 | BCE Loss: 1.0191502571105957\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.740060806274414 | KNN Loss: 3.69632887840271 | BCE Loss: 1.043731689453125\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.702694416046143 | KNN Loss: 3.700615882873535 | BCE Loss: 1.0020785331726074\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.709539890289307 | KNN Loss: 3.715376377105713 | BCE Loss: 0.9941633343696594\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.768465995788574 | KNN Loss: 3.7409863471984863 | BCE Loss: 1.027479648590088\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.706398963928223 | KNN Loss: 3.672713041305542 | BCE Loss: 1.0336861610412598\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.728830337524414 | KNN Loss: 3.7174155712127686 | BCE Loss: 1.0114150047302246\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.65001916885376 | KNN Loss: 3.6440398693084717 | BCE Loss: 1.005979299545288\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.760612487792969 | KNN Loss: 3.7185020446777344 | BCE Loss: 1.0421103239059448\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.682380199432373 | KNN Loss: 3.6920406818389893 | BCE Loss: 0.9903395175933838\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.767398834228516 | KNN Loss: 3.7301244735717773 | BCE Loss: 1.0372743606567383\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.730483531951904 | KNN Loss: 3.6927120685577393 | BCE Loss: 1.0377715826034546\n",
      "Epoch   346: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.684413909912109 | KNN Loss: 3.693575143814087 | BCE Loss: 0.9908387660980225\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.7103776931762695 | KNN Loss: 3.7016615867614746 | BCE Loss: 1.008716344833374\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.71159029006958 | KNN Loss: 3.690215587615967 | BCE Loss: 1.0213747024536133\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.717071533203125 | KNN Loss: 3.689791202545166 | BCE Loss: 1.027280330657959\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.7102766036987305 | KNN Loss: 3.709235429763794 | BCE Loss: 1.0010409355163574\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.734783172607422 | KNN Loss: 3.703622579574585 | BCE Loss: 1.031160593032837\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.681312084197998 | KNN Loss: 3.666078567504883 | BCE Loss: 1.0152336359024048\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.758466720581055 | KNN Loss: 3.7104010581970215 | BCE Loss: 1.0480659008026123\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.675594329833984 | KNN Loss: 3.6700992584228516 | BCE Loss: 1.0054950714111328\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.769662857055664 | KNN Loss: 3.712040662765503 | BCE Loss: 1.057621955871582\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.702358245849609 | KNN Loss: 3.680715560913086 | BCE Loss: 1.0216426849365234\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.733003616333008 | KNN Loss: 3.7004430294036865 | BCE Loss: 1.0325604677200317\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.674208641052246 | KNN Loss: 3.6580772399902344 | BCE Loss: 1.0161311626434326\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.703037261962891 | KNN Loss: 3.6646342277526855 | BCE Loss: 1.038403034210205\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.728206157684326 | KNN Loss: 3.6941986083984375 | BCE Loss: 1.0340075492858887\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.771120548248291 | KNN Loss: 3.730088472366333 | BCE Loss: 1.0410321950912476\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.7393341064453125 | KNN Loss: 3.7028868198394775 | BCE Loss: 1.036447286605835\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.696352958679199 | KNN Loss: 3.68245005607605 | BCE Loss: 1.0139031410217285\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.7083516120910645 | KNN Loss: 3.6777799129486084 | BCE Loss: 1.030571699142456\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.689236164093018 | KNN Loss: 3.682722568511963 | BCE Loss: 1.0065134763717651\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.695990562438965 | KNN Loss: 3.685678482055664 | BCE Loss: 1.0103123188018799\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.718469619750977 | KNN Loss: 3.6907694339752197 | BCE Loss: 1.0277001857757568\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.704709529876709 | KNN Loss: 3.6897928714752197 | BCE Loss: 1.0149166584014893\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.726674556732178 | KNN Loss: 3.67514967918396 | BCE Loss: 1.0515247583389282\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.742603778839111 | KNN Loss: 3.7017979621887207 | BCE Loss: 1.0408058166503906\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.755189895629883 | KNN Loss: 3.6933014392852783 | BCE Loss: 1.061888575553894\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.721086025238037 | KNN Loss: 3.6709256172180176 | BCE Loss: 1.0501604080200195\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.685521125793457 | KNN Loss: 3.666943073272705 | BCE Loss: 1.018578052520752\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.712100982666016 | KNN Loss: 3.7044179439544678 | BCE Loss: 1.007683277130127\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.724740982055664 | KNN Loss: 3.7033004760742188 | BCE Loss: 1.0214405059814453\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.698217391967773 | KNN Loss: 3.6626837253570557 | BCE Loss: 1.0355336666107178\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.722248554229736 | KNN Loss: 3.7045605182647705 | BCE Loss: 1.0176880359649658\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.694768905639648 | KNN Loss: 3.676239013671875 | BCE Loss: 1.0185296535491943\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.703474044799805 | KNN Loss: 3.6991934776306152 | BCE Loss: 1.0042804479599\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.721631050109863 | KNN Loss: 3.6652300357818604 | BCE Loss: 1.056401252746582\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.702986717224121 | KNN Loss: 3.6658308506011963 | BCE Loss: 1.0371556282043457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.727409362792969 | KNN Loss: 3.696769952774048 | BCE Loss: 1.030639410018921\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.745514869689941 | KNN Loss: 3.717909812927246 | BCE Loss: 1.0276048183441162\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.725104808807373 | KNN Loss: 3.6868364810943604 | BCE Loss: 1.0382683277130127\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.6664652824401855 | KNN Loss: 3.6726791858673096 | BCE Loss: 0.9937859773635864\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.759120464324951 | KNN Loss: 3.714693069458008 | BCE Loss: 1.0444272756576538\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.744544982910156 | KNN Loss: 3.7120320796966553 | BCE Loss: 1.03251314163208\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.697330474853516 | KNN Loss: 3.6638405323028564 | BCE Loss: 1.0334899425506592\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.731172561645508 | KNN Loss: 3.694321393966675 | BCE Loss: 1.036851167678833\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.667116641998291 | KNN Loss: 3.6757290363311768 | BCE Loss: 0.9913875460624695\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.720518589019775 | KNN Loss: 3.7051353454589844 | BCE Loss: 1.015383243560791\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.70797061920166 | KNN Loss: 3.6845781803131104 | BCE Loss: 1.023392677307129\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.691078186035156 | KNN Loss: 3.691352605819702 | BCE Loss: 0.999725341796875\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.677635192871094 | KNN Loss: 3.6670467853546143 | BCE Loss: 1.01058828830719\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.760880470275879 | KNN Loss: 3.712977886199951 | BCE Loss: 1.0479024648666382\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.746490478515625 | KNN Loss: 3.6889100074768066 | BCE Loss: 1.0575802326202393\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.693624496459961 | KNN Loss: 3.6850497722625732 | BCE Loss: 1.0085746049880981\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.738793849945068 | KNN Loss: 3.6977314949035645 | BCE Loss: 1.0410624742507935\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.755537033081055 | KNN Loss: 3.7242655754089355 | BCE Loss: 1.0312716960906982\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.744636535644531 | KNN Loss: 3.6900458335876465 | BCE Loss: 1.0545904636383057\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.720310211181641 | KNN Loss: 3.694007158279419 | BCE Loss: 1.0263030529022217\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.702713966369629 | KNN Loss: 3.6748576164245605 | BCE Loss: 1.0278563499450684\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.721887111663818 | KNN Loss: 3.691976547241211 | BCE Loss: 1.0299105644226074\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.7198920249938965 | KNN Loss: 3.711123466491699 | BCE Loss: 1.0087684392929077\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.711840629577637 | KNN Loss: 3.7020962238311768 | BCE Loss: 1.00974440574646\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.7871832847595215 | KNN Loss: 3.7478115558624268 | BCE Loss: 1.0393717288970947\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.7363715171813965 | KNN Loss: 3.695998191833496 | BCE Loss: 1.0403732061386108\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.710519790649414 | KNN Loss: 3.6903133392333984 | BCE Loss: 1.0202066898345947\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.7123589515686035 | KNN Loss: 3.6779980659484863 | BCE Loss: 1.0343608856201172\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.761937618255615 | KNN Loss: 3.714205741882324 | BCE Loss: 1.0477317571640015\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.715265274047852 | KNN Loss: 3.714164972305298 | BCE Loss: 1.0011005401611328\n",
      "Epoch   357: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.683643817901611 | KNN Loss: 3.6902835369110107 | BCE Loss: 0.9933603405952454\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.677593231201172 | KNN Loss: 3.6942713260650635 | BCE Loss: 0.9833220839500427\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.700347900390625 | KNN Loss: 3.663680076599121 | BCE Loss: 1.0366677045822144\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.72584342956543 | KNN Loss: 3.6824839115142822 | BCE Loss: 1.0433595180511475\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.687705039978027 | KNN Loss: 3.687147378921509 | BCE Loss: 1.000557780265808\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.829704284667969 | KNN Loss: 3.780231475830078 | BCE Loss: 1.0494729280471802\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.6942138671875 | KNN Loss: 3.6710398197174072 | BCE Loss: 1.0231739282608032\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.760558128356934 | KNN Loss: 3.711169481277466 | BCE Loss: 1.0493887662887573\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.728967189788818 | KNN Loss: 3.692478895187378 | BCE Loss: 1.0364882946014404\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.671508312225342 | KNN Loss: 3.6762661933898926 | BCE Loss: 0.9952419400215149\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.697103977203369 | KNN Loss: 3.6792078018188477 | BCE Loss: 1.017896294593811\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.755699634552002 | KNN Loss: 3.710319757461548 | BCE Loss: 1.0453799962997437\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.75890588760376 | KNN Loss: 3.733431339263916 | BCE Loss: 1.0254744291305542\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.728689193725586 | KNN Loss: 3.696302890777588 | BCE Loss: 1.032386302947998\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.689949989318848 | KNN Loss: 3.6708452701568604 | BCE Loss: 1.0191049575805664\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.728102684020996 | KNN Loss: 3.6844992637634277 | BCE Loss: 1.0436036586761475\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.731564521789551 | KNN Loss: 3.705345630645752 | BCE Loss: 1.0262188911437988\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.705455780029297 | KNN Loss: 3.704119920730591 | BCE Loss: 1.0013360977172852\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.722416400909424 | KNN Loss: 3.6762962341308594 | BCE Loss: 1.0461201667785645\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.696568489074707 | KNN Loss: 3.6930770874023438 | BCE Loss: 1.0034914016723633\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.684441566467285 | KNN Loss: 3.6815361976623535 | BCE Loss: 1.0029054880142212\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.676453113555908 | KNN Loss: 3.6695337295532227 | BCE Loss: 1.0069193840026855\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.706345558166504 | KNN Loss: 3.6710455417633057 | BCE Loss: 1.0352998971939087\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.747965335845947 | KNN Loss: 3.702702522277832 | BCE Loss: 1.0452628135681152\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.722546577453613 | KNN Loss: 3.7072622776031494 | BCE Loss: 1.0152840614318848\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.735907554626465 | KNN Loss: 3.7254104614257812 | BCE Loss: 1.0104968547821045\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.725834369659424 | KNN Loss: 3.689107656478882 | BCE Loss: 1.036726713180542\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.639523506164551 | KNN Loss: 3.639604330062866 | BCE Loss: 0.9999193549156189\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.757356643676758 | KNN Loss: 3.7217302322387695 | BCE Loss: 1.0356264114379883\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.717962265014648 | KNN Loss: 3.700627565383911 | BCE Loss: 1.0173345804214478\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.750336647033691 | KNN Loss: 3.722630023956299 | BCE Loss: 1.0277063846588135\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.687153339385986 | KNN Loss: 3.6661489009857178 | BCE Loss: 1.0210044384002686\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.744535446166992 | KNN Loss: 3.71048641204834 | BCE Loss: 1.0340490341186523\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.748632907867432 | KNN Loss: 3.7224438190460205 | BCE Loss: 1.0261890888214111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.7355546951293945 | KNN Loss: 3.690927743911743 | BCE Loss: 1.0446271896362305\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.736426830291748 | KNN Loss: 3.6933820247650146 | BCE Loss: 1.0430446863174438\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.707645893096924 | KNN Loss: 3.70054292678833 | BCE Loss: 1.0071029663085938\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.683151721954346 | KNN Loss: 3.6527087688446045 | BCE Loss: 1.0304430723190308\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.737252235412598 | KNN Loss: 3.7109155654907227 | BCE Loss: 1.026336431503296\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.796880722045898 | KNN Loss: 3.7512059211730957 | BCE Loss: 1.0456750392913818\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.676353454589844 | KNN Loss: 3.6539981365203857 | BCE Loss: 1.022355318069458\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.700132369995117 | KNN Loss: 3.6815578937530518 | BCE Loss: 1.0185744762420654\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.68783712387085 | KNN Loss: 3.6718640327453613 | BCE Loss: 1.0159729719161987\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.755026817321777 | KNN Loss: 3.7297542095184326 | BCE Loss: 1.0252724885940552\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.674652099609375 | KNN Loss: 3.657329797744751 | BCE Loss: 1.017322301864624\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.729283332824707 | KNN Loss: 3.707683563232422 | BCE Loss: 1.0215997695922852\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.736294746398926 | KNN Loss: 3.703144073486328 | BCE Loss: 1.0331504344940186\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.735772609710693 | KNN Loss: 3.6941006183624268 | BCE Loss: 1.0416719913482666\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.627871990203857 | KNN Loss: 3.6396596431732178 | BCE Loss: 0.9882124662399292\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.684868812561035 | KNN Loss: 3.6926209926605225 | BCE Loss: 0.9922477602958679\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.708599090576172 | KNN Loss: 3.688596487045288 | BCE Loss: 1.020002841949463\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.72172737121582 | KNN Loss: 3.6709840297698975 | BCE Loss: 1.050743579864502\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.673832416534424 | KNN Loss: 3.6626949310302734 | BCE Loss: 1.0111374855041504\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.697444438934326 | KNN Loss: 3.6587789058685303 | BCE Loss: 1.038665533065796\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.775177955627441 | KNN Loss: 3.73158597946167 | BCE Loss: 1.0435917377471924\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.657627105712891 | KNN Loss: 3.641781806945801 | BCE Loss: 1.0158452987670898\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.736751079559326 | KNN Loss: 3.697112798690796 | BCE Loss: 1.0396382808685303\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.660209655761719 | KNN Loss: 3.649367094039917 | BCE Loss: 1.0108426809310913\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.747626304626465 | KNN Loss: 3.735123634338379 | BCE Loss: 1.012502908706665\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.724127292633057 | KNN Loss: 3.7241387367248535 | BCE Loss: 0.9999884366989136\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.728523254394531 | KNN Loss: 3.6986002922058105 | BCE Loss: 1.0299227237701416\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.651981830596924 | KNN Loss: 3.647829055786133 | BCE Loss: 1.0041526556015015\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.6668243408203125 | KNN Loss: 3.6249094009399414 | BCE Loss: 1.041914939880371\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.687559127807617 | KNN Loss: 3.6853244304656982 | BCE Loss: 1.0022344589233398\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.710886478424072 | KNN Loss: 3.664921283721924 | BCE Loss: 1.045965313911438\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.7143964767456055 | KNN Loss: 3.7069525718688965 | BCE Loss: 1.007443904876709\n",
      "Epoch   368: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.662308692932129 | KNN Loss: 3.6523571014404297 | BCE Loss: 1.0099518299102783\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.703571319580078 | KNN Loss: 3.682239294052124 | BCE Loss: 1.021332025527954\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.767621994018555 | KNN Loss: 3.723323106765747 | BCE Loss: 1.0442986488342285\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.696747779846191 | KNN Loss: 3.663802146911621 | BCE Loss: 1.0329458713531494\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.756870269775391 | KNN Loss: 3.7246387004852295 | BCE Loss: 1.0322314500808716\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.752856254577637 | KNN Loss: 3.714543342590332 | BCE Loss: 1.0383129119873047\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.75417947769165 | KNN Loss: 3.7485013008117676 | BCE Loss: 1.0056780576705933\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.6830735206604 | KNN Loss: 3.66715669631958 | BCE Loss: 1.0159168243408203\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.715829372406006 | KNN Loss: 3.6876251697540283 | BCE Loss: 1.0282042026519775\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.714504718780518 | KNN Loss: 3.674590826034546 | BCE Loss: 1.0399140119552612\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.7348527908325195 | KNN Loss: 3.7074434757232666 | BCE Loss: 1.027409553527832\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.72871208190918 | KNN Loss: 3.6811177730560303 | BCE Loss: 1.047594428062439\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.66960334777832 | KNN Loss: 3.653921604156494 | BCE Loss: 1.0156816244125366\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.760839462280273 | KNN Loss: 3.7086617946624756 | BCE Loss: 1.0521774291992188\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.762754917144775 | KNN Loss: 3.7196757793426514 | BCE Loss: 1.043079137802124\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.681020736694336 | KNN Loss: 3.671175479888916 | BCE Loss: 1.0098450183868408\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.719085693359375 | KNN Loss: 3.7002694606781006 | BCE Loss: 1.0188164710998535\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.776051044464111 | KNN Loss: 3.740027904510498 | BCE Loss: 1.0360231399536133\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.6845293045043945 | KNN Loss: 3.6712911128997803 | BCE Loss: 1.0132379531860352\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.7398200035095215 | KNN Loss: 3.705146551132202 | BCE Loss: 1.0346734523773193\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.713730812072754 | KNN Loss: 3.687053680419922 | BCE Loss: 1.026676893234253\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.671712875366211 | KNN Loss: 3.67728328704834 | BCE Loss: 0.9944294691085815\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.724259853363037 | KNN Loss: 3.7053937911987305 | BCE Loss: 1.0188660621643066\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.721158027648926 | KNN Loss: 3.7186481952667236 | BCE Loss: 1.0025097131729126\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.677482604980469 | KNN Loss: 3.6685147285461426 | BCE Loss: 1.0089681148529053\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.7413740158081055 | KNN Loss: 3.7178187370300293 | BCE Loss: 1.0235552787780762\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.690338611602783 | KNN Loss: 3.664548635482788 | BCE Loss: 1.0257899761199951\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.753178596496582 | KNN Loss: 3.7355165481567383 | BCE Loss: 1.0176621675491333\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.684417724609375 | KNN Loss: 3.6759531497955322 | BCE Loss: 1.0084643363952637\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.7027587890625 | KNN Loss: 3.679622173309326 | BCE Loss: 1.023136854171753\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.741225242614746 | KNN Loss: 3.6889290809631348 | BCE Loss: 1.0522962808609009\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.659928321838379 | KNN Loss: 3.665492296218872 | BCE Loss: 0.9944358468055725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.703348159790039 | KNN Loss: 3.6668667793273926 | BCE Loss: 1.0364813804626465\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.722272872924805 | KNN Loss: 3.7025625705718994 | BCE Loss: 1.0197105407714844\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.696122646331787 | KNN Loss: 3.6575300693511963 | BCE Loss: 1.0385925769805908\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.72933292388916 | KNN Loss: 3.7062461376190186 | BCE Loss: 1.0230870246887207\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.753696918487549 | KNN Loss: 3.732390880584717 | BCE Loss: 1.021306037902832\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.737729072570801 | KNN Loss: 3.699394941329956 | BCE Loss: 1.0383341312408447\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.7061872482299805 | KNN Loss: 3.680590867996216 | BCE Loss: 1.0255961418151855\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.765904903411865 | KNN Loss: 3.7431998252868652 | BCE Loss: 1.0227051973342896\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.708181381225586 | KNN Loss: 3.6879796981811523 | BCE Loss: 1.0202019214630127\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.687906265258789 | KNN Loss: 3.6703569889068604 | BCE Loss: 1.0175493955612183\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.7101874351501465 | KNN Loss: 3.687631368637085 | BCE Loss: 1.0225560665130615\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.711216926574707 | KNN Loss: 3.693293571472168 | BCE Loss: 1.0179232358932495\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.698251247406006 | KNN Loss: 3.6811323165893555 | BCE Loss: 1.0171189308166504\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.748940467834473 | KNN Loss: 3.7093048095703125 | BCE Loss: 1.0396355390548706\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.743896961212158 | KNN Loss: 3.6991641521453857 | BCE Loss: 1.044732689857483\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.67381477355957 | KNN Loss: 3.6631014347076416 | BCE Loss: 1.0107131004333496\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.726545333862305 | KNN Loss: 3.711076259613037 | BCE Loss: 1.015468955039978\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.738763809204102 | KNN Loss: 3.6939587593078613 | BCE Loss: 1.0448050498962402\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.668307781219482 | KNN Loss: 3.654127597808838 | BCE Loss: 1.014180064201355\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.706500053405762 | KNN Loss: 3.682742118835449 | BCE Loss: 1.0237581729888916\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.707648277282715 | KNN Loss: 3.6624386310577393 | BCE Loss: 1.0452097654342651\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.732967376708984 | KNN Loss: 3.707319498062134 | BCE Loss: 1.0256481170654297\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.709942817687988 | KNN Loss: 3.6850392818450928 | BCE Loss: 1.024903655052185\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.702641487121582 | KNN Loss: 3.6725246906280518 | BCE Loss: 1.0301169157028198\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.662231922149658 | KNN Loss: 3.6613097190856934 | BCE Loss: 1.0009220838546753\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.770327568054199 | KNN Loss: 3.729576349258423 | BCE Loss: 1.040751338005066\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.709048748016357 | KNN Loss: 3.710012912750244 | BCE Loss: 0.9990359544754028\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.667462348937988 | KNN Loss: 3.663172483444214 | BCE Loss: 1.0042901039123535\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.719045639038086 | KNN Loss: 3.6749188899993896 | BCE Loss: 1.0441268682479858\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.685144424438477 | KNN Loss: 3.6871771812438965 | BCE Loss: 0.9979674220085144\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.680293083190918 | KNN Loss: 3.6577203273773193 | BCE Loss: 1.022572636604309\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.710885047912598 | KNN Loss: 3.7142722606658936 | BCE Loss: 0.9966130256652832\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.721909999847412 | KNN Loss: 3.707045078277588 | BCE Loss: 1.0148649215698242\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.68034553527832 | KNN Loss: 3.6612584590911865 | BCE Loss: 1.0190871953964233\n",
      "Epoch   379: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.664552688598633 | KNN Loss: 3.6756908893585205 | BCE Loss: 0.9888620376586914\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.717595100402832 | KNN Loss: 3.7005186080932617 | BCE Loss: 1.0170763731002808\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.70552921295166 | KNN Loss: 3.6987357139587402 | BCE Loss: 1.0067936182022095\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.774016380310059 | KNN Loss: 3.7322006225585938 | BCE Loss: 1.0418157577514648\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.706370830535889 | KNN Loss: 3.673046350479126 | BCE Loss: 1.0333244800567627\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.733858108520508 | KNN Loss: 3.713029146194458 | BCE Loss: 1.020829200744629\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.70208740234375 | KNN Loss: 3.6778202056884766 | BCE Loss: 1.0242670774459839\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.72574520111084 | KNN Loss: 3.685633897781372 | BCE Loss: 1.0401115417480469\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.714766502380371 | KNN Loss: 3.6933062076568604 | BCE Loss: 1.0214600563049316\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.7501115798950195 | KNN Loss: 3.72005033493042 | BCE Loss: 1.0300612449645996\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.7286577224731445 | KNN Loss: 3.686859607696533 | BCE Loss: 1.0417983531951904\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.742274284362793 | KNN Loss: 3.696260452270508 | BCE Loss: 1.046013593673706\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.740951061248779 | KNN Loss: 3.7104454040527344 | BCE Loss: 1.030505657196045\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.7534332275390625 | KNN Loss: 3.71041202545166 | BCE Loss: 1.0430210828781128\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.701258659362793 | KNN Loss: 3.6841673851013184 | BCE Loss: 1.0170912742614746\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.731808662414551 | KNN Loss: 3.6985926628112793 | BCE Loss: 1.0332159996032715\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.720507621765137 | KNN Loss: 3.690225601196289 | BCE Loss: 1.0302822589874268\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.735598087310791 | KNN Loss: 3.6954004764556885 | BCE Loss: 1.040197730064392\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.710260391235352 | KNN Loss: 3.7004711627960205 | BCE Loss: 1.009788990020752\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.736965179443359 | KNN Loss: 3.6763339042663574 | BCE Loss: 1.060631275177002\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.754901885986328 | KNN Loss: 3.7401788234710693 | BCE Loss: 1.014723300933838\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.6984453201293945 | KNN Loss: 3.6611883640289307 | BCE Loss: 1.0372568368911743\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.714380264282227 | KNN Loss: 3.696610450744629 | BCE Loss: 1.0177698135375977\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.711069583892822 | KNN Loss: 3.698348045349121 | BCE Loss: 1.0127215385437012\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.706870079040527 | KNN Loss: 3.705620527267456 | BCE Loss: 1.0012494325637817\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.782575607299805 | KNN Loss: 3.763124942779541 | BCE Loss: 1.0194504261016846\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.785728931427002 | KNN Loss: 3.7327239513397217 | BCE Loss: 1.0530049800872803\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.70872163772583 | KNN Loss: 3.6968202590942383 | BCE Loss: 1.0119012594223022\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.7460737228393555 | KNN Loss: 3.727400541305542 | BCE Loss: 1.0186729431152344\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.653772354125977 | KNN Loss: 3.6524035930633545 | BCE Loss: 1.0013688802719116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.689777851104736 | KNN Loss: 3.66196608543396 | BCE Loss: 1.0278117656707764\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.719875335693359 | KNN Loss: 3.691113233566284 | BCE Loss: 1.0287621021270752\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.70038366317749 | KNN Loss: 3.6917409896850586 | BCE Loss: 1.0086426734924316\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.702990531921387 | KNN Loss: 3.702233076095581 | BCE Loss: 1.0007572174072266\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.702019691467285 | KNN Loss: 3.698847532272339 | BCE Loss: 1.0031723976135254\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.731573104858398 | KNN Loss: 3.7131006717681885 | BCE Loss: 1.01847243309021\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.720511436462402 | KNN Loss: 3.703808307647705 | BCE Loss: 1.0167031288146973\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.687342643737793 | KNN Loss: 3.674984931945801 | BCE Loss: 1.0123579502105713\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.701555252075195 | KNN Loss: 3.683011293411255 | BCE Loss: 1.0185441970825195\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.680578231811523 | KNN Loss: 3.6684563159942627 | BCE Loss: 1.0121220350265503\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.724370002746582 | KNN Loss: 3.6758127212524414 | BCE Loss: 1.0485572814941406\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.738834381103516 | KNN Loss: 3.720604181289673 | BCE Loss: 1.0182300806045532\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.703802108764648 | KNN Loss: 3.6659598350524902 | BCE Loss: 1.0378425121307373\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.737429618835449 | KNN Loss: 3.7286577224731445 | BCE Loss: 1.0087718963623047\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.721255302429199 | KNN Loss: 3.6932532787323 | BCE Loss: 1.0280017852783203\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.704517364501953 | KNN Loss: 3.668534278869629 | BCE Loss: 1.0359833240509033\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.724917411804199 | KNN Loss: 3.70531964302063 | BCE Loss: 1.0195975303649902\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.699729919433594 | KNN Loss: 3.663583278656006 | BCE Loss: 1.0361464023590088\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.718523979187012 | KNN Loss: 3.6712794303894043 | BCE Loss: 1.047244668006897\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.699915885925293 | KNN Loss: 3.6962900161743164 | BCE Loss: 1.003625750541687\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.671067714691162 | KNN Loss: 3.6535539627075195 | BCE Loss: 1.017513632774353\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.728979110717773 | KNN Loss: 3.6961607933044434 | BCE Loss: 1.03281831741333\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.727179527282715 | KNN Loss: 3.7110209465026855 | BCE Loss: 1.0161588191986084\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.662474632263184 | KNN Loss: 3.6503307819366455 | BCE Loss: 1.012143611907959\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.715177059173584 | KNN Loss: 3.7222344875335693 | BCE Loss: 0.9929425716400146\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.6771955490112305 | KNN Loss: 3.6625208854675293 | BCE Loss: 1.0146749019622803\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.710579872131348 | KNN Loss: 3.685717821121216 | BCE Loss: 1.0248618125915527\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.69244909286499 | KNN Loss: 3.662156581878662 | BCE Loss: 1.0302925109863281\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.712702751159668 | KNN Loss: 3.689791202545166 | BCE Loss: 1.0229114294052124\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.702028274536133 | KNN Loss: 3.6946306228637695 | BCE Loss: 1.0073974132537842\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.802608489990234 | KNN Loss: 3.731996774673462 | BCE Loss: 1.0706119537353516\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.703001976013184 | KNN Loss: 3.6692118644714355 | BCE Loss: 1.033790111541748\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.747208595275879 | KNN Loss: 3.7293593883514404 | BCE Loss: 1.0178494453430176\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.783320903778076 | KNN Loss: 3.714322090148926 | BCE Loss: 1.0689988136291504\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.7005414962768555 | KNN Loss: 3.673236131668091 | BCE Loss: 1.0273053646087646\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.725277423858643 | KNN Loss: 3.7194290161132812 | BCE Loss: 1.0058485269546509\n",
      "Epoch   390: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.729584693908691 | KNN Loss: 3.6783151626586914 | BCE Loss: 1.05126953125\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.693587303161621 | KNN Loss: 3.6852941513061523 | BCE Loss: 1.0082930326461792\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.705930709838867 | KNN Loss: 3.693882703781128 | BCE Loss: 1.0120477676391602\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.7020111083984375 | KNN Loss: 3.699921131134033 | BCE Loss: 1.0020900964736938\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.795193672180176 | KNN Loss: 3.729792594909668 | BCE Loss: 1.0654011964797974\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.700625419616699 | KNN Loss: 3.680210590362549 | BCE Loss: 1.0204145908355713\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.767753601074219 | KNN Loss: 3.723041534423828 | BCE Loss: 1.0447123050689697\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.649300575256348 | KNN Loss: 3.641202926635742 | BCE Loss: 1.0080976486206055\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.714970111846924 | KNN Loss: 3.66617751121521 | BCE Loss: 1.0487926006317139\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.757261276245117 | KNN Loss: 3.702364206314087 | BCE Loss: 1.0548968315124512\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.725335121154785 | KNN Loss: 3.6768248081207275 | BCE Loss: 1.048510193824768\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.727799415588379 | KNN Loss: 3.7109696865081787 | BCE Loss: 1.0168297290802002\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.7650628089904785 | KNN Loss: 3.7151873111724854 | BCE Loss: 1.0498756170272827\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.7578840255737305 | KNN Loss: 3.7097718715667725 | BCE Loss: 1.048112154006958\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.679337024688721 | KNN Loss: 3.6711528301239014 | BCE Loss: 1.0081843137741089\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.675436019897461 | KNN Loss: 3.653606414794922 | BCE Loss: 1.0218294858932495\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.701148986816406 | KNN Loss: 3.7099907398223877 | BCE Loss: 0.9911583065986633\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.7215375900268555 | KNN Loss: 3.699340581893921 | BCE Loss: 1.0221970081329346\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.713589668273926 | KNN Loss: 3.6855244636535645 | BCE Loss: 1.0280649662017822\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.699934005737305 | KNN Loss: 3.673614501953125 | BCE Loss: 1.0263192653656006\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.794390678405762 | KNN Loss: 3.741933584213257 | BCE Loss: 1.0524570941925049\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.759106159210205 | KNN Loss: 3.70812726020813 | BCE Loss: 1.0509788990020752\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.736064910888672 | KNN Loss: 3.728785514831543 | BCE Loss: 1.007279396057129\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.665761470794678 | KNN Loss: 3.680056095123291 | BCE Loss: 0.9857052564620972\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.718353271484375 | KNN Loss: 3.6897411346435547 | BCE Loss: 1.0286120176315308\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.683418273925781 | KNN Loss: 3.6604583263397217 | BCE Loss: 1.0229597091674805\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.747738361358643 | KNN Loss: 3.6910407543182373 | BCE Loss: 1.0566974878311157\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.6784210205078125 | KNN Loss: 3.677534818649292 | BCE Loss: 1.00088632106781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.687875747680664 | KNN Loss: 3.679211378097534 | BCE Loss: 1.008664608001709\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.673733234405518 | KNN Loss: 3.669077157974243 | BCE Loss: 1.004656195640564\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.79136323928833 | KNN Loss: 3.7708029747009277 | BCE Loss: 1.0205602645874023\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.729035377502441 | KNN Loss: 3.6943230628967285 | BCE Loss: 1.034712553024292\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.778444290161133 | KNN Loss: 3.7483811378479004 | BCE Loss: 1.0300633907318115\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.731167793273926 | KNN Loss: 3.691953659057617 | BCE Loss: 1.039214015007019\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.701098918914795 | KNN Loss: 3.6822140216827393 | BCE Loss: 1.0188848972320557\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.76290225982666 | KNN Loss: 3.714416027069092 | BCE Loss: 1.0484862327575684\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.721563339233398 | KNN Loss: 3.7100181579589844 | BCE Loss: 1.011545181274414\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.772424697875977 | KNN Loss: 3.7033536434173584 | BCE Loss: 1.0690712928771973\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.72584867477417 | KNN Loss: 3.6952171325683594 | BCE Loss: 1.0306315422058105\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.671689987182617 | KNN Loss: 3.6465492248535156 | BCE Loss: 1.0251407623291016\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.713730335235596 | KNN Loss: 3.691423177719116 | BCE Loss: 1.0223071575164795\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.737504005432129 | KNN Loss: 3.694871187210083 | BCE Loss: 1.042633056640625\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.72776985168457 | KNN Loss: 3.7089672088623047 | BCE Loss: 1.0188028812408447\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.726945400238037 | KNN Loss: 3.6993355751037598 | BCE Loss: 1.0276097059249878\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.7132391929626465 | KNN Loss: 3.7032439708709717 | BCE Loss: 1.0099951028823853\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.748691082000732 | KNN Loss: 3.718816041946411 | BCE Loss: 1.0298750400543213\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.722673416137695 | KNN Loss: 3.715510845184326 | BCE Loss: 1.0071625709533691\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.703432083129883 | KNN Loss: 3.7020113468170166 | BCE Loss: 1.0014209747314453\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.702810287475586 | KNN Loss: 3.677799701690674 | BCE Loss: 1.025010347366333\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.783547401428223 | KNN Loss: 3.7314414978027344 | BCE Loss: 1.0521061420440674\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.780855178833008 | KNN Loss: 3.7286903858184814 | BCE Loss: 1.0521650314331055\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.73198127746582 | KNN Loss: 3.707244634628296 | BCE Loss: 1.0247368812561035\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.689845085144043 | KNN Loss: 3.683584213256836 | BCE Loss: 1.006260633468628\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.700685977935791 | KNN Loss: 3.65403413772583 | BCE Loss: 1.0466519594192505\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.70636510848999 | KNN Loss: 3.701263189315796 | BCE Loss: 1.0051020383834839\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.685033798217773 | KNN Loss: 3.676568031311035 | BCE Loss: 1.0084655284881592\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.763284683227539 | KNN Loss: 3.715691566467285 | BCE Loss: 1.047593355178833\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.657594680786133 | KNN Loss: 3.6679534912109375 | BCE Loss: 0.9896413683891296\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.758275985717773 | KNN Loss: 3.72860050201416 | BCE Loss: 1.0296754837036133\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.682452201843262 | KNN Loss: 3.6584670543670654 | BCE Loss: 1.0239851474761963\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.734250545501709 | KNN Loss: 3.684556245803833 | BCE Loss: 1.049694299697876\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.667838096618652 | KNN Loss: 3.6737236976623535 | BCE Loss: 0.9941145181655884\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.733082294464111 | KNN Loss: 3.6997056007385254 | BCE Loss: 1.033376693725586\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.747236251831055 | KNN Loss: 3.7191853523254395 | BCE Loss: 1.0280510187149048\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.702746391296387 | KNN Loss: 3.693476438522339 | BCE Loss: 1.0092697143554688\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.752941608428955 | KNN Loss: 3.751521348953247 | BCE Loss: 1.001420259475708\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.790512561798096 | KNN Loss: 3.7217860221862793 | BCE Loss: 1.0687265396118164\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.745577335357666 | KNN Loss: 3.686107635498047 | BCE Loss: 1.0594695806503296\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.751861572265625 | KNN Loss: 3.7058637142181396 | BCE Loss: 1.0459980964660645\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.747448921203613 | KNN Loss: 3.687659502029419 | BCE Loss: 1.0597891807556152\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.664620876312256 | KNN Loss: 3.6709327697753906 | BCE Loss: 0.9936882853507996\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.691011428833008 | KNN Loss: 3.6886041164398193 | BCE Loss: 1.0024070739746094\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.7138261795043945 | KNN Loss: 3.691962718963623 | BCE Loss: 1.0218634605407715\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.7765727043151855 | KNN Loss: 3.76529860496521 | BCE Loss: 1.0112740993499756\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.711153030395508 | KNN Loss: 3.71329927444458 | BCE Loss: 0.9978535175323486\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.7617926597595215 | KNN Loss: 3.7357778549194336 | BCE Loss: 1.026014804840088\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.7000508308410645 | KNN Loss: 3.664699077606201 | BCE Loss: 1.0353518724441528\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.7797160148620605 | KNN Loss: 3.729673147201538 | BCE Loss: 1.0500428676605225\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.709859848022461 | KNN Loss: 3.692772150039673 | BCE Loss: 1.017087697982788\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.705360412597656 | KNN Loss: 3.6775853633880615 | BCE Loss: 1.0277750492095947\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.740983009338379 | KNN Loss: 3.691467046737671 | BCE Loss: 1.049515724182129\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.745797157287598 | KNN Loss: 3.7004482746124268 | BCE Loss: 1.0453487634658813\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.669356822967529 | KNN Loss: 3.6636552810668945 | BCE Loss: 1.0057015419006348\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.664999008178711 | KNN Loss: 3.6746573448181152 | BCE Loss: 0.9903415441513062\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.681498050689697 | KNN Loss: 3.66757869720459 | BCE Loss: 1.013919472694397\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.673628807067871 | KNN Loss: 3.649380683898926 | BCE Loss: 1.0242481231689453\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.696020603179932 | KNN Loss: 3.7029225826263428 | BCE Loss: 0.9930980205535889\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.724670886993408 | KNN Loss: 3.694300889968872 | BCE Loss: 1.0303698778152466\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.674648761749268 | KNN Loss: 3.6697678565979004 | BCE Loss: 1.0048809051513672\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.669368743896484 | KNN Loss: 3.6903955936431885 | BCE Loss: 0.9789729118347168\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.726936340332031 | KNN Loss: 3.706603527069092 | BCE Loss: 1.0203325748443604\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.767740249633789 | KNN Loss: 3.7215871810913086 | BCE Loss: 1.04615318775177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.810971260070801 | KNN Loss: 3.74129056930542 | BCE Loss: 1.06968092918396\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.755423545837402 | KNN Loss: 3.7108614444732666 | BCE Loss: 1.0445623397827148\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.628955841064453 | KNN Loss: 3.644928455352783 | BCE Loss: 0.984027624130249\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.719236373901367 | KNN Loss: 3.690999746322632 | BCE Loss: 1.0282363891601562\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.7324724197387695 | KNN Loss: 3.6964871883392334 | BCE Loss: 1.0359851121902466\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.7882490158081055 | KNN Loss: 3.7428901195526123 | BCE Loss: 1.0453590154647827\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.724094867706299 | KNN Loss: 3.725226402282715 | BCE Loss: 0.9988684058189392\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.711143493652344 | KNN Loss: 3.681917667388916 | BCE Loss: 1.0292260646820068\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.715526103973389 | KNN Loss: 3.6899285316467285 | BCE Loss: 1.0255976915359497\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.69733190536499 | KNN Loss: 3.672837257385254 | BCE Loss: 1.0244947671890259\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.745540142059326 | KNN Loss: 3.720618963241577 | BCE Loss: 1.0249212980270386\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.7405595779418945 | KNN Loss: 3.6869359016418457 | BCE Loss: 1.0536235570907593\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.685455322265625 | KNN Loss: 3.6437783241271973 | BCE Loss: 1.0416767597198486\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.791897773742676 | KNN Loss: 3.7330949306488037 | BCE Loss: 1.0588029623031616\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.697537422180176 | KNN Loss: 3.7068753242492676 | BCE Loss: 0.990662157535553\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.681408882141113 | KNN Loss: 3.6603221893310547 | BCE Loss: 1.0210869312286377\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.752039432525635 | KNN Loss: 3.721529006958008 | BCE Loss: 1.0305105447769165\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.733758926391602 | KNN Loss: 3.7020483016967773 | BCE Loss: 1.0317103862762451\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.698855876922607 | KNN Loss: 3.679194688796997 | BCE Loss: 1.0196611881256104\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.741677284240723 | KNN Loss: 3.712606430053711 | BCE Loss: 1.0290706157684326\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.748706340789795 | KNN Loss: 3.701133966445923 | BCE Loss: 1.047572374343872\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.699646472930908 | KNN Loss: 3.681004047393799 | BCE Loss: 1.0186423063278198\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.663918972015381 | KNN Loss: 3.654839038848877 | BCE Loss: 1.009079933166504\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.685088157653809 | KNN Loss: 3.6730988025665283 | BCE Loss: 1.0119891166687012\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.707818031311035 | KNN Loss: 3.67637300491333 | BCE Loss: 1.0314452648162842\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.750328063964844 | KNN Loss: 3.714303493499756 | BCE Loss: 1.0360243320465088\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.761849403381348 | KNN Loss: 3.732743501663208 | BCE Loss: 1.0291059017181396\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.68917989730835 | KNN Loss: 3.672034502029419 | BCE Loss: 1.0171453952789307\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.7098708152771 | KNN Loss: 3.6809134483337402 | BCE Loss: 1.028957486152649\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.692598342895508 | KNN Loss: 3.6993372440338135 | BCE Loss: 0.99326092004776\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.715175628662109 | KNN Loss: 3.696918249130249 | BCE Loss: 1.0182576179504395\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.661665916442871 | KNN Loss: 3.6487555503845215 | BCE Loss: 1.0129103660583496\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.729063987731934 | KNN Loss: 3.7088708877563477 | BCE Loss: 1.020193338394165\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.737298965454102 | KNN Loss: 3.695080518722534 | BCE Loss: 1.0422186851501465\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.686516284942627 | KNN Loss: 3.676406145095825 | BCE Loss: 1.0101100206375122\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.706790924072266 | KNN Loss: 3.662515163421631 | BCE Loss: 1.0442756414413452\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.7206549644470215 | KNN Loss: 3.7060863971710205 | BCE Loss: 1.014568567276001\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.752309322357178 | KNN Loss: 3.7341837882995605 | BCE Loss: 1.0181256532669067\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.721405029296875 | KNN Loss: 3.674966335296631 | BCE Loss: 1.0464389324188232\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.765566825866699 | KNN Loss: 3.721999168395996 | BCE Loss: 1.0435676574707031\n",
      "Epoch   412: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.7091569900512695 | KNN Loss: 3.690194606781006 | BCE Loss: 1.0189623832702637\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.7014312744140625 | KNN Loss: 3.6834661960601807 | BCE Loss: 1.0179650783538818\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.723453521728516 | KNN Loss: 3.7074077129364014 | BCE Loss: 1.0160460472106934\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.695169925689697 | KNN Loss: 3.639119863510132 | BCE Loss: 1.0560499429702759\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.774900436401367 | KNN Loss: 3.7105579376220703 | BCE Loss: 1.0643426179885864\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.75601053237915 | KNN Loss: 3.705099582672119 | BCE Loss: 1.0509108304977417\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.715570449829102 | KNN Loss: 3.694324254989624 | BCE Loss: 1.021246075630188\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.712159633636475 | KNN Loss: 3.679993152618408 | BCE Loss: 1.0321663618087769\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.785946369171143 | KNN Loss: 3.737281084060669 | BCE Loss: 1.0486652851104736\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.727024078369141 | KNN Loss: 3.6858458518981934 | BCE Loss: 1.0411779880523682\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.676158428192139 | KNN Loss: 3.651071548461914 | BCE Loss: 1.0250868797302246\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.700940132141113 | KNN Loss: 3.6666364669799805 | BCE Loss: 1.034303903579712\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.69243049621582 | KNN Loss: 3.6832308769226074 | BCE Loss: 1.0091997385025024\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.705044269561768 | KNN Loss: 3.699220895767212 | BCE Loss: 1.0058232545852661\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.743142127990723 | KNN Loss: 3.700000524520874 | BCE Loss: 1.0431416034698486\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.69718074798584 | KNN Loss: 3.6686277389526367 | BCE Loss: 1.0285530090332031\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.679518699645996 | KNN Loss: 3.6794822216033936 | BCE Loss: 1.0000364780426025\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.676564693450928 | KNN Loss: 3.6873891353607178 | BCE Loss: 0.9891756176948547\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.729123115539551 | KNN Loss: 3.701967477798462 | BCE Loss: 1.0271556377410889\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.649040222167969 | KNN Loss: 3.637977361679077 | BCE Loss: 1.0110628604888916\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.67473030090332 | KNN Loss: 3.6653976440429688 | BCE Loss: 1.009332537651062\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.713144779205322 | KNN Loss: 3.6789987087249756 | BCE Loss: 1.0341460704803467\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.6951775550842285 | KNN Loss: 3.6855101585388184 | BCE Loss: 1.0096673965454102\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.72019100189209 | KNN Loss: 3.7137646675109863 | BCE Loss: 1.0064263343811035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.684908866882324 | KNN Loss: 3.662559747695923 | BCE Loss: 1.0223493576049805\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.7171783447265625 | KNN Loss: 3.684363842010498 | BCE Loss: 1.032814621925354\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.682908535003662 | KNN Loss: 3.674025297164917 | BCE Loss: 1.0088833570480347\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.692817211151123 | KNN Loss: 3.6803700923919678 | BCE Loss: 1.0124471187591553\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.732419013977051 | KNN Loss: 3.6941699981689453 | BCE Loss: 1.038248896598816\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.702141284942627 | KNN Loss: 3.6764721870422363 | BCE Loss: 1.0256690979003906\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.671271324157715 | KNN Loss: 3.6699178218841553 | BCE Loss: 1.0013535022735596\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.748252868652344 | KNN Loss: 3.741379499435425 | BCE Loss: 1.0068731307983398\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.782351493835449 | KNN Loss: 3.7374556064605713 | BCE Loss: 1.044895887374878\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.788811683654785 | KNN Loss: 3.745276689529419 | BCE Loss: 1.0435352325439453\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.703551292419434 | KNN Loss: 3.6779234409332275 | BCE Loss: 1.025627851486206\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.736301422119141 | KNN Loss: 3.714749813079834 | BCE Loss: 1.0215513706207275\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.7558159828186035 | KNN Loss: 3.716762065887451 | BCE Loss: 1.039054036140442\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.723130702972412 | KNN Loss: 3.6939828395843506 | BCE Loss: 1.0291478633880615\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.720450401306152 | KNN Loss: 3.6963021755218506 | BCE Loss: 1.0241484642028809\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.767495632171631 | KNN Loss: 3.740640878677368 | BCE Loss: 1.0268548727035522\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.686356544494629 | KNN Loss: 3.6836016178131104 | BCE Loss: 1.002755045890808\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.733619689941406 | KNN Loss: 3.7162699699401855 | BCE Loss: 1.0173497200012207\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.7408599853515625 | KNN Loss: 3.6849963665008545 | BCE Loss: 1.055863618850708\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.717125415802002 | KNN Loss: 3.6925103664398193 | BCE Loss: 1.0246150493621826\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.7158918380737305 | KNN Loss: 3.6893935203552246 | BCE Loss: 1.0264984369277954\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.72687292098999 | KNN Loss: 3.7190489768981934 | BCE Loss: 1.0078239440917969\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.730390548706055 | KNN Loss: 3.681056022644043 | BCE Loss: 1.0493347644805908\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.684431552886963 | KNN Loss: 3.6670055389404297 | BCE Loss: 1.0174261331558228\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.702555179595947 | KNN Loss: 3.7163712978363037 | BCE Loss: 0.986183762550354\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.72772216796875 | KNN Loss: 3.669170379638672 | BCE Loss: 1.058551549911499\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.770315647125244 | KNN Loss: 3.724764585494995 | BCE Loss: 1.045551061630249\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.702215194702148 | KNN Loss: 3.695786714553833 | BCE Loss: 1.0064282417297363\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.718912124633789 | KNN Loss: 3.688096284866333 | BCE Loss: 1.0308160781860352\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.713303565979004 | KNN Loss: 3.691892385482788 | BCE Loss: 1.0214111804962158\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.675843238830566 | KNN Loss: 3.669267416000366 | BCE Loss: 1.0065760612487793\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.743229866027832 | KNN Loss: 3.7121152877807617 | BCE Loss: 1.0311143398284912\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.727908611297607 | KNN Loss: 3.680492401123047 | BCE Loss: 1.04741632938385\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.764383316040039 | KNN Loss: 3.7286691665649414 | BCE Loss: 1.0357139110565186\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.759502410888672 | KNN Loss: 3.7045655250549316 | BCE Loss: 1.0549367666244507\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.691478252410889 | KNN Loss: 3.700587749481201 | BCE Loss: 0.990890622138977\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.697147369384766 | KNN Loss: 3.6965761184692383 | BCE Loss: 1.0005714893341064\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.657565116882324 | KNN Loss: 3.664931297302246 | BCE Loss: 0.9926338195800781\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.742549896240234 | KNN Loss: 3.6962850093841553 | BCE Loss: 1.0462647676467896\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.691755294799805 | KNN Loss: 3.6824491024017334 | BCE Loss: 1.0093063116073608\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.773178577423096 | KNN Loss: 3.7308003902435303 | BCE Loss: 1.0423781871795654\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.73488712310791 | KNN Loss: 3.714137315750122 | BCE Loss: 1.0207496881484985\n",
      "Epoch   423: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.656403064727783 | KNN Loss: 3.653470277786255 | BCE Loss: 1.0029326677322388\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.684406757354736 | KNN Loss: 3.665031671524048 | BCE Loss: 1.0193750858306885\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.700263977050781 | KNN Loss: 3.6684200763702393 | BCE Loss: 1.031843900680542\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.752514839172363 | KNN Loss: 3.745645761489868 | BCE Loss: 1.006868839263916\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.768247604370117 | KNN Loss: 3.715420961380005 | BCE Loss: 1.0528266429901123\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.722308158874512 | KNN Loss: 3.6886532306671143 | BCE Loss: 1.0336551666259766\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.700886249542236 | KNN Loss: 3.70500111579895 | BCE Loss: 0.9958850145339966\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.70571756362915 | KNN Loss: 3.670938730239868 | BCE Loss: 1.0347788333892822\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.699836254119873 | KNN Loss: 3.6740756034851074 | BCE Loss: 1.0257606506347656\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.724308013916016 | KNN Loss: 3.699395179748535 | BCE Loss: 1.0249130725860596\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.708486080169678 | KNN Loss: 3.689192295074463 | BCE Loss: 1.0192939043045044\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.674000263214111 | KNN Loss: 3.6933279037475586 | BCE Loss: 0.9806724190711975\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.709020614624023 | KNN Loss: 3.6607065200805664 | BCE Loss: 1.048314094543457\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.734253883361816 | KNN Loss: 3.7010157108306885 | BCE Loss: 1.0332382917404175\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.689723014831543 | KNN Loss: 3.671572208404541 | BCE Loss: 1.018151044845581\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.6723504066467285 | KNN Loss: 3.6619668006896973 | BCE Loss: 1.0103834867477417\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.658041954040527 | KNN Loss: 3.657355308532715 | BCE Loss: 1.0006868839263916\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.670438289642334 | KNN Loss: 3.683149814605713 | BCE Loss: 0.9872885346412659\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.739871025085449 | KNN Loss: 3.7009332180023193 | BCE Loss: 1.038938045501709\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.746763229370117 | KNN Loss: 3.738410472869873 | BCE Loss: 1.0083529949188232\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.74587869644165 | KNN Loss: 3.6983864307403564 | BCE Loss: 1.0474921464920044\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.757030487060547 | KNN Loss: 3.7134275436401367 | BCE Loss: 1.0436029434204102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.705377101898193 | KNN Loss: 3.6603331565856934 | BCE Loss: 1.0450438261032104\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.7135844230651855 | KNN Loss: 3.716151475906372 | BCE Loss: 0.9974328875541687\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.737201690673828 | KNN Loss: 3.733917713165283 | BCE Loss: 1.0032837390899658\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.707576751708984 | KNN Loss: 3.685514211654663 | BCE Loss: 1.0220624208450317\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.718943119049072 | KNN Loss: 3.7135863304138184 | BCE Loss: 1.0053566694259644\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.658566474914551 | KNN Loss: 3.649756669998169 | BCE Loss: 1.0088098049163818\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.688265800476074 | KNN Loss: 3.68343186378479 | BCE Loss: 1.0048339366912842\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.697124481201172 | KNN Loss: 3.706594467163086 | BCE Loss: 0.9905301332473755\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.70297908782959 | KNN Loss: 3.7089977264404297 | BCE Loss: 0.9939811825752258\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.713513374328613 | KNN Loss: 3.675757646560669 | BCE Loss: 1.0377558469772339\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.7141313552856445 | KNN Loss: 3.6650547981262207 | BCE Loss: 1.049076795578003\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.7486724853515625 | KNN Loss: 3.716897487640381 | BCE Loss: 1.0317752361297607\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.659869194030762 | KNN Loss: 3.6481809616088867 | BCE Loss: 1.011687994003296\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.6895599365234375 | KNN Loss: 3.6774916648864746 | BCE Loss: 1.0120680332183838\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.731747150421143 | KNN Loss: 3.681018829345703 | BCE Loss: 1.0507283210754395\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.750260353088379 | KNN Loss: 3.722607374191284 | BCE Loss: 1.0276528596878052\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.6812744140625 | KNN Loss: 3.6675682067871094 | BCE Loss: 1.0137063264846802\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.751286029815674 | KNN Loss: 3.7098379135131836 | BCE Loss: 1.0414482355117798\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.687465667724609 | KNN Loss: 3.673732280731201 | BCE Loss: 1.0137336254119873\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.763618469238281 | KNN Loss: 3.7198474407196045 | BCE Loss: 1.0437710285186768\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.699116230010986 | KNN Loss: 3.6561830043792725 | BCE Loss: 1.0429333448410034\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.711850166320801 | KNN Loss: 3.691438674926758 | BCE Loss: 1.020411491394043\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.685574531555176 | KNN Loss: 3.690016031265259 | BCE Loss: 0.9955582618713379\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.737060546875 | KNN Loss: 3.702868700027466 | BCE Loss: 1.0341917276382446\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.730067253112793 | KNN Loss: 3.6903629302978516 | BCE Loss: 1.0397040843963623\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.664795875549316 | KNN Loss: 3.67641544342041 | BCE Loss: 0.9883806109428406\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.754269599914551 | KNN Loss: 3.7289535999298096 | BCE Loss: 1.0253162384033203\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.739401817321777 | KNN Loss: 3.691629648208618 | BCE Loss: 1.0477720499038696\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.771658897399902 | KNN Loss: 3.709040403366089 | BCE Loss: 1.0626184940338135\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.66044282913208 | KNN Loss: 3.6431162357330322 | BCE Loss: 1.0173265933990479\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.729867935180664 | KNN Loss: 3.6833019256591797 | BCE Loss: 1.0465658903121948\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.700788497924805 | KNN Loss: 3.695600986480713 | BCE Loss: 1.0051875114440918\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.6880784034729 | KNN Loss: 3.6714487075805664 | BCE Loss: 1.0166298151016235\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.6956095695495605 | KNN Loss: 3.685340166091919 | BCE Loss: 1.0102694034576416\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.697112083435059 | KNN Loss: 3.687243938446045 | BCE Loss: 1.0098681449890137\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.664276123046875 | KNN Loss: 3.6515133380889893 | BCE Loss: 1.0127626657485962\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.706984043121338 | KNN Loss: 3.6734542846679688 | BCE Loss: 1.0335297584533691\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.707455635070801 | KNN Loss: 3.682839870452881 | BCE Loss: 1.02461576461792\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.750855445861816 | KNN Loss: 3.695714235305786 | BCE Loss: 1.0551413297653198\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.709107398986816 | KNN Loss: 3.676786184310913 | BCE Loss: 1.0323212146759033\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.756934642791748 | KNN Loss: 3.7259066104888916 | BCE Loss: 1.031027913093567\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.731772422790527 | KNN Loss: 3.693319082260132 | BCE Loss: 1.0384533405303955\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.745650768280029 | KNN Loss: 3.699155330657959 | BCE Loss: 1.0464955568313599\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.706737518310547 | KNN Loss: 3.692870855331421 | BCE Loss: 1.013866901397705\n",
      "Epoch   434: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.702088356018066 | KNN Loss: 3.6957428455352783 | BCE Loss: 1.0063456296920776\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.743732452392578 | KNN Loss: 3.7082250118255615 | BCE Loss: 1.0355075597763062\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.683895111083984 | KNN Loss: 3.6800577640533447 | BCE Loss: 1.00383722782135\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.755300998687744 | KNN Loss: 3.7305028438568115 | BCE Loss: 1.0247981548309326\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.753427505493164 | KNN Loss: 3.6952309608459473 | BCE Loss: 1.0581964254379272\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.68383264541626 | KNN Loss: 3.645009994506836 | BCE Loss: 1.0388226509094238\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.783360481262207 | KNN Loss: 3.72301983833313 | BCE Loss: 1.0603408813476562\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.697685241699219 | KNN Loss: 3.6662380695343018 | BCE Loss: 1.031447410583496\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.750605583190918 | KNN Loss: 3.7095377445220947 | BCE Loss: 1.0410676002502441\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.694713115692139 | KNN Loss: 3.6830475330352783 | BCE Loss: 1.0116655826568604\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.733475685119629 | KNN Loss: 3.6877729892730713 | BCE Loss: 1.0457029342651367\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.6959428787231445 | KNN Loss: 3.668659210205078 | BCE Loss: 1.0272834300994873\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.694329261779785 | KNN Loss: 3.6684045791625977 | BCE Loss: 1.0259249210357666\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.767289161682129 | KNN Loss: 3.7408623695373535 | BCE Loss: 1.0264265537261963\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.690159797668457 | KNN Loss: 3.6670942306518555 | BCE Loss: 1.0230655670166016\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.718693733215332 | KNN Loss: 3.6912155151367188 | BCE Loss: 1.0274783372879028\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.645013809204102 | KNN Loss: 3.6351349353790283 | BCE Loss: 1.0098786354064941\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.679662704467773 | KNN Loss: 3.678328514099121 | BCE Loss: 1.0013344287872314\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.710045337677002 | KNN Loss: 3.7159998416900635 | BCE Loss: 0.9940456748008728\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.738089561462402 | KNN Loss: 3.6743431091308594 | BCE Loss: 1.063746452331543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.740231513977051 | KNN Loss: 3.7092111110687256 | BCE Loss: 1.0310204029083252\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.740328788757324 | KNN Loss: 3.7277040481567383 | BCE Loss: 1.0126245021820068\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.723419189453125 | KNN Loss: 3.7056901454925537 | BCE Loss: 1.0177291631698608\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.709522247314453 | KNN Loss: 3.6893835067749023 | BCE Loss: 1.0201387405395508\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.759857177734375 | KNN Loss: 3.7159714698791504 | BCE Loss: 1.0438858270645142\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.73109245300293 | KNN Loss: 3.7003979682922363 | BCE Loss: 1.0306947231292725\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.711421966552734 | KNN Loss: 3.6733806133270264 | BCE Loss: 1.038041591644287\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.681744575500488 | KNN Loss: 3.6871581077575684 | BCE Loss: 0.9945865273475647\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.6795735359191895 | KNN Loss: 3.6713476181030273 | BCE Loss: 1.008225917816162\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.691206932067871 | KNN Loss: 3.6608405113220215 | BCE Loss: 1.0303661823272705\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.664175033569336 | KNN Loss: 3.6378235816955566 | BCE Loss: 1.0263516902923584\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.756932735443115 | KNN Loss: 3.709212303161621 | BCE Loss: 1.0477203130722046\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.718214511871338 | KNN Loss: 3.684453010559082 | BCE Loss: 1.0337615013122559\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.740996837615967 | KNN Loss: 3.7137882709503174 | BCE Loss: 1.0272085666656494\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.720388412475586 | KNN Loss: 3.688669443130493 | BCE Loss: 1.0317189693450928\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.684742450714111 | KNN Loss: 3.667793035507202 | BCE Loss: 1.0169494152069092\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.6998443603515625 | KNN Loss: 3.6869568824768066 | BCE Loss: 1.0128875970840454\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.718033790588379 | KNN Loss: 3.7039146423339844 | BCE Loss: 1.0141193866729736\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.693788528442383 | KNN Loss: 3.647181749343872 | BCE Loss: 1.0466068983078003\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.68609094619751 | KNN Loss: 3.661837339401245 | BCE Loss: 1.0242536067962646\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.749029159545898 | KNN Loss: 3.701361894607544 | BCE Loss: 1.047667384147644\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.712393760681152 | KNN Loss: 3.710285186767578 | BCE Loss: 1.0021084547042847\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.724153995513916 | KNN Loss: 3.6953940391540527 | BCE Loss: 1.0287599563598633\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.704051494598389 | KNN Loss: 3.6775641441345215 | BCE Loss: 1.0264873504638672\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.711805820465088 | KNN Loss: 3.6990132331848145 | BCE Loss: 1.012792706489563\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.715904235839844 | KNN Loss: 3.6777071952819824 | BCE Loss: 1.0381968021392822\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.712735176086426 | KNN Loss: 3.6953415870666504 | BCE Loss: 1.0173933506011963\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.733217239379883 | KNN Loss: 3.6901493072509766 | BCE Loss: 1.0430678129196167\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.686190605163574 | KNN Loss: 3.6379966735839844 | BCE Loss: 1.0481936931610107\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.7017436027526855 | KNN Loss: 3.6764485836029053 | BCE Loss: 1.0252948999404907\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.702836990356445 | KNN Loss: 3.666907548904419 | BCE Loss: 1.035929560661316\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.712003231048584 | KNN Loss: 3.7008392810821533 | BCE Loss: 1.0111639499664307\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.730523109436035 | KNN Loss: 3.70613956451416 | BCE Loss: 1.024383544921875\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.678536415100098 | KNN Loss: 3.650517463684082 | BCE Loss: 1.0280187129974365\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.695757865905762 | KNN Loss: 3.6657707691192627 | BCE Loss: 1.02998685836792\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.719150543212891 | KNN Loss: 3.681609869003296 | BCE Loss: 1.0375409126281738\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.735439777374268 | KNN Loss: 3.7012438774108887 | BCE Loss: 1.0341960191726685\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.705977439880371 | KNN Loss: 3.688993453979492 | BCE Loss: 1.0169837474822998\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.712185382843018 | KNN Loss: 3.6862080097198486 | BCE Loss: 1.0259772539138794\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.664463996887207 | KNN Loss: 3.6515841484069824 | BCE Loss: 1.0128798484802246\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.804154396057129 | KNN Loss: 3.7689208984375 | BCE Loss: 1.035233497619629\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.698185920715332 | KNN Loss: 3.686366319656372 | BCE Loss: 1.0118193626403809\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.711963653564453 | KNN Loss: 3.692612886428833 | BCE Loss: 1.0193507671356201\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.715585708618164 | KNN Loss: 3.676532030105591 | BCE Loss: 1.0390534400939941\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.729672431945801 | KNN Loss: 3.6901206970214844 | BCE Loss: 1.0395514965057373\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.720390319824219 | KNN Loss: 3.7102859020233154 | BCE Loss: 1.0101046562194824\n",
      "Epoch   445: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.707535266876221 | KNN Loss: 3.6911845207214355 | BCE Loss: 1.0163506269454956\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.752654075622559 | KNN Loss: 3.6940548419952393 | BCE Loss: 1.0585992336273193\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.6823320388793945 | KNN Loss: 3.7039103507995605 | BCE Loss: 0.9784219264984131\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.763611316680908 | KNN Loss: 3.742368698120117 | BCE Loss: 1.021242618560791\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.737215518951416 | KNN Loss: 3.6890788078308105 | BCE Loss: 1.048136830329895\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.71494722366333 | KNN Loss: 3.687316656112671 | BCE Loss: 1.0276304483413696\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.733234405517578 | KNN Loss: 3.7070794105529785 | BCE Loss: 1.0261549949645996\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.63506555557251 | KNN Loss: 3.646245002746582 | BCE Loss: 0.988820493221283\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.744178771972656 | KNN Loss: 3.7253499031066895 | BCE Loss: 1.018829107284546\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.700209617614746 | KNN Loss: 3.680922746658325 | BCE Loss: 1.019287109375\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.710399627685547 | KNN Loss: 3.693232536315918 | BCE Loss: 1.017167329788208\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.735831260681152 | KNN Loss: 3.6933085918426514 | BCE Loss: 1.042522668838501\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.704681873321533 | KNN Loss: 3.6870148181915283 | BCE Loss: 1.0176671743392944\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.7211833000183105 | KNN Loss: 3.687861919403076 | BCE Loss: 1.0333213806152344\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.713840961456299 | KNN Loss: 3.6823079586029053 | BCE Loss: 1.0315330028533936\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.674931526184082 | KNN Loss: 3.667506217956543 | BCE Loss: 1.007425308227539\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.722620964050293 | KNN Loss: 3.7121498584747314 | BCE Loss: 1.010470986366272\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.718183517456055 | KNN Loss: 3.686302900314331 | BCE Loss: 1.0318806171417236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.736355781555176 | KNN Loss: 3.720778465270996 | BCE Loss: 1.0155770778656006\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.739109039306641 | KNN Loss: 3.7167129516601562 | BCE Loss: 1.0223963260650635\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.711991786956787 | KNN Loss: 3.691059112548828 | BCE Loss: 1.0209325551986694\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.701459884643555 | KNN Loss: 3.704157590866089 | BCE Loss: 0.9973020553588867\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.701648712158203 | KNN Loss: 3.671044111251831 | BCE Loss: 1.030604600906372\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.761867523193359 | KNN Loss: 3.7083561420440674 | BCE Loss: 1.0535115003585815\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.688438415527344 | KNN Loss: 3.6693243980407715 | BCE Loss: 1.0191140174865723\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.73390007019043 | KNN Loss: 3.678891181945801 | BCE Loss: 1.0550086498260498\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.7200703620910645 | KNN Loss: 3.688633918762207 | BCE Loss: 1.0314364433288574\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.687787055969238 | KNN Loss: 3.665205717086792 | BCE Loss: 1.0225813388824463\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.647651195526123 | KNN Loss: 3.6735613346099854 | BCE Loss: 0.9740899205207825\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.756015777587891 | KNN Loss: 3.705124616622925 | BCE Loss: 1.0508909225463867\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.686326026916504 | KNN Loss: 3.6661903858184814 | BCE Loss: 1.0201358795166016\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.747515678405762 | KNN Loss: 3.7012152671813965 | BCE Loss: 1.0463001728057861\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.709262371063232 | KNN Loss: 3.6720829010009766 | BCE Loss: 1.0371794700622559\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.7111406326293945 | KNN Loss: 3.689067840576172 | BCE Loss: 1.0220729112625122\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.71953821182251 | KNN Loss: 3.704373598098755 | BCE Loss: 1.0151647329330444\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.7304558753967285 | KNN Loss: 3.7340188026428223 | BCE Loss: 0.9964368939399719\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.698246479034424 | KNN Loss: 3.656121253967285 | BCE Loss: 1.0421251058578491\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.7222700119018555 | KNN Loss: 3.6825923919677734 | BCE Loss: 1.0396778583526611\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.7230424880981445 | KNN Loss: 3.7047605514526367 | BCE Loss: 1.018282175064087\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.768859386444092 | KNN Loss: 3.733368158340454 | BCE Loss: 1.0354911088943481\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.732196807861328 | KNN Loss: 3.7002804279327393 | BCE Loss: 1.0319163799285889\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.703592300415039 | KNN Loss: 3.6699576377868652 | BCE Loss: 1.0336344242095947\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.6769866943359375 | KNN Loss: 3.6716434955596924 | BCE Loss: 1.0053431987762451\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.729658126831055 | KNN Loss: 3.690899133682251 | BCE Loss: 1.0387589931488037\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.726522445678711 | KNN Loss: 3.7069344520568848 | BCE Loss: 1.0195882320404053\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.725782871246338 | KNN Loss: 3.7086238861083984 | BCE Loss: 1.0171589851379395\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.66510009765625 | KNN Loss: 3.662412643432617 | BCE Loss: 1.0026873350143433\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.742159843444824 | KNN Loss: 3.6977550983428955 | BCE Loss: 1.0444049835205078\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.742171287536621 | KNN Loss: 3.6984951496124268 | BCE Loss: 1.0436761379241943\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.715281009674072 | KNN Loss: 3.6734776496887207 | BCE Loss: 1.0418033599853516\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.702110290527344 | KNN Loss: 3.7017455101013184 | BCE Loss: 1.000364899635315\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.6983842849731445 | KNN Loss: 3.662986993789673 | BCE Loss: 1.0353972911834717\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.7167229652404785 | KNN Loss: 3.7026708126068115 | BCE Loss: 1.0140522718429565\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.70276403427124 | KNN Loss: 3.682493209838867 | BCE Loss: 1.0202709436416626\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.717532634735107 | KNN Loss: 3.670929193496704 | BCE Loss: 1.0466034412384033\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.707018852233887 | KNN Loss: 3.6927225589752197 | BCE Loss: 1.014296531677246\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.732509136199951 | KNN Loss: 3.6770474910736084 | BCE Loss: 1.0554616451263428\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.688536643981934 | KNN Loss: 3.6971940994262695 | BCE Loss: 0.991342306137085\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.730208396911621 | KNN Loss: 3.698458433151245 | BCE Loss: 1.0317497253417969\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.744017601013184 | KNN Loss: 3.6874916553497314 | BCE Loss: 1.056525707244873\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.743342876434326 | KNN Loss: 3.718679904937744 | BCE Loss: 1.024662971496582\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.68853235244751 | KNN Loss: 3.6921629905700684 | BCE Loss: 0.9963693022727966\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.641141414642334 | KNN Loss: 3.6484930515289307 | BCE Loss: 0.9926483035087585\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.766002655029297 | KNN Loss: 3.734135627746582 | BCE Loss: 1.0318669080734253\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.7572479248046875 | KNN Loss: 3.741516590118408 | BCE Loss: 1.0157314538955688\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.712446212768555 | KNN Loss: 3.69958233833313 | BCE Loss: 1.0128638744354248\n",
      "Epoch   456: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.696709632873535 | KNN Loss: 3.663173198699951 | BCE Loss: 1.033536434173584\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.7301554679870605 | KNN Loss: 3.6923129558563232 | BCE Loss: 1.0378425121307373\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.713238716125488 | KNN Loss: 3.6893250942230225 | BCE Loss: 1.0239133834838867\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.708648204803467 | KNN Loss: 3.6770408153533936 | BCE Loss: 1.0316072702407837\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.679460048675537 | KNN Loss: 3.6842243671417236 | BCE Loss: 0.9952358603477478\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.71681022644043 | KNN Loss: 3.680922269821167 | BCE Loss: 1.0358881950378418\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.715065956115723 | KNN Loss: 3.714855432510376 | BCE Loss: 1.0002105236053467\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.702205657958984 | KNN Loss: 3.668673515319824 | BCE Loss: 1.033531904220581\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.679997444152832 | KNN Loss: 3.6639885902404785 | BCE Loss: 1.0160086154937744\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.747810363769531 | KNN Loss: 3.710228443145752 | BCE Loss: 1.0375818014144897\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.724822044372559 | KNN Loss: 3.714014768600464 | BCE Loss: 1.0108072757720947\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.763489723205566 | KNN Loss: 3.7057504653930664 | BCE Loss: 1.0577392578125\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.72343111038208 | KNN Loss: 3.6691720485687256 | BCE Loss: 1.0542590618133545\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.696100234985352 | KNN Loss: 3.681068181991577 | BCE Loss: 1.0150318145751953\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.71550178527832 | KNN Loss: 3.6630890369415283 | BCE Loss: 1.052412748336792\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.710766792297363 | KNN Loss: 3.708852767944336 | BCE Loss: 1.001914143562317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.711792945861816 | KNN Loss: 3.7024762630462646 | BCE Loss: 1.0093164443969727\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.749874114990234 | KNN Loss: 3.71945858001709 | BCE Loss: 1.030415415763855\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.678637504577637 | KNN Loss: 3.655155658721924 | BCE Loss: 1.0234819650650024\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.727014541625977 | KNN Loss: 3.693755865097046 | BCE Loss: 1.0332589149475098\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.726995468139648 | KNN Loss: 3.689645290374756 | BCE Loss: 1.0373499393463135\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.734009742736816 | KNN Loss: 3.701765537261963 | BCE Loss: 1.0322442054748535\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.679590702056885 | KNN Loss: 3.6703453063964844 | BCE Loss: 1.0092453956604004\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.7523698806762695 | KNN Loss: 3.700995683670044 | BCE Loss: 1.051374077796936\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.710375785827637 | KNN Loss: 3.6743593215942383 | BCE Loss: 1.036016583442688\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.753425598144531 | KNN Loss: 3.7074661254882812 | BCE Loss: 1.045959711074829\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.724580764770508 | KNN Loss: 3.710270643234253 | BCE Loss: 1.0143098831176758\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.705110549926758 | KNN Loss: 3.6898059844970703 | BCE Loss: 1.015304446220398\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.738259315490723 | KNN Loss: 3.722172737121582 | BCE Loss: 1.016086459159851\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.697948455810547 | KNN Loss: 3.682189702987671 | BCE Loss: 1.0157588720321655\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.679219722747803 | KNN Loss: 3.6835529804229736 | BCE Loss: 0.9956668615341187\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.671758651733398 | KNN Loss: 3.6600019931793213 | BCE Loss: 1.011756420135498\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.712226867675781 | KNN Loss: 3.70434832572937 | BCE Loss: 1.0078787803649902\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.735442638397217 | KNN Loss: 3.7199490070343018 | BCE Loss: 1.015493631362915\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.712034225463867 | KNN Loss: 3.6874935626983643 | BCE Loss: 1.0245404243469238\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.673221111297607 | KNN Loss: 3.635803699493408 | BCE Loss: 1.0374174118041992\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.735645771026611 | KNN Loss: 3.7159759998321533 | BCE Loss: 1.0196696519851685\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.717649459838867 | KNN Loss: 3.684641122817993 | BCE Loss: 1.0330085754394531\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.74065637588501 | KNN Loss: 3.711980104446411 | BCE Loss: 1.0286762714385986\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.6959428787231445 | KNN Loss: 3.689629077911377 | BCE Loss: 1.0063135623931885\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.690220832824707 | KNN Loss: 3.657454013824463 | BCE Loss: 1.032766580581665\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.708442687988281 | KNN Loss: 3.692075252532959 | BCE Loss: 1.0163675546646118\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.686612606048584 | KNN Loss: 3.672895908355713 | BCE Loss: 1.013716697692871\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.652608394622803 | KNN Loss: 3.646991729736328 | BCE Loss: 1.0056167840957642\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.691385746002197 | KNN Loss: 3.678112030029297 | BCE Loss: 1.0132737159729004\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.7001447677612305 | KNN Loss: 3.698596239089966 | BCE Loss: 1.0015482902526855\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.725425720214844 | KNN Loss: 3.702120542526245 | BCE Loss: 1.0233054161071777\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.6862969398498535 | KNN Loss: 3.687058210372925 | BCE Loss: 0.9992386102676392\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.740083694458008 | KNN Loss: 3.7222068309783936 | BCE Loss: 1.0178768634796143\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.75247049331665 | KNN Loss: 3.7444753646850586 | BCE Loss: 1.0079952478408813\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.736504554748535 | KNN Loss: 3.7205538749694824 | BCE Loss: 1.0159506797790527\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.707151412963867 | KNN Loss: 3.660264015197754 | BCE Loss: 1.0468873977661133\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.728456497192383 | KNN Loss: 3.718515634536743 | BCE Loss: 1.00994074344635\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.667476654052734 | KNN Loss: 3.6659483909606934 | BCE Loss: 1.001528024673462\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.741926193237305 | KNN Loss: 3.7247512340545654 | BCE Loss: 1.0171747207641602\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.741060733795166 | KNN Loss: 3.6920886039733887 | BCE Loss: 1.0489721298217773\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.720691680908203 | KNN Loss: 3.695556879043579 | BCE Loss: 1.025134801864624\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.759247779846191 | KNN Loss: 3.7115516662597656 | BCE Loss: 1.0476961135864258\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.772686004638672 | KNN Loss: 3.7404866218566895 | BCE Loss: 1.0321992635726929\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.694330215454102 | KNN Loss: 3.676311492919922 | BCE Loss: 1.0180189609527588\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.7135467529296875 | KNN Loss: 3.663637161254883 | BCE Loss: 1.0499095916748047\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.691291809082031 | KNN Loss: 3.6553070545196533 | BCE Loss: 1.0359845161437988\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.703523635864258 | KNN Loss: 3.6539146900177 | BCE Loss: 1.0496089458465576\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.713384628295898 | KNN Loss: 3.6869349479675293 | BCE Loss: 1.02644944190979\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.796085357666016 | KNN Loss: 3.746605157852173 | BCE Loss: 1.0494799613952637\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.691605567932129 | KNN Loss: 3.6860291957855225 | BCE Loss: 1.0055766105651855\n",
      "Epoch   467: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.7855939865112305 | KNN Loss: 3.733342170715332 | BCE Loss: 1.0522518157958984\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.689117908477783 | KNN Loss: 3.6863343715667725 | BCE Loss: 1.0027835369110107\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.723229885101318 | KNN Loss: 3.6827852725982666 | BCE Loss: 1.0404446125030518\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.750357151031494 | KNN Loss: 3.7045366764068604 | BCE Loss: 1.0458203554153442\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.750731468200684 | KNN Loss: 3.725642204284668 | BCE Loss: 1.0250895023345947\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.740919589996338 | KNN Loss: 3.717381477355957 | BCE Loss: 1.0235381126403809\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.749473571777344 | KNN Loss: 3.718785524368286 | BCE Loss: 1.0306880474090576\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.705992221832275 | KNN Loss: 3.6885151863098145 | BCE Loss: 1.017477035522461\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.6930036544799805 | KNN Loss: 3.672858715057373 | BCE Loss: 1.0201451778411865\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.759457111358643 | KNN Loss: 3.72454833984375 | BCE Loss: 1.0349088907241821\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.752791404724121 | KNN Loss: 3.7207117080688477 | BCE Loss: 1.0320794582366943\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.711728572845459 | KNN Loss: 3.6905128955841064 | BCE Loss: 1.021215558052063\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.73051643371582 | KNN Loss: 3.6964118480682373 | BCE Loss: 1.034104824066162\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.716423034667969 | KNN Loss: 3.6969778537750244 | BCE Loss: 1.0194454193115234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.683858871459961 | KNN Loss: 3.6704912185668945 | BCE Loss: 1.0133675336837769\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.695420265197754 | KNN Loss: 3.6804099082946777 | BCE Loss: 1.0150105953216553\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.741342544555664 | KNN Loss: 3.7153501510620117 | BCE Loss: 1.0259926319122314\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.7017364501953125 | KNN Loss: 3.69100022315979 | BCE Loss: 1.0107359886169434\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.734338283538818 | KNN Loss: 3.6815943717956543 | BCE Loss: 1.052743911743164\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.714425563812256 | KNN Loss: 3.6779732704162598 | BCE Loss: 1.0364524126052856\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.703989505767822 | KNN Loss: 3.6965994834899902 | BCE Loss: 1.0073901414871216\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.701154708862305 | KNN Loss: 3.6662096977233887 | BCE Loss: 1.0349452495574951\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.686209678649902 | KNN Loss: 3.65932559967041 | BCE Loss: 1.0268839597702026\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.721285820007324 | KNN Loss: 3.7013511657714844 | BCE Loss: 1.0199344158172607\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.684117317199707 | KNN Loss: 3.6700422763824463 | BCE Loss: 1.0140750408172607\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.700189590454102 | KNN Loss: 3.673737049102783 | BCE Loss: 1.0264523029327393\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.735194206237793 | KNN Loss: 3.696634292602539 | BCE Loss: 1.0385600328445435\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.6628217697143555 | KNN Loss: 3.6585960388183594 | BCE Loss: 1.004225730895996\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.767182350158691 | KNN Loss: 3.746882438659668 | BCE Loss: 1.0203001499176025\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.792418479919434 | KNN Loss: 3.7501485347747803 | BCE Loss: 1.0422701835632324\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.708991527557373 | KNN Loss: 3.6879920959472656 | BCE Loss: 1.020999550819397\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.699594497680664 | KNN Loss: 3.663463592529297 | BCE Loss: 1.036130666732788\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.680239677429199 | KNN Loss: 3.677408456802368 | BCE Loss: 1.0028311014175415\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.7240777015686035 | KNN Loss: 3.696420669555664 | BCE Loss: 1.027657151222229\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.770562648773193 | KNN Loss: 3.7291347980499268 | BCE Loss: 1.0414279699325562\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.740866661071777 | KNN Loss: 3.683161497116089 | BCE Loss: 1.0577054023742676\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.698359489440918 | KNN Loss: 3.6787378787994385 | BCE Loss: 1.01962149143219\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.707211494445801 | KNN Loss: 3.685359239578247 | BCE Loss: 1.0218521356582642\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.745752334594727 | KNN Loss: 3.675539255142212 | BCE Loss: 1.0702133178710938\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.713014125823975 | KNN Loss: 3.703925609588623 | BCE Loss: 1.0090886354446411\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.785052299499512 | KNN Loss: 3.74043869972229 | BCE Loss: 1.0446138381958008\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.695546627044678 | KNN Loss: 3.6863417625427246 | BCE Loss: 1.0092048645019531\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.728549003601074 | KNN Loss: 3.6891942024230957 | BCE Loss: 1.0393545627593994\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.7042036056518555 | KNN Loss: 3.6757450103759766 | BCE Loss: 1.0284583568572998\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.765256404876709 | KNN Loss: 3.722202777862549 | BCE Loss: 1.0430535078048706\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.729313373565674 | KNN Loss: 3.7064125537872314 | BCE Loss: 1.022900938987732\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.729833602905273 | KNN Loss: 3.701138734817505 | BCE Loss: 1.028694748878479\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.744871139526367 | KNN Loss: 3.715256929397583 | BCE Loss: 1.0296142101287842\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.661343574523926 | KNN Loss: 3.663616418838501 | BCE Loss: 0.9977270364761353\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.6968560218811035 | KNN Loss: 3.660036563873291 | BCE Loss: 1.0368194580078125\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.720517158508301 | KNN Loss: 3.6936240196228027 | BCE Loss: 1.0268933773040771\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.71127986907959 | KNN Loss: 3.670654296875 | BCE Loss: 1.0406253337860107\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.70426082611084 | KNN Loss: 3.6873116493225098 | BCE Loss: 1.0169494152069092\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.710433006286621 | KNN Loss: 3.701277732849121 | BCE Loss: 1.0091551542282104\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.75269889831543 | KNN Loss: 3.7100741863250732 | BCE Loss: 1.042624831199646\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.672625541687012 | KNN Loss: 3.690473794937134 | BCE Loss: 0.9821519255638123\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.723797798156738 | KNN Loss: 3.7164318561553955 | BCE Loss: 1.0073659420013428\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.6927690505981445 | KNN Loss: 3.6657588481903076 | BCE Loss: 1.0270099639892578\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.6883039474487305 | KNN Loss: 3.690892219543457 | BCE Loss: 0.9974114894866943\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.703210830688477 | KNN Loss: 3.6727261543273926 | BCE Loss: 1.030484914779663\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.752468109130859 | KNN Loss: 3.7249867916107178 | BCE Loss: 1.0274813175201416\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.696108818054199 | KNN Loss: 3.6657612323760986 | BCE Loss: 1.0303473472595215\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.688517093658447 | KNN Loss: 3.653404712677002 | BCE Loss: 1.0351125001907349\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.705284118652344 | KNN Loss: 3.663790702819824 | BCE Loss: 1.0414936542510986\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.740286827087402 | KNN Loss: 3.698415756225586 | BCE Loss: 1.0418710708618164\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.743379592895508 | KNN Loss: 3.7124197483062744 | BCE Loss: 1.0309600830078125\n",
      "Epoch   478: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.6536102294921875 | KNN Loss: 3.6706647872924805 | BCE Loss: 0.982945442199707\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.716335296630859 | KNN Loss: 3.6784942150115967 | BCE Loss: 1.0378410816192627\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.668856620788574 | KNN Loss: 3.6504032611846924 | BCE Loss: 1.018453598022461\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.732091903686523 | KNN Loss: 3.7437479496002197 | BCE Loss: 0.9883439540863037\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.723297119140625 | KNN Loss: 3.6804211139678955 | BCE Loss: 1.0428760051727295\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.785635948181152 | KNN Loss: 3.7263972759246826 | BCE Loss: 1.0592386722564697\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.674349784851074 | KNN Loss: 3.655705213546753 | BCE Loss: 1.0186448097229004\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.668740272521973 | KNN Loss: 3.6506638526916504 | BCE Loss: 1.0180764198303223\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.701981544494629 | KNN Loss: 3.687769651412964 | BCE Loss: 1.0142121315002441\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.702507495880127 | KNN Loss: 3.667170524597168 | BCE Loss: 1.035336971282959\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.714451789855957 | KNN Loss: 3.7071645259857178 | BCE Loss: 1.0072872638702393\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.753904342651367 | KNN Loss: 3.7261056900024414 | BCE Loss: 1.0277986526489258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.7608184814453125 | KNN Loss: 3.7304775714874268 | BCE Loss: 1.0303411483764648\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.711490631103516 | KNN Loss: 3.676882743835449 | BCE Loss: 1.0346076488494873\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.69797420501709 | KNN Loss: 3.6910603046417236 | BCE Loss: 1.006913661956787\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.758475303649902 | KNN Loss: 3.7306647300720215 | BCE Loss: 1.0278103351593018\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.7572455406188965 | KNN Loss: 3.742795705795288 | BCE Loss: 1.014449954032898\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.720937728881836 | KNN Loss: 3.6801161766052246 | BCE Loss: 1.0408214330673218\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.767848491668701 | KNN Loss: 3.7440948486328125 | BCE Loss: 1.0237535238265991\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.680918216705322 | KNN Loss: 3.698016405105591 | BCE Loss: 0.9829018115997314\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.6287841796875 | KNN Loss: 3.6346094608306885 | BCE Loss: 0.9941749572753906\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.726746082305908 | KNN Loss: 3.694746732711792 | BCE Loss: 1.0319994688034058\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.744074821472168 | KNN Loss: 3.703965187072754 | BCE Loss: 1.0401095151901245\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.702507019042969 | KNN Loss: 3.6736392974853516 | BCE Loss: 1.028867483139038\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.64695930480957 | KNN Loss: 3.667876958847046 | BCE Loss: 0.9790823459625244\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.688852787017822 | KNN Loss: 3.6819915771484375 | BCE Loss: 1.0068610906600952\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.734577178955078 | KNN Loss: 3.694863796234131 | BCE Loss: 1.0397133827209473\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.69398832321167 | KNN Loss: 3.674560785293579 | BCE Loss: 1.0194275379180908\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.713465690612793 | KNN Loss: 3.6789982318878174 | BCE Loss: 1.0344672203063965\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.7291483879089355 | KNN Loss: 3.702077865600586 | BCE Loss: 1.02707040309906\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.710333824157715 | KNN Loss: 3.6951916217803955 | BCE Loss: 1.0151424407958984\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.738607406616211 | KNN Loss: 3.6954257488250732 | BCE Loss: 1.0431816577911377\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.703166961669922 | KNN Loss: 3.687652111053467 | BCE Loss: 1.015514612197876\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.7239556312561035 | KNN Loss: 3.6689672470092773 | BCE Loss: 1.0549883842468262\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.744167327880859 | KNN Loss: 3.7413551807403564 | BCE Loss: 1.002812147140503\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.716536998748779 | KNN Loss: 3.6732826232910156 | BCE Loss: 1.0432542562484741\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.746833801269531 | KNN Loss: 3.6970200538635254 | BCE Loss: 1.0498136281967163\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.71968412399292 | KNN Loss: 3.7106056213378906 | BCE Loss: 1.0090783834457397\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.742687225341797 | KNN Loss: 3.6847445964813232 | BCE Loss: 1.0579423904418945\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.700891494750977 | KNN Loss: 3.686231851577759 | BCE Loss: 1.0146596431732178\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.692697048187256 | KNN Loss: 3.67753529548645 | BCE Loss: 1.0151618719100952\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.717738151550293 | KNN Loss: 3.7073659896850586 | BCE Loss: 1.0103721618652344\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.75452184677124 | KNN Loss: 3.712637424468994 | BCE Loss: 1.0418845415115356\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.699458122253418 | KNN Loss: 3.686411142349243 | BCE Loss: 1.0130467414855957\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.714943885803223 | KNN Loss: 3.7069149017333984 | BCE Loss: 1.0080287456512451\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.701440811157227 | KNN Loss: 3.660203695297241 | BCE Loss: 1.0412373542785645\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.729120254516602 | KNN Loss: 3.680314779281616 | BCE Loss: 1.0488054752349854\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.768085479736328 | KNN Loss: 3.7263669967651367 | BCE Loss: 1.041718602180481\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.670893669128418 | KNN Loss: 3.6590418815612793 | BCE Loss: 1.0118516683578491\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.715218544006348 | KNN Loss: 3.6892287731170654 | BCE Loss: 1.0259900093078613\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.67618989944458 | KNN Loss: 3.6490583419799805 | BCE Loss: 1.0271315574645996\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.706539154052734 | KNN Loss: 3.6790504455566406 | BCE Loss: 1.0274887084960938\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.7211151123046875 | KNN Loss: 3.7059078216552734 | BCE Loss: 1.0152075290679932\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.710191249847412 | KNN Loss: 3.6622982025146484 | BCE Loss: 1.0478930473327637\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.752901077270508 | KNN Loss: 3.7237749099731445 | BCE Loss: 1.0291261672973633\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.72675895690918 | KNN Loss: 3.697563886642456 | BCE Loss: 1.0291951894760132\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.697360038757324 | KNN Loss: 3.6726489067077637 | BCE Loss: 1.0247113704681396\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.740338325500488 | KNN Loss: 3.678438901901245 | BCE Loss: 1.0618996620178223\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.7421698570251465 | KNN Loss: 3.6788339614868164 | BCE Loss: 1.06333589553833\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.709905624389648 | KNN Loss: 3.693234920501709 | BCE Loss: 1.0166709423065186\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.770423889160156 | KNN Loss: 3.7199180126190186 | BCE Loss: 1.0505058765411377\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.666070461273193 | KNN Loss: 3.657719373703003 | BCE Loss: 1.00835120677948\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.715126037597656 | KNN Loss: 3.6795170307159424 | BCE Loss: 1.0356090068817139\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.731799125671387 | KNN Loss: 3.716168165206909 | BCE Loss: 1.0156311988830566\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.735830307006836 | KNN Loss: 3.697909355163574 | BCE Loss: 1.0379209518432617\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.719512462615967 | KNN Loss: 3.701690435409546 | BCE Loss: 1.0178221464157104\n",
      "Epoch   489: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.778507709503174 | KNN Loss: 3.7585599422454834 | BCE Loss: 1.0199477672576904\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.724520683288574 | KNN Loss: 3.714193344116211 | BCE Loss: 1.0103273391723633\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.673200607299805 | KNN Loss: 3.6626789569854736 | BCE Loss: 1.0105218887329102\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.713674068450928 | KNN Loss: 3.697988271713257 | BCE Loss: 1.0156856775283813\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.742083549499512 | KNN Loss: 3.7252368927001953 | BCE Loss: 1.0168468952178955\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.714357376098633 | KNN Loss: 3.685744285583496 | BCE Loss: 1.0286128520965576\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.729732513427734 | KNN Loss: 3.692566156387329 | BCE Loss: 1.0371663570404053\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.683403968811035 | KNN Loss: 3.662519931793213 | BCE Loss: 1.0208841562271118\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.685688495635986 | KNN Loss: 3.6761631965637207 | BCE Loss: 1.0095252990722656\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.718247413635254 | KNN Loss: 3.6755428314208984 | BCE Loss: 1.0427043437957764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.692294120788574 | KNN Loss: 3.6814565658569336 | BCE Loss: 1.0108373165130615\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.7550048828125 | KNN Loss: 3.698802947998047 | BCE Loss: 1.0562021732330322\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.733845233917236 | KNN Loss: 3.694500684738159 | BCE Loss: 1.0393446683883667\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.711842060089111 | KNN Loss: 3.6803207397460938 | BCE Loss: 1.031521201133728\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.733510494232178 | KNN Loss: 3.692293643951416 | BCE Loss: 1.0412169694900513\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.728346824645996 | KNN Loss: 3.7061851024627686 | BCE Loss: 1.022161602973938\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.738253116607666 | KNN Loss: 3.701615810394287 | BCE Loss: 1.036637306213379\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.7347564697265625 | KNN Loss: 3.7237050533294678 | BCE Loss: 1.0110516548156738\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.722529411315918 | KNN Loss: 3.7119545936584473 | BCE Loss: 1.0105748176574707\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.7270660400390625 | KNN Loss: 3.706188440322876 | BCE Loss: 1.0208773612976074\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.697620391845703 | KNN Loss: 3.6813418865203857 | BCE Loss: 1.0162787437438965\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.696498394012451 | KNN Loss: 3.676145553588867 | BCE Loss: 1.020352840423584\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.821172714233398 | KNN Loss: 3.772254467010498 | BCE Loss: 1.04891836643219\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.706917762756348 | KNN Loss: 3.6881604194641113 | BCE Loss: 1.0187572240829468\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.702601432800293 | KNN Loss: 3.694995403289795 | BCE Loss: 1.007605791091919\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.7282609939575195 | KNN Loss: 3.705962657928467 | BCE Loss: 1.0222982168197632\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.745587348937988 | KNN Loss: 3.7196245193481445 | BCE Loss: 1.0259628295898438\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.725907802581787 | KNN Loss: 3.681455373764038 | BCE Loss: 1.0444525480270386\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.733606338500977 | KNN Loss: 3.6673386096954346 | BCE Loss: 1.0662676095962524\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.678668022155762 | KNN Loss: 3.669116973876953 | BCE Loss: 1.0095512866973877\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.749078273773193 | KNN Loss: 3.682405471801758 | BCE Loss: 1.0666728019714355\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.712821006774902 | KNN Loss: 3.679781913757324 | BCE Loss: 1.0330390930175781\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.717294692993164 | KNN Loss: 3.7033839225769043 | BCE Loss: 1.0139106512069702\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.681063652038574 | KNN Loss: 3.676206111907959 | BCE Loss: 1.0048574209213257\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.697442054748535 | KNN Loss: 3.7045748233795166 | BCE Loss: 0.9928672313690186\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.759393692016602 | KNN Loss: 3.6990036964416504 | BCE Loss: 1.060389757156372\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.717098236083984 | KNN Loss: 3.6915485858917236 | BCE Loss: 1.0255494117736816\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.720536231994629 | KNN Loss: 3.694329023361206 | BCE Loss: 1.0262072086334229\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.726154804229736 | KNN Loss: 3.7003424167633057 | BCE Loss: 1.0258123874664307\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.70949649810791 | KNN Loss: 3.692168712615967 | BCE Loss: 1.0173275470733643\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.688868045806885 | KNN Loss: 3.653069496154785 | BCE Loss: 1.0357985496520996\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.732920169830322 | KNN Loss: 3.7008919715881348 | BCE Loss: 1.032028079032898\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.746337413787842 | KNN Loss: 3.706129312515259 | BCE Loss: 1.040208101272583\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.705944061279297 | KNN Loss: 3.681232213973999 | BCE Loss: 1.024712085723877\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.719973087310791 | KNN Loss: 3.6960864067077637 | BCE Loss: 1.0238866806030273\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.69864559173584 | KNN Loss: 3.6777524948120117 | BCE Loss: 1.0208930969238281\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.706644058227539 | KNN Loss: 3.684807300567627 | BCE Loss: 1.021836757659912\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.701866149902344 | KNN Loss: 3.6907243728637695 | BCE Loss: 1.0111416578292847\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.735710144042969 | KNN Loss: 3.696359872817993 | BCE Loss: 1.0393502712249756\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.724255561828613 | KNN Loss: 3.6994469165802 | BCE Loss: 1.024808406829834\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.716772079467773 | KNN Loss: 3.693239212036133 | BCE Loss: 1.0235326290130615\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.801501274108887 | KNN Loss: 3.744227886199951 | BCE Loss: 1.057273507118225\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.699641704559326 | KNN Loss: 3.6800549030303955 | BCE Loss: 1.0195866823196411\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.724320888519287 | KNN Loss: 3.6829347610473633 | BCE Loss: 1.0413860082626343\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.722299575805664 | KNN Loss: 3.6944549083709717 | BCE Loss: 1.0278449058532715\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.664116859436035 | KNN Loss: 3.663954257965088 | BCE Loss: 1.0001623630523682\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.7513580322265625 | KNN Loss: 3.6926333904266357 | BCE Loss: 1.0587248802185059\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.738773822784424 | KNN Loss: 3.7175369262695312 | BCE Loss: 1.0212370157241821\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.689510822296143 | KNN Loss: 3.686840534210205 | BCE Loss: 1.002670168876648\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.722420692443848 | KNN Loss: 3.700401544570923 | BCE Loss: 1.0220189094543457\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.708139419555664 | KNN Loss: 3.6891326904296875 | BCE Loss: 1.0190067291259766\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.694498538970947 | KNN Loss: 3.679638147354126 | BCE Loss: 1.0148605108261108\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.682057857513428 | KNN Loss: 3.648977756500244 | BCE Loss: 1.0330801010131836\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.773489952087402 | KNN Loss: 3.7447569370269775 | BCE Loss: 1.028733253479004\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.7071733474731445 | KNN Loss: 3.676492214202881 | BCE Loss: 1.0306811332702637\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.693978786468506 | KNN Loss: 3.6832640171051025 | BCE Loss: 1.0107148885726929\n",
      "Epoch   500: reducing learning rate of group 0 to 7.8888e-08.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4957e+00,  2.2320e+00,  3.2307e+00,  1.8412e+00,  4.1501e+00,\n",
      "          1.0916e+00,  1.3062e+00,  2.3809e+00,  1.4414e+00,  2.5832e+00,\n",
      "          2.7500e+00,  2.0658e+00,  8.5737e-01,  2.2502e+00,  1.5931e+00,\n",
      "          2.1076e+00,  1.5929e+00,  3.8431e+00,  1.5590e+00,  2.7104e+00,\n",
      "          1.8890e+00,  2.1590e+00,  1.8157e+00,  1.7746e+00,  2.1227e+00,\n",
      "          2.1804e+00,  2.2866e+00,  1.7074e+00,  1.2637e+00,  7.7037e-01,\n",
      "         -7.2234e-03,  1.2120e+00,  4.1476e-01,  1.2188e+00,  1.2754e+00,\n",
      "          1.6367e+00,  1.2974e+00,  3.5390e+00,  8.6547e-01,  1.8028e+00,\n",
      "          1.3451e+00, -7.5467e-01,  2.4334e-01,  2.4294e+00,  2.7278e+00,\n",
      "          1.0045e+00, -5.0151e-01,  9.3344e-02,  1.2230e+00,  1.9194e+00,\n",
      "          2.4503e+00, -1.1153e-01,  9.0962e-01,  3.8952e-01, -4.6239e-01,\n",
      "          9.9071e-01,  1.9920e+00,  1.6375e+00,  1.2462e+00,  1.6857e+00,\n",
      "          1.9570e-01,  6.7696e-01,  5.1394e-01,  1.1245e+00,  1.8090e+00,\n",
      "          1.5869e+00, -1.8152e+00,  1.7847e-01,  1.7830e+00,  2.3677e+00,\n",
      "          2.9388e+00,  5.7642e-01,  1.8222e+00,  3.1271e+00,  2.2820e+00,\n",
      "          9.6953e-01,  1.8051e-01,  1.1274e+00,  2.6634e-01,  1.1190e+00,\n",
      "          1.9455e-01,  7.2724e-01,  2.3595e+00, -4.3911e-01,  5.8251e-01,\n",
      "         -8.0936e-01, -2.1799e+00, -2.0203e-01,  8.4003e-01, -1.3900e+00,\n",
      "          6.8821e-01, -2.0756e-01, -2.8598e-01, -1.0058e+00,  8.9617e-01,\n",
      "          1.4494e+00, -8.2069e-01, -7.5239e-01,  2.8829e-01,  1.0056e+00,\n",
      "          8.6434e-01, -1.1166e+00,  1.1135e+00,  1.0130e+00, -1.5452e+00,\n",
      "         -1.1559e+00, -6.0221e-02,  1.9352e-02, -8.2256e-01, -1.6078e+00,\n",
      "         -8.1549e-01, -2.4600e+00, -4.8651e-01,  1.8210e+00,  1.9824e+00,\n",
      "         -1.9661e-01, -5.2583e-01,  1.7482e-01,  1.4830e+00, -2.3079e+00,\n",
      "          5.6589e-01, -2.2400e-03,  5.3318e-01, -6.3995e-01,  5.1463e-01,\n",
      "         -5.7737e-01, -8.2510e-01,  8.5709e-01,  4.5332e-01, -5.3238e-01,\n",
      "          8.0452e-01, -1.0552e+00, -1.1257e+00, -4.5453e-01, -3.1012e-01,\n",
      "          9.3485e-01, -3.7894e-01, -1.1839e-02, -1.8878e+00, -7.1653e-01,\n",
      "         -1.2295e+00,  1.2134e+00, -1.5133e+00, -8.1832e-01, -1.1119e+00,\n",
      "         -4.2751e-01, -1.4700e+00, -1.1001e+00, -2.3965e+00, -1.0065e+00,\n",
      "         -1.4103e+00, -8.9644e-01, -1.6815e+00,  6.8402e-01, -1.4747e+00,\n",
      "         -7.0633e-01, -3.3224e+00,  3.3453e-01,  1.5258e-01, -7.8042e-01,\n",
      "         -2.1392e+00, -1.2579e+00, -1.3403e+00, -1.5106e+00, -2.1586e+00,\n",
      "         -2.0155e+00, -3.2312e+00]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.3224, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.1501, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be78016cf304e14bd8694c3314e41d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 18.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ddf486fd804b8b95445171255454f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2674d7810f2d445c97412eb32293fb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a831389d3ae4f2391c6b96551bb9ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "Epoch: 00 | Batch: 000 / 027 | Total loss: 9.630 | Reg loss: 0.009 | Tree loss: 9.630 | Accuracy: 0.000000 | 0.58 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 027 | Total loss: 9.626 | Reg loss: 0.009 | Tree loss: 9.626 | Accuracy: 0.000000 | 0.434 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 027 | Total loss: 9.620 | Reg loss: 0.008 | Tree loss: 9.620 | Accuracy: 0.000000 | 0.387 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 027 | Total loss: 9.616 | Reg loss: 0.008 | Tree loss: 9.616 | Accuracy: 0.000000 | 0.365 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 027 | Total loss: 9.606 | Reg loss: 0.008 | Tree loss: 9.606 | Accuracy: 0.000000 | 0.351 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 027 | Total loss: 9.603 | Reg loss: 0.008 | Tree loss: 9.603 | Accuracy: 0.000000 | 0.343 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 027 | Total loss: 9.594 | Reg loss: 0.007 | Tree loss: 9.594 | Accuracy: 0.000000 | 0.346 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 027 | Total loss: 9.591 | Reg loss: 0.007 | Tree loss: 9.591 | Accuracy: 0.000000 | 0.341 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 027 | Total loss: 9.585 | Reg loss: 0.007 | Tree loss: 9.585 | Accuracy: 0.000000 | 0.336 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 027 | Total loss: 9.582 | Reg loss: 0.007 | Tree loss: 9.582 | Accuracy: 0.000000 | 0.333 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 027 | Total loss: 9.575 | Reg loss: 0.007 | Tree loss: 9.575 | Accuracy: 0.000000 | 0.331 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 027 | Total loss: 9.569 | Reg loss: 0.007 | Tree loss: 9.569 | Accuracy: 0.000000 | 0.329 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 027 | Total loss: 9.566 | Reg loss: 0.007 | Tree loss: 9.566 | Accuracy: 0.000000 | 0.33 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 027 | Total loss: 9.563 | Reg loss: 0.007 | Tree loss: 9.563 | Accuracy: 0.000000 | 0.331 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 027 | Total loss: 9.554 | Reg loss: 0.008 | Tree loss: 9.554 | Accuracy: 0.000000 | 0.331 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 027 | Total loss: 9.553 | Reg loss: 0.008 | Tree loss: 9.553 | Accuracy: 0.000000 | 0.332 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 027 | Total loss: 9.548 | Reg loss: 0.008 | Tree loss: 9.548 | Accuracy: 0.005859 | 0.331 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 027 | Total loss: 9.536 | Reg loss: 0.008 | Tree loss: 9.536 | Accuracy: 0.007812 | 0.331 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 027 | Total loss: 9.534 | Reg loss: 0.008 | Tree loss: 9.534 | Accuracy: 0.001953 | 0.335 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 027 | Total loss: 9.525 | Reg loss: 0.009 | Tree loss: 9.525 | Accuracy: 0.003906 | 0.338 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 027 | Total loss: 9.519 | Reg loss: 0.009 | Tree loss: 9.519 | Accuracy: 0.003906 | 0.339 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 027 | Total loss: 9.512 | Reg loss: 0.009 | Tree loss: 9.512 | Accuracy: 0.011719 | 0.342 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 027 | Total loss: 9.511 | Reg loss: 0.009 | Tree loss: 9.511 | Accuracy: 0.013672 | 0.34 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 027 | Total loss: 9.508 | Reg loss: 0.010 | Tree loss: 9.508 | Accuracy: 0.011719 | 0.34 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 027 | Total loss: 9.497 | Reg loss: 0.010 | Tree loss: 9.497 | Accuracy: 0.029297 | 0.339 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 027 | Total loss: 9.493 | Reg loss: 0.010 | Tree loss: 9.493 | Accuracy: 0.042969 | 0.336 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 027 | Total loss: 9.522 | Reg loss: 0.010 | Tree loss: 9.522 | Accuracy: 0.000000 | 0.337 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 01 | Batch: 000 / 027 | Total loss: 9.551 | Reg loss: 0.004 | Tree loss: 9.551 | Accuracy: 0.000000 | 0.348 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 027 | Total loss: 9.543 | Reg loss: 0.004 | Tree loss: 9.543 | Accuracy: 0.000000 | 0.345 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 027 | Total loss: 9.542 | Reg loss: 0.004 | Tree loss: 9.542 | Accuracy: 0.000000 | 0.343 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 027 | Total loss: 9.534 | Reg loss: 0.004 | Tree loss: 9.534 | Accuracy: 0.000000 | 0.342 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 027 | Total loss: 9.531 | Reg loss: 0.005 | Tree loss: 9.531 | Accuracy: 0.001953 | 0.343 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 027 | Total loss: 9.526 | Reg loss: 0.005 | Tree loss: 9.526 | Accuracy: 0.000000 | 0.341 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 027 | Total loss: 9.518 | Reg loss: 0.005 | Tree loss: 9.518 | Accuracy: 0.000000 | 0.339 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 027 | Total loss: 9.512 | Reg loss: 0.005 | Tree loss: 9.512 | Accuracy: 0.005859 | 0.338 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 027 | Total loss: 9.507 | Reg loss: 0.006 | Tree loss: 9.507 | Accuracy: 0.013672 | 0.336 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 027 | Total loss: 9.499 | Reg loss: 0.006 | Tree loss: 9.499 | Accuracy: 0.013672 | 0.334 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 027 | Total loss: 9.496 | Reg loss: 0.006 | Tree loss: 9.496 | Accuracy: 0.023438 | 0.333 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 027 | Total loss: 9.493 | Reg loss: 0.006 | Tree loss: 9.493 | Accuracy: 0.023438 | 0.332 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 027 | Total loss: 9.490 | Reg loss: 0.007 | Tree loss: 9.490 | Accuracy: 0.023438 | 0.33 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 027 | Total loss: 9.485 | Reg loss: 0.007 | Tree loss: 9.485 | Accuracy: 0.058594 | 0.329 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 027 | Total loss: 9.476 | Reg loss: 0.007 | Tree loss: 9.476 | Accuracy: 0.080078 | 0.328 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 027 | Total loss: 9.467 | Reg loss: 0.008 | Tree loss: 9.467 | Accuracy: 0.076172 | 0.327 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 027 | Total loss: 9.469 | Reg loss: 0.008 | Tree loss: 9.469 | Accuracy: 0.083984 | 0.326 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 027 | Total loss: 9.460 | Reg loss: 0.008 | Tree loss: 9.460 | Accuracy: 0.082031 | 0.326 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 027 | Total loss: 9.454 | Reg loss: 0.009 | Tree loss: 9.454 | Accuracy: 0.105469 | 0.325 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 027 | Total loss: 9.446 | Reg loss: 0.009 | Tree loss: 9.446 | Accuracy: 0.107422 | 0.324 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 027 | Total loss: 9.445 | Reg loss: 0.009 | Tree loss: 9.445 | Accuracy: 0.119141 | 0.324 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 027 | Total loss: 9.441 | Reg loss: 0.010 | Tree loss: 9.441 | Accuracy: 0.097656 | 0.323 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 027 | Total loss: 9.431 | Reg loss: 0.010 | Tree loss: 9.431 | Accuracy: 0.101562 | 0.322 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 027 | Total loss: 9.426 | Reg loss: 0.010 | Tree loss: 9.426 | Accuracy: 0.115234 | 0.321 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 027 | Total loss: 9.424 | Reg loss: 0.010 | Tree loss: 9.424 | Accuracy: 0.099609 | 0.321 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 027 | Total loss: 9.417 | Reg loss: 0.011 | Tree loss: 9.417 | Accuracy: 0.093750 | 0.32 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 027 | Total loss: 9.396 | Reg loss: 0.011 | Tree loss: 9.396 | Accuracy: 0.166667 | 0.321 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 02 | Batch: 000 / 027 | Total loss: 9.470 | Reg loss: 0.006 | Tree loss: 9.470 | Accuracy: 0.087891 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 027 | Total loss: 9.466 | Reg loss: 0.006 | Tree loss: 9.466 | Accuracy: 0.109375 | 0.33 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 027 | Total loss: 9.461 | Reg loss: 0.006 | Tree loss: 9.461 | Accuracy: 0.101562 | 0.33 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 027 | Total loss: 9.458 | Reg loss: 0.007 | Tree loss: 9.458 | Accuracy: 0.101562 | 0.331 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 027 | Total loss: 9.451 | Reg loss: 0.007 | Tree loss: 9.451 | Accuracy: 0.105469 | 0.33 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 027 | Total loss: 9.445 | Reg loss: 0.007 | Tree loss: 9.445 | Accuracy: 0.099609 | 0.33 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 027 | Total loss: 9.444 | Reg loss: 0.007 | Tree loss: 9.444 | Accuracy: 0.101562 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 027 | Total loss: 9.439 | Reg loss: 0.007 | Tree loss: 9.439 | Accuracy: 0.103516 | 0.329 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 008 / 027 | Total loss: 9.426 | Reg loss: 0.008 | Tree loss: 9.426 | Accuracy: 0.105469 | 0.328 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 027 | Total loss: 9.433 | Reg loss: 0.008 | Tree loss: 9.433 | Accuracy: 0.087891 | 0.328 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 027 | Total loss: 9.418 | Reg loss: 0.008 | Tree loss: 9.418 | Accuracy: 0.097656 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 027 | Total loss: 9.411 | Reg loss: 0.008 | Tree loss: 9.411 | Accuracy: 0.105469 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 027 | Total loss: 9.403 | Reg loss: 0.009 | Tree loss: 9.403 | Accuracy: 0.119141 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 027 | Total loss: 9.394 | Reg loss: 0.009 | Tree loss: 9.394 | Accuracy: 0.134766 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 027 | Total loss: 9.398 | Reg loss: 0.009 | Tree loss: 9.398 | Accuracy: 0.101562 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 027 | Total loss: 9.391 | Reg loss: 0.010 | Tree loss: 9.391 | Accuracy: 0.103516 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 027 | Total loss: 9.384 | Reg loss: 0.010 | Tree loss: 9.384 | Accuracy: 0.105469 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 027 | Total loss: 9.380 | Reg loss: 0.010 | Tree loss: 9.380 | Accuracy: 0.093750 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 027 | Total loss: 9.381 | Reg loss: 0.010 | Tree loss: 9.381 | Accuracy: 0.089844 | 0.33 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 027 | Total loss: 9.373 | Reg loss: 0.011 | Tree loss: 9.373 | Accuracy: 0.082031 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 027 | Total loss: 9.362 | Reg loss: 0.011 | Tree loss: 9.362 | Accuracy: 0.105469 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 027 | Total loss: 9.359 | Reg loss: 0.011 | Tree loss: 9.359 | Accuracy: 0.093750 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 027 | Total loss: 9.346 | Reg loss: 0.012 | Tree loss: 9.346 | Accuracy: 0.107422 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 027 | Total loss: 9.349 | Reg loss: 0.012 | Tree loss: 9.349 | Accuracy: 0.089844 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 027 | Total loss: 9.332 | Reg loss: 0.012 | Tree loss: 9.332 | Accuracy: 0.126953 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 027 | Total loss: 9.335 | Reg loss: 0.013 | Tree loss: 9.335 | Accuracy: 0.095703 | 0.329 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 027 | Total loss: 9.340 | Reg loss: 0.013 | Tree loss: 9.340 | Accuracy: 0.083333 | 0.329 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 03 | Batch: 000 / 027 | Total loss: 9.396 | Reg loss: 0.008 | Tree loss: 9.396 | Accuracy: 0.099609 | 0.333 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 027 | Total loss: 9.387 | Reg loss: 0.008 | Tree loss: 9.387 | Accuracy: 0.103516 | 0.333 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 027 | Total loss: 9.387 | Reg loss: 0.008 | Tree loss: 9.387 | Accuracy: 0.082031 | 0.332 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 027 | Total loss: 9.380 | Reg loss: 0.009 | Tree loss: 9.380 | Accuracy: 0.080078 | 0.332 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 027 | Total loss: 9.369 | Reg loss: 0.009 | Tree loss: 9.369 | Accuracy: 0.123047 | 0.332 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 027 | Total loss: 9.372 | Reg loss: 0.009 | Tree loss: 9.372 | Accuracy: 0.085938 | 0.331 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 027 | Total loss: 9.362 | Reg loss: 0.009 | Tree loss: 9.362 | Accuracy: 0.097656 | 0.331 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 027 | Total loss: 9.351 | Reg loss: 0.009 | Tree loss: 9.351 | Accuracy: 0.107422 | 0.331 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 027 | Total loss: 9.342 | Reg loss: 0.009 | Tree loss: 9.342 | Accuracy: 0.105469 | 0.331 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 027 | Total loss: 9.339 | Reg loss: 0.010 | Tree loss: 9.339 | Accuracy: 0.111328 | 0.332 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 027 | Total loss: 9.331 | Reg loss: 0.010 | Tree loss: 9.331 | Accuracy: 0.095703 | 0.332 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 027 | Total loss: 9.332 | Reg loss: 0.010 | Tree loss: 9.332 | Accuracy: 0.097656 | 0.332 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 027 | Total loss: 9.322 | Reg loss: 0.010 | Tree loss: 9.322 | Accuracy: 0.099609 | 0.332 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 027 | Total loss: 9.314 | Reg loss: 0.011 | Tree loss: 9.314 | Accuracy: 0.109375 | 0.331 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 027 | Total loss: 9.310 | Reg loss: 0.011 | Tree loss: 9.310 | Accuracy: 0.097656 | 0.331 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 027 | Total loss: 9.300 | Reg loss: 0.011 | Tree loss: 9.300 | Accuracy: 0.115234 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 027 | Total loss: 9.296 | Reg loss: 0.012 | Tree loss: 9.296 | Accuracy: 0.113281 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 027 | Total loss: 9.286 | Reg loss: 0.012 | Tree loss: 9.286 | Accuracy: 0.126953 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 027 | Total loss: 9.284 | Reg loss: 0.012 | Tree loss: 9.284 | Accuracy: 0.099609 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 027 | Total loss: 9.272 | Reg loss: 0.013 | Tree loss: 9.272 | Accuracy: 0.101562 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 027 | Total loss: 9.267 | Reg loss: 0.013 | Tree loss: 9.267 | Accuracy: 0.101562 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 027 | Total loss: 9.257 | Reg loss: 0.013 | Tree loss: 9.257 | Accuracy: 0.125000 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 027 | Total loss: 9.252 | Reg loss: 0.013 | Tree loss: 9.252 | Accuracy: 0.109375 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 027 | Total loss: 9.246 | Reg loss: 0.014 | Tree loss: 9.246 | Accuracy: 0.099609 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 027 | Total loss: 9.242 | Reg loss: 0.014 | Tree loss: 9.242 | Accuracy: 0.111328 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 027 | Total loss: 9.234 | Reg loss: 0.014 | Tree loss: 9.234 | Accuracy: 0.105469 | 0.33 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 027 | Total loss: 9.231 | Reg loss: 0.015 | Tree loss: 9.231 | Accuracy: 0.000000 | 0.33 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 04 | Batch: 000 / 027 | Total loss: 9.309 | Reg loss: 0.010 | Tree loss: 9.309 | Accuracy: 0.117188 | 0.333 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 027 | Total loss: 9.305 | Reg loss: 0.010 | Tree loss: 9.305 | Accuracy: 0.093750 | 0.333 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 027 | Total loss: 9.294 | Reg loss: 0.010 | Tree loss: 9.294 | Accuracy: 0.095703 | 0.333 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 027 | Total loss: 9.287 | Reg loss: 0.011 | Tree loss: 9.287 | Accuracy: 0.101562 | 0.333 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 027 | Total loss: 9.278 | Reg loss: 0.011 | Tree loss: 9.278 | Accuracy: 0.107422 | 0.333 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 027 | Total loss: 9.282 | Reg loss: 0.011 | Tree loss: 9.282 | Accuracy: 0.083984 | 0.333 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 027 | Total loss: 9.270 | Reg loss: 0.011 | Tree loss: 9.270 | Accuracy: 0.087891 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 027 | Total loss: 9.258 | Reg loss: 0.011 | Tree loss: 9.258 | Accuracy: 0.103516 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 027 | Total loss: 9.252 | Reg loss: 0.011 | Tree loss: 9.252 | Accuracy: 0.089844 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 027 | Total loss: 9.245 | Reg loss: 0.012 | Tree loss: 9.245 | Accuracy: 0.093750 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 027 | Total loss: 9.238 | Reg loss: 0.012 | Tree loss: 9.238 | Accuracy: 0.082031 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 027 | Total loss: 9.227 | Reg loss: 0.012 | Tree loss: 9.227 | Accuracy: 0.111328 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 027 | Total loss: 9.214 | Reg loss: 0.012 | Tree loss: 9.214 | Accuracy: 0.111328 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 027 | Total loss: 9.206 | Reg loss: 0.013 | Tree loss: 9.206 | Accuracy: 0.138672 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 027 | Total loss: 9.201 | Reg loss: 0.013 | Tree loss: 9.201 | Accuracy: 0.115234 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 027 | Total loss: 9.194 | Reg loss: 0.013 | Tree loss: 9.194 | Accuracy: 0.099609 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 027 | Total loss: 9.187 | Reg loss: 0.014 | Tree loss: 9.187 | Accuracy: 0.095703 | 0.331 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 017 / 027 | Total loss: 9.175 | Reg loss: 0.014 | Tree loss: 9.175 | Accuracy: 0.109375 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 027 | Total loss: 9.161 | Reg loss: 0.014 | Tree loss: 9.161 | Accuracy: 0.113281 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 027 | Total loss: 9.165 | Reg loss: 0.014 | Tree loss: 9.165 | Accuracy: 0.097656 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 027 | Total loss: 9.144 | Reg loss: 0.015 | Tree loss: 9.144 | Accuracy: 0.099609 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 027 | Total loss: 9.134 | Reg loss: 0.015 | Tree loss: 9.134 | Accuracy: 0.134766 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 027 | Total loss: 9.128 | Reg loss: 0.015 | Tree loss: 9.128 | Accuracy: 0.113281 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 027 | Total loss: 9.119 | Reg loss: 0.016 | Tree loss: 9.119 | Accuracy: 0.105469 | 0.332 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 027 | Total loss: 9.115 | Reg loss: 0.016 | Tree loss: 9.115 | Accuracy: 0.093750 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 027 | Total loss: 9.100 | Reg loss: 0.016 | Tree loss: 9.100 | Accuracy: 0.109375 | 0.331 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 027 | Total loss: 9.091 | Reg loss: 0.017 | Tree loss: 9.091 | Accuracy: 0.000000 | 0.331 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 05 | Batch: 000 / 027 | Total loss: 9.209 | Reg loss: 0.012 | Tree loss: 9.209 | Accuracy: 0.095703 | 0.333 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 027 | Total loss: 9.195 | Reg loss: 0.012 | Tree loss: 9.195 | Accuracy: 0.109375 | 0.333 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 027 | Total loss: 9.190 | Reg loss: 0.012 | Tree loss: 9.190 | Accuracy: 0.123047 | 0.333 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 027 | Total loss: 9.184 | Reg loss: 0.012 | Tree loss: 9.184 | Accuracy: 0.074219 | 0.333 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 027 | Total loss: 9.169 | Reg loss: 0.013 | Tree loss: 9.169 | Accuracy: 0.140625 | 0.333 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 027 | Total loss: 9.156 | Reg loss: 0.013 | Tree loss: 9.156 | Accuracy: 0.113281 | 0.332 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 027 | Total loss: 9.148 | Reg loss: 0.013 | Tree loss: 9.148 | Accuracy: 0.123047 | 0.332 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 027 | Total loss: 9.137 | Reg loss: 0.013 | Tree loss: 9.137 | Accuracy: 0.111328 | 0.332 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 027 | Total loss: 9.132 | Reg loss: 0.013 | Tree loss: 9.132 | Accuracy: 0.091797 | 0.332 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 027 | Total loss: 9.120 | Reg loss: 0.014 | Tree loss: 9.120 | Accuracy: 0.119141 | 0.332 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 027 | Total loss: 9.104 | Reg loss: 0.014 | Tree loss: 9.104 | Accuracy: 0.117188 | 0.332 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 027 | Total loss: 9.095 | Reg loss: 0.014 | Tree loss: 9.095 | Accuracy: 0.123047 | 0.332 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 027 | Total loss: 9.091 | Reg loss: 0.014 | Tree loss: 9.091 | Accuracy: 0.105469 | 0.331 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 027 | Total loss: 9.087 | Reg loss: 0.015 | Tree loss: 9.087 | Accuracy: 0.080078 | 0.331 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 027 | Total loss: 9.062 | Reg loss: 0.015 | Tree loss: 9.062 | Accuracy: 0.123047 | 0.331 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 027 | Total loss: 9.059 | Reg loss: 0.015 | Tree loss: 9.059 | Accuracy: 0.097656 | 0.33 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 027 | Total loss: 9.054 | Reg loss: 0.016 | Tree loss: 9.054 | Accuracy: 0.091797 | 0.33 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 027 | Total loss: 9.043 | Reg loss: 0.016 | Tree loss: 9.043 | Accuracy: 0.099609 | 0.33 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 027 | Total loss: 9.028 | Reg loss: 0.016 | Tree loss: 9.028 | Accuracy: 0.070312 | 0.329 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 027 | Total loss: 9.008 | Reg loss: 0.017 | Tree loss: 9.008 | Accuracy: 0.103516 | 0.329 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 027 | Total loss: 8.999 | Reg loss: 0.017 | Tree loss: 8.999 | Accuracy: 0.087891 | 0.329 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 027 | Total loss: 9.001 | Reg loss: 0.017 | Tree loss: 9.001 | Accuracy: 0.095703 | 0.328 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 027 | Total loss: 8.969 | Reg loss: 0.018 | Tree loss: 8.969 | Accuracy: 0.093750 | 0.328 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 027 | Total loss: 8.960 | Reg loss: 0.018 | Tree loss: 8.960 | Accuracy: 0.087891 | 0.328 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 027 | Total loss: 8.935 | Reg loss: 0.019 | Tree loss: 8.935 | Accuracy: 0.115234 | 0.328 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 027 | Total loss: 8.929 | Reg loss: 0.019 | Tree loss: 8.929 | Accuracy: 0.103516 | 0.328 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 027 | Total loss: 8.893 | Reg loss: 0.019 | Tree loss: 8.893 | Accuracy: 0.166667 | 0.328 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 06 | Batch: 000 / 027 | Total loss: 9.086 | Reg loss: 0.014 | Tree loss: 9.086 | Accuracy: 0.091797 | 0.33 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 027 | Total loss: 9.073 | Reg loss: 0.014 | Tree loss: 9.073 | Accuracy: 0.103516 | 0.33 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 027 | Total loss: 9.063 | Reg loss: 0.014 | Tree loss: 9.063 | Accuracy: 0.093750 | 0.33 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 027 | Total loss: 9.058 | Reg loss: 0.014 | Tree loss: 9.058 | Accuracy: 0.080078 | 0.33 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 027 | Total loss: 9.040 | Reg loss: 0.014 | Tree loss: 9.040 | Accuracy: 0.113281 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 027 | Total loss: 9.025 | Reg loss: 0.015 | Tree loss: 9.025 | Accuracy: 0.107422 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 027 | Total loss: 9.014 | Reg loss: 0.015 | Tree loss: 9.014 | Accuracy: 0.105469 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 027 | Total loss: 8.981 | Reg loss: 0.015 | Tree loss: 8.981 | Accuracy: 0.136719 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 027 | Total loss: 8.990 | Reg loss: 0.015 | Tree loss: 8.990 | Accuracy: 0.091797 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 027 | Total loss: 8.982 | Reg loss: 0.016 | Tree loss: 8.982 | Accuracy: 0.083984 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 027 | Total loss: 8.955 | Reg loss: 0.016 | Tree loss: 8.955 | Accuracy: 0.107422 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 027 | Total loss: 8.949 | Reg loss: 0.016 | Tree loss: 8.949 | Accuracy: 0.097656 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 027 | Total loss: 8.929 | Reg loss: 0.017 | Tree loss: 8.929 | Accuracy: 0.109375 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 027 | Total loss: 8.899 | Reg loss: 0.017 | Tree loss: 8.899 | Accuracy: 0.126953 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 027 | Total loss: 8.907 | Reg loss: 0.018 | Tree loss: 8.907 | Accuracy: 0.087891 | 0.329 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 027 | Total loss: 8.886 | Reg loss: 0.018 | Tree loss: 8.886 | Accuracy: 0.103516 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 027 | Total loss: 8.876 | Reg loss: 0.018 | Tree loss: 8.876 | Accuracy: 0.095703 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 027 | Total loss: 8.849 | Reg loss: 0.019 | Tree loss: 8.849 | Accuracy: 0.105469 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 027 | Total loss: 8.851 | Reg loss: 0.019 | Tree loss: 8.851 | Accuracy: 0.089844 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 027 | Total loss: 8.816 | Reg loss: 0.020 | Tree loss: 8.816 | Accuracy: 0.115234 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 027 | Total loss: 8.798 | Reg loss: 0.020 | Tree loss: 8.798 | Accuracy: 0.107422 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 027 | Total loss: 8.789 | Reg loss: 0.021 | Tree loss: 8.789 | Accuracy: 0.070312 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 027 | Total loss: 8.777 | Reg loss: 0.021 | Tree loss: 8.777 | Accuracy: 0.097656 | 0.327 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 027 | Total loss: 8.743 | Reg loss: 0.022 | Tree loss: 8.743 | Accuracy: 0.126953 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 027 | Total loss: 8.733 | Reg loss: 0.022 | Tree loss: 8.733 | Accuracy: 0.134766 | 0.328 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 027 | Total loss: 8.719 | Reg loss: 0.023 | Tree loss: 8.719 | Accuracy: 0.111328 | 0.328 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 026 / 027 | Total loss: 8.677 | Reg loss: 0.023 | Tree loss: 8.677 | Accuracy: 0.083333 | 0.328 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 07 | Batch: 000 / 027 | Total loss: 8.944 | Reg loss: 0.016 | Tree loss: 8.944 | Accuracy: 0.111328 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 027 | Total loss: 8.928 | Reg loss: 0.016 | Tree loss: 8.928 | Accuracy: 0.099609 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 027 | Total loss: 8.905 | Reg loss: 0.016 | Tree loss: 8.905 | Accuracy: 0.125000 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 027 | Total loss: 8.897 | Reg loss: 0.017 | Tree loss: 8.897 | Accuracy: 0.103516 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 027 | Total loss: 8.897 | Reg loss: 0.017 | Tree loss: 8.897 | Accuracy: 0.091797 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 027 | Total loss: 8.848 | Reg loss: 0.017 | Tree loss: 8.848 | Accuracy: 0.121094 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 027 | Total loss: 8.869 | Reg loss: 0.017 | Tree loss: 8.869 | Accuracy: 0.078125 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 027 | Total loss: 8.833 | Reg loss: 0.018 | Tree loss: 8.833 | Accuracy: 0.093750 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 027 | Total loss: 8.819 | Reg loss: 0.018 | Tree loss: 8.819 | Accuracy: 0.095703 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 027 | Total loss: 8.785 | Reg loss: 0.018 | Tree loss: 8.785 | Accuracy: 0.089844 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 027 | Total loss: 8.778 | Reg loss: 0.019 | Tree loss: 8.778 | Accuracy: 0.095703 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 027 | Total loss: 8.754 | Reg loss: 0.019 | Tree loss: 8.754 | Accuracy: 0.093750 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 027 | Total loss: 8.729 | Reg loss: 0.020 | Tree loss: 8.729 | Accuracy: 0.119141 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 027 | Total loss: 8.709 | Reg loss: 0.020 | Tree loss: 8.709 | Accuracy: 0.107422 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 027 | Total loss: 8.696 | Reg loss: 0.020 | Tree loss: 8.696 | Accuracy: 0.115234 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 027 | Total loss: 8.676 | Reg loss: 0.021 | Tree loss: 8.676 | Accuracy: 0.095703 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 027 | Total loss: 8.664 | Reg loss: 0.021 | Tree loss: 8.664 | Accuracy: 0.082031 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 027 | Total loss: 8.640 | Reg loss: 0.022 | Tree loss: 8.640 | Accuracy: 0.093750 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 027 | Total loss: 8.620 | Reg loss: 0.022 | Tree loss: 8.620 | Accuracy: 0.101562 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 027 | Total loss: 8.603 | Reg loss: 0.023 | Tree loss: 8.603 | Accuracy: 0.101562 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 027 | Total loss: 8.590 | Reg loss: 0.023 | Tree loss: 8.590 | Accuracy: 0.103516 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 027 | Total loss: 8.560 | Reg loss: 0.024 | Tree loss: 8.560 | Accuracy: 0.115234 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 027 | Total loss: 8.560 | Reg loss: 0.024 | Tree loss: 8.560 | Accuracy: 0.107422 | 0.33 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 027 | Total loss: 8.534 | Reg loss: 0.025 | Tree loss: 8.534 | Accuracy: 0.099609 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 027 | Total loss: 8.482 | Reg loss: 0.025 | Tree loss: 8.482 | Accuracy: 0.132812 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 027 | Total loss: 8.467 | Reg loss: 0.026 | Tree loss: 8.467 | Accuracy: 0.121094 | 0.329 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 027 | Total loss: 8.397 | Reg loss: 0.026 | Tree loss: 8.397 | Accuracy: 0.250000 | 0.329 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 08 | Batch: 000 / 027 | Total loss: 8.753 | Reg loss: 0.019 | Tree loss: 8.753 | Accuracy: 0.109375 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 027 | Total loss: 8.759 | Reg loss: 0.019 | Tree loss: 8.759 | Accuracy: 0.085938 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 027 | Total loss: 8.735 | Reg loss: 0.019 | Tree loss: 8.735 | Accuracy: 0.111328 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 027 | Total loss: 8.706 | Reg loss: 0.019 | Tree loss: 8.706 | Accuracy: 0.089844 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 027 | Total loss: 8.692 | Reg loss: 0.020 | Tree loss: 8.692 | Accuracy: 0.103516 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 027 | Total loss: 8.684 | Reg loss: 0.020 | Tree loss: 8.684 | Accuracy: 0.099609 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 027 | Total loss: 8.650 | Reg loss: 0.020 | Tree loss: 8.650 | Accuracy: 0.099609 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 027 | Total loss: 8.637 | Reg loss: 0.020 | Tree loss: 8.637 | Accuracy: 0.089844 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 027 | Total loss: 8.620 | Reg loss: 0.021 | Tree loss: 8.620 | Accuracy: 0.097656 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 027 | Total loss: 8.616 | Reg loss: 0.021 | Tree loss: 8.616 | Accuracy: 0.099609 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 027 | Total loss: 8.576 | Reg loss: 0.021 | Tree loss: 8.576 | Accuracy: 0.107422 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 027 | Total loss: 8.521 | Reg loss: 0.022 | Tree loss: 8.521 | Accuracy: 0.111328 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 027 | Total loss: 8.533 | Reg loss: 0.022 | Tree loss: 8.533 | Accuracy: 0.087891 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 027 | Total loss: 8.500 | Reg loss: 0.022 | Tree loss: 8.500 | Accuracy: 0.105469 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 027 | Total loss: 8.488 | Reg loss: 0.023 | Tree loss: 8.488 | Accuracy: 0.115234 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 027 | Total loss: 8.441 | Reg loss: 0.023 | Tree loss: 8.441 | Accuracy: 0.119141 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 027 | Total loss: 8.457 | Reg loss: 0.024 | Tree loss: 8.457 | Accuracy: 0.097656 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 027 | Total loss: 8.428 | Reg loss: 0.024 | Tree loss: 8.428 | Accuracy: 0.103516 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 027 | Total loss: 8.387 | Reg loss: 0.024 | Tree loss: 8.387 | Accuracy: 0.101562 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 027 | Total loss: 8.384 | Reg loss: 0.025 | Tree loss: 8.384 | Accuracy: 0.107422 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 027 | Total loss: 8.363 | Reg loss: 0.025 | Tree loss: 8.363 | Accuracy: 0.105469 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 027 | Total loss: 8.348 | Reg loss: 0.026 | Tree loss: 8.348 | Accuracy: 0.083984 | 0.332 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 027 | Total loss: 8.273 | Reg loss: 0.026 | Tree loss: 8.273 | Accuracy: 0.138672 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 027 | Total loss: 8.283 | Reg loss: 0.027 | Tree loss: 8.283 | Accuracy: 0.103516 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 027 | Total loss: 8.271 | Reg loss: 0.027 | Tree loss: 8.271 | Accuracy: 0.117188 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 027 | Total loss: 8.268 | Reg loss: 0.027 | Tree loss: 8.268 | Accuracy: 0.105469 | 0.331 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 027 | Total loss: 8.268 | Reg loss: 0.028 | Tree loss: 8.268 | Accuracy: 0.083333 | 0.332 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 09 | Batch: 000 / 027 | Total loss: 8.545 | Reg loss: 0.021 | Tree loss: 8.545 | Accuracy: 0.128906 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 027 | Total loss: 8.550 | Reg loss: 0.021 | Tree loss: 8.550 | Accuracy: 0.083984 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 027 | Total loss: 8.522 | Reg loss: 0.021 | Tree loss: 8.522 | Accuracy: 0.111328 | 0.334 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 027 | Total loss: 8.502 | Reg loss: 0.022 | Tree loss: 8.502 | Accuracy: 0.087891 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 027 | Total loss: 8.477 | Reg loss: 0.022 | Tree loss: 8.477 | Accuracy: 0.130859 | 0.334 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 027 | Total loss: 8.462 | Reg loss: 0.022 | Tree loss: 8.462 | Accuracy: 0.097656 | 0.334 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Batch: 006 / 027 | Total loss: 8.447 | Reg loss: 0.022 | Tree loss: 8.447 | Accuracy: 0.105469 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 027 | Total loss: 8.458 | Reg loss: 0.022 | Tree loss: 8.458 | Accuracy: 0.082031 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 027 | Total loss: 8.404 | Reg loss: 0.023 | Tree loss: 8.404 | Accuracy: 0.089844 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 027 | Total loss: 8.377 | Reg loss: 0.023 | Tree loss: 8.377 | Accuracy: 0.111328 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 027 | Total loss: 8.342 | Reg loss: 0.023 | Tree loss: 8.342 | Accuracy: 0.099609 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 027 | Total loss: 8.327 | Reg loss: 0.024 | Tree loss: 8.327 | Accuracy: 0.121094 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 027 | Total loss: 8.309 | Reg loss: 0.024 | Tree loss: 8.309 | Accuracy: 0.125000 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 027 | Total loss: 8.277 | Reg loss: 0.024 | Tree loss: 8.277 | Accuracy: 0.105469 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 027 | Total loss: 8.261 | Reg loss: 0.025 | Tree loss: 8.261 | Accuracy: 0.095703 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 027 | Total loss: 8.260 | Reg loss: 0.025 | Tree loss: 8.260 | Accuracy: 0.095703 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 027 | Total loss: 8.189 | Reg loss: 0.025 | Tree loss: 8.189 | Accuracy: 0.101562 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 027 | Total loss: 8.211 | Reg loss: 0.026 | Tree loss: 8.211 | Accuracy: 0.089844 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 027 | Total loss: 8.153 | Reg loss: 0.026 | Tree loss: 8.153 | Accuracy: 0.113281 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 027 | Total loss: 8.137 | Reg loss: 0.026 | Tree loss: 8.137 | Accuracy: 0.087891 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 027 | Total loss: 8.118 | Reg loss: 0.027 | Tree loss: 8.118 | Accuracy: 0.109375 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 027 | Total loss: 8.109 | Reg loss: 0.027 | Tree loss: 8.109 | Accuracy: 0.115234 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 027 | Total loss: 8.079 | Reg loss: 0.027 | Tree loss: 8.079 | Accuracy: 0.101562 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 027 | Total loss: 8.040 | Reg loss: 0.028 | Tree loss: 8.040 | Accuracy: 0.117188 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 027 | Total loss: 8.051 | Reg loss: 0.028 | Tree loss: 8.051 | Accuracy: 0.089844 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 027 | Total loss: 8.025 | Reg loss: 0.029 | Tree loss: 8.025 | Accuracy: 0.103516 | 0.333 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 027 | Total loss: 7.937 | Reg loss: 0.029 | Tree loss: 7.937 | Accuracy: 0.166667 | 0.333 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 10 | Batch: 000 / 027 | Total loss: 8.355 | Reg loss: 0.023 | Tree loss: 8.355 | Accuracy: 0.089844 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 027 | Total loss: 8.334 | Reg loss: 0.023 | Tree loss: 8.334 | Accuracy: 0.083984 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 027 | Total loss: 8.275 | Reg loss: 0.023 | Tree loss: 8.275 | Accuracy: 0.111328 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 027 | Total loss: 8.294 | Reg loss: 0.024 | Tree loss: 8.294 | Accuracy: 0.091797 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 027 | Total loss: 8.229 | Reg loss: 0.024 | Tree loss: 8.229 | Accuracy: 0.115234 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 027 | Total loss: 8.218 | Reg loss: 0.024 | Tree loss: 8.218 | Accuracy: 0.097656 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 027 | Total loss: 8.197 | Reg loss: 0.024 | Tree loss: 8.197 | Accuracy: 0.128906 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 027 | Total loss: 8.163 | Reg loss: 0.024 | Tree loss: 8.163 | Accuracy: 0.111328 | 0.335 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 027 | Total loss: 8.159 | Reg loss: 0.024 | Tree loss: 8.159 | Accuracy: 0.074219 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 027 | Total loss: 8.146 | Reg loss: 0.025 | Tree loss: 8.146 | Accuracy: 0.121094 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 027 | Total loss: 8.131 | Reg loss: 0.025 | Tree loss: 8.131 | Accuracy: 0.087891 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 027 | Total loss: 8.066 | Reg loss: 0.025 | Tree loss: 8.066 | Accuracy: 0.105469 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 027 | Total loss: 8.033 | Reg loss: 0.025 | Tree loss: 8.033 | Accuracy: 0.115234 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 027 | Total loss: 8.034 | Reg loss: 0.026 | Tree loss: 8.034 | Accuracy: 0.095703 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 027 | Total loss: 8.042 | Reg loss: 0.026 | Tree loss: 8.042 | Accuracy: 0.095703 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 027 | Total loss: 8.015 | Reg loss: 0.026 | Tree loss: 8.015 | Accuracy: 0.103516 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 027 | Total loss: 7.959 | Reg loss: 0.027 | Tree loss: 7.959 | Accuracy: 0.105469 | 0.336 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 027 | Total loss: 7.938 | Reg loss: 0.027 | Tree loss: 7.938 | Accuracy: 0.101562 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 027 | Total loss: 7.942 | Reg loss: 0.027 | Tree loss: 7.942 | Accuracy: 0.115234 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 027 | Total loss: 7.932 | Reg loss: 0.028 | Tree loss: 7.932 | Accuracy: 0.091797 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 027 | Total loss: 7.847 | Reg loss: 0.028 | Tree loss: 7.847 | Accuracy: 0.138672 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 027 | Total loss: 7.881 | Reg loss: 0.028 | Tree loss: 7.881 | Accuracy: 0.099609 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 027 | Total loss: 7.821 | Reg loss: 0.029 | Tree loss: 7.821 | Accuracy: 0.123047 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 027 | Total loss: 7.806 | Reg loss: 0.029 | Tree loss: 7.806 | Accuracy: 0.091797 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 027 | Total loss: 7.789 | Reg loss: 0.029 | Tree loss: 7.789 | Accuracy: 0.105469 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 027 | Total loss: 7.785 | Reg loss: 0.029 | Tree loss: 7.785 | Accuracy: 0.103516 | 0.337 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 027 | Total loss: 7.750 | Reg loss: 0.030 | Tree loss: 7.750 | Accuracy: 0.000000 | 0.337 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 11 | Batch: 000 / 027 | Total loss: 8.100 | Reg loss: 0.025 | Tree loss: 8.100 | Accuracy: 0.097656 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 027 | Total loss: 8.069 | Reg loss: 0.025 | Tree loss: 8.069 | Accuracy: 0.111328 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 027 | Total loss: 8.027 | Reg loss: 0.025 | Tree loss: 8.027 | Accuracy: 0.107422 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 027 | Total loss: 8.038 | Reg loss: 0.025 | Tree loss: 8.038 | Accuracy: 0.115234 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 027 | Total loss: 8.014 | Reg loss: 0.025 | Tree loss: 8.014 | Accuracy: 0.103516 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 027 | Total loss: 7.961 | Reg loss: 0.025 | Tree loss: 7.961 | Accuracy: 0.126953 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 027 | Total loss: 7.957 | Reg loss: 0.026 | Tree loss: 7.957 | Accuracy: 0.093750 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 027 | Total loss: 7.940 | Reg loss: 0.026 | Tree loss: 7.940 | Accuracy: 0.089844 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 027 | Total loss: 7.920 | Reg loss: 0.026 | Tree loss: 7.920 | Accuracy: 0.105469 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 027 | Total loss: 7.896 | Reg loss: 0.026 | Tree loss: 7.896 | Accuracy: 0.082031 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 027 | Total loss: 7.836 | Reg loss: 0.026 | Tree loss: 7.836 | Accuracy: 0.109375 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 027 | Total loss: 7.860 | Reg loss: 0.027 | Tree loss: 7.860 | Accuracy: 0.072266 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 027 | Total loss: 7.801 | Reg loss: 0.027 | Tree loss: 7.801 | Accuracy: 0.109375 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 027 | Total loss: 7.761 | Reg loss: 0.027 | Tree loss: 7.761 | Accuracy: 0.117188 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 027 | Total loss: 7.767 | Reg loss: 0.027 | Tree loss: 7.767 | Accuracy: 0.115234 | 0.339 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batch: 015 / 027 | Total loss: 7.717 | Reg loss: 0.028 | Tree loss: 7.717 | Accuracy: 0.109375 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 027 | Total loss: 7.720 | Reg loss: 0.028 | Tree loss: 7.720 | Accuracy: 0.089844 | 0.338 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 027 | Total loss: 7.693 | Reg loss: 0.028 | Tree loss: 7.693 | Accuracy: 0.097656 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 027 | Total loss: 7.647 | Reg loss: 0.028 | Tree loss: 7.647 | Accuracy: 0.126953 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 027 | Total loss: 7.672 | Reg loss: 0.029 | Tree loss: 7.672 | Accuracy: 0.082031 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 027 | Total loss: 7.640 | Reg loss: 0.029 | Tree loss: 7.640 | Accuracy: 0.103516 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 027 | Total loss: 7.608 | Reg loss: 0.029 | Tree loss: 7.608 | Accuracy: 0.109375 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 027 | Total loss: 7.583 | Reg loss: 0.029 | Tree loss: 7.583 | Accuracy: 0.107422 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 027 | Total loss: 7.581 | Reg loss: 0.030 | Tree loss: 7.581 | Accuracy: 0.121094 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 027 | Total loss: 7.572 | Reg loss: 0.030 | Tree loss: 7.572 | Accuracy: 0.095703 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 027 | Total loss: 7.529 | Reg loss: 0.030 | Tree loss: 7.529 | Accuracy: 0.107422 | 0.339 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 027 | Total loss: 7.552 | Reg loss: 0.030 | Tree loss: 7.552 | Accuracy: 0.000000 | 0.339 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 12 | Batch: 000 / 027 | Total loss: 7.832 | Reg loss: 0.026 | Tree loss: 7.832 | Accuracy: 0.113281 | 0.34 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 027 | Total loss: 7.812 | Reg loss: 0.027 | Tree loss: 7.812 | Accuracy: 0.101562 | 0.34 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 027 | Total loss: 7.785 | Reg loss: 0.027 | Tree loss: 7.785 | Accuracy: 0.125000 | 0.34 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 027 | Total loss: 7.773 | Reg loss: 0.027 | Tree loss: 7.773 | Accuracy: 0.093750 | 0.34 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 027 | Total loss: 7.724 | Reg loss: 0.027 | Tree loss: 7.724 | Accuracy: 0.105469 | 0.34 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 027 | Total loss: 7.744 | Reg loss: 0.027 | Tree loss: 7.744 | Accuracy: 0.103516 | 0.34 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 027 | Total loss: 7.678 | Reg loss: 0.027 | Tree loss: 7.678 | Accuracy: 0.111328 | 0.34 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 027 | Total loss: 7.666 | Reg loss: 0.027 | Tree loss: 7.666 | Accuracy: 0.093750 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 027 | Total loss: 7.659 | Reg loss: 0.027 | Tree loss: 7.659 | Accuracy: 0.105469 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 027 | Total loss: 7.623 | Reg loss: 0.027 | Tree loss: 7.623 | Accuracy: 0.101562 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 027 | Total loss: 7.595 | Reg loss: 0.028 | Tree loss: 7.595 | Accuracy: 0.101562 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 027 | Total loss: 7.551 | Reg loss: 0.028 | Tree loss: 7.551 | Accuracy: 0.105469 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 027 | Total loss: 7.572 | Reg loss: 0.028 | Tree loss: 7.572 | Accuracy: 0.107422 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 027 | Total loss: 7.538 | Reg loss: 0.028 | Tree loss: 7.538 | Accuracy: 0.093750 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 027 | Total loss: 7.507 | Reg loss: 0.028 | Tree loss: 7.507 | Accuracy: 0.105469 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 027 | Total loss: 7.490 | Reg loss: 0.029 | Tree loss: 7.490 | Accuracy: 0.093750 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 027 | Total loss: 7.460 | Reg loss: 0.029 | Tree loss: 7.460 | Accuracy: 0.091797 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 027 | Total loss: 7.448 | Reg loss: 0.029 | Tree loss: 7.448 | Accuracy: 0.113281 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 027 | Total loss: 7.437 | Reg loss: 0.029 | Tree loss: 7.437 | Accuracy: 0.083984 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 027 | Total loss: 7.473 | Reg loss: 0.029 | Tree loss: 7.473 | Accuracy: 0.087891 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 027 | Total loss: 7.379 | Reg loss: 0.030 | Tree loss: 7.379 | Accuracy: 0.119141 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 027 | Total loss: 7.371 | Reg loss: 0.030 | Tree loss: 7.371 | Accuracy: 0.113281 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 027 | Total loss: 7.348 | Reg loss: 0.030 | Tree loss: 7.348 | Accuracy: 0.093750 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 027 | Total loss: 7.318 | Reg loss: 0.030 | Tree loss: 7.318 | Accuracy: 0.121094 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 027 | Total loss: 7.353 | Reg loss: 0.030 | Tree loss: 7.353 | Accuracy: 0.107422 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 027 | Total loss: 7.289 | Reg loss: 0.031 | Tree loss: 7.289 | Accuracy: 0.111328 | 0.341 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 027 | Total loss: 7.303 | Reg loss: 0.031 | Tree loss: 7.303 | Accuracy: 0.000000 | 0.341 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 13 | Batch: 000 / 027 | Total loss: 7.577 | Reg loss: 0.028 | Tree loss: 7.577 | Accuracy: 0.130859 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 027 | Total loss: 7.538 | Reg loss: 0.028 | Tree loss: 7.538 | Accuracy: 0.126953 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 027 | Total loss: 7.567 | Reg loss: 0.028 | Tree loss: 7.567 | Accuracy: 0.085938 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 027 | Total loss: 7.504 | Reg loss: 0.028 | Tree loss: 7.504 | Accuracy: 0.126953 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 027 | Total loss: 7.498 | Reg loss: 0.028 | Tree loss: 7.498 | Accuracy: 0.099609 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 027 | Total loss: 7.456 | Reg loss: 0.028 | Tree loss: 7.456 | Accuracy: 0.115234 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 027 | Total loss: 7.461 | Reg loss: 0.028 | Tree loss: 7.461 | Accuracy: 0.080078 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 027 | Total loss: 7.403 | Reg loss: 0.028 | Tree loss: 7.403 | Accuracy: 0.101562 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 027 | Total loss: 7.386 | Reg loss: 0.028 | Tree loss: 7.386 | Accuracy: 0.107422 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 027 | Total loss: 7.384 | Reg loss: 0.028 | Tree loss: 7.384 | Accuracy: 0.091797 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 027 | Total loss: 7.375 | Reg loss: 0.028 | Tree loss: 7.375 | Accuracy: 0.095703 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 027 | Total loss: 7.321 | Reg loss: 0.029 | Tree loss: 7.321 | Accuracy: 0.121094 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 027 | Total loss: 7.305 | Reg loss: 0.029 | Tree loss: 7.305 | Accuracy: 0.123047 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 027 | Total loss: 7.302 | Reg loss: 0.029 | Tree loss: 7.302 | Accuracy: 0.089844 | 0.342 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 027 | Total loss: 7.287 | Reg loss: 0.029 | Tree loss: 7.287 | Accuracy: 0.113281 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 027 | Total loss: 7.234 | Reg loss: 0.029 | Tree loss: 7.234 | Accuracy: 0.105469 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 027 | Total loss: 7.223 | Reg loss: 0.029 | Tree loss: 7.223 | Accuracy: 0.107422 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 027 | Total loss: 7.208 | Reg loss: 0.030 | Tree loss: 7.208 | Accuracy: 0.099609 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 027 | Total loss: 7.180 | Reg loss: 0.030 | Tree loss: 7.180 | Accuracy: 0.125000 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 027 | Total loss: 7.172 | Reg loss: 0.030 | Tree loss: 7.172 | Accuracy: 0.076172 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 027 | Total loss: 7.161 | Reg loss: 0.030 | Tree loss: 7.161 | Accuracy: 0.103516 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 027 | Total loss: 7.146 | Reg loss: 0.030 | Tree loss: 7.146 | Accuracy: 0.087891 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 027 | Total loss: 7.116 | Reg loss: 0.030 | Tree loss: 7.116 | Accuracy: 0.097656 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 027 | Total loss: 7.126 | Reg loss: 0.031 | Tree loss: 7.126 | Accuracy: 0.093750 | 0.343 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 024 / 027 | Total loss: 7.099 | Reg loss: 0.031 | Tree loss: 7.099 | Accuracy: 0.099609 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 027 | Total loss: 7.060 | Reg loss: 0.031 | Tree loss: 7.060 | Accuracy: 0.095703 | 0.343 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 027 | Total loss: 7.194 | Reg loss: 0.031 | Tree loss: 7.194 | Accuracy: 0.250000 | 0.344 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 14 | Batch: 000 / 027 | Total loss: 7.301 | Reg loss: 0.028 | Tree loss: 7.301 | Accuracy: 0.103516 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 027 | Total loss: 7.322 | Reg loss: 0.029 | Tree loss: 7.322 | Accuracy: 0.091797 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 027 | Total loss: 7.276 | Reg loss: 0.029 | Tree loss: 7.276 | Accuracy: 0.109375 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 027 | Total loss: 7.283 | Reg loss: 0.029 | Tree loss: 7.283 | Accuracy: 0.113281 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 027 | Total loss: 7.259 | Reg loss: 0.029 | Tree loss: 7.259 | Accuracy: 0.072266 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 027 | Total loss: 7.244 | Reg loss: 0.029 | Tree loss: 7.244 | Accuracy: 0.093750 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 027 | Total loss: 7.200 | Reg loss: 0.029 | Tree loss: 7.200 | Accuracy: 0.080078 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 027 | Total loss: 7.182 | Reg loss: 0.029 | Tree loss: 7.182 | Accuracy: 0.107422 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 027 | Total loss: 7.137 | Reg loss: 0.029 | Tree loss: 7.137 | Accuracy: 0.119141 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 027 | Total loss: 7.156 | Reg loss: 0.029 | Tree loss: 7.156 | Accuracy: 0.105469 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 027 | Total loss: 7.119 | Reg loss: 0.029 | Tree loss: 7.119 | Accuracy: 0.121094 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 027 | Total loss: 7.109 | Reg loss: 0.029 | Tree loss: 7.109 | Accuracy: 0.089844 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 027 | Total loss: 7.042 | Reg loss: 0.029 | Tree loss: 7.042 | Accuracy: 0.123047 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 027 | Total loss: 7.071 | Reg loss: 0.029 | Tree loss: 7.071 | Accuracy: 0.105469 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 027 | Total loss: 7.038 | Reg loss: 0.030 | Tree loss: 7.038 | Accuracy: 0.099609 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 027 | Total loss: 7.004 | Reg loss: 0.030 | Tree loss: 7.004 | Accuracy: 0.105469 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 027 | Total loss: 7.020 | Reg loss: 0.030 | Tree loss: 7.020 | Accuracy: 0.107422 | 0.343 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 027 | Total loss: 6.977 | Reg loss: 0.030 | Tree loss: 6.977 | Accuracy: 0.115234 | 0.343 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 027 | Total loss: 6.981 | Reg loss: 0.030 | Tree loss: 6.981 | Accuracy: 0.111328 | 0.343 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 027 | Total loss: 6.955 | Reg loss: 0.030 | Tree loss: 6.955 | Accuracy: 0.115234 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 027 | Total loss: 6.914 | Reg loss: 0.030 | Tree loss: 6.914 | Accuracy: 0.115234 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 027 | Total loss: 6.899 | Reg loss: 0.031 | Tree loss: 6.899 | Accuracy: 0.121094 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 027 | Total loss: 6.905 | Reg loss: 0.031 | Tree loss: 6.905 | Accuracy: 0.101562 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 027 | Total loss: 6.867 | Reg loss: 0.031 | Tree loss: 6.867 | Accuracy: 0.103516 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 027 | Total loss: 6.855 | Reg loss: 0.031 | Tree loss: 6.855 | Accuracy: 0.099609 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 027 | Total loss: 6.831 | Reg loss: 0.031 | Tree loss: 6.831 | Accuracy: 0.072266 | 0.344 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 027 | Total loss: 6.806 | Reg loss: 0.031 | Tree loss: 6.806 | Accuracy: 0.000000 | 0.344 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 15 | Batch: 000 / 027 | Total loss: 7.084 | Reg loss: 0.029 | Tree loss: 7.084 | Accuracy: 0.111328 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 027 | Total loss: 7.037 | Reg loss: 0.029 | Tree loss: 7.037 | Accuracy: 0.101562 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 027 | Total loss: 7.029 | Reg loss: 0.029 | Tree loss: 7.029 | Accuracy: 0.117188 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 027 | Total loss: 7.011 | Reg loss: 0.029 | Tree loss: 7.011 | Accuracy: 0.103516 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 027 | Total loss: 6.994 | Reg loss: 0.029 | Tree loss: 6.994 | Accuracy: 0.099609 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 027 | Total loss: 6.963 | Reg loss: 0.029 | Tree loss: 6.963 | Accuracy: 0.121094 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 027 | Total loss: 6.921 | Reg loss: 0.029 | Tree loss: 6.921 | Accuracy: 0.117188 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 027 | Total loss: 6.934 | Reg loss: 0.029 | Tree loss: 6.934 | Accuracy: 0.138672 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 027 | Total loss: 6.922 | Reg loss: 0.029 | Tree loss: 6.922 | Accuracy: 0.097656 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 027 | Total loss: 6.903 | Reg loss: 0.030 | Tree loss: 6.903 | Accuracy: 0.095703 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 027 | Total loss: 6.879 | Reg loss: 0.030 | Tree loss: 6.879 | Accuracy: 0.113281 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 027 | Total loss: 6.866 | Reg loss: 0.030 | Tree loss: 6.866 | Accuracy: 0.109375 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 027 | Total loss: 6.867 | Reg loss: 0.030 | Tree loss: 6.867 | Accuracy: 0.087891 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 027 | Total loss: 6.874 | Reg loss: 0.030 | Tree loss: 6.874 | Accuracy: 0.091797 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 027 | Total loss: 6.833 | Reg loss: 0.030 | Tree loss: 6.833 | Accuracy: 0.097656 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 027 | Total loss: 6.828 | Reg loss: 0.030 | Tree loss: 6.828 | Accuracy: 0.076172 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 027 | Total loss: 6.759 | Reg loss: 0.030 | Tree loss: 6.759 | Accuracy: 0.097656 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 027 | Total loss: 6.739 | Reg loss: 0.030 | Tree loss: 6.739 | Accuracy: 0.091797 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 027 | Total loss: 6.785 | Reg loss: 0.030 | Tree loss: 6.785 | Accuracy: 0.083984 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 027 | Total loss: 6.721 | Reg loss: 0.031 | Tree loss: 6.721 | Accuracy: 0.109375 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 027 | Total loss: 6.747 | Reg loss: 0.031 | Tree loss: 6.747 | Accuracy: 0.113281 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 027 | Total loss: 6.687 | Reg loss: 0.031 | Tree loss: 6.687 | Accuracy: 0.101562 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 027 | Total loss: 6.674 | Reg loss: 0.031 | Tree loss: 6.674 | Accuracy: 0.125000 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 027 | Total loss: 6.686 | Reg loss: 0.031 | Tree loss: 6.686 | Accuracy: 0.091797 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 027 | Total loss: 6.633 | Reg loss: 0.031 | Tree loss: 6.633 | Accuracy: 0.109375 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 027 | Total loss: 6.648 | Reg loss: 0.031 | Tree loss: 6.648 | Accuracy: 0.099609 | 0.345 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 027 | Total loss: 6.591 | Reg loss: 0.031 | Tree loss: 6.591 | Accuracy: 0.083333 | 0.345 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 16 | Batch: 000 / 027 | Total loss: 6.851 | Reg loss: 0.030 | Tree loss: 6.851 | Accuracy: 0.111328 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 027 | Total loss: 6.791 | Reg loss: 0.030 | Tree loss: 6.791 | Accuracy: 0.119141 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 027 | Total loss: 6.833 | Reg loss: 0.030 | Tree loss: 6.833 | Accuracy: 0.080078 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 027 | Total loss: 6.779 | Reg loss: 0.030 | Tree loss: 6.779 | Accuracy: 0.097656 | 0.346 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 004 / 027 | Total loss: 6.799 | Reg loss: 0.030 | Tree loss: 6.799 | Accuracy: 0.103516 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 027 | Total loss: 6.778 | Reg loss: 0.030 | Tree loss: 6.778 | Accuracy: 0.103516 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 027 | Total loss: 6.754 | Reg loss: 0.030 | Tree loss: 6.754 | Accuracy: 0.089844 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 027 | Total loss: 6.691 | Reg loss: 0.030 | Tree loss: 6.691 | Accuracy: 0.109375 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 027 | Total loss: 6.724 | Reg loss: 0.030 | Tree loss: 6.724 | Accuracy: 0.087891 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 027 | Total loss: 6.668 | Reg loss: 0.030 | Tree loss: 6.668 | Accuracy: 0.115234 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 027 | Total loss: 6.665 | Reg loss: 0.030 | Tree loss: 6.665 | Accuracy: 0.097656 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 027 | Total loss: 6.657 | Reg loss: 0.030 | Tree loss: 6.657 | Accuracy: 0.109375 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 027 | Total loss: 6.639 | Reg loss: 0.030 | Tree loss: 6.639 | Accuracy: 0.093750 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 027 | Total loss: 6.639 | Reg loss: 0.030 | Tree loss: 6.639 | Accuracy: 0.103516 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 027 | Total loss: 6.634 | Reg loss: 0.030 | Tree loss: 6.634 | Accuracy: 0.105469 | 0.345 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 027 | Total loss: 6.555 | Reg loss: 0.030 | Tree loss: 6.555 | Accuracy: 0.119141 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 027 | Total loss: 6.590 | Reg loss: 0.031 | Tree loss: 6.590 | Accuracy: 0.085938 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 027 | Total loss: 6.543 | Reg loss: 0.031 | Tree loss: 6.543 | Accuracy: 0.125000 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 027 | Total loss: 6.556 | Reg loss: 0.031 | Tree loss: 6.556 | Accuracy: 0.089844 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 027 | Total loss: 6.502 | Reg loss: 0.031 | Tree loss: 6.502 | Accuracy: 0.117188 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 027 | Total loss: 6.509 | Reg loss: 0.031 | Tree loss: 6.509 | Accuracy: 0.099609 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 027 | Total loss: 6.480 | Reg loss: 0.031 | Tree loss: 6.480 | Accuracy: 0.087891 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 027 | Total loss: 6.460 | Reg loss: 0.031 | Tree loss: 6.460 | Accuracy: 0.083984 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 027 | Total loss: 6.413 | Reg loss: 0.031 | Tree loss: 6.413 | Accuracy: 0.128906 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 027 | Total loss: 6.384 | Reg loss: 0.031 | Tree loss: 6.384 | Accuracy: 0.115234 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 027 | Total loss: 6.402 | Reg loss: 0.032 | Tree loss: 6.402 | Accuracy: 0.123047 | 0.346 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 027 | Total loss: 6.343 | Reg loss: 0.032 | Tree loss: 6.343 | Accuracy: 0.000000 | 0.345 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 17 | Batch: 000 / 027 | Total loss: 6.662 | Reg loss: 0.030 | Tree loss: 6.662 | Accuracy: 0.089844 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 027 | Total loss: 6.616 | Reg loss: 0.030 | Tree loss: 6.616 | Accuracy: 0.107422 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 027 | Total loss: 6.594 | Reg loss: 0.030 | Tree loss: 6.594 | Accuracy: 0.113281 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 027 | Total loss: 6.558 | Reg loss: 0.030 | Tree loss: 6.558 | Accuracy: 0.148438 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 027 | Total loss: 6.565 | Reg loss: 0.030 | Tree loss: 6.565 | Accuracy: 0.082031 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 027 | Total loss: 6.508 | Reg loss: 0.030 | Tree loss: 6.508 | Accuracy: 0.105469 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 027 | Total loss: 6.507 | Reg loss: 0.030 | Tree loss: 6.507 | Accuracy: 0.123047 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 027 | Total loss: 6.517 | Reg loss: 0.030 | Tree loss: 6.517 | Accuracy: 0.091797 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 027 | Total loss: 6.504 | Reg loss: 0.030 | Tree loss: 6.504 | Accuracy: 0.093750 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 027 | Total loss: 6.509 | Reg loss: 0.030 | Tree loss: 6.509 | Accuracy: 0.089844 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 027 | Total loss: 6.430 | Reg loss: 0.030 | Tree loss: 6.430 | Accuracy: 0.097656 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 027 | Total loss: 6.441 | Reg loss: 0.030 | Tree loss: 6.441 | Accuracy: 0.107422 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 027 | Total loss: 6.409 | Reg loss: 0.030 | Tree loss: 6.409 | Accuracy: 0.115234 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 027 | Total loss: 6.398 | Reg loss: 0.031 | Tree loss: 6.398 | Accuracy: 0.091797 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 027 | Total loss: 6.383 | Reg loss: 0.031 | Tree loss: 6.383 | Accuracy: 0.117188 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 027 | Total loss: 6.343 | Reg loss: 0.031 | Tree loss: 6.343 | Accuracy: 0.085938 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 027 | Total loss: 6.348 | Reg loss: 0.031 | Tree loss: 6.348 | Accuracy: 0.103516 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 027 | Total loss: 6.324 | Reg loss: 0.031 | Tree loss: 6.324 | Accuracy: 0.119141 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 027 | Total loss: 6.313 | Reg loss: 0.031 | Tree loss: 6.313 | Accuracy: 0.089844 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 027 | Total loss: 6.329 | Reg loss: 0.031 | Tree loss: 6.329 | Accuracy: 0.115234 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 027 | Total loss: 6.267 | Reg loss: 0.031 | Tree loss: 6.267 | Accuracy: 0.099609 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 027 | Total loss: 6.273 | Reg loss: 0.031 | Tree loss: 6.273 | Accuracy: 0.105469 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 027 | Total loss: 6.260 | Reg loss: 0.031 | Tree loss: 6.260 | Accuracy: 0.091797 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 027 | Total loss: 6.241 | Reg loss: 0.031 | Tree loss: 6.241 | Accuracy: 0.111328 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 027 | Total loss: 6.231 | Reg loss: 0.032 | Tree loss: 6.231 | Accuracy: 0.099609 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 027 | Total loss: 6.207 | Reg loss: 0.032 | Tree loss: 6.207 | Accuracy: 0.097656 | 0.346 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 027 | Total loss: 6.240 | Reg loss: 0.032 | Tree loss: 6.240 | Accuracy: 0.416667 | 0.346 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 18 | Batch: 000 / 027 | Total loss: 6.433 | Reg loss: 0.030 | Tree loss: 6.433 | Accuracy: 0.105469 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 027 | Total loss: 6.410 | Reg loss: 0.030 | Tree loss: 6.410 | Accuracy: 0.109375 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 027 | Total loss: 6.369 | Reg loss: 0.030 | Tree loss: 6.369 | Accuracy: 0.107422 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 027 | Total loss: 6.381 | Reg loss: 0.030 | Tree loss: 6.381 | Accuracy: 0.095703 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 027 | Total loss: 6.346 | Reg loss: 0.030 | Tree loss: 6.346 | Accuracy: 0.111328 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 027 | Total loss: 6.325 | Reg loss: 0.030 | Tree loss: 6.325 | Accuracy: 0.132812 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 027 | Total loss: 6.327 | Reg loss: 0.030 | Tree loss: 6.327 | Accuracy: 0.121094 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 027 | Total loss: 6.285 | Reg loss: 0.030 | Tree loss: 6.285 | Accuracy: 0.087891 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 027 | Total loss: 6.296 | Reg loss: 0.030 | Tree loss: 6.296 | Accuracy: 0.085938 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 027 | Total loss: 6.260 | Reg loss: 0.030 | Tree loss: 6.260 | Accuracy: 0.117188 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 027 | Total loss: 6.200 | Reg loss: 0.031 | Tree loss: 6.200 | Accuracy: 0.125000 | 0.347 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 027 | Total loss: 6.262 | Reg loss: 0.031 | Tree loss: 6.262 | Accuracy: 0.103516 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 027 | Total loss: 6.176 | Reg loss: 0.031 | Tree loss: 6.176 | Accuracy: 0.101562 | 0.346 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 013 / 027 | Total loss: 6.198 | Reg loss: 0.031 | Tree loss: 6.198 | Accuracy: 0.095703 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 027 | Total loss: 6.157 | Reg loss: 0.031 | Tree loss: 6.157 | Accuracy: 0.117188 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 027 | Total loss: 6.145 | Reg loss: 0.031 | Tree loss: 6.145 | Accuracy: 0.087891 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 027 | Total loss: 6.121 | Reg loss: 0.031 | Tree loss: 6.121 | Accuracy: 0.113281 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 027 | Total loss: 6.177 | Reg loss: 0.031 | Tree loss: 6.177 | Accuracy: 0.095703 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 027 | Total loss: 6.137 | Reg loss: 0.031 | Tree loss: 6.137 | Accuracy: 0.082031 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 027 | Total loss: 6.110 | Reg loss: 0.031 | Tree loss: 6.110 | Accuracy: 0.105469 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 027 | Total loss: 6.087 | Reg loss: 0.031 | Tree loss: 6.087 | Accuracy: 0.109375 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 027 | Total loss: 6.090 | Reg loss: 0.031 | Tree loss: 6.090 | Accuracy: 0.076172 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 027 | Total loss: 6.032 | Reg loss: 0.032 | Tree loss: 6.032 | Accuracy: 0.103516 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 027 | Total loss: 6.090 | Reg loss: 0.032 | Tree loss: 6.090 | Accuracy: 0.087891 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 027 | Total loss: 6.018 | Reg loss: 0.032 | Tree loss: 6.018 | Accuracy: 0.083984 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 027 | Total loss: 5.976 | Reg loss: 0.032 | Tree loss: 5.976 | Accuracy: 0.125000 | 0.346 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 027 | Total loss: 5.785 | Reg loss: 0.032 | Tree loss: 5.785 | Accuracy: 0.083333 | 0.346 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 19 | Batch: 000 / 027 | Total loss: 6.241 | Reg loss: 0.030 | Tree loss: 6.241 | Accuracy: 0.105469 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 027 | Total loss: 6.190 | Reg loss: 0.030 | Tree loss: 6.190 | Accuracy: 0.125000 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 027 | Total loss: 6.164 | Reg loss: 0.030 | Tree loss: 6.164 | Accuracy: 0.101562 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 027 | Total loss: 6.208 | Reg loss: 0.030 | Tree loss: 6.208 | Accuracy: 0.095703 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 027 | Total loss: 6.116 | Reg loss: 0.030 | Tree loss: 6.116 | Accuracy: 0.111328 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 027 | Total loss: 6.114 | Reg loss: 0.030 | Tree loss: 6.114 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 027 | Total loss: 6.142 | Reg loss: 0.030 | Tree loss: 6.142 | Accuracy: 0.111328 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 027 | Total loss: 6.086 | Reg loss: 0.030 | Tree loss: 6.086 | Accuracy: 0.103516 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 027 | Total loss: 6.107 | Reg loss: 0.031 | Tree loss: 6.107 | Accuracy: 0.107422 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 027 | Total loss: 6.051 | Reg loss: 0.031 | Tree loss: 6.051 | Accuracy: 0.091797 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 027 | Total loss: 6.079 | Reg loss: 0.031 | Tree loss: 6.079 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 027 | Total loss: 6.028 | Reg loss: 0.031 | Tree loss: 6.028 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 027 | Total loss: 5.998 | Reg loss: 0.031 | Tree loss: 5.998 | Accuracy: 0.085938 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 027 | Total loss: 6.023 | Reg loss: 0.031 | Tree loss: 6.023 | Accuracy: 0.105469 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 027 | Total loss: 5.995 | Reg loss: 0.031 | Tree loss: 5.995 | Accuracy: 0.099609 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 027 | Total loss: 5.961 | Reg loss: 0.031 | Tree loss: 5.961 | Accuracy: 0.107422 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 027 | Total loss: 5.958 | Reg loss: 0.031 | Tree loss: 5.958 | Accuracy: 0.105469 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 027 | Total loss: 5.950 | Reg loss: 0.031 | Tree loss: 5.950 | Accuracy: 0.111328 | 0.348 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 027 | Total loss: 5.903 | Reg loss: 0.031 | Tree loss: 5.903 | Accuracy: 0.117188 | 0.348 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 027 | Total loss: 5.867 | Reg loss: 0.031 | Tree loss: 5.867 | Accuracy: 0.125000 | 0.348 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 027 | Total loss: 5.870 | Reg loss: 0.031 | Tree loss: 5.870 | Accuracy: 0.105469 | 0.348 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 027 | Total loss: 5.877 | Reg loss: 0.032 | Tree loss: 5.877 | Accuracy: 0.109375 | 0.348 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 027 | Total loss: 5.879 | Reg loss: 0.032 | Tree loss: 5.879 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 027 | Total loss: 5.828 | Reg loss: 0.032 | Tree loss: 5.828 | Accuracy: 0.093750 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 027 | Total loss: 5.782 | Reg loss: 0.032 | Tree loss: 5.782 | Accuracy: 0.093750 | 0.347 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 027 | Total loss: 5.852 | Reg loss: 0.032 | Tree loss: 5.852 | Accuracy: 0.083984 | 0.348 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 027 | Total loss: 5.583 | Reg loss: 0.032 | Tree loss: 5.583 | Accuracy: 0.250000 | 0.347 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 20 | Batch: 000 / 027 | Total loss: 6.018 | Reg loss: 0.030 | Tree loss: 6.018 | Accuracy: 0.089844 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 027 | Total loss: 6.019 | Reg loss: 0.030 | Tree loss: 6.019 | Accuracy: 0.101562 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 027 | Total loss: 5.973 | Reg loss: 0.030 | Tree loss: 5.973 | Accuracy: 0.101562 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 027 | Total loss: 6.034 | Reg loss: 0.030 | Tree loss: 6.034 | Accuracy: 0.085938 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 027 | Total loss: 5.963 | Reg loss: 0.030 | Tree loss: 5.963 | Accuracy: 0.105469 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 027 | Total loss: 5.937 | Reg loss: 0.030 | Tree loss: 5.937 | Accuracy: 0.109375 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 027 | Total loss: 5.951 | Reg loss: 0.030 | Tree loss: 5.951 | Accuracy: 0.089844 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 027 | Total loss: 5.953 | Reg loss: 0.031 | Tree loss: 5.953 | Accuracy: 0.109375 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 027 | Total loss: 5.909 | Reg loss: 0.031 | Tree loss: 5.909 | Accuracy: 0.097656 | 0.348 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 027 | Total loss: 5.903 | Reg loss: 0.031 | Tree loss: 5.903 | Accuracy: 0.095703 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 027 | Total loss: 5.857 | Reg loss: 0.031 | Tree loss: 5.857 | Accuracy: 0.091797 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 027 | Total loss: 5.811 | Reg loss: 0.031 | Tree loss: 5.811 | Accuracy: 0.082031 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 027 | Total loss: 5.889 | Reg loss: 0.031 | Tree loss: 5.889 | Accuracy: 0.091797 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 027 | Total loss: 5.806 | Reg loss: 0.031 | Tree loss: 5.806 | Accuracy: 0.091797 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 027 | Total loss: 5.769 | Reg loss: 0.031 | Tree loss: 5.769 | Accuracy: 0.111328 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 027 | Total loss: 5.770 | Reg loss: 0.031 | Tree loss: 5.770 | Accuracy: 0.117188 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 027 | Total loss: 5.724 | Reg loss: 0.031 | Tree loss: 5.724 | Accuracy: 0.123047 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 027 | Total loss: 5.698 | Reg loss: 0.031 | Tree loss: 5.698 | Accuracy: 0.136719 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 027 | Total loss: 5.766 | Reg loss: 0.031 | Tree loss: 5.766 | Accuracy: 0.105469 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 027 | Total loss: 5.722 | Reg loss: 0.031 | Tree loss: 5.722 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 027 | Total loss: 5.701 | Reg loss: 0.031 | Tree loss: 5.701 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 027 | Total loss: 5.647 | Reg loss: 0.032 | Tree loss: 5.647 | Accuracy: 0.132812 | 0.347 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 022 / 027 | Total loss: 5.639 | Reg loss: 0.032 | Tree loss: 5.639 | Accuracy: 0.089844 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 027 | Total loss: 5.640 | Reg loss: 0.032 | Tree loss: 5.640 | Accuracy: 0.126953 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 027 | Total loss: 5.633 | Reg loss: 0.032 | Tree loss: 5.633 | Accuracy: 0.095703 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 027 | Total loss: 5.619 | Reg loss: 0.032 | Tree loss: 5.619 | Accuracy: 0.105469 | 0.347 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 027 | Total loss: 5.578 | Reg loss: 0.032 | Tree loss: 5.578 | Accuracy: 0.166667 | 0.347 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 21 | Batch: 000 / 027 | Total loss: 5.854 | Reg loss: 0.030 | Tree loss: 5.854 | Accuracy: 0.101562 | 0.348 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 027 | Total loss: 5.831 | Reg loss: 0.030 | Tree loss: 5.831 | Accuracy: 0.125000 | 0.348 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 027 | Total loss: 5.812 | Reg loss: 0.030 | Tree loss: 5.812 | Accuracy: 0.087891 | 0.348 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 027 | Total loss: 5.746 | Reg loss: 0.030 | Tree loss: 5.746 | Accuracy: 0.125000 | 0.348 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 027 | Total loss: 5.773 | Reg loss: 0.030 | Tree loss: 5.773 | Accuracy: 0.087891 | 0.348 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 027 | Total loss: 5.757 | Reg loss: 0.030 | Tree loss: 5.757 | Accuracy: 0.083984 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 027 | Total loss: 5.764 | Reg loss: 0.030 | Tree loss: 5.764 | Accuracy: 0.109375 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 027 | Total loss: 5.737 | Reg loss: 0.030 | Tree loss: 5.737 | Accuracy: 0.105469 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 027 | Total loss: 5.686 | Reg loss: 0.030 | Tree loss: 5.686 | Accuracy: 0.115234 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 027 | Total loss: 5.662 | Reg loss: 0.031 | Tree loss: 5.662 | Accuracy: 0.103516 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 027 | Total loss: 5.693 | Reg loss: 0.031 | Tree loss: 5.693 | Accuracy: 0.113281 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 027 | Total loss: 5.621 | Reg loss: 0.031 | Tree loss: 5.621 | Accuracy: 0.103516 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 027 | Total loss: 5.648 | Reg loss: 0.031 | Tree loss: 5.648 | Accuracy: 0.103516 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 027 | Total loss: 5.592 | Reg loss: 0.031 | Tree loss: 5.592 | Accuracy: 0.107422 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 027 | Total loss: 5.634 | Reg loss: 0.031 | Tree loss: 5.634 | Accuracy: 0.115234 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 027 | Total loss: 5.633 | Reg loss: 0.031 | Tree loss: 5.633 | Accuracy: 0.109375 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 027 | Total loss: 5.601 | Reg loss: 0.031 | Tree loss: 5.601 | Accuracy: 0.095703 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 027 | Total loss: 5.556 | Reg loss: 0.031 | Tree loss: 5.556 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 027 | Total loss: 5.583 | Reg loss: 0.031 | Tree loss: 5.583 | Accuracy: 0.111328 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 027 | Total loss: 5.516 | Reg loss: 0.031 | Tree loss: 5.516 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 027 | Total loss: 5.469 | Reg loss: 0.032 | Tree loss: 5.469 | Accuracy: 0.119141 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 027 | Total loss: 5.520 | Reg loss: 0.032 | Tree loss: 5.520 | Accuracy: 0.103516 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 027 | Total loss: 5.509 | Reg loss: 0.032 | Tree loss: 5.509 | Accuracy: 0.076172 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 027 | Total loss: 5.484 | Reg loss: 0.032 | Tree loss: 5.484 | Accuracy: 0.080078 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 027 | Total loss: 5.517 | Reg loss: 0.032 | Tree loss: 5.517 | Accuracy: 0.097656 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 027 | Total loss: 5.436 | Reg loss: 0.032 | Tree loss: 5.436 | Accuracy: 0.099609 | 0.347 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 027 | Total loss: 5.294 | Reg loss: 0.032 | Tree loss: 5.294 | Accuracy: 0.166667 | 0.347 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 22 | Batch: 000 / 027 | Total loss: 5.703 | Reg loss: 0.030 | Tree loss: 5.703 | Accuracy: 0.136719 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 027 | Total loss: 5.656 | Reg loss: 0.030 | Tree loss: 5.656 | Accuracy: 0.099609 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 027 | Total loss: 5.655 | Reg loss: 0.030 | Tree loss: 5.655 | Accuracy: 0.080078 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 027 | Total loss: 5.568 | Reg loss: 0.030 | Tree loss: 5.568 | Accuracy: 0.113281 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 027 | Total loss: 5.575 | Reg loss: 0.030 | Tree loss: 5.575 | Accuracy: 0.123047 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 027 | Total loss: 5.559 | Reg loss: 0.030 | Tree loss: 5.559 | Accuracy: 0.113281 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 027 | Total loss: 5.588 | Reg loss: 0.030 | Tree loss: 5.588 | Accuracy: 0.119141 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 027 | Total loss: 5.586 | Reg loss: 0.030 | Tree loss: 5.586 | Accuracy: 0.091797 | 0.347 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 027 | Total loss: 5.566 | Reg loss: 0.030 | Tree loss: 5.566 | Accuracy: 0.099609 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 027 | Total loss: 5.513 | Reg loss: 0.030 | Tree loss: 5.513 | Accuracy: 0.113281 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 027 | Total loss: 5.496 | Reg loss: 0.031 | Tree loss: 5.496 | Accuracy: 0.107422 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 027 | Total loss: 5.508 | Reg loss: 0.031 | Tree loss: 5.508 | Accuracy: 0.093750 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 027 | Total loss: 5.420 | Reg loss: 0.031 | Tree loss: 5.420 | Accuracy: 0.128906 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 027 | Total loss: 5.437 | Reg loss: 0.031 | Tree loss: 5.437 | Accuracy: 0.105469 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 027 | Total loss: 5.474 | Reg loss: 0.031 | Tree loss: 5.474 | Accuracy: 0.080078 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 027 | Total loss: 5.387 | Reg loss: 0.031 | Tree loss: 5.387 | Accuracy: 0.123047 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 027 | Total loss: 5.382 | Reg loss: 0.031 | Tree loss: 5.382 | Accuracy: 0.111328 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 027 | Total loss: 5.405 | Reg loss: 0.031 | Tree loss: 5.405 | Accuracy: 0.085938 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 027 | Total loss: 5.420 | Reg loss: 0.031 | Tree loss: 5.420 | Accuracy: 0.091797 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 027 | Total loss: 5.340 | Reg loss: 0.031 | Tree loss: 5.340 | Accuracy: 0.091797 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 027 | Total loss: 5.327 | Reg loss: 0.032 | Tree loss: 5.327 | Accuracy: 0.097656 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 027 | Total loss: 5.339 | Reg loss: 0.032 | Tree loss: 5.339 | Accuracy: 0.105469 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 027 | Total loss: 5.278 | Reg loss: 0.032 | Tree loss: 5.278 | Accuracy: 0.087891 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 027 | Total loss: 5.240 | Reg loss: 0.032 | Tree loss: 5.240 | Accuracy: 0.109375 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 027 | Total loss: 5.324 | Reg loss: 0.032 | Tree loss: 5.324 | Accuracy: 0.085938 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 027 | Total loss: 5.302 | Reg loss: 0.032 | Tree loss: 5.302 | Accuracy: 0.070312 | 0.348 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 027 | Total loss: 5.390 | Reg loss: 0.032 | Tree loss: 5.390 | Accuracy: 0.000000 | 0.348 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 23 | Batch: 000 / 027 | Total loss: 5.494 | Reg loss: 0.030 | Tree loss: 5.494 | Accuracy: 0.125000 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 027 | Total loss: 5.491 | Reg loss: 0.030 | Tree loss: 5.491 | Accuracy: 0.099609 | 0.348 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 002 / 027 | Total loss: 5.488 | Reg loss: 0.030 | Tree loss: 5.488 | Accuracy: 0.103516 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 027 | Total loss: 5.483 | Reg loss: 0.030 | Tree loss: 5.483 | Accuracy: 0.109375 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 027 | Total loss: 5.467 | Reg loss: 0.030 | Tree loss: 5.467 | Accuracy: 0.103516 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 027 | Total loss: 5.426 | Reg loss: 0.030 | Tree loss: 5.426 | Accuracy: 0.103516 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 027 | Total loss: 5.411 | Reg loss: 0.030 | Tree loss: 5.411 | Accuracy: 0.101562 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 027 | Total loss: 5.388 | Reg loss: 0.030 | Tree loss: 5.388 | Accuracy: 0.101562 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 027 | Total loss: 5.367 | Reg loss: 0.030 | Tree loss: 5.367 | Accuracy: 0.111328 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 027 | Total loss: 5.358 | Reg loss: 0.030 | Tree loss: 5.358 | Accuracy: 0.093750 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 027 | Total loss: 5.322 | Reg loss: 0.030 | Tree loss: 5.322 | Accuracy: 0.103516 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 027 | Total loss: 5.296 | Reg loss: 0.031 | Tree loss: 5.296 | Accuracy: 0.095703 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 027 | Total loss: 5.269 | Reg loss: 0.031 | Tree loss: 5.269 | Accuracy: 0.132812 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 027 | Total loss: 5.219 | Reg loss: 0.031 | Tree loss: 5.219 | Accuracy: 0.093750 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 027 | Total loss: 5.271 | Reg loss: 0.031 | Tree loss: 5.271 | Accuracy: 0.107422 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 027 | Total loss: 5.211 | Reg loss: 0.031 | Tree loss: 5.211 | Accuracy: 0.119141 | 0.348 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 027 | Total loss: 5.263 | Reg loss: 0.031 | Tree loss: 5.263 | Accuracy: 0.072266 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 027 | Total loss: 5.248 | Reg loss: 0.031 | Tree loss: 5.248 | Accuracy: 0.111328 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 027 | Total loss: 5.223 | Reg loss: 0.031 | Tree loss: 5.223 | Accuracy: 0.076172 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 027 | Total loss: 5.176 | Reg loss: 0.032 | Tree loss: 5.176 | Accuracy: 0.101562 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 027 | Total loss: 5.156 | Reg loss: 0.032 | Tree loss: 5.156 | Accuracy: 0.103516 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 027 | Total loss: 5.148 | Reg loss: 0.032 | Tree loss: 5.148 | Accuracy: 0.109375 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 027 | Total loss: 5.166 | Reg loss: 0.032 | Tree loss: 5.166 | Accuracy: 0.083984 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 027 | Total loss: 5.100 | Reg loss: 0.032 | Tree loss: 5.100 | Accuracy: 0.097656 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 027 | Total loss: 5.112 | Reg loss: 0.032 | Tree loss: 5.112 | Accuracy: 0.101562 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 027 | Total loss: 5.116 | Reg loss: 0.032 | Tree loss: 5.116 | Accuracy: 0.082031 | 0.349 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 027 | Total loss: 5.362 | Reg loss: 0.033 | Tree loss: 5.362 | Accuracy: 0.000000 | 0.349 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 24 | Batch: 000 / 027 | Total loss: 5.300 | Reg loss: 0.030 | Tree loss: 5.300 | Accuracy: 0.111328 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 027 | Total loss: 5.297 | Reg loss: 0.030 | Tree loss: 5.297 | Accuracy: 0.111328 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 027 | Total loss: 5.286 | Reg loss: 0.030 | Tree loss: 5.286 | Accuracy: 0.085938 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 027 | Total loss: 5.225 | Reg loss: 0.030 | Tree loss: 5.225 | Accuracy: 0.136719 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 027 | Total loss: 5.210 | Reg loss: 0.030 | Tree loss: 5.210 | Accuracy: 0.087891 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 027 | Total loss: 5.225 | Reg loss: 0.030 | Tree loss: 5.225 | Accuracy: 0.089844 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 027 | Total loss: 5.189 | Reg loss: 0.030 | Tree loss: 5.189 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 027 | Total loss: 5.182 | Reg loss: 0.030 | Tree loss: 5.182 | Accuracy: 0.105469 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 027 | Total loss: 5.191 | Reg loss: 0.030 | Tree loss: 5.191 | Accuracy: 0.117188 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 027 | Total loss: 5.149 | Reg loss: 0.031 | Tree loss: 5.149 | Accuracy: 0.105469 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 027 | Total loss: 5.147 | Reg loss: 0.031 | Tree loss: 5.147 | Accuracy: 0.076172 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 027 | Total loss: 5.105 | Reg loss: 0.031 | Tree loss: 5.105 | Accuracy: 0.085938 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 027 | Total loss: 5.098 | Reg loss: 0.031 | Tree loss: 5.098 | Accuracy: 0.085938 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 027 | Total loss: 5.035 | Reg loss: 0.031 | Tree loss: 5.035 | Accuracy: 0.111328 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 027 | Total loss: 5.081 | Reg loss: 0.031 | Tree loss: 5.081 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 027 | Total loss: 5.044 | Reg loss: 0.031 | Tree loss: 5.044 | Accuracy: 0.117188 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 027 | Total loss: 5.022 | Reg loss: 0.031 | Tree loss: 5.022 | Accuracy: 0.103516 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 027 | Total loss: 5.005 | Reg loss: 0.032 | Tree loss: 5.005 | Accuracy: 0.117188 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 027 | Total loss: 4.964 | Reg loss: 0.032 | Tree loss: 4.964 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 027 | Total loss: 4.999 | Reg loss: 0.032 | Tree loss: 4.999 | Accuracy: 0.078125 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 027 | Total loss: 4.946 | Reg loss: 0.032 | Tree loss: 4.946 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 027 | Total loss: 4.945 | Reg loss: 0.032 | Tree loss: 4.945 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 027 | Total loss: 4.905 | Reg loss: 0.032 | Tree loss: 4.905 | Accuracy: 0.123047 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 027 | Total loss: 4.915 | Reg loss: 0.033 | Tree loss: 4.915 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 027 | Total loss: 4.813 | Reg loss: 0.033 | Tree loss: 4.813 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 027 | Total loss: 4.869 | Reg loss: 0.033 | Tree loss: 4.869 | Accuracy: 0.078125 | 0.35 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 027 | Total loss: 4.716 | Reg loss: 0.033 | Tree loss: 4.716 | Accuracy: 0.000000 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 25 | Batch: 000 / 027 | Total loss: 5.056 | Reg loss: 0.031 | Tree loss: 5.056 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 027 | Total loss: 5.129 | Reg loss: 0.031 | Tree loss: 5.129 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 027 | Total loss: 5.096 | Reg loss: 0.031 | Tree loss: 5.096 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 027 | Total loss: 5.035 | Reg loss: 0.031 | Tree loss: 5.035 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 027 | Total loss: 5.018 | Reg loss: 0.031 | Tree loss: 5.018 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 027 | Total loss: 5.045 | Reg loss: 0.031 | Tree loss: 5.045 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 027 | Total loss: 5.054 | Reg loss: 0.031 | Tree loss: 5.054 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 027 | Total loss: 4.947 | Reg loss: 0.031 | Tree loss: 4.947 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 027 | Total loss: 4.936 | Reg loss: 0.031 | Tree loss: 4.936 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 027 | Total loss: 4.966 | Reg loss: 0.031 | Tree loss: 4.966 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 027 | Total loss: 4.896 | Reg loss: 0.031 | Tree loss: 4.896 | Accuracy: 0.119141 | 0.35 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 011 / 027 | Total loss: 4.922 | Reg loss: 0.031 | Tree loss: 4.922 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 027 | Total loss: 4.862 | Reg loss: 0.031 | Tree loss: 4.862 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 027 | Total loss: 4.843 | Reg loss: 0.032 | Tree loss: 4.843 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 027 | Total loss: 4.802 | Reg loss: 0.032 | Tree loss: 4.802 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 027 | Total loss: 4.818 | Reg loss: 0.032 | Tree loss: 4.818 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 027 | Total loss: 4.870 | Reg loss: 0.032 | Tree loss: 4.870 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 027 | Total loss: 4.748 | Reg loss: 0.032 | Tree loss: 4.748 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 027 | Total loss: 4.763 | Reg loss: 0.032 | Tree loss: 4.763 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 027 | Total loss: 4.763 | Reg loss: 0.032 | Tree loss: 4.763 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 027 | Total loss: 4.765 | Reg loss: 0.033 | Tree loss: 4.765 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 027 | Total loss: 4.703 | Reg loss: 0.033 | Tree loss: 4.703 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 027 | Total loss: 4.736 | Reg loss: 0.033 | Tree loss: 4.736 | Accuracy: 0.076172 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 027 | Total loss: 4.696 | Reg loss: 0.033 | Tree loss: 4.696 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 027 | Total loss: 4.684 | Reg loss: 0.033 | Tree loss: 4.684 | Accuracy: 0.078125 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 027 | Total loss: 4.696 | Reg loss: 0.033 | Tree loss: 4.696 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 027 | Total loss: 4.702 | Reg loss: 0.034 | Tree loss: 4.702 | Accuracy: 0.000000 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 26 | Batch: 000 / 027 | Total loss: 4.885 | Reg loss: 0.031 | Tree loss: 4.885 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 027 | Total loss: 4.913 | Reg loss: 0.031 | Tree loss: 4.913 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 027 | Total loss: 4.873 | Reg loss: 0.031 | Tree loss: 4.873 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 027 | Total loss: 4.809 | Reg loss: 0.031 | Tree loss: 4.809 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 027 | Total loss: 4.850 | Reg loss: 0.031 | Tree loss: 4.850 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 027 | Total loss: 4.886 | Reg loss: 0.031 | Tree loss: 4.886 | Accuracy: 0.078125 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 027 | Total loss: 4.815 | Reg loss: 0.031 | Tree loss: 4.815 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 027 | Total loss: 4.783 | Reg loss: 0.032 | Tree loss: 4.783 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 027 | Total loss: 4.774 | Reg loss: 0.032 | Tree loss: 4.774 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 027 | Total loss: 4.772 | Reg loss: 0.032 | Tree loss: 4.772 | Accuracy: 0.076172 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 027 | Total loss: 4.729 | Reg loss: 0.032 | Tree loss: 4.729 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 027 | Total loss: 4.664 | Reg loss: 0.032 | Tree loss: 4.664 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 027 | Total loss: 4.704 | Reg loss: 0.032 | Tree loss: 4.704 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 027 | Total loss: 4.676 | Reg loss: 0.032 | Tree loss: 4.676 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 027 | Total loss: 4.678 | Reg loss: 0.032 | Tree loss: 4.678 | Accuracy: 0.078125 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 027 | Total loss: 4.617 | Reg loss: 0.032 | Tree loss: 4.617 | Accuracy: 0.103516 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 027 | Total loss: 4.555 | Reg loss: 0.033 | Tree loss: 4.555 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 027 | Total loss: 4.649 | Reg loss: 0.033 | Tree loss: 4.649 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 027 | Total loss: 4.657 | Reg loss: 0.033 | Tree loss: 4.657 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 027 | Total loss: 4.570 | Reg loss: 0.033 | Tree loss: 4.570 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 027 | Total loss: 4.581 | Reg loss: 0.033 | Tree loss: 4.581 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 027 | Total loss: 4.583 | Reg loss: 0.033 | Tree loss: 4.583 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 027 | Total loss: 4.498 | Reg loss: 0.033 | Tree loss: 4.498 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 027 | Total loss: 4.490 | Reg loss: 0.034 | Tree loss: 4.490 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 027 | Total loss: 4.472 | Reg loss: 0.034 | Tree loss: 4.472 | Accuracy: 0.125000 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 027 | Total loss: 4.453 | Reg loss: 0.034 | Tree loss: 4.453 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 027 | Total loss: 4.249 | Reg loss: 0.034 | Tree loss: 4.249 | Accuracy: 0.083333 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 27 | Batch: 000 / 027 | Total loss: 4.737 | Reg loss: 0.032 | Tree loss: 4.737 | Accuracy: 0.119141 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 027 | Total loss: 4.695 | Reg loss: 0.032 | Tree loss: 4.695 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 027 | Total loss: 4.690 | Reg loss: 0.032 | Tree loss: 4.690 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 027 | Total loss: 4.666 | Reg loss: 0.032 | Tree loss: 4.666 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 027 | Total loss: 4.662 | Reg loss: 0.032 | Tree loss: 4.662 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 027 | Total loss: 4.686 | Reg loss: 0.032 | Tree loss: 4.686 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 027 | Total loss: 4.622 | Reg loss: 0.032 | Tree loss: 4.622 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 027 | Total loss: 4.588 | Reg loss: 0.032 | Tree loss: 4.588 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 027 | Total loss: 4.542 | Reg loss: 0.032 | Tree loss: 4.542 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 027 | Total loss: 4.579 | Reg loss: 0.032 | Tree loss: 4.579 | Accuracy: 0.074219 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 027 | Total loss: 4.555 | Reg loss: 0.032 | Tree loss: 4.555 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 027 | Total loss: 4.517 | Reg loss: 0.032 | Tree loss: 4.517 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 027 | Total loss: 4.504 | Reg loss: 0.033 | Tree loss: 4.504 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 027 | Total loss: 4.475 | Reg loss: 0.033 | Tree loss: 4.475 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 027 | Total loss: 4.474 | Reg loss: 0.033 | Tree loss: 4.474 | Accuracy: 0.068359 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 027 | Total loss: 4.523 | Reg loss: 0.033 | Tree loss: 4.523 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 027 | Total loss: 4.503 | Reg loss: 0.033 | Tree loss: 4.503 | Accuracy: 0.126953 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 027 | Total loss: 4.433 | Reg loss: 0.033 | Tree loss: 4.433 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 027 | Total loss: 4.422 | Reg loss: 0.033 | Tree loss: 4.422 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 027 | Total loss: 4.414 | Reg loss: 0.033 | Tree loss: 4.414 | Accuracy: 0.103516 | 0.35 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 020 / 027 | Total loss: 4.400 | Reg loss: 0.034 | Tree loss: 4.400 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 027 | Total loss: 4.406 | Reg loss: 0.034 | Tree loss: 4.406 | Accuracy: 0.076172 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 027 | Total loss: 4.398 | Reg loss: 0.034 | Tree loss: 4.398 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 027 | Total loss: 4.362 | Reg loss: 0.034 | Tree loss: 4.362 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 027 | Total loss: 4.298 | Reg loss: 0.034 | Tree loss: 4.298 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 027 | Total loss: 4.246 | Reg loss: 0.034 | Tree loss: 4.246 | Accuracy: 0.117188 | 0.35 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 027 | Total loss: 4.230 | Reg loss: 0.034 | Tree loss: 4.230 | Accuracy: 0.166667 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 28 | Batch: 000 / 027 | Total loss: 4.559 | Reg loss: 0.032 | Tree loss: 4.559 | Accuracy: 0.111328 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 027 | Total loss: 4.551 | Reg loss: 0.032 | Tree loss: 4.551 | Accuracy: 0.113281 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 027 | Total loss: 4.531 | Reg loss: 0.032 | Tree loss: 4.531 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 027 | Total loss: 4.513 | Reg loss: 0.032 | Tree loss: 4.513 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 027 | Total loss: 4.500 | Reg loss: 0.033 | Tree loss: 4.500 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 027 | Total loss: 4.431 | Reg loss: 0.033 | Tree loss: 4.431 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 027 | Total loss: 4.478 | Reg loss: 0.033 | Tree loss: 4.478 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 027 | Total loss: 4.429 | Reg loss: 0.033 | Tree loss: 4.429 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 027 | Total loss: 4.426 | Reg loss: 0.033 | Tree loss: 4.426 | Accuracy: 0.130859 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 027 | Total loss: 4.385 | Reg loss: 0.033 | Tree loss: 4.385 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 027 | Total loss: 4.367 | Reg loss: 0.033 | Tree loss: 4.367 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 027 | Total loss: 4.416 | Reg loss: 0.033 | Tree loss: 4.416 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 027 | Total loss: 4.304 | Reg loss: 0.033 | Tree loss: 4.304 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 027 | Total loss: 4.298 | Reg loss: 0.033 | Tree loss: 4.298 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 027 | Total loss: 4.316 | Reg loss: 0.033 | Tree loss: 4.316 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 027 | Total loss: 4.329 | Reg loss: 0.033 | Tree loss: 4.329 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 027 | Total loss: 4.327 | Reg loss: 0.034 | Tree loss: 4.327 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 027 | Total loss: 4.254 | Reg loss: 0.034 | Tree loss: 4.254 | Accuracy: 0.121094 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 027 | Total loss: 4.328 | Reg loss: 0.034 | Tree loss: 4.328 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 027 | Total loss: 4.287 | Reg loss: 0.034 | Tree loss: 4.287 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 027 | Total loss: 4.243 | Reg loss: 0.034 | Tree loss: 4.243 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 027 | Total loss: 4.200 | Reg loss: 0.034 | Tree loss: 4.200 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 027 | Total loss: 4.280 | Reg loss: 0.034 | Tree loss: 4.280 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 027 | Total loss: 4.241 | Reg loss: 0.034 | Tree loss: 4.241 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 027 | Total loss: 4.237 | Reg loss: 0.034 | Tree loss: 4.237 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 027 | Total loss: 4.102 | Reg loss: 0.035 | Tree loss: 4.102 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 027 | Total loss: 4.072 | Reg loss: 0.035 | Tree loss: 4.072 | Accuracy: 0.083333 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 29 | Batch: 000 / 027 | Total loss: 4.408 | Reg loss: 0.033 | Tree loss: 4.408 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 027 | Total loss: 4.420 | Reg loss: 0.033 | Tree loss: 4.420 | Accuracy: 0.121094 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 027 | Total loss: 4.393 | Reg loss: 0.033 | Tree loss: 4.393 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 027 | Total loss: 4.387 | Reg loss: 0.033 | Tree loss: 4.387 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 027 | Total loss: 4.352 | Reg loss: 0.033 | Tree loss: 4.352 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 027 | Total loss: 4.355 | Reg loss: 0.033 | Tree loss: 4.355 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 027 | Total loss: 4.294 | Reg loss: 0.033 | Tree loss: 4.294 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 027 | Total loss: 4.257 | Reg loss: 0.033 | Tree loss: 4.257 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 027 | Total loss: 4.336 | Reg loss: 0.033 | Tree loss: 4.336 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 027 | Total loss: 4.219 | Reg loss: 0.033 | Tree loss: 4.219 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 027 | Total loss: 4.225 | Reg loss: 0.033 | Tree loss: 4.225 | Accuracy: 0.111328 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 027 | Total loss: 4.246 | Reg loss: 0.033 | Tree loss: 4.246 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 027 | Total loss: 4.254 | Reg loss: 0.034 | Tree loss: 4.254 | Accuracy: 0.070312 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 027 | Total loss: 4.183 | Reg loss: 0.034 | Tree loss: 4.183 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 027 | Total loss: 4.207 | Reg loss: 0.034 | Tree loss: 4.207 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 027 | Total loss: 4.171 | Reg loss: 0.034 | Tree loss: 4.171 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 027 | Total loss: 4.163 | Reg loss: 0.034 | Tree loss: 4.163 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 027 | Total loss: 4.140 | Reg loss: 0.034 | Tree loss: 4.140 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 027 | Total loss: 4.158 | Reg loss: 0.034 | Tree loss: 4.158 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 027 | Total loss: 4.090 | Reg loss: 0.034 | Tree loss: 4.090 | Accuracy: 0.119141 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 027 | Total loss: 4.150 | Reg loss: 0.034 | Tree loss: 4.150 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 027 | Total loss: 4.111 | Reg loss: 0.034 | Tree loss: 4.111 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 027 | Total loss: 4.120 | Reg loss: 0.035 | Tree loss: 4.120 | Accuracy: 0.078125 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 027 | Total loss: 4.067 | Reg loss: 0.035 | Tree loss: 4.067 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 027 | Total loss: 4.024 | Reg loss: 0.035 | Tree loss: 4.024 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 027 | Total loss: 3.979 | Reg loss: 0.035 | Tree loss: 3.979 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 027 | Total loss: 4.448 | Reg loss: 0.035 | Tree loss: 4.448 | Accuracy: 0.000000 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 000 / 027 | Total loss: 4.253 | Reg loss: 0.033 | Tree loss: 4.253 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 027 | Total loss: 4.216 | Reg loss: 0.033 | Tree loss: 4.216 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 027 | Total loss: 4.243 | Reg loss: 0.033 | Tree loss: 4.243 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 027 | Total loss: 4.199 | Reg loss: 0.033 | Tree loss: 4.199 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 027 | Total loss: 4.192 | Reg loss: 0.033 | Tree loss: 4.192 | Accuracy: 0.113281 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 027 | Total loss: 4.170 | Reg loss: 0.033 | Tree loss: 4.170 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 027 | Total loss: 4.160 | Reg loss: 0.033 | Tree loss: 4.160 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 027 | Total loss: 4.148 | Reg loss: 0.034 | Tree loss: 4.148 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 027 | Total loss: 4.179 | Reg loss: 0.034 | Tree loss: 4.179 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 027 | Total loss: 4.108 | Reg loss: 0.034 | Tree loss: 4.108 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 027 | Total loss: 4.146 | Reg loss: 0.034 | Tree loss: 4.146 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 027 | Total loss: 4.129 | Reg loss: 0.034 | Tree loss: 4.129 | Accuracy: 0.074219 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 027 | Total loss: 4.050 | Reg loss: 0.034 | Tree loss: 4.050 | Accuracy: 0.113281 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 027 | Total loss: 4.132 | Reg loss: 0.034 | Tree loss: 4.132 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 027 | Total loss: 4.129 | Reg loss: 0.034 | Tree loss: 4.129 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 027 | Total loss: 4.068 | Reg loss: 0.034 | Tree loss: 4.068 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 027 | Total loss: 4.046 | Reg loss: 0.034 | Tree loss: 4.046 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 027 | Total loss: 4.081 | Reg loss: 0.034 | Tree loss: 4.081 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 027 | Total loss: 3.964 | Reg loss: 0.034 | Tree loss: 3.964 | Accuracy: 0.121094 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 027 | Total loss: 4.034 | Reg loss: 0.035 | Tree loss: 4.034 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 027 | Total loss: 4.047 | Reg loss: 0.035 | Tree loss: 4.047 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 027 | Total loss: 3.937 | Reg loss: 0.035 | Tree loss: 3.937 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 027 | Total loss: 3.968 | Reg loss: 0.035 | Tree loss: 3.968 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 027 | Total loss: 3.950 | Reg loss: 0.035 | Tree loss: 3.950 | Accuracy: 0.068359 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 027 | Total loss: 3.997 | Reg loss: 0.035 | Tree loss: 3.997 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 027 | Total loss: 3.952 | Reg loss: 0.035 | Tree loss: 3.952 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 027 | Total loss: 3.680 | Reg loss: 0.035 | Tree loss: 3.680 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 31 | Batch: 000 / 027 | Total loss: 4.154 | Reg loss: 0.034 | Tree loss: 4.154 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 027 | Total loss: 4.188 | Reg loss: 0.034 | Tree loss: 4.188 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 027 | Total loss: 4.140 | Reg loss: 0.034 | Tree loss: 4.140 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 027 | Total loss: 4.121 | Reg loss: 0.034 | Tree loss: 4.121 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 027 | Total loss: 4.038 | Reg loss: 0.034 | Tree loss: 4.038 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 027 | Total loss: 4.130 | Reg loss: 0.034 | Tree loss: 4.130 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 027 | Total loss: 4.096 | Reg loss: 0.034 | Tree loss: 4.096 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 027 | Total loss: 4.095 | Reg loss: 0.034 | Tree loss: 4.095 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 027 | Total loss: 4.090 | Reg loss: 0.034 | Tree loss: 4.090 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 027 | Total loss: 3.974 | Reg loss: 0.034 | Tree loss: 3.974 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 027 | Total loss: 4.036 | Reg loss: 0.034 | Tree loss: 4.036 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 027 | Total loss: 4.021 | Reg loss: 0.034 | Tree loss: 4.021 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 027 | Total loss: 3.997 | Reg loss: 0.034 | Tree loss: 3.997 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 027 | Total loss: 3.945 | Reg loss: 0.034 | Tree loss: 3.945 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 027 | Total loss: 3.948 | Reg loss: 0.034 | Tree loss: 3.948 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 027 | Total loss: 4.002 | Reg loss: 0.034 | Tree loss: 4.002 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 027 | Total loss: 3.982 | Reg loss: 0.034 | Tree loss: 3.982 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 027 | Total loss: 3.932 | Reg loss: 0.035 | Tree loss: 3.932 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 027 | Total loss: 3.864 | Reg loss: 0.035 | Tree loss: 3.864 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 027 | Total loss: 3.880 | Reg loss: 0.035 | Tree loss: 3.880 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 027 | Total loss: 3.849 | Reg loss: 0.035 | Tree loss: 3.849 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 027 | Total loss: 3.862 | Reg loss: 0.035 | Tree loss: 3.862 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 027 | Total loss: 3.824 | Reg loss: 0.035 | Tree loss: 3.824 | Accuracy: 0.123047 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 027 | Total loss: 3.813 | Reg loss: 0.035 | Tree loss: 3.813 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 027 | Total loss: 3.840 | Reg loss: 0.035 | Tree loss: 3.840 | Accuracy: 0.119141 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 027 | Total loss: 3.805 | Reg loss: 0.035 | Tree loss: 3.805 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 027 | Total loss: 3.822 | Reg loss: 0.035 | Tree loss: 3.822 | Accuracy: 0.083333 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 32 | Batch: 000 / 027 | Total loss: 4.047 | Reg loss: 0.034 | Tree loss: 4.047 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 027 | Total loss: 3.979 | Reg loss: 0.034 | Tree loss: 3.979 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 027 | Total loss: 4.030 | Reg loss: 0.034 | Tree loss: 4.030 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 027 | Total loss: 4.012 | Reg loss: 0.034 | Tree loss: 4.012 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 027 | Total loss: 3.964 | Reg loss: 0.034 | Tree loss: 3.964 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 027 | Total loss: 4.019 | Reg loss: 0.034 | Tree loss: 4.019 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 027 | Total loss: 3.971 | Reg loss: 0.034 | Tree loss: 3.971 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 027 | Total loss: 3.922 | Reg loss: 0.034 | Tree loss: 3.922 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 027 | Total loss: 3.915 | Reg loss: 0.034 | Tree loss: 3.915 | Accuracy: 0.113281 | 0.352 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 009 / 027 | Total loss: 3.971 | Reg loss: 0.034 | Tree loss: 3.971 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 027 | Total loss: 3.888 | Reg loss: 0.034 | Tree loss: 3.888 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 027 | Total loss: 3.934 | Reg loss: 0.034 | Tree loss: 3.934 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 027 | Total loss: 3.822 | Reg loss: 0.034 | Tree loss: 3.822 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 027 | Total loss: 3.955 | Reg loss: 0.034 | Tree loss: 3.955 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 027 | Total loss: 3.883 | Reg loss: 0.035 | Tree loss: 3.883 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 027 | Total loss: 3.833 | Reg loss: 0.035 | Tree loss: 3.833 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 027 | Total loss: 3.859 | Reg loss: 0.035 | Tree loss: 3.859 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 027 | Total loss: 3.865 | Reg loss: 0.035 | Tree loss: 3.865 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 027 | Total loss: 3.845 | Reg loss: 0.035 | Tree loss: 3.845 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 027 | Total loss: 3.792 | Reg loss: 0.035 | Tree loss: 3.792 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 027 | Total loss: 3.746 | Reg loss: 0.035 | Tree loss: 3.746 | Accuracy: 0.119141 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 027 | Total loss: 3.780 | Reg loss: 0.035 | Tree loss: 3.780 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 027 | Total loss: 3.750 | Reg loss: 0.035 | Tree loss: 3.750 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 027 | Total loss: 3.815 | Reg loss: 0.035 | Tree loss: 3.815 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 027 | Total loss: 3.713 | Reg loss: 0.035 | Tree loss: 3.713 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 027 | Total loss: 3.786 | Reg loss: 0.035 | Tree loss: 3.786 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 027 | Total loss: 4.416 | Reg loss: 0.035 | Tree loss: 4.416 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 33 | Batch: 000 / 027 | Total loss: 3.969 | Reg loss: 0.034 | Tree loss: 3.969 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 027 | Total loss: 4.078 | Reg loss: 0.034 | Tree loss: 4.078 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 027 | Total loss: 3.922 | Reg loss: 0.034 | Tree loss: 3.922 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 027 | Total loss: 3.944 | Reg loss: 0.034 | Tree loss: 3.944 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 027 | Total loss: 3.876 | Reg loss: 0.034 | Tree loss: 3.876 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 027 | Total loss: 3.940 | Reg loss: 0.034 | Tree loss: 3.940 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 027 | Total loss: 3.914 | Reg loss: 0.034 | Tree loss: 3.914 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 027 | Total loss: 3.793 | Reg loss: 0.034 | Tree loss: 3.793 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 027 | Total loss: 3.892 | Reg loss: 0.034 | Tree loss: 3.892 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 027 | Total loss: 3.777 | Reg loss: 0.034 | Tree loss: 3.777 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 027 | Total loss: 3.818 | Reg loss: 0.034 | Tree loss: 3.818 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 027 | Total loss: 3.772 | Reg loss: 0.035 | Tree loss: 3.772 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 027 | Total loss: 3.851 | Reg loss: 0.035 | Tree loss: 3.851 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 027 | Total loss: 3.800 | Reg loss: 0.035 | Tree loss: 3.800 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 027 | Total loss: 3.841 | Reg loss: 0.035 | Tree loss: 3.841 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 027 | Total loss: 3.756 | Reg loss: 0.035 | Tree loss: 3.756 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 027 | Total loss: 3.712 | Reg loss: 0.035 | Tree loss: 3.712 | Accuracy: 0.121094 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 027 | Total loss: 3.772 | Reg loss: 0.035 | Tree loss: 3.772 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 027 | Total loss: 3.706 | Reg loss: 0.035 | Tree loss: 3.706 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 027 | Total loss: 3.705 | Reg loss: 0.035 | Tree loss: 3.705 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 027 | Total loss: 3.742 | Reg loss: 0.035 | Tree loss: 3.742 | Accuracy: 0.068359 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 027 | Total loss: 3.732 | Reg loss: 0.035 | Tree loss: 3.732 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 027 | Total loss: 3.665 | Reg loss: 0.035 | Tree loss: 3.665 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 027 | Total loss: 3.692 | Reg loss: 0.035 | Tree loss: 3.692 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 027 | Total loss: 3.636 | Reg loss: 0.035 | Tree loss: 3.636 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 027 | Total loss: 3.652 | Reg loss: 0.035 | Tree loss: 3.652 | Accuracy: 0.117188 | 0.352 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 027 | Total loss: 3.638 | Reg loss: 0.036 | Tree loss: 3.638 | Accuracy: 0.083333 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 34 | Batch: 000 / 027 | Total loss: 3.872 | Reg loss: 0.034 | Tree loss: 3.872 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 027 | Total loss: 3.892 | Reg loss: 0.034 | Tree loss: 3.892 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 027 | Total loss: 3.864 | Reg loss: 0.034 | Tree loss: 3.864 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 027 | Total loss: 3.845 | Reg loss: 0.034 | Tree loss: 3.845 | Accuracy: 0.123047 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 027 | Total loss: 3.832 | Reg loss: 0.034 | Tree loss: 3.832 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 027 | Total loss: 3.799 | Reg loss: 0.034 | Tree loss: 3.799 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 027 | Total loss: 3.780 | Reg loss: 0.034 | Tree loss: 3.780 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 027 | Total loss: 3.785 | Reg loss: 0.035 | Tree loss: 3.785 | Accuracy: 0.078125 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 027 | Total loss: 3.747 | Reg loss: 0.035 | Tree loss: 3.747 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 027 | Total loss: 3.735 | Reg loss: 0.035 | Tree loss: 3.735 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 027 | Total loss: 3.795 | Reg loss: 0.035 | Tree loss: 3.795 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 027 | Total loss: 3.721 | Reg loss: 0.035 | Tree loss: 3.721 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 027 | Total loss: 3.715 | Reg loss: 0.035 | Tree loss: 3.715 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 027 | Total loss: 3.694 | Reg loss: 0.035 | Tree loss: 3.694 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 027 | Total loss: 3.676 | Reg loss: 0.035 | Tree loss: 3.676 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 027 | Total loss: 3.729 | Reg loss: 0.035 | Tree loss: 3.729 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 027 | Total loss: 3.772 | Reg loss: 0.035 | Tree loss: 3.772 | Accuracy: 0.072266 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 027 | Total loss: 3.632 | Reg loss: 0.035 | Tree loss: 3.632 | Accuracy: 0.113281 | 0.352 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 018 / 027 | Total loss: 3.662 | Reg loss: 0.035 | Tree loss: 3.662 | Accuracy: 0.078125 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 027 | Total loss: 3.672 | Reg loss: 0.035 | Tree loss: 3.672 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 027 | Total loss: 3.695 | Reg loss: 0.035 | Tree loss: 3.695 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 027 | Total loss: 3.624 | Reg loss: 0.035 | Tree loss: 3.624 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 027 | Total loss: 3.623 | Reg loss: 0.035 | Tree loss: 3.623 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 027 | Total loss: 3.638 | Reg loss: 0.035 | Tree loss: 3.638 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 027 | Total loss: 3.601 | Reg loss: 0.035 | Tree loss: 3.601 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 027 | Total loss: 3.563 | Reg loss: 0.036 | Tree loss: 3.563 | Accuracy: 0.111328 | 0.351 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 027 | Total loss: 3.922 | Reg loss: 0.036 | Tree loss: 3.922 | Accuracy: 0.083333 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 35 | Batch: 000 / 027 | Total loss: 3.737 | Reg loss: 0.035 | Tree loss: 3.737 | Accuracy: 0.070312 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 027 | Total loss: 3.782 | Reg loss: 0.035 | Tree loss: 3.782 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 027 | Total loss: 3.796 | Reg loss: 0.035 | Tree loss: 3.796 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 027 | Total loss: 3.804 | Reg loss: 0.035 | Tree loss: 3.804 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 027 | Total loss: 3.798 | Reg loss: 0.035 | Tree loss: 3.798 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 027 | Total loss: 3.785 | Reg loss: 0.035 | Tree loss: 3.785 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 027 | Total loss: 3.727 | Reg loss: 0.035 | Tree loss: 3.727 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 027 | Total loss: 3.777 | Reg loss: 0.035 | Tree loss: 3.777 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 027 | Total loss: 3.694 | Reg loss: 0.035 | Tree loss: 3.694 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 027 | Total loss: 3.735 | Reg loss: 0.035 | Tree loss: 3.735 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 027 | Total loss: 3.658 | Reg loss: 0.035 | Tree loss: 3.658 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 027 | Total loss: 3.658 | Reg loss: 0.035 | Tree loss: 3.658 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 027 | Total loss: 3.671 | Reg loss: 0.035 | Tree loss: 3.671 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 027 | Total loss: 3.644 | Reg loss: 0.035 | Tree loss: 3.644 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 027 | Total loss: 3.627 | Reg loss: 0.035 | Tree loss: 3.627 | Accuracy: 0.121094 | 0.352 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 027 | Total loss: 3.601 | Reg loss: 0.035 | Tree loss: 3.601 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 027 | Total loss: 3.577 | Reg loss: 0.035 | Tree loss: 3.577 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 027 | Total loss: 3.637 | Reg loss: 0.035 | Tree loss: 3.637 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 027 | Total loss: 3.623 | Reg loss: 0.035 | Tree loss: 3.623 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 027 | Total loss: 3.600 | Reg loss: 0.035 | Tree loss: 3.600 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 027 | Total loss: 3.517 | Reg loss: 0.035 | Tree loss: 3.517 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 027 | Total loss: 3.627 | Reg loss: 0.035 | Tree loss: 3.627 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 027 | Total loss: 3.525 | Reg loss: 0.035 | Tree loss: 3.525 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 027 | Total loss: 3.540 | Reg loss: 0.035 | Tree loss: 3.540 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 027 | Total loss: 3.505 | Reg loss: 0.036 | Tree loss: 3.505 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 027 | Total loss: 3.554 | Reg loss: 0.036 | Tree loss: 3.554 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 027 | Total loss: 3.617 | Reg loss: 0.036 | Tree loss: 3.617 | Accuracy: 0.083333 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 36 | Batch: 000 / 027 | Total loss: 3.738 | Reg loss: 0.035 | Tree loss: 3.738 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 027 | Total loss: 3.760 | Reg loss: 0.035 | Tree loss: 3.760 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 027 | Total loss: 3.748 | Reg loss: 0.035 | Tree loss: 3.748 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 027 | Total loss: 3.681 | Reg loss: 0.035 | Tree loss: 3.681 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 027 | Total loss: 3.725 | Reg loss: 0.035 | Tree loss: 3.725 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 027 | Total loss: 3.696 | Reg loss: 0.035 | Tree loss: 3.696 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 027 | Total loss: 3.632 | Reg loss: 0.035 | Tree loss: 3.632 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 027 | Total loss: 3.645 | Reg loss: 0.035 | Tree loss: 3.645 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 027 | Total loss: 3.637 | Reg loss: 0.035 | Tree loss: 3.637 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 027 | Total loss: 3.641 | Reg loss: 0.035 | Tree loss: 3.641 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 027 | Total loss: 3.659 | Reg loss: 0.035 | Tree loss: 3.659 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 027 | Total loss: 3.559 | Reg loss: 0.035 | Tree loss: 3.559 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 027 | Total loss: 3.588 | Reg loss: 0.035 | Tree loss: 3.588 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 027 | Total loss: 3.614 | Reg loss: 0.035 | Tree loss: 3.614 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 027 | Total loss: 3.599 | Reg loss: 0.035 | Tree loss: 3.599 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 027 | Total loss: 3.576 | Reg loss: 0.035 | Tree loss: 3.576 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 027 | Total loss: 3.541 | Reg loss: 0.035 | Tree loss: 3.541 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 027 | Total loss: 3.531 | Reg loss: 0.035 | Tree loss: 3.531 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 027 | Total loss: 3.568 | Reg loss: 0.035 | Tree loss: 3.568 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 027 | Total loss: 3.483 | Reg loss: 0.035 | Tree loss: 3.483 | Accuracy: 0.130859 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 027 | Total loss: 3.554 | Reg loss: 0.035 | Tree loss: 3.554 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 027 | Total loss: 3.517 | Reg loss: 0.035 | Tree loss: 3.517 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 027 | Total loss: 3.506 | Reg loss: 0.035 | Tree loss: 3.506 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 027 | Total loss: 3.462 | Reg loss: 0.035 | Tree loss: 3.462 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 027 | Total loss: 3.553 | Reg loss: 0.036 | Tree loss: 3.553 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 027 | Total loss: 3.499 | Reg loss: 0.036 | Tree loss: 3.499 | Accuracy: 0.074219 | 0.351 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 027 | Total loss: 3.489 | Reg loss: 0.036 | Tree loss: 3.489 | Accuracy: 0.083333 | 0.351 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 37 | Batch: 000 / 027 | Total loss: 3.693 | Reg loss: 0.035 | Tree loss: 3.693 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 027 | Total loss: 3.662 | Reg loss: 0.035 | Tree loss: 3.662 | Accuracy: 0.119141 | 0.351 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 027 | Total loss: 3.635 | Reg loss: 0.035 | Tree loss: 3.635 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 027 | Total loss: 3.647 | Reg loss: 0.035 | Tree loss: 3.647 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 027 | Total loss: 3.660 | Reg loss: 0.035 | Tree loss: 3.660 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 027 | Total loss: 3.637 | Reg loss: 0.035 | Tree loss: 3.637 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 027 | Total loss: 3.589 | Reg loss: 0.035 | Tree loss: 3.589 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 027 | Total loss: 3.649 | Reg loss: 0.035 | Tree loss: 3.649 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 027 | Total loss: 3.548 | Reg loss: 0.035 | Tree loss: 3.548 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 027 | Total loss: 3.629 | Reg loss: 0.035 | Tree loss: 3.629 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 027 | Total loss: 3.557 | Reg loss: 0.035 | Tree loss: 3.557 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 027 | Total loss: 3.590 | Reg loss: 0.035 | Tree loss: 3.590 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 027 | Total loss: 3.629 | Reg loss: 0.035 | Tree loss: 3.629 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 027 | Total loss: 3.558 | Reg loss: 0.035 | Tree loss: 3.558 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 027 | Total loss: 3.558 | Reg loss: 0.035 | Tree loss: 3.558 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 027 | Total loss: 3.511 | Reg loss: 0.035 | Tree loss: 3.511 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 027 | Total loss: 3.494 | Reg loss: 0.035 | Tree loss: 3.494 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 027 | Total loss: 3.481 | Reg loss: 0.035 | Tree loss: 3.481 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 027 | Total loss: 3.500 | Reg loss: 0.035 | Tree loss: 3.500 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 027 | Total loss: 3.519 | Reg loss: 0.035 | Tree loss: 3.519 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 027 | Total loss: 3.427 | Reg loss: 0.035 | Tree loss: 3.427 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 027 | Total loss: 3.444 | Reg loss: 0.035 | Tree loss: 3.444 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 027 | Total loss: 3.458 | Reg loss: 0.035 | Tree loss: 3.458 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 027 | Total loss: 3.467 | Reg loss: 0.035 | Tree loss: 3.467 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 027 | Total loss: 3.386 | Reg loss: 0.036 | Tree loss: 3.386 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 027 | Total loss: 3.344 | Reg loss: 0.036 | Tree loss: 3.344 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 027 | Total loss: 3.423 | Reg loss: 0.036 | Tree loss: 3.423 | Accuracy: 0.083333 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 38 | Batch: 000 / 027 | Total loss: 3.600 | Reg loss: 0.035 | Tree loss: 3.600 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 027 | Total loss: 3.626 | Reg loss: 0.035 | Tree loss: 3.626 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 027 | Total loss: 3.580 | Reg loss: 0.035 | Tree loss: 3.580 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 027 | Total loss: 3.598 | Reg loss: 0.035 | Tree loss: 3.598 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 027 | Total loss: 3.675 | Reg loss: 0.035 | Tree loss: 3.675 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 027 | Total loss: 3.534 | Reg loss: 0.035 | Tree loss: 3.534 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 027 | Total loss: 3.589 | Reg loss: 0.035 | Tree loss: 3.589 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 027 | Total loss: 3.591 | Reg loss: 0.035 | Tree loss: 3.591 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 027 | Total loss: 3.534 | Reg loss: 0.035 | Tree loss: 3.534 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 027 | Total loss: 3.561 | Reg loss: 0.035 | Tree loss: 3.561 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 027 | Total loss: 3.524 | Reg loss: 0.035 | Tree loss: 3.524 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 027 | Total loss: 3.492 | Reg loss: 0.035 | Tree loss: 3.492 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 027 | Total loss: 3.549 | Reg loss: 0.035 | Tree loss: 3.549 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 027 | Total loss: 3.479 | Reg loss: 0.035 | Tree loss: 3.479 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 027 | Total loss: 3.441 | Reg loss: 0.035 | Tree loss: 3.441 | Accuracy: 0.113281 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 027 | Total loss: 3.467 | Reg loss: 0.035 | Tree loss: 3.467 | Accuracy: 0.113281 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 027 | Total loss: 3.469 | Reg loss: 0.035 | Tree loss: 3.469 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 027 | Total loss: 3.429 | Reg loss: 0.035 | Tree loss: 3.429 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 027 | Total loss: 3.446 | Reg loss: 0.035 | Tree loss: 3.446 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 027 | Total loss: 3.373 | Reg loss: 0.035 | Tree loss: 3.373 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 027 | Total loss: 3.492 | Reg loss: 0.035 | Tree loss: 3.492 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 027 | Total loss: 3.431 | Reg loss: 0.035 | Tree loss: 3.431 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 027 | Total loss: 3.385 | Reg loss: 0.035 | Tree loss: 3.385 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 027 | Total loss: 3.374 | Reg loss: 0.035 | Tree loss: 3.374 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 027 | Total loss: 3.389 | Reg loss: 0.035 | Tree loss: 3.389 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 027 | Total loss: 3.407 | Reg loss: 0.036 | Tree loss: 3.407 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 027 | Total loss: 3.645 | Reg loss: 0.036 | Tree loss: 3.645 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 39 | Batch: 000 / 027 | Total loss: 3.579 | Reg loss: 0.035 | Tree loss: 3.579 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 027 | Total loss: 3.559 | Reg loss: 0.035 | Tree loss: 3.559 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 027 | Total loss: 3.580 | Reg loss: 0.035 | Tree loss: 3.580 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 027 | Total loss: 3.583 | Reg loss: 0.035 | Tree loss: 3.583 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 027 | Total loss: 3.538 | Reg loss: 0.035 | Tree loss: 3.538 | Accuracy: 0.113281 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 027 | Total loss: 3.563 | Reg loss: 0.035 | Tree loss: 3.563 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 027 | Total loss: 3.524 | Reg loss: 0.035 | Tree loss: 3.524 | Accuracy: 0.113281 | 0.352 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 007 / 027 | Total loss: 3.521 | Reg loss: 0.035 | Tree loss: 3.521 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 027 | Total loss: 3.522 | Reg loss: 0.035 | Tree loss: 3.522 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 027 | Total loss: 3.435 | Reg loss: 0.035 | Tree loss: 3.435 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 027 | Total loss: 3.461 | Reg loss: 0.035 | Tree loss: 3.461 | Accuracy: 0.125000 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 027 | Total loss: 3.471 | Reg loss: 0.035 | Tree loss: 3.471 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 027 | Total loss: 3.472 | Reg loss: 0.035 | Tree loss: 3.472 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 027 | Total loss: 3.434 | Reg loss: 0.035 | Tree loss: 3.434 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 027 | Total loss: 3.467 | Reg loss: 0.035 | Tree loss: 3.467 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 027 | Total loss: 3.420 | Reg loss: 0.035 | Tree loss: 3.420 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 027 | Total loss: 3.388 | Reg loss: 0.035 | Tree loss: 3.388 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 027 | Total loss: 3.437 | Reg loss: 0.035 | Tree loss: 3.437 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 027 | Total loss: 3.382 | Reg loss: 0.035 | Tree loss: 3.382 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 027 | Total loss: 3.367 | Reg loss: 0.035 | Tree loss: 3.367 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 027 | Total loss: 3.412 | Reg loss: 0.035 | Tree loss: 3.412 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 027 | Total loss: 3.408 | Reg loss: 0.035 | Tree loss: 3.408 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 027 | Total loss: 3.360 | Reg loss: 0.035 | Tree loss: 3.360 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 027 | Total loss: 3.346 | Reg loss: 0.035 | Tree loss: 3.346 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 027 | Total loss: 3.374 | Reg loss: 0.035 | Tree loss: 3.374 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 027 | Total loss: 3.275 | Reg loss: 0.036 | Tree loss: 3.275 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 027 | Total loss: 3.582 | Reg loss: 0.036 | Tree loss: 3.582 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 40 | Batch: 000 / 027 | Total loss: 3.586 | Reg loss: 0.035 | Tree loss: 3.586 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 027 | Total loss: 3.497 | Reg loss: 0.035 | Tree loss: 3.497 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 027 | Total loss: 3.497 | Reg loss: 0.035 | Tree loss: 3.497 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 027 | Total loss: 3.469 | Reg loss: 0.035 | Tree loss: 3.469 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 027 | Total loss: 3.533 | Reg loss: 0.035 | Tree loss: 3.533 | Accuracy: 0.076172 | 0.353 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 027 | Total loss: 3.483 | Reg loss: 0.035 | Tree loss: 3.483 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 027 | Total loss: 3.482 | Reg loss: 0.035 | Tree loss: 3.482 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 027 | Total loss: 3.463 | Reg loss: 0.035 | Tree loss: 3.463 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 027 | Total loss: 3.467 | Reg loss: 0.035 | Tree loss: 3.467 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 027 | Total loss: 3.351 | Reg loss: 0.035 | Tree loss: 3.351 | Accuracy: 0.125000 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 027 | Total loss: 3.498 | Reg loss: 0.035 | Tree loss: 3.498 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 027 | Total loss: 3.401 | Reg loss: 0.035 | Tree loss: 3.401 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 027 | Total loss: 3.453 | Reg loss: 0.035 | Tree loss: 3.453 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 027 | Total loss: 3.452 | Reg loss: 0.035 | Tree loss: 3.452 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 027 | Total loss: 3.391 | Reg loss: 0.035 | Tree loss: 3.391 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 027 | Total loss: 3.441 | Reg loss: 0.035 | Tree loss: 3.441 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 027 | Total loss: 3.362 | Reg loss: 0.035 | Tree loss: 3.362 | Accuracy: 0.130859 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 027 | Total loss: 3.366 | Reg loss: 0.035 | Tree loss: 3.366 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 027 | Total loss: 3.341 | Reg loss: 0.035 | Tree loss: 3.341 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 027 | Total loss: 3.318 | Reg loss: 0.035 | Tree loss: 3.318 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 027 | Total loss: 3.347 | Reg loss: 0.035 | Tree loss: 3.347 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 027 | Total loss: 3.322 | Reg loss: 0.035 | Tree loss: 3.322 | Accuracy: 0.117188 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 027 | Total loss: 3.385 | Reg loss: 0.035 | Tree loss: 3.385 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 027 | Total loss: 3.325 | Reg loss: 0.035 | Tree loss: 3.325 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 027 | Total loss: 3.338 | Reg loss: 0.035 | Tree loss: 3.338 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 027 | Total loss: 3.314 | Reg loss: 0.036 | Tree loss: 3.314 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 027 | Total loss: 3.345 | Reg loss: 0.036 | Tree loss: 3.345 | Accuracy: 0.166667 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 41 | Batch: 000 / 027 | Total loss: 3.543 | Reg loss: 0.035 | Tree loss: 3.543 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 027 | Total loss: 3.558 | Reg loss: 0.035 | Tree loss: 3.558 | Accuracy: 0.078125 | 0.352 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 027 | Total loss: 3.450 | Reg loss: 0.035 | Tree loss: 3.450 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 027 | Total loss: 3.463 | Reg loss: 0.035 | Tree loss: 3.463 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 027 | Total loss: 3.427 | Reg loss: 0.035 | Tree loss: 3.427 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 027 | Total loss: 3.417 | Reg loss: 0.035 | Tree loss: 3.417 | Accuracy: 0.076172 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 027 | Total loss: 3.426 | Reg loss: 0.035 | Tree loss: 3.426 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 027 | Total loss: 3.455 | Reg loss: 0.035 | Tree loss: 3.455 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 027 | Total loss: 3.433 | Reg loss: 0.035 | Tree loss: 3.433 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 027 | Total loss: 3.368 | Reg loss: 0.035 | Tree loss: 3.368 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 027 | Total loss: 3.406 | Reg loss: 0.035 | Tree loss: 3.406 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 027 | Total loss: 3.435 | Reg loss: 0.035 | Tree loss: 3.435 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 027 | Total loss: 3.370 | Reg loss: 0.035 | Tree loss: 3.370 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 027 | Total loss: 3.408 | Reg loss: 0.035 | Tree loss: 3.408 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 027 | Total loss: 3.363 | Reg loss: 0.035 | Tree loss: 3.363 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 027 | Total loss: 3.337 | Reg loss: 0.035 | Tree loss: 3.337 | Accuracy: 0.125000 | 0.351 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 016 / 027 | Total loss: 3.368 | Reg loss: 0.035 | Tree loss: 3.368 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 027 | Total loss: 3.294 | Reg loss: 0.035 | Tree loss: 3.294 | Accuracy: 0.111328 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 027 | Total loss: 3.314 | Reg loss: 0.035 | Tree loss: 3.314 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 027 | Total loss: 3.280 | Reg loss: 0.035 | Tree loss: 3.280 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 027 | Total loss: 3.290 | Reg loss: 0.035 | Tree loss: 3.290 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 027 | Total loss: 3.317 | Reg loss: 0.035 | Tree loss: 3.317 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 027 | Total loss: 3.348 | Reg loss: 0.035 | Tree loss: 3.348 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 027 | Total loss: 3.297 | Reg loss: 0.035 | Tree loss: 3.297 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 027 | Total loss: 3.321 | Reg loss: 0.035 | Tree loss: 3.321 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 027 | Total loss: 3.258 | Reg loss: 0.035 | Tree loss: 3.258 | Accuracy: 0.103516 | 0.35 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 027 | Total loss: 2.986 | Reg loss: 0.036 | Tree loss: 2.986 | Accuracy: 0.250000 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 42 | Batch: 000 / 027 | Total loss: 3.473 | Reg loss: 0.035 | Tree loss: 3.473 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 027 | Total loss: 3.470 | Reg loss: 0.035 | Tree loss: 3.470 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 027 | Total loss: 3.413 | Reg loss: 0.035 | Tree loss: 3.413 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 027 | Total loss: 3.420 | Reg loss: 0.035 | Tree loss: 3.420 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 027 | Total loss: 3.436 | Reg loss: 0.035 | Tree loss: 3.436 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 027 | Total loss: 3.391 | Reg loss: 0.035 | Tree loss: 3.391 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 027 | Total loss: 3.321 | Reg loss: 0.035 | Tree loss: 3.321 | Accuracy: 0.125000 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 027 | Total loss: 3.426 | Reg loss: 0.035 | Tree loss: 3.426 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 027 | Total loss: 3.308 | Reg loss: 0.035 | Tree loss: 3.308 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 027 | Total loss: 3.404 | Reg loss: 0.035 | Tree loss: 3.404 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 027 | Total loss: 3.346 | Reg loss: 0.035 | Tree loss: 3.346 | Accuracy: 0.072266 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 027 | Total loss: 3.410 | Reg loss: 0.035 | Tree loss: 3.410 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 027 | Total loss: 3.373 | Reg loss: 0.035 | Tree loss: 3.373 | Accuracy: 0.103516 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 027 | Total loss: 3.347 | Reg loss: 0.035 | Tree loss: 3.347 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 027 | Total loss: 3.386 | Reg loss: 0.035 | Tree loss: 3.386 | Accuracy: 0.074219 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 027 | Total loss: 3.271 | Reg loss: 0.035 | Tree loss: 3.271 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 027 | Total loss: 3.323 | Reg loss: 0.035 | Tree loss: 3.323 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 027 | Total loss: 3.340 | Reg loss: 0.035 | Tree loss: 3.340 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 027 | Total loss: 3.262 | Reg loss: 0.035 | Tree loss: 3.262 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 027 | Total loss: 3.256 | Reg loss: 0.035 | Tree loss: 3.256 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 027 | Total loss: 3.332 | Reg loss: 0.035 | Tree loss: 3.332 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 027 | Total loss: 3.305 | Reg loss: 0.035 | Tree loss: 3.305 | Accuracy: 0.109375 | 0.349 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 027 | Total loss: 3.327 | Reg loss: 0.035 | Tree loss: 3.327 | Accuracy: 0.082031 | 0.349 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 027 | Total loss: 3.229 | Reg loss: 0.035 | Tree loss: 3.229 | Accuracy: 0.097656 | 0.349 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 027 | Total loss: 3.265 | Reg loss: 0.035 | Tree loss: 3.265 | Accuracy: 0.109375 | 0.349 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 027 | Total loss: 3.272 | Reg loss: 0.035 | Tree loss: 3.272 | Accuracy: 0.082031 | 0.349 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 027 | Total loss: 3.301 | Reg loss: 0.036 | Tree loss: 3.301 | Accuracy: 0.083333 | 0.349 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 43 | Batch: 000 / 027 | Total loss: 3.434 | Reg loss: 0.035 | Tree loss: 3.434 | Accuracy: 0.101562 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 027 | Total loss: 3.447 | Reg loss: 0.035 | Tree loss: 3.447 | Accuracy: 0.082031 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 027 | Total loss: 3.425 | Reg loss: 0.035 | Tree loss: 3.425 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 027 | Total loss: 3.469 | Reg loss: 0.035 | Tree loss: 3.469 | Accuracy: 0.089844 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 027 | Total loss: 3.383 | Reg loss: 0.035 | Tree loss: 3.383 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 027 | Total loss: 3.417 | Reg loss: 0.035 | Tree loss: 3.417 | Accuracy: 0.115234 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 027 | Total loss: 3.343 | Reg loss: 0.035 | Tree loss: 3.343 | Accuracy: 0.119141 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 027 | Total loss: 3.371 | Reg loss: 0.035 | Tree loss: 3.371 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 027 | Total loss: 3.301 | Reg loss: 0.035 | Tree loss: 3.301 | Accuracy: 0.105469 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 027 | Total loss: 3.363 | Reg loss: 0.035 | Tree loss: 3.363 | Accuracy: 0.070312 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 027 | Total loss: 3.330 | Reg loss: 0.035 | Tree loss: 3.330 | Accuracy: 0.103516 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 027 | Total loss: 3.323 | Reg loss: 0.035 | Tree loss: 3.323 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 027 | Total loss: 3.292 | Reg loss: 0.035 | Tree loss: 3.292 | Accuracy: 0.109375 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 027 | Total loss: 3.280 | Reg loss: 0.035 | Tree loss: 3.280 | Accuracy: 0.132812 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 027 | Total loss: 3.286 | Reg loss: 0.035 | Tree loss: 3.286 | Accuracy: 0.115234 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 027 | Total loss: 3.266 | Reg loss: 0.035 | Tree loss: 3.266 | Accuracy: 0.101562 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 027 | Total loss: 3.233 | Reg loss: 0.035 | Tree loss: 3.233 | Accuracy: 0.126953 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 027 | Total loss: 3.285 | Reg loss: 0.035 | Tree loss: 3.285 | Accuracy: 0.089844 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 027 | Total loss: 3.296 | Reg loss: 0.035 | Tree loss: 3.296 | Accuracy: 0.085938 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 027 | Total loss: 3.313 | Reg loss: 0.035 | Tree loss: 3.313 | Accuracy: 0.091797 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 027 | Total loss: 3.302 | Reg loss: 0.035 | Tree loss: 3.302 | Accuracy: 0.089844 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 027 | Total loss: 3.254 | Reg loss: 0.035 | Tree loss: 3.254 | Accuracy: 0.115234 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 027 | Total loss: 3.245 | Reg loss: 0.035 | Tree loss: 3.245 | Accuracy: 0.074219 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 027 | Total loss: 3.203 | Reg loss: 0.035 | Tree loss: 3.203 | Accuracy: 0.091797 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 027 | Total loss: 3.286 | Reg loss: 0.035 | Tree loss: 3.286 | Accuracy: 0.083984 | 0.349 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 025 / 027 | Total loss: 3.200 | Reg loss: 0.035 | Tree loss: 3.200 | Accuracy: 0.103516 | 0.349 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 027 | Total loss: 3.005 | Reg loss: 0.035 | Tree loss: 3.005 | Accuracy: 0.000000 | 0.349 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 44 | Batch: 000 / 027 | Total loss: 3.424 | Reg loss: 0.035 | Tree loss: 3.424 | Accuracy: 0.083984 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 027 | Total loss: 3.379 | Reg loss: 0.035 | Tree loss: 3.379 | Accuracy: 0.119141 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 027 | Total loss: 3.406 | Reg loss: 0.035 | Tree loss: 3.406 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 027 | Total loss: 3.368 | Reg loss: 0.035 | Tree loss: 3.368 | Accuracy: 0.085938 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 027 | Total loss: 3.317 | Reg loss: 0.035 | Tree loss: 3.317 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 027 | Total loss: 3.356 | Reg loss: 0.035 | Tree loss: 3.356 | Accuracy: 0.097656 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 027 | Total loss: 3.381 | Reg loss: 0.035 | Tree loss: 3.381 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 027 | Total loss: 3.341 | Reg loss: 0.035 | Tree loss: 3.341 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 027 | Total loss: 3.290 | Reg loss: 0.035 | Tree loss: 3.290 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 027 | Total loss: 3.330 | Reg loss: 0.035 | Tree loss: 3.330 | Accuracy: 0.103516 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 027 | Total loss: 3.316 | Reg loss: 0.035 | Tree loss: 3.316 | Accuracy: 0.111328 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 027 | Total loss: 3.353 | Reg loss: 0.035 | Tree loss: 3.353 | Accuracy: 0.091797 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 027 | Total loss: 3.267 | Reg loss: 0.035 | Tree loss: 3.267 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 027 | Total loss: 3.241 | Reg loss: 0.035 | Tree loss: 3.241 | Accuracy: 0.105469 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 027 | Total loss: 3.309 | Reg loss: 0.035 | Tree loss: 3.309 | Accuracy: 0.097656 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 027 | Total loss: 3.228 | Reg loss: 0.035 | Tree loss: 3.228 | Accuracy: 0.123047 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 027 | Total loss: 3.205 | Reg loss: 0.035 | Tree loss: 3.205 | Accuracy: 0.111328 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 027 | Total loss: 3.277 | Reg loss: 0.035 | Tree loss: 3.277 | Accuracy: 0.134766 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 027 | Total loss: 3.272 | Reg loss: 0.035 | Tree loss: 3.272 | Accuracy: 0.103516 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 027 | Total loss: 3.207 | Reg loss: 0.035 | Tree loss: 3.207 | Accuracy: 0.125000 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 027 | Total loss: 3.231 | Reg loss: 0.035 | Tree loss: 3.231 | Accuracy: 0.097656 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 027 | Total loss: 3.285 | Reg loss: 0.035 | Tree loss: 3.285 | Accuracy: 0.076172 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 027 | Total loss: 3.257 | Reg loss: 0.035 | Tree loss: 3.257 | Accuracy: 0.078125 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 027 | Total loss: 3.182 | Reg loss: 0.035 | Tree loss: 3.182 | Accuracy: 0.103516 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 027 | Total loss: 3.174 | Reg loss: 0.035 | Tree loss: 3.174 | Accuracy: 0.082031 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 027 | Total loss: 3.218 | Reg loss: 0.035 | Tree loss: 3.218 | Accuracy: 0.072266 | 0.349 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 027 | Total loss: 3.585 | Reg loss: 0.035 | Tree loss: 3.585 | Accuracy: 0.083333 | 0.349 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 45 | Batch: 000 / 027 | Total loss: 3.423 | Reg loss: 0.035 | Tree loss: 3.423 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 027 | Total loss: 3.361 | Reg loss: 0.035 | Tree loss: 3.361 | Accuracy: 0.091797 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 027 | Total loss: 3.407 | Reg loss: 0.035 | Tree loss: 3.407 | Accuracy: 0.105469 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 027 | Total loss: 3.308 | Reg loss: 0.035 | Tree loss: 3.308 | Accuracy: 0.113281 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 027 | Total loss: 3.331 | Reg loss: 0.035 | Tree loss: 3.331 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 027 | Total loss: 3.384 | Reg loss: 0.035 | Tree loss: 3.384 | Accuracy: 0.080078 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 027 | Total loss: 3.337 | Reg loss: 0.035 | Tree loss: 3.337 | Accuracy: 0.089844 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 027 | Total loss: 3.331 | Reg loss: 0.035 | Tree loss: 3.331 | Accuracy: 0.105469 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 027 | Total loss: 3.270 | Reg loss: 0.035 | Tree loss: 3.270 | Accuracy: 0.089844 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 027 | Total loss: 3.334 | Reg loss: 0.035 | Tree loss: 3.334 | Accuracy: 0.113281 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 027 | Total loss: 3.289 | Reg loss: 0.035 | Tree loss: 3.289 | Accuracy: 0.072266 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 027 | Total loss: 3.295 | Reg loss: 0.035 | Tree loss: 3.295 | Accuracy: 0.089844 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 027 | Total loss: 3.280 | Reg loss: 0.035 | Tree loss: 3.280 | Accuracy: 0.117188 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 027 | Total loss: 3.241 | Reg loss: 0.035 | Tree loss: 3.241 | Accuracy: 0.113281 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 027 | Total loss: 3.253 | Reg loss: 0.035 | Tree loss: 3.253 | Accuracy: 0.087891 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 027 | Total loss: 3.220 | Reg loss: 0.035 | Tree loss: 3.220 | Accuracy: 0.097656 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 027 | Total loss: 3.229 | Reg loss: 0.035 | Tree loss: 3.229 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 027 | Total loss: 3.202 | Reg loss: 0.035 | Tree loss: 3.202 | Accuracy: 0.103516 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 027 | Total loss: 3.234 | Reg loss: 0.035 | Tree loss: 3.234 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 027 | Total loss: 3.175 | Reg loss: 0.035 | Tree loss: 3.175 | Accuracy: 0.109375 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 027 | Total loss: 3.174 | Reg loss: 0.035 | Tree loss: 3.174 | Accuracy: 0.128906 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 027 | Total loss: 3.196 | Reg loss: 0.035 | Tree loss: 3.196 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 027 | Total loss: 3.158 | Reg loss: 0.035 | Tree loss: 3.158 | Accuracy: 0.111328 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 027 | Total loss: 3.174 | Reg loss: 0.035 | Tree loss: 3.174 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 027 | Total loss: 3.166 | Reg loss: 0.035 | Tree loss: 3.166 | Accuracy: 0.083984 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 027 | Total loss: 3.203 | Reg loss: 0.035 | Tree loss: 3.203 | Accuracy: 0.087891 | 0.349 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 027 | Total loss: 3.239 | Reg loss: 0.035 | Tree loss: 3.239 | Accuracy: 0.083333 | 0.349 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 46 | Batch: 000 / 027 | Total loss: 3.364 | Reg loss: 0.035 | Tree loss: 3.364 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 027 | Total loss: 3.341 | Reg loss: 0.035 | Tree loss: 3.341 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 027 | Total loss: 3.367 | Reg loss: 0.035 | Tree loss: 3.367 | Accuracy: 0.091797 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 027 | Total loss: 3.408 | Reg loss: 0.035 | Tree loss: 3.408 | Accuracy: 0.068359 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 027 | Total loss: 3.294 | Reg loss: 0.035 | Tree loss: 3.294 | Accuracy: 0.074219 | 0.349 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 005 / 027 | Total loss: 3.324 | Reg loss: 0.035 | Tree loss: 3.324 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 027 | Total loss: 3.330 | Reg loss: 0.035 | Tree loss: 3.330 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 027 | Total loss: 3.264 | Reg loss: 0.035 | Tree loss: 3.264 | Accuracy: 0.093750 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 027 | Total loss: 3.272 | Reg loss: 0.035 | Tree loss: 3.272 | Accuracy: 0.083984 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 027 | Total loss: 3.275 | Reg loss: 0.035 | Tree loss: 3.275 | Accuracy: 0.109375 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 027 | Total loss: 3.293 | Reg loss: 0.035 | Tree loss: 3.293 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 027 | Total loss: 3.254 | Reg loss: 0.035 | Tree loss: 3.254 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 027 | Total loss: 3.237 | Reg loss: 0.035 | Tree loss: 3.237 | Accuracy: 0.095703 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 027 | Total loss: 3.237 | Reg loss: 0.035 | Tree loss: 3.237 | Accuracy: 0.115234 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 027 | Total loss: 3.216 | Reg loss: 0.035 | Tree loss: 3.216 | Accuracy: 0.101562 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 027 | Total loss: 3.211 | Reg loss: 0.035 | Tree loss: 3.211 | Accuracy: 0.107422 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 027 | Total loss: 3.197 | Reg loss: 0.035 | Tree loss: 3.197 | Accuracy: 0.121094 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 027 | Total loss: 3.257 | Reg loss: 0.035 | Tree loss: 3.257 | Accuracy: 0.099609 | 0.349 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 027 | Total loss: 3.184 | Reg loss: 0.035 | Tree loss: 3.184 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 027 | Total loss: 3.161 | Reg loss: 0.035 | Tree loss: 3.161 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 027 | Total loss: 3.175 | Reg loss: 0.035 | Tree loss: 3.175 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 027 | Total loss: 3.190 | Reg loss: 0.035 | Tree loss: 3.190 | Accuracy: 0.117188 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 027 | Total loss: 3.103 | Reg loss: 0.035 | Tree loss: 3.103 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 027 | Total loss: 3.148 | Reg loss: 0.035 | Tree loss: 3.148 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 027 | Total loss: 3.183 | Reg loss: 0.035 | Tree loss: 3.183 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 027 | Total loss: 3.143 | Reg loss: 0.035 | Tree loss: 3.143 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 027 | Total loss: 2.969 | Reg loss: 0.035 | Tree loss: 2.969 | Accuracy: 0.250000 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 47 | Batch: 000 / 027 | Total loss: 3.369 | Reg loss: 0.035 | Tree loss: 3.369 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 027 | Total loss: 3.335 | Reg loss: 0.035 | Tree loss: 3.335 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 027 | Total loss: 3.355 | Reg loss: 0.035 | Tree loss: 3.355 | Accuracy: 0.076172 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 027 | Total loss: 3.269 | Reg loss: 0.035 | Tree loss: 3.269 | Accuracy: 0.123047 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 027 | Total loss: 3.323 | Reg loss: 0.035 | Tree loss: 3.323 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 027 | Total loss: 3.263 | Reg loss: 0.035 | Tree loss: 3.263 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 027 | Total loss: 3.291 | Reg loss: 0.035 | Tree loss: 3.291 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 027 | Total loss: 3.269 | Reg loss: 0.035 | Tree loss: 3.269 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 027 | Total loss: 3.264 | Reg loss: 0.035 | Tree loss: 3.264 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 027 | Total loss: 3.239 | Reg loss: 0.035 | Tree loss: 3.239 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 027 | Total loss: 3.278 | Reg loss: 0.035 | Tree loss: 3.278 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 027 | Total loss: 3.234 | Reg loss: 0.035 | Tree loss: 3.234 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 027 | Total loss: 3.239 | Reg loss: 0.035 | Tree loss: 3.239 | Accuracy: 0.082031 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 027 | Total loss: 3.173 | Reg loss: 0.035 | Tree loss: 3.173 | Accuracy: 0.121094 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 027 | Total loss: 3.192 | Reg loss: 0.035 | Tree loss: 3.192 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 027 | Total loss: 3.234 | Reg loss: 0.035 | Tree loss: 3.234 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 027 | Total loss: 3.187 | Reg loss: 0.035 | Tree loss: 3.187 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 027 | Total loss: 3.177 | Reg loss: 0.035 | Tree loss: 3.177 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 027 | Total loss: 3.151 | Reg loss: 0.035 | Tree loss: 3.151 | Accuracy: 0.121094 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 027 | Total loss: 3.153 | Reg loss: 0.035 | Tree loss: 3.153 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 027 | Total loss: 3.171 | Reg loss: 0.035 | Tree loss: 3.171 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 027 | Total loss: 3.146 | Reg loss: 0.035 | Tree loss: 3.146 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 027 | Total loss: 3.167 | Reg loss: 0.035 | Tree loss: 3.167 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 027 | Total loss: 3.136 | Reg loss: 0.035 | Tree loss: 3.136 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 027 | Total loss: 3.163 | Reg loss: 0.035 | Tree loss: 3.163 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 027 | Total loss: 3.106 | Reg loss: 0.035 | Tree loss: 3.106 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 027 | Total loss: 3.162 | Reg loss: 0.035 | Tree loss: 3.162 | Accuracy: 0.000000 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 48 | Batch: 000 / 027 | Total loss: 3.335 | Reg loss: 0.035 | Tree loss: 3.335 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 027 | Total loss: 3.301 | Reg loss: 0.035 | Tree loss: 3.301 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 027 | Total loss: 3.278 | Reg loss: 0.035 | Tree loss: 3.278 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 027 | Total loss: 3.303 | Reg loss: 0.035 | Tree loss: 3.303 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 027 | Total loss: 3.312 | Reg loss: 0.035 | Tree loss: 3.312 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 027 | Total loss: 3.272 | Reg loss: 0.035 | Tree loss: 3.272 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 027 | Total loss: 3.301 | Reg loss: 0.035 | Tree loss: 3.301 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 027 | Total loss: 3.313 | Reg loss: 0.035 | Tree loss: 3.313 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 027 | Total loss: 3.258 | Reg loss: 0.035 | Tree loss: 3.258 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 027 | Total loss: 3.251 | Reg loss: 0.035 | Tree loss: 3.251 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 027 | Total loss: 3.223 | Reg loss: 0.035 | Tree loss: 3.223 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 027 | Total loss: 3.183 | Reg loss: 0.035 | Tree loss: 3.183 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 027 | Total loss: 3.233 | Reg loss: 0.035 | Tree loss: 3.233 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 027 | Total loss: 3.165 | Reg loss: 0.035 | Tree loss: 3.165 | Accuracy: 0.097656 | 0.35 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 014 / 027 | Total loss: 3.276 | Reg loss: 0.035 | Tree loss: 3.276 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 027 | Total loss: 3.148 | Reg loss: 0.035 | Tree loss: 3.148 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 027 | Total loss: 3.158 | Reg loss: 0.035 | Tree loss: 3.158 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 027 | Total loss: 3.155 | Reg loss: 0.035 | Tree loss: 3.155 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 027 | Total loss: 3.157 | Reg loss: 0.035 | Tree loss: 3.157 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 027 | Total loss: 3.105 | Reg loss: 0.035 | Tree loss: 3.105 | Accuracy: 0.123047 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 027 | Total loss: 3.066 | Reg loss: 0.035 | Tree loss: 3.066 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 027 | Total loss: 3.128 | Reg loss: 0.035 | Tree loss: 3.128 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 027 | Total loss: 3.125 | Reg loss: 0.035 | Tree loss: 3.125 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 027 | Total loss: 3.131 | Reg loss: 0.035 | Tree loss: 3.131 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 027 | Total loss: 3.135 | Reg loss: 0.035 | Tree loss: 3.135 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 027 | Total loss: 3.106 | Reg loss: 0.035 | Tree loss: 3.106 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 027 | Total loss: 2.758 | Reg loss: 0.035 | Tree loss: 2.758 | Accuracy: 0.333333 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 49 | Batch: 000 / 027 | Total loss: 3.283 | Reg loss: 0.035 | Tree loss: 3.283 | Accuracy: 0.085938 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 027 | Total loss: 3.298 | Reg loss: 0.035 | Tree loss: 3.298 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 027 | Total loss: 3.259 | Reg loss: 0.035 | Tree loss: 3.259 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 027 | Total loss: 3.299 | Reg loss: 0.035 | Tree loss: 3.299 | Accuracy: 0.080078 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 027 | Total loss: 3.346 | Reg loss: 0.035 | Tree loss: 3.346 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 027 | Total loss: 3.234 | Reg loss: 0.035 | Tree loss: 3.234 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 027 | Total loss: 3.226 | Reg loss: 0.035 | Tree loss: 3.226 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 027 | Total loss: 3.193 | Reg loss: 0.035 | Tree loss: 3.193 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 027 | Total loss: 3.230 | Reg loss: 0.035 | Tree loss: 3.230 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 027 | Total loss: 3.218 | Reg loss: 0.035 | Tree loss: 3.218 | Accuracy: 0.093750 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 027 | Total loss: 3.184 | Reg loss: 0.035 | Tree loss: 3.184 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 027 | Total loss: 3.186 | Reg loss: 0.035 | Tree loss: 3.186 | Accuracy: 0.117188 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 027 | Total loss: 3.162 | Reg loss: 0.035 | Tree loss: 3.162 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 027 | Total loss: 3.226 | Reg loss: 0.035 | Tree loss: 3.226 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 027 | Total loss: 3.184 | Reg loss: 0.035 | Tree loss: 3.184 | Accuracy: 0.103516 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 027 | Total loss: 3.191 | Reg loss: 0.035 | Tree loss: 3.191 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 027 | Total loss: 3.157 | Reg loss: 0.035 | Tree loss: 3.157 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 027 | Total loss: 3.155 | Reg loss: 0.035 | Tree loss: 3.155 | Accuracy: 0.074219 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 027 | Total loss: 3.122 | Reg loss: 0.035 | Tree loss: 3.122 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 027 | Total loss: 3.117 | Reg loss: 0.035 | Tree loss: 3.117 | Accuracy: 0.080078 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 027 | Total loss: 3.118 | Reg loss: 0.035 | Tree loss: 3.118 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 027 | Total loss: 3.172 | Reg loss: 0.035 | Tree loss: 3.172 | Accuracy: 0.082031 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 027 | Total loss: 3.122 | Reg loss: 0.035 | Tree loss: 3.122 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 027 | Total loss: 3.091 | Reg loss: 0.035 | Tree loss: 3.091 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 027 | Total loss: 3.101 | Reg loss: 0.035 | Tree loss: 3.101 | Accuracy: 0.119141 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 027 | Total loss: 3.104 | Reg loss: 0.035 | Tree loss: 3.104 | Accuracy: 0.080078 | 0.35 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 027 | Total loss: 2.975 | Reg loss: 0.035 | Tree loss: 2.975 | Accuracy: 0.083333 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 50 | Batch: 000 / 027 | Total loss: 3.240 | Reg loss: 0.034 | Tree loss: 3.240 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 027 | Total loss: 3.260 | Reg loss: 0.034 | Tree loss: 3.260 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 027 | Total loss: 3.275 | Reg loss: 0.034 | Tree loss: 3.275 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 027 | Total loss: 3.234 | Reg loss: 0.034 | Tree loss: 3.234 | Accuracy: 0.117188 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 027 | Total loss: 3.217 | Reg loss: 0.034 | Tree loss: 3.217 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 027 | Total loss: 3.210 | Reg loss: 0.034 | Tree loss: 3.210 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 027 | Total loss: 3.230 | Reg loss: 0.034 | Tree loss: 3.230 | Accuracy: 0.080078 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 027 | Total loss: 3.205 | Reg loss: 0.034 | Tree loss: 3.205 | Accuracy: 0.076172 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 027 | Total loss: 3.224 | Reg loss: 0.035 | Tree loss: 3.224 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 027 | Total loss: 3.194 | Reg loss: 0.035 | Tree loss: 3.194 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 027 | Total loss: 3.193 | Reg loss: 0.035 | Tree loss: 3.193 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 027 | Total loss: 3.199 | Reg loss: 0.035 | Tree loss: 3.199 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 027 | Total loss: 3.169 | Reg loss: 0.035 | Tree loss: 3.169 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 027 | Total loss: 3.266 | Reg loss: 0.035 | Tree loss: 3.266 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 027 | Total loss: 3.136 | Reg loss: 0.035 | Tree loss: 3.136 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 027 | Total loss: 3.189 | Reg loss: 0.035 | Tree loss: 3.189 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 027 | Total loss: 3.098 | Reg loss: 0.035 | Tree loss: 3.098 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 027 | Total loss: 3.113 | Reg loss: 0.035 | Tree loss: 3.113 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 027 | Total loss: 3.141 | Reg loss: 0.035 | Tree loss: 3.141 | Accuracy: 0.074219 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 027 | Total loss: 3.107 | Reg loss: 0.035 | Tree loss: 3.107 | Accuracy: 0.130859 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 027 | Total loss: 3.178 | Reg loss: 0.035 | Tree loss: 3.178 | Accuracy: 0.072266 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 027 | Total loss: 3.109 | Reg loss: 0.035 | Tree loss: 3.109 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 027 | Total loss: 3.112 | Reg loss: 0.035 | Tree loss: 3.112 | Accuracy: 0.097656 | 0.35 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 023 / 027 | Total loss: 3.069 | Reg loss: 0.035 | Tree loss: 3.069 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 027 | Total loss: 3.100 | Reg loss: 0.035 | Tree loss: 3.100 | Accuracy: 0.080078 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 027 | Total loss: 3.065 | Reg loss: 0.035 | Tree loss: 3.065 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 027 | Total loss: 3.202 | Reg loss: 0.035 | Tree loss: 3.202 | Accuracy: 0.083333 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 51 | Batch: 000 / 027 | Total loss: 3.227 | Reg loss: 0.034 | Tree loss: 3.227 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 027 | Total loss: 3.305 | Reg loss: 0.034 | Tree loss: 3.305 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 027 | Total loss: 3.226 | Reg loss: 0.034 | Tree loss: 3.226 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 027 | Total loss: 3.201 | Reg loss: 0.034 | Tree loss: 3.201 | Accuracy: 0.117188 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 027 | Total loss: 3.250 | Reg loss: 0.034 | Tree loss: 3.250 | Accuracy: 0.109375 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 027 | Total loss: 3.271 | Reg loss: 0.034 | Tree loss: 3.271 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 027 | Total loss: 3.196 | Reg loss: 0.034 | Tree loss: 3.196 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 027 | Total loss: 3.215 | Reg loss: 0.034 | Tree loss: 3.215 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 027 | Total loss: 3.213 | Reg loss: 0.034 | Tree loss: 3.213 | Accuracy: 0.082031 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 027 | Total loss: 3.155 | Reg loss: 0.035 | Tree loss: 3.155 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 027 | Total loss: 3.156 | Reg loss: 0.035 | Tree loss: 3.156 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 027 | Total loss: 3.181 | Reg loss: 0.035 | Tree loss: 3.181 | Accuracy: 0.101562 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 027 | Total loss: 3.176 | Reg loss: 0.035 | Tree loss: 3.176 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 027 | Total loss: 3.141 | Reg loss: 0.035 | Tree loss: 3.141 | Accuracy: 0.107422 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 027 | Total loss: 3.129 | Reg loss: 0.035 | Tree loss: 3.129 | Accuracy: 0.113281 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 027 | Total loss: 3.116 | Reg loss: 0.035 | Tree loss: 3.116 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 027 | Total loss: 3.119 | Reg loss: 0.035 | Tree loss: 3.119 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 027 | Total loss: 3.086 | Reg loss: 0.035 | Tree loss: 3.086 | Accuracy: 0.125000 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 027 | Total loss: 3.163 | Reg loss: 0.035 | Tree loss: 3.163 | Accuracy: 0.062500 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 027 | Total loss: 3.123 | Reg loss: 0.035 | Tree loss: 3.123 | Accuracy: 0.136719 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 027 | Total loss: 3.130 | Reg loss: 0.035 | Tree loss: 3.130 | Accuracy: 0.095703 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 027 | Total loss: 3.117 | Reg loss: 0.035 | Tree loss: 3.117 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 027 | Total loss: 3.093 | Reg loss: 0.035 | Tree loss: 3.093 | Accuracy: 0.082031 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 027 | Total loss: 3.054 | Reg loss: 0.035 | Tree loss: 3.054 | Accuracy: 0.076172 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 027 | Total loss: 3.090 | Reg loss: 0.035 | Tree loss: 3.090 | Accuracy: 0.076172 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 027 | Total loss: 3.068 | Reg loss: 0.035 | Tree loss: 3.068 | Accuracy: 0.111328 | 0.35 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 027 | Total loss: 3.085 | Reg loss: 0.035 | Tree loss: 3.085 | Accuracy: 0.333333 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 52 | Batch: 000 / 027 | Total loss: 3.206 | Reg loss: 0.034 | Tree loss: 3.206 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 027 | Total loss: 3.250 | Reg loss: 0.034 | Tree loss: 3.250 | Accuracy: 0.117188 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 027 | Total loss: 3.292 | Reg loss: 0.034 | Tree loss: 3.292 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 027 | Total loss: 3.239 | Reg loss: 0.034 | Tree loss: 3.239 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 027 | Total loss: 3.240 | Reg loss: 0.034 | Tree loss: 3.240 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 027 | Total loss: 3.181 | Reg loss: 0.034 | Tree loss: 3.181 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 027 | Total loss: 3.201 | Reg loss: 0.034 | Tree loss: 3.201 | Accuracy: 0.072266 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 027 | Total loss: 3.194 | Reg loss: 0.034 | Tree loss: 3.194 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 027 | Total loss: 3.203 | Reg loss: 0.034 | Tree loss: 3.203 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 027 | Total loss: 3.171 | Reg loss: 0.034 | Tree loss: 3.171 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 027 | Total loss: 3.156 | Reg loss: 0.035 | Tree loss: 3.156 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 027 | Total loss: 3.187 | Reg loss: 0.035 | Tree loss: 3.187 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 027 | Total loss: 3.170 | Reg loss: 0.035 | Tree loss: 3.170 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 027 | Total loss: 3.150 | Reg loss: 0.035 | Tree loss: 3.150 | Accuracy: 0.078125 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 027 | Total loss: 3.126 | Reg loss: 0.035 | Tree loss: 3.126 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 027 | Total loss: 3.107 | Reg loss: 0.035 | Tree loss: 3.107 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 027 | Total loss: 3.131 | Reg loss: 0.035 | Tree loss: 3.131 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 027 | Total loss: 3.094 | Reg loss: 0.035 | Tree loss: 3.094 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 027 | Total loss: 3.101 | Reg loss: 0.035 | Tree loss: 3.101 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 027 | Total loss: 3.097 | Reg loss: 0.035 | Tree loss: 3.097 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 027 | Total loss: 3.081 | Reg loss: 0.035 | Tree loss: 3.081 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 027 | Total loss: 3.038 | Reg loss: 0.035 | Tree loss: 3.038 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 027 | Total loss: 3.048 | Reg loss: 0.035 | Tree loss: 3.048 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 027 | Total loss: 3.076 | Reg loss: 0.035 | Tree loss: 3.076 | Accuracy: 0.113281 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 027 | Total loss: 3.069 | Reg loss: 0.035 | Tree loss: 3.069 | Accuracy: 0.121094 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 027 | Total loss: 3.014 | Reg loss: 0.035 | Tree loss: 3.014 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 027 | Total loss: 3.028 | Reg loss: 0.035 | Tree loss: 3.028 | Accuracy: 0.083333 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 53 | Batch: 000 / 027 | Total loss: 3.222 | Reg loss: 0.034 | Tree loss: 3.222 | Accuracy: 0.121094 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 027 | Total loss: 3.213 | Reg loss: 0.034 | Tree loss: 3.213 | Accuracy: 0.080078 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 027 | Total loss: 3.187 | Reg loss: 0.034 | Tree loss: 3.187 | Accuracy: 0.101562 | 0.351 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 003 / 027 | Total loss: 3.217 | Reg loss: 0.034 | Tree loss: 3.217 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 027 | Total loss: 3.206 | Reg loss: 0.034 | Tree loss: 3.206 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 027 | Total loss: 3.187 | Reg loss: 0.034 | Tree loss: 3.187 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 027 | Total loss: 3.155 | Reg loss: 0.034 | Tree loss: 3.155 | Accuracy: 0.125000 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 027 | Total loss: 3.194 | Reg loss: 0.034 | Tree loss: 3.194 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 027 | Total loss: 3.228 | Reg loss: 0.034 | Tree loss: 3.228 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 027 | Total loss: 3.157 | Reg loss: 0.034 | Tree loss: 3.157 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 027 | Total loss: 3.142 | Reg loss: 0.034 | Tree loss: 3.142 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 027 | Total loss: 3.128 | Reg loss: 0.034 | Tree loss: 3.128 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 027 | Total loss: 3.126 | Reg loss: 0.035 | Tree loss: 3.126 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 027 | Total loss: 3.133 | Reg loss: 0.035 | Tree loss: 3.133 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 027 | Total loss: 3.114 | Reg loss: 0.035 | Tree loss: 3.114 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 027 | Total loss: 3.098 | Reg loss: 0.035 | Tree loss: 3.098 | Accuracy: 0.111328 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 027 | Total loss: 3.114 | Reg loss: 0.035 | Tree loss: 3.114 | Accuracy: 0.121094 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 027 | Total loss: 3.087 | Reg loss: 0.035 | Tree loss: 3.087 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 027 | Total loss: 3.139 | Reg loss: 0.035 | Tree loss: 3.139 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 027 | Total loss: 3.127 | Reg loss: 0.035 | Tree loss: 3.127 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 027 | Total loss: 3.061 | Reg loss: 0.035 | Tree loss: 3.061 | Accuracy: 0.097656 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 027 | Total loss: 3.055 | Reg loss: 0.035 | Tree loss: 3.055 | Accuracy: 0.105469 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 027 | Total loss: 3.075 | Reg loss: 0.035 | Tree loss: 3.075 | Accuracy: 0.099609 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 027 | Total loss: 3.097 | Reg loss: 0.035 | Tree loss: 3.097 | Accuracy: 0.083984 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 027 | Total loss: 3.030 | Reg loss: 0.035 | Tree loss: 3.030 | Accuracy: 0.091797 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 027 | Total loss: 3.041 | Reg loss: 0.035 | Tree loss: 3.041 | Accuracy: 0.089844 | 0.35 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 027 | Total loss: 2.836 | Reg loss: 0.035 | Tree loss: 2.836 | Accuracy: 0.250000 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 54 | Batch: 000 / 027 | Total loss: 3.265 | Reg loss: 0.034 | Tree loss: 3.265 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 027 | Total loss: 3.206 | Reg loss: 0.034 | Tree loss: 3.206 | Accuracy: 0.132812 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 027 | Total loss: 3.194 | Reg loss: 0.034 | Tree loss: 3.194 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 027 | Total loss: 3.245 | Reg loss: 0.034 | Tree loss: 3.245 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 027 | Total loss: 3.247 | Reg loss: 0.034 | Tree loss: 3.247 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 027 | Total loss: 3.180 | Reg loss: 0.034 | Tree loss: 3.180 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 027 | Total loss: 3.149 | Reg loss: 0.034 | Tree loss: 3.149 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 027 | Total loss: 3.149 | Reg loss: 0.034 | Tree loss: 3.149 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 027 | Total loss: 3.142 | Reg loss: 0.034 | Tree loss: 3.142 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 027 | Total loss: 3.168 | Reg loss: 0.034 | Tree loss: 3.168 | Accuracy: 0.076172 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 027 | Total loss: 3.122 | Reg loss: 0.034 | Tree loss: 3.122 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 027 | Total loss: 3.159 | Reg loss: 0.034 | Tree loss: 3.159 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 027 | Total loss: 3.070 | Reg loss: 0.034 | Tree loss: 3.070 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 027 | Total loss: 3.123 | Reg loss: 0.034 | Tree loss: 3.123 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 027 | Total loss: 3.060 | Reg loss: 0.035 | Tree loss: 3.060 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 027 | Total loss: 3.152 | Reg loss: 0.035 | Tree loss: 3.152 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 027 | Total loss: 3.083 | Reg loss: 0.035 | Tree loss: 3.083 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 027 | Total loss: 3.104 | Reg loss: 0.035 | Tree loss: 3.104 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 027 | Total loss: 3.098 | Reg loss: 0.035 | Tree loss: 3.098 | Accuracy: 0.113281 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 027 | Total loss: 3.045 | Reg loss: 0.035 | Tree loss: 3.045 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 027 | Total loss: 3.047 | Reg loss: 0.035 | Tree loss: 3.047 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 027 | Total loss: 3.062 | Reg loss: 0.035 | Tree loss: 3.062 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 027 | Total loss: 3.035 | Reg loss: 0.035 | Tree loss: 3.035 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 027 | Total loss: 2.994 | Reg loss: 0.035 | Tree loss: 2.994 | Accuracy: 0.115234 | 0.35 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 027 | Total loss: 3.093 | Reg loss: 0.035 | Tree loss: 3.093 | Accuracy: 0.087891 | 0.35 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 027 | Total loss: 3.037 | Reg loss: 0.035 | Tree loss: 3.037 | Accuracy: 0.103516 | 0.35 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 027 | Total loss: 2.948 | Reg loss: 0.035 | Tree loss: 2.948 | Accuracy: 0.166667 | 0.35 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 55 | Batch: 000 / 027 | Total loss: 3.212 | Reg loss: 0.034 | Tree loss: 3.212 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 027 | Total loss: 3.209 | Reg loss: 0.034 | Tree loss: 3.209 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 027 | Total loss: 3.226 | Reg loss: 0.034 | Tree loss: 3.226 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 027 | Total loss: 3.249 | Reg loss: 0.034 | Tree loss: 3.249 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 027 | Total loss: 3.151 | Reg loss: 0.034 | Tree loss: 3.151 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 027 | Total loss: 3.203 | Reg loss: 0.034 | Tree loss: 3.203 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 027 | Total loss: 3.228 | Reg loss: 0.034 | Tree loss: 3.228 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 027 | Total loss: 3.171 | Reg loss: 0.034 | Tree loss: 3.171 | Accuracy: 0.072266 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 027 | Total loss: 3.138 | Reg loss: 0.034 | Tree loss: 3.138 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 027 | Total loss: 3.166 | Reg loss: 0.034 | Tree loss: 3.166 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 027 | Total loss: 3.136 | Reg loss: 0.034 | Tree loss: 3.136 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 027 | Total loss: 3.154 | Reg loss: 0.034 | Tree loss: 3.154 | Accuracy: 0.097656 | 0.351 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 012 / 027 | Total loss: 3.141 | Reg loss: 0.034 | Tree loss: 3.141 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 027 | Total loss: 3.112 | Reg loss: 0.034 | Tree loss: 3.112 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 027 | Total loss: 3.092 | Reg loss: 0.034 | Tree loss: 3.092 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 027 | Total loss: 3.066 | Reg loss: 0.034 | Tree loss: 3.066 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 027 | Total loss: 3.084 | Reg loss: 0.035 | Tree loss: 3.084 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 027 | Total loss: 3.036 | Reg loss: 0.035 | Tree loss: 3.036 | Accuracy: 0.126953 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 027 | Total loss: 3.045 | Reg loss: 0.035 | Tree loss: 3.045 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 027 | Total loss: 3.036 | Reg loss: 0.035 | Tree loss: 3.036 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 027 | Total loss: 3.051 | Reg loss: 0.035 | Tree loss: 3.051 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 027 | Total loss: 3.021 | Reg loss: 0.035 | Tree loss: 3.021 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 027 | Total loss: 3.024 | Reg loss: 0.035 | Tree loss: 3.024 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 027 | Total loss: 2.987 | Reg loss: 0.035 | Tree loss: 2.987 | Accuracy: 0.080078 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 027 | Total loss: 3.016 | Reg loss: 0.035 | Tree loss: 3.016 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 027 | Total loss: 3.022 | Reg loss: 0.035 | Tree loss: 3.022 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 027 | Total loss: 2.815 | Reg loss: 0.035 | Tree loss: 2.815 | Accuracy: 0.166667 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 56 | Batch: 000 / 027 | Total loss: 3.235 | Reg loss: 0.034 | Tree loss: 3.235 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 027 | Total loss: 3.202 | Reg loss: 0.034 | Tree loss: 3.202 | Accuracy: 0.128906 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 027 | Total loss: 3.264 | Reg loss: 0.034 | Tree loss: 3.264 | Accuracy: 0.072266 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 027 | Total loss: 3.157 | Reg loss: 0.034 | Tree loss: 3.157 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 027 | Total loss: 3.169 | Reg loss: 0.034 | Tree loss: 3.169 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 027 | Total loss: 3.119 | Reg loss: 0.034 | Tree loss: 3.119 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 027 | Total loss: 3.141 | Reg loss: 0.034 | Tree loss: 3.141 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 027 | Total loss: 3.154 | Reg loss: 0.034 | Tree loss: 3.154 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 027 | Total loss: 3.123 | Reg loss: 0.034 | Tree loss: 3.123 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 027 | Total loss: 3.152 | Reg loss: 0.034 | Tree loss: 3.152 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 027 | Total loss: 3.055 | Reg loss: 0.034 | Tree loss: 3.055 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 027 | Total loss: 3.090 | Reg loss: 0.034 | Tree loss: 3.090 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 027 | Total loss: 3.100 | Reg loss: 0.034 | Tree loss: 3.100 | Accuracy: 0.130859 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 027 | Total loss: 3.107 | Reg loss: 0.034 | Tree loss: 3.107 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 027 | Total loss: 3.065 | Reg loss: 0.034 | Tree loss: 3.065 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 027 | Total loss: 3.084 | Reg loss: 0.034 | Tree loss: 3.084 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 027 | Total loss: 3.076 | Reg loss: 0.034 | Tree loss: 3.076 | Accuracy: 0.126953 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 027 | Total loss: 3.110 | Reg loss: 0.035 | Tree loss: 3.110 | Accuracy: 0.121094 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 027 | Total loss: 3.026 | Reg loss: 0.035 | Tree loss: 3.026 | Accuracy: 0.080078 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 027 | Total loss: 3.056 | Reg loss: 0.035 | Tree loss: 3.056 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 027 | Total loss: 3.060 | Reg loss: 0.035 | Tree loss: 3.060 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 027 | Total loss: 3.047 | Reg loss: 0.035 | Tree loss: 3.047 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 027 | Total loss: 3.005 | Reg loss: 0.035 | Tree loss: 3.005 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 027 | Total loss: 3.068 | Reg loss: 0.035 | Tree loss: 3.068 | Accuracy: 0.062500 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 027 | Total loss: 3.040 | Reg loss: 0.035 | Tree loss: 3.040 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 027 | Total loss: 3.011 | Reg loss: 0.035 | Tree loss: 3.011 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 027 | Total loss: 2.850 | Reg loss: 0.035 | Tree loss: 2.850 | Accuracy: 0.250000 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 57 | Batch: 000 / 027 | Total loss: 3.172 | Reg loss: 0.034 | Tree loss: 3.172 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 027 | Total loss: 3.225 | Reg loss: 0.034 | Tree loss: 3.225 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 027 | Total loss: 3.168 | Reg loss: 0.034 | Tree loss: 3.168 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 027 | Total loss: 3.245 | Reg loss: 0.034 | Tree loss: 3.245 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 027 | Total loss: 3.180 | Reg loss: 0.034 | Tree loss: 3.180 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 027 | Total loss: 3.240 | Reg loss: 0.034 | Tree loss: 3.240 | Accuracy: 0.072266 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 027 | Total loss: 3.117 | Reg loss: 0.034 | Tree loss: 3.117 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 027 | Total loss: 3.102 | Reg loss: 0.034 | Tree loss: 3.102 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 027 | Total loss: 3.111 | Reg loss: 0.034 | Tree loss: 3.111 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 027 | Total loss: 3.141 | Reg loss: 0.034 | Tree loss: 3.141 | Accuracy: 0.083984 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 027 | Total loss: 3.116 | Reg loss: 0.034 | Tree loss: 3.116 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 027 | Total loss: 3.084 | Reg loss: 0.034 | Tree loss: 3.084 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 027 | Total loss: 3.063 | Reg loss: 0.034 | Tree loss: 3.063 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 027 | Total loss: 3.100 | Reg loss: 0.034 | Tree loss: 3.100 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 027 | Total loss: 3.085 | Reg loss: 0.034 | Tree loss: 3.085 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 027 | Total loss: 3.035 | Reg loss: 0.034 | Tree loss: 3.035 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 027 | Total loss: 3.022 | Reg loss: 0.034 | Tree loss: 3.022 | Accuracy: 0.113281 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 027 | Total loss: 3.110 | Reg loss: 0.034 | Tree loss: 3.110 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 027 | Total loss: 3.042 | Reg loss: 0.034 | Tree loss: 3.042 | Accuracy: 0.111328 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 027 | Total loss: 3.009 | Reg loss: 0.035 | Tree loss: 3.009 | Accuracy: 0.107422 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 027 | Total loss: 3.065 | Reg loss: 0.035 | Tree loss: 3.065 | Accuracy: 0.091797 | 0.351 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 021 / 027 | Total loss: 3.039 | Reg loss: 0.035 | Tree loss: 3.039 | Accuracy: 0.113281 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 027 | Total loss: 3.022 | Reg loss: 0.035 | Tree loss: 3.022 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 027 | Total loss: 2.986 | Reg loss: 0.035 | Tree loss: 2.986 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 027 | Total loss: 3.015 | Reg loss: 0.035 | Tree loss: 3.015 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 027 | Total loss: 2.981 | Reg loss: 0.035 | Tree loss: 2.981 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 027 | Total loss: 2.969 | Reg loss: 0.035 | Tree loss: 2.969 | Accuracy: 0.000000 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 58 | Batch: 000 / 027 | Total loss: 3.175 | Reg loss: 0.034 | Tree loss: 3.175 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 027 | Total loss: 3.194 | Reg loss: 0.034 | Tree loss: 3.194 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 027 | Total loss: 3.195 | Reg loss: 0.034 | Tree loss: 3.195 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 027 | Total loss: 3.192 | Reg loss: 0.034 | Tree loss: 3.192 | Accuracy: 0.089844 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 027 | Total loss: 3.152 | Reg loss: 0.034 | Tree loss: 3.152 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 027 | Total loss: 3.127 | Reg loss: 0.034 | Tree loss: 3.127 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 027 | Total loss: 3.109 | Reg loss: 0.034 | Tree loss: 3.109 | Accuracy: 0.113281 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 027 | Total loss: 3.180 | Reg loss: 0.034 | Tree loss: 3.180 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 027 | Total loss: 3.104 | Reg loss: 0.034 | Tree loss: 3.104 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 027 | Total loss: 3.075 | Reg loss: 0.034 | Tree loss: 3.075 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 027 | Total loss: 3.157 | Reg loss: 0.034 | Tree loss: 3.157 | Accuracy: 0.113281 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 027 | Total loss: 3.106 | Reg loss: 0.034 | Tree loss: 3.106 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 027 | Total loss: 3.096 | Reg loss: 0.034 | Tree loss: 3.096 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 027 | Total loss: 3.084 | Reg loss: 0.034 | Tree loss: 3.084 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 027 | Total loss: 3.058 | Reg loss: 0.034 | Tree loss: 3.058 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 027 | Total loss: 3.021 | Reg loss: 0.034 | Tree loss: 3.021 | Accuracy: 0.115234 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 027 | Total loss: 3.043 | Reg loss: 0.034 | Tree loss: 3.043 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 027 | Total loss: 3.033 | Reg loss: 0.034 | Tree loss: 3.033 | Accuracy: 0.119141 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 027 | Total loss: 3.010 | Reg loss: 0.034 | Tree loss: 3.010 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 027 | Total loss: 3.047 | Reg loss: 0.034 | Tree loss: 3.047 | Accuracy: 0.103516 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 027 | Total loss: 3.015 | Reg loss: 0.034 | Tree loss: 3.015 | Accuracy: 0.091797 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 027 | Total loss: 3.058 | Reg loss: 0.035 | Tree loss: 3.058 | Accuracy: 0.082031 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 027 | Total loss: 3.010 | Reg loss: 0.035 | Tree loss: 3.010 | Accuracy: 0.126953 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 027 | Total loss: 2.971 | Reg loss: 0.035 | Tree loss: 2.971 | Accuracy: 0.109375 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 027 | Total loss: 3.009 | Reg loss: 0.035 | Tree loss: 3.009 | Accuracy: 0.125000 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 027 | Total loss: 3.018 | Reg loss: 0.035 | Tree loss: 3.018 | Accuracy: 0.080078 | 0.351 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 027 | Total loss: 3.070 | Reg loss: 0.035 | Tree loss: 3.070 | Accuracy: 0.000000 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 59 | Batch: 000 / 027 | Total loss: 3.220 | Reg loss: 0.034 | Tree loss: 3.220 | Accuracy: 0.076172 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 027 | Total loss: 3.149 | Reg loss: 0.034 | Tree loss: 3.149 | Accuracy: 0.080078 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 027 | Total loss: 3.123 | Reg loss: 0.034 | Tree loss: 3.123 | Accuracy: 0.111328 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 027 | Total loss: 3.143 | Reg loss: 0.034 | Tree loss: 3.143 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 027 | Total loss: 3.173 | Reg loss: 0.034 | Tree loss: 3.173 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 027 | Total loss: 3.120 | Reg loss: 0.034 | Tree loss: 3.120 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 027 | Total loss: 3.140 | Reg loss: 0.034 | Tree loss: 3.140 | Accuracy: 0.093750 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 027 | Total loss: 3.119 | Reg loss: 0.034 | Tree loss: 3.119 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 027 | Total loss: 3.134 | Reg loss: 0.034 | Tree loss: 3.134 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 027 | Total loss: 3.090 | Reg loss: 0.034 | Tree loss: 3.090 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 027 | Total loss: 3.101 | Reg loss: 0.034 | Tree loss: 3.101 | Accuracy: 0.095703 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 027 | Total loss: 3.128 | Reg loss: 0.034 | Tree loss: 3.128 | Accuracy: 0.099609 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 027 | Total loss: 3.049 | Reg loss: 0.034 | Tree loss: 3.049 | Accuracy: 0.087891 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 027 | Total loss: 3.088 | Reg loss: 0.034 | Tree loss: 3.088 | Accuracy: 0.101562 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 027 | Total loss: 3.103 | Reg loss: 0.034 | Tree loss: 3.103 | Accuracy: 0.085938 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 027 | Total loss: 3.046 | Reg loss: 0.034 | Tree loss: 3.046 | Accuracy: 0.097656 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 027 | Total loss: 3.095 | Reg loss: 0.034 | Tree loss: 3.095 | Accuracy: 0.123047 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 027 | Total loss: 3.032 | Reg loss: 0.034 | Tree loss: 3.032 | Accuracy: 0.105469 | 0.351 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 027 | Total loss: 3.021 | Reg loss: 0.034 | Tree loss: 3.021 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 027 | Total loss: 3.016 | Reg loss: 0.034 | Tree loss: 3.016 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 027 | Total loss: 3.002 | Reg loss: 0.034 | Tree loss: 3.002 | Accuracy: 0.138672 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 027 | Total loss: 3.013 | Reg loss: 0.034 | Tree loss: 3.013 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 027 | Total loss: 2.991 | Reg loss: 0.035 | Tree loss: 2.991 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 027 | Total loss: 2.997 | Reg loss: 0.035 | Tree loss: 2.997 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 027 | Total loss: 2.952 | Reg loss: 0.035 | Tree loss: 2.952 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 027 | Total loss: 3.017 | Reg loss: 0.035 | Tree loss: 3.017 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 027 | Total loss: 2.923 | Reg loss: 0.035 | Tree loss: 2.923 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 60 | Batch: 000 / 027 | Total loss: 3.212 | Reg loss: 0.034 | Tree loss: 3.212 | Accuracy: 0.068359 | 0.352 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 001 / 027 | Total loss: 3.205 | Reg loss: 0.034 | Tree loss: 3.205 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 027 | Total loss: 3.179 | Reg loss: 0.034 | Tree loss: 3.179 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 027 | Total loss: 3.153 | Reg loss: 0.034 | Tree loss: 3.153 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 027 | Total loss: 3.120 | Reg loss: 0.034 | Tree loss: 3.120 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 027 | Total loss: 3.112 | Reg loss: 0.034 | Tree loss: 3.112 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 027 | Total loss: 3.115 | Reg loss: 0.034 | Tree loss: 3.115 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 027 | Total loss: 3.125 | Reg loss: 0.034 | Tree loss: 3.125 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 027 | Total loss: 3.129 | Reg loss: 0.034 | Tree loss: 3.129 | Accuracy: 0.125000 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 027 | Total loss: 3.075 | Reg loss: 0.034 | Tree loss: 3.075 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 027 | Total loss: 3.074 | Reg loss: 0.034 | Tree loss: 3.074 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 027 | Total loss: 3.061 | Reg loss: 0.034 | Tree loss: 3.061 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 027 | Total loss: 3.135 | Reg loss: 0.034 | Tree loss: 3.135 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 027 | Total loss: 3.066 | Reg loss: 0.034 | Tree loss: 3.066 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 027 | Total loss: 3.058 | Reg loss: 0.034 | Tree loss: 3.058 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 027 | Total loss: 3.045 | Reg loss: 0.034 | Tree loss: 3.045 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 027 | Total loss: 3.014 | Reg loss: 0.034 | Tree loss: 3.014 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 027 | Total loss: 3.053 | Reg loss: 0.034 | Tree loss: 3.053 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 027 | Total loss: 2.988 | Reg loss: 0.034 | Tree loss: 2.988 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 027 | Total loss: 2.957 | Reg loss: 0.034 | Tree loss: 2.957 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 027 | Total loss: 2.988 | Reg loss: 0.034 | Tree loss: 2.988 | Accuracy: 0.117188 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 027 | Total loss: 2.994 | Reg loss: 0.034 | Tree loss: 2.994 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 027 | Total loss: 3.005 | Reg loss: 0.035 | Tree loss: 3.005 | Accuracy: 0.126953 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 027 | Total loss: 2.994 | Reg loss: 0.035 | Tree loss: 2.994 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 027 | Total loss: 2.984 | Reg loss: 0.035 | Tree loss: 2.984 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 027 | Total loss: 2.983 | Reg loss: 0.035 | Tree loss: 2.983 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 027 | Total loss: 2.874 | Reg loss: 0.035 | Tree loss: 2.874 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 61 | Batch: 000 / 027 | Total loss: 3.144 | Reg loss: 0.034 | Tree loss: 3.144 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 027 | Total loss: 3.132 | Reg loss: 0.034 | Tree loss: 3.132 | Accuracy: 0.126953 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 027 | Total loss: 3.122 | Reg loss: 0.034 | Tree loss: 3.122 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 027 | Total loss: 3.177 | Reg loss: 0.034 | Tree loss: 3.177 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 027 | Total loss: 3.119 | Reg loss: 0.034 | Tree loss: 3.119 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 027 | Total loss: 3.097 | Reg loss: 0.034 | Tree loss: 3.097 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 027 | Total loss: 3.097 | Reg loss: 0.034 | Tree loss: 3.097 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 027 | Total loss: 3.094 | Reg loss: 0.034 | Tree loss: 3.094 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 027 | Total loss: 3.048 | Reg loss: 0.034 | Tree loss: 3.048 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 027 | Total loss: 3.073 | Reg loss: 0.034 | Tree loss: 3.073 | Accuracy: 0.123047 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 027 | Total loss: 3.137 | Reg loss: 0.034 | Tree loss: 3.137 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 027 | Total loss: 3.078 | Reg loss: 0.034 | Tree loss: 3.078 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 027 | Total loss: 3.058 | Reg loss: 0.034 | Tree loss: 3.058 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 027 | Total loss: 3.060 | Reg loss: 0.034 | Tree loss: 3.060 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 027 | Total loss: 3.019 | Reg loss: 0.034 | Tree loss: 3.019 | Accuracy: 0.123047 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 027 | Total loss: 3.055 | Reg loss: 0.034 | Tree loss: 3.055 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 027 | Total loss: 3.063 | Reg loss: 0.034 | Tree loss: 3.063 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 027 | Total loss: 3.001 | Reg loss: 0.034 | Tree loss: 3.001 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 027 | Total loss: 3.085 | Reg loss: 0.034 | Tree loss: 3.085 | Accuracy: 0.078125 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 027 | Total loss: 3.026 | Reg loss: 0.034 | Tree loss: 3.026 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 027 | Total loss: 3.005 | Reg loss: 0.034 | Tree loss: 3.005 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 027 | Total loss: 2.994 | Reg loss: 0.034 | Tree loss: 2.994 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 027 | Total loss: 2.983 | Reg loss: 0.034 | Tree loss: 2.983 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 027 | Total loss: 3.058 | Reg loss: 0.035 | Tree loss: 3.058 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 027 | Total loss: 2.968 | Reg loss: 0.035 | Tree loss: 2.968 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 027 | Total loss: 2.995 | Reg loss: 0.035 | Tree loss: 2.995 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 027 | Total loss: 3.344 | Reg loss: 0.035 | Tree loss: 3.344 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 62 | Batch: 000 / 027 | Total loss: 3.124 | Reg loss: 0.034 | Tree loss: 3.124 | Accuracy: 0.119141 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 027 | Total loss: 3.128 | Reg loss: 0.034 | Tree loss: 3.128 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 027 | Total loss: 3.151 | Reg loss: 0.034 | Tree loss: 3.151 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 027 | Total loss: 3.142 | Reg loss: 0.034 | Tree loss: 3.142 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 027 | Total loss: 3.156 | Reg loss: 0.034 | Tree loss: 3.156 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 027 | Total loss: 3.101 | Reg loss: 0.034 | Tree loss: 3.101 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 027 | Total loss: 3.093 | Reg loss: 0.034 | Tree loss: 3.093 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 027 | Total loss: 3.124 | Reg loss: 0.034 | Tree loss: 3.124 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 027 | Total loss: 3.061 | Reg loss: 0.034 | Tree loss: 3.061 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 027 | Total loss: 3.060 | Reg loss: 0.034 | Tree loss: 3.060 | Accuracy: 0.117188 | 0.352 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 010 / 027 | Total loss: 3.099 | Reg loss: 0.034 | Tree loss: 3.099 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 027 | Total loss: 3.107 | Reg loss: 0.034 | Tree loss: 3.107 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 027 | Total loss: 3.028 | Reg loss: 0.034 | Tree loss: 3.028 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 027 | Total loss: 3.075 | Reg loss: 0.034 | Tree loss: 3.075 | Accuracy: 0.119141 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 027 | Total loss: 3.064 | Reg loss: 0.034 | Tree loss: 3.064 | Accuracy: 0.070312 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 027 | Total loss: 2.985 | Reg loss: 0.034 | Tree loss: 2.985 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 027 | Total loss: 3.000 | Reg loss: 0.034 | Tree loss: 3.000 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 027 | Total loss: 3.030 | Reg loss: 0.034 | Tree loss: 3.030 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 027 | Total loss: 3.029 | Reg loss: 0.034 | Tree loss: 3.029 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 027 | Total loss: 3.008 | Reg loss: 0.034 | Tree loss: 3.008 | Accuracy: 0.078125 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 027 | Total loss: 3.026 | Reg loss: 0.034 | Tree loss: 3.026 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 027 | Total loss: 2.994 | Reg loss: 0.034 | Tree loss: 2.994 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 027 | Total loss: 2.978 | Reg loss: 0.034 | Tree loss: 2.978 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 027 | Total loss: 2.949 | Reg loss: 0.034 | Tree loss: 2.949 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 027 | Total loss: 3.024 | Reg loss: 0.034 | Tree loss: 3.024 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 027 | Total loss: 2.969 | Reg loss: 0.035 | Tree loss: 2.969 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 027 | Total loss: 3.177 | Reg loss: 0.035 | Tree loss: 3.177 | Accuracy: 0.250000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 63 | Batch: 000 / 027 | Total loss: 3.181 | Reg loss: 0.034 | Tree loss: 3.181 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 027 | Total loss: 3.148 | Reg loss: 0.034 | Tree loss: 3.148 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 027 | Total loss: 3.093 | Reg loss: 0.034 | Tree loss: 3.093 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 027 | Total loss: 3.114 | Reg loss: 0.034 | Tree loss: 3.114 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 027 | Total loss: 3.153 | Reg loss: 0.034 | Tree loss: 3.153 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 027 | Total loss: 3.122 | Reg loss: 0.034 | Tree loss: 3.122 | Accuracy: 0.082031 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 027 | Total loss: 3.092 | Reg loss: 0.034 | Tree loss: 3.092 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 027 | Total loss: 3.124 | Reg loss: 0.034 | Tree loss: 3.124 | Accuracy: 0.117188 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 027 | Total loss: 3.033 | Reg loss: 0.034 | Tree loss: 3.033 | Accuracy: 0.123047 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 027 | Total loss: 3.071 | Reg loss: 0.034 | Tree loss: 3.071 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 027 | Total loss: 3.106 | Reg loss: 0.034 | Tree loss: 3.106 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 027 | Total loss: 3.101 | Reg loss: 0.034 | Tree loss: 3.101 | Accuracy: 0.083984 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 027 | Total loss: 3.051 | Reg loss: 0.034 | Tree loss: 3.051 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 027 | Total loss: 2.999 | Reg loss: 0.034 | Tree loss: 2.999 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 027 | Total loss: 3.027 | Reg loss: 0.034 | Tree loss: 3.027 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 027 | Total loss: 3.031 | Reg loss: 0.034 | Tree loss: 3.031 | Accuracy: 0.119141 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 027 | Total loss: 3.061 | Reg loss: 0.034 | Tree loss: 3.061 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 027 | Total loss: 2.994 | Reg loss: 0.034 | Tree loss: 2.994 | Accuracy: 0.072266 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 027 | Total loss: 3.042 | Reg loss: 0.034 | Tree loss: 3.042 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 027 | Total loss: 2.979 | Reg loss: 0.034 | Tree loss: 2.979 | Accuracy: 0.107422 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 027 | Total loss: 2.930 | Reg loss: 0.034 | Tree loss: 2.930 | Accuracy: 0.136719 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 027 | Total loss: 3.003 | Reg loss: 0.034 | Tree loss: 3.003 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 027 | Total loss: 2.959 | Reg loss: 0.034 | Tree loss: 2.959 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 027 | Total loss: 2.980 | Reg loss: 0.034 | Tree loss: 2.980 | Accuracy: 0.089844 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 027 | Total loss: 2.980 | Reg loss: 0.034 | Tree loss: 2.980 | Accuracy: 0.078125 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 027 | Total loss: 2.969 | Reg loss: 0.034 | Tree loss: 2.969 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 027 | Total loss: 2.984 | Reg loss: 0.035 | Tree loss: 2.984 | Accuracy: 0.083333 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 64 | Batch: 000 / 027 | Total loss: 3.190 | Reg loss: 0.034 | Tree loss: 3.190 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 027 | Total loss: 3.160 | Reg loss: 0.034 | Tree loss: 3.160 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 027 | Total loss: 3.179 | Reg loss: 0.034 | Tree loss: 3.179 | Accuracy: 0.125000 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 027 | Total loss: 3.151 | Reg loss: 0.034 | Tree loss: 3.151 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 027 | Total loss: 3.119 | Reg loss: 0.034 | Tree loss: 3.119 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 027 | Total loss: 3.086 | Reg loss: 0.034 | Tree loss: 3.086 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 027 | Total loss: 3.047 | Reg loss: 0.034 | Tree loss: 3.047 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 027 | Total loss: 3.117 | Reg loss: 0.034 | Tree loss: 3.117 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 027 | Total loss: 3.026 | Reg loss: 0.034 | Tree loss: 3.026 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 027 | Total loss: 3.034 | Reg loss: 0.034 | Tree loss: 3.034 | Accuracy: 0.093750 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 027 | Total loss: 3.067 | Reg loss: 0.034 | Tree loss: 3.067 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 027 | Total loss: 3.050 | Reg loss: 0.034 | Tree loss: 3.050 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 027 | Total loss: 3.044 | Reg loss: 0.034 | Tree loss: 3.044 | Accuracy: 0.113281 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 027 | Total loss: 3.046 | Reg loss: 0.034 | Tree loss: 3.046 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 027 | Total loss: 2.984 | Reg loss: 0.034 | Tree loss: 2.984 | Accuracy: 0.087891 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 027 | Total loss: 3.038 | Reg loss: 0.034 | Tree loss: 3.038 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 027 | Total loss: 3.080 | Reg loss: 0.034 | Tree loss: 3.080 | Accuracy: 0.085938 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 027 | Total loss: 2.952 | Reg loss: 0.034 | Tree loss: 2.952 | Accuracy: 0.111328 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 027 | Total loss: 2.997 | Reg loss: 0.034 | Tree loss: 2.997 | Accuracy: 0.123047 | 0.352 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Batch: 019 / 027 | Total loss: 3.014 | Reg loss: 0.034 | Tree loss: 3.014 | Accuracy: 0.076172 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 027 | Total loss: 2.964 | Reg loss: 0.034 | Tree loss: 2.964 | Accuracy: 0.128906 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 027 | Total loss: 2.981 | Reg loss: 0.034 | Tree loss: 2.981 | Accuracy: 0.117188 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 027 | Total loss: 2.970 | Reg loss: 0.034 | Tree loss: 2.970 | Accuracy: 0.105469 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 027 | Total loss: 2.965 | Reg loss: 0.034 | Tree loss: 2.965 | Accuracy: 0.113281 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 027 | Total loss: 3.018 | Reg loss: 0.034 | Tree loss: 3.018 | Accuracy: 0.101562 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 027 | Total loss: 2.940 | Reg loss: 0.034 | Tree loss: 2.940 | Accuracy: 0.078125 | 0.352 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 027 | Total loss: 2.823 | Reg loss: 0.035 | Tree loss: 2.823 | Accuracy: 0.083333 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 65 | Batch: 000 / 027 | Total loss: 3.169 | Reg loss: 0.034 | Tree loss: 3.169 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 027 | Total loss: 3.129 | Reg loss: 0.034 | Tree loss: 3.129 | Accuracy: 0.080078 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 027 | Total loss: 3.161 | Reg loss: 0.034 | Tree loss: 3.161 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 027 | Total loss: 3.107 | Reg loss: 0.034 | Tree loss: 3.107 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 027 | Total loss: 3.112 | Reg loss: 0.034 | Tree loss: 3.112 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 027 | Total loss: 3.113 | Reg loss: 0.034 | Tree loss: 3.113 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 027 | Total loss: 3.105 | Reg loss: 0.034 | Tree loss: 3.105 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 027 | Total loss: 3.076 | Reg loss: 0.034 | Tree loss: 3.076 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 027 | Total loss: 3.056 | Reg loss: 0.034 | Tree loss: 3.056 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 027 | Total loss: 3.022 | Reg loss: 0.034 | Tree loss: 3.022 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 027 | Total loss: 3.045 | Reg loss: 0.034 | Tree loss: 3.045 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 027 | Total loss: 3.056 | Reg loss: 0.034 | Tree loss: 3.056 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 027 | Total loss: 3.012 | Reg loss: 0.034 | Tree loss: 3.012 | Accuracy: 0.109375 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 027 | Total loss: 3.021 | Reg loss: 0.034 | Tree loss: 3.021 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 027 | Total loss: 3.040 | Reg loss: 0.034 | Tree loss: 3.040 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 027 | Total loss: 2.989 | Reg loss: 0.034 | Tree loss: 2.989 | Accuracy: 0.091797 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 027 | Total loss: 3.013 | Reg loss: 0.034 | Tree loss: 3.013 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 027 | Total loss: 3.050 | Reg loss: 0.034 | Tree loss: 3.050 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 027 | Total loss: 2.988 | Reg loss: 0.034 | Tree loss: 2.988 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 027 | Total loss: 2.968 | Reg loss: 0.034 | Tree loss: 2.968 | Accuracy: 0.097656 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 027 | Total loss: 3.039 | Reg loss: 0.034 | Tree loss: 3.039 | Accuracy: 0.095703 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 027 | Total loss: 2.932 | Reg loss: 0.034 | Tree loss: 2.932 | Accuracy: 0.103516 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 027 | Total loss: 2.961 | Reg loss: 0.034 | Tree loss: 2.961 | Accuracy: 0.080078 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 027 | Total loss: 2.972 | Reg loss: 0.034 | Tree loss: 2.972 | Accuracy: 0.115234 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 027 | Total loss: 2.949 | Reg loss: 0.034 | Tree loss: 2.949 | Accuracy: 0.099609 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 027 | Total loss: 2.949 | Reg loss: 0.034 | Tree loss: 2.949 | Accuracy: 0.117188 | 0.352 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 027 | Total loss: 2.919 | Reg loss: 0.034 | Tree loss: 2.919 | Accuracy: 0.000000 | 0.352 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 66 | Batch: 000 / 027 | Total loss: 3.130 | Reg loss: 0.034 | Tree loss: 3.130 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 027 | Total loss: 3.159 | Reg loss: 0.034 | Tree loss: 3.159 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 027 | Total loss: 3.109 | Reg loss: 0.034 | Tree loss: 3.109 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 027 | Total loss: 3.099 | Reg loss: 0.034 | Tree loss: 3.099 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 027 | Total loss: 3.094 | Reg loss: 0.034 | Tree loss: 3.094 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 027 | Total loss: 3.059 | Reg loss: 0.034 | Tree loss: 3.059 | Accuracy: 0.130859 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 027 | Total loss: 3.075 | Reg loss: 0.034 | Tree loss: 3.075 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 027 | Total loss: 3.070 | Reg loss: 0.034 | Tree loss: 3.070 | Accuracy: 0.074219 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 027 | Total loss: 3.057 | Reg loss: 0.034 | Tree loss: 3.057 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 027 | Total loss: 3.052 | Reg loss: 0.034 | Tree loss: 3.052 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 027 | Total loss: 3.068 | Reg loss: 0.034 | Tree loss: 3.068 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 027 | Total loss: 3.009 | Reg loss: 0.034 | Tree loss: 3.009 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 027 | Total loss: 3.062 | Reg loss: 0.034 | Tree loss: 3.062 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 027 | Total loss: 3.083 | Reg loss: 0.034 | Tree loss: 3.083 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 027 | Total loss: 3.041 | Reg loss: 0.034 | Tree loss: 3.041 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 027 | Total loss: 2.998 | Reg loss: 0.034 | Tree loss: 2.998 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 027 | Total loss: 3.002 | Reg loss: 0.034 | Tree loss: 3.002 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 027 | Total loss: 3.010 | Reg loss: 0.034 | Tree loss: 3.010 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 027 | Total loss: 2.995 | Reg loss: 0.034 | Tree loss: 2.995 | Accuracy: 0.072266 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 027 | Total loss: 3.001 | Reg loss: 0.034 | Tree loss: 3.001 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 027 | Total loss: 2.973 | Reg loss: 0.034 | Tree loss: 2.973 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 027 | Total loss: 2.955 | Reg loss: 0.034 | Tree loss: 2.955 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 027 | Total loss: 3.006 | Reg loss: 0.034 | Tree loss: 3.006 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 027 | Total loss: 2.933 | Reg loss: 0.034 | Tree loss: 2.933 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 027 | Total loss: 2.963 | Reg loss: 0.034 | Tree loss: 2.963 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 027 | Total loss: 2.929 | Reg loss: 0.034 | Tree loss: 2.929 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 027 | Total loss: 2.845 | Reg loss: 0.034 | Tree loss: 2.845 | Accuracy: 0.333333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 000 / 027 | Total loss: 3.115 | Reg loss: 0.034 | Tree loss: 3.115 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 027 | Total loss: 3.117 | Reg loss: 0.034 | Tree loss: 3.117 | Accuracy: 0.087891 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 027 | Total loss: 3.112 | Reg loss: 0.034 | Tree loss: 3.112 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 027 | Total loss: 3.070 | Reg loss: 0.034 | Tree loss: 3.070 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 027 | Total loss: 3.077 | Reg loss: 0.034 | Tree loss: 3.077 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 027 | Total loss: 3.085 | Reg loss: 0.034 | Tree loss: 3.085 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 027 | Total loss: 3.017 | Reg loss: 0.034 | Tree loss: 3.017 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 027 | Total loss: 3.132 | Reg loss: 0.034 | Tree loss: 3.132 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 027 | Total loss: 3.072 | Reg loss: 0.034 | Tree loss: 3.072 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 027 | Total loss: 3.036 | Reg loss: 0.034 | Tree loss: 3.036 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 027 | Total loss: 3.036 | Reg loss: 0.034 | Tree loss: 3.036 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 027 | Total loss: 3.033 | Reg loss: 0.034 | Tree loss: 3.033 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 027 | Total loss: 3.049 | Reg loss: 0.034 | Tree loss: 3.049 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 027 | Total loss: 2.998 | Reg loss: 0.034 | Tree loss: 2.998 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 027 | Total loss: 3.040 | Reg loss: 0.034 | Tree loss: 3.040 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 027 | Total loss: 2.998 | Reg loss: 0.034 | Tree loss: 2.998 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 027 | Total loss: 3.014 | Reg loss: 0.034 | Tree loss: 3.014 | Accuracy: 0.078125 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 027 | Total loss: 3.010 | Reg loss: 0.034 | Tree loss: 3.010 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 027 | Total loss: 2.988 | Reg loss: 0.034 | Tree loss: 2.988 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 027 | Total loss: 2.949 | Reg loss: 0.034 | Tree loss: 2.949 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 027 | Total loss: 2.961 | Reg loss: 0.034 | Tree loss: 2.961 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 027 | Total loss: 3.000 | Reg loss: 0.034 | Tree loss: 3.000 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 027 | Total loss: 3.000 | Reg loss: 0.034 | Tree loss: 3.000 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 027 | Total loss: 2.946 | Reg loss: 0.034 | Tree loss: 2.946 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 027 | Total loss: 2.947 | Reg loss: 0.034 | Tree loss: 2.947 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 027 | Total loss: 2.999 | Reg loss: 0.034 | Tree loss: 2.999 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 027 | Total loss: 2.795 | Reg loss: 0.034 | Tree loss: 2.795 | Accuracy: 0.083333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 68 | Batch: 000 / 027 | Total loss: 3.149 | Reg loss: 0.034 | Tree loss: 3.149 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 027 | Total loss: 3.096 | Reg loss: 0.034 | Tree loss: 3.096 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 027 | Total loss: 3.112 | Reg loss: 0.034 | Tree loss: 3.112 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 027 | Total loss: 3.074 | Reg loss: 0.034 | Tree loss: 3.074 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 027 | Total loss: 3.122 | Reg loss: 0.034 | Tree loss: 3.122 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 027 | Total loss: 3.054 | Reg loss: 0.034 | Tree loss: 3.054 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 027 | Total loss: 3.028 | Reg loss: 0.034 | Tree loss: 3.028 | Accuracy: 0.128906 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 027 | Total loss: 3.086 | Reg loss: 0.034 | Tree loss: 3.086 | Accuracy: 0.080078 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 027 | Total loss: 3.058 | Reg loss: 0.034 | Tree loss: 3.058 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 027 | Total loss: 3.038 | Reg loss: 0.034 | Tree loss: 3.038 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 027 | Total loss: 3.008 | Reg loss: 0.034 | Tree loss: 3.008 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 027 | Total loss: 3.070 | Reg loss: 0.034 | Tree loss: 3.070 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 027 | Total loss: 3.031 | Reg loss: 0.034 | Tree loss: 3.031 | Accuracy: 0.087891 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 027 | Total loss: 3.028 | Reg loss: 0.034 | Tree loss: 3.028 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 027 | Total loss: 3.015 | Reg loss: 0.034 | Tree loss: 3.015 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 027 | Total loss: 3.028 | Reg loss: 0.034 | Tree loss: 3.028 | Accuracy: 0.087891 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 027 | Total loss: 3.000 | Reg loss: 0.034 | Tree loss: 3.000 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 027 | Total loss: 2.974 | Reg loss: 0.034 | Tree loss: 2.974 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 027 | Total loss: 3.005 | Reg loss: 0.034 | Tree loss: 3.005 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 027 | Total loss: 2.989 | Reg loss: 0.034 | Tree loss: 2.989 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 027 | Total loss: 2.914 | Reg loss: 0.034 | Tree loss: 2.914 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 027 | Total loss: 2.981 | Reg loss: 0.034 | Tree loss: 2.981 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 027 | Total loss: 2.941 | Reg loss: 0.034 | Tree loss: 2.941 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 027 | Total loss: 2.980 | Reg loss: 0.034 | Tree loss: 2.980 | Accuracy: 0.087891 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 027 | Total loss: 2.958 | Reg loss: 0.034 | Tree loss: 2.958 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 027 | Total loss: 2.962 | Reg loss: 0.034 | Tree loss: 2.962 | Accuracy: 0.080078 | 0.353 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 027 | Total loss: 3.151 | Reg loss: 0.034 | Tree loss: 3.151 | Accuracy: 0.083333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 69 | Batch: 000 / 027 | Total loss: 3.160 | Reg loss: 0.034 | Tree loss: 3.160 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 027 | Total loss: 3.144 | Reg loss: 0.034 | Tree loss: 3.144 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 027 | Total loss: 3.082 | Reg loss: 0.034 | Tree loss: 3.082 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 027 | Total loss: 3.115 | Reg loss: 0.034 | Tree loss: 3.115 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 027 | Total loss: 3.084 | Reg loss: 0.034 | Tree loss: 3.084 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 027 | Total loss: 3.065 | Reg loss: 0.034 | Tree loss: 3.065 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 027 | Total loss: 3.053 | Reg loss: 0.034 | Tree loss: 3.053 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 027 | Total loss: 3.068 | Reg loss: 0.034 | Tree loss: 3.068 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 027 | Total loss: 3.063 | Reg loss: 0.034 | Tree loss: 3.063 | Accuracy: 0.089844 | 0.353 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 009 / 027 | Total loss: 3.054 | Reg loss: 0.034 | Tree loss: 3.054 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 027 | Total loss: 3.008 | Reg loss: 0.034 | Tree loss: 3.008 | Accuracy: 0.132812 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 027 | Total loss: 3.042 | Reg loss: 0.034 | Tree loss: 3.042 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 027 | Total loss: 3.008 | Reg loss: 0.034 | Tree loss: 3.008 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 027 | Total loss: 2.997 | Reg loss: 0.034 | Tree loss: 2.997 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 027 | Total loss: 2.987 | Reg loss: 0.034 | Tree loss: 2.987 | Accuracy: 0.132812 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 027 | Total loss: 2.979 | Reg loss: 0.034 | Tree loss: 2.979 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 027 | Total loss: 3.004 | Reg loss: 0.034 | Tree loss: 3.004 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 027 | Total loss: 3.005 | Reg loss: 0.034 | Tree loss: 3.005 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 027 | Total loss: 2.956 | Reg loss: 0.034 | Tree loss: 2.956 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 027 | Total loss: 2.953 | Reg loss: 0.034 | Tree loss: 2.953 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 027 | Total loss: 2.976 | Reg loss: 0.034 | Tree loss: 2.976 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 027 | Total loss: 2.976 | Reg loss: 0.034 | Tree loss: 2.976 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 027 | Total loss: 2.938 | Reg loss: 0.034 | Tree loss: 2.938 | Accuracy: 0.128906 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 027 | Total loss: 2.945 | Reg loss: 0.034 | Tree loss: 2.945 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 027 | Total loss: 2.983 | Reg loss: 0.034 | Tree loss: 2.983 | Accuracy: 0.080078 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 027 | Total loss: 2.944 | Reg loss: 0.034 | Tree loss: 2.944 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 027 | Total loss: 2.933 | Reg loss: 0.034 | Tree loss: 2.933 | Accuracy: 0.000000 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 70 | Batch: 000 / 027 | Total loss: 3.171 | Reg loss: 0.034 | Tree loss: 3.171 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 027 | Total loss: 3.092 | Reg loss: 0.034 | Tree loss: 3.092 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 027 | Total loss: 3.095 | Reg loss: 0.034 | Tree loss: 3.095 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 027 | Total loss: 3.110 | Reg loss: 0.034 | Tree loss: 3.110 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 027 | Total loss: 3.053 | Reg loss: 0.034 | Tree loss: 3.053 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 027 | Total loss: 3.072 | Reg loss: 0.034 | Tree loss: 3.072 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 027 | Total loss: 3.041 | Reg loss: 0.034 | Tree loss: 3.041 | Accuracy: 0.126953 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 027 | Total loss: 3.094 | Reg loss: 0.034 | Tree loss: 3.094 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 027 | Total loss: 3.058 | Reg loss: 0.034 | Tree loss: 3.058 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 027 | Total loss: 3.028 | Reg loss: 0.034 | Tree loss: 3.028 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 027 | Total loss: 3.097 | Reg loss: 0.034 | Tree loss: 3.097 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 027 | Total loss: 2.995 | Reg loss: 0.034 | Tree loss: 2.995 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 027 | Total loss: 2.986 | Reg loss: 0.034 | Tree loss: 2.986 | Accuracy: 0.130859 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 027 | Total loss: 2.977 | Reg loss: 0.034 | Tree loss: 2.977 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 027 | Total loss: 3.016 | Reg loss: 0.034 | Tree loss: 3.016 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 027 | Total loss: 3.015 | Reg loss: 0.034 | Tree loss: 3.015 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 027 | Total loss: 2.951 | Reg loss: 0.034 | Tree loss: 2.951 | Accuracy: 0.076172 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 027 | Total loss: 2.967 | Reg loss: 0.034 | Tree loss: 2.967 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 027 | Total loss: 2.963 | Reg loss: 0.034 | Tree loss: 2.963 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 027 | Total loss: 2.998 | Reg loss: 0.034 | Tree loss: 2.998 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 027 | Total loss: 2.972 | Reg loss: 0.034 | Tree loss: 2.972 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 027 | Total loss: 2.961 | Reg loss: 0.034 | Tree loss: 2.961 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 027 | Total loss: 2.934 | Reg loss: 0.034 | Tree loss: 2.934 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 027 | Total loss: 2.930 | Reg loss: 0.034 | Tree loss: 2.930 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 027 | Total loss: 2.954 | Reg loss: 0.034 | Tree loss: 2.954 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 027 | Total loss: 2.933 | Reg loss: 0.034 | Tree loss: 2.933 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 027 | Total loss: 2.913 | Reg loss: 0.034 | Tree loss: 2.913 | Accuracy: 0.083333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 71 | Batch: 000 / 027 | Total loss: 3.102 | Reg loss: 0.034 | Tree loss: 3.102 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 027 | Total loss: 3.109 | Reg loss: 0.034 | Tree loss: 3.109 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 027 | Total loss: 3.161 | Reg loss: 0.034 | Tree loss: 3.161 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 027 | Total loss: 3.055 | Reg loss: 0.034 | Tree loss: 3.055 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 027 | Total loss: 3.085 | Reg loss: 0.034 | Tree loss: 3.085 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 027 | Total loss: 3.072 | Reg loss: 0.034 | Tree loss: 3.072 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 027 | Total loss: 3.061 | Reg loss: 0.034 | Tree loss: 3.061 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 027 | Total loss: 3.053 | Reg loss: 0.034 | Tree loss: 3.053 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 027 | Total loss: 3.057 | Reg loss: 0.034 | Tree loss: 3.057 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 027 | Total loss: 3.014 | Reg loss: 0.034 | Tree loss: 3.014 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 027 | Total loss: 3.058 | Reg loss: 0.034 | Tree loss: 3.058 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 027 | Total loss: 3.089 | Reg loss: 0.034 | Tree loss: 3.089 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 027 | Total loss: 3.004 | Reg loss: 0.034 | Tree loss: 3.004 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 027 | Total loss: 2.995 | Reg loss: 0.034 | Tree loss: 2.995 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 027 | Total loss: 2.988 | Reg loss: 0.034 | Tree loss: 2.988 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 027 | Total loss: 2.986 | Reg loss: 0.034 | Tree loss: 2.986 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 027 | Total loss: 2.997 | Reg loss: 0.034 | Tree loss: 2.997 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 027 | Total loss: 2.924 | Reg loss: 0.034 | Tree loss: 2.924 | Accuracy: 0.105469 | 0.353 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 018 / 027 | Total loss: 2.922 | Reg loss: 0.034 | Tree loss: 2.922 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 027 | Total loss: 2.940 | Reg loss: 0.034 | Tree loss: 2.940 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 027 | Total loss: 2.981 | Reg loss: 0.034 | Tree loss: 2.981 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 027 | Total loss: 2.936 | Reg loss: 0.034 | Tree loss: 2.936 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 027 | Total loss: 2.921 | Reg loss: 0.034 | Tree loss: 2.921 | Accuracy: 0.074219 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 027 | Total loss: 2.980 | Reg loss: 0.034 | Tree loss: 2.980 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 027 | Total loss: 2.942 | Reg loss: 0.034 | Tree loss: 2.942 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 027 | Total loss: 2.932 | Reg loss: 0.034 | Tree loss: 2.932 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 027 | Total loss: 3.021 | Reg loss: 0.034 | Tree loss: 3.021 | Accuracy: 0.250000 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 72 | Batch: 000 / 027 | Total loss: 3.123 | Reg loss: 0.034 | Tree loss: 3.123 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 027 | Total loss: 3.095 | Reg loss: 0.034 | Tree loss: 3.095 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 027 | Total loss: 3.120 | Reg loss: 0.034 | Tree loss: 3.120 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 027 | Total loss: 3.088 | Reg loss: 0.034 | Tree loss: 3.088 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 027 | Total loss: 3.066 | Reg loss: 0.034 | Tree loss: 3.066 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 027 | Total loss: 3.073 | Reg loss: 0.034 | Tree loss: 3.073 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 027 | Total loss: 3.090 | Reg loss: 0.034 | Tree loss: 3.090 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 027 | Total loss: 3.054 | Reg loss: 0.034 | Tree loss: 3.054 | Accuracy: 0.068359 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 027 | Total loss: 3.063 | Reg loss: 0.034 | Tree loss: 3.063 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 027 | Total loss: 3.060 | Reg loss: 0.034 | Tree loss: 3.060 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 027 | Total loss: 3.000 | Reg loss: 0.034 | Tree loss: 3.000 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 027 | Total loss: 3.018 | Reg loss: 0.034 | Tree loss: 3.018 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 027 | Total loss: 3.046 | Reg loss: 0.034 | Tree loss: 3.046 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 027 | Total loss: 3.010 | Reg loss: 0.034 | Tree loss: 3.010 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 027 | Total loss: 2.984 | Reg loss: 0.034 | Tree loss: 2.984 | Accuracy: 0.123047 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 027 | Total loss: 2.977 | Reg loss: 0.034 | Tree loss: 2.977 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 027 | Total loss: 2.918 | Reg loss: 0.034 | Tree loss: 2.918 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 027 | Total loss: 2.984 | Reg loss: 0.034 | Tree loss: 2.984 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 027 | Total loss: 2.941 | Reg loss: 0.034 | Tree loss: 2.941 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 027 | Total loss: 2.958 | Reg loss: 0.034 | Tree loss: 2.958 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 027 | Total loss: 2.954 | Reg loss: 0.034 | Tree loss: 2.954 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 027 | Total loss: 2.927 | Reg loss: 0.034 | Tree loss: 2.927 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 027 | Total loss: 2.925 | Reg loss: 0.034 | Tree loss: 2.925 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 027 | Total loss: 2.906 | Reg loss: 0.034 | Tree loss: 2.906 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 027 | Total loss: 2.972 | Reg loss: 0.034 | Tree loss: 2.972 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 027 | Total loss: 2.909 | Reg loss: 0.034 | Tree loss: 2.909 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 027 | Total loss: 2.812 | Reg loss: 0.034 | Tree loss: 2.812 | Accuracy: 0.083333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 73 | Batch: 000 / 027 | Total loss: 3.082 | Reg loss: 0.033 | Tree loss: 3.082 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 027 | Total loss: 3.062 | Reg loss: 0.033 | Tree loss: 3.062 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 027 | Total loss: 3.135 | Reg loss: 0.033 | Tree loss: 3.135 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 027 | Total loss: 3.090 | Reg loss: 0.033 | Tree loss: 3.090 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 027 | Total loss: 3.058 | Reg loss: 0.033 | Tree loss: 3.058 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 027 | Total loss: 3.056 | Reg loss: 0.034 | Tree loss: 3.056 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 027 | Total loss: 3.041 | Reg loss: 0.034 | Tree loss: 3.041 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 027 | Total loss: 3.040 | Reg loss: 0.034 | Tree loss: 3.040 | Accuracy: 0.126953 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 027 | Total loss: 3.075 | Reg loss: 0.034 | Tree loss: 3.075 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 027 | Total loss: 3.045 | Reg loss: 0.034 | Tree loss: 3.045 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 027 | Total loss: 2.990 | Reg loss: 0.034 | Tree loss: 2.990 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 027 | Total loss: 3.016 | Reg loss: 0.034 | Tree loss: 3.016 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 027 | Total loss: 3.012 | Reg loss: 0.034 | Tree loss: 3.012 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 027 | Total loss: 2.965 | Reg loss: 0.034 | Tree loss: 2.965 | Accuracy: 0.076172 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 027 | Total loss: 3.006 | Reg loss: 0.034 | Tree loss: 3.006 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 027 | Total loss: 2.946 | Reg loss: 0.034 | Tree loss: 2.946 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 027 | Total loss: 2.978 | Reg loss: 0.034 | Tree loss: 2.978 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 027 | Total loss: 2.972 | Reg loss: 0.034 | Tree loss: 2.972 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 027 | Total loss: 2.948 | Reg loss: 0.034 | Tree loss: 2.948 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 027 | Total loss: 2.989 | Reg loss: 0.034 | Tree loss: 2.989 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 027 | Total loss: 2.917 | Reg loss: 0.034 | Tree loss: 2.917 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 027 | Total loss: 2.950 | Reg loss: 0.034 | Tree loss: 2.950 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 027 | Total loss: 2.950 | Reg loss: 0.034 | Tree loss: 2.950 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 027 | Total loss: 2.955 | Reg loss: 0.034 | Tree loss: 2.955 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 027 | Total loss: 2.949 | Reg loss: 0.034 | Tree loss: 2.949 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 027 | Total loss: 2.956 | Reg loss: 0.034 | Tree loss: 2.956 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 027 | Total loss: 2.869 | Reg loss: 0.034 | Tree loss: 2.869 | Accuracy: 0.000000 | 0.353 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 74 | Batch: 000 / 027 | Total loss: 3.080 | Reg loss: 0.033 | Tree loss: 3.080 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 027 | Total loss: 3.115 | Reg loss: 0.033 | Tree loss: 3.115 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 027 | Total loss: 3.112 | Reg loss: 0.033 | Tree loss: 3.112 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 027 | Total loss: 3.152 | Reg loss: 0.033 | Tree loss: 3.152 | Accuracy: 0.080078 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 027 | Total loss: 3.041 | Reg loss: 0.033 | Tree loss: 3.041 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 027 | Total loss: 3.023 | Reg loss: 0.033 | Tree loss: 3.023 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 027 | Total loss: 3.084 | Reg loss: 0.033 | Tree loss: 3.084 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 027 | Total loss: 3.033 | Reg loss: 0.033 | Tree loss: 3.033 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 027 | Total loss: 3.037 | Reg loss: 0.033 | Tree loss: 3.037 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 027 | Total loss: 3.019 | Reg loss: 0.033 | Tree loss: 3.019 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 027 | Total loss: 2.997 | Reg loss: 0.034 | Tree loss: 2.997 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 027 | Total loss: 3.001 | Reg loss: 0.034 | Tree loss: 3.001 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 027 | Total loss: 2.988 | Reg loss: 0.034 | Tree loss: 2.988 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 027 | Total loss: 2.975 | Reg loss: 0.034 | Tree loss: 2.975 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 027 | Total loss: 3.003 | Reg loss: 0.034 | Tree loss: 3.003 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 027 | Total loss: 2.942 | Reg loss: 0.034 | Tree loss: 2.942 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 027 | Total loss: 2.952 | Reg loss: 0.034 | Tree loss: 2.952 | Accuracy: 0.123047 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 027 | Total loss: 2.978 | Reg loss: 0.034 | Tree loss: 2.978 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 027 | Total loss: 2.928 | Reg loss: 0.034 | Tree loss: 2.928 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 027 | Total loss: 2.983 | Reg loss: 0.034 | Tree loss: 2.983 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 027 | Total loss: 2.979 | Reg loss: 0.034 | Tree loss: 2.979 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 027 | Total loss: 2.952 | Reg loss: 0.034 | Tree loss: 2.952 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 027 | Total loss: 2.931 | Reg loss: 0.034 | Tree loss: 2.931 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 027 | Total loss: 2.956 | Reg loss: 0.034 | Tree loss: 2.956 | Accuracy: 0.125000 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 027 | Total loss: 2.933 | Reg loss: 0.034 | Tree loss: 2.933 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 027 | Total loss: 2.888 | Reg loss: 0.034 | Tree loss: 2.888 | Accuracy: 0.134766 | 0.353 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 027 | Total loss: 3.089 | Reg loss: 0.034 | Tree loss: 3.089 | Accuracy: 0.083333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 75 | Batch: 000 / 027 | Total loss: 3.109 | Reg loss: 0.033 | Tree loss: 3.109 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 027 | Total loss: 3.071 | Reg loss: 0.033 | Tree loss: 3.071 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 027 | Total loss: 3.103 | Reg loss: 0.033 | Tree loss: 3.103 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 027 | Total loss: 3.072 | Reg loss: 0.033 | Tree loss: 3.072 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 027 | Total loss: 3.076 | Reg loss: 0.033 | Tree loss: 3.076 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 027 | Total loss: 3.059 | Reg loss: 0.033 | Tree loss: 3.059 | Accuracy: 0.126953 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 027 | Total loss: 3.025 | Reg loss: 0.033 | Tree loss: 3.025 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 027 | Total loss: 3.021 | Reg loss: 0.033 | Tree loss: 3.021 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 027 | Total loss: 2.989 | Reg loss: 0.033 | Tree loss: 2.989 | Accuracy: 0.123047 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 027 | Total loss: 3.077 | Reg loss: 0.033 | Tree loss: 3.077 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 027 | Total loss: 3.041 | Reg loss: 0.033 | Tree loss: 3.041 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 027 | Total loss: 2.952 | Reg loss: 0.033 | Tree loss: 2.952 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 027 | Total loss: 3.020 | Reg loss: 0.034 | Tree loss: 3.020 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 027 | Total loss: 3.017 | Reg loss: 0.034 | Tree loss: 3.017 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 027 | Total loss: 2.948 | Reg loss: 0.034 | Tree loss: 2.948 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 027 | Total loss: 2.998 | Reg loss: 0.034 | Tree loss: 2.998 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 027 | Total loss: 2.964 | Reg loss: 0.034 | Tree loss: 2.964 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 027 | Total loss: 2.946 | Reg loss: 0.034 | Tree loss: 2.946 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 027 | Total loss: 2.954 | Reg loss: 0.034 | Tree loss: 2.954 | Accuracy: 0.078125 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 027 | Total loss: 2.939 | Reg loss: 0.034 | Tree loss: 2.939 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 027 | Total loss: 2.911 | Reg loss: 0.034 | Tree loss: 2.911 | Accuracy: 0.126953 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 027 | Total loss: 2.922 | Reg loss: 0.034 | Tree loss: 2.922 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 027 | Total loss: 3.011 | Reg loss: 0.034 | Tree loss: 3.011 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 027 | Total loss: 2.930 | Reg loss: 0.034 | Tree loss: 2.930 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 027 | Total loss: 2.896 | Reg loss: 0.034 | Tree loss: 2.896 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 027 | Total loss: 2.942 | Reg loss: 0.034 | Tree loss: 2.942 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 027 | Total loss: 2.654 | Reg loss: 0.034 | Tree loss: 2.654 | Accuracy: 0.166667 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 76 | Batch: 000 / 027 | Total loss: 3.077 | Reg loss: 0.033 | Tree loss: 3.077 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 027 | Total loss: 3.152 | Reg loss: 0.033 | Tree loss: 3.152 | Accuracy: 0.068359 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 027 | Total loss: 3.083 | Reg loss: 0.033 | Tree loss: 3.083 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 027 | Total loss: 3.085 | Reg loss: 0.033 | Tree loss: 3.085 | Accuracy: 0.074219 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 027 | Total loss: 3.029 | Reg loss: 0.033 | Tree loss: 3.029 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 027 | Total loss: 3.031 | Reg loss: 0.033 | Tree loss: 3.031 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 027 | Total loss: 3.057 | Reg loss: 0.033 | Tree loss: 3.057 | Accuracy: 0.105469 | 0.353 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 007 / 027 | Total loss: 3.051 | Reg loss: 0.033 | Tree loss: 3.051 | Accuracy: 0.146484 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 027 | Total loss: 3.050 | Reg loss: 0.033 | Tree loss: 3.050 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 027 | Total loss: 3.029 | Reg loss: 0.033 | Tree loss: 3.029 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 027 | Total loss: 3.095 | Reg loss: 0.033 | Tree loss: 3.095 | Accuracy: 0.070312 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 027 | Total loss: 3.027 | Reg loss: 0.033 | Tree loss: 3.027 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 027 | Total loss: 2.987 | Reg loss: 0.033 | Tree loss: 2.987 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 027 | Total loss: 2.998 | Reg loss: 0.033 | Tree loss: 2.998 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 027 | Total loss: 3.025 | Reg loss: 0.034 | Tree loss: 3.025 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 027 | Total loss: 2.954 | Reg loss: 0.034 | Tree loss: 2.954 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 027 | Total loss: 2.946 | Reg loss: 0.034 | Tree loss: 2.946 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 027 | Total loss: 2.931 | Reg loss: 0.034 | Tree loss: 2.931 | Accuracy: 0.134766 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 027 | Total loss: 2.984 | Reg loss: 0.034 | Tree loss: 2.984 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 027 | Total loss: 2.948 | Reg loss: 0.034 | Tree loss: 2.948 | Accuracy: 0.087891 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 027 | Total loss: 2.922 | Reg loss: 0.034 | Tree loss: 2.922 | Accuracy: 0.076172 | 0.354 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 027 | Total loss: 2.899 | Reg loss: 0.034 | Tree loss: 2.899 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 027 | Total loss: 2.909 | Reg loss: 0.034 | Tree loss: 2.909 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 027 | Total loss: 2.928 | Reg loss: 0.034 | Tree loss: 2.928 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 027 | Total loss: 2.890 | Reg loss: 0.034 | Tree loss: 2.890 | Accuracy: 0.136719 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 027 | Total loss: 2.876 | Reg loss: 0.034 | Tree loss: 2.876 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 027 | Total loss: 2.917 | Reg loss: 0.034 | Tree loss: 2.917 | Accuracy: 0.000000 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 77 | Batch: 000 / 027 | Total loss: 3.114 | Reg loss: 0.033 | Tree loss: 3.114 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 027 | Total loss: 3.067 | Reg loss: 0.033 | Tree loss: 3.067 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 027 | Total loss: 3.065 | Reg loss: 0.033 | Tree loss: 3.065 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 027 | Total loss: 3.073 | Reg loss: 0.033 | Tree loss: 3.073 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 027 | Total loss: 3.085 | Reg loss: 0.033 | Tree loss: 3.085 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 027 | Total loss: 3.078 | Reg loss: 0.033 | Tree loss: 3.078 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 027 | Total loss: 3.031 | Reg loss: 0.033 | Tree loss: 3.031 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 027 | Total loss: 3.051 | Reg loss: 0.033 | Tree loss: 3.051 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 027 | Total loss: 3.053 | Reg loss: 0.033 | Tree loss: 3.053 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 027 | Total loss: 3.012 | Reg loss: 0.033 | Tree loss: 3.012 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 027 | Total loss: 3.017 | Reg loss: 0.033 | Tree loss: 3.017 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 027 | Total loss: 2.977 | Reg loss: 0.033 | Tree loss: 2.977 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 027 | Total loss: 2.988 | Reg loss: 0.033 | Tree loss: 2.988 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 027 | Total loss: 2.978 | Reg loss: 0.033 | Tree loss: 2.978 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 027 | Total loss: 2.993 | Reg loss: 0.033 | Tree loss: 2.993 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 027 | Total loss: 2.968 | Reg loss: 0.033 | Tree loss: 2.968 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 027 | Total loss: 2.936 | Reg loss: 0.034 | Tree loss: 2.936 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 027 | Total loss: 2.939 | Reg loss: 0.034 | Tree loss: 2.939 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 027 | Total loss: 2.924 | Reg loss: 0.034 | Tree loss: 2.924 | Accuracy: 0.130859 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 027 | Total loss: 2.962 | Reg loss: 0.034 | Tree loss: 2.962 | Accuracy: 0.083984 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 027 | Total loss: 2.902 | Reg loss: 0.034 | Tree loss: 2.902 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 027 | Total loss: 2.936 | Reg loss: 0.034 | Tree loss: 2.936 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 027 | Total loss: 2.939 | Reg loss: 0.034 | Tree loss: 2.939 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 027 | Total loss: 2.914 | Reg loss: 0.034 | Tree loss: 2.914 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 027 | Total loss: 2.947 | Reg loss: 0.034 | Tree loss: 2.947 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 027 | Total loss: 2.910 | Reg loss: 0.034 | Tree loss: 2.910 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 027 | Total loss: 2.811 | Reg loss: 0.034 | Tree loss: 2.811 | Accuracy: 0.083333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 78 | Batch: 000 / 027 | Total loss: 3.091 | Reg loss: 0.033 | Tree loss: 3.091 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 027 | Total loss: 3.094 | Reg loss: 0.033 | Tree loss: 3.094 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 027 | Total loss: 3.084 | Reg loss: 0.033 | Tree loss: 3.084 | Accuracy: 0.128906 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 027 | Total loss: 3.065 | Reg loss: 0.033 | Tree loss: 3.065 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 027 | Total loss: 3.040 | Reg loss: 0.033 | Tree loss: 3.040 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 027 | Total loss: 3.053 | Reg loss: 0.033 | Tree loss: 3.053 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 027 | Total loss: 3.035 | Reg loss: 0.033 | Tree loss: 3.035 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 027 | Total loss: 3.076 | Reg loss: 0.033 | Tree loss: 3.076 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 027 | Total loss: 3.014 | Reg loss: 0.033 | Tree loss: 3.014 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 027 | Total loss: 3.020 | Reg loss: 0.033 | Tree loss: 3.020 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 027 | Total loss: 3.000 | Reg loss: 0.033 | Tree loss: 3.000 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 027 | Total loss: 3.041 | Reg loss: 0.033 | Tree loss: 3.041 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 027 | Total loss: 3.013 | Reg loss: 0.033 | Tree loss: 3.013 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 027 | Total loss: 2.992 | Reg loss: 0.033 | Tree loss: 2.992 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 027 | Total loss: 2.957 | Reg loss: 0.033 | Tree loss: 2.957 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 027 | Total loss: 2.962 | Reg loss: 0.033 | Tree loss: 2.962 | Accuracy: 0.095703 | 0.353 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 016 / 027 | Total loss: 2.992 | Reg loss: 0.033 | Tree loss: 2.992 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 027 | Total loss: 2.955 | Reg loss: 0.034 | Tree loss: 2.955 | Accuracy: 0.126953 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 027 | Total loss: 2.971 | Reg loss: 0.034 | Tree loss: 2.971 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 027 | Total loss: 2.943 | Reg loss: 0.034 | Tree loss: 2.943 | Accuracy: 0.080078 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 027 | Total loss: 2.945 | Reg loss: 0.034 | Tree loss: 2.945 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 027 | Total loss: 2.907 | Reg loss: 0.034 | Tree loss: 2.907 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 027 | Total loss: 2.879 | Reg loss: 0.034 | Tree loss: 2.879 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 027 | Total loss: 2.899 | Reg loss: 0.034 | Tree loss: 2.899 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 027 | Total loss: 2.902 | Reg loss: 0.034 | Tree loss: 2.902 | Accuracy: 0.130859 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 027 | Total loss: 2.854 | Reg loss: 0.034 | Tree loss: 2.854 | Accuracy: 0.134766 | 0.353 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 027 | Total loss: 2.866 | Reg loss: 0.034 | Tree loss: 2.866 | Accuracy: 0.000000 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 79 | Batch: 000 / 027 | Total loss: 3.074 | Reg loss: 0.033 | Tree loss: 3.074 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 027 | Total loss: 3.067 | Reg loss: 0.033 | Tree loss: 3.067 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 027 | Total loss: 3.052 | Reg loss: 0.033 | Tree loss: 3.052 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 027 | Total loss: 3.057 | Reg loss: 0.033 | Tree loss: 3.057 | Accuracy: 0.128906 | 0.354 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 027 | Total loss: 3.069 | Reg loss: 0.033 | Tree loss: 3.069 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 027 | Total loss: 3.059 | Reg loss: 0.033 | Tree loss: 3.059 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 027 | Total loss: 3.031 | Reg loss: 0.033 | Tree loss: 3.031 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 027 | Total loss: 3.024 | Reg loss: 0.033 | Tree loss: 3.024 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 027 | Total loss: 3.022 | Reg loss: 0.033 | Tree loss: 3.022 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 027 | Total loss: 3.024 | Reg loss: 0.033 | Tree loss: 3.024 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 027 | Total loss: 3.005 | Reg loss: 0.033 | Tree loss: 3.005 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 027 | Total loss: 3.013 | Reg loss: 0.033 | Tree loss: 3.013 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 027 | Total loss: 2.942 | Reg loss: 0.033 | Tree loss: 2.942 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 027 | Total loss: 2.959 | Reg loss: 0.033 | Tree loss: 2.959 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 027 | Total loss: 2.987 | Reg loss: 0.033 | Tree loss: 2.987 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 027 | Total loss: 2.966 | Reg loss: 0.033 | Tree loss: 2.966 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 027 | Total loss: 2.960 | Reg loss: 0.033 | Tree loss: 2.960 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 027 | Total loss: 2.975 | Reg loss: 0.033 | Tree loss: 2.975 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 027 | Total loss: 2.924 | Reg loss: 0.033 | Tree loss: 2.924 | Accuracy: 0.103516 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 027 | Total loss: 2.942 | Reg loss: 0.034 | Tree loss: 2.942 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 027 | Total loss: 2.910 | Reg loss: 0.034 | Tree loss: 2.910 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 027 | Total loss: 2.920 | Reg loss: 0.034 | Tree loss: 2.920 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 027 | Total loss: 2.958 | Reg loss: 0.034 | Tree loss: 2.958 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 027 | Total loss: 2.934 | Reg loss: 0.034 | Tree loss: 2.934 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 027 | Total loss: 2.897 | Reg loss: 0.034 | Tree loss: 2.897 | Accuracy: 0.146484 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 027 | Total loss: 2.935 | Reg loss: 0.034 | Tree loss: 2.935 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 027 | Total loss: 2.830 | Reg loss: 0.034 | Tree loss: 2.830 | Accuracy: 0.166667 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 80 | Batch: 000 / 027 | Total loss: 3.080 | Reg loss: 0.033 | Tree loss: 3.080 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 027 | Total loss: 3.074 | Reg loss: 0.033 | Tree loss: 3.074 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 027 | Total loss: 3.031 | Reg loss: 0.033 | Tree loss: 3.031 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 027 | Total loss: 3.084 | Reg loss: 0.033 | Tree loss: 3.084 | Accuracy: 0.076172 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 027 | Total loss: 3.032 | Reg loss: 0.033 | Tree loss: 3.032 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 027 | Total loss: 3.117 | Reg loss: 0.033 | Tree loss: 3.117 | Accuracy: 0.087891 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 027 | Total loss: 2.989 | Reg loss: 0.033 | Tree loss: 2.989 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 027 | Total loss: 2.997 | Reg loss: 0.033 | Tree loss: 2.997 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 027 | Total loss: 3.044 | Reg loss: 0.033 | Tree loss: 3.044 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 027 | Total loss: 2.999 | Reg loss: 0.033 | Tree loss: 2.999 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 027 | Total loss: 2.984 | Reg loss: 0.033 | Tree loss: 2.984 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 027 | Total loss: 2.980 | Reg loss: 0.033 | Tree loss: 2.980 | Accuracy: 0.126953 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 027 | Total loss: 3.037 | Reg loss: 0.033 | Tree loss: 3.037 | Accuracy: 0.093750 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 027 | Total loss: 2.970 | Reg loss: 0.033 | Tree loss: 2.970 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 027 | Total loss: 2.993 | Reg loss: 0.033 | Tree loss: 2.993 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 027 | Total loss: 2.953 | Reg loss: 0.033 | Tree loss: 2.953 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 027 | Total loss: 2.904 | Reg loss: 0.033 | Tree loss: 2.904 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 027 | Total loss: 2.969 | Reg loss: 0.033 | Tree loss: 2.969 | Accuracy: 0.115234 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 027 | Total loss: 2.963 | Reg loss: 0.033 | Tree loss: 2.963 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 027 | Total loss: 2.907 | Reg loss: 0.033 | Tree loss: 2.907 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 027 | Total loss: 2.989 | Reg loss: 0.033 | Tree loss: 2.989 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 027 | Total loss: 2.952 | Reg loss: 0.034 | Tree loss: 2.952 | Accuracy: 0.091797 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 027 | Total loss: 2.874 | Reg loss: 0.034 | Tree loss: 2.874 | Accuracy: 0.125000 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 027 | Total loss: 2.954 | Reg loss: 0.034 | Tree loss: 2.954 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 027 | Total loss: 2.871 | Reg loss: 0.034 | Tree loss: 2.871 | Accuracy: 0.103516 | 0.353 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 025 / 027 | Total loss: 2.906 | Reg loss: 0.034 | Tree loss: 2.906 | Accuracy: 0.099609 | 0.353 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 027 | Total loss: 3.005 | Reg loss: 0.034 | Tree loss: 3.005 | Accuracy: 0.083333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 81 | Batch: 000 / 027 | Total loss: 3.091 | Reg loss: 0.033 | Tree loss: 3.091 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 027 | Total loss: 3.077 | Reg loss: 0.033 | Tree loss: 3.077 | Accuracy: 0.125000 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 027 | Total loss: 3.073 | Reg loss: 0.033 | Tree loss: 3.073 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 027 | Total loss: 3.132 | Reg loss: 0.033 | Tree loss: 3.132 | Accuracy: 0.121094 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 027 | Total loss: 3.011 | Reg loss: 0.033 | Tree loss: 3.011 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 027 | Total loss: 3.049 | Reg loss: 0.033 | Tree loss: 3.049 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 027 | Total loss: 3.022 | Reg loss: 0.033 | Tree loss: 3.022 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 027 | Total loss: 3.069 | Reg loss: 0.033 | Tree loss: 3.069 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 027 | Total loss: 3.014 | Reg loss: 0.033 | Tree loss: 3.014 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 027 | Total loss: 2.984 | Reg loss: 0.033 | Tree loss: 2.984 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 027 | Total loss: 2.968 | Reg loss: 0.033 | Tree loss: 2.968 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 027 | Total loss: 2.997 | Reg loss: 0.033 | Tree loss: 2.997 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 027 | Total loss: 2.945 | Reg loss: 0.033 | Tree loss: 2.945 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 027 | Total loss: 2.953 | Reg loss: 0.033 | Tree loss: 2.953 | Accuracy: 0.076172 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 027 | Total loss: 3.002 | Reg loss: 0.033 | Tree loss: 3.002 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 027 | Total loss: 2.960 | Reg loss: 0.033 | Tree loss: 2.960 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 027 | Total loss: 2.930 | Reg loss: 0.033 | Tree loss: 2.930 | Accuracy: 0.117188 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 027 | Total loss: 2.936 | Reg loss: 0.033 | Tree loss: 2.936 | Accuracy: 0.097656 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 027 | Total loss: 2.949 | Reg loss: 0.033 | Tree loss: 2.949 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 027 | Total loss: 2.937 | Reg loss: 0.033 | Tree loss: 2.937 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 027 | Total loss: 2.904 | Reg loss: 0.033 | Tree loss: 2.904 | Accuracy: 0.113281 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 027 | Total loss: 2.916 | Reg loss: 0.033 | Tree loss: 2.916 | Accuracy: 0.089844 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 027 | Total loss: 2.917 | Reg loss: 0.033 | Tree loss: 2.917 | Accuracy: 0.085938 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 027 | Total loss: 2.913 | Reg loss: 0.034 | Tree loss: 2.913 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 027 | Total loss: 2.931 | Reg loss: 0.034 | Tree loss: 2.931 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 027 | Total loss: 2.898 | Reg loss: 0.034 | Tree loss: 2.898 | Accuracy: 0.125000 | 0.353 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 027 | Total loss: 2.811 | Reg loss: 0.034 | Tree loss: 2.811 | Accuracy: 0.000000 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 82 | Batch: 000 / 027 | Total loss: 3.038 | Reg loss: 0.033 | Tree loss: 3.038 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 027 | Total loss: 3.094 | Reg loss: 0.033 | Tree loss: 3.094 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 027 | Total loss: 3.024 | Reg loss: 0.033 | Tree loss: 3.024 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 027 | Total loss: 3.042 | Reg loss: 0.033 | Tree loss: 3.042 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 027 | Total loss: 3.072 | Reg loss: 0.033 | Tree loss: 3.072 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 027 | Total loss: 3.005 | Reg loss: 0.033 | Tree loss: 3.005 | Accuracy: 0.111328 | 0.353 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 027 | Total loss: 3.063 | Reg loss: 0.033 | Tree loss: 3.063 | Accuracy: 0.121094 | 0.353 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 027 | Total loss: 2.981 | Reg loss: 0.033 | Tree loss: 2.981 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 027 | Total loss: 3.023 | Reg loss: 0.033 | Tree loss: 3.023 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 027 | Total loss: 3.019 | Reg loss: 0.033 | Tree loss: 3.019 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 027 | Total loss: 3.017 | Reg loss: 0.033 | Tree loss: 3.017 | Accuracy: 0.076172 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 027 | Total loss: 2.991 | Reg loss: 0.033 | Tree loss: 2.991 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 027 | Total loss: 2.984 | Reg loss: 0.033 | Tree loss: 2.984 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 027 | Total loss: 2.984 | Reg loss: 0.033 | Tree loss: 2.984 | Accuracy: 0.128906 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 027 | Total loss: 2.963 | Reg loss: 0.033 | Tree loss: 2.963 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 027 | Total loss: 2.945 | Reg loss: 0.033 | Tree loss: 2.945 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 027 | Total loss: 2.962 | Reg loss: 0.033 | Tree loss: 2.962 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 027 | Total loss: 2.959 | Reg loss: 0.033 | Tree loss: 2.959 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 027 | Total loss: 2.954 | Reg loss: 0.033 | Tree loss: 2.954 | Accuracy: 0.066406 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 027 | Total loss: 2.941 | Reg loss: 0.033 | Tree loss: 2.941 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 027 | Total loss: 2.936 | Reg loss: 0.033 | Tree loss: 2.936 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 027 | Total loss: 2.877 | Reg loss: 0.033 | Tree loss: 2.877 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 027 | Total loss: 2.913 | Reg loss: 0.033 | Tree loss: 2.913 | Accuracy: 0.121094 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 027 | Total loss: 2.880 | Reg loss: 0.033 | Tree loss: 2.880 | Accuracy: 0.074219 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 027 | Total loss: 2.923 | Reg loss: 0.034 | Tree loss: 2.923 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 027 | Total loss: 2.847 | Reg loss: 0.034 | Tree loss: 2.847 | Accuracy: 0.000000 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 83 | Batch: 000 / 027 | Total loss: 3.084 | Reg loss: 0.033 | Tree loss: 3.084 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 027 | Total loss: 3.110 | Reg loss: 0.033 | Tree loss: 3.110 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 027 | Total loss: 3.055 | Reg loss: 0.033 | Tree loss: 3.055 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 027 | Total loss: 3.041 | Reg loss: 0.033 | Tree loss: 3.041 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 027 | Total loss: 3.058 | Reg loss: 0.033 | Tree loss: 3.058 | Accuracy: 0.087891 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 005 / 027 | Total loss: 3.091 | Reg loss: 0.033 | Tree loss: 3.091 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 027 | Total loss: 2.982 | Reg loss: 0.033 | Tree loss: 2.982 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 027 | Total loss: 3.001 | Reg loss: 0.033 | Tree loss: 3.001 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 027 | Total loss: 3.057 | Reg loss: 0.033 | Tree loss: 3.057 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 027 | Total loss: 3.044 | Reg loss: 0.033 | Tree loss: 3.044 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 027 | Total loss: 2.976 | Reg loss: 0.033 | Tree loss: 2.976 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 027 | Total loss: 2.969 | Reg loss: 0.033 | Tree loss: 2.969 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 027 | Total loss: 2.967 | Reg loss: 0.033 | Tree loss: 2.967 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 027 | Total loss: 2.952 | Reg loss: 0.033 | Tree loss: 2.952 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 027 | Total loss: 2.998 | Reg loss: 0.033 | Tree loss: 2.998 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 027 | Total loss: 2.950 | Reg loss: 0.033 | Tree loss: 2.950 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 027 | Total loss: 2.964 | Reg loss: 0.033 | Tree loss: 2.964 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 027 | Total loss: 2.939 | Reg loss: 0.033 | Tree loss: 2.939 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 027 | Total loss: 2.917 | Reg loss: 0.033 | Tree loss: 2.917 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 027 | Total loss: 2.922 | Reg loss: 0.033 | Tree loss: 2.922 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 027 | Total loss: 2.922 | Reg loss: 0.033 | Tree loss: 2.922 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 027 | Total loss: 2.902 | Reg loss: 0.033 | Tree loss: 2.902 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 027 | Total loss: 2.890 | Reg loss: 0.033 | Tree loss: 2.890 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 027 | Total loss: 2.903 | Reg loss: 0.033 | Tree loss: 2.903 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 027 | Total loss: 2.899 | Reg loss: 0.033 | Tree loss: 2.899 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 027 | Total loss: 2.876 | Reg loss: 0.033 | Tree loss: 2.876 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 027 | Total loss: 2.792 | Reg loss: 0.033 | Tree loss: 2.792 | Accuracy: 0.333333 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 84 | Batch: 000 / 027 | Total loss: 3.120 | Reg loss: 0.033 | Tree loss: 3.120 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 027 | Total loss: 3.070 | Reg loss: 0.033 | Tree loss: 3.070 | Accuracy: 0.072266 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 027 | Total loss: 3.043 | Reg loss: 0.033 | Tree loss: 3.043 | Accuracy: 0.068359 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 027 | Total loss: 3.074 | Reg loss: 0.033 | Tree loss: 3.074 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 027 | Total loss: 3.052 | Reg loss: 0.033 | Tree loss: 3.052 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 027 | Total loss: 3.026 | Reg loss: 0.033 | Tree loss: 3.026 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 027 | Total loss: 2.996 | Reg loss: 0.033 | Tree loss: 2.996 | Accuracy: 0.125000 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 027 | Total loss: 3.037 | Reg loss: 0.033 | Tree loss: 3.037 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 027 | Total loss: 3.022 | Reg loss: 0.033 | Tree loss: 3.022 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 027 | Total loss: 2.984 | Reg loss: 0.033 | Tree loss: 2.984 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 027 | Total loss: 2.994 | Reg loss: 0.033 | Tree loss: 2.994 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 027 | Total loss: 2.949 | Reg loss: 0.033 | Tree loss: 2.949 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 027 | Total loss: 2.973 | Reg loss: 0.033 | Tree loss: 2.973 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 027 | Total loss: 2.923 | Reg loss: 0.033 | Tree loss: 2.923 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 027 | Total loss: 2.956 | Reg loss: 0.033 | Tree loss: 2.956 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 027 | Total loss: 2.970 | Reg loss: 0.033 | Tree loss: 2.970 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 027 | Total loss: 2.976 | Reg loss: 0.033 | Tree loss: 2.976 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 027 | Total loss: 2.922 | Reg loss: 0.033 | Tree loss: 2.922 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 027 | Total loss: 2.927 | Reg loss: 0.033 | Tree loss: 2.927 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 027 | Total loss: 2.887 | Reg loss: 0.033 | Tree loss: 2.887 | Accuracy: 0.095703 | 0.353 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 027 | Total loss: 2.898 | Reg loss: 0.033 | Tree loss: 2.898 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 027 | Total loss: 2.906 | Reg loss: 0.033 | Tree loss: 2.906 | Accuracy: 0.101562 | 0.353 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 027 | Total loss: 2.951 | Reg loss: 0.033 | Tree loss: 2.951 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 027 | Total loss: 2.938 | Reg loss: 0.033 | Tree loss: 2.938 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 027 | Total loss: 2.900 | Reg loss: 0.033 | Tree loss: 2.900 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 027 | Total loss: 2.887 | Reg loss: 0.033 | Tree loss: 2.887 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 027 | Total loss: 2.682 | Reg loss: 0.033 | Tree loss: 2.682 | Accuracy: 0.083333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 85 | Batch: 000 / 027 | Total loss: 3.051 | Reg loss: 0.033 | Tree loss: 3.051 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 027 | Total loss: 3.048 | Reg loss: 0.033 | Tree loss: 3.048 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 027 | Total loss: 3.043 | Reg loss: 0.033 | Tree loss: 3.043 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 027 | Total loss: 3.056 | Reg loss: 0.033 | Tree loss: 3.056 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 027 | Total loss: 3.046 | Reg loss: 0.033 | Tree loss: 3.046 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 027 | Total loss: 3.039 | Reg loss: 0.033 | Tree loss: 3.039 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 027 | Total loss: 3.052 | Reg loss: 0.033 | Tree loss: 3.052 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 027 | Total loss: 2.969 | Reg loss: 0.033 | Tree loss: 2.969 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 027 | Total loss: 3.028 | Reg loss: 0.033 | Tree loss: 3.028 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 027 | Total loss: 3.047 | Reg loss: 0.033 | Tree loss: 3.047 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 027 | Total loss: 2.929 | Reg loss: 0.033 | Tree loss: 2.929 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 027 | Total loss: 2.979 | Reg loss: 0.033 | Tree loss: 2.979 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 027 | Total loss: 2.989 | Reg loss: 0.033 | Tree loss: 2.989 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 027 | Total loss: 2.967 | Reg loss: 0.033 | Tree loss: 2.967 | Accuracy: 0.119141 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 014 / 027 | Total loss: 2.962 | Reg loss: 0.033 | Tree loss: 2.962 | Accuracy: 0.125000 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 027 | Total loss: 2.919 | Reg loss: 0.033 | Tree loss: 2.919 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 027 | Total loss: 2.945 | Reg loss: 0.033 | Tree loss: 2.945 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 027 | Total loss: 2.908 | Reg loss: 0.033 | Tree loss: 2.908 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 027 | Total loss: 2.888 | Reg loss: 0.033 | Tree loss: 2.888 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 027 | Total loss: 2.986 | Reg loss: 0.033 | Tree loss: 2.986 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 027 | Total loss: 2.926 | Reg loss: 0.033 | Tree loss: 2.926 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 027 | Total loss: 2.928 | Reg loss: 0.033 | Tree loss: 2.928 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 027 | Total loss: 2.909 | Reg loss: 0.033 | Tree loss: 2.909 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 027 | Total loss: 2.895 | Reg loss: 0.033 | Tree loss: 2.895 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 027 | Total loss: 2.913 | Reg loss: 0.033 | Tree loss: 2.913 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 027 | Total loss: 2.936 | Reg loss: 0.033 | Tree loss: 2.936 | Accuracy: 0.078125 | 0.354 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 027 | Total loss: 2.887 | Reg loss: 0.033 | Tree loss: 2.887 | Accuracy: 0.166667 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 86 | Batch: 000 / 027 | Total loss: 3.129 | Reg loss: 0.033 | Tree loss: 3.129 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 027 | Total loss: 3.027 | Reg loss: 0.033 | Tree loss: 3.027 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 027 | Total loss: 3.037 | Reg loss: 0.033 | Tree loss: 3.037 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 027 | Total loss: 3.063 | Reg loss: 0.033 | Tree loss: 3.063 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 027 | Total loss: 3.034 | Reg loss: 0.033 | Tree loss: 3.034 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 027 | Total loss: 3.043 | Reg loss: 0.033 | Tree loss: 3.043 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 027 | Total loss: 2.990 | Reg loss: 0.033 | Tree loss: 2.990 | Accuracy: 0.136719 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 027 | Total loss: 3.034 | Reg loss: 0.033 | Tree loss: 3.034 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 027 | Total loss: 3.020 | Reg loss: 0.033 | Tree loss: 3.020 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 027 | Total loss: 3.005 | Reg loss: 0.033 | Tree loss: 3.005 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 027 | Total loss: 2.993 | Reg loss: 0.033 | Tree loss: 2.993 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 027 | Total loss: 2.994 | Reg loss: 0.033 | Tree loss: 2.994 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 027 | Total loss: 2.981 | Reg loss: 0.033 | Tree loss: 2.981 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 027 | Total loss: 2.995 | Reg loss: 0.033 | Tree loss: 2.995 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 027 | Total loss: 3.005 | Reg loss: 0.033 | Tree loss: 3.005 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 027 | Total loss: 2.949 | Reg loss: 0.033 | Tree loss: 2.949 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 027 | Total loss: 2.921 | Reg loss: 0.033 | Tree loss: 2.921 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 027 | Total loss: 2.865 | Reg loss: 0.033 | Tree loss: 2.865 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 027 | Total loss: 2.896 | Reg loss: 0.033 | Tree loss: 2.896 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 027 | Total loss: 2.901 | Reg loss: 0.033 | Tree loss: 2.901 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 027 | Total loss: 2.930 | Reg loss: 0.033 | Tree loss: 2.930 | Accuracy: 0.121094 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 027 | Total loss: 2.890 | Reg loss: 0.033 | Tree loss: 2.890 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 027 | Total loss: 2.919 | Reg loss: 0.033 | Tree loss: 2.919 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 027 | Total loss: 2.898 | Reg loss: 0.033 | Tree loss: 2.898 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 027 | Total loss: 2.877 | Reg loss: 0.033 | Tree loss: 2.877 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 027 | Total loss: 3.059 | Reg loss: 0.033 | Tree loss: 3.059 | Accuracy: 0.083333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 87 | Batch: 000 / 027 | Total loss: 3.060 | Reg loss: 0.033 | Tree loss: 3.060 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 027 | Total loss: 3.069 | Reg loss: 0.033 | Tree loss: 3.069 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 027 | Total loss: 3.043 | Reg loss: 0.033 | Tree loss: 3.043 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 027 | Total loss: 3.093 | Reg loss: 0.033 | Tree loss: 3.093 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 027 | Total loss: 3.039 | Reg loss: 0.033 | Tree loss: 3.039 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 027 | Total loss: 3.047 | Reg loss: 0.033 | Tree loss: 3.047 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 027 | Total loss: 3.021 | Reg loss: 0.033 | Tree loss: 3.021 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 027 | Total loss: 3.014 | Reg loss: 0.033 | Tree loss: 3.014 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 027 | Total loss: 3.024 | Reg loss: 0.033 | Tree loss: 3.024 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 027 | Total loss: 2.961 | Reg loss: 0.033 | Tree loss: 2.961 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 027 | Total loss: 2.939 | Reg loss: 0.033 | Tree loss: 2.939 | Accuracy: 0.125000 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 027 | Total loss: 3.015 | Reg loss: 0.033 | Tree loss: 3.015 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 027 | Total loss: 2.951 | Reg loss: 0.033 | Tree loss: 2.951 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 027 | Total loss: 2.956 | Reg loss: 0.033 | Tree loss: 2.956 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 027 | Total loss: 2.981 | Reg loss: 0.033 | Tree loss: 2.981 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 027 | Total loss: 2.914 | Reg loss: 0.033 | Tree loss: 2.914 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 027 | Total loss: 2.923 | Reg loss: 0.033 | Tree loss: 2.923 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 027 | Total loss: 2.940 | Reg loss: 0.033 | Tree loss: 2.940 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 027 | Total loss: 2.927 | Reg loss: 0.033 | Tree loss: 2.927 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.134766 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 027 | Total loss: 2.943 | Reg loss: 0.033 | Tree loss: 2.943 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 027 | Total loss: 2.908 | Reg loss: 0.033 | Tree loss: 2.908 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 027 | Total loss: 2.906 | Reg loss: 0.033 | Tree loss: 2.906 | Accuracy: 0.091797 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 023 / 027 | Total loss: 2.888 | Reg loss: 0.033 | Tree loss: 2.888 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 027 | Total loss: 2.879 | Reg loss: 0.033 | Tree loss: 2.879 | Accuracy: 0.138672 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 027 | Total loss: 2.768 | Reg loss: 0.033 | Tree loss: 2.768 | Accuracy: 0.000000 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 88 | Batch: 000 / 027 | Total loss: 3.080 | Reg loss: 0.033 | Tree loss: 3.080 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 027 | Total loss: 3.061 | Reg loss: 0.033 | Tree loss: 3.061 | Accuracy: 0.074219 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 027 | Total loss: 3.053 | Reg loss: 0.033 | Tree loss: 3.053 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 027 | Total loss: 3.045 | Reg loss: 0.033 | Tree loss: 3.045 | Accuracy: 0.080078 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 027 | Total loss: 3.078 | Reg loss: 0.033 | Tree loss: 3.078 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 027 | Total loss: 3.021 | Reg loss: 0.033 | Tree loss: 3.021 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 027 | Total loss: 2.997 | Reg loss: 0.033 | Tree loss: 2.997 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 027 | Total loss: 3.014 | Reg loss: 0.033 | Tree loss: 3.014 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 027 | Total loss: 3.020 | Reg loss: 0.033 | Tree loss: 3.020 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 027 | Total loss: 2.973 | Reg loss: 0.033 | Tree loss: 2.973 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 027 | Total loss: 2.979 | Reg loss: 0.033 | Tree loss: 2.979 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 027 | Total loss: 2.963 | Reg loss: 0.033 | Tree loss: 2.963 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 027 | Total loss: 2.945 | Reg loss: 0.033 | Tree loss: 2.945 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 027 | Total loss: 2.995 | Reg loss: 0.033 | Tree loss: 2.995 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 027 | Total loss: 2.887 | Reg loss: 0.033 | Tree loss: 2.887 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 027 | Total loss: 2.965 | Reg loss: 0.033 | Tree loss: 2.965 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 027 | Total loss: 2.929 | Reg loss: 0.033 | Tree loss: 2.929 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 027 | Total loss: 2.923 | Reg loss: 0.033 | Tree loss: 2.923 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 027 | Total loss: 2.885 | Reg loss: 0.033 | Tree loss: 2.885 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 027 | Total loss: 2.930 | Reg loss: 0.033 | Tree loss: 2.930 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 027 | Total loss: 2.950 | Reg loss: 0.033 | Tree loss: 2.950 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 027 | Total loss: 2.876 | Reg loss: 0.033 | Tree loss: 2.876 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 027 | Total loss: 2.918 | Reg loss: 0.033 | Tree loss: 2.918 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 027 | Total loss: 2.857 | Reg loss: 0.033 | Tree loss: 2.857 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 027 | Total loss: 2.943 | Reg loss: 0.033 | Tree loss: 2.943 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.083333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 89 | Batch: 000 / 027 | Total loss: 3.121 | Reg loss: 0.033 | Tree loss: 3.121 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 027 | Total loss: 3.098 | Reg loss: 0.033 | Tree loss: 3.098 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 027 | Total loss: 3.043 | Reg loss: 0.033 | Tree loss: 3.043 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 027 | Total loss: 3.041 | Reg loss: 0.033 | Tree loss: 3.041 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 027 | Total loss: 3.044 | Reg loss: 0.033 | Tree loss: 3.044 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 027 | Total loss: 2.970 | Reg loss: 0.033 | Tree loss: 2.970 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 027 | Total loss: 3.055 | Reg loss: 0.033 | Tree loss: 3.055 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 027 | Total loss: 2.995 | Reg loss: 0.033 | Tree loss: 2.995 | Accuracy: 0.136719 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 027 | Total loss: 2.986 | Reg loss: 0.033 | Tree loss: 2.986 | Accuracy: 0.126953 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 027 | Total loss: 3.038 | Reg loss: 0.033 | Tree loss: 3.038 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 027 | Total loss: 2.953 | Reg loss: 0.033 | Tree loss: 2.953 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 027 | Total loss: 2.902 | Reg loss: 0.033 | Tree loss: 2.902 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 027 | Total loss: 3.021 | Reg loss: 0.033 | Tree loss: 3.021 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 027 | Total loss: 2.913 | Reg loss: 0.033 | Tree loss: 2.913 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 027 | Total loss: 2.964 | Reg loss: 0.033 | Tree loss: 2.964 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 027 | Total loss: 2.910 | Reg loss: 0.033 | Tree loss: 2.910 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 027 | Total loss: 2.904 | Reg loss: 0.033 | Tree loss: 2.904 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 027 | Total loss: 2.920 | Reg loss: 0.033 | Tree loss: 2.920 | Accuracy: 0.128906 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 027 | Total loss: 2.910 | Reg loss: 0.033 | Tree loss: 2.910 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 027 | Total loss: 2.912 | Reg loss: 0.033 | Tree loss: 2.912 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 027 | Total loss: 2.938 | Reg loss: 0.033 | Tree loss: 2.938 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 027 | Total loss: 2.939 | Reg loss: 0.033 | Tree loss: 2.939 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 027 | Total loss: 2.900 | Reg loss: 0.033 | Tree loss: 2.900 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 027 | Total loss: 2.913 | Reg loss: 0.033 | Tree loss: 2.913 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 027 | Total loss: 2.874 | Reg loss: 0.033 | Tree loss: 2.874 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 027 | Total loss: 3.078 | Reg loss: 0.033 | Tree loss: 3.078 | Accuracy: 0.000000 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 90 | Batch: 000 / 027 | Total loss: 3.089 | Reg loss: 0.033 | Tree loss: 3.089 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 027 | Total loss: 3.074 | Reg loss: 0.033 | Tree loss: 3.074 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 027 | Total loss: 3.062 | Reg loss: 0.033 | Tree loss: 3.062 | Accuracy: 0.103516 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 003 / 027 | Total loss: 3.029 | Reg loss: 0.033 | Tree loss: 3.029 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 027 | Total loss: 3.026 | Reg loss: 0.033 | Tree loss: 3.026 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 027 | Total loss: 3.037 | Reg loss: 0.033 | Tree loss: 3.037 | Accuracy: 0.126953 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 027 | Total loss: 2.983 | Reg loss: 0.033 | Tree loss: 2.983 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 027 | Total loss: 2.987 | Reg loss: 0.033 | Tree loss: 2.987 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 027 | Total loss: 3.000 | Reg loss: 0.033 | Tree loss: 3.000 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 027 | Total loss: 3.001 | Reg loss: 0.033 | Tree loss: 3.001 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 027 | Total loss: 2.973 | Reg loss: 0.033 | Tree loss: 2.973 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 027 | Total loss: 2.962 | Reg loss: 0.033 | Tree loss: 2.962 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 027 | Total loss: 2.933 | Reg loss: 0.033 | Tree loss: 2.933 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 027 | Total loss: 2.950 | Reg loss: 0.033 | Tree loss: 2.950 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 027 | Total loss: 2.954 | Reg loss: 0.033 | Tree loss: 2.954 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 027 | Total loss: 2.899 | Reg loss: 0.033 | Tree loss: 2.899 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 027 | Total loss: 2.928 | Reg loss: 0.033 | Tree loss: 2.928 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 027 | Total loss: 2.969 | Reg loss: 0.033 | Tree loss: 2.969 | Accuracy: 0.068359 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 027 | Total loss: 2.936 | Reg loss: 0.033 | Tree loss: 2.936 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 027 | Total loss: 2.889 | Reg loss: 0.033 | Tree loss: 2.889 | Accuracy: 0.121094 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 027 | Total loss: 2.956 | Reg loss: 0.033 | Tree loss: 2.956 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 027 | Total loss: 2.924 | Reg loss: 0.033 | Tree loss: 2.924 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 027 | Total loss: 2.887 | Reg loss: 0.033 | Tree loss: 2.887 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 027 | Total loss: 2.907 | Reg loss: 0.033 | Tree loss: 2.907 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 027 | Total loss: 2.845 | Reg loss: 0.033 | Tree loss: 2.845 | Accuracy: 0.080078 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 027 | Total loss: 2.886 | Reg loss: 0.033 | Tree loss: 2.886 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 027 | Total loss: 2.950 | Reg loss: 0.033 | Tree loss: 2.950 | Accuracy: 0.083333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 91 | Batch: 000 / 027 | Total loss: 3.087 | Reg loss: 0.032 | Tree loss: 3.087 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 027 | Total loss: 3.079 | Reg loss: 0.032 | Tree loss: 3.079 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 027 | Total loss: 3.056 | Reg loss: 0.032 | Tree loss: 3.056 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 027 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 027 | Total loss: 3.067 | Reg loss: 0.032 | Tree loss: 3.067 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 027 | Total loss: 3.049 | Reg loss: 0.032 | Tree loss: 3.049 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 027 | Total loss: 3.031 | Reg loss: 0.032 | Tree loss: 3.031 | Accuracy: 0.076172 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 027 | Total loss: 2.968 | Reg loss: 0.033 | Tree loss: 2.968 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 027 | Total loss: 2.979 | Reg loss: 0.033 | Tree loss: 2.979 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 027 | Total loss: 2.996 | Reg loss: 0.033 | Tree loss: 2.996 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 027 | Total loss: 3.057 | Reg loss: 0.033 | Tree loss: 3.057 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 027 | Total loss: 2.930 | Reg loss: 0.033 | Tree loss: 2.930 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 027 | Total loss: 2.955 | Reg loss: 0.033 | Tree loss: 2.955 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 027 | Total loss: 2.998 | Reg loss: 0.033 | Tree loss: 2.998 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 027 | Total loss: 2.955 | Reg loss: 0.033 | Tree loss: 2.955 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 027 | Total loss: 2.920 | Reg loss: 0.033 | Tree loss: 2.920 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 027 | Total loss: 2.943 | Reg loss: 0.033 | Tree loss: 2.943 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 027 | Total loss: 2.941 | Reg loss: 0.033 | Tree loss: 2.941 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 027 | Total loss: 2.901 | Reg loss: 0.033 | Tree loss: 2.901 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 027 | Total loss: 2.876 | Reg loss: 0.033 | Tree loss: 2.876 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 027 | Total loss: 2.900 | Reg loss: 0.033 | Tree loss: 2.900 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 027 | Total loss: 2.877 | Reg loss: 0.033 | Tree loss: 2.877 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 027 | Total loss: 2.850 | Reg loss: 0.033 | Tree loss: 2.850 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 027 | Total loss: 2.865 | Reg loss: 0.033 | Tree loss: 2.865 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 027 | Total loss: 2.873 | Reg loss: 0.033 | Tree loss: 2.873 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 027 | Total loss: 2.892 | Reg loss: 0.033 | Tree loss: 2.892 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 027 | Total loss: 2.822 | Reg loss: 0.033 | Tree loss: 2.822 | Accuracy: 0.083333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 92 | Batch: 000 / 027 | Total loss: 3.092 | Reg loss: 0.032 | Tree loss: 3.092 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 027 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 027 | Total loss: 3.042 | Reg loss: 0.032 | Tree loss: 3.042 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 027 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 027 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 027 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.078125 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 027 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 027 | Total loss: 2.982 | Reg loss: 0.032 | Tree loss: 2.982 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 027 | Total loss: 3.018 | Reg loss: 0.032 | Tree loss: 3.018 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 027 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 027 | Total loss: 3.001 | Reg loss: 0.033 | Tree loss: 3.001 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 027 | Total loss: 3.003 | Reg loss: 0.033 | Tree loss: 3.003 | Accuracy: 0.111328 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 012 / 027 | Total loss: 2.961 | Reg loss: 0.033 | Tree loss: 2.961 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 027 | Total loss: 2.939 | Reg loss: 0.033 | Tree loss: 2.939 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 027 | Total loss: 2.882 | Reg loss: 0.033 | Tree loss: 2.882 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 027 | Total loss: 2.927 | Reg loss: 0.033 | Tree loss: 2.927 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 027 | Total loss: 2.908 | Reg loss: 0.033 | Tree loss: 2.908 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 027 | Total loss: 2.956 | Reg loss: 0.033 | Tree loss: 2.956 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 027 | Total loss: 2.900 | Reg loss: 0.033 | Tree loss: 2.900 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 027 | Total loss: 2.902 | Reg loss: 0.033 | Tree loss: 2.902 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 027 | Total loss: 2.901 | Reg loss: 0.033 | Tree loss: 2.901 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 027 | Total loss: 2.911 | Reg loss: 0.033 | Tree loss: 2.911 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 027 | Total loss: 2.913 | Reg loss: 0.033 | Tree loss: 2.913 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 027 | Total loss: 2.826 | Reg loss: 0.033 | Tree loss: 2.826 | Accuracy: 0.126953 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 027 | Total loss: 2.874 | Reg loss: 0.033 | Tree loss: 2.874 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 027 | Total loss: 2.910 | Reg loss: 0.033 | Tree loss: 2.910 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 027 | Total loss: 2.946 | Reg loss: 0.033 | Tree loss: 2.946 | Accuracy: 0.083333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 93 | Batch: 000 / 027 | Total loss: 3.068 | Reg loss: 0.032 | Tree loss: 3.068 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 027 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 027 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 027 | Total loss: 3.049 | Reg loss: 0.032 | Tree loss: 3.049 | Accuracy: 0.080078 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 027 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 027 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 027 | Total loss: 3.005 | Reg loss: 0.032 | Tree loss: 3.005 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 027 | Total loss: 2.981 | Reg loss: 0.032 | Tree loss: 2.981 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 027 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 027 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 027 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 027 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.068359 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 027 | Total loss: 2.968 | Reg loss: 0.033 | Tree loss: 2.968 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 027 | Total loss: 2.935 | Reg loss: 0.033 | Tree loss: 2.935 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 027 | Total loss: 2.911 | Reg loss: 0.033 | Tree loss: 2.911 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 027 | Total loss: 2.927 | Reg loss: 0.033 | Tree loss: 2.927 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 027 | Total loss: 2.929 | Reg loss: 0.033 | Tree loss: 2.929 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 027 | Total loss: 2.944 | Reg loss: 0.033 | Tree loss: 2.944 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 027 | Total loss: 2.920 | Reg loss: 0.033 | Tree loss: 2.920 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 027 | Total loss: 2.940 | Reg loss: 0.033 | Tree loss: 2.940 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 027 | Total loss: 2.885 | Reg loss: 0.033 | Tree loss: 2.885 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 027 | Total loss: 2.879 | Reg loss: 0.033 | Tree loss: 2.879 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 027 | Total loss: 2.866 | Reg loss: 0.033 | Tree loss: 2.866 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 027 | Total loss: 2.874 | Reg loss: 0.033 | Tree loss: 2.874 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 027 | Total loss: 2.844 | Reg loss: 0.033 | Tree loss: 2.844 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 027 | Total loss: 2.894 | Reg loss: 0.033 | Tree loss: 2.894 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 027 | Total loss: 2.873 | Reg loss: 0.033 | Tree loss: 2.873 | Accuracy: 0.000000 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 94 | Batch: 000 / 027 | Total loss: 3.043 | Reg loss: 0.032 | Tree loss: 3.043 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 027 | Total loss: 3.062 | Reg loss: 0.032 | Tree loss: 3.062 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 027 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 027 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 027 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.130859 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 027 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 027 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 027 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 027 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 027 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.078125 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 027 | Total loss: 3.011 | Reg loss: 0.032 | Tree loss: 3.011 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 027 | Total loss: 2.986 | Reg loss: 0.032 | Tree loss: 2.986 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 027 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 027 | Total loss: 2.986 | Reg loss: 0.032 | Tree loss: 2.986 | Accuracy: 0.068359 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 027 | Total loss: 2.932 | Reg loss: 0.033 | Tree loss: 2.932 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 027 | Total loss: 2.926 | Reg loss: 0.033 | Tree loss: 2.926 | Accuracy: 0.074219 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 027 | Total loss: 2.937 | Reg loss: 0.033 | Tree loss: 2.937 | Accuracy: 0.080078 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 027 | Total loss: 2.932 | Reg loss: 0.033 | Tree loss: 2.932 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 027 | Total loss: 2.893 | Reg loss: 0.033 | Tree loss: 2.893 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 027 | Total loss: 2.911 | Reg loss: 0.033 | Tree loss: 2.911 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 027 | Total loss: 2.935 | Reg loss: 0.033 | Tree loss: 2.935 | Accuracy: 0.095703 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 021 / 027 | Total loss: 2.906 | Reg loss: 0.033 | Tree loss: 2.906 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 027 | Total loss: 2.881 | Reg loss: 0.033 | Tree loss: 2.881 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 027 | Total loss: 2.867 | Reg loss: 0.033 | Tree loss: 2.867 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 027 | Total loss: 2.889 | Reg loss: 0.033 | Tree loss: 2.889 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 027 | Total loss: 2.862 | Reg loss: 0.033 | Tree loss: 2.862 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 027 | Total loss: 2.712 | Reg loss: 0.033 | Tree loss: 2.712 | Accuracy: 0.166667 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 95 | Batch: 000 / 027 | Total loss: 3.055 | Reg loss: 0.032 | Tree loss: 3.055 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 027 | Total loss: 3.070 | Reg loss: 0.032 | Tree loss: 3.070 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 027 | Total loss: 3.060 | Reg loss: 0.032 | Tree loss: 3.060 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 027 | Total loss: 3.021 | Reg loss: 0.032 | Tree loss: 3.021 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 027 | Total loss: 3.060 | Reg loss: 0.032 | Tree loss: 3.060 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 027 | Total loss: 2.998 | Reg loss: 0.032 | Tree loss: 2.998 | Accuracy: 0.138672 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 027 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 027 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 027 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.126953 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 027 | Total loss: 2.977 | Reg loss: 0.032 | Tree loss: 2.977 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 027 | Total loss: 2.952 | Reg loss: 0.032 | Tree loss: 2.952 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 027 | Total loss: 2.928 | Reg loss: 0.032 | Tree loss: 2.928 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 027 | Total loss: 2.940 | Reg loss: 0.032 | Tree loss: 2.940 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 027 | Total loss: 2.923 | Reg loss: 0.032 | Tree loss: 2.923 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 027 | Total loss: 3.030 | Reg loss: 0.032 | Tree loss: 3.030 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 027 | Total loss: 2.862 | Reg loss: 0.032 | Tree loss: 2.862 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 027 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 027 | Total loss: 2.899 | Reg loss: 0.033 | Tree loss: 2.899 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 027 | Total loss: 2.970 | Reg loss: 0.033 | Tree loss: 2.970 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 027 | Total loss: 2.928 | Reg loss: 0.033 | Tree loss: 2.928 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 027 | Total loss: 2.895 | Reg loss: 0.033 | Tree loss: 2.895 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 027 | Total loss: 2.916 | Reg loss: 0.033 | Tree loss: 2.916 | Accuracy: 0.121094 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 027 | Total loss: 2.893 | Reg loss: 0.033 | Tree loss: 2.893 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 027 | Total loss: 2.891 | Reg loss: 0.033 | Tree loss: 2.891 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 027 | Total loss: 2.895 | Reg loss: 0.033 | Tree loss: 2.895 | Accuracy: 0.128906 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 027 | Total loss: 2.847 | Reg loss: 0.033 | Tree loss: 2.847 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 027 | Total loss: 2.750 | Reg loss: 0.033 | Tree loss: 2.750 | Accuracy: 0.000000 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 96 | Batch: 000 / 027 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 027 | Total loss: 3.053 | Reg loss: 0.032 | Tree loss: 3.053 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 027 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.125000 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 027 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 027 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 027 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 027 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 027 | Total loss: 2.981 | Reg loss: 0.032 | Tree loss: 2.981 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 027 | Total loss: 2.969 | Reg loss: 0.032 | Tree loss: 2.969 | Accuracy: 0.082031 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 027 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 027 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 027 | Total loss: 3.005 | Reg loss: 0.032 | Tree loss: 3.005 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 027 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 027 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.130859 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 027 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 027 | Total loss: 2.882 | Reg loss: 0.032 | Tree loss: 2.882 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 027 | Total loss: 2.931 | Reg loss: 0.032 | Tree loss: 2.931 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 027 | Total loss: 2.911 | Reg loss: 0.032 | Tree loss: 2.911 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 027 | Total loss: 2.941 | Reg loss: 0.032 | Tree loss: 2.941 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 027 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 027 | Total loss: 2.880 | Reg loss: 0.033 | Tree loss: 2.880 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 027 | Total loss: 2.886 | Reg loss: 0.033 | Tree loss: 2.886 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 027 | Total loss: 2.869 | Reg loss: 0.033 | Tree loss: 2.869 | Accuracy: 0.080078 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 027 | Total loss: 2.837 | Reg loss: 0.033 | Tree loss: 2.837 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 027 | Total loss: 2.893 | Reg loss: 0.033 | Tree loss: 2.893 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 027 | Total loss: 2.839 | Reg loss: 0.033 | Tree loss: 2.839 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 027 | Total loss: 2.925 | Reg loss: 0.033 | Tree loss: 2.925 | Accuracy: 0.166667 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 97 | Batch: 000 / 027 | Total loss: 3.021 | Reg loss: 0.032 | Tree loss: 3.021 | Accuracy: 0.103516 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 001 / 027 | Total loss: 3.044 | Reg loss: 0.032 | Tree loss: 3.044 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 027 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 027 | Total loss: 3.054 | Reg loss: 0.032 | Tree loss: 3.054 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 027 | Total loss: 3.054 | Reg loss: 0.032 | Tree loss: 3.054 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 027 | Total loss: 3.000 | Reg loss: 0.032 | Tree loss: 3.000 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 027 | Total loss: 3.048 | Reg loss: 0.032 | Tree loss: 3.048 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 027 | Total loss: 2.966 | Reg loss: 0.032 | Tree loss: 2.966 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 027 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.123047 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 027 | Total loss: 2.998 | Reg loss: 0.032 | Tree loss: 2.998 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 027 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 027 | Total loss: 2.916 | Reg loss: 0.032 | Tree loss: 2.916 | Accuracy: 0.083984 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 027 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 027 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.119141 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 027 | Total loss: 2.904 | Reg loss: 0.032 | Tree loss: 2.904 | Accuracy: 0.132812 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 027 | Total loss: 2.911 | Reg loss: 0.032 | Tree loss: 2.911 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 027 | Total loss: 2.886 | Reg loss: 0.032 | Tree loss: 2.886 | Accuracy: 0.128906 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 027 | Total loss: 2.942 | Reg loss: 0.032 | Tree loss: 2.942 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 027 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 027 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 027 | Total loss: 2.876 | Reg loss: 0.032 | Tree loss: 2.876 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 027 | Total loss: 2.907 | Reg loss: 0.033 | Tree loss: 2.907 | Accuracy: 0.093750 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 027 | Total loss: 2.913 | Reg loss: 0.033 | Tree loss: 2.913 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 027 | Total loss: 2.885 | Reg loss: 0.033 | Tree loss: 2.885 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 027 | Total loss: 2.889 | Reg loss: 0.033 | Tree loss: 2.889 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 027 | Total loss: 2.821 | Reg loss: 0.033 | Tree loss: 2.821 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 027 | Total loss: 2.920 | Reg loss: 0.033 | Tree loss: 2.920 | Accuracy: 0.083333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 98 | Batch: 000 / 027 | Total loss: 3.099 | Reg loss: 0.032 | Tree loss: 3.099 | Accuracy: 0.080078 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 027 | Total loss: 3.078 | Reg loss: 0.032 | Tree loss: 3.078 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 027 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 027 | Total loss: 3.069 | Reg loss: 0.032 | Tree loss: 3.069 | Accuracy: 0.076172 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 027 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 027 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 027 | Total loss: 2.976 | Reg loss: 0.032 | Tree loss: 2.976 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 027 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 027 | Total loss: 2.969 | Reg loss: 0.032 | Tree loss: 2.969 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 027 | Total loss: 3.070 | Reg loss: 0.032 | Tree loss: 3.070 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 027 | Total loss: 2.950 | Reg loss: 0.032 | Tree loss: 2.950 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 027 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.113281 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 027 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 027 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.125000 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 027 | Total loss: 2.899 | Reg loss: 0.032 | Tree loss: 2.899 | Accuracy: 0.111328 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 027 | Total loss: 2.894 | Reg loss: 0.032 | Tree loss: 2.894 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 027 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 027 | Total loss: 2.912 | Reg loss: 0.032 | Tree loss: 2.912 | Accuracy: 0.099609 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 027 | Total loss: 2.881 | Reg loss: 0.032 | Tree loss: 2.881 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 027 | Total loss: 2.898 | Reg loss: 0.032 | Tree loss: 2.898 | Accuracy: 0.117188 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 027 | Total loss: 2.876 | Reg loss: 0.032 | Tree loss: 2.876 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 027 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.126953 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 027 | Total loss: 2.911 | Reg loss: 0.033 | Tree loss: 2.911 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 027 | Total loss: 2.861 | Reg loss: 0.033 | Tree loss: 2.861 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 027 | Total loss: 2.860 | Reg loss: 0.033 | Tree loss: 2.860 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 027 | Total loss: 2.867 | Reg loss: 0.033 | Tree loss: 2.867 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 027 | Total loss: 2.745 | Reg loss: 0.033 | Tree loss: 2.745 | Accuracy: 0.333333 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 99 | Batch: 000 / 027 | Total loss: 3.087 | Reg loss: 0.032 | Tree loss: 3.087 | Accuracy: 0.097656 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 027 | Total loss: 3.081 | Reg loss: 0.032 | Tree loss: 3.081 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 027 | Total loss: 3.005 | Reg loss: 0.032 | Tree loss: 3.005 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 027 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.107422 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 027 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.085938 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 027 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.125000 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 027 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.087891 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 027 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 027 | Total loss: 3.011 | Reg loss: 0.032 | Tree loss: 3.011 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 027 | Total loss: 2.984 | Reg loss: 0.032 | Tree loss: 2.984 | Accuracy: 0.097656 | 0.354 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 010 / 027 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.126953 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 027 | Total loss: 2.961 | Reg loss: 0.032 | Tree loss: 2.961 | Accuracy: 0.103516 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 027 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.095703 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 027 | Total loss: 2.915 | Reg loss: 0.032 | Tree loss: 2.915 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 027 | Total loss: 2.969 | Reg loss: 0.032 | Tree loss: 2.969 | Accuracy: 0.105469 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 027 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.115234 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 027 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.091797 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 027 | Total loss: 2.899 | Reg loss: 0.032 | Tree loss: 2.899 | Accuracy: 0.109375 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 027 | Total loss: 2.898 | Reg loss: 0.032 | Tree loss: 2.898 | Accuracy: 0.101562 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 027 | Total loss: 2.861 | Reg loss: 0.032 | Tree loss: 2.861 | Accuracy: 0.089844 | 0.354 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 027 | Total loss: 2.863 | Reg loss: 0.032 | Tree loss: 2.863 | Accuracy: 0.134766 | 0.353 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 027 | Total loss: 2.856 | Reg loss: 0.032 | Tree loss: 2.856 | Accuracy: 0.082031 | 0.353 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 027 | Total loss: 2.901 | Reg loss: 0.032 | Tree loss: 2.901 | Accuracy: 0.107422 | 0.353 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 027 | Total loss: 2.865 | Reg loss: 0.032 | Tree loss: 2.865 | Accuracy: 0.119141 | 0.353 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 027 | Total loss: 2.902 | Reg loss: 0.033 | Tree loss: 2.902 | Accuracy: 0.105469 | 0.353 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 027 | Total loss: 2.865 | Reg loss: 0.033 | Tree loss: 2.865 | Accuracy: 0.109375 | 0.353 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 027 | Total loss: 2.847 | Reg loss: 0.033 | Tree loss: 2.847 | Accuracy: 0.000000 | 0.353 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31199a0961ae4dbf8faf248d7a642630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5ceb8f6b21457a94dd2aff47b7e702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f4a17b775244d1b1ef34dca9958010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4596670eadb6453a9c25d755a2f6c836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.625\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 160\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/miniconda3/envs/rambo/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "10132\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "3192\n",
      "============== Pattern 160 ==============\n",
      "Average comprehensibility: 36.4\n",
      "std comprehensibility: 4.7791212581394085\n",
      "var comprehensibility: 22.84\n",
      "minimum comprehensibility: 18\n",
      "maximum comprehensibility: 44\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
