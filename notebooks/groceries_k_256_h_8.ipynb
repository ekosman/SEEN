{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 256\n",
    "tree_depth = 8\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.134346008300781 | KNN Loss: 6.232436656951904 | BCE Loss: 1.9019089937210083\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.142910957336426 | KNN Loss: 6.23236083984375 | BCE Loss: 1.9105498790740967\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.195171356201172 | KNN Loss: 6.232297420501709 | BCE Loss: 1.962874412536621\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.213438034057617 | KNN Loss: 6.232086658477783 | BCE Loss: 1.981351375579834\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.177547454833984 | KNN Loss: 6.232169151306152 | BCE Loss: 1.945378303527832\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.185166358947754 | KNN Loss: 6.23225212097168 | BCE Loss: 1.9529139995574951\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.142288208007812 | KNN Loss: 6.232115745544434 | BCE Loss: 1.910172939300537\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.10138988494873 | KNN Loss: 6.232104778289795 | BCE Loss: 1.8692848682403564\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.127714157104492 | KNN Loss: 6.231963634490967 | BCE Loss: 1.8957509994506836\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.097206115722656 | KNN Loss: 6.231761455535889 | BCE Loss: 1.8654451370239258\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.137293815612793 | KNN Loss: 6.2317891120910645 | BCE Loss: 1.9055049419403076\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.064188003540039 | KNN Loss: 6.231938362121582 | BCE Loss: 1.8322501182556152\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.092589378356934 | KNN Loss: 6.231809139251709 | BCE Loss: 1.8607805967330933\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.079907417297363 | KNN Loss: 6.231413841247559 | BCE Loss: 1.8484934568405151\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.086827278137207 | KNN Loss: 6.231234550476074 | BCE Loss: 1.8555927276611328\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.06965160369873 | KNN Loss: 6.231508255004883 | BCE Loss: 1.8381434679031372\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.073204040527344 | KNN Loss: 6.231276988983154 | BCE Loss: 1.8419275283813477\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.040755271911621 | KNN Loss: 6.2311930656433105 | BCE Loss: 1.8095622062683105\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.089496612548828 | KNN Loss: 6.230740070343018 | BCE Loss: 1.8587565422058105\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.035655975341797 | KNN Loss: 6.231256484985352 | BCE Loss: 1.8043997287750244\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.041646957397461 | KNN Loss: 6.231100082397461 | BCE Loss: 1.810547113418579\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 7.9777398109436035 | KNN Loss: 6.230752944946289 | BCE Loss: 1.746986746788025\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.971304893493652 | KNN Loss: 6.230453968048096 | BCE Loss: 1.7408510446548462\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.951624393463135 | KNN Loss: 6.230228900909424 | BCE Loss: 1.7213956117630005\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.9864606857299805 | KNN Loss: 6.230398178100586 | BCE Loss: 1.7560625076293945\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.968618392944336 | KNN Loss: 6.229536533355713 | BCE Loss: 1.7390819787979126\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.962595462799072 | KNN Loss: 6.229659080505371 | BCE Loss: 1.7329365015029907\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.933805465698242 | KNN Loss: 6.22990608215332 | BCE Loss: 1.7038991451263428\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.903253555297852 | KNN Loss: 6.229235649108887 | BCE Loss: 1.6740176677703857\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.910070419311523 | KNN Loss: 6.2291998863220215 | BCE Loss: 1.6808702945709229\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.908759117126465 | KNN Loss: 6.228830814361572 | BCE Loss: 1.6799285411834717\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.8531646728515625 | KNN Loss: 6.228430271148682 | BCE Loss: 1.6247344017028809\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.864051818847656 | KNN Loss: 6.22855806350708 | BCE Loss: 1.6354937553405762\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.861555099487305 | KNN Loss: 6.22807502746582 | BCE Loss: 1.6334803104400635\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.792901992797852 | KNN Loss: 6.227726936340332 | BCE Loss: 1.5651752948760986\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.783999443054199 | KNN Loss: 6.227174758911133 | BCE Loss: 1.5568244457244873\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.799962043762207 | KNN Loss: 6.227467060089111 | BCE Loss: 1.5724949836730957\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.786449909210205 | KNN Loss: 6.225993633270264 | BCE Loss: 1.5604562759399414\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.73784875869751 | KNN Loss: 6.225246429443359 | BCE Loss: 1.5126023292541504\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.750924110412598 | KNN Loss: 6.226351737976074 | BCE Loss: 1.5245723724365234\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.706981658935547 | KNN Loss: 6.226250171661377 | BCE Loss: 1.4807312488555908\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.752247333526611 | KNN Loss: 6.2240471839904785 | BCE Loss: 1.5282001495361328\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.626592636108398 | KNN Loss: 6.222455024719238 | BCE Loss: 1.404137372970581\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.661260604858398 | KNN Loss: 6.22411584854126 | BCE Loss: 1.4371449947357178\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.613431930541992 | KNN Loss: 6.223733425140381 | BCE Loss: 1.3896986246109009\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.600901126861572 | KNN Loss: 6.223447322845459 | BCE Loss: 1.3774538040161133\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.598854064941406 | KNN Loss: 6.2204203605651855 | BCE Loss: 1.3784337043762207\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.541945934295654 | KNN Loss: 6.221107482910156 | BCE Loss: 1.3208385705947876\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.4883713722229 | KNN Loss: 6.219124794006348 | BCE Loss: 1.2692465782165527\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.519637107849121 | KNN Loss: 6.217347621917725 | BCE Loss: 1.3022892475128174\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.483245849609375 | KNN Loss: 6.218451976776123 | BCE Loss: 1.2647937536239624\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.466116905212402 | KNN Loss: 6.2158942222595215 | BCE Loss: 1.25022292137146\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.452201843261719 | KNN Loss: 6.216094493865967 | BCE Loss: 1.2361072301864624\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.425987243652344 | KNN Loss: 6.215145587921143 | BCE Loss: 1.210841417312622\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.3771162033081055 | KNN Loss: 6.213272571563721 | BCE Loss: 1.1638436317443848\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.381107330322266 | KNN Loss: 6.2113776206970215 | BCE Loss: 1.169729471206665\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.365263938903809 | KNN Loss: 6.2102766036987305 | BCE Loss: 1.1549873352050781\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.348942756652832 | KNN Loss: 6.205291271209717 | BCE Loss: 1.1436516046524048\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.369332790374756 | KNN Loss: 6.206536769866943 | BCE Loss: 1.1627960205078125\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.3429975509643555 | KNN Loss: 6.203067302703857 | BCE Loss: 1.139930248260498\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.320013046264648 | KNN Loss: 6.204254150390625 | BCE Loss: 1.1157588958740234\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.338940620422363 | KNN Loss: 6.199380397796631 | BCE Loss: 1.139560341835022\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.301154136657715 | KNN Loss: 6.198658466339111 | BCE Loss: 1.1024956703186035\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.316831588745117 | KNN Loss: 6.198545455932617 | BCE Loss: 1.118285894393921\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.264711380004883 | KNN Loss: 6.196404457092285 | BCE Loss: 1.0683071613311768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.31865930557251 | KNN Loss: 6.1899638175964355 | BCE Loss: 1.1286954879760742\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.280426025390625 | KNN Loss: 6.181915283203125 | BCE Loss: 1.098510980606079\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.262902736663818 | KNN Loss: 6.180129051208496 | BCE Loss: 1.0827738046646118\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 7.279411315917969 | KNN Loss: 6.1774139404296875 | BCE Loss: 1.1019973754882812\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 7.265653133392334 | KNN Loss: 6.172086715698242 | BCE Loss: 1.0935664176940918\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 7.274707794189453 | KNN Loss: 6.172283172607422 | BCE Loss: 1.1024248600006104\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 7.233590126037598 | KNN Loss: 6.16810941696167 | BCE Loss: 1.0654809474945068\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 7.221923828125 | KNN Loss: 6.165285587310791 | BCE Loss: 1.0566381216049194\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 7.225013256072998 | KNN Loss: 6.154275894165039 | BCE Loss: 1.070737361907959\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 7.253413200378418 | KNN Loss: 6.146209716796875 | BCE Loss: 1.107203722000122\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 7.227694511413574 | KNN Loss: 6.14245080947876 | BCE Loss: 1.0852439403533936\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 7.213210105895996 | KNN Loss: 6.142943382263184 | BCE Loss: 1.0702664852142334\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 7.221132278442383 | KNN Loss: 6.1289143562316895 | BCE Loss: 1.0922179222106934\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 7.217275619506836 | KNN Loss: 6.130559921264648 | BCE Loss: 1.0867154598236084\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 7.162721157073975 | KNN Loss: 6.113600730895996 | BCE Loss: 1.049120545387268\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 7.18724250793457 | KNN Loss: 6.112740993499756 | BCE Loss: 1.0745017528533936\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 7.189916610717773 | KNN Loss: 6.106407165527344 | BCE Loss: 1.0835094451904297\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 7.155241966247559 | KNN Loss: 6.09694242477417 | BCE Loss: 1.0582997798919678\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 7.139094829559326 | KNN Loss: 6.085857391357422 | BCE Loss: 1.0532375574111938\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 7.130978107452393 | KNN Loss: 6.075067043304443 | BCE Loss: 1.0559110641479492\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 7.099660873413086 | KNN Loss: 6.0600666999816895 | BCE Loss: 1.0395941734313965\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 7.1481828689575195 | KNN Loss: 6.053655624389648 | BCE Loss: 1.0945271253585815\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 7.100319862365723 | KNN Loss: 6.050667762756348 | BCE Loss: 1.0496519804000854\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 7.121452331542969 | KNN Loss: 6.0374531745910645 | BCE Loss: 1.0839991569519043\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 7.105568885803223 | KNN Loss: 6.023183345794678 | BCE Loss: 1.082385778427124\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 7.08184814453125 | KNN Loss: 6.012240409851074 | BCE Loss: 1.0696079730987549\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 7.064464569091797 | KNN Loss: 6.001653671264648 | BCE Loss: 1.062811017036438\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 7.052726745605469 | KNN Loss: 5.986395359039307 | BCE Loss: 1.066331148147583\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 7.020047187805176 | KNN Loss: 5.981554985046387 | BCE Loss: 1.03849196434021\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 7.016924858093262 | KNN Loss: 5.963519096374512 | BCE Loss: 1.05340576171875\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 7.020635604858398 | KNN Loss: 5.948685646057129 | BCE Loss: 1.071950078010559\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 6.989797592163086 | KNN Loss: 5.939053535461426 | BCE Loss: 1.0507440567016602\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 7.000337600708008 | KNN Loss: 5.933811187744141 | BCE Loss: 1.0665264129638672\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 6.979948043823242 | KNN Loss: 5.920871257781982 | BCE Loss: 1.0590765476226807\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 6.976339817047119 | KNN Loss: 5.914297103881836 | BCE Loss: 1.0620428323745728\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 6.968330383300781 | KNN Loss: 5.905001640319824 | BCE Loss: 1.0633289813995361\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 6.9760332107543945 | KNN Loss: 5.904808521270752 | BCE Loss: 1.0712248086929321\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 6.968138694763184 | KNN Loss: 5.885881423950195 | BCE Loss: 1.0822573900222778\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.918434143066406 | KNN Loss: 5.881442546844482 | BCE Loss: 1.0369913578033447\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.929806232452393 | KNN Loss: 5.875732898712158 | BCE Loss: 1.0540733337402344\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.944831848144531 | KNN Loss: 5.867213249206543 | BCE Loss: 1.0776185989379883\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.945044040679932 | KNN Loss: 5.872186183929443 | BCE Loss: 1.0728578567504883\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.9087724685668945 | KNN Loss: 5.85225248336792 | BCE Loss: 1.0565197467803955\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.920531749725342 | KNN Loss: 5.84962272644043 | BCE Loss: 1.070909023284912\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.907042980194092 | KNN Loss: 5.8500871658325195 | BCE Loss: 1.0569556951522827\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.932577133178711 | KNN Loss: 5.8448286056518555 | BCE Loss: 1.0877482891082764\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.875606536865234 | KNN Loss: 5.841541767120361 | BCE Loss: 1.034064531326294\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.885775566101074 | KNN Loss: 5.839766502380371 | BCE Loss: 1.0460093021392822\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.873800754547119 | KNN Loss: 5.836774826049805 | BCE Loss: 1.0370259284973145\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.8806047439575195 | KNN Loss: 5.833694934844971 | BCE Loss: 1.0469095706939697\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.897346496582031 | KNN Loss: 5.832162380218506 | BCE Loss: 1.0651839971542358\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.9036712646484375 | KNN Loss: 5.8359479904174805 | BCE Loss: 1.0677233934402466\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.894488334655762 | KNN Loss: 5.828195095062256 | BCE Loss: 1.0662932395935059\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.9069671630859375 | KNN Loss: 5.831007480621338 | BCE Loss: 1.0759599208831787\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.920795440673828 | KNN Loss: 5.824087619781494 | BCE Loss: 1.0967075824737549\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.907583236694336 | KNN Loss: 5.822632312774658 | BCE Loss: 1.0849506855010986\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.876719951629639 | KNN Loss: 5.8251214027404785 | BCE Loss: 1.0515986680984497\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.855138778686523 | KNN Loss: 5.810033321380615 | BCE Loss: 1.0451055765151978\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.8868408203125 | KNN Loss: 5.816778659820557 | BCE Loss: 1.0700619220733643\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.849941253662109 | KNN Loss: 5.806128025054932 | BCE Loss: 1.0438134670257568\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.879687309265137 | KNN Loss: 5.818395137786865 | BCE Loss: 1.061292290687561\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.857672691345215 | KNN Loss: 5.815231800079346 | BCE Loss: 1.0424408912658691\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.88633918762207 | KNN Loss: 5.805459976196289 | BCE Loss: 1.0808789730072021\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.857332229614258 | KNN Loss: 5.799723148345947 | BCE Loss: 1.0576093196868896\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.869192123413086 | KNN Loss: 5.806182861328125 | BCE Loss: 1.06300950050354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.826141834259033 | KNN Loss: 5.800817489624023 | BCE Loss: 1.0253244638442993\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.8504414558410645 | KNN Loss: 5.806095600128174 | BCE Loss: 1.044345736503601\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.882237911224365 | KNN Loss: 5.80863094329834 | BCE Loss: 1.0736068487167358\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.842680931091309 | KNN Loss: 5.801398277282715 | BCE Loss: 1.0412826538085938\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.855913162231445 | KNN Loss: 5.80914831161499 | BCE Loss: 1.046764850616455\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.846343040466309 | KNN Loss: 5.790359020233154 | BCE Loss: 1.0559839010238647\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.848967552185059 | KNN Loss: 5.792078018188477 | BCE Loss: 1.056889533996582\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.8172688484191895 | KNN Loss: 5.775050163269043 | BCE Loss: 1.042218565940857\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.8735857009887695 | KNN Loss: 5.789022922515869 | BCE Loss: 1.0845627784729004\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.874144554138184 | KNN Loss: 5.785270690917969 | BCE Loss: 1.088874101638794\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.834368705749512 | KNN Loss: 5.784259796142578 | BCE Loss: 1.0501089096069336\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.83469295501709 | KNN Loss: 5.7629218101501465 | BCE Loss: 1.0717713832855225\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.82273006439209 | KNN Loss: 5.759035110473633 | BCE Loss: 1.063694715499878\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.828364372253418 | KNN Loss: 5.768266677856445 | BCE Loss: 1.0600974559783936\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.817382335662842 | KNN Loss: 5.770763874053955 | BCE Loss: 1.0466184616088867\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.80359411239624 | KNN Loss: 5.752574443817139 | BCE Loss: 1.0510196685791016\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.824498653411865 | KNN Loss: 5.745448589324951 | BCE Loss: 1.079050064086914\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.801214694976807 | KNN Loss: 5.749210357666016 | BCE Loss: 1.0520044565200806\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.809124946594238 | KNN Loss: 5.7450480461120605 | BCE Loss: 1.0640769004821777\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.815180778503418 | KNN Loss: 5.771834373474121 | BCE Loss: 1.043346643447876\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.806760787963867 | KNN Loss: 5.744705677032471 | BCE Loss: 1.0620548725128174\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.788606643676758 | KNN Loss: 5.7438883781433105 | BCE Loss: 1.0447185039520264\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.803628921508789 | KNN Loss: 5.737722396850586 | BCE Loss: 1.0659066438674927\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.79907751083374 | KNN Loss: 5.736745834350586 | BCE Loss: 1.0623317956924438\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.807382106781006 | KNN Loss: 5.7429022789001465 | BCE Loss: 1.0644797086715698\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.782880783081055 | KNN Loss: 5.703189849853516 | BCE Loss: 1.079690933227539\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.78446626663208 | KNN Loss: 5.720254421234131 | BCE Loss: 1.0642117261886597\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.788904190063477 | KNN Loss: 5.712957859039307 | BCE Loss: 1.0759460926055908\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.75679874420166 | KNN Loss: 5.697916030883789 | BCE Loss: 1.058882474899292\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.7487897872924805 | KNN Loss: 5.713955879211426 | BCE Loss: 1.0348341464996338\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.783812522888184 | KNN Loss: 5.7088623046875 | BCE Loss: 1.0749504566192627\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.7551984786987305 | KNN Loss: 5.710185527801514 | BCE Loss: 1.0450130701065063\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.82368278503418 | KNN Loss: 5.749614715576172 | BCE Loss: 1.0740681886672974\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.78549861907959 | KNN Loss: 5.696939468383789 | BCE Loss: 1.0885593891143799\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.7891740798950195 | KNN Loss: 5.722530841827393 | BCE Loss: 1.066643476486206\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.715399742126465 | KNN Loss: 5.678709983825684 | BCE Loss: 1.0366897583007812\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.753383636474609 | KNN Loss: 5.717874526977539 | BCE Loss: 1.0355091094970703\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.733091831207275 | KNN Loss: 5.670019626617432 | BCE Loss: 1.0630723237991333\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.816768646240234 | KNN Loss: 5.736661434173584 | BCE Loss: 1.08010733127594\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.775112152099609 | KNN Loss: 5.7302632331848145 | BCE Loss: 1.044849157333374\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.773222923278809 | KNN Loss: 5.71287727355957 | BCE Loss: 1.0603457689285278\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.74484920501709 | KNN Loss: 5.6975250244140625 | BCE Loss: 1.0473240613937378\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.7442474365234375 | KNN Loss: 5.690328598022461 | BCE Loss: 1.0539190769195557\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.750972270965576 | KNN Loss: 5.701937198638916 | BCE Loss: 1.0490350723266602\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.781822204589844 | KNN Loss: 5.677326202392578 | BCE Loss: 1.1044961214065552\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.75761604309082 | KNN Loss: 5.699029922485352 | BCE Loss: 1.0585861206054688\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.711901664733887 | KNN Loss: 5.664260387420654 | BCE Loss: 1.047641396522522\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.724888801574707 | KNN Loss: 5.661664962768555 | BCE Loss: 1.0632240772247314\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.732282638549805 | KNN Loss: 5.687941551208496 | BCE Loss: 1.044340968132019\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.7701849937438965 | KNN Loss: 5.703773498535156 | BCE Loss: 1.0664114952087402\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.730478763580322 | KNN Loss: 5.669614315032959 | BCE Loss: 1.0608645677566528\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.780749797821045 | KNN Loss: 5.707337856292725 | BCE Loss: 1.0734118223190308\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.698168754577637 | KNN Loss: 5.669885635375977 | BCE Loss: 1.0282831192016602\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.7699456214904785 | KNN Loss: 5.688726425170898 | BCE Loss: 1.0812193155288696\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.702672004699707 | KNN Loss: 5.664909362792969 | BCE Loss: 1.0377627611160278\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.714160919189453 | KNN Loss: 5.66160774230957 | BCE Loss: 1.0525529384613037\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.693327903747559 | KNN Loss: 5.647583484649658 | BCE Loss: 1.0457446575164795\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.7438063621521 | KNN Loss: 5.683860778808594 | BCE Loss: 1.0599455833435059\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.711007118225098 | KNN Loss: 5.678701877593994 | BCE Loss: 1.032305121421814\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.786128997802734 | KNN Loss: 5.726524829864502 | BCE Loss: 1.0596041679382324\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.745316505432129 | KNN Loss: 5.694771766662598 | BCE Loss: 1.0505449771881104\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.683591842651367 | KNN Loss: 5.641051769256592 | BCE Loss: 1.0425398349761963\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.750957489013672 | KNN Loss: 5.70673131942749 | BCE Loss: 1.044226050376892\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.752244472503662 | KNN Loss: 5.658280849456787 | BCE Loss: 1.0939637422561646\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.691798210144043 | KNN Loss: 5.659022808074951 | BCE Loss: 1.032775640487671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.697096824645996 | KNN Loss: 5.649101257324219 | BCE Loss: 1.0479955673217773\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.687907695770264 | KNN Loss: 5.642653942108154 | BCE Loss: 1.0452537536621094\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.684504985809326 | KNN Loss: 5.637027740478516 | BCE Loss: 1.047477126121521\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.726513862609863 | KNN Loss: 5.665369033813477 | BCE Loss: 1.0611450672149658\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.746542930603027 | KNN Loss: 5.689077854156494 | BCE Loss: 1.057464838027954\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.70589017868042 | KNN Loss: 5.639133453369141 | BCE Loss: 1.0667567253112793\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.704431056976318 | KNN Loss: 5.657406806945801 | BCE Loss: 1.0470243692398071\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.731531620025635 | KNN Loss: 5.679787635803223 | BCE Loss: 1.051743984222412\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.7201337814331055 | KNN Loss: 5.652302265167236 | BCE Loss: 1.0678315162658691\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.702264785766602 | KNN Loss: 5.639845848083496 | BCE Loss: 1.0624191761016846\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.684398651123047 | KNN Loss: 5.637824535369873 | BCE Loss: 1.0465741157531738\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.780978679656982 | KNN Loss: 5.711906433105469 | BCE Loss: 1.0690721273422241\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.733804702758789 | KNN Loss: 5.662053108215332 | BCE Loss: 1.071751594543457\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.713545799255371 | KNN Loss: 5.637019157409668 | BCE Loss: 1.0765265226364136\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.74409294128418 | KNN Loss: 5.6664605140686035 | BCE Loss: 1.0776323080062866\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.75630521774292 | KNN Loss: 5.662899494171143 | BCE Loss: 1.0934056043624878\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.688539505004883 | KNN Loss: 5.657040119171143 | BCE Loss: 1.0314996242523193\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.68848991394043 | KNN Loss: 5.636531352996826 | BCE Loss: 1.0519583225250244\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.685729026794434 | KNN Loss: 5.657073497772217 | BCE Loss: 1.0286555290222168\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.7267632484436035 | KNN Loss: 5.6764912605285645 | BCE Loss: 1.0502718687057495\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.68000602722168 | KNN Loss: 5.632598876953125 | BCE Loss: 1.0474069118499756\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.717677116394043 | KNN Loss: 5.659031391143799 | BCE Loss: 1.0586459636688232\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.700577735900879 | KNN Loss: 5.638800144195557 | BCE Loss: 1.0617778301239014\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.709742546081543 | KNN Loss: 5.642988204956055 | BCE Loss: 1.0667542219161987\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.7451558113098145 | KNN Loss: 5.691246509552002 | BCE Loss: 1.053909420967102\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.671963691711426 | KNN Loss: 5.646203517913818 | BCE Loss: 1.0257601737976074\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.702826023101807 | KNN Loss: 5.644251823425293 | BCE Loss: 1.0585741996765137\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.690702438354492 | KNN Loss: 5.637358665466309 | BCE Loss: 1.0533437728881836\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.689741611480713 | KNN Loss: 5.648475170135498 | BCE Loss: 1.0412665605545044\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.666133880615234 | KNN Loss: 5.632736682891846 | BCE Loss: 1.0333969593048096\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.7035369873046875 | KNN Loss: 5.654353618621826 | BCE Loss: 1.0491834878921509\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.670770168304443 | KNN Loss: 5.632920742034912 | BCE Loss: 1.0378494262695312\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.681031227111816 | KNN Loss: 5.619943618774414 | BCE Loss: 1.061087727546692\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.692209243774414 | KNN Loss: 5.649155616760254 | BCE Loss: 1.043053388595581\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.7327399253845215 | KNN Loss: 5.679299354553223 | BCE Loss: 1.0534406900405884\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.681477069854736 | KNN Loss: 5.642742156982422 | BCE Loss: 1.038735032081604\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.665868282318115 | KNN Loss: 5.620475769042969 | BCE Loss: 1.045392394065857\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.6905388832092285 | KNN Loss: 5.656367778778076 | BCE Loss: 1.0341711044311523\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.739590644836426 | KNN Loss: 5.653517723083496 | BCE Loss: 1.0860731601715088\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.667145729064941 | KNN Loss: 5.642765045166016 | BCE Loss: 1.0243804454803467\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.743259906768799 | KNN Loss: 5.701015949249268 | BCE Loss: 1.0422439575195312\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.624295234680176 | KNN Loss: 5.618433475494385 | BCE Loss: 1.0058616399765015\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.642027854919434 | KNN Loss: 5.625187397003174 | BCE Loss: 1.0168406963348389\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.7054667472839355 | KNN Loss: 5.673905372619629 | BCE Loss: 1.0315613746643066\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.662139892578125 | KNN Loss: 5.621363162994385 | BCE Loss: 1.0407766103744507\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.735995769500732 | KNN Loss: 5.674176216125488 | BCE Loss: 1.0618196725845337\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.672446250915527 | KNN Loss: 5.643878936767578 | BCE Loss: 1.0285670757293701\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.713145732879639 | KNN Loss: 5.676095008850098 | BCE Loss: 1.0370506048202515\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.65597677230835 | KNN Loss: 5.616081714630127 | BCE Loss: 1.0398951768875122\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.698524475097656 | KNN Loss: 5.638542652130127 | BCE Loss: 1.0599820613861084\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.71610689163208 | KNN Loss: 5.674151420593262 | BCE Loss: 1.0419554710388184\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.690829277038574 | KNN Loss: 5.619656562805176 | BCE Loss: 1.0711724758148193\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.672677040100098 | KNN Loss: 5.6139302253723145 | BCE Loss: 1.0587468147277832\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.662862777709961 | KNN Loss: 5.62252140045166 | BCE Loss: 1.0403411388397217\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.663726329803467 | KNN Loss: 5.625638008117676 | BCE Loss: 1.038088321685791\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.766040802001953 | KNN Loss: 5.730106830596924 | BCE Loss: 1.0359342098236084\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.753235340118408 | KNN Loss: 5.717024326324463 | BCE Loss: 1.0362110137939453\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.646340370178223 | KNN Loss: 5.613246917724609 | BCE Loss: 1.0330934524536133\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.687380313873291 | KNN Loss: 5.6235198974609375 | BCE Loss: 1.0638604164123535\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.6840925216674805 | KNN Loss: 5.610925674438477 | BCE Loss: 1.073166847229004\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.686054706573486 | KNN Loss: 5.649342060089111 | BCE Loss: 1.0367125272750854\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.710264205932617 | KNN Loss: 5.640682697296143 | BCE Loss: 1.0695812702178955\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.687519550323486 | KNN Loss: 5.644282341003418 | BCE Loss: 1.0432372093200684\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.694196701049805 | KNN Loss: 5.628920555114746 | BCE Loss: 1.0652763843536377\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.730284690856934 | KNN Loss: 5.655311107635498 | BCE Loss: 1.074973702430725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.752091884613037 | KNN Loss: 5.704764366149902 | BCE Loss: 1.0473275184631348\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.636539459228516 | KNN Loss: 5.615149021148682 | BCE Loss: 1.0213905572891235\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.690989971160889 | KNN Loss: 5.654325485229492 | BCE Loss: 1.036664366722107\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.6825852394104 | KNN Loss: 5.641018867492676 | BCE Loss: 1.041566252708435\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.712943077087402 | KNN Loss: 5.646514415740967 | BCE Loss: 1.066428780555725\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.7194294929504395 | KNN Loss: 5.655757427215576 | BCE Loss: 1.0636720657348633\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.678214073181152 | KNN Loss: 5.614256858825684 | BCE Loss: 1.0639570951461792\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.651150703430176 | KNN Loss: 5.6342315673828125 | BCE Loss: 1.0169193744659424\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.68801212310791 | KNN Loss: 5.623937129974365 | BCE Loss: 1.064074993133545\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.721380233764648 | KNN Loss: 5.6956071853637695 | BCE Loss: 1.0257728099822998\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.6644086837768555 | KNN Loss: 5.634987831115723 | BCE Loss: 1.0294206142425537\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.677641868591309 | KNN Loss: 5.663289546966553 | BCE Loss: 1.0143524408340454\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.686093807220459 | KNN Loss: 5.639803886413574 | BCE Loss: 1.0462899208068848\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.685502052307129 | KNN Loss: 5.633417129516602 | BCE Loss: 1.0520848035812378\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.730267524719238 | KNN Loss: 5.663509368896484 | BCE Loss: 1.0667580366134644\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.649949550628662 | KNN Loss: 5.605968952178955 | BCE Loss: 1.043980598449707\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.681273460388184 | KNN Loss: 5.645730972290039 | BCE Loss: 1.0355424880981445\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.686616897583008 | KNN Loss: 5.63240385055542 | BCE Loss: 1.0542128086090088\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.678349018096924 | KNN Loss: 5.635219097137451 | BCE Loss: 1.0431300401687622\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.68686056137085 | KNN Loss: 5.613508224487305 | BCE Loss: 1.0733522176742554\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.705326080322266 | KNN Loss: 5.647198677062988 | BCE Loss: 1.0581276416778564\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.723431587219238 | KNN Loss: 5.661792755126953 | BCE Loss: 1.0616388320922852\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.699937343597412 | KNN Loss: 5.6537017822265625 | BCE Loss: 1.0462355613708496\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.784122943878174 | KNN Loss: 5.703547954559326 | BCE Loss: 1.0805749893188477\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.644775867462158 | KNN Loss: 5.615654945373535 | BCE Loss: 1.029120922088623\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.671739101409912 | KNN Loss: 5.609330177307129 | BCE Loss: 1.0624089241027832\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.644310474395752 | KNN Loss: 5.612130165100098 | BCE Loss: 1.0321801900863647\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.701738357543945 | KNN Loss: 5.666956424713135 | BCE Loss: 1.0347821712493896\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.755928039550781 | KNN Loss: 5.705122470855713 | BCE Loss: 1.0508058071136475\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.67252779006958 | KNN Loss: 5.646763324737549 | BCE Loss: 1.0257643461227417\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.65169620513916 | KNN Loss: 5.606431007385254 | BCE Loss: 1.0452651977539062\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.6960625648498535 | KNN Loss: 5.652697563171387 | BCE Loss: 1.0433650016784668\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.762118339538574 | KNN Loss: 5.705799102783203 | BCE Loss: 1.0563194751739502\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.706371307373047 | KNN Loss: 5.642818927764893 | BCE Loss: 1.0635526180267334\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.726243495941162 | KNN Loss: 5.664941310882568 | BCE Loss: 1.0613021850585938\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.763623237609863 | KNN Loss: 5.705652236938477 | BCE Loss: 1.0579710006713867\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.660921096801758 | KNN Loss: 5.60885763168335 | BCE Loss: 1.052063226699829\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.6725544929504395 | KNN Loss: 5.639113903045654 | BCE Loss: 1.0334405899047852\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.6425886154174805 | KNN Loss: 5.601104736328125 | BCE Loss: 1.0414841175079346\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.629240989685059 | KNN Loss: 5.606011867523193 | BCE Loss: 1.0232293605804443\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.664059638977051 | KNN Loss: 5.622630596160889 | BCE Loss: 1.0414291620254517\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.709255695343018 | KNN Loss: 5.639803886413574 | BCE Loss: 1.0694516897201538\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.668152809143066 | KNN Loss: 5.616247177124023 | BCE Loss: 1.051905870437622\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.673041343688965 | KNN Loss: 5.608214855194092 | BCE Loss: 1.0648263692855835\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.727297782897949 | KNN Loss: 5.669315338134766 | BCE Loss: 1.0579822063446045\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.655855178833008 | KNN Loss: 5.621316909790039 | BCE Loss: 1.0345380306243896\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.657359600067139 | KNN Loss: 5.637890338897705 | BCE Loss: 1.019469141960144\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.763055324554443 | KNN Loss: 5.705956935882568 | BCE Loss: 1.057098388671875\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.646714210510254 | KNN Loss: 5.617307186126709 | BCE Loss: 1.0294071435928345\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.672125339508057 | KNN Loss: 5.625228404998779 | BCE Loss: 1.0468969345092773\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.662125587463379 | KNN Loss: 5.62681245803833 | BCE Loss: 1.035313367843628\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.650864601135254 | KNN Loss: 5.610055446624756 | BCE Loss: 1.0408093929290771\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.691765785217285 | KNN Loss: 5.6155171394348145 | BCE Loss: 1.0762484073638916\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.664770126342773 | KNN Loss: 5.6134796142578125 | BCE Loss: 1.051290512084961\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.765644073486328 | KNN Loss: 5.675481796264648 | BCE Loss: 1.0901622772216797\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.691257953643799 | KNN Loss: 5.653739929199219 | BCE Loss: 1.0375179052352905\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.615759372711182 | KNN Loss: 5.610628604888916 | BCE Loss: 1.0051307678222656\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.687078475952148 | KNN Loss: 5.628726482391357 | BCE Loss: 1.058351993560791\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.696239471435547 | KNN Loss: 5.64192008972168 | BCE Loss: 1.054319143295288\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.684271812438965 | KNN Loss: 5.637825012207031 | BCE Loss: 1.0464465618133545\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.703939437866211 | KNN Loss: 5.631733417510986 | BCE Loss: 1.0722061395645142\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.674140453338623 | KNN Loss: 5.627394199371338 | BCE Loss: 1.0467462539672852\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.715553283691406 | KNN Loss: 5.661341190338135 | BCE Loss: 1.0542120933532715\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.689133644104004 | KNN Loss: 5.62261962890625 | BCE Loss: 1.066514253616333\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.71431303024292 | KNN Loss: 5.665066242218018 | BCE Loss: 1.0492466688156128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.708770275115967 | KNN Loss: 5.668721675872803 | BCE Loss: 1.040048599243164\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.696259498596191 | KNN Loss: 5.674571514129639 | BCE Loss: 1.0216882228851318\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.712340831756592 | KNN Loss: 5.658931255340576 | BCE Loss: 1.053409457206726\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.7107768058776855 | KNN Loss: 5.620892524719238 | BCE Loss: 1.0898842811584473\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.682238578796387 | KNN Loss: 5.6255388259887695 | BCE Loss: 1.0566998720169067\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.720672130584717 | KNN Loss: 5.663945198059082 | BCE Loss: 1.0567269325256348\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.668315887451172 | KNN Loss: 5.620479106903076 | BCE Loss: 1.0478367805480957\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.702477931976318 | KNN Loss: 5.645752906799316 | BCE Loss: 1.0567249059677124\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.668252944946289 | KNN Loss: 5.612016677856445 | BCE Loss: 1.0562362670898438\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.6837615966796875 | KNN Loss: 5.623750686645508 | BCE Loss: 1.0600110292434692\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.666790962219238 | KNN Loss: 5.631169319152832 | BCE Loss: 1.0356218814849854\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.654254913330078 | KNN Loss: 5.613458633422852 | BCE Loss: 1.0407962799072266\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.729005336761475 | KNN Loss: 5.6840739250183105 | BCE Loss: 1.044931411743164\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.72008752822876 | KNN Loss: 5.633867263793945 | BCE Loss: 1.086220145225525\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.669257640838623 | KNN Loss: 5.6134843826293945 | BCE Loss: 1.0557732582092285\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.683691024780273 | KNN Loss: 5.643496513366699 | BCE Loss: 1.0401942729949951\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.642406940460205 | KNN Loss: 5.60953426361084 | BCE Loss: 1.0328727960586548\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.6472930908203125 | KNN Loss: 5.610551357269287 | BCE Loss: 1.0367414951324463\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.669614315032959 | KNN Loss: 5.604092597961426 | BCE Loss: 1.0655217170715332\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.662553787231445 | KNN Loss: 5.6079607009887695 | BCE Loss: 1.0545928478240967\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.656167507171631 | KNN Loss: 5.611566066741943 | BCE Loss: 1.0446014404296875\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.650109767913818 | KNN Loss: 5.601593971252441 | BCE Loss: 1.048515796661377\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.691160678863525 | KNN Loss: 5.6459126472473145 | BCE Loss: 1.0452479124069214\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.637385845184326 | KNN Loss: 5.597078323364258 | BCE Loss: 1.0403075218200684\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.66732120513916 | KNN Loss: 5.610010623931885 | BCE Loss: 1.0573103427886963\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.665881156921387 | KNN Loss: 5.603055477142334 | BCE Loss: 1.0628254413604736\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.734333038330078 | KNN Loss: 5.6536078453063965 | BCE Loss: 1.0807251930236816\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.709209442138672 | KNN Loss: 5.651902675628662 | BCE Loss: 1.0573070049285889\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.699000358581543 | KNN Loss: 5.657315731048584 | BCE Loss: 1.041684627532959\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.672384738922119 | KNN Loss: 5.620645999908447 | BCE Loss: 1.0517388582229614\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.698258399963379 | KNN Loss: 5.648138999938965 | BCE Loss: 1.0501195192337036\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.656945705413818 | KNN Loss: 5.632784366607666 | BCE Loss: 1.024161458015442\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.669321060180664 | KNN Loss: 5.627696514129639 | BCE Loss: 1.0416244268417358\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.692811965942383 | KNN Loss: 5.635351657867432 | BCE Loss: 1.0574603080749512\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.694387912750244 | KNN Loss: 5.648828506469727 | BCE Loss: 1.0455594062805176\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.6420817375183105 | KNN Loss: 5.605444431304932 | BCE Loss: 1.0366371870040894\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.6923980712890625 | KNN Loss: 5.6114397048950195 | BCE Loss: 1.080958604812622\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.6897783279418945 | KNN Loss: 5.650718688964844 | BCE Loss: 1.0390594005584717\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.728041172027588 | KNN Loss: 5.659936428070068 | BCE Loss: 1.06810462474823\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.754803657531738 | KNN Loss: 5.66887092590332 | BCE Loss: 1.0859324932098389\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.677618503570557 | KNN Loss: 5.646472454071045 | BCE Loss: 1.0311461687088013\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.686751365661621 | KNN Loss: 5.601718425750732 | BCE Loss: 1.0850330591201782\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.7302093505859375 | KNN Loss: 5.652624130249023 | BCE Loss: 1.0775853395462036\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.645175933837891 | KNN Loss: 5.612500190734863 | BCE Loss: 1.0326755046844482\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.673030853271484 | KNN Loss: 5.6007399559021 | BCE Loss: 1.0722906589508057\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.707857131958008 | KNN Loss: 5.6086859703063965 | BCE Loss: 1.0991712808609009\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.662788391113281 | KNN Loss: 5.596839904785156 | BCE Loss: 1.065948724746704\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.830722332000732 | KNN Loss: 5.764806270599365 | BCE Loss: 1.0659161806106567\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.682731628417969 | KNN Loss: 5.668636322021484 | BCE Loss: 1.0140955448150635\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.694484710693359 | KNN Loss: 5.628362655639648 | BCE Loss: 1.066122055053711\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.735513210296631 | KNN Loss: 5.676411151885986 | BCE Loss: 1.059102177619934\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.65534782409668 | KNN Loss: 5.609118938446045 | BCE Loss: 1.0462287664413452\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.680037498474121 | KNN Loss: 5.610089302062988 | BCE Loss: 1.069948434829712\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.632801532745361 | KNN Loss: 5.606169700622559 | BCE Loss: 1.0266317129135132\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.646246910095215 | KNN Loss: 5.599498271942139 | BCE Loss: 1.0467486381530762\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.655562400817871 | KNN Loss: 5.613110065460205 | BCE Loss: 1.042452335357666\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.697207450866699 | KNN Loss: 5.6546454429626465 | BCE Loss: 1.0425620079040527\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.714910507202148 | KNN Loss: 5.670142650604248 | BCE Loss: 1.04476797580719\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.686188697814941 | KNN Loss: 5.65240478515625 | BCE Loss: 1.0337841510772705\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.6618194580078125 | KNN Loss: 5.631681442260742 | BCE Loss: 1.0301377773284912\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.692631721496582 | KNN Loss: 5.639871120452881 | BCE Loss: 1.0527606010437012\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.726324081420898 | KNN Loss: 5.681915283203125 | BCE Loss: 1.0444087982177734\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.709088325500488 | KNN Loss: 5.6631622314453125 | BCE Loss: 1.0459263324737549\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.668830871582031 | KNN Loss: 5.610211372375488 | BCE Loss: 1.058619737625122\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.634523391723633 | KNN Loss: 5.6032938957214355 | BCE Loss: 1.0312292575836182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.648127555847168 | KNN Loss: 5.601924896240234 | BCE Loss: 1.0462024211883545\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.660036087036133 | KNN Loss: 5.614432334899902 | BCE Loss: 1.0456039905548096\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.674197673797607 | KNN Loss: 5.626425743103027 | BCE Loss: 1.04777193069458\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.666938304901123 | KNN Loss: 5.613509178161621 | BCE Loss: 1.0534290075302124\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.675251007080078 | KNN Loss: 5.637765407562256 | BCE Loss: 1.0374853610992432\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.763779163360596 | KNN Loss: 5.714779853820801 | BCE Loss: 1.0489994287490845\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.637813091278076 | KNN Loss: 5.600454807281494 | BCE Loss: 1.037358283996582\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.673925399780273 | KNN Loss: 5.616410732269287 | BCE Loss: 1.0575146675109863\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.651289939880371 | KNN Loss: 5.614027976989746 | BCE Loss: 1.0372620820999146\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.705018997192383 | KNN Loss: 5.657279014587402 | BCE Loss: 1.0477399826049805\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.656854152679443 | KNN Loss: 5.601006507873535 | BCE Loss: 1.0558476448059082\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.690593719482422 | KNN Loss: 5.617786407470703 | BCE Loss: 1.0728070735931396\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.683481693267822 | KNN Loss: 5.641775131225586 | BCE Loss: 1.0417065620422363\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.702809810638428 | KNN Loss: 5.618446350097656 | BCE Loss: 1.0843634605407715\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.653491973876953 | KNN Loss: 5.624622821807861 | BCE Loss: 1.0288689136505127\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.666815280914307 | KNN Loss: 5.630101203918457 | BCE Loss: 1.03671395778656\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.660295009613037 | KNN Loss: 5.644432067871094 | BCE Loss: 1.0158629417419434\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.688447952270508 | KNN Loss: 5.600135803222656 | BCE Loss: 1.088312029838562\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.758569717407227 | KNN Loss: 5.701570987701416 | BCE Loss: 1.0569987297058105\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.6826395988464355 | KNN Loss: 5.646176815032959 | BCE Loss: 1.0364627838134766\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.696220397949219 | KNN Loss: 5.636949062347412 | BCE Loss: 1.0592710971832275\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.749244213104248 | KNN Loss: 5.663463592529297 | BCE Loss: 1.0857806205749512\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.6852617263793945 | KNN Loss: 5.633002281188965 | BCE Loss: 1.0522592067718506\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.673323631286621 | KNN Loss: 5.633930683135986 | BCE Loss: 1.0393931865692139\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.6539435386657715 | KNN Loss: 5.61464262008667 | BCE Loss: 1.0393009185791016\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.66231632232666 | KNN Loss: 5.601461410522461 | BCE Loss: 1.0608546733856201\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.684126853942871 | KNN Loss: 5.632859230041504 | BCE Loss: 1.0512678623199463\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.688189506530762 | KNN Loss: 5.644881248474121 | BCE Loss: 1.0433084964752197\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.672057628631592 | KNN Loss: 5.620397567749023 | BCE Loss: 1.0516600608825684\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.661304950714111 | KNN Loss: 5.60566520690918 | BCE Loss: 1.0556398630142212\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.654585361480713 | KNN Loss: 5.61479377746582 | BCE Loss: 1.0397915840148926\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.693678379058838 | KNN Loss: 5.641783714294434 | BCE Loss: 1.0518946647644043\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.722484588623047 | KNN Loss: 5.651623725891113 | BCE Loss: 1.0708611011505127\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.721942901611328 | KNN Loss: 5.685883045196533 | BCE Loss: 1.036059856414795\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.697221279144287 | KNN Loss: 5.641047477722168 | BCE Loss: 1.0561736822128296\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.639630317687988 | KNN Loss: 5.609292030334473 | BCE Loss: 1.0303385257720947\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.650364398956299 | KNN Loss: 5.60422420501709 | BCE Loss: 1.0461400747299194\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.693190574645996 | KNN Loss: 5.635955333709717 | BCE Loss: 1.0572352409362793\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.710465908050537 | KNN Loss: 5.664513111114502 | BCE Loss: 1.0459527969360352\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.669708251953125 | KNN Loss: 5.619937419891357 | BCE Loss: 1.0497705936431885\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.652444839477539 | KNN Loss: 5.604349136352539 | BCE Loss: 1.048095703125\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.652814865112305 | KNN Loss: 5.620873928070068 | BCE Loss: 1.0319409370422363\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.663602352142334 | KNN Loss: 5.6319122314453125 | BCE Loss: 1.0316901206970215\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.650005340576172 | KNN Loss: 5.604613780975342 | BCE Loss: 1.04539155960083\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.754961013793945 | KNN Loss: 5.69667911529541 | BCE Loss: 1.0582818984985352\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.736595153808594 | KNN Loss: 5.681993007659912 | BCE Loss: 1.0546022653579712\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.709072113037109 | KNN Loss: 5.664290428161621 | BCE Loss: 1.0447814464569092\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.682198524475098 | KNN Loss: 5.599259853363037 | BCE Loss: 1.0829384326934814\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.655033111572266 | KNN Loss: 5.593092441558838 | BCE Loss: 1.0619405508041382\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.6122236251831055 | KNN Loss: 5.628215789794922 | BCE Loss: 0.9840077757835388\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.717409610748291 | KNN Loss: 5.672704696655273 | BCE Loss: 1.044704794883728\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.717320442199707 | KNN Loss: 5.658436298370361 | BCE Loss: 1.0588843822479248\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.638797760009766 | KNN Loss: 5.6046271324157715 | BCE Loss: 1.0341706275939941\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.661861896514893 | KNN Loss: 5.626379013061523 | BCE Loss: 1.0354828834533691\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.659107685089111 | KNN Loss: 5.597719192504883 | BCE Loss: 1.0613884925842285\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.665400505065918 | KNN Loss: 5.610474586486816 | BCE Loss: 1.0549259185791016\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.724729537963867 | KNN Loss: 5.677562713623047 | BCE Loss: 1.0471670627593994\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.7341694831848145 | KNN Loss: 5.687708854675293 | BCE Loss: 1.0464606285095215\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.639821529388428 | KNN Loss: 5.616563320159912 | BCE Loss: 1.0232582092285156\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.651905059814453 | KNN Loss: 5.605366230010986 | BCE Loss: 1.0465385913848877\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.727553367614746 | KNN Loss: 5.650297164916992 | BCE Loss: 1.0772559642791748\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.687855243682861 | KNN Loss: 5.627825736999512 | BCE Loss: 1.0600295066833496\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.697423934936523 | KNN Loss: 5.63107967376709 | BCE Loss: 1.0663440227508545\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.710719585418701 | KNN Loss: 5.665297985076904 | BCE Loss: 1.0454217195510864\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.737534999847412 | KNN Loss: 5.7038774490356445 | BCE Loss: 1.0336575508117676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.664184093475342 | KNN Loss: 5.608595371246338 | BCE Loss: 1.0555886030197144\n",
      "Epoch    76: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.741213321685791 | KNN Loss: 5.684236526489258 | BCE Loss: 1.0569767951965332\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.6903486251831055 | KNN Loss: 5.609631538391113 | BCE Loss: 1.0807173252105713\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.673060417175293 | KNN Loss: 5.642778396606445 | BCE Loss: 1.0302822589874268\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.763747692108154 | KNN Loss: 5.694355010986328 | BCE Loss: 1.0693926811218262\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.699502468109131 | KNN Loss: 5.643534183502197 | BCE Loss: 1.055968165397644\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.661111831665039 | KNN Loss: 5.623355388641357 | BCE Loss: 1.0377564430236816\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.6215667724609375 | KNN Loss: 5.608458995819092 | BCE Loss: 1.0131078958511353\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.768523693084717 | KNN Loss: 5.674842357635498 | BCE Loss: 1.0936812162399292\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.669631004333496 | KNN Loss: 5.604918479919434 | BCE Loss: 1.0647125244140625\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.772515773773193 | KNN Loss: 5.700238227844238 | BCE Loss: 1.0722774267196655\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.658382892608643 | KNN Loss: 5.625998497009277 | BCE Loss: 1.0323842763900757\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.650467395782471 | KNN Loss: 5.619037628173828 | BCE Loss: 1.031429648399353\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.681302070617676 | KNN Loss: 5.621626853942871 | BCE Loss: 1.0596749782562256\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.712904453277588 | KNN Loss: 5.641345024108887 | BCE Loss: 1.0715595483779907\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.68487548828125 | KNN Loss: 5.621592044830322 | BCE Loss: 1.0632836818695068\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.655810832977295 | KNN Loss: 5.6130266189575195 | BCE Loss: 1.042784333229065\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.651489734649658 | KNN Loss: 5.619857311248779 | BCE Loss: 1.0316325426101685\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.734385013580322 | KNN Loss: 5.692151069641113 | BCE Loss: 1.042233943939209\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.6888427734375 | KNN Loss: 5.640894412994385 | BCE Loss: 1.0479481220245361\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.6791253089904785 | KNN Loss: 5.629159450531006 | BCE Loss: 1.049965739250183\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.671438217163086 | KNN Loss: 5.604204177856445 | BCE Loss: 1.0672342777252197\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.6566481590271 | KNN Loss: 5.638738632202148 | BCE Loss: 1.0179096460342407\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.66977596282959 | KNN Loss: 5.6445698738098145 | BCE Loss: 1.0252063274383545\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.665528297424316 | KNN Loss: 5.623775959014893 | BCE Loss: 1.0417520999908447\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.669335842132568 | KNN Loss: 5.643395900726318 | BCE Loss: 1.0259400606155396\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.681475639343262 | KNN Loss: 5.631256103515625 | BCE Loss: 1.0502194166183472\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.707968711853027 | KNN Loss: 5.6378889083862305 | BCE Loss: 1.0700799226760864\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.6491827964782715 | KNN Loss: 5.615873336791992 | BCE Loss: 1.0333094596862793\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.6807684898376465 | KNN Loss: 5.627320766448975 | BCE Loss: 1.0534477233886719\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.686645030975342 | KNN Loss: 5.652815341949463 | BCE Loss: 1.033829689025879\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.678714275360107 | KNN Loss: 5.64299201965332 | BCE Loss: 1.0357221364974976\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.655247211456299 | KNN Loss: 5.614699363708496 | BCE Loss: 1.0405477285385132\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.67204475402832 | KNN Loss: 5.617523670196533 | BCE Loss: 1.0545213222503662\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.703713417053223 | KNN Loss: 5.6250200271606445 | BCE Loss: 1.078693151473999\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.621020793914795 | KNN Loss: 5.596960067749023 | BCE Loss: 1.024060845375061\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.6402764320373535 | KNN Loss: 5.603707313537598 | BCE Loss: 1.0365691184997559\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.785527229309082 | KNN Loss: 5.728871822357178 | BCE Loss: 1.0566554069519043\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.719588756561279 | KNN Loss: 5.649948596954346 | BCE Loss: 1.069640040397644\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.668303489685059 | KNN Loss: 5.602107048034668 | BCE Loss: 1.0661966800689697\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.761659622192383 | KNN Loss: 5.737128257751465 | BCE Loss: 1.024531602859497\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.675508499145508 | KNN Loss: 5.636512279510498 | BCE Loss: 1.0389962196350098\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.676296234130859 | KNN Loss: 5.6346330642700195 | BCE Loss: 1.0416630506515503\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.626181125640869 | KNN Loss: 5.602498531341553 | BCE Loss: 1.0236825942993164\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.68143367767334 | KNN Loss: 5.611725330352783 | BCE Loss: 1.069708228111267\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.653010368347168 | KNN Loss: 5.602166175842285 | BCE Loss: 1.0508439540863037\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.71221923828125 | KNN Loss: 5.651236057281494 | BCE Loss: 1.060983419418335\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.669489860534668 | KNN Loss: 5.620922088623047 | BCE Loss: 1.0485680103302002\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.687459468841553 | KNN Loss: 5.6514153480529785 | BCE Loss: 1.0360442399978638\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.651978492736816 | KNN Loss: 5.599050045013428 | BCE Loss: 1.0529282093048096\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.661299228668213 | KNN Loss: 5.599151134490967 | BCE Loss: 1.062148094177246\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.64960241317749 | KNN Loss: 5.611125469207764 | BCE Loss: 1.0384769439697266\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.729083061218262 | KNN Loss: 5.664815902709961 | BCE Loss: 1.0642669200897217\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.662841796875 | KNN Loss: 5.597340106964111 | BCE Loss: 1.0655014514923096\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.702201843261719 | KNN Loss: 5.614870071411133 | BCE Loss: 1.087332010269165\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.666927814483643 | KNN Loss: 5.629234313964844 | BCE Loss: 1.0376935005187988\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.729305267333984 | KNN Loss: 5.6717095375061035 | BCE Loss: 1.0575958490371704\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.759150505065918 | KNN Loss: 5.718064785003662 | BCE Loss: 1.0410857200622559\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.636456489562988 | KNN Loss: 5.609427452087402 | BCE Loss: 1.027029275894165\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.645094871520996 | KNN Loss: 5.601372718811035 | BCE Loss: 1.04372239112854\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.668278694152832 | KNN Loss: 5.615089416503906 | BCE Loss: 1.0531892776489258\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.630035400390625 | KNN Loss: 5.615218639373779 | BCE Loss: 1.0148169994354248\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.640075206756592 | KNN Loss: 5.601024627685547 | BCE Loss: 1.039050579071045\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.651424407958984 | KNN Loss: 5.6221537590026855 | BCE Loss: 1.0292704105377197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.652132511138916 | KNN Loss: 5.604006767272949 | BCE Loss: 1.0481258630752563\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.720600605010986 | KNN Loss: 5.674674034118652 | BCE Loss: 1.045926570892334\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.62612771987915 | KNN Loss: 5.615839004516602 | BCE Loss: 1.0102887153625488\n",
      "Epoch    87: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.633025646209717 | KNN Loss: 5.601206302642822 | BCE Loss: 1.031819462776184\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.683670997619629 | KNN Loss: 5.6762213706970215 | BCE Loss: 1.0074496269226074\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.679852485656738 | KNN Loss: 5.623786926269531 | BCE Loss: 1.0560657978057861\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.6787495613098145 | KNN Loss: 5.649400234222412 | BCE Loss: 1.0293493270874023\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.701619625091553 | KNN Loss: 5.661675453186035 | BCE Loss: 1.0399441719055176\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.666923522949219 | KNN Loss: 5.621335506439209 | BCE Loss: 1.0455880165100098\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.767805099487305 | KNN Loss: 5.690256118774414 | BCE Loss: 1.0775487422943115\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.764163494110107 | KNN Loss: 5.708550930023193 | BCE Loss: 1.0556126832962036\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.6697282791137695 | KNN Loss: 5.622706413269043 | BCE Loss: 1.047021746635437\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.659011363983154 | KNN Loss: 5.615455627441406 | BCE Loss: 1.0435556173324585\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.64810848236084 | KNN Loss: 5.619400978088379 | BCE Loss: 1.0287072658538818\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.727299690246582 | KNN Loss: 5.658149242401123 | BCE Loss: 1.0691503286361694\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.6475043296813965 | KNN Loss: 5.608813762664795 | BCE Loss: 1.038690447807312\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.653816223144531 | KNN Loss: 5.603165149688721 | BCE Loss: 1.0506511926651\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.7033209800720215 | KNN Loss: 5.671926498413086 | BCE Loss: 1.031394600868225\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.668723106384277 | KNN Loss: 5.612023830413818 | BCE Loss: 1.0566990375518799\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.666304111480713 | KNN Loss: 5.596905708312988 | BCE Loss: 1.0693984031677246\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.731504917144775 | KNN Loss: 5.660785675048828 | BCE Loss: 1.0707193613052368\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.718732833862305 | KNN Loss: 5.683661460876465 | BCE Loss: 1.035071611404419\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.686121940612793 | KNN Loss: 5.6690802574157715 | BCE Loss: 1.0170419216156006\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.641429424285889 | KNN Loss: 5.608386993408203 | BCE Loss: 1.033042550086975\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.747704982757568 | KNN Loss: 5.641136169433594 | BCE Loss: 1.1065688133239746\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.687427520751953 | KNN Loss: 5.637539863586426 | BCE Loss: 1.049887776374817\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.656095504760742 | KNN Loss: 5.634504318237305 | BCE Loss: 1.0215911865234375\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.64982795715332 | KNN Loss: 5.611217021942139 | BCE Loss: 1.0386111736297607\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.685481071472168 | KNN Loss: 5.639865875244141 | BCE Loss: 1.0456151962280273\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.731693267822266 | KNN Loss: 5.664974212646484 | BCE Loss: 1.0667192935943604\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.659364700317383 | KNN Loss: 5.628143310546875 | BCE Loss: 1.0312211513519287\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.66054630279541 | KNN Loss: 5.6059136390686035 | BCE Loss: 1.054632544517517\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 6.685235500335693 | KNN Loss: 5.614531517028809 | BCE Loss: 1.0707039833068848\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.6504387855529785 | KNN Loss: 5.617720127105713 | BCE Loss: 1.0327186584472656\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.675852298736572 | KNN Loss: 5.612666606903076 | BCE Loss: 1.063185691833496\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.708652973175049 | KNN Loss: 5.663513660430908 | BCE Loss: 1.0451393127441406\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.6603617668151855 | KNN Loss: 5.625953674316406 | BCE Loss: 1.0344082117080688\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.67388916015625 | KNN Loss: 5.6005425453186035 | BCE Loss: 1.0733463764190674\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.674930095672607 | KNN Loss: 5.632697582244873 | BCE Loss: 1.0422323942184448\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.668707847595215 | KNN Loss: 5.625667095184326 | BCE Loss: 1.0430407524108887\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.655816078186035 | KNN Loss: 5.594042778015137 | BCE Loss: 1.061773419380188\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.709012985229492 | KNN Loss: 5.649998664855957 | BCE Loss: 1.059014081954956\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.648945331573486 | KNN Loss: 5.5965471267700195 | BCE Loss: 1.0523982048034668\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.652681350708008 | KNN Loss: 5.592461109161377 | BCE Loss: 1.06022047996521\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.662710189819336 | KNN Loss: 5.628904819488525 | BCE Loss: 1.0338051319122314\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.679789066314697 | KNN Loss: 5.653909206390381 | BCE Loss: 1.0258798599243164\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.693022727966309 | KNN Loss: 5.62549352645874 | BCE Loss: 1.0675289630889893\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.6459808349609375 | KNN Loss: 5.594559669494629 | BCE Loss: 1.0514211654663086\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.70564079284668 | KNN Loss: 5.652706146240234 | BCE Loss: 1.0529346466064453\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.690374374389648 | KNN Loss: 5.635483264923096 | BCE Loss: 1.0548908710479736\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.693657398223877 | KNN Loss: 5.695550441741943 | BCE Loss: 0.9981068968772888\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.655797481536865 | KNN Loss: 5.61985445022583 | BCE Loss: 1.0359431505203247\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.666596412658691 | KNN Loss: 5.625895023345947 | BCE Loss: 1.040701150894165\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.6684956550598145 | KNN Loss: 5.616800308227539 | BCE Loss: 1.0516953468322754\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.6838836669921875 | KNN Loss: 5.644904136657715 | BCE Loss: 1.0389795303344727\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.654596328735352 | KNN Loss: 5.610345840454102 | BCE Loss: 1.04425048828125\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.6351423263549805 | KNN Loss: 5.607838153839111 | BCE Loss: 1.0273040533065796\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.7092204093933105 | KNN Loss: 5.632899761199951 | BCE Loss: 1.0763206481933594\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.748404502868652 | KNN Loss: 5.703718662261963 | BCE Loss: 1.0446858406066895\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.632056713104248 | KNN Loss: 5.600792407989502 | BCE Loss: 1.0312644243240356\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.717953205108643 | KNN Loss: 5.644245624542236 | BCE Loss: 1.0737075805664062\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.634235858917236 | KNN Loss: 5.609591484069824 | BCE Loss: 1.0246442556381226\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.660017490386963 | KNN Loss: 5.618647575378418 | BCE Loss: 1.041369915008545\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.66828727722168 | KNN Loss: 5.60419225692749 | BCE Loss: 1.0640947818756104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.631680488586426 | KNN Loss: 5.6056647300720215 | BCE Loss: 1.0260157585144043\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.675940990447998 | KNN Loss: 5.615682601928711 | BCE Loss: 1.0602585077285767\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.635870456695557 | KNN Loss: 5.600407123565674 | BCE Loss: 1.0354633331298828\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.744009017944336 | KNN Loss: 5.690992832183838 | BCE Loss: 1.053016185760498\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.644431114196777 | KNN Loss: 5.592933654785156 | BCE Loss: 1.051497459411621\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.630541801452637 | KNN Loss: 5.60539436340332 | BCE Loss: 1.0251474380493164\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.730459213256836 | KNN Loss: 5.694844722747803 | BCE Loss: 1.0356147289276123\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.668277740478516 | KNN Loss: 5.62092399597168 | BCE Loss: 1.0473538637161255\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.703751087188721 | KNN Loss: 5.6480021476745605 | BCE Loss: 1.0557489395141602\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.661629676818848 | KNN Loss: 5.612344741821289 | BCE Loss: 1.0492851734161377\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.651501178741455 | KNN Loss: 5.5974555015563965 | BCE Loss: 1.0540456771850586\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.778680801391602 | KNN Loss: 5.7134504318237305 | BCE Loss: 1.065230369567871\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.661963939666748 | KNN Loss: 5.623041152954102 | BCE Loss: 1.038922905921936\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.66850471496582 | KNN Loss: 5.620450019836426 | BCE Loss: 1.0480544567108154\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.674628257751465 | KNN Loss: 5.620898246765137 | BCE Loss: 1.053729772567749\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.697649002075195 | KNN Loss: 5.6062774658203125 | BCE Loss: 1.0913712978363037\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.6355390548706055 | KNN Loss: 5.608534812927246 | BCE Loss: 1.0270042419433594\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.744544982910156 | KNN Loss: 5.671270370483398 | BCE Loss: 1.0732746124267578\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.800735950469971 | KNN Loss: 5.731105804443359 | BCE Loss: 1.0696301460266113\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.6622443199157715 | KNN Loss: 5.602242946624756 | BCE Loss: 1.060001254081726\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.666053771972656 | KNN Loss: 5.633244514465332 | BCE Loss: 1.0328092575073242\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.668707847595215 | KNN Loss: 5.617596626281738 | BCE Loss: 1.051111102104187\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.6572065353393555 | KNN Loss: 5.615002632141113 | BCE Loss: 1.0422041416168213\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.657768726348877 | KNN Loss: 5.618770122528076 | BCE Loss: 1.0389984846115112\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.649879455566406 | KNN Loss: 5.637572765350342 | BCE Loss: 1.0123066902160645\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.662015914916992 | KNN Loss: 5.600865364074707 | BCE Loss: 1.0611505508422852\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.634635925292969 | KNN Loss: 5.628239154815674 | BCE Loss: 1.006397008895874\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.6251115798950195 | KNN Loss: 5.596953868865967 | BCE Loss: 1.0281575918197632\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.631753921508789 | KNN Loss: 5.604146957397461 | BCE Loss: 1.0276070833206177\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.636666774749756 | KNN Loss: 5.608525276184082 | BCE Loss: 1.0281416177749634\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.671678066253662 | KNN Loss: 5.606862545013428 | BCE Loss: 1.0648154020309448\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.670241355895996 | KNN Loss: 5.605632305145264 | BCE Loss: 1.0646092891693115\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.735163688659668 | KNN Loss: 5.671998977661133 | BCE Loss: 1.0631647109985352\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.638266563415527 | KNN Loss: 5.6002092361450195 | BCE Loss: 1.0380573272705078\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.647692680358887 | KNN Loss: 5.642127990722656 | BCE Loss: 1.005564570426941\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.615543365478516 | KNN Loss: 5.596068859100342 | BCE Loss: 1.0194745063781738\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.636274814605713 | KNN Loss: 5.596312046051025 | BCE Loss: 1.0399627685546875\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.655336380004883 | KNN Loss: 5.595201015472412 | BCE Loss: 1.0601352453231812\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.659387588500977 | KNN Loss: 5.601529598236084 | BCE Loss: 1.0578579902648926\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.686692237854004 | KNN Loss: 5.634402751922607 | BCE Loss: 1.052289366722107\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.670144557952881 | KNN Loss: 5.6274943351745605 | BCE Loss: 1.0426503419876099\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.750710487365723 | KNN Loss: 5.692015647888184 | BCE Loss: 1.058694839477539\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.650801658630371 | KNN Loss: 5.605957508087158 | BCE Loss: 1.0448439121246338\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.676312446594238 | KNN Loss: 5.630570888519287 | BCE Loss: 1.0457415580749512\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.742500305175781 | KNN Loss: 5.694902420043945 | BCE Loss: 1.0475976467132568\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.679767608642578 | KNN Loss: 5.658760070800781 | BCE Loss: 1.0210075378417969\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.677701473236084 | KNN Loss: 5.60266637802124 | BCE Loss: 1.0750350952148438\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.685795307159424 | KNN Loss: 5.616543769836426 | BCE Loss: 1.0692516565322876\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.6910624504089355 | KNN Loss: 5.628319263458252 | BCE Loss: 1.0627433061599731\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.668887138366699 | KNN Loss: 5.6042585372924805 | BCE Loss: 1.0646288394927979\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.683158874511719 | KNN Loss: 5.627927303314209 | BCE Loss: 1.0552318096160889\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.698668479919434 | KNN Loss: 5.669761657714844 | BCE Loss: 1.0289068222045898\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.637319564819336 | KNN Loss: 5.598618030548096 | BCE Loss: 1.0387015342712402\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.675402641296387 | KNN Loss: 5.615189075469971 | BCE Loss: 1.0602136850357056\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.6601762771606445 | KNN Loss: 5.597722053527832 | BCE Loss: 1.0624542236328125\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.750333786010742 | KNN Loss: 5.7126851081848145 | BCE Loss: 1.0376486778259277\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.674600601196289 | KNN Loss: 5.605340957641602 | BCE Loss: 1.0692598819732666\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.654229164123535 | KNN Loss: 5.616273403167725 | BCE Loss: 1.0379555225372314\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.679788589477539 | KNN Loss: 5.623409271240234 | BCE Loss: 1.0563794374465942\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.680021286010742 | KNN Loss: 5.629422664642334 | BCE Loss: 1.0505985021591187\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.75168514251709 | KNN Loss: 5.703036308288574 | BCE Loss: 1.0486490726470947\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.696920871734619 | KNN Loss: 5.63650369644165 | BCE Loss: 1.0604171752929688\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.640081882476807 | KNN Loss: 5.604522228240967 | BCE Loss: 1.0355596542358398\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.640442848205566 | KNN Loss: 5.601934909820557 | BCE Loss: 1.0385079383850098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.658070087432861 | KNN Loss: 5.632130146026611 | BCE Loss: 1.0259400606155396\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.688243389129639 | KNN Loss: 5.659222602844238 | BCE Loss: 1.0290207862854004\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.672596454620361 | KNN Loss: 5.618656635284424 | BCE Loss: 1.053939700126648\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.6593017578125 | KNN Loss: 5.598618030548096 | BCE Loss: 1.0606837272644043\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.634176254272461 | KNN Loss: 5.597323417663574 | BCE Loss: 1.0368525981903076\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.699206829071045 | KNN Loss: 5.653608322143555 | BCE Loss: 1.0455985069274902\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.661615371704102 | KNN Loss: 5.602289199829102 | BCE Loss: 1.0593262910842896\n",
      "Epoch   109: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.634082794189453 | KNN Loss: 5.6209187507629395 | BCE Loss: 1.0131642818450928\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.639461517333984 | KNN Loss: 5.611577033996582 | BCE Loss: 1.0278842449188232\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.692082405090332 | KNN Loss: 5.635257720947266 | BCE Loss: 1.0568245649337769\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.703630447387695 | KNN Loss: 5.641437530517578 | BCE Loss: 1.062192678451538\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.649724960327148 | KNN Loss: 5.604165077209473 | BCE Loss: 1.0455601215362549\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.644372940063477 | KNN Loss: 5.596347332000732 | BCE Loss: 1.0480256080627441\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.714571952819824 | KNN Loss: 5.6799468994140625 | BCE Loss: 1.0346252918243408\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.654936790466309 | KNN Loss: 5.612752437591553 | BCE Loss: 1.0421843528747559\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.696608543395996 | KNN Loss: 5.664212703704834 | BCE Loss: 1.032395601272583\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.748025894165039 | KNN Loss: 5.653969764709473 | BCE Loss: 1.0940558910369873\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.659740447998047 | KNN Loss: 5.626091957092285 | BCE Loss: 1.0336487293243408\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.657815456390381 | KNN Loss: 5.621856212615967 | BCE Loss: 1.035959243774414\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.677680015563965 | KNN Loss: 5.61405086517334 | BCE Loss: 1.0636290311813354\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.662540435791016 | KNN Loss: 5.603331565856934 | BCE Loss: 1.059208631515503\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.649529457092285 | KNN Loss: 5.604444980621338 | BCE Loss: 1.0450847148895264\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.659743309020996 | KNN Loss: 5.606945991516113 | BCE Loss: 1.0527970790863037\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.694276809692383 | KNN Loss: 5.645737648010254 | BCE Loss: 1.0485389232635498\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.687972068786621 | KNN Loss: 5.649205207824707 | BCE Loss: 1.0387669801712036\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.643360137939453 | KNN Loss: 5.602455139160156 | BCE Loss: 1.0409049987792969\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.689718723297119 | KNN Loss: 5.63262414932251 | BCE Loss: 1.0570945739746094\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.693289756774902 | KNN Loss: 5.646020889282227 | BCE Loss: 1.0472686290740967\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.680654525756836 | KNN Loss: 5.663041114807129 | BCE Loss: 1.0176132917404175\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.716454029083252 | KNN Loss: 5.653319835662842 | BCE Loss: 1.0631341934204102\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.725963592529297 | KNN Loss: 5.66290807723999 | BCE Loss: 1.0630557537078857\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.673064231872559 | KNN Loss: 5.628884792327881 | BCE Loss: 1.0441796779632568\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.670040130615234 | KNN Loss: 5.602335453033447 | BCE Loss: 1.067704439163208\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.736579895019531 | KNN Loss: 5.672734260559082 | BCE Loss: 1.0638456344604492\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.664118766784668 | KNN Loss: 5.612368583679199 | BCE Loss: 1.0517500638961792\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 6.647382736206055 | KNN Loss: 5.611031532287598 | BCE Loss: 1.036350965499878\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.627805233001709 | KNN Loss: 5.589871406555176 | BCE Loss: 1.0379338264465332\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.65170431137085 | KNN Loss: 5.594931125640869 | BCE Loss: 1.0567731857299805\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.646966934204102 | KNN Loss: 5.626828670501709 | BCE Loss: 1.0201380252838135\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.689919471740723 | KNN Loss: 5.656768321990967 | BCE Loss: 1.033151388168335\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.67406702041626 | KNN Loss: 5.607140064239502 | BCE Loss: 1.0669270753860474\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.687381267547607 | KNN Loss: 5.637587070465088 | BCE Loss: 1.0497941970825195\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.747135162353516 | KNN Loss: 5.681911468505859 | BCE Loss: 1.0652235746383667\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.601663112640381 | KNN Loss: 5.592515468597412 | BCE Loss: 1.0091477632522583\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 6.644335746765137 | KNN Loss: 5.606440544128418 | BCE Loss: 1.0378949642181396\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.693867206573486 | KNN Loss: 5.641772747039795 | BCE Loss: 1.0520944595336914\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.689971446990967 | KNN Loss: 5.654278755187988 | BCE Loss: 1.0356926918029785\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.6405439376831055 | KNN Loss: 5.6076507568359375 | BCE Loss: 1.032893180847168\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.663531303405762 | KNN Loss: 5.627263069152832 | BCE Loss: 1.0362682342529297\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.677822113037109 | KNN Loss: 5.642675876617432 | BCE Loss: 1.0351464748382568\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.6286516189575195 | KNN Loss: 5.625955581665039 | BCE Loss: 1.0026960372924805\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 6.754652976989746 | KNN Loss: 5.6937408447265625 | BCE Loss: 1.0609118938446045\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.625744342803955 | KNN Loss: 5.60377836227417 | BCE Loss: 1.0219658613204956\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.694709300994873 | KNN Loss: 5.649319648742676 | BCE Loss: 1.0453895330429077\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 6.657707214355469 | KNN Loss: 5.642210483551025 | BCE Loss: 1.0154966115951538\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.66574764251709 | KNN Loss: 5.600201606750488 | BCE Loss: 1.0655457973480225\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.689935207366943 | KNN Loss: 5.638012409210205 | BCE Loss: 1.0519227981567383\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.694951057434082 | KNN Loss: 5.658905982971191 | BCE Loss: 1.0360453128814697\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.635880947113037 | KNN Loss: 5.624354839324951 | BCE Loss: 1.0115259885787964\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.6750922203063965 | KNN Loss: 5.610249042510986 | BCE Loss: 1.0648431777954102\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.660297870635986 | KNN Loss: 5.609882354736328 | BCE Loss: 1.0504153966903687\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.698239803314209 | KNN Loss: 5.651462554931641 | BCE Loss: 1.046777367591858\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 6.629569053649902 | KNN Loss: 5.592555999755859 | BCE Loss: 1.037013292312622\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.672560691833496 | KNN Loss: 5.608154773712158 | BCE Loss: 1.064406156539917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.67404317855835 | KNN Loss: 5.633572101593018 | BCE Loss: 1.040471076965332\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.711287498474121 | KNN Loss: 5.689304828643799 | BCE Loss: 1.0219826698303223\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.646989822387695 | KNN Loss: 5.6071672439575195 | BCE Loss: 1.0398228168487549\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.760867118835449 | KNN Loss: 5.693462371826172 | BCE Loss: 1.0674049854278564\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.62050199508667 | KNN Loss: 5.5963134765625 | BCE Loss: 1.0241886377334595\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.706087112426758 | KNN Loss: 5.681237697601318 | BCE Loss: 1.0248496532440186\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.6842851638793945 | KNN Loss: 5.635470390319824 | BCE Loss: 1.0488150119781494\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.648004531860352 | KNN Loss: 5.608229637145996 | BCE Loss: 1.0397746562957764\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.632288932800293 | KNN Loss: 5.599431037902832 | BCE Loss: 1.0328580141067505\n",
      "Epoch   120: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.687589645385742 | KNN Loss: 5.625153064727783 | BCE Loss: 1.062436580657959\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.725656509399414 | KNN Loss: 5.615790367126465 | BCE Loss: 1.1098660230636597\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.651830196380615 | KNN Loss: 5.607666015625 | BCE Loss: 1.0441641807556152\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.707942962646484 | KNN Loss: 5.64551305770874 | BCE Loss: 1.0624299049377441\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.691230297088623 | KNN Loss: 5.622660160064697 | BCE Loss: 1.0685702562332153\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.684482097625732 | KNN Loss: 5.676197528839111 | BCE Loss: 1.008284568786621\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.676497459411621 | KNN Loss: 5.627408027648926 | BCE Loss: 1.0490891933441162\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.707953453063965 | KNN Loss: 5.632587909698486 | BCE Loss: 1.0753655433654785\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.6650004386901855 | KNN Loss: 5.610795021057129 | BCE Loss: 1.054205298423767\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.641910076141357 | KNN Loss: 5.604105472564697 | BCE Loss: 1.0378044843673706\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.700281620025635 | KNN Loss: 5.647070407867432 | BCE Loss: 1.0532112121582031\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.6797285079956055 | KNN Loss: 5.636051654815674 | BCE Loss: 1.0436770915985107\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.713994026184082 | KNN Loss: 5.6574859619140625 | BCE Loss: 1.0565078258514404\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.655156135559082 | KNN Loss: 5.595285415649414 | BCE Loss: 1.0598708391189575\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.647871017456055 | KNN Loss: 5.596357822418213 | BCE Loss: 1.0515133142471313\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.6557936668396 | KNN Loss: 5.6118879318237305 | BCE Loss: 1.0439056158065796\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.67427921295166 | KNN Loss: 5.620913505554199 | BCE Loss: 1.0533658266067505\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.674978256225586 | KNN Loss: 5.607695579528809 | BCE Loss: 1.0672829151153564\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.652854919433594 | KNN Loss: 5.604632377624512 | BCE Loss: 1.048222303390503\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.64469051361084 | KNN Loss: 5.598414421081543 | BCE Loss: 1.0462758541107178\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.629629611968994 | KNN Loss: 5.604557991027832 | BCE Loss: 1.0250715017318726\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.657536506652832 | KNN Loss: 5.635994911193848 | BCE Loss: 1.0215413570404053\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.6782355308532715 | KNN Loss: 5.62713098526001 | BCE Loss: 1.0511045455932617\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.717755317687988 | KNN Loss: 5.650965690612793 | BCE Loss: 1.0667893886566162\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.70058536529541 | KNN Loss: 5.646087646484375 | BCE Loss: 1.054497480392456\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.725069999694824 | KNN Loss: 5.669676780700684 | BCE Loss: 1.0553932189941406\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.629538536071777 | KNN Loss: 5.61072301864624 | BCE Loss: 1.018815279006958\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.654273986816406 | KNN Loss: 5.611073017120361 | BCE Loss: 1.0432010889053345\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.683471202850342 | KNN Loss: 5.618327617645264 | BCE Loss: 1.0651434659957886\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.645522594451904 | KNN Loss: 5.606081485748291 | BCE Loss: 1.0394411087036133\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.6600799560546875 | KNN Loss: 5.6258158683776855 | BCE Loss: 1.0342642068862915\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.792135238647461 | KNN Loss: 5.7195725440979 | BCE Loss: 1.072562575340271\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.656825065612793 | KNN Loss: 5.604919910430908 | BCE Loss: 1.0519052743911743\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.650603771209717 | KNN Loss: 5.591761589050293 | BCE Loss: 1.0588421821594238\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.780303001403809 | KNN Loss: 5.7232279777526855 | BCE Loss: 1.057074785232544\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.657393455505371 | KNN Loss: 5.6157402992248535 | BCE Loss: 1.0416533946990967\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.629392623901367 | KNN Loss: 5.618741512298584 | BCE Loss: 1.010650873184204\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.7444000244140625 | KNN Loss: 5.680804252624512 | BCE Loss: 1.0635960102081299\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.6723480224609375 | KNN Loss: 5.615708351135254 | BCE Loss: 1.0566399097442627\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.640867233276367 | KNN Loss: 5.596095085144043 | BCE Loss: 1.0447721481323242\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.744255065917969 | KNN Loss: 5.712609767913818 | BCE Loss: 1.03164541721344\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.651052951812744 | KNN Loss: 5.607372760772705 | BCE Loss: 1.043680191040039\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.659347057342529 | KNN Loss: 5.603128910064697 | BCE Loss: 1.056218147277832\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.666910171508789 | KNN Loss: 5.605172157287598 | BCE Loss: 1.0617380142211914\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.677315711975098 | KNN Loss: 5.627784252166748 | BCE Loss: 1.0495312213897705\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.728122711181641 | KNN Loss: 5.698578357696533 | BCE Loss: 1.0295441150665283\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.676130294799805 | KNN Loss: 5.60148811340332 | BCE Loss: 1.0746419429779053\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.6485772132873535 | KNN Loss: 5.6066789627075195 | BCE Loss: 1.0418983697891235\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.6903276443481445 | KNN Loss: 5.640713691711426 | BCE Loss: 1.0496137142181396\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.636348247528076 | KNN Loss: 5.595319747924805 | BCE Loss: 1.0410284996032715\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.665152072906494 | KNN Loss: 5.616815567016602 | BCE Loss: 1.048336386680603\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.648659706115723 | KNN Loss: 5.627945899963379 | BCE Loss: 1.0207139253616333\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.67140531539917 | KNN Loss: 5.613527297973633 | BCE Loss: 1.057878017425537\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.754995346069336 | KNN Loss: 5.690177917480469 | BCE Loss: 1.0648176670074463\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.650959491729736 | KNN Loss: 5.592747688293457 | BCE Loss: 1.0582116842269897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.704456329345703 | KNN Loss: 5.672240734100342 | BCE Loss: 1.0322155952453613\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.653114318847656 | KNN Loss: 5.595937728881836 | BCE Loss: 1.0571763515472412\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.709204196929932 | KNN Loss: 5.65390682220459 | BCE Loss: 1.0552973747253418\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.687353134155273 | KNN Loss: 5.6281633377075195 | BCE Loss: 1.0591899156570435\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 6.65064001083374 | KNN Loss: 5.596179008483887 | BCE Loss: 1.054460883140564\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.714920997619629 | KNN Loss: 5.661226749420166 | BCE Loss: 1.053694248199463\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.652503967285156 | KNN Loss: 5.608149528503418 | BCE Loss: 1.0443546772003174\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.6246256828308105 | KNN Loss: 5.599148273468018 | BCE Loss: 1.025477409362793\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.6684746742248535 | KNN Loss: 5.593377113342285 | BCE Loss: 1.0750974416732788\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.6679534912109375 | KNN Loss: 5.618769645690918 | BCE Loss: 1.0491838455200195\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.641983985900879 | KNN Loss: 5.604367256164551 | BCE Loss: 1.0376169681549072\n",
      "Epoch   131: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.66377067565918 | KNN Loss: 5.595736980438232 | BCE Loss: 1.0680339336395264\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.685309886932373 | KNN Loss: 5.631974220275879 | BCE Loss: 1.0533356666564941\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.764732360839844 | KNN Loss: 5.75116491317749 | BCE Loss: 1.013567566871643\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.693130970001221 | KNN Loss: 5.653499126434326 | BCE Loss: 1.0396318435668945\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.648428440093994 | KNN Loss: 5.5976386070251465 | BCE Loss: 1.0507898330688477\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.687185287475586 | KNN Loss: 5.637542724609375 | BCE Loss: 1.049642562866211\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.662683010101318 | KNN Loss: 5.6007256507873535 | BCE Loss: 1.0619572401046753\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.7532124519348145 | KNN Loss: 5.6810150146484375 | BCE Loss: 1.072197437286377\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.670376777648926 | KNN Loss: 5.597478866577148 | BCE Loss: 1.0728977918624878\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.638174057006836 | KNN Loss: 5.608883857727051 | BCE Loss: 1.029289960861206\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.694441795349121 | KNN Loss: 5.663793563842773 | BCE Loss: 1.0306479930877686\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.657217979431152 | KNN Loss: 5.615208625793457 | BCE Loss: 1.0420091152191162\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.695753574371338 | KNN Loss: 5.652841567993164 | BCE Loss: 1.0429118871688843\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.623727798461914 | KNN Loss: 5.596024990081787 | BCE Loss: 1.027702808380127\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.693723678588867 | KNN Loss: 5.642787456512451 | BCE Loss: 1.050936222076416\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.6256303787231445 | KNN Loss: 5.604269504547119 | BCE Loss: 1.0213606357574463\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.6413726806640625 | KNN Loss: 5.614577770233154 | BCE Loss: 1.0267949104309082\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.808030128479004 | KNN Loss: 5.734320163726807 | BCE Loss: 1.0737102031707764\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.635557174682617 | KNN Loss: 5.594184875488281 | BCE Loss: 1.0413724184036255\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.718087196350098 | KNN Loss: 5.672079563140869 | BCE Loss: 1.0460076332092285\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.635504245758057 | KNN Loss: 5.596311569213867 | BCE Loss: 1.039192795753479\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.6422529220581055 | KNN Loss: 5.614979267120361 | BCE Loss: 1.0272736549377441\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.6960906982421875 | KNN Loss: 5.617879390716553 | BCE Loss: 1.0782115459442139\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.6844892501831055 | KNN Loss: 5.621380805969238 | BCE Loss: 1.0631086826324463\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.6837334632873535 | KNN Loss: 5.617356300354004 | BCE Loss: 1.06637704372406\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.716985702514648 | KNN Loss: 5.6462082862854 | BCE Loss: 1.070777416229248\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.635767936706543 | KNN Loss: 5.601032257080078 | BCE Loss: 1.0347356796264648\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.6719818115234375 | KNN Loss: 5.6146087646484375 | BCE Loss: 1.057373285293579\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.65327787399292 | KNN Loss: 5.631027698516846 | BCE Loss: 1.0222501754760742\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.668124198913574 | KNN Loss: 5.619847774505615 | BCE Loss: 1.0482763051986694\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.621878147125244 | KNN Loss: 5.59742546081543 | BCE Loss: 1.024452805519104\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.674185752868652 | KNN Loss: 5.629082679748535 | BCE Loss: 1.0451033115386963\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.645079135894775 | KNN Loss: 5.613672256469727 | BCE Loss: 1.0314069986343384\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.730059623718262 | KNN Loss: 5.649210453033447 | BCE Loss: 1.0808491706848145\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.632219314575195 | KNN Loss: 5.622427940368652 | BCE Loss: 1.0097911357879639\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.722048759460449 | KNN Loss: 5.690385818481445 | BCE Loss: 1.0316628217697144\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.658174991607666 | KNN Loss: 5.616740703582764 | BCE Loss: 1.041434407234192\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.670456409454346 | KNN Loss: 5.596006870269775 | BCE Loss: 1.0744495391845703\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.703956127166748 | KNN Loss: 5.638424396514893 | BCE Loss: 1.0655317306518555\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.682492256164551 | KNN Loss: 5.623520374298096 | BCE Loss: 1.058971643447876\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.667606353759766 | KNN Loss: 5.596378326416016 | BCE Loss: 1.071227788925171\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.676120281219482 | KNN Loss: 5.606502532958984 | BCE Loss: 1.0696178674697876\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 6.651344299316406 | KNN Loss: 5.6033172607421875 | BCE Loss: 1.0480268001556396\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.747241973876953 | KNN Loss: 5.71047830581665 | BCE Loss: 1.0367639064788818\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.676438808441162 | KNN Loss: 5.601926803588867 | BCE Loss: 1.0745118856430054\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.684578895568848 | KNN Loss: 5.64373779296875 | BCE Loss: 1.0408411026000977\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.649319171905518 | KNN Loss: 5.59286642074585 | BCE Loss: 1.0564526319503784\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.631514549255371 | KNN Loss: 5.594328880310059 | BCE Loss: 1.0371854305267334\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.683459281921387 | KNN Loss: 5.650006294250488 | BCE Loss: 1.0334527492523193\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.634231090545654 | KNN Loss: 5.6041975021362305 | BCE Loss: 1.0300335884094238\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.641732215881348 | KNN Loss: 5.609961986541748 | BCE Loss: 1.0317699909210205\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.674717903137207 | KNN Loss: 5.6088714599609375 | BCE Loss: 1.0658464431762695\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.72981071472168 | KNN Loss: 5.661425590515137 | BCE Loss: 1.0683852434158325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.698829650878906 | KNN Loss: 5.633424758911133 | BCE Loss: 1.0654048919677734\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.625405311584473 | KNN Loss: 5.594020366668701 | BCE Loss: 1.0313847064971924\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.707432746887207 | KNN Loss: 5.647252082824707 | BCE Loss: 1.060180902481079\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.672046184539795 | KNN Loss: 5.6268157958984375 | BCE Loss: 1.0452302694320679\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.658307075500488 | KNN Loss: 5.604001522064209 | BCE Loss: 1.0543056726455688\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.6671857833862305 | KNN Loss: 5.611615180969238 | BCE Loss: 1.0555706024169922\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.750798225402832 | KNN Loss: 5.708104133605957 | BCE Loss: 1.042694091796875\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.663018226623535 | KNN Loss: 5.617671012878418 | BCE Loss: 1.0453474521636963\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.687006950378418 | KNN Loss: 5.633431911468506 | BCE Loss: 1.0535752773284912\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.687244892120361 | KNN Loss: 5.60737943649292 | BCE Loss: 1.0798654556274414\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.740835189819336 | KNN Loss: 5.714035511016846 | BCE Loss: 1.0267997980117798\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.6799235343933105 | KNN Loss: 5.633563041687012 | BCE Loss: 1.0463604927062988\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 6.678177833557129 | KNN Loss: 5.623154163360596 | BCE Loss: 1.0550236701965332\n",
      "Epoch   142: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.662995338439941 | KNN Loss: 5.621446132659912 | BCE Loss: 1.0415494441986084\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.649311065673828 | KNN Loss: 5.59931755065918 | BCE Loss: 1.0499935150146484\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.706265926361084 | KNN Loss: 5.663158893585205 | BCE Loss: 1.0431071519851685\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.6231513023376465 | KNN Loss: 5.622847080230713 | BCE Loss: 1.000304102897644\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.673214912414551 | KNN Loss: 5.6231303215026855 | BCE Loss: 1.0500848293304443\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.634838581085205 | KNN Loss: 5.610916614532471 | BCE Loss: 1.0239219665527344\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.632392883300781 | KNN Loss: 5.603368282318115 | BCE Loss: 1.029024362564087\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.696435928344727 | KNN Loss: 5.6357879638671875 | BCE Loss: 1.060647964477539\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 6.702766418457031 | KNN Loss: 5.641717910766602 | BCE Loss: 1.0610483884811401\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.682313919067383 | KNN Loss: 5.6134443283081055 | BCE Loss: 1.068869709968567\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.644808769226074 | KNN Loss: 5.5954718589782715 | BCE Loss: 1.0493370294570923\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.614958763122559 | KNN Loss: 5.593806266784668 | BCE Loss: 1.0211527347564697\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.684723854064941 | KNN Loss: 5.618910312652588 | BCE Loss: 1.065813422203064\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 6.664786338806152 | KNN Loss: 5.613578796386719 | BCE Loss: 1.051207423210144\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.651501178741455 | KNN Loss: 5.609170436859131 | BCE Loss: 1.0423306226730347\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.615597724914551 | KNN Loss: 5.602203369140625 | BCE Loss: 1.0133945941925049\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.6815690994262695 | KNN Loss: 5.605686664581299 | BCE Loss: 1.0758824348449707\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.624124526977539 | KNN Loss: 5.619406223297119 | BCE Loss: 1.0047180652618408\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.633542537689209 | KNN Loss: 5.592569828033447 | BCE Loss: 1.0409728288650513\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.6945929527282715 | KNN Loss: 5.640535831451416 | BCE Loss: 1.054057002067566\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.688867568969727 | KNN Loss: 5.653158664703369 | BCE Loss: 1.0357091426849365\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.665860176086426 | KNN Loss: 5.6175456047058105 | BCE Loss: 1.0483148097991943\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.69008207321167 | KNN Loss: 5.628356456756592 | BCE Loss: 1.0617254972457886\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 6.740272521972656 | KNN Loss: 5.675260543823242 | BCE Loss: 1.065011978149414\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.629415512084961 | KNN Loss: 5.5973286628723145 | BCE Loss: 1.0320866107940674\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.750237464904785 | KNN Loss: 5.68501091003418 | BCE Loss: 1.0652265548706055\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.6857008934021 | KNN Loss: 5.6049065589904785 | BCE Loss: 1.0807942152023315\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.671195983886719 | KNN Loss: 5.63378381729126 | BCE Loss: 1.037412166595459\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.663421630859375 | KNN Loss: 5.607535362243652 | BCE Loss: 1.055886149406433\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.657431602478027 | KNN Loss: 5.635061740875244 | BCE Loss: 1.0223701000213623\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.669321060180664 | KNN Loss: 5.611251354217529 | BCE Loss: 1.0580699443817139\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 6.6419901847839355 | KNN Loss: 5.62749719619751 | BCE Loss: 1.0144929885864258\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.643007278442383 | KNN Loss: 5.596588134765625 | BCE Loss: 1.0464192628860474\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.6498284339904785 | KNN Loss: 5.597525596618652 | BCE Loss: 1.0523028373718262\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.625350475311279 | KNN Loss: 5.589909076690674 | BCE Loss: 1.0354413986206055\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.635847091674805 | KNN Loss: 5.604708194732666 | BCE Loss: 1.0311388969421387\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.781927108764648 | KNN Loss: 5.738990783691406 | BCE Loss: 1.0429364442825317\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.664834976196289 | KNN Loss: 5.622411251068115 | BCE Loss: 1.042423963546753\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.638228416442871 | KNN Loss: 5.598893165588379 | BCE Loss: 1.0393351316452026\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.753248691558838 | KNN Loss: 5.6901702880859375 | BCE Loss: 1.0630782842636108\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.636826515197754 | KNN Loss: 5.6211347579956055 | BCE Loss: 1.0156915187835693\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.710674285888672 | KNN Loss: 5.643321514129639 | BCE Loss: 1.0673526525497437\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.654249668121338 | KNN Loss: 5.61189603805542 | BCE Loss: 1.0423537492752075\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.666723728179932 | KNN Loss: 5.630631923675537 | BCE Loss: 1.036091923713684\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.655092239379883 | KNN Loss: 5.601378440856934 | BCE Loss: 1.0537135601043701\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.680564880371094 | KNN Loss: 5.6208086013793945 | BCE Loss: 1.0597563982009888\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.6498613357543945 | KNN Loss: 5.6305413246154785 | BCE Loss: 1.0193202495574951\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.719600677490234 | KNN Loss: 5.690706253051758 | BCE Loss: 1.0288944244384766\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.648836135864258 | KNN Loss: 5.599607467651367 | BCE Loss: 1.0492284297943115\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.681248664855957 | KNN Loss: 5.6657867431640625 | BCE Loss: 1.0154621601104736\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.7329816818237305 | KNN Loss: 5.665162563323975 | BCE Loss: 1.0678192377090454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.661893844604492 | KNN Loss: 5.606217861175537 | BCE Loss: 1.055675745010376\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.6903815269470215 | KNN Loss: 5.643754959106445 | BCE Loss: 1.0466265678405762\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.752562046051025 | KNN Loss: 5.692263603210449 | BCE Loss: 1.0602985620498657\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.657244682312012 | KNN Loss: 5.609175682067871 | BCE Loss: 1.0480687618255615\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.66038703918457 | KNN Loss: 5.635445594787598 | BCE Loss: 1.0249414443969727\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.649491310119629 | KNN Loss: 5.5976338386535645 | BCE Loss: 1.0518572330474854\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.681614875793457 | KNN Loss: 5.608395099639893 | BCE Loss: 1.0732195377349854\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.6671528816223145 | KNN Loss: 5.628996849060059 | BCE Loss: 1.0381560325622559\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.683130741119385 | KNN Loss: 5.633351802825928 | BCE Loss: 1.049778938293457\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.629607200622559 | KNN Loss: 5.5997467041015625 | BCE Loss: 1.0298603773117065\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.652609825134277 | KNN Loss: 5.600924015045166 | BCE Loss: 1.0516855716705322\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.650357723236084 | KNN Loss: 5.628716468811035 | BCE Loss: 1.0216412544250488\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 6.65681266784668 | KNN Loss: 5.604559898376465 | BCE Loss: 1.0522525310516357\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.611961364746094 | KNN Loss: 5.593272686004639 | BCE Loss: 1.0186889171600342\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.695066452026367 | KNN Loss: 5.66062068939209 | BCE Loss: 1.0344457626342773\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.758203983306885 | KNN Loss: 5.688933372497559 | BCE Loss: 1.0692706108093262\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.635942459106445 | KNN Loss: 5.5934295654296875 | BCE Loss: 1.0425130128860474\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.697605133056641 | KNN Loss: 5.635566234588623 | BCE Loss: 1.0620390176773071\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.709981918334961 | KNN Loss: 5.672930717468262 | BCE Loss: 1.0370509624481201\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.698276519775391 | KNN Loss: 5.654080390930176 | BCE Loss: 1.0441962480545044\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.633389472961426 | KNN Loss: 5.591195106506348 | BCE Loss: 1.0421943664550781\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.685541152954102 | KNN Loss: 5.603428363800049 | BCE Loss: 1.0821125507354736\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.6929426193237305 | KNN Loss: 5.653017997741699 | BCE Loss: 1.0399248600006104\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.648499488830566 | KNN Loss: 5.598265647888184 | BCE Loss: 1.0502339601516724\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.678683757781982 | KNN Loss: 5.632896423339844 | BCE Loss: 1.0457872152328491\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.629303932189941 | KNN Loss: 5.591446876525879 | BCE Loss: 1.0378572940826416\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.741489410400391 | KNN Loss: 5.654613494873047 | BCE Loss: 1.0868760347366333\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.640112400054932 | KNN Loss: 5.598543167114258 | BCE Loss: 1.0415693521499634\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.698524475097656 | KNN Loss: 5.627676486968994 | BCE Loss: 1.070847988128662\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.617778778076172 | KNN Loss: 5.593170642852783 | BCE Loss: 1.0246078968048096\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.724639415740967 | KNN Loss: 5.701947212219238 | BCE Loss: 1.022692322731018\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.636020660400391 | KNN Loss: 5.597404479980469 | BCE Loss: 1.0386159420013428\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.703158855438232 | KNN Loss: 5.63087272644043 | BCE Loss: 1.0722861289978027\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.731541633605957 | KNN Loss: 5.7257981300354 | BCE Loss: 1.0057435035705566\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.657661437988281 | KNN Loss: 5.629981517791748 | BCE Loss: 1.0276798009872437\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.697505950927734 | KNN Loss: 5.663198947906494 | BCE Loss: 1.0343067646026611\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.663974761962891 | KNN Loss: 5.647743225097656 | BCE Loss: 1.0162312984466553\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.735142707824707 | KNN Loss: 5.6934285163879395 | BCE Loss: 1.0417139530181885\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.642787933349609 | KNN Loss: 5.605055809020996 | BCE Loss: 1.0377321243286133\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.683370590209961 | KNN Loss: 5.614952087402344 | BCE Loss: 1.0684187412261963\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.62653923034668 | KNN Loss: 5.597707271575928 | BCE Loss: 1.028832197189331\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.6457719802856445 | KNN Loss: 5.595600128173828 | BCE Loss: 1.0501720905303955\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 6.7296905517578125 | KNN Loss: 5.6568074226379395 | BCE Loss: 1.0728833675384521\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.6977996826171875 | KNN Loss: 5.667619228363037 | BCE Loss: 1.03018057346344\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.659997940063477 | KNN Loss: 5.605631351470947 | BCE Loss: 1.0543667078018188\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.689251899719238 | KNN Loss: 5.6321539878845215 | BCE Loss: 1.0570976734161377\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.667047500610352 | KNN Loss: 5.627651214599609 | BCE Loss: 1.0393964052200317\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.659311294555664 | KNN Loss: 5.61627197265625 | BCE Loss: 1.043039321899414\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.69892692565918 | KNN Loss: 5.614883899688721 | BCE Loss: 1.0840427875518799\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.635557651519775 | KNN Loss: 5.60875129699707 | BCE Loss: 1.026806354522705\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.657756328582764 | KNN Loss: 5.6406097412109375 | BCE Loss: 1.0171464681625366\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.637686252593994 | KNN Loss: 5.6011857986450195 | BCE Loss: 1.0365004539489746\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.642760276794434 | KNN Loss: 5.594604015350342 | BCE Loss: 1.0481561422348022\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.673585891723633 | KNN Loss: 5.60956335067749 | BCE Loss: 1.0640227794647217\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.649831295013428 | KNN Loss: 5.606354236602783 | BCE Loss: 1.0434770584106445\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.6520771980285645 | KNN Loss: 5.602107048034668 | BCE Loss: 1.0499701499938965\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.731658458709717 | KNN Loss: 5.69960355758667 | BCE Loss: 1.0320547819137573\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.713512897491455 | KNN Loss: 5.653317928314209 | BCE Loss: 1.060194969177246\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.6846771240234375 | KNN Loss: 5.617249965667725 | BCE Loss: 1.0674272775650024\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 6.661153793334961 | KNN Loss: 5.623020648956299 | BCE Loss: 1.0381333827972412\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.66567850112915 | KNN Loss: 5.611163139343262 | BCE Loss: 1.0545153617858887\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.6306071281433105 | KNN Loss: 5.61386775970459 | BCE Loss: 1.0167394876480103\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.702476501464844 | KNN Loss: 5.659976005554199 | BCE Loss: 1.0425002574920654\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.682952880859375 | KNN Loss: 5.631435394287109 | BCE Loss: 1.051517367362976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.674901962280273 | KNN Loss: 5.630231857299805 | BCE Loss: 1.0446702241897583\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.654296398162842 | KNN Loss: 5.601962566375732 | BCE Loss: 1.052333950996399\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.682249069213867 | KNN Loss: 5.6502532958984375 | BCE Loss: 1.0319960117340088\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.679445266723633 | KNN Loss: 5.625283718109131 | BCE Loss: 1.0541613101959229\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.658733367919922 | KNN Loss: 5.603339195251465 | BCE Loss: 1.055393934249878\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.667007923126221 | KNN Loss: 5.627679347991943 | BCE Loss: 1.0393284559249878\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.674077033996582 | KNN Loss: 5.599569797515869 | BCE Loss: 1.0745073556900024\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.671327590942383 | KNN Loss: 5.607838153839111 | BCE Loss: 1.063489317893982\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.60685920715332 | KNN Loss: 5.5896830558776855 | BCE Loss: 1.0171763896942139\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.62722635269165 | KNN Loss: 5.597324848175049 | BCE Loss: 1.0299015045166016\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.668781280517578 | KNN Loss: 5.614484786987305 | BCE Loss: 1.0542964935302734\n",
      "Epoch   163: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.716740608215332 | KNN Loss: 5.681565284729004 | BCE Loss: 1.0351754426956177\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.6477508544921875 | KNN Loss: 5.597056865692139 | BCE Loss: 1.0506939888000488\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.690239429473877 | KNN Loss: 5.660683631896973 | BCE Loss: 1.0295556783676147\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.657827854156494 | KNN Loss: 5.620752811431885 | BCE Loss: 1.0370749235153198\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.65180778503418 | KNN Loss: 5.621984958648682 | BCE Loss: 1.029822587966919\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.656003475189209 | KNN Loss: 5.606939315795898 | BCE Loss: 1.0490641593933105\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.617214679718018 | KNN Loss: 5.611726760864258 | BCE Loss: 1.0054879188537598\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 6.687573432922363 | KNN Loss: 5.65428352355957 | BCE Loss: 1.033289909362793\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.666611671447754 | KNN Loss: 5.608145236968994 | BCE Loss: 1.0584665536880493\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.652926445007324 | KNN Loss: 5.6034135818481445 | BCE Loss: 1.0495128631591797\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 6.782268524169922 | KNN Loss: 5.671014785766602 | BCE Loss: 1.1112539768218994\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.646679878234863 | KNN Loss: 5.595546245574951 | BCE Loss: 1.051133632659912\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.648237705230713 | KNN Loss: 5.617937088012695 | BCE Loss: 1.030300498008728\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.7361741065979 | KNN Loss: 5.646944046020508 | BCE Loss: 1.089229941368103\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.7964091300964355 | KNN Loss: 5.7354230880737305 | BCE Loss: 1.060986042022705\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.641503810882568 | KNN Loss: 5.592823505401611 | BCE Loss: 1.048680305480957\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.66079044342041 | KNN Loss: 5.643609046936035 | BCE Loss: 1.0171812772750854\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.6415486335754395 | KNN Loss: 5.610037326812744 | BCE Loss: 1.0315114259719849\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.635048866271973 | KNN Loss: 5.590649127960205 | BCE Loss: 1.0443997383117676\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.657442092895508 | KNN Loss: 5.612597465515137 | BCE Loss: 1.0448448657989502\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.660896301269531 | KNN Loss: 5.60296630859375 | BCE Loss: 1.0579297542572021\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.654831886291504 | KNN Loss: 5.604846954345703 | BCE Loss: 1.0499849319458008\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.666034698486328 | KNN Loss: 5.647911548614502 | BCE Loss: 1.0181233882904053\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.682003021240234 | KNN Loss: 5.621079444885254 | BCE Loss: 1.0609238147735596\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.622651100158691 | KNN Loss: 5.596312999725342 | BCE Loss: 1.02633798122406\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 6.675821304321289 | KNN Loss: 5.641263008117676 | BCE Loss: 1.0345581769943237\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.634696006774902 | KNN Loss: 5.59330415725708 | BCE Loss: 1.0413916110992432\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.631619453430176 | KNN Loss: 5.594960689544678 | BCE Loss: 1.036658763885498\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.66303014755249 | KNN Loss: 5.593236923217773 | BCE Loss: 1.0697932243347168\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.656521797180176 | KNN Loss: 5.612495422363281 | BCE Loss: 1.0440263748168945\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.670588493347168 | KNN Loss: 5.635944843292236 | BCE Loss: 1.0346438884735107\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.6580424308776855 | KNN Loss: 5.594332218170166 | BCE Loss: 1.063710331916809\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.630619049072266 | KNN Loss: 5.594768047332764 | BCE Loss: 1.0358507633209229\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.710137367248535 | KNN Loss: 5.653966426849365 | BCE Loss: 1.05617094039917\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.657090663909912 | KNN Loss: 5.609189510345459 | BCE Loss: 1.0479011535644531\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.733007431030273 | KNN Loss: 5.677944183349609 | BCE Loss: 1.055063009262085\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.653295040130615 | KNN Loss: 5.59674596786499 | BCE Loss: 1.056549072265625\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.65470027923584 | KNN Loss: 5.596547603607178 | BCE Loss: 1.0581529140472412\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.642528533935547 | KNN Loss: 5.588706016540527 | BCE Loss: 1.05382239818573\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.664875507354736 | KNN Loss: 5.6239423751831055 | BCE Loss: 1.0409331321716309\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.622615814208984 | KNN Loss: 5.592468738555908 | BCE Loss: 1.0301470756530762\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.7484211921691895 | KNN Loss: 5.688986301422119 | BCE Loss: 1.0594350099563599\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.650398254394531 | KNN Loss: 5.59498405456543 | BCE Loss: 1.0554139614105225\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.649500846862793 | KNN Loss: 5.611731052398682 | BCE Loss: 1.0377700328826904\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.765349864959717 | KNN Loss: 5.707230091094971 | BCE Loss: 1.0581196546554565\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 6.713925361633301 | KNN Loss: 5.646010875701904 | BCE Loss: 1.0679144859313965\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.630107402801514 | KNN Loss: 5.592898845672607 | BCE Loss: 1.0372085571289062\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 6.682156085968018 | KNN Loss: 5.632171154022217 | BCE Loss: 1.0499848127365112\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.672726631164551 | KNN Loss: 5.623392105102539 | BCE Loss: 1.0493346452713013\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.663586616516113 | KNN Loss: 5.610029220581055 | BCE Loss: 1.0535571575164795\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.713475227355957 | KNN Loss: 5.678947448730469 | BCE Loss: 1.0345277786254883\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.731146335601807 | KNN Loss: 5.683520317077637 | BCE Loss: 1.04762601852417\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.6232590675354 | KNN Loss: 5.594508647918701 | BCE Loss: 1.0287505388259888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.654973983764648 | KNN Loss: 5.608983516693115 | BCE Loss: 1.0459905862808228\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 6.742824554443359 | KNN Loss: 5.693508148193359 | BCE Loss: 1.049316644668579\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.654591083526611 | KNN Loss: 5.618764877319336 | BCE Loss: 1.035826325416565\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.7074432373046875 | KNN Loss: 5.6508283615112305 | BCE Loss: 1.0566151142120361\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.663857936859131 | KNN Loss: 5.627274990081787 | BCE Loss: 1.0365828275680542\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.692356586456299 | KNN Loss: 5.63015079498291 | BCE Loss: 1.0622057914733887\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.702936172485352 | KNN Loss: 5.620359897613525 | BCE Loss: 1.0825763940811157\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.643446922302246 | KNN Loss: 5.60283088684082 | BCE Loss: 1.0406157970428467\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.704602241516113 | KNN Loss: 5.6637444496154785 | BCE Loss: 1.0408577919006348\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.666299819946289 | KNN Loss: 5.624188423156738 | BCE Loss: 1.0421111583709717\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.677550792694092 | KNN Loss: 5.625328540802002 | BCE Loss: 1.0522223711013794\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.667430400848389 | KNN Loss: 5.654623508453369 | BCE Loss: 1.0128068923950195\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.62060022354126 | KNN Loss: 5.599074363708496 | BCE Loss: 1.0215257406234741\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.642664432525635 | KNN Loss: 5.599193572998047 | BCE Loss: 1.043470859527588\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 6.642809867858887 | KNN Loss: 5.592326641082764 | BCE Loss: 1.050483226776123\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.645156383514404 | KNN Loss: 5.607497215270996 | BCE Loss: 1.0376592874526978\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.659317970275879 | KNN Loss: 5.5974602699279785 | BCE Loss: 1.0618579387664795\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.6944355964660645 | KNN Loss: 5.653518199920654 | BCE Loss: 1.0409173965454102\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 6.671734809875488 | KNN Loss: 5.617713928222656 | BCE Loss: 1.054020881652832\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.633347034454346 | KNN Loss: 5.592506408691406 | BCE Loss: 1.0408406257629395\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.733389854431152 | KNN Loss: 5.676973342895508 | BCE Loss: 1.056416392326355\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.644464492797852 | KNN Loss: 5.595939636230469 | BCE Loss: 1.0485246181488037\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.75223445892334 | KNN Loss: 5.691464424133301 | BCE Loss: 1.0607702732086182\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.71912956237793 | KNN Loss: 5.683925628662109 | BCE Loss: 1.0352036952972412\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.676196575164795 | KNN Loss: 5.636780738830566 | BCE Loss: 1.039415717124939\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.657896041870117 | KNN Loss: 5.6185479164123535 | BCE Loss: 1.0393481254577637\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.715915679931641 | KNN Loss: 5.672015190124512 | BCE Loss: 1.043900728225708\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.672729015350342 | KNN Loss: 5.627396583557129 | BCE Loss: 1.0453325510025024\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.686333656311035 | KNN Loss: 5.631507873535156 | BCE Loss: 1.0548255443572998\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.614363670349121 | KNN Loss: 5.59229850769043 | BCE Loss: 1.022065281867981\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.663008689880371 | KNN Loss: 5.601231098175049 | BCE Loss: 1.0617774724960327\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.680354118347168 | KNN Loss: 5.625770568847656 | BCE Loss: 1.0545833110809326\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.646646499633789 | KNN Loss: 5.592041492462158 | BCE Loss: 1.0546050071716309\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.7417707443237305 | KNN Loss: 5.714930057525635 | BCE Loss: 1.0268404483795166\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.780806541442871 | KNN Loss: 5.737887382507324 | BCE Loss: 1.0429191589355469\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.665509223937988 | KNN Loss: 5.625738143920898 | BCE Loss: 1.0397708415985107\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 6.7037882804870605 | KNN Loss: 5.652591228485107 | BCE Loss: 1.0511970520019531\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 6.65164041519165 | KNN Loss: 5.5984272956848145 | BCE Loss: 1.0532132387161255\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.715035915374756 | KNN Loss: 5.656567573547363 | BCE Loss: 1.0584684610366821\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.665952205657959 | KNN Loss: 5.603052616119385 | BCE Loss: 1.0628995895385742\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.657905578613281 | KNN Loss: 5.612773895263672 | BCE Loss: 1.0451315641403198\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 6.696872711181641 | KNN Loss: 5.64949893951416 | BCE Loss: 1.0473737716674805\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.760385513305664 | KNN Loss: 5.70510721206665 | BCE Loss: 1.0552783012390137\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.774497985839844 | KNN Loss: 5.7012038230896 | BCE Loss: 1.073293924331665\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.735751628875732 | KNN Loss: 5.6714911460876465 | BCE Loss: 1.064260482788086\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.698240756988525 | KNN Loss: 5.6539130210876465 | BCE Loss: 1.044327735900879\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.642987251281738 | KNN Loss: 5.603447914123535 | BCE Loss: 1.039539098739624\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.658287048339844 | KNN Loss: 5.616041660308838 | BCE Loss: 1.042245626449585\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 6.727516174316406 | KNN Loss: 5.675737380981445 | BCE Loss: 1.051778793334961\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 6.700475692749023 | KNN Loss: 5.621894359588623 | BCE Loss: 1.0785815715789795\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.6693434715271 | KNN Loss: 5.625261306762695 | BCE Loss: 1.0440820455551147\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.665868759155273 | KNN Loss: 5.622889995574951 | BCE Loss: 1.0429786443710327\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.713018417358398 | KNN Loss: 5.66652250289917 | BCE Loss: 1.0464956760406494\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.676231861114502 | KNN Loss: 5.603509902954102 | BCE Loss: 1.0727218389511108\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.659812927246094 | KNN Loss: 5.607029914855957 | BCE Loss: 1.0527827739715576\n",
      "Epoch   181: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.645097732543945 | KNN Loss: 5.61021614074707 | BCE Loss: 1.034881353378296\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.620265483856201 | KNN Loss: 5.611718654632568 | BCE Loss: 1.0085467100143433\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.661795616149902 | KNN Loss: 5.599176406860352 | BCE Loss: 1.0626193284988403\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.643850326538086 | KNN Loss: 5.596012115478516 | BCE Loss: 1.0478384494781494\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.709211826324463 | KNN Loss: 5.65189790725708 | BCE Loss: 1.0573139190673828\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.643625259399414 | KNN Loss: 5.591190338134766 | BCE Loss: 1.0524348020553589\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.684331893920898 | KNN Loss: 5.642394065856934 | BCE Loss: 1.041938066482544\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.624683380126953 | KNN Loss: 5.591408729553223 | BCE Loss: 1.0332744121551514\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 6.766915321350098 | KNN Loss: 5.716124534606934 | BCE Loss: 1.050790786743164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.694148540496826 | KNN Loss: 5.653868198394775 | BCE Loss: 1.0402802228927612\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 6.639190673828125 | KNN Loss: 5.594297885894775 | BCE Loss: 1.0448927879333496\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.691929817199707 | KNN Loss: 5.653535842895508 | BCE Loss: 1.0383939743041992\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.645488739013672 | KNN Loss: 5.59893274307251 | BCE Loss: 1.046555995941162\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.681106090545654 | KNN Loss: 5.605096340179443 | BCE Loss: 1.0760098695755005\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.676749229431152 | KNN Loss: 5.617606163024902 | BCE Loss: 1.059142827987671\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.642585754394531 | KNN Loss: 5.596022129058838 | BCE Loss: 1.0465636253356934\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.689332008361816 | KNN Loss: 5.602673053741455 | BCE Loss: 1.0866589546203613\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.65557861328125 | KNN Loss: 5.607397079467773 | BCE Loss: 1.0481817722320557\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.7242302894592285 | KNN Loss: 5.68742561340332 | BCE Loss: 1.0368046760559082\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.702349662780762 | KNN Loss: 5.630254745483398 | BCE Loss: 1.0720950365066528\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.6810302734375 | KNN Loss: 5.62977409362793 | BCE Loss: 1.0512559413909912\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.693647861480713 | KNN Loss: 5.647350311279297 | BCE Loss: 1.046297550201416\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.613674163818359 | KNN Loss: 5.606838703155518 | BCE Loss: 1.0068352222442627\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.708939552307129 | KNN Loss: 5.663044452667236 | BCE Loss: 1.0458953380584717\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 6.643725395202637 | KNN Loss: 5.5962347984313965 | BCE Loss: 1.0474903583526611\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 6.654457092285156 | KNN Loss: 5.605939865112305 | BCE Loss: 1.0485174655914307\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 6.664793014526367 | KNN Loss: 5.640763282775879 | BCE Loss: 1.0240297317504883\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.697295665740967 | KNN Loss: 5.6427130699157715 | BCE Loss: 1.0545825958251953\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.673571586608887 | KNN Loss: 5.652718544006348 | BCE Loss: 1.020853042602539\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.698725700378418 | KNN Loss: 5.626236438751221 | BCE Loss: 1.0724895000457764\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 6.7132887840271 | KNN Loss: 5.6610870361328125 | BCE Loss: 1.0522018671035767\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.702243804931641 | KNN Loss: 5.631772041320801 | BCE Loss: 1.070472002029419\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 6.693281173706055 | KNN Loss: 5.630444526672363 | BCE Loss: 1.0628365278244019\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.654687404632568 | KNN Loss: 5.611048698425293 | BCE Loss: 1.0436387062072754\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.711731433868408 | KNN Loss: 5.645083904266357 | BCE Loss: 1.0666474103927612\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.632140159606934 | KNN Loss: 5.59596586227417 | BCE Loss: 1.0361742973327637\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.649789333343506 | KNN Loss: 5.598448276519775 | BCE Loss: 1.0513410568237305\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 6.674901008605957 | KNN Loss: 5.616269111633301 | BCE Loss: 1.0586317777633667\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.653558731079102 | KNN Loss: 5.604782581329346 | BCE Loss: 1.0487761497497559\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.6670732498168945 | KNN Loss: 5.616029262542725 | BCE Loss: 1.0510437488555908\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.666094779968262 | KNN Loss: 5.619174957275391 | BCE Loss: 1.0469200611114502\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.672420024871826 | KNN Loss: 5.6321868896484375 | BCE Loss: 1.0402330160140991\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.647655963897705 | KNN Loss: 5.59761905670166 | BCE Loss: 1.050036907196045\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.66633939743042 | KNN Loss: 5.598221302032471 | BCE Loss: 1.0681180953979492\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.630786895751953 | KNN Loss: 5.595939636230469 | BCE Loss: 1.0348471403121948\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.680647850036621 | KNN Loss: 5.619309902191162 | BCE Loss: 1.0613380670547485\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.718530654907227 | KNN Loss: 5.660173416137695 | BCE Loss: 1.0583570003509521\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.641836643218994 | KNN Loss: 5.620401859283447 | BCE Loss: 1.0214347839355469\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.714077949523926 | KNN Loss: 5.672852516174316 | BCE Loss: 1.041225552558899\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.761075496673584 | KNN Loss: 5.707075595855713 | BCE Loss: 1.0540000200271606\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.666865348815918 | KNN Loss: 5.606706619262695 | BCE Loss: 1.060158610343933\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.667209148406982 | KNN Loss: 5.623343467712402 | BCE Loss: 1.04386568069458\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.63213586807251 | KNN Loss: 5.595374584197998 | BCE Loss: 1.0367611646652222\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.712669372558594 | KNN Loss: 5.626229286193848 | BCE Loss: 1.086439847946167\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.834510326385498 | KNN Loss: 5.760093688964844 | BCE Loss: 1.0744166374206543\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.696465492248535 | KNN Loss: 5.630383014678955 | BCE Loss: 1.066082239151001\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.665355682373047 | KNN Loss: 5.6204400062561035 | BCE Loss: 1.0449156761169434\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.720666885375977 | KNN Loss: 5.671135902404785 | BCE Loss: 1.0495312213897705\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.674497604370117 | KNN Loss: 5.615046977996826 | BCE Loss: 1.0594508647918701\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.710619926452637 | KNN Loss: 5.6677656173706055 | BCE Loss: 1.0428541898727417\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.735175132751465 | KNN Loss: 5.693960666656494 | BCE Loss: 1.0412143468856812\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.6998114585876465 | KNN Loss: 5.6622467041015625 | BCE Loss: 1.0375646352767944\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.678738594055176 | KNN Loss: 5.632595062255859 | BCE Loss: 1.0461434125900269\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.661052227020264 | KNN Loss: 5.631847858428955 | BCE Loss: 1.0292043685913086\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.676271915435791 | KNN Loss: 5.610637187957764 | BCE Loss: 1.0656347274780273\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.663151741027832 | KNN Loss: 5.640478134155273 | BCE Loss: 1.0226736068725586\n",
      "Epoch   192: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.712100982666016 | KNN Loss: 5.636288166046143 | BCE Loss: 1.0758130550384521\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 6.681848526000977 | KNN Loss: 5.633777618408203 | BCE Loss: 1.0480711460113525\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.6567182540893555 | KNN Loss: 5.595551490783691 | BCE Loss: 1.061166524887085\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.73638391494751 | KNN Loss: 5.68402624130249 | BCE Loss: 1.0523576736450195\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.705385684967041 | KNN Loss: 5.638980865478516 | BCE Loss: 1.0664048194885254\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.672165870666504 | KNN Loss: 5.602936744689941 | BCE Loss: 1.069229006767273\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.695031642913818 | KNN Loss: 5.658602237701416 | BCE Loss: 1.0364294052124023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.627572059631348 | KNN Loss: 5.595192909240723 | BCE Loss: 1.032378911972046\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.663183212280273 | KNN Loss: 5.635003089904785 | BCE Loss: 1.0281801223754883\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.699776649475098 | KNN Loss: 5.630991458892822 | BCE Loss: 1.0687849521636963\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.659555435180664 | KNN Loss: 5.627314567565918 | BCE Loss: 1.032240629196167\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.647723197937012 | KNN Loss: 5.598362922668457 | BCE Loss: 1.0493602752685547\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.6737213134765625 | KNN Loss: 5.616128921508789 | BCE Loss: 1.0575923919677734\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.685408115386963 | KNN Loss: 5.64432430267334 | BCE Loss: 1.041083812713623\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.655383586883545 | KNN Loss: 5.611464500427246 | BCE Loss: 1.0439190864562988\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.647482395172119 | KNN Loss: 5.595941543579102 | BCE Loss: 1.0515408515930176\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.648582935333252 | KNN Loss: 5.59807825088501 | BCE Loss: 1.0505046844482422\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.749391555786133 | KNN Loss: 5.702918529510498 | BCE Loss: 1.0464727878570557\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.6946492195129395 | KNN Loss: 5.6547369956970215 | BCE Loss: 1.0399121046066284\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.771511077880859 | KNN Loss: 5.68051290512085 | BCE Loss: 1.0909982919692993\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.663723945617676 | KNN Loss: 5.6035919189453125 | BCE Loss: 1.0601322650909424\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.670204162597656 | KNN Loss: 5.615386962890625 | BCE Loss: 1.0548169612884521\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.641780376434326 | KNN Loss: 5.61572265625 | BCE Loss: 1.0260578393936157\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.666545867919922 | KNN Loss: 5.621708869934082 | BCE Loss: 1.0448369979858398\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 6.722959518432617 | KNN Loss: 5.6391706466674805 | BCE Loss: 1.0837887525558472\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.6507487297058105 | KNN Loss: 5.608299255371094 | BCE Loss: 1.0424493551254272\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.677617073059082 | KNN Loss: 5.595850467681885 | BCE Loss: 1.0817663669586182\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.655523300170898 | KNN Loss: 5.608222484588623 | BCE Loss: 1.0473008155822754\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 6.631750583648682 | KNN Loss: 5.604430198669434 | BCE Loss: 1.027320384979248\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.6703643798828125 | KNN Loss: 5.608485698699951 | BCE Loss: 1.0618784427642822\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 6.665034294128418 | KNN Loss: 5.599043369293213 | BCE Loss: 1.065990924835205\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.6407856941223145 | KNN Loss: 5.595258712768555 | BCE Loss: 1.0455269813537598\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.6562628746032715 | KNN Loss: 5.61525821685791 | BCE Loss: 1.0410047769546509\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.690544128417969 | KNN Loss: 5.672050476074219 | BCE Loss: 1.01849365234375\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.663464069366455 | KNN Loss: 5.607539653778076 | BCE Loss: 1.0559242963790894\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.6930036544799805 | KNN Loss: 5.6412858963012695 | BCE Loss: 1.051717758178711\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.713115692138672 | KNN Loss: 5.649431228637695 | BCE Loss: 1.0636847019195557\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.629539489746094 | KNN Loss: 5.6176018714904785 | BCE Loss: 1.0119378566741943\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.665187358856201 | KNN Loss: 5.6311564445495605 | BCE Loss: 1.0340309143066406\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.644589900970459 | KNN Loss: 5.607189178466797 | BCE Loss: 1.037400722503662\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.755563259124756 | KNN Loss: 5.698914527893066 | BCE Loss: 1.0566487312316895\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.752326011657715 | KNN Loss: 5.683462619781494 | BCE Loss: 1.0688633918762207\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.704501152038574 | KNN Loss: 5.645061492919922 | BCE Loss: 1.0594394207000732\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.678787708282471 | KNN Loss: 5.620828628540039 | BCE Loss: 1.0579590797424316\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 6.640617370605469 | KNN Loss: 5.607167720794678 | BCE Loss: 1.0334497690200806\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 6.716415882110596 | KNN Loss: 5.666840553283691 | BCE Loss: 1.0495753288269043\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.648444175720215 | KNN Loss: 5.592474460601807 | BCE Loss: 1.0559699535369873\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.6736955642700195 | KNN Loss: 5.635215759277344 | BCE Loss: 1.0384800434112549\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.70492696762085 | KNN Loss: 5.665757656097412 | BCE Loss: 1.0391693115234375\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.664850234985352 | KNN Loss: 5.619416236877441 | BCE Loss: 1.0454339981079102\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.655174255371094 | KNN Loss: 5.6086249351501465 | BCE Loss: 1.0465494394302368\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.6513566970825195 | KNN Loss: 5.606119632720947 | BCE Loss: 1.0452373027801514\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.616242408752441 | KNN Loss: 5.597336769104004 | BCE Loss: 1.018905758857727\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.706177711486816 | KNN Loss: 5.6896891593933105 | BCE Loss: 1.016488790512085\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.67109489440918 | KNN Loss: 5.625655651092529 | BCE Loss: 1.0454390048980713\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.650172233581543 | KNN Loss: 5.595767021179199 | BCE Loss: 1.0544049739837646\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 6.701413154602051 | KNN Loss: 5.623163223266602 | BCE Loss: 1.0782496929168701\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.636954307556152 | KNN Loss: 5.595143795013428 | BCE Loss: 1.0418102741241455\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.683112621307373 | KNN Loss: 5.641610145568848 | BCE Loss: 1.0415024757385254\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.701038837432861 | KNN Loss: 5.667819976806641 | BCE Loss: 1.0332189798355103\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 6.686594009399414 | KNN Loss: 5.650192737579346 | BCE Loss: 1.0364011526107788\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.6513566970825195 | KNN Loss: 5.619158744812012 | BCE Loss: 1.0321977138519287\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.654452800750732 | KNN Loss: 5.621617317199707 | BCE Loss: 1.0328354835510254\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.694471836090088 | KNN Loss: 5.639679908752441 | BCE Loss: 1.054792046546936\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.689599514007568 | KNN Loss: 5.632941246032715 | BCE Loss: 1.0566582679748535\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.65186882019043 | KNN Loss: 5.602517604827881 | BCE Loss: 1.0493509769439697\n",
      "Epoch   203: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.672637939453125 | KNN Loss: 5.606061935424805 | BCE Loss: 1.0665760040283203\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.629590034484863 | KNN Loss: 5.602289199829102 | BCE Loss: 1.0273005962371826\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 6.702179431915283 | KNN Loss: 5.66610050201416 | BCE Loss: 1.0360790491104126\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.634102821350098 | KNN Loss: 5.608615398406982 | BCE Loss: 1.0254874229431152\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 6.658662796020508 | KNN Loss: 5.596412181854248 | BCE Loss: 1.0622503757476807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 6.655445575714111 | KNN Loss: 5.625586986541748 | BCE Loss: 1.0298585891723633\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.628536224365234 | KNN Loss: 5.599133491516113 | BCE Loss: 1.0294029712677002\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.652048587799072 | KNN Loss: 5.597593307495117 | BCE Loss: 1.054455280303955\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.661632537841797 | KNN Loss: 5.63989782333374 | BCE Loss: 1.0217347145080566\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.678160667419434 | KNN Loss: 5.622011661529541 | BCE Loss: 1.0561492443084717\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 6.617369651794434 | KNN Loss: 5.591113567352295 | BCE Loss: 1.0262560844421387\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.713399887084961 | KNN Loss: 5.659091472625732 | BCE Loss: 1.0543086528778076\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.675411701202393 | KNN Loss: 5.647708415985107 | BCE Loss: 1.0277032852172852\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.687699317932129 | KNN Loss: 5.616171836853027 | BCE Loss: 1.0715277194976807\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.668482303619385 | KNN Loss: 5.605801105499268 | BCE Loss: 1.0626810789108276\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.7654595375061035 | KNN Loss: 5.711859703063965 | BCE Loss: 1.0535999536514282\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.6040143966674805 | KNN Loss: 5.590758800506592 | BCE Loss: 1.0132557153701782\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.819131851196289 | KNN Loss: 5.752657413482666 | BCE Loss: 1.066474437713623\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.624960899353027 | KNN Loss: 5.604017734527588 | BCE Loss: 1.0209429264068604\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 6.6953277587890625 | KNN Loss: 5.662552833557129 | BCE Loss: 1.0327750444412231\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 6.641967296600342 | KNN Loss: 5.593354225158691 | BCE Loss: 1.0486130714416504\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.718476295471191 | KNN Loss: 5.681814670562744 | BCE Loss: 1.0366616249084473\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.650107383728027 | KNN Loss: 5.597195625305176 | BCE Loss: 1.0529119968414307\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.64108943939209 | KNN Loss: 5.604508876800537 | BCE Loss: 1.0365808010101318\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.760395526885986 | KNN Loss: 5.680063724517822 | BCE Loss: 1.080331802368164\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.645596504211426 | KNN Loss: 5.593471050262451 | BCE Loss: 1.0521252155303955\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.648594379425049 | KNN Loss: 5.6254448890686035 | BCE Loss: 1.0231494903564453\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.632676124572754 | KNN Loss: 5.639896392822266 | BCE Loss: 0.9927794933319092\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.684421062469482 | KNN Loss: 5.62761116027832 | BCE Loss: 1.056809902191162\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.727197647094727 | KNN Loss: 5.674657344818115 | BCE Loss: 1.0525403022766113\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.646320343017578 | KNN Loss: 5.619121551513672 | BCE Loss: 1.0271989107131958\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.711422443389893 | KNN Loss: 5.652357578277588 | BCE Loss: 1.0590648651123047\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.691875457763672 | KNN Loss: 5.631689548492432 | BCE Loss: 1.0601859092712402\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.641887664794922 | KNN Loss: 5.616554260253906 | BCE Loss: 1.0253331661224365\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.678229331970215 | KNN Loss: 5.604586601257324 | BCE Loss: 1.0736428499221802\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.67108154296875 | KNN Loss: 5.595502853393555 | BCE Loss: 1.0755784511566162\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.659609317779541 | KNN Loss: 5.652412414550781 | BCE Loss: 1.0071970224380493\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.662015438079834 | KNN Loss: 5.631618976593018 | BCE Loss: 1.0303964614868164\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.639847755432129 | KNN Loss: 5.598962783813477 | BCE Loss: 1.0408849716186523\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.744568824768066 | KNN Loss: 5.6835150718688965 | BCE Loss: 1.06105375289917\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.775121688842773 | KNN Loss: 5.713830947875977 | BCE Loss: 1.0612905025482178\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.696544170379639 | KNN Loss: 5.637533664703369 | BCE Loss: 1.059010624885559\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.626966953277588 | KNN Loss: 5.597034931182861 | BCE Loss: 1.0299320220947266\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 6.674801826477051 | KNN Loss: 5.630092144012451 | BCE Loss: 1.0447094440460205\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.674152374267578 | KNN Loss: 5.593016147613525 | BCE Loss: 1.0811363458633423\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.665027618408203 | KNN Loss: 5.5980448722839355 | BCE Loss: 1.066982626914978\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.700962066650391 | KNN Loss: 5.664492130279541 | BCE Loss: 1.03646981716156\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.652519702911377 | KNN Loss: 5.626148223876953 | BCE Loss: 1.0263713598251343\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.667276859283447 | KNN Loss: 5.606434345245361 | BCE Loss: 1.0608423948287964\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.636047840118408 | KNN Loss: 5.593622207641602 | BCE Loss: 1.0424256324768066\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.719197750091553 | KNN Loss: 5.656583309173584 | BCE Loss: 1.0626143217086792\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.705726623535156 | KNN Loss: 5.632383346557617 | BCE Loss: 1.07334303855896\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.662920951843262 | KNN Loss: 5.594603538513184 | BCE Loss: 1.068317174911499\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.672457695007324 | KNN Loss: 5.628652572631836 | BCE Loss: 1.0438051223754883\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.684872150421143 | KNN Loss: 5.635614395141602 | BCE Loss: 1.049257755279541\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.659142017364502 | KNN Loss: 5.604542255401611 | BCE Loss: 1.054599642753601\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.704631805419922 | KNN Loss: 5.6739277839660645 | BCE Loss: 1.0307042598724365\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.668036460876465 | KNN Loss: 5.626561641693115 | BCE Loss: 1.0414750576019287\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.655282974243164 | KNN Loss: 5.6148200035095215 | BCE Loss: 1.0404632091522217\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.624436378479004 | KNN Loss: 5.597240924835205 | BCE Loss: 1.0271952152252197\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.6517333984375 | KNN Loss: 5.602242469787598 | BCE Loss: 1.0494911670684814\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.702683925628662 | KNN Loss: 5.663501262664795 | BCE Loss: 1.0391826629638672\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.673330307006836 | KNN Loss: 5.665069103240967 | BCE Loss: 1.0082610845565796\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.760958194732666 | KNN Loss: 5.700803756713867 | BCE Loss: 1.0601544380187988\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.664580345153809 | KNN Loss: 5.604726791381836 | BCE Loss: 1.0598533153533936\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.7110490798950195 | KNN Loss: 5.66041374206543 | BCE Loss: 1.050635576248169\n",
      "Epoch   214: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 6.621594429016113 | KNN Loss: 5.609478950500488 | BCE Loss: 1.0121155977249146\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.659711837768555 | KNN Loss: 5.607079029083252 | BCE Loss: 1.0526330471038818\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.65231990814209 | KNN Loss: 5.606994152069092 | BCE Loss: 1.0453258752822876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.657649993896484 | KNN Loss: 5.607687473297119 | BCE Loss: 1.0499622821807861\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.754766941070557 | KNN Loss: 5.709828853607178 | BCE Loss: 1.044938087463379\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.678420066833496 | KNN Loss: 5.601779937744141 | BCE Loss: 1.0766398906707764\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.679011344909668 | KNN Loss: 5.625884532928467 | BCE Loss: 1.0531266927719116\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.633805274963379 | KNN Loss: 5.612529277801514 | BCE Loss: 1.0212759971618652\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.6993584632873535 | KNN Loss: 5.666172027587891 | BCE Loss: 1.033186435699463\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.652082443237305 | KNN Loss: 5.599139213562012 | BCE Loss: 1.052943468093872\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.6958818435668945 | KNN Loss: 5.645003318786621 | BCE Loss: 1.0508787631988525\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.676232814788818 | KNN Loss: 5.625577449798584 | BCE Loss: 1.0506553649902344\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.664672374725342 | KNN Loss: 5.6219048500061035 | BCE Loss: 1.0427676439285278\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.685139179229736 | KNN Loss: 5.638120651245117 | BCE Loss: 1.0470184087753296\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.730881214141846 | KNN Loss: 5.631291389465332 | BCE Loss: 1.0995898246765137\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.649261951446533 | KNN Loss: 5.611692428588867 | BCE Loss: 1.0375696420669556\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.626760959625244 | KNN Loss: 5.592070579528809 | BCE Loss: 1.034690260887146\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.692347049713135 | KNN Loss: 5.641286849975586 | BCE Loss: 1.0510601997375488\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 6.662670135498047 | KNN Loss: 5.6075358390808105 | BCE Loss: 1.0551340579986572\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.6722002029418945 | KNN Loss: 5.606078147888184 | BCE Loss: 1.0661221742630005\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.654696464538574 | KNN Loss: 5.595543384552002 | BCE Loss: 1.0591533184051514\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.669142723083496 | KNN Loss: 5.621620178222656 | BCE Loss: 1.047522783279419\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.680390357971191 | KNN Loss: 5.647380352020264 | BCE Loss: 1.0330097675323486\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.624083518981934 | KNN Loss: 5.596729278564453 | BCE Loss: 1.027354121208191\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.684377670288086 | KNN Loss: 5.638844013214111 | BCE Loss: 1.0455334186553955\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.659813404083252 | KNN Loss: 5.616251468658447 | BCE Loss: 1.0435619354248047\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.64447546005249 | KNN Loss: 5.596260070800781 | BCE Loss: 1.0482155084609985\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.724451065063477 | KNN Loss: 5.670193672180176 | BCE Loss: 1.0542571544647217\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.626734256744385 | KNN Loss: 5.5981316566467285 | BCE Loss: 1.0286024808883667\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.692980766296387 | KNN Loss: 5.663427352905273 | BCE Loss: 1.0295531749725342\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.684024810791016 | KNN Loss: 5.615893363952637 | BCE Loss: 1.0681315660476685\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.626253604888916 | KNN Loss: 5.596127510070801 | BCE Loss: 1.0301259756088257\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.639895915985107 | KNN Loss: 5.6065354347229 | BCE Loss: 1.033360481262207\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.689297199249268 | KNN Loss: 5.639235496520996 | BCE Loss: 1.050061821937561\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.666512489318848 | KNN Loss: 5.60110330581665 | BCE Loss: 1.0654091835021973\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.722409248352051 | KNN Loss: 5.669322967529297 | BCE Loss: 1.053086280822754\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.6584062576293945 | KNN Loss: 5.624000072479248 | BCE Loss: 1.034406304359436\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 6.676667213439941 | KNN Loss: 5.643877983093262 | BCE Loss: 1.0327894687652588\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.770708084106445 | KNN Loss: 5.743157863616943 | BCE Loss: 1.027550220489502\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.688265800476074 | KNN Loss: 5.634883880615234 | BCE Loss: 1.0533819198608398\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.647784233093262 | KNN Loss: 5.6056742668151855 | BCE Loss: 1.0421100854873657\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.637049674987793 | KNN Loss: 5.593296527862549 | BCE Loss: 1.0437531471252441\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.6532368659973145 | KNN Loss: 5.633702754974365 | BCE Loss: 1.0195339918136597\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.651821136474609 | KNN Loss: 5.601873397827148 | BCE Loss: 1.0499475002288818\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.6474456787109375 | KNN Loss: 5.6011223793029785 | BCE Loss: 1.0463234186172485\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 6.735238075256348 | KNN Loss: 5.667049407958984 | BCE Loss: 1.0681889057159424\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.684574127197266 | KNN Loss: 5.669119358062744 | BCE Loss: 1.015454649925232\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.7294816970825195 | KNN Loss: 5.663289546966553 | BCE Loss: 1.0661921501159668\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.669401168823242 | KNN Loss: 5.624844074249268 | BCE Loss: 1.0445570945739746\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.6387715339660645 | KNN Loss: 5.606161594390869 | BCE Loss: 1.0326099395751953\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.688600063323975 | KNN Loss: 5.650357246398926 | BCE Loss: 1.0382428169250488\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.663833141326904 | KNN Loss: 5.613478660583496 | BCE Loss: 1.0503543615341187\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.65908145904541 | KNN Loss: 5.610986709594727 | BCE Loss: 1.048094630241394\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.658951759338379 | KNN Loss: 5.623584270477295 | BCE Loss: 1.0353676080703735\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.640993118286133 | KNN Loss: 5.616071701049805 | BCE Loss: 1.024921178817749\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.678279876708984 | KNN Loss: 5.620288848876953 | BCE Loss: 1.0579909086227417\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.664699554443359 | KNN Loss: 5.602924346923828 | BCE Loss: 1.0617749691009521\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 6.677284240722656 | KNN Loss: 5.635512351989746 | BCE Loss: 1.0417718887329102\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.682075500488281 | KNN Loss: 5.653884410858154 | BCE Loss: 1.028191328048706\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 6.658891677856445 | KNN Loss: 5.602036476135254 | BCE Loss: 1.0568552017211914\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.7323784828186035 | KNN Loss: 5.679809093475342 | BCE Loss: 1.0525693893432617\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.664907455444336 | KNN Loss: 5.625556468963623 | BCE Loss: 1.0393508672714233\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.747013092041016 | KNN Loss: 5.702073097229004 | BCE Loss: 1.0449397563934326\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.778255939483643 | KNN Loss: 5.7250213623046875 | BCE Loss: 1.053234577178955\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.68271017074585 | KNN Loss: 5.649654865264893 | BCE Loss: 1.0330554246902466\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.685251235961914 | KNN Loss: 5.644219398498535 | BCE Loss: 1.0410315990447998\n",
      "Epoch   225: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.697453498840332 | KNN Loss: 5.6459832191467285 | BCE Loss: 1.0514705181121826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.653926372528076 | KNN Loss: 5.606147766113281 | BCE Loss: 1.047778606414795\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.676407814025879 | KNN Loss: 5.63228178024292 | BCE Loss: 1.044126033782959\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.656899929046631 | KNN Loss: 5.595702648162842 | BCE Loss: 1.0611974000930786\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.660134315490723 | KNN Loss: 5.617269515991211 | BCE Loss: 1.0428646802902222\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.662383079528809 | KNN Loss: 5.634780406951904 | BCE Loss: 1.0276026725769043\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.758236408233643 | KNN Loss: 5.689918041229248 | BCE Loss: 1.0683183670043945\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 6.651419639587402 | KNN Loss: 5.6304168701171875 | BCE Loss: 1.0210027694702148\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 6.6547675132751465 | KNN Loss: 5.621790409088135 | BCE Loss: 1.0329771041870117\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.6988701820373535 | KNN Loss: 5.665838241577148 | BCE Loss: 1.033031940460205\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.651337623596191 | KNN Loss: 5.596070289611816 | BCE Loss: 1.055267572402954\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 6.734427452087402 | KNN Loss: 5.682512283325195 | BCE Loss: 1.0519154071807861\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.665553569793701 | KNN Loss: 5.628541946411133 | BCE Loss: 1.0370116233825684\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.655155181884766 | KNN Loss: 5.59715461730957 | BCE Loss: 1.0580004453659058\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.658312797546387 | KNN Loss: 5.622119903564453 | BCE Loss: 1.0361926555633545\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.709965705871582 | KNN Loss: 5.670595645904541 | BCE Loss: 1.0393699407577515\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.697979927062988 | KNN Loss: 5.65009069442749 | BCE Loss: 1.047888994216919\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.6575469970703125 | KNN Loss: 5.61693000793457 | BCE Loss: 1.0406169891357422\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.640582084655762 | KNN Loss: 5.596599102020264 | BCE Loss: 1.043982744216919\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.6481523513793945 | KNN Loss: 5.600536346435547 | BCE Loss: 1.047615885734558\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 6.67115592956543 | KNN Loss: 5.645569324493408 | BCE Loss: 1.0255863666534424\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.6872076988220215 | KNN Loss: 5.627596378326416 | BCE Loss: 1.0596113204956055\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.621991157531738 | KNN Loss: 5.599303722381592 | BCE Loss: 1.022687554359436\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.628541946411133 | KNN Loss: 5.597182750701904 | BCE Loss: 1.0313594341278076\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.719590663909912 | KNN Loss: 5.674686431884766 | BCE Loss: 1.0449042320251465\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.684911727905273 | KNN Loss: 5.649670124053955 | BCE Loss: 1.0352413654327393\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.745680809020996 | KNN Loss: 5.662229061126709 | BCE Loss: 1.0834516286849976\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 6.651470184326172 | KNN Loss: 5.592433452606201 | BCE Loss: 1.0590367317199707\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.6924309730529785 | KNN Loss: 5.660207271575928 | BCE Loss: 1.0322238206863403\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.710092544555664 | KNN Loss: 5.671885967254639 | BCE Loss: 1.0382068157196045\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.683333396911621 | KNN Loss: 5.622350692749023 | BCE Loss: 1.060982584953308\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.662281036376953 | KNN Loss: 5.601042747497559 | BCE Loss: 1.0612382888793945\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.6686906814575195 | KNN Loss: 5.601025581359863 | BCE Loss: 1.0676651000976562\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.718701362609863 | KNN Loss: 5.667018413543701 | BCE Loss: 1.051682710647583\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.646413803100586 | KNN Loss: 5.602460861206055 | BCE Loss: 1.0439530611038208\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.645514488220215 | KNN Loss: 5.607831001281738 | BCE Loss: 1.0376832485198975\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.702747344970703 | KNN Loss: 5.647847652435303 | BCE Loss: 1.0548999309539795\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.62987756729126 | KNN Loss: 5.595454216003418 | BCE Loss: 1.0344233512878418\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.662469387054443 | KNN Loss: 5.611523151397705 | BCE Loss: 1.0509462356567383\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.669029235839844 | KNN Loss: 5.626523494720459 | BCE Loss: 1.0425057411193848\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.67549991607666 | KNN Loss: 5.609590530395508 | BCE Loss: 1.065909504890442\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 6.650290489196777 | KNN Loss: 5.619859218597412 | BCE Loss: 1.0304312705993652\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.638819217681885 | KNN Loss: 5.598738193511963 | BCE Loss: 1.0400811433792114\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.638399124145508 | KNN Loss: 5.613844394683838 | BCE Loss: 1.02455472946167\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.72612190246582 | KNN Loss: 5.660351276397705 | BCE Loss: 1.0657706260681152\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.660455703735352 | KNN Loss: 5.600878715515137 | BCE Loss: 1.0595767498016357\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.6636152267456055 | KNN Loss: 5.604526996612549 | BCE Loss: 1.0590884685516357\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.657444000244141 | KNN Loss: 5.599217891693115 | BCE Loss: 1.0582261085510254\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.643614768981934 | KNN Loss: 5.610662460327148 | BCE Loss: 1.0329525470733643\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.660709381103516 | KNN Loss: 5.638064861297607 | BCE Loss: 1.022644281387329\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.67450475692749 | KNN Loss: 5.626040935516357 | BCE Loss: 1.0484638214111328\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.641103744506836 | KNN Loss: 5.621913433074951 | BCE Loss: 1.0191900730133057\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.725226879119873 | KNN Loss: 5.660435676574707 | BCE Loss: 1.064791202545166\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.712778091430664 | KNN Loss: 5.631772041320801 | BCE Loss: 1.0810060501098633\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.628803253173828 | KNN Loss: 5.603346824645996 | BCE Loss: 1.0254565477371216\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.620209217071533 | KNN Loss: 5.607906818389893 | BCE Loss: 1.0123023986816406\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.711600303649902 | KNN Loss: 5.652300834655762 | BCE Loss: 1.0592994689941406\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.6402387619018555 | KNN Loss: 5.619227886199951 | BCE Loss: 1.0210111141204834\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.694265365600586 | KNN Loss: 5.657063961029053 | BCE Loss: 1.037201166152954\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.632373809814453 | KNN Loss: 5.601620674133301 | BCE Loss: 1.0307531356811523\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.685266494750977 | KNN Loss: 5.626454830169678 | BCE Loss: 1.0588117837905884\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.639167308807373 | KNN Loss: 5.597334861755371 | BCE Loss: 1.041832447052002\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.688403129577637 | KNN Loss: 5.645214557647705 | BCE Loss: 1.0431883335113525\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.729643821716309 | KNN Loss: 5.682187557220459 | BCE Loss: 1.0474562644958496\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.61756706237793 | KNN Loss: 5.59005880355835 | BCE Loss: 1.027508020401001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.696529388427734 | KNN Loss: 5.639762878417969 | BCE Loss: 1.0567665100097656\n",
      "Epoch   236: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.649460315704346 | KNN Loss: 5.628605842590332 | BCE Loss: 1.0208545923233032\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.685639381408691 | KNN Loss: 5.610720157623291 | BCE Loss: 1.0749194622039795\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.641822338104248 | KNN Loss: 5.592301845550537 | BCE Loss: 1.0495203733444214\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.626049041748047 | KNN Loss: 5.608924865722656 | BCE Loss: 1.017124056816101\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.674127578735352 | KNN Loss: 5.6397809982299805 | BCE Loss: 1.0343466997146606\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 6.650354862213135 | KNN Loss: 5.605289459228516 | BCE Loss: 1.0450654029846191\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.648110389709473 | KNN Loss: 5.598254203796387 | BCE Loss: 1.0498559474945068\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 6.662115097045898 | KNN Loss: 5.616056442260742 | BCE Loss: 1.0460586547851562\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.648099899291992 | KNN Loss: 5.602233409881592 | BCE Loss: 1.0458667278289795\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.717043876647949 | KNN Loss: 5.688408374786377 | BCE Loss: 1.0286352634429932\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.717010974884033 | KNN Loss: 5.654329776763916 | BCE Loss: 1.0626813173294067\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.80124568939209 | KNN Loss: 5.7116827964782715 | BCE Loss: 1.0895626544952393\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.641783714294434 | KNN Loss: 5.593879699707031 | BCE Loss: 1.0479037761688232\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.716943264007568 | KNN Loss: 5.654684066772461 | BCE Loss: 1.0622590780258179\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.741263389587402 | KNN Loss: 5.659844398498535 | BCE Loss: 1.0814188718795776\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.682961463928223 | KNN Loss: 5.620177268981934 | BCE Loss: 1.0627840757369995\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.632566452026367 | KNN Loss: 5.605301380157471 | BCE Loss: 1.0272650718688965\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.674142837524414 | KNN Loss: 5.648495197296143 | BCE Loss: 1.0256478786468506\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.665072917938232 | KNN Loss: 5.623001575469971 | BCE Loss: 1.0420712232589722\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.623064994812012 | KNN Loss: 5.601018905639648 | BCE Loss: 1.0220463275909424\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.668844699859619 | KNN Loss: 5.652027130126953 | BCE Loss: 1.0168174505233765\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.623032093048096 | KNN Loss: 5.599052429199219 | BCE Loss: 1.023979663848877\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.686648845672607 | KNN Loss: 5.651160717010498 | BCE Loss: 1.0354881286621094\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.654074668884277 | KNN Loss: 5.626341819763184 | BCE Loss: 1.0277330875396729\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.6456217765808105 | KNN Loss: 5.601916790008545 | BCE Loss: 1.0437049865722656\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.699117660522461 | KNN Loss: 5.634932994842529 | BCE Loss: 1.0641844272613525\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.6771955490112305 | KNN Loss: 5.630936622619629 | BCE Loss: 1.0462589263916016\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.633424282073975 | KNN Loss: 5.60366153717041 | BCE Loss: 1.029762864112854\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.721068859100342 | KNN Loss: 5.674200534820557 | BCE Loss: 1.0468682050704956\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.758634567260742 | KNN Loss: 5.704641342163086 | BCE Loss: 1.0539932250976562\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.64594030380249 | KNN Loss: 5.604199409484863 | BCE Loss: 1.0417410135269165\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 6.660797119140625 | KNN Loss: 5.60612678527832 | BCE Loss: 1.0546705722808838\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.641724586486816 | KNN Loss: 5.595550060272217 | BCE Loss: 1.0461745262145996\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.663636207580566 | KNN Loss: 5.603238582611084 | BCE Loss: 1.0603973865509033\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.629595756530762 | KNN Loss: 5.6006059646606445 | BCE Loss: 1.028989553451538\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.683162689208984 | KNN Loss: 5.654409408569336 | BCE Loss: 1.0287530422210693\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.7401204109191895 | KNN Loss: 5.680582523345947 | BCE Loss: 1.0595378875732422\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.637204647064209 | KNN Loss: 5.594127655029297 | BCE Loss: 1.0430771112442017\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.671954154968262 | KNN Loss: 5.592618942260742 | BCE Loss: 1.0793354511260986\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.722330570220947 | KNN Loss: 5.677102565765381 | BCE Loss: 1.0452280044555664\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.712357521057129 | KNN Loss: 5.6767096519470215 | BCE Loss: 1.0356476306915283\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.650285720825195 | KNN Loss: 5.628819942474365 | BCE Loss: 1.0214660167694092\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.66121768951416 | KNN Loss: 5.597803592681885 | BCE Loss: 1.0634139776229858\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.737648010253906 | KNN Loss: 5.682032108306885 | BCE Loss: 1.0556156635284424\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.670845031738281 | KNN Loss: 5.611578464508057 | BCE Loss: 1.059266448020935\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.686897277832031 | KNN Loss: 5.634432315826416 | BCE Loss: 1.0524652004241943\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.665579795837402 | KNN Loss: 5.6371307373046875 | BCE Loss: 1.028449296951294\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.642319679260254 | KNN Loss: 5.601692199707031 | BCE Loss: 1.0406272411346436\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.664251327514648 | KNN Loss: 5.6214752197265625 | BCE Loss: 1.042776346206665\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.614096164703369 | KNN Loss: 5.592635631561279 | BCE Loss: 1.0214605331420898\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.647740364074707 | KNN Loss: 5.601417541503906 | BCE Loss: 1.0463228225708008\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.647449493408203 | KNN Loss: 5.60181188583374 | BCE Loss: 1.045637845993042\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.673735618591309 | KNN Loss: 5.5957932472229 | BCE Loss: 1.0779423713684082\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.724377155303955 | KNN Loss: 5.670931816101074 | BCE Loss: 1.0534453392028809\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.654179573059082 | KNN Loss: 5.597780227661133 | BCE Loss: 1.0563995838165283\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.653346538543701 | KNN Loss: 5.603785991668701 | BCE Loss: 1.0495606660842896\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.635291576385498 | KNN Loss: 5.599027633666992 | BCE Loss: 1.0362640619277954\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.6768574714660645 | KNN Loss: 5.629191875457764 | BCE Loss: 1.0476655960083008\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.676595687866211 | KNN Loss: 5.624950408935547 | BCE Loss: 1.0516455173492432\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.683047294616699 | KNN Loss: 5.64604377746582 | BCE Loss: 1.0370036363601685\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.637474060058594 | KNN Loss: 5.596023082733154 | BCE Loss: 1.0414509773254395\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.650032043457031 | KNN Loss: 5.602757453918457 | BCE Loss: 1.0472743511199951\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.618402481079102 | KNN Loss: 5.593022346496582 | BCE Loss: 1.0253803730010986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.6576457023620605 | KNN Loss: 5.5944600105285645 | BCE Loss: 1.0631855726242065\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.663517951965332 | KNN Loss: 5.638101577758789 | BCE Loss: 1.025416612625122\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.626543045043945 | KNN Loss: 5.609468936920166 | BCE Loss: 1.0170741081237793\n",
      "Epoch   247: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.650997638702393 | KNN Loss: 5.62204647064209 | BCE Loss: 1.0289510488510132\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.646271705627441 | KNN Loss: 5.599305152893066 | BCE Loss: 1.046966314315796\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 6.669759750366211 | KNN Loss: 5.625309467315674 | BCE Loss: 1.0444505214691162\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.7035112380981445 | KNN Loss: 5.647756576538086 | BCE Loss: 1.0557544231414795\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.6341447830200195 | KNN Loss: 5.601439952850342 | BCE Loss: 1.0327048301696777\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.656554222106934 | KNN Loss: 5.6222100257873535 | BCE Loss: 1.0343444347381592\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.730445384979248 | KNN Loss: 5.6603522300720215 | BCE Loss: 1.070093035697937\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.661196231842041 | KNN Loss: 5.599626064300537 | BCE Loss: 1.061570167541504\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.688840866088867 | KNN Loss: 5.637722015380859 | BCE Loss: 1.0511186122894287\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.633973121643066 | KNN Loss: 5.615817546844482 | BCE Loss: 1.0181553363800049\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.637873649597168 | KNN Loss: 5.605285167694092 | BCE Loss: 1.0325887203216553\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.666411876678467 | KNN Loss: 5.596452236175537 | BCE Loss: 1.0699595212936401\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.682840347290039 | KNN Loss: 5.664399147033691 | BCE Loss: 1.0184409618377686\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.647881507873535 | KNN Loss: 5.60735559463501 | BCE Loss: 1.0405261516571045\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.677746772766113 | KNN Loss: 5.603769779205322 | BCE Loss: 1.0739768743515015\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.703444480895996 | KNN Loss: 5.6224236488342285 | BCE Loss: 1.0810205936431885\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.659222602844238 | KNN Loss: 5.610628128051758 | BCE Loss: 1.048594355583191\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.667095184326172 | KNN Loss: 5.613504886627197 | BCE Loss: 1.0535905361175537\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.697408676147461 | KNN Loss: 5.667056560516357 | BCE Loss: 1.0303518772125244\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.6852617263793945 | KNN Loss: 5.643149375915527 | BCE Loss: 1.0421123504638672\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.59036111831665 | KNN Loss: 5.599117755889893 | BCE Loss: 0.9912432432174683\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.642269134521484 | KNN Loss: 5.602593898773193 | BCE Loss: 1.039674997329712\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.674660682678223 | KNN Loss: 5.638017654418945 | BCE Loss: 1.0366432666778564\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.6937971115112305 | KNN Loss: 5.6165313720703125 | BCE Loss: 1.077265977859497\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.7068634033203125 | KNN Loss: 5.647504806518555 | BCE Loss: 1.0593585968017578\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.680722236633301 | KNN Loss: 5.662118434906006 | BCE Loss: 1.0186035633087158\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.6261162757873535 | KNN Loss: 5.599725723266602 | BCE Loss: 1.0263904333114624\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.694099426269531 | KNN Loss: 5.63587760925293 | BCE Loss: 1.058221697807312\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.7178425788879395 | KNN Loss: 5.639225006103516 | BCE Loss: 1.0786174535751343\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.645661354064941 | KNN Loss: 5.6150803565979 | BCE Loss: 1.030580759048462\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.697236061096191 | KNN Loss: 5.626218795776367 | BCE Loss: 1.0710173845291138\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.636118412017822 | KNN Loss: 5.607996463775635 | BCE Loss: 1.0281219482421875\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.652168273925781 | KNN Loss: 5.624100208282471 | BCE Loss: 1.0280680656433105\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.639575004577637 | KNN Loss: 5.605206489562988 | BCE Loss: 1.0343685150146484\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.675388336181641 | KNN Loss: 5.618389129638672 | BCE Loss: 1.0569989681243896\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 6.715559005737305 | KNN Loss: 5.637342929840088 | BCE Loss: 1.0782160758972168\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.660465240478516 | KNN Loss: 5.605963230133057 | BCE Loss: 1.0545017719268799\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.6704912185668945 | KNN Loss: 5.608959197998047 | BCE Loss: 1.0615320205688477\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.703929901123047 | KNN Loss: 5.63494348526001 | BCE Loss: 1.068986177444458\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.665020942687988 | KNN Loss: 5.605181694030762 | BCE Loss: 1.0598392486572266\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.636110305786133 | KNN Loss: 5.6181721687316895 | BCE Loss: 1.0179381370544434\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.656826019287109 | KNN Loss: 5.605733871459961 | BCE Loss: 1.0510919094085693\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.645313739776611 | KNN Loss: 5.595783710479736 | BCE Loss: 1.0495301485061646\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.63795804977417 | KNN Loss: 5.597070693969727 | BCE Loss: 1.040887475013733\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.645981788635254 | KNN Loss: 5.603940486907959 | BCE Loss: 1.042041540145874\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.670576095581055 | KNN Loss: 5.598406791687012 | BCE Loss: 1.0721690654754639\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.639197826385498 | KNN Loss: 5.595282554626465 | BCE Loss: 1.0439153909683228\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.663448333740234 | KNN Loss: 5.614758014678955 | BCE Loss: 1.0486900806427002\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.668607711791992 | KNN Loss: 5.646790981292725 | BCE Loss: 1.0218164920806885\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.671075344085693 | KNN Loss: 5.61942195892334 | BCE Loss: 1.051653504371643\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.661688804626465 | KNN Loss: 5.62610387802124 | BCE Loss: 1.0355850458145142\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.7289934158325195 | KNN Loss: 5.6787028312683105 | BCE Loss: 1.050290822982788\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.656657695770264 | KNN Loss: 5.60237455368042 | BCE Loss: 1.0542831420898438\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.702133655548096 | KNN Loss: 5.663126468658447 | BCE Loss: 1.0390071868896484\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.70719575881958 | KNN Loss: 5.647334098815918 | BCE Loss: 1.0598617792129517\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.6633710861206055 | KNN Loss: 5.592892646789551 | BCE Loss: 1.0704785585403442\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.65445613861084 | KNN Loss: 5.610424995422363 | BCE Loss: 1.0440311431884766\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.739137649536133 | KNN Loss: 5.708161354064941 | BCE Loss: 1.0309762954711914\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.651947021484375 | KNN Loss: 5.600144386291504 | BCE Loss: 1.051802635192871\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.691751480102539 | KNN Loss: 5.647127151489258 | BCE Loss: 1.0446243286132812\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.709809303283691 | KNN Loss: 5.649195194244385 | BCE Loss: 1.0606141090393066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.667695045471191 | KNN Loss: 5.6216912269592285 | BCE Loss: 1.0460035800933838\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.692686080932617 | KNN Loss: 5.645289897918701 | BCE Loss: 1.047395944595337\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.649008274078369 | KNN Loss: 5.617607116699219 | BCE Loss: 1.0314011573791504\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.6841535568237305 | KNN Loss: 5.596723556518555 | BCE Loss: 1.0874302387237549\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.634777069091797 | KNN Loss: 5.604469299316406 | BCE Loss: 1.0303077697753906\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.67627477645874 | KNN Loss: 5.621228218078613 | BCE Loss: 1.0550464391708374\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.69715690612793 | KNN Loss: 5.625401020050049 | BCE Loss: 1.0717560052871704\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.637969017028809 | KNN Loss: 5.601071357727051 | BCE Loss: 1.0368974208831787\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.71348237991333 | KNN Loss: 5.654999732971191 | BCE Loss: 1.0584826469421387\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.651178359985352 | KNN Loss: 5.6181182861328125 | BCE Loss: 1.03305983543396\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.649964332580566 | KNN Loss: 5.628081321716309 | BCE Loss: 1.021883249282837\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.682328224182129 | KNN Loss: 5.6426591873168945 | BCE Loss: 1.0396692752838135\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.63140869140625 | KNN Loss: 5.597745895385742 | BCE Loss: 1.0336626768112183\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.643495082855225 | KNN Loss: 5.60294246673584 | BCE Loss: 1.0405526161193848\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.653805732727051 | KNN Loss: 5.60339879989624 | BCE Loss: 1.0504069328308105\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.716488361358643 | KNN Loss: 5.641717910766602 | BCE Loss: 1.074770450592041\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.682896614074707 | KNN Loss: 5.61956262588501 | BCE Loss: 1.0633342266082764\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.676695823669434 | KNN Loss: 5.633674144744873 | BCE Loss: 1.0430216789245605\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.6412858963012695 | KNN Loss: 5.614799499511719 | BCE Loss: 1.0264863967895508\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.711199760437012 | KNN Loss: 5.652684688568115 | BCE Loss: 1.0585153102874756\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.642483234405518 | KNN Loss: 5.603385925292969 | BCE Loss: 1.0390973091125488\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.63746976852417 | KNN Loss: 5.614932537078857 | BCE Loss: 1.022537112236023\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.6633501052856445 | KNN Loss: 5.609231472015381 | BCE Loss: 1.0541187524795532\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.6894378662109375 | KNN Loss: 5.636891841888428 | BCE Loss: 1.0525460243225098\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.706714153289795 | KNN Loss: 5.653716087341309 | BCE Loss: 1.0529979467391968\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.682005405426025 | KNN Loss: 5.6131439208984375 | BCE Loss: 1.068861484527588\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.63252592086792 | KNN Loss: 5.607021808624268 | BCE Loss: 1.0255041122436523\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.625832557678223 | KNN Loss: 5.59259557723999 | BCE Loss: 1.033237099647522\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.6728515625 | KNN Loss: 5.629055976867676 | BCE Loss: 1.0437955856323242\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.635127067565918 | KNN Loss: 5.5938005447387695 | BCE Loss: 1.0413267612457275\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.740197658538818 | KNN Loss: 5.654056549072266 | BCE Loss: 1.0861409902572632\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.729783058166504 | KNN Loss: 5.703726291656494 | BCE Loss: 1.0260565280914307\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.62211799621582 | KNN Loss: 5.592779636383057 | BCE Loss: 1.0293381214141846\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.63332462310791 | KNN Loss: 5.598774433135986 | BCE Loss: 1.0345501899719238\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.731438636779785 | KNN Loss: 5.651928901672363 | BCE Loss: 1.0795097351074219\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.714156150817871 | KNN Loss: 5.667420864105225 | BCE Loss: 1.0467350482940674\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.662705898284912 | KNN Loss: 5.611677169799805 | BCE Loss: 1.0510287284851074\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.707417011260986 | KNN Loss: 5.667814254760742 | BCE Loss: 1.0396027565002441\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.673310279846191 | KNN Loss: 5.6495208740234375 | BCE Loss: 1.0237895250320435\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.7056427001953125 | KNN Loss: 5.651343822479248 | BCE Loss: 1.054298758506775\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.649089813232422 | KNN Loss: 5.602032661437988 | BCE Loss: 1.0470573902130127\n",
      "Epoch   264: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.673588275909424 | KNN Loss: 5.626804828643799 | BCE Loss: 1.046783447265625\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.697851181030273 | KNN Loss: 5.6482954025268555 | BCE Loss: 1.0495556592941284\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.664778709411621 | KNN Loss: 5.602915287017822 | BCE Loss: 1.0618631839752197\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.615523338317871 | KNN Loss: 5.593196868896484 | BCE Loss: 1.0223262310028076\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.660881996154785 | KNN Loss: 5.609858512878418 | BCE Loss: 1.051023244857788\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.720170974731445 | KNN Loss: 5.658143997192383 | BCE Loss: 1.0620272159576416\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.675088882446289 | KNN Loss: 5.628094673156738 | BCE Loss: 1.0469939708709717\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.667231559753418 | KNN Loss: 5.643459796905518 | BCE Loss: 1.0237715244293213\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.673192024230957 | KNN Loss: 5.615041255950928 | BCE Loss: 1.0581510066986084\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.645972728729248 | KNN Loss: 5.600493431091309 | BCE Loss: 1.0454792976379395\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.744392395019531 | KNN Loss: 5.65031623840332 | BCE Loss: 1.09407639503479\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.694290637969971 | KNN Loss: 5.635743141174316 | BCE Loss: 1.0585474967956543\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.660427570343018 | KNN Loss: 5.6290283203125 | BCE Loss: 1.031399130821228\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.639888763427734 | KNN Loss: 5.589328289031982 | BCE Loss: 1.0505602359771729\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.646241188049316 | KNN Loss: 5.603754043579102 | BCE Loss: 1.0424869060516357\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.6623077392578125 | KNN Loss: 5.636455535888672 | BCE Loss: 1.0258522033691406\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.707508087158203 | KNN Loss: 5.636974811553955 | BCE Loss: 1.0705335140228271\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.625227928161621 | KNN Loss: 5.594942092895508 | BCE Loss: 1.0302860736846924\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.669696807861328 | KNN Loss: 5.633380889892578 | BCE Loss: 1.036316156387329\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.709194183349609 | KNN Loss: 5.641189098358154 | BCE Loss: 1.068004846572876\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.660281658172607 | KNN Loss: 5.613398551940918 | BCE Loss: 1.046883225440979\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.6500420570373535 | KNN Loss: 5.621733665466309 | BCE Loss: 1.0283085107803345\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.670827388763428 | KNN Loss: 5.635648727416992 | BCE Loss: 1.035178780555725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.664780616760254 | KNN Loss: 5.627842426300049 | BCE Loss: 1.0369383096694946\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 6.676797866821289 | KNN Loss: 5.615804195404053 | BCE Loss: 1.0609939098358154\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.64719295501709 | KNN Loss: 5.5883612632751465 | BCE Loss: 1.0588319301605225\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.666759967803955 | KNN Loss: 5.632730484008789 | BCE Loss: 1.034029483795166\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.625604629516602 | KNN Loss: 5.595890998840332 | BCE Loss: 1.0297133922576904\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.802513122558594 | KNN Loss: 5.718184947967529 | BCE Loss: 1.0843284130096436\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.638161659240723 | KNN Loss: 5.5976948738098145 | BCE Loss: 1.0404669046401978\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 6.642302989959717 | KNN Loss: 5.595124244689941 | BCE Loss: 1.0471787452697754\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.771618843078613 | KNN Loss: 5.719684600830078 | BCE Loss: 1.051934003829956\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 6.693889141082764 | KNN Loss: 5.605366230010986 | BCE Loss: 1.0885227918624878\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.637356281280518 | KNN Loss: 5.599209785461426 | BCE Loss: 1.0381463766098022\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.6793317794799805 | KNN Loss: 5.631401062011719 | BCE Loss: 1.0479308366775513\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 6.6268463134765625 | KNN Loss: 5.609295845031738 | BCE Loss: 1.0175507068634033\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.721738815307617 | KNN Loss: 5.660453796386719 | BCE Loss: 1.061285138130188\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.731109142303467 | KNN Loss: 5.675540924072266 | BCE Loss: 1.0555680990219116\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.666584014892578 | KNN Loss: 5.618999481201172 | BCE Loss: 1.0475842952728271\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.781930923461914 | KNN Loss: 5.749492168426514 | BCE Loss: 1.03243887424469\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 6.636516094207764 | KNN Loss: 5.604517936706543 | BCE Loss: 1.0319981575012207\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.7010931968688965 | KNN Loss: 5.666816711425781 | BCE Loss: 1.0342763662338257\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.7214179039001465 | KNN Loss: 5.658603668212891 | BCE Loss: 1.0628143548965454\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.720054626464844 | KNN Loss: 5.6559953689575195 | BCE Loss: 1.0640593767166138\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.684476852416992 | KNN Loss: 5.6189751625061035 | BCE Loss: 1.0655014514923096\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.654476165771484 | KNN Loss: 5.6224365234375 | BCE Loss: 1.0320396423339844\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.668420314788818 | KNN Loss: 5.609042644500732 | BCE Loss: 1.0593777894973755\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.636582851409912 | KNN Loss: 5.5928425788879395 | BCE Loss: 1.0437402725219727\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 6.654123783111572 | KNN Loss: 5.627508640289307 | BCE Loss: 1.0266152620315552\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.618146896362305 | KNN Loss: 5.594393730163574 | BCE Loss: 1.0237529277801514\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.656272888183594 | KNN Loss: 5.594715595245361 | BCE Loss: 1.0615570545196533\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.670201778411865 | KNN Loss: 5.6021728515625 | BCE Loss: 1.0680290460586548\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.705216407775879 | KNN Loss: 5.653037071228027 | BCE Loss: 1.0521790981292725\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.663990020751953 | KNN Loss: 5.622241497039795 | BCE Loss: 1.041748285293579\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.657493591308594 | KNN Loss: 5.5981125831604 | BCE Loss: 1.0593810081481934\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.637821197509766 | KNN Loss: 5.603602886199951 | BCE Loss: 1.0342180728912354\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.665309906005859 | KNN Loss: 5.596989631652832 | BCE Loss: 1.0683202743530273\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 6.627315521240234 | KNN Loss: 5.609755516052246 | BCE Loss: 1.0175600051879883\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 6.745512008666992 | KNN Loss: 5.699437141418457 | BCE Loss: 1.0460747480392456\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.622110366821289 | KNN Loss: 5.592972278594971 | BCE Loss: 1.0291380882263184\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 6.631295680999756 | KNN Loss: 5.598849773406982 | BCE Loss: 1.0324457883834839\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.6280083656311035 | KNN Loss: 5.595466613769531 | BCE Loss: 1.0325417518615723\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.6556549072265625 | KNN Loss: 5.624320030212402 | BCE Loss: 1.0313351154327393\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.738088130950928 | KNN Loss: 5.661891460418701 | BCE Loss: 1.0761966705322266\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.69046688079834 | KNN Loss: 5.612880706787109 | BCE Loss: 1.0775859355926514\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.7017083168029785 | KNN Loss: 5.668113708496094 | BCE Loss: 1.0335947275161743\n",
      "Epoch   275: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.735434532165527 | KNN Loss: 5.655562877655029 | BCE Loss: 1.0798717737197876\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.649039268493652 | KNN Loss: 5.613441467285156 | BCE Loss: 1.035597801208496\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.6647186279296875 | KNN Loss: 5.6088643074035645 | BCE Loss: 1.0558542013168335\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.699370861053467 | KNN Loss: 5.646256446838379 | BCE Loss: 1.053114414215088\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 6.6634521484375 | KNN Loss: 5.643439292907715 | BCE Loss: 1.0200128555297852\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.661781311035156 | KNN Loss: 5.617552757263184 | BCE Loss: 1.0442283153533936\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.698147773742676 | KNN Loss: 5.621644496917725 | BCE Loss: 1.076503038406372\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 6.694863319396973 | KNN Loss: 5.6215057373046875 | BCE Loss: 1.0733578205108643\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.63095235824585 | KNN Loss: 5.596275806427002 | BCE Loss: 1.0346765518188477\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.650321006774902 | KNN Loss: 5.616672992706299 | BCE Loss: 1.0336477756500244\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.74436616897583 | KNN Loss: 5.723016262054443 | BCE Loss: 1.0213499069213867\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.717320442199707 | KNN Loss: 5.662060260772705 | BCE Loss: 1.055260181427002\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.667794227600098 | KNN Loss: 5.6419854164123535 | BCE Loss: 1.0258086919784546\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.670650482177734 | KNN Loss: 5.603808879852295 | BCE Loss: 1.0668413639068604\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.640073776245117 | KNN Loss: 5.6216559410095215 | BCE Loss: 1.0184175968170166\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.710052490234375 | KNN Loss: 5.639803409576416 | BCE Loss: 1.070249319076538\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.648611545562744 | KNN Loss: 5.601479530334473 | BCE Loss: 1.047131896018982\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.668368339538574 | KNN Loss: 5.620224475860596 | BCE Loss: 1.048143744468689\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.724735260009766 | KNN Loss: 5.671808242797852 | BCE Loss: 1.0529272556304932\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.670570373535156 | KNN Loss: 5.598013877868652 | BCE Loss: 1.072556734085083\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.627352714538574 | KNN Loss: 5.605854034423828 | BCE Loss: 1.0214987993240356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.674072265625 | KNN Loss: 5.660765647888184 | BCE Loss: 1.013306736946106\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.715733528137207 | KNN Loss: 5.669051647186279 | BCE Loss: 1.0466818809509277\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.679945945739746 | KNN Loss: 5.631691932678223 | BCE Loss: 1.0482540130615234\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.69443941116333 | KNN Loss: 5.64022970199585 | BCE Loss: 1.054209589958191\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.630275726318359 | KNN Loss: 5.594376564025879 | BCE Loss: 1.035899043083191\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.631745338439941 | KNN Loss: 5.596572399139404 | BCE Loss: 1.0351728200912476\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.754539966583252 | KNN Loss: 5.698551177978516 | BCE Loss: 1.0559886693954468\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.6699090003967285 | KNN Loss: 5.611634731292725 | BCE Loss: 1.058274269104004\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.671866416931152 | KNN Loss: 5.608193397521973 | BCE Loss: 1.0636727809906006\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.732933044433594 | KNN Loss: 5.642240524291992 | BCE Loss: 1.0906927585601807\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.6424560546875 | KNN Loss: 5.616645812988281 | BCE Loss: 1.0258102416992188\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.728690147399902 | KNN Loss: 5.685863971710205 | BCE Loss: 1.0428262948989868\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.752612113952637 | KNN Loss: 5.675595760345459 | BCE Loss: 1.0770162343978882\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.671538352966309 | KNN Loss: 5.604264259338379 | BCE Loss: 1.0672743320465088\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.637020587921143 | KNN Loss: 5.602162837982178 | BCE Loss: 1.0348577499389648\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 6.679340362548828 | KNN Loss: 5.611639499664307 | BCE Loss: 1.0677006244659424\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.702903747558594 | KNN Loss: 5.65542459487915 | BCE Loss: 1.0474791526794434\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.65030574798584 | KNN Loss: 5.592463493347168 | BCE Loss: 1.0578422546386719\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.746512413024902 | KNN Loss: 5.699045181274414 | BCE Loss: 1.0474674701690674\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.701881408691406 | KNN Loss: 5.60947322845459 | BCE Loss: 1.0924084186553955\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.665408134460449 | KNN Loss: 5.608599662780762 | BCE Loss: 1.0568082332611084\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.6429290771484375 | KNN Loss: 5.614800930023193 | BCE Loss: 1.0281281471252441\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.651122093200684 | KNN Loss: 5.6081767082214355 | BCE Loss: 1.042945146560669\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 6.624001502990723 | KNN Loss: 5.602301597595215 | BCE Loss: 1.0217000246047974\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.717400074005127 | KNN Loss: 5.650609016418457 | BCE Loss: 1.06679105758667\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.694304466247559 | KNN Loss: 5.639726161956787 | BCE Loss: 1.054578423500061\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.645664215087891 | KNN Loss: 5.642359733581543 | BCE Loss: 1.0033047199249268\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.737141132354736 | KNN Loss: 5.6782050132751465 | BCE Loss: 1.0589361190795898\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.700833320617676 | KNN Loss: 5.658919811248779 | BCE Loss: 1.0419137477874756\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.624905109405518 | KNN Loss: 5.6003546714782715 | BCE Loss: 1.024550437927246\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.649453163146973 | KNN Loss: 5.607254505157471 | BCE Loss: 1.042198896408081\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.6621503829956055 | KNN Loss: 5.607910633087158 | BCE Loss: 1.0542399883270264\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.660335540771484 | KNN Loss: 5.622903823852539 | BCE Loss: 1.0374319553375244\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.654706954956055 | KNN Loss: 5.646562099456787 | BCE Loss: 1.0081446170806885\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.737960338592529 | KNN Loss: 5.6680498123168945 | BCE Loss: 1.0699105262756348\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.700583457946777 | KNN Loss: 5.66333532333374 | BCE Loss: 1.0372482538223267\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.666535377502441 | KNN Loss: 5.611307621002197 | BCE Loss: 1.055227518081665\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.667813301086426 | KNN Loss: 5.632277488708496 | BCE Loss: 1.0355360507965088\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.668797969818115 | KNN Loss: 5.609114170074463 | BCE Loss: 1.059683918952942\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.681831359863281 | KNN Loss: 5.605576038360596 | BCE Loss: 1.0762555599212646\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.632419586181641 | KNN Loss: 5.599481105804443 | BCE Loss: 1.0329387187957764\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.6775336265563965 | KNN Loss: 5.660654067993164 | BCE Loss: 1.0168794393539429\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.678129196166992 | KNN Loss: 5.666845798492432 | BCE Loss: 1.0112833976745605\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.741196155548096 | KNN Loss: 5.687705993652344 | BCE Loss: 1.053490161895752\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.652652740478516 | KNN Loss: 5.598768711090088 | BCE Loss: 1.0538842678070068\n",
      "Epoch   286: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.632837772369385 | KNN Loss: 5.603394985198975 | BCE Loss: 1.0294427871704102\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 6.631628513336182 | KNN Loss: 5.594382286071777 | BCE Loss: 1.0372462272644043\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.669660568237305 | KNN Loss: 5.639711380004883 | BCE Loss: 1.0299490690231323\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.688347816467285 | KNN Loss: 5.626810073852539 | BCE Loss: 1.061537742614746\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.706246376037598 | KNN Loss: 5.626764297485352 | BCE Loss: 1.0794823169708252\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.687726974487305 | KNN Loss: 5.633508205413818 | BCE Loss: 1.0542185306549072\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.700167655944824 | KNN Loss: 5.632442951202393 | BCE Loss: 1.0677244663238525\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.623075485229492 | KNN Loss: 5.595719814300537 | BCE Loss: 1.027355670928955\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.606320381164551 | KNN Loss: 5.594906330108643 | BCE Loss: 1.0114142894744873\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 6.6412458419799805 | KNN Loss: 5.591941833496094 | BCE Loss: 1.0493041276931763\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.659995079040527 | KNN Loss: 5.61315393447876 | BCE Loss: 1.0468411445617676\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.637192726135254 | KNN Loss: 5.604038238525391 | BCE Loss: 1.0331542491912842\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 6.710252285003662 | KNN Loss: 5.650632381439209 | BCE Loss: 1.0596199035644531\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.679072856903076 | KNN Loss: 5.640480995178223 | BCE Loss: 1.0385918617248535\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.69281005859375 | KNN Loss: 5.640557765960693 | BCE Loss: 1.0522525310516357\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 6.707533359527588 | KNN Loss: 5.680802345275879 | BCE Loss: 1.026731014251709\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.724833011627197 | KNN Loss: 5.681066513061523 | BCE Loss: 1.0437666177749634\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.759465217590332 | KNN Loss: 5.69701623916626 | BCE Loss: 1.0624489784240723\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.65954065322876 | KNN Loss: 5.605513572692871 | BCE Loss: 1.0540270805358887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.671802520751953 | KNN Loss: 5.5978875160217285 | BCE Loss: 1.0739150047302246\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.651455879211426 | KNN Loss: 5.603970527648926 | BCE Loss: 1.0474852323532104\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.652804374694824 | KNN Loss: 5.61592435836792 | BCE Loss: 1.0368802547454834\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.71419095993042 | KNN Loss: 5.644701957702637 | BCE Loss: 1.0694891214370728\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 6.685623645782471 | KNN Loss: 5.640969276428223 | BCE Loss: 1.044654369354248\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.697491645812988 | KNN Loss: 5.651240348815918 | BCE Loss: 1.0462511777877808\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.675634384155273 | KNN Loss: 5.6447858810424805 | BCE Loss: 1.030848741531372\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.704216957092285 | KNN Loss: 5.651259422302246 | BCE Loss: 1.05295729637146\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.653996467590332 | KNN Loss: 5.600724697113037 | BCE Loss: 1.0532715320587158\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.655229568481445 | KNN Loss: 5.636659622192383 | BCE Loss: 1.0185697078704834\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.598996639251709 | KNN Loss: 5.591373443603516 | BCE Loss: 1.007623314857483\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.693108558654785 | KNN Loss: 5.6412858963012695 | BCE Loss: 1.0518224239349365\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.6369547843933105 | KNN Loss: 5.597568988800049 | BCE Loss: 1.0393857955932617\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.659903526306152 | KNN Loss: 5.630269527435303 | BCE Loss: 1.0296339988708496\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.721086502075195 | KNN Loss: 5.67062520980835 | BCE Loss: 1.0504614114761353\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 6.645173072814941 | KNN Loss: 5.613530158996582 | BCE Loss: 1.0316429138183594\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.731174945831299 | KNN Loss: 5.68333101272583 | BCE Loss: 1.0478439331054688\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.677713394165039 | KNN Loss: 5.639216899871826 | BCE Loss: 1.038496494293213\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.702509880065918 | KNN Loss: 5.631693363189697 | BCE Loss: 1.0708167552947998\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.67453670501709 | KNN Loss: 5.605155944824219 | BCE Loss: 1.0693808794021606\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.5967535972595215 | KNN Loss: 5.593317031860352 | BCE Loss: 1.00343656539917\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.690513610839844 | KNN Loss: 5.640907287597656 | BCE Loss: 1.049606204032898\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.64555549621582 | KNN Loss: 5.614992618560791 | BCE Loss: 1.0305629968643188\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.6941375732421875 | KNN Loss: 5.631751537322998 | BCE Loss: 1.0623860359191895\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.743072032928467 | KNN Loss: 5.667553901672363 | BCE Loss: 1.0755181312561035\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.739813804626465 | KNN Loss: 5.709749698638916 | BCE Loss: 1.030064344406128\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.699386119842529 | KNN Loss: 5.657989025115967 | BCE Loss: 1.0413970947265625\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.6854705810546875 | KNN Loss: 5.6219940185546875 | BCE Loss: 1.063476800918579\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.621654033660889 | KNN Loss: 5.59981107711792 | BCE Loss: 1.0218429565429688\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.64926815032959 | KNN Loss: 5.60093355178833 | BCE Loss: 1.0483345985412598\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.6524457931518555 | KNN Loss: 5.608274936676025 | BCE Loss: 1.0441710948944092\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.66286563873291 | KNN Loss: 5.629061222076416 | BCE Loss: 1.0338046550750732\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 6.696220397949219 | KNN Loss: 5.636587142944336 | BCE Loss: 1.059633493423462\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.652449607849121 | KNN Loss: 5.607515335083008 | BCE Loss: 1.0449345111846924\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.68451452255249 | KNN Loss: 5.625611305236816 | BCE Loss: 1.0589032173156738\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.668651580810547 | KNN Loss: 5.62102746963501 | BCE Loss: 1.047624111175537\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.682603359222412 | KNN Loss: 5.6471076011657715 | BCE Loss: 1.035495638847351\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.654311180114746 | KNN Loss: 5.61036491394043 | BCE Loss: 1.0439460277557373\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.708621025085449 | KNN Loss: 5.640539646148682 | BCE Loss: 1.0680811405181885\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.771873474121094 | KNN Loss: 5.732923984527588 | BCE Loss: 1.038949728012085\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.632915496826172 | KNN Loss: 5.597245216369629 | BCE Loss: 1.035670280456543\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.638572692871094 | KNN Loss: 5.627931118011475 | BCE Loss: 1.0106414556503296\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.717990875244141 | KNN Loss: 5.650978088378906 | BCE Loss: 1.0670127868652344\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.668562412261963 | KNN Loss: 5.5962910652160645 | BCE Loss: 1.0722713470458984\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.665585041046143 | KNN Loss: 5.595834255218506 | BCE Loss: 1.0697507858276367\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.642131328582764 | KNN Loss: 5.593509197235107 | BCE Loss: 1.0486220121383667\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 6.661468505859375 | KNN Loss: 5.59337043762207 | BCE Loss: 1.0680979490280151\n",
      "Epoch   297: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.754300117492676 | KNN Loss: 5.661590576171875 | BCE Loss: 1.0927096605300903\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.6863203048706055 | KNN Loss: 5.645120143890381 | BCE Loss: 1.041200041770935\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.637945175170898 | KNN Loss: 5.590808868408203 | BCE Loss: 1.0471361875534058\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.640527725219727 | KNN Loss: 5.601027488708496 | BCE Loss: 1.0395004749298096\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.698856353759766 | KNN Loss: 5.660815238952637 | BCE Loss: 1.0380412340164185\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.678333282470703 | KNN Loss: 5.610317230224609 | BCE Loss: 1.0680158138275146\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.719221115112305 | KNN Loss: 5.650529861450195 | BCE Loss: 1.0686910152435303\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.6783833503723145 | KNN Loss: 5.6178388595581055 | BCE Loss: 1.0605446100234985\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.621105670928955 | KNN Loss: 5.59501314163208 | BCE Loss: 1.0260924100875854\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.642858028411865 | KNN Loss: 5.596548557281494 | BCE Loss: 1.046309471130371\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.633856773376465 | KNN Loss: 5.597000598907471 | BCE Loss: 1.0368561744689941\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.675082206726074 | KNN Loss: 5.63095760345459 | BCE Loss: 1.0441246032714844\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.686197757720947 | KNN Loss: 5.605323791503906 | BCE Loss: 1.0808738470077515\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.66246223449707 | KNN Loss: 5.615307331085205 | BCE Loss: 1.0471549034118652\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.6803789138793945 | KNN Loss: 5.59817361831665 | BCE Loss: 1.082205057144165\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.71873140335083 | KNN Loss: 5.671395301818848 | BCE Loss: 1.047336220741272\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.719660758972168 | KNN Loss: 5.68408727645874 | BCE Loss: 1.0355737209320068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.656129360198975 | KNN Loss: 5.616641521453857 | BCE Loss: 1.0394878387451172\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.625397205352783 | KNN Loss: 5.59857702255249 | BCE Loss: 1.0268203020095825\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.697272300720215 | KNN Loss: 5.645955562591553 | BCE Loss: 1.0513166189193726\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.641472816467285 | KNN Loss: 5.615651607513428 | BCE Loss: 1.025821328163147\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.6731109619140625 | KNN Loss: 5.607153415679932 | BCE Loss: 1.0659573078155518\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.7098565101623535 | KNN Loss: 5.660873889923096 | BCE Loss: 1.0489826202392578\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.67380952835083 | KNN Loss: 5.613523960113525 | BCE Loss: 1.0602856874465942\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.668851852416992 | KNN Loss: 5.606689929962158 | BCE Loss: 1.062162160873413\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.695405006408691 | KNN Loss: 5.656817436218262 | BCE Loss: 1.0385873317718506\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.690798759460449 | KNN Loss: 5.653562545776367 | BCE Loss: 1.0372364521026611\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.619436264038086 | KNN Loss: 5.594874382019043 | BCE Loss: 1.024561882019043\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.6697773933410645 | KNN Loss: 5.650447845458984 | BCE Loss: 1.0193294286727905\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.641100883483887 | KNN Loss: 5.595995903015137 | BCE Loss: 1.045105218887329\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 6.662254810333252 | KNN Loss: 5.607117652893066 | BCE Loss: 1.0551371574401855\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.675105094909668 | KNN Loss: 5.654489994049072 | BCE Loss: 1.0206151008605957\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.710859298706055 | KNN Loss: 5.685487747192383 | BCE Loss: 1.0253716707229614\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.627135276794434 | KNN Loss: 5.605583190917969 | BCE Loss: 1.021552324295044\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.714578628540039 | KNN Loss: 5.662484645843506 | BCE Loss: 1.0520939826965332\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.698128700256348 | KNN Loss: 5.654291152954102 | BCE Loss: 1.043837547302246\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.660674095153809 | KNN Loss: 5.606560230255127 | BCE Loss: 1.0541141033172607\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.6879072189331055 | KNN Loss: 5.641414642333984 | BCE Loss: 1.0464928150177002\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.704407691955566 | KNN Loss: 5.659587383270264 | BCE Loss: 1.0448204278945923\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.78164005279541 | KNN Loss: 5.726731300354004 | BCE Loss: 1.0549089908599854\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.664926052093506 | KNN Loss: 5.620108127593994 | BCE Loss: 1.0448179244995117\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.653124809265137 | KNN Loss: 5.594968795776367 | BCE Loss: 1.0581560134887695\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.694472312927246 | KNN Loss: 5.62982177734375 | BCE Loss: 1.0646504163742065\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.6959991455078125 | KNN Loss: 5.644881725311279 | BCE Loss: 1.0511174201965332\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.693437576293945 | KNN Loss: 5.646810531616211 | BCE Loss: 1.0466270446777344\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.696430683135986 | KNN Loss: 5.630985260009766 | BCE Loss: 1.0654454231262207\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.633709907531738 | KNN Loss: 5.601202964782715 | BCE Loss: 1.032507061958313\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.633862018585205 | KNN Loss: 5.6109771728515625 | BCE Loss: 1.0228848457336426\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.678262233734131 | KNN Loss: 5.652284145355225 | BCE Loss: 1.0259780883789062\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.653968334197998 | KNN Loss: 5.611349582672119 | BCE Loss: 1.042618751525879\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.650984287261963 | KNN Loss: 5.603888988494873 | BCE Loss: 1.0470952987670898\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.668959617614746 | KNN Loss: 5.609125137329102 | BCE Loss: 1.0598347187042236\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.648093223571777 | KNN Loss: 5.596006870269775 | BCE Loss: 1.052086353302002\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.640469074249268 | KNN Loss: 5.61481237411499 | BCE Loss: 1.0256567001342773\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.654684543609619 | KNN Loss: 5.619330883026123 | BCE Loss: 1.035353660583496\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.659874439239502 | KNN Loss: 5.606410026550293 | BCE Loss: 1.0534642934799194\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.681304931640625 | KNN Loss: 5.6440229415893555 | BCE Loss: 1.03728187084198\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 6.680908203125 | KNN Loss: 5.595649242401123 | BCE Loss: 1.0852588415145874\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.658570289611816 | KNN Loss: 5.635850429534912 | BCE Loss: 1.0227200984954834\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.65627384185791 | KNN Loss: 5.6179094314575195 | BCE Loss: 1.0383646488189697\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.656638145446777 | KNN Loss: 5.60593843460083 | BCE Loss: 1.0506997108459473\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.640893936157227 | KNN Loss: 5.603952884674072 | BCE Loss: 1.0369412899017334\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.669164180755615 | KNN Loss: 5.611043930053711 | BCE Loss: 1.0581201314926147\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.678104877471924 | KNN Loss: 5.595106601715088 | BCE Loss: 1.082998275756836\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.650262355804443 | KNN Loss: 5.601613998413086 | BCE Loss: 1.0486483573913574\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.6413702964782715 | KNN Loss: 5.6016154289245605 | BCE Loss: 1.039754867553711\n",
      "Epoch   308: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 6.789439678192139 | KNN Loss: 5.718162536621094 | BCE Loss: 1.071277141571045\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.681638240814209 | KNN Loss: 5.593756198883057 | BCE Loss: 1.0878820419311523\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.691685676574707 | KNN Loss: 5.620417594909668 | BCE Loss: 1.0712683200836182\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.651139259338379 | KNN Loss: 5.612979888916016 | BCE Loss: 1.0381594896316528\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.608858108520508 | KNN Loss: 5.604598045349121 | BCE Loss: 1.0042599439620972\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.697955131530762 | KNN Loss: 5.645716190338135 | BCE Loss: 1.0522388219833374\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.680622577667236 | KNN Loss: 5.598801136016846 | BCE Loss: 1.0818214416503906\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.672856330871582 | KNN Loss: 5.63120698928833 | BCE Loss: 1.041649341583252\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.709566116333008 | KNN Loss: 5.680813312530518 | BCE Loss: 1.0287528038024902\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 6.658161640167236 | KNN Loss: 5.623069763183594 | BCE Loss: 1.0350918769836426\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.670291900634766 | KNN Loss: 5.626108646392822 | BCE Loss: 1.0441830158233643\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.697731018066406 | KNN Loss: 5.626911640167236 | BCE Loss: 1.07081937789917\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.6818742752075195 | KNN Loss: 5.643733978271484 | BCE Loss: 1.0381402969360352\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.74330472946167 | KNN Loss: 5.678446292877197 | BCE Loss: 1.0648584365844727\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.686182022094727 | KNN Loss: 5.636491298675537 | BCE Loss: 1.0496907234191895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.672600746154785 | KNN Loss: 5.624458312988281 | BCE Loss: 1.0481421947479248\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.650379657745361 | KNN Loss: 5.622498989105225 | BCE Loss: 1.0278807878494263\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.665004253387451 | KNN Loss: 5.5984039306640625 | BCE Loss: 1.0666003227233887\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.640080451965332 | KNN Loss: 5.607560634613037 | BCE Loss: 1.032520055770874\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.65714168548584 | KNN Loss: 5.631250381469727 | BCE Loss: 1.0258913040161133\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.616027355194092 | KNN Loss: 5.594976902008057 | BCE Loss: 1.0210504531860352\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.681960105895996 | KNN Loss: 5.64342737197876 | BCE Loss: 1.0385327339172363\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.656435489654541 | KNN Loss: 5.596707820892334 | BCE Loss: 1.0597277879714966\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.660758972167969 | KNN Loss: 5.622009754180908 | BCE Loss: 1.0387489795684814\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.618378162384033 | KNN Loss: 5.597327709197998 | BCE Loss: 1.0210504531860352\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.740356922149658 | KNN Loss: 5.681191444396973 | BCE Loss: 1.059165596961975\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.6611456871032715 | KNN Loss: 5.595544815063477 | BCE Loss: 1.065600872039795\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 6.7525811195373535 | KNN Loss: 5.662899971008301 | BCE Loss: 1.0896810293197632\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.683176040649414 | KNN Loss: 5.643585205078125 | BCE Loss: 1.0395909547805786\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.645381450653076 | KNN Loss: 5.6053571701049805 | BCE Loss: 1.0400242805480957\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.681557655334473 | KNN Loss: 5.636929988861084 | BCE Loss: 1.0446276664733887\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.687128067016602 | KNN Loss: 5.6056413650512695 | BCE Loss: 1.0814869403839111\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.670053482055664 | KNN Loss: 5.660243988037109 | BCE Loss: 1.0098092555999756\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.651327610015869 | KNN Loss: 5.628115177154541 | BCE Loss: 1.0232125520706177\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.643246650695801 | KNN Loss: 5.592702388763428 | BCE Loss: 1.0505445003509521\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.749824523925781 | KNN Loss: 5.675979137420654 | BCE Loss: 1.0738451480865479\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 6.6357340812683105 | KNN Loss: 5.589616775512695 | BCE Loss: 1.0461174249649048\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.692888259887695 | KNN Loss: 5.656033039093018 | BCE Loss: 1.0368551015853882\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.669360160827637 | KNN Loss: 5.621204376220703 | BCE Loss: 1.0481557846069336\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.6634840965271 | KNN Loss: 5.608218193054199 | BCE Loss: 1.0552659034729004\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.632310390472412 | KNN Loss: 5.598735809326172 | BCE Loss: 1.0335747003555298\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.754593372344971 | KNN Loss: 5.693406105041504 | BCE Loss: 1.0611872673034668\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.656801223754883 | KNN Loss: 5.60127067565918 | BCE Loss: 1.0555305480957031\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.6299519538879395 | KNN Loss: 5.599040508270264 | BCE Loss: 1.0309114456176758\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.635241985321045 | KNN Loss: 5.609165668487549 | BCE Loss: 1.0260764360427856\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 6.633657455444336 | KNN Loss: 5.610500335693359 | BCE Loss: 1.0231571197509766\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.657536506652832 | KNN Loss: 5.609081745147705 | BCE Loss: 1.048454999923706\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.711782455444336 | KNN Loss: 5.655050277709961 | BCE Loss: 1.056732177734375\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.6856794357299805 | KNN Loss: 5.609416961669922 | BCE Loss: 1.0762622356414795\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.659320831298828 | KNN Loss: 5.616624355316162 | BCE Loss: 1.0426965951919556\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.766434669494629 | KNN Loss: 5.718339443206787 | BCE Loss: 1.048095464706421\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.682196617126465 | KNN Loss: 5.635143756866455 | BCE Loss: 1.0470530986785889\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 6.661967754364014 | KNN Loss: 5.631620407104492 | BCE Loss: 1.030347228050232\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.650219917297363 | KNN Loss: 5.596900939941406 | BCE Loss: 1.0533190965652466\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.647817611694336 | KNN Loss: 5.610394477844238 | BCE Loss: 1.0374228954315186\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.654646873474121 | KNN Loss: 5.612988471984863 | BCE Loss: 1.041658639907837\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.651897430419922 | KNN Loss: 5.633309364318848 | BCE Loss: 1.0185883045196533\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.738380432128906 | KNN Loss: 5.693740367889404 | BCE Loss: 1.0446398258209229\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 6.661949157714844 | KNN Loss: 5.628175735473633 | BCE Loss: 1.033773422241211\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.673184394836426 | KNN Loss: 5.636727809906006 | BCE Loss: 1.036456823348999\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.6602253913879395 | KNN Loss: 5.624463081359863 | BCE Loss: 1.0357621908187866\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.685004234313965 | KNN Loss: 5.647697925567627 | BCE Loss: 1.037306308746338\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.68950080871582 | KNN Loss: 5.640364170074463 | BCE Loss: 1.0491365194320679\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.636385917663574 | KNN Loss: 5.608047008514404 | BCE Loss: 1.0283390283584595\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.632506847381592 | KNN Loss: 5.594242095947266 | BCE Loss: 1.0382647514343262\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 6.678513526916504 | KNN Loss: 5.613037109375 | BCE Loss: 1.065476655960083\n",
      "Epoch   319: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.657599449157715 | KNN Loss: 5.630486965179443 | BCE Loss: 1.0271122455596924\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.700244903564453 | KNN Loss: 5.66786003112793 | BCE Loss: 1.0323846340179443\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.661576747894287 | KNN Loss: 5.59336519241333 | BCE Loss: 1.068211555480957\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.676478385925293 | KNN Loss: 5.626832962036133 | BCE Loss: 1.0496454238891602\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.662937164306641 | KNN Loss: 5.627142906188965 | BCE Loss: 1.0357942581176758\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.671801567077637 | KNN Loss: 5.616246700286865 | BCE Loss: 1.0555551052093506\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.678584098815918 | KNN Loss: 5.623561382293701 | BCE Loss: 1.0550227165222168\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.650816440582275 | KNN Loss: 5.635115623474121 | BCE Loss: 1.0157006978988647\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.766276836395264 | KNN Loss: 5.693068504333496 | BCE Loss: 1.0732084512710571\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.693681716918945 | KNN Loss: 5.634282112121582 | BCE Loss: 1.0593994855880737\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.666760444641113 | KNN Loss: 5.597565174102783 | BCE Loss: 1.06919527053833\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.687702178955078 | KNN Loss: 5.658466339111328 | BCE Loss: 1.02923583984375\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.710903167724609 | KNN Loss: 5.637782096862793 | BCE Loss: 1.073121190071106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.678255558013916 | KNN Loss: 5.6677680015563965 | BCE Loss: 1.0104875564575195\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.689375877380371 | KNN Loss: 5.652961254119873 | BCE Loss: 1.036414384841919\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.680019855499268 | KNN Loss: 5.643674373626709 | BCE Loss: 1.036345362663269\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.6661505699157715 | KNN Loss: 5.605766296386719 | BCE Loss: 1.0603841543197632\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 6.694101333618164 | KNN Loss: 5.628669261932373 | BCE Loss: 1.0654323101043701\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.675800323486328 | KNN Loss: 5.61116886138916 | BCE Loss: 1.0646312236785889\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.72736930847168 | KNN Loss: 5.654639720916748 | BCE Loss: 1.0727298259735107\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.694095611572266 | KNN Loss: 5.659055709838867 | BCE Loss: 1.0350399017333984\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.6711039543151855 | KNN Loss: 5.657002925872803 | BCE Loss: 1.0141011476516724\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 6.734339714050293 | KNN Loss: 5.662780284881592 | BCE Loss: 1.0715596675872803\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.704326629638672 | KNN Loss: 5.6733551025390625 | BCE Loss: 1.0309714078903198\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.654388427734375 | KNN Loss: 5.604307651519775 | BCE Loss: 1.0500808954238892\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.712773323059082 | KNN Loss: 5.6589436531066895 | BCE Loss: 1.0538296699523926\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.632495880126953 | KNN Loss: 5.602828502655029 | BCE Loss: 1.029667615890503\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.731213092803955 | KNN Loss: 5.655945777893066 | BCE Loss: 1.0752674341201782\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.73026704788208 | KNN Loss: 5.692159652709961 | BCE Loss: 1.0381073951721191\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 6.645583629608154 | KNN Loss: 5.602874279022217 | BCE Loss: 1.0427093505859375\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.668558120727539 | KNN Loss: 5.6099019050598145 | BCE Loss: 1.0586563348770142\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 6.657503128051758 | KNN Loss: 5.6086554527282715 | BCE Loss: 1.0488479137420654\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.648648262023926 | KNN Loss: 5.595170974731445 | BCE Loss: 1.0534775257110596\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.679451942443848 | KNN Loss: 5.650516510009766 | BCE Loss: 1.0289353132247925\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.6783599853515625 | KNN Loss: 5.624034881591797 | BCE Loss: 1.0543251037597656\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.660824298858643 | KNN Loss: 5.596583843231201 | BCE Loss: 1.0642404556274414\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.671377182006836 | KNN Loss: 5.643019676208496 | BCE Loss: 1.028357744216919\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.673395156860352 | KNN Loss: 5.6086602210998535 | BCE Loss: 1.0647351741790771\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.686468124389648 | KNN Loss: 5.622596263885498 | BCE Loss: 1.0638720989227295\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.710535526275635 | KNN Loss: 5.659832000732422 | BCE Loss: 1.050703525543213\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.71399450302124 | KNN Loss: 5.672728538513184 | BCE Loss: 1.0412659645080566\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.712484359741211 | KNN Loss: 5.655195713043213 | BCE Loss: 1.0572888851165771\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.755884170532227 | KNN Loss: 5.709323883056641 | BCE Loss: 1.0465600490570068\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.643278121948242 | KNN Loss: 5.6122260093688965 | BCE Loss: 1.0310522317886353\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.640913486480713 | KNN Loss: 5.600680351257324 | BCE Loss: 1.0402331352233887\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.71059513092041 | KNN Loss: 5.67380428314209 | BCE Loss: 1.0367907285690308\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.7154340744018555 | KNN Loss: 5.670833110809326 | BCE Loss: 1.0446009635925293\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.63136625289917 | KNN Loss: 5.591212272644043 | BCE Loss: 1.040153980255127\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.709725379943848 | KNN Loss: 5.650906085968018 | BCE Loss: 1.05881929397583\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.71929931640625 | KNN Loss: 5.665403842926025 | BCE Loss: 1.0538957118988037\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.625674247741699 | KNN Loss: 5.6039814949035645 | BCE Loss: 1.0216927528381348\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.706363677978516 | KNN Loss: 5.659222602844238 | BCE Loss: 1.0471408367156982\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.753273010253906 | KNN Loss: 5.70217752456665 | BCE Loss: 1.0510956048965454\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.65683650970459 | KNN Loss: 5.594480991363525 | BCE Loss: 1.0623552799224854\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.658265113830566 | KNN Loss: 5.609596252441406 | BCE Loss: 1.0486688613891602\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.658374786376953 | KNN Loss: 5.614299297332764 | BCE Loss: 1.0440757274627686\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.638653755187988 | KNN Loss: 5.599185943603516 | BCE Loss: 1.039467692375183\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.694038391113281 | KNN Loss: 5.609169960021973 | BCE Loss: 1.0848684310913086\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 6.711063385009766 | KNN Loss: 5.645454406738281 | BCE Loss: 1.0656087398529053\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 6.681901454925537 | KNN Loss: 5.641001224517822 | BCE Loss: 1.0409001111984253\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.609026908874512 | KNN Loss: 5.59863805770874 | BCE Loss: 1.0103890895843506\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.606595039367676 | KNN Loss: 5.593800067901611 | BCE Loss: 1.012795090675354\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.70670223236084 | KNN Loss: 5.6270318031311035 | BCE Loss: 1.0796703100204468\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.652545928955078 | KNN Loss: 5.615009307861328 | BCE Loss: 1.03753662109375\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.63094425201416 | KNN Loss: 5.594979763031006 | BCE Loss: 1.0359643697738647\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.6467719078063965 | KNN Loss: 5.610604286193848 | BCE Loss: 1.0361676216125488\n",
      "Epoch   330: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.683311462402344 | KNN Loss: 5.632801532745361 | BCE Loss: 1.0505099296569824\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.659788608551025 | KNN Loss: 5.618289470672607 | BCE Loss: 1.041499137878418\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.7367448806762695 | KNN Loss: 5.669722080230713 | BCE Loss: 1.0670228004455566\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.7139892578125 | KNN Loss: 5.640805244445801 | BCE Loss: 1.0731840133666992\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.692877769470215 | KNN Loss: 5.6312994956970215 | BCE Loss: 1.061578392982483\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.648954391479492 | KNN Loss: 5.615321159362793 | BCE Loss: 1.0336334705352783\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.6603593826293945 | KNN Loss: 5.599508285522461 | BCE Loss: 1.0608508586883545\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.6098809242248535 | KNN Loss: 5.589145183563232 | BCE Loss: 1.020735740661621\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.692999362945557 | KNN Loss: 5.6585774421691895 | BCE Loss: 1.0344220399856567\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.6559038162231445 | KNN Loss: 5.603366851806641 | BCE Loss: 1.052536964416504\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.697597503662109 | KNN Loss: 5.642421245574951 | BCE Loss: 1.055176019668579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.678648948669434 | KNN Loss: 5.5883588790893555 | BCE Loss: 1.0902900695800781\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.739222526550293 | KNN Loss: 5.660601615905762 | BCE Loss: 1.0786211490631104\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.646421432495117 | KNN Loss: 5.591630458831787 | BCE Loss: 1.05479097366333\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.64228630065918 | KNN Loss: 5.595375061035156 | BCE Loss: 1.0469114780426025\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.666668891906738 | KNN Loss: 5.597116947174072 | BCE Loss: 1.069551944732666\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.706603050231934 | KNN Loss: 5.650901794433594 | BCE Loss: 1.0557011365890503\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.7288289070129395 | KNN Loss: 5.684493064880371 | BCE Loss: 1.0443357229232788\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.691821098327637 | KNN Loss: 5.651113033294678 | BCE Loss: 1.0407078266143799\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.6796979904174805 | KNN Loss: 5.648438453674316 | BCE Loss: 1.031259536743164\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.664945125579834 | KNN Loss: 5.606152057647705 | BCE Loss: 1.0587929487228394\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.691606521606445 | KNN Loss: 5.605626106262207 | BCE Loss: 1.0859806537628174\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.668985366821289 | KNN Loss: 5.631926536560059 | BCE Loss: 1.03705894947052\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.712961673736572 | KNN Loss: 5.660070896148682 | BCE Loss: 1.0528908967971802\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.667167663574219 | KNN Loss: 5.621634483337402 | BCE Loss: 1.045533299446106\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.688714027404785 | KNN Loss: 5.629138469696045 | BCE Loss: 1.0595753192901611\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.727290153503418 | KNN Loss: 5.688981533050537 | BCE Loss: 1.0383083820343018\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.720234394073486 | KNN Loss: 5.647532939910889 | BCE Loss: 1.0727014541625977\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.67392635345459 | KNN Loss: 5.626216888427734 | BCE Loss: 1.0477094650268555\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.670373916625977 | KNN Loss: 5.619602203369141 | BCE Loss: 1.0507715940475464\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.657606601715088 | KNN Loss: 5.616959095001221 | BCE Loss: 1.0406475067138672\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.641059875488281 | KNN Loss: 5.6046833992004395 | BCE Loss: 1.0363764762878418\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.677122592926025 | KNN Loss: 5.649044036865234 | BCE Loss: 1.028078556060791\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.664034843444824 | KNN Loss: 5.6182427406311035 | BCE Loss: 1.0457922220230103\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.669013977050781 | KNN Loss: 5.604811668395996 | BCE Loss: 1.0642023086547852\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.624086380004883 | KNN Loss: 5.600093364715576 | BCE Loss: 1.0239930152893066\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.7034077644348145 | KNN Loss: 5.655916690826416 | BCE Loss: 1.047491192817688\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.620689868927002 | KNN Loss: 5.594583511352539 | BCE Loss: 1.0261064767837524\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.638856887817383 | KNN Loss: 5.602002143859863 | BCE Loss: 1.0368545055389404\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.730224132537842 | KNN Loss: 5.689639568328857 | BCE Loss: 1.0405845642089844\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.641281604766846 | KNN Loss: 5.605040073394775 | BCE Loss: 1.0362416505813599\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.6654791831970215 | KNN Loss: 5.598430156707764 | BCE Loss: 1.0670490264892578\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.670039653778076 | KNN Loss: 5.612679958343506 | BCE Loss: 1.0573598146438599\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.642451286315918 | KNN Loss: 5.600308418273926 | BCE Loss: 1.042142629623413\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.716935157775879 | KNN Loss: 5.636289119720459 | BCE Loss: 1.0806457996368408\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 6.77601432800293 | KNN Loss: 5.734374523162842 | BCE Loss: 1.041640043258667\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.688340663909912 | KNN Loss: 5.648725986480713 | BCE Loss: 1.0396146774291992\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.725377082824707 | KNN Loss: 5.674376487731934 | BCE Loss: 1.0510005950927734\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.621061325073242 | KNN Loss: 5.593642234802246 | BCE Loss: 1.0274193286895752\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.6343584060668945 | KNN Loss: 5.605095386505127 | BCE Loss: 1.0292632579803467\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.636375427246094 | KNN Loss: 5.601559638977051 | BCE Loss: 1.0348155498504639\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.681991100311279 | KNN Loss: 5.636467456817627 | BCE Loss: 1.0455235242843628\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 6.670952796936035 | KNN Loss: 5.624493598937988 | BCE Loss: 1.0464591979980469\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 6.656729221343994 | KNN Loss: 5.609899997711182 | BCE Loss: 1.0468292236328125\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.857172966003418 | KNN Loss: 5.7813591957092285 | BCE Loss: 1.0758135318756104\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.725542068481445 | KNN Loss: 5.675380229949951 | BCE Loss: 1.0501617193222046\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.651715278625488 | KNN Loss: 5.624413967132568 | BCE Loss: 1.02730131149292\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.683490753173828 | KNN Loss: 5.658074855804443 | BCE Loss: 1.0254161357879639\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.640130043029785 | KNN Loss: 5.608078479766846 | BCE Loss: 1.0320515632629395\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.6322340965271 | KNN Loss: 5.5999932289123535 | BCE Loss: 1.0322407484054565\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.652808666229248 | KNN Loss: 5.59533166885376 | BCE Loss: 1.0574768781661987\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 6.6854567527771 | KNN Loss: 5.620514869689941 | BCE Loss: 1.0649418830871582\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.657382011413574 | KNN Loss: 5.636640548706055 | BCE Loss: 1.0207414627075195\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.650951862335205 | KNN Loss: 5.602880477905273 | BCE Loss: 1.048071265220642\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.641088485717773 | KNN Loss: 5.597252368927002 | BCE Loss: 1.043836236000061\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.651142120361328 | KNN Loss: 5.599343299865723 | BCE Loss: 1.0517988204956055\n",
      "Epoch   341: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.692729473114014 | KNN Loss: 5.676598072052002 | BCE Loss: 1.0161315202713013\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.6862006187438965 | KNN Loss: 5.631702423095703 | BCE Loss: 1.0544981956481934\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.662065505981445 | KNN Loss: 5.611152172088623 | BCE Loss: 1.0509132146835327\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.6474199295043945 | KNN Loss: 5.6034626960754395 | BCE Loss: 1.0439574718475342\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.643786430358887 | KNN Loss: 5.5959792137146 | BCE Loss: 1.047806978225708\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.650304794311523 | KNN Loss: 5.597394943237305 | BCE Loss: 1.0529096126556396\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.6465582847595215 | KNN Loss: 5.592521667480469 | BCE Loss: 1.0540366172790527\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.660828590393066 | KNN Loss: 5.615548610687256 | BCE Loss: 1.0452797412872314\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.662163257598877 | KNN Loss: 5.611691474914551 | BCE Loss: 1.0504717826843262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.646759986877441 | KNN Loss: 5.597620010375977 | BCE Loss: 1.0491397380828857\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.764778137207031 | KNN Loss: 5.709144115447998 | BCE Loss: 1.055633783340454\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.627460479736328 | KNN Loss: 5.6091413497924805 | BCE Loss: 1.0183193683624268\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.647134780883789 | KNN Loss: 5.595400810241699 | BCE Loss: 1.051734209060669\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.679005146026611 | KNN Loss: 5.637478828430176 | BCE Loss: 1.0415263175964355\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.653043746948242 | KNN Loss: 5.622384548187256 | BCE Loss: 1.0306590795516968\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.637462615966797 | KNN Loss: 5.598940849304199 | BCE Loss: 1.0385217666625977\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.659110069274902 | KNN Loss: 5.6266913414001465 | BCE Loss: 1.0324186086654663\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.683846950531006 | KNN Loss: 5.624373912811279 | BCE Loss: 1.0594730377197266\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.667204856872559 | KNN Loss: 5.634565353393555 | BCE Loss: 1.032639741897583\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.631047248840332 | KNN Loss: 5.605125427246094 | BCE Loss: 1.0259215831756592\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.718435287475586 | KNN Loss: 5.665853500366211 | BCE Loss: 1.052581787109375\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.6559648513793945 | KNN Loss: 5.622386932373047 | BCE Loss: 1.0335781574249268\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 6.68398904800415 | KNN Loss: 5.653308868408203 | BCE Loss: 1.0306800603866577\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.703716278076172 | KNN Loss: 5.674396991729736 | BCE Loss: 1.0293192863464355\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.664482593536377 | KNN Loss: 5.648250579833984 | BCE Loss: 1.0162320137023926\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.625523567199707 | KNN Loss: 5.594419002532959 | BCE Loss: 1.031104564666748\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.7763166427612305 | KNN Loss: 5.762935161590576 | BCE Loss: 1.0133816003799438\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.647160530090332 | KNN Loss: 5.605668067932129 | BCE Loss: 1.041492223739624\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.708415508270264 | KNN Loss: 5.6619672775268555 | BCE Loss: 1.0464482307434082\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.676718235015869 | KNN Loss: 5.606382846832275 | BCE Loss: 1.0703353881835938\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.660362243652344 | KNN Loss: 5.600931644439697 | BCE Loss: 1.0594308376312256\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 6.706998825073242 | KNN Loss: 5.660136699676514 | BCE Loss: 1.0468623638153076\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.686210632324219 | KNN Loss: 5.625645637512207 | BCE Loss: 1.0605652332305908\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.609817028045654 | KNN Loss: 5.5908637046813965 | BCE Loss: 1.0189533233642578\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.626505374908447 | KNN Loss: 5.591796875 | BCE Loss: 1.0347086191177368\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.685953140258789 | KNN Loss: 5.591854572296143 | BCE Loss: 1.0940985679626465\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 6.7671613693237305 | KNN Loss: 5.72305965423584 | BCE Loss: 1.0441018342971802\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.65005350112915 | KNN Loss: 5.6035075187683105 | BCE Loss: 1.0465458631515503\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.626497745513916 | KNN Loss: 5.604184150695801 | BCE Loss: 1.0223134756088257\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.6228108406066895 | KNN Loss: 5.5938286781311035 | BCE Loss: 1.028982162475586\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.672987937927246 | KNN Loss: 5.601315021514893 | BCE Loss: 1.0716731548309326\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.675327301025391 | KNN Loss: 5.634655952453613 | BCE Loss: 1.0406711101531982\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.64631462097168 | KNN Loss: 5.606086254119873 | BCE Loss: 1.0402281284332275\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.642977714538574 | KNN Loss: 5.618839263916016 | BCE Loss: 1.0241384506225586\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.6552228927612305 | KNN Loss: 5.618826866149902 | BCE Loss: 1.0363959074020386\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.7126288414001465 | KNN Loss: 5.655246734619141 | BCE Loss: 1.0573821067810059\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.672775745391846 | KNN Loss: 5.6308512687683105 | BCE Loss: 1.0419244766235352\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.659327030181885 | KNN Loss: 5.6178412437438965 | BCE Loss: 1.0414859056472778\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.677924156188965 | KNN Loss: 5.633586406707764 | BCE Loss: 1.044337511062622\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.671305179595947 | KNN Loss: 5.643775939941406 | BCE Loss: 1.0275293588638306\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.67262077331543 | KNN Loss: 5.615699768066406 | BCE Loss: 1.0569207668304443\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.651322364807129 | KNN Loss: 5.621485710144043 | BCE Loss: 1.0298367738723755\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 6.726868629455566 | KNN Loss: 5.684970855712891 | BCE Loss: 1.0418977737426758\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.670210361480713 | KNN Loss: 5.657585620880127 | BCE Loss: 1.012624740600586\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.657449245452881 | KNN Loss: 5.594322681427002 | BCE Loss: 1.0631266832351685\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.703303337097168 | KNN Loss: 5.637366771697998 | BCE Loss: 1.06593656539917\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.669914245605469 | KNN Loss: 5.607232570648193 | BCE Loss: 1.0626815557479858\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.6754608154296875 | KNN Loss: 5.657893180847168 | BCE Loss: 1.0175678730010986\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.661085605621338 | KNN Loss: 5.60737943649292 | BCE Loss: 1.0537062883377075\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.646293640136719 | KNN Loss: 5.593914031982422 | BCE Loss: 1.0523796081542969\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.718440055847168 | KNN Loss: 5.670557022094727 | BCE Loss: 1.0478830337524414\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.668118476867676 | KNN Loss: 5.613458633422852 | BCE Loss: 1.0546600818634033\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.642807960510254 | KNN Loss: 5.603049278259277 | BCE Loss: 1.0397584438323975\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.66488790512085 | KNN Loss: 5.617305278778076 | BCE Loss: 1.0475826263427734\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.680484771728516 | KNN Loss: 5.648404598236084 | BCE Loss: 1.0320801734924316\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.633249282836914 | KNN Loss: 5.593766689300537 | BCE Loss: 1.039482831954956\n",
      "Epoch   352: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.679558753967285 | KNN Loss: 5.624003887176514 | BCE Loss: 1.0555546283721924\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.685011863708496 | KNN Loss: 5.658271312713623 | BCE Loss: 1.026740312576294\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.695525169372559 | KNN Loss: 5.646116256713867 | BCE Loss: 1.0494091510772705\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.685727119445801 | KNN Loss: 5.628244400024414 | BCE Loss: 1.0574828386306763\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.6621994972229 | KNN Loss: 5.635922908782959 | BCE Loss: 1.0262764692306519\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.635824203491211 | KNN Loss: 5.611316204071045 | BCE Loss: 1.0245082378387451\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.652169227600098 | KNN Loss: 5.63932991027832 | BCE Loss: 1.0128390789031982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 6.65253210067749 | KNN Loss: 5.621780872344971 | BCE Loss: 1.03075110912323\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.707886695861816 | KNN Loss: 5.651646137237549 | BCE Loss: 1.0562403202056885\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.715741157531738 | KNN Loss: 5.6532769203186035 | BCE Loss: 1.0624641180038452\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.67443323135376 | KNN Loss: 5.591287136077881 | BCE Loss: 1.083146095275879\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.659801959991455 | KNN Loss: 5.597300052642822 | BCE Loss: 1.0625019073486328\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.667236328125 | KNN Loss: 5.633293628692627 | BCE Loss: 1.033942461013794\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.644109725952148 | KNN Loss: 5.598114013671875 | BCE Loss: 1.0459957122802734\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.669895648956299 | KNN Loss: 5.616996765136719 | BCE Loss: 1.0528990030288696\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.61371374130249 | KNN Loss: 5.602535724639893 | BCE Loss: 1.011177897453308\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.7341718673706055 | KNN Loss: 5.654162406921387 | BCE Loss: 1.0800092220306396\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.681013584136963 | KNN Loss: 5.6225457191467285 | BCE Loss: 1.0584678649902344\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.644867420196533 | KNN Loss: 5.601358890533447 | BCE Loss: 1.043508529663086\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.6787109375 | KNN Loss: 5.629999160766602 | BCE Loss: 1.0487117767333984\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.68656587600708 | KNN Loss: 5.6195807456970215 | BCE Loss: 1.0669851303100586\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.674903869628906 | KNN Loss: 5.620889186859131 | BCE Loss: 1.054014801979065\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.6450934410095215 | KNN Loss: 5.606437683105469 | BCE Loss: 1.0386557579040527\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.663445949554443 | KNN Loss: 5.619694709777832 | BCE Loss: 1.0437512397766113\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.748281955718994 | KNN Loss: 5.703157424926758 | BCE Loss: 1.0451245307922363\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.664186477661133 | KNN Loss: 5.6089768409729 | BCE Loss: 1.055209755897522\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.640730381011963 | KNN Loss: 5.599923133850098 | BCE Loss: 1.0408071279525757\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.680768966674805 | KNN Loss: 5.626027584075928 | BCE Loss: 1.054741382598877\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.622250556945801 | KNN Loss: 5.600395679473877 | BCE Loss: 1.021855115890503\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.661141395568848 | KNN Loss: 5.633763313293457 | BCE Loss: 1.0273780822753906\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.65243673324585 | KNN Loss: 5.630399227142334 | BCE Loss: 1.0220376253128052\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.715721607208252 | KNN Loss: 5.695807933807373 | BCE Loss: 1.0199137926101685\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.694399833679199 | KNN Loss: 5.642196178436279 | BCE Loss: 1.0522035360336304\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 6.673425674438477 | KNN Loss: 5.630058765411377 | BCE Loss: 1.0433670282363892\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.655388832092285 | KNN Loss: 5.602975368499756 | BCE Loss: 1.0524137020111084\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.73435115814209 | KNN Loss: 5.711180210113525 | BCE Loss: 1.0231711864471436\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.717051982879639 | KNN Loss: 5.673053741455078 | BCE Loss: 1.0439982414245605\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.6787919998168945 | KNN Loss: 5.609261989593506 | BCE Loss: 1.0695298910140991\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.675600051879883 | KNN Loss: 5.6300368309021 | BCE Loss: 1.045562982559204\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.610721588134766 | KNN Loss: 5.593998432159424 | BCE Loss: 1.0167231559753418\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.6607441902160645 | KNN Loss: 5.6143269538879395 | BCE Loss: 1.046417236328125\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.733593940734863 | KNN Loss: 5.71110200881958 | BCE Loss: 1.0224919319152832\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.652831077575684 | KNN Loss: 5.591921329498291 | BCE Loss: 1.0609095096588135\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.624418258666992 | KNN Loss: 5.5981621742248535 | BCE Loss: 1.0262560844421387\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.6915788650512695 | KNN Loss: 5.619857311248779 | BCE Loss: 1.0717217922210693\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 6.66163444519043 | KNN Loss: 5.603943347930908 | BCE Loss: 1.0576910972595215\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.63783597946167 | KNN Loss: 5.606081485748291 | BCE Loss: 1.031754493713379\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.660578727722168 | KNN Loss: 5.607204914093018 | BCE Loss: 1.0533740520477295\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.627939701080322 | KNN Loss: 5.5938496589660645 | BCE Loss: 1.0340901613235474\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.7223358154296875 | KNN Loss: 5.67085599899292 | BCE Loss: 1.0514795780181885\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.663726806640625 | KNN Loss: 5.594640731811523 | BCE Loss: 1.0690863132476807\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.647204399108887 | KNN Loss: 5.619646072387695 | BCE Loss: 1.0275582075119019\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.6549787521362305 | KNN Loss: 5.606988906860352 | BCE Loss: 1.047990083694458\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 6.616693019866943 | KNN Loss: 5.602908611297607 | BCE Loss: 1.013784408569336\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.629934310913086 | KNN Loss: 5.592534065246582 | BCE Loss: 1.0374001264572144\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 6.626891136169434 | KNN Loss: 5.612112045288086 | BCE Loss: 1.0147788524627686\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.703542232513428 | KNN Loss: 5.645625114440918 | BCE Loss: 1.0579171180725098\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.743544578552246 | KNN Loss: 5.699728012084961 | BCE Loss: 1.043816328048706\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.689578056335449 | KNN Loss: 5.640935897827148 | BCE Loss: 1.0486423969268799\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.638331413269043 | KNN Loss: 5.597917079925537 | BCE Loss: 1.0404142141342163\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.679760456085205 | KNN Loss: 5.64047908782959 | BCE Loss: 1.0392814874649048\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.728606700897217 | KNN Loss: 5.671403408050537 | BCE Loss: 1.0572031736373901\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.6376566886901855 | KNN Loss: 5.611010551452637 | BCE Loss: 1.0266461372375488\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.6873016357421875 | KNN Loss: 5.594597339630127 | BCE Loss: 1.09270441532135\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.723925590515137 | KNN Loss: 5.637560844421387 | BCE Loss: 1.086364984512329\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.6330885887146 | KNN Loss: 5.618034839630127 | BCE Loss: 1.015053629875183\n",
      "Epoch   363: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.7437591552734375 | KNN Loss: 5.66738224029541 | BCE Loss: 1.0763769149780273\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.6680707931518555 | KNN Loss: 5.638050079345703 | BCE Loss: 1.0300204753875732\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.64107608795166 | KNN Loss: 5.599774360656738 | BCE Loss: 1.0413017272949219\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.690389156341553 | KNN Loss: 5.650391578674316 | BCE Loss: 1.0399976968765259\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.67195987701416 | KNN Loss: 5.632568836212158 | BCE Loss: 1.039391040802002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.68305778503418 | KNN Loss: 5.632984638214111 | BCE Loss: 1.0500731468200684\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.68613862991333 | KNN Loss: 5.650823593139648 | BCE Loss: 1.0353151559829712\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.624023914337158 | KNN Loss: 5.597300052642822 | BCE Loss: 1.026723861694336\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.684288024902344 | KNN Loss: 5.6822404861450195 | BCE Loss: 1.0020475387573242\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.667510032653809 | KNN Loss: 5.615283012390137 | BCE Loss: 1.0522270202636719\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.641329288482666 | KNN Loss: 5.63900089263916 | BCE Loss: 1.0023282766342163\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.627747535705566 | KNN Loss: 5.5930304527282715 | BCE Loss: 1.0347168445587158\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.639228820800781 | KNN Loss: 5.6053595542907715 | BCE Loss: 1.0338690280914307\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.727634429931641 | KNN Loss: 5.647984981536865 | BCE Loss: 1.0796492099761963\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.655526161193848 | KNN Loss: 5.607811450958252 | BCE Loss: 1.0477144718170166\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.653831481933594 | KNN Loss: 5.594302177429199 | BCE Loss: 1.059529423713684\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 6.6627583503723145 | KNN Loss: 5.619729042053223 | BCE Loss: 1.0430293083190918\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.668145179748535 | KNN Loss: 5.606564998626709 | BCE Loss: 1.0615801811218262\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.648332118988037 | KNN Loss: 5.611107349395752 | BCE Loss: 1.0372247695922852\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.670485496520996 | KNN Loss: 5.59954309463501 | BCE Loss: 1.0709421634674072\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.685820579528809 | KNN Loss: 5.6146440505981445 | BCE Loss: 1.0711766481399536\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 6.709785461425781 | KNN Loss: 5.656012535095215 | BCE Loss: 1.0537731647491455\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.645116329193115 | KNN Loss: 5.6121134757995605 | BCE Loss: 1.0330028533935547\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.67680549621582 | KNN Loss: 5.645570278167725 | BCE Loss: 1.0312352180480957\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.647858619689941 | KNN Loss: 5.602774620056152 | BCE Loss: 1.045083999633789\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.692766189575195 | KNN Loss: 5.63430643081665 | BCE Loss: 1.0584595203399658\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.6308417320251465 | KNN Loss: 5.598386287689209 | BCE Loss: 1.032455325126648\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.668256759643555 | KNN Loss: 5.626395225524902 | BCE Loss: 1.0418615341186523\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.690197944641113 | KNN Loss: 5.616047382354736 | BCE Loss: 1.0741504430770874\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.608103275299072 | KNN Loss: 5.5936784744262695 | BCE Loss: 1.0144246816635132\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.67656946182251 | KNN Loss: 5.619602680206299 | BCE Loss: 1.0569669008255005\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.65660285949707 | KNN Loss: 5.606213569641113 | BCE Loss: 1.050389289855957\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.65902042388916 | KNN Loss: 5.622166156768799 | BCE Loss: 1.0368542671203613\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.6366777420043945 | KNN Loss: 5.595614910125732 | BCE Loss: 1.041062593460083\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.611380100250244 | KNN Loss: 5.589918613433838 | BCE Loss: 1.0214614868164062\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.676576614379883 | KNN Loss: 5.607316017150879 | BCE Loss: 1.0692604780197144\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.645055770874023 | KNN Loss: 5.60584831237793 | BCE Loss: 1.0392072200775146\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.721739768981934 | KNN Loss: 5.681094646453857 | BCE Loss: 1.040644884109497\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.647450923919678 | KNN Loss: 5.606378078460693 | BCE Loss: 1.041072964668274\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.708266258239746 | KNN Loss: 5.640298843383789 | BCE Loss: 1.0679676532745361\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.67116117477417 | KNN Loss: 5.599976062774658 | BCE Loss: 1.0711852312088013\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.646475791931152 | KNN Loss: 5.6104021072387695 | BCE Loss: 1.0360734462738037\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.639999866485596 | KNN Loss: 5.597710132598877 | BCE Loss: 1.0422898530960083\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.691323757171631 | KNN Loss: 5.653928756713867 | BCE Loss: 1.0373951196670532\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.622241020202637 | KNN Loss: 5.5997161865234375 | BCE Loss: 1.0225248336791992\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.688143730163574 | KNN Loss: 5.64725923538208 | BCE Loss: 1.040884256362915\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.685267448425293 | KNN Loss: 5.619852542877197 | BCE Loss: 1.0654151439666748\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.637638092041016 | KNN Loss: 5.601871490478516 | BCE Loss: 1.0357666015625\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.7018046379089355 | KNN Loss: 5.661036491394043 | BCE Loss: 1.0407681465148926\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.630565166473389 | KNN Loss: 5.592855930328369 | BCE Loss: 1.0377092361450195\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.731554985046387 | KNN Loss: 5.685587406158447 | BCE Loss: 1.0459678173065186\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.665375232696533 | KNN Loss: 5.626722812652588 | BCE Loss: 1.0386525392532349\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 6.693472862243652 | KNN Loss: 5.621881484985352 | BCE Loss: 1.0715916156768799\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.668858528137207 | KNN Loss: 5.619020938873291 | BCE Loss: 1.049837350845337\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.743388652801514 | KNN Loss: 5.708693504333496 | BCE Loss: 1.0346951484680176\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.663686752319336 | KNN Loss: 5.597574234008789 | BCE Loss: 1.0661122798919678\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.633963584899902 | KNN Loss: 5.602699279785156 | BCE Loss: 1.0312641859054565\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.683109283447266 | KNN Loss: 5.647853851318359 | BCE Loss: 1.0352551937103271\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.6526689529418945 | KNN Loss: 5.6414794921875 | BCE Loss: 1.0111894607543945\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.718130111694336 | KNN Loss: 5.707557201385498 | BCE Loss: 1.0105726718902588\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.667466640472412 | KNN Loss: 5.614466190338135 | BCE Loss: 1.0530004501342773\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.643394470214844 | KNN Loss: 5.595127105712891 | BCE Loss: 1.0482676029205322\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.631202697753906 | KNN Loss: 5.602987289428711 | BCE Loss: 1.0282155275344849\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 6.648706436157227 | KNN Loss: 5.596072673797607 | BCE Loss: 1.0526340007781982\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.725939750671387 | KNN Loss: 5.665258884429932 | BCE Loss: 1.0606811046600342\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.624151229858398 | KNN Loss: 5.616719722747803 | BCE Loss: 1.0074312686920166\n",
      "Epoch   374: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.68355131149292 | KNN Loss: 5.616973876953125 | BCE Loss: 1.066577434539795\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.619729995727539 | KNN Loss: 5.594104290008545 | BCE Loss: 1.0256259441375732\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.788560390472412 | KNN Loss: 5.733752250671387 | BCE Loss: 1.0548080205917358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.65596342086792 | KNN Loss: 5.624782085418701 | BCE Loss: 1.0311812162399292\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 6.692930221557617 | KNN Loss: 5.64038610458374 | BCE Loss: 1.052544116973877\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.738572597503662 | KNN Loss: 5.692989826202393 | BCE Loss: 1.0455827713012695\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.660191059112549 | KNN Loss: 5.621275424957275 | BCE Loss: 1.038915753364563\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 6.649115085601807 | KNN Loss: 5.595417499542236 | BCE Loss: 1.0536975860595703\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.64787483215332 | KNN Loss: 5.6032562255859375 | BCE Loss: 1.044618844985962\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.65924072265625 | KNN Loss: 5.599930286407471 | BCE Loss: 1.0593105554580688\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.699351787567139 | KNN Loss: 5.653830051422119 | BCE Loss: 1.0455217361450195\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.707160949707031 | KNN Loss: 5.643906593322754 | BCE Loss: 1.0632545948028564\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.670583248138428 | KNN Loss: 5.61018180847168 | BCE Loss: 1.060401439666748\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.637014389038086 | KNN Loss: 5.596303462982178 | BCE Loss: 1.040710687637329\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.67311954498291 | KNN Loss: 5.66619873046875 | BCE Loss: 1.0069206953048706\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.614048004150391 | KNN Loss: 5.598570346832275 | BCE Loss: 1.0154778957366943\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.6376848220825195 | KNN Loss: 5.5943603515625 | BCE Loss: 1.0433242321014404\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.658855438232422 | KNN Loss: 5.634212493896484 | BCE Loss: 1.024642825126648\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.680476665496826 | KNN Loss: 5.656002998352051 | BCE Loss: 1.0244735479354858\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.639122486114502 | KNN Loss: 5.612329483032227 | BCE Loss: 1.026793122291565\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.65877628326416 | KNN Loss: 5.60323429107666 | BCE Loss: 1.055542230606079\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.670190811157227 | KNN Loss: 5.60999870300293 | BCE Loss: 1.060192346572876\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.6681318283081055 | KNN Loss: 5.6171674728393555 | BCE Loss: 1.050964117050171\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.683239459991455 | KNN Loss: 5.596498012542725 | BCE Loss: 1.08674156665802\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.676050186157227 | KNN Loss: 5.613018035888672 | BCE Loss: 1.0630319118499756\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.616819858551025 | KNN Loss: 5.596249580383301 | BCE Loss: 1.0205702781677246\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.690903663635254 | KNN Loss: 5.627954959869385 | BCE Loss: 1.06294846534729\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.730975151062012 | KNN Loss: 5.693083763122559 | BCE Loss: 1.0378916263580322\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.68064546585083 | KNN Loss: 5.6301093101501465 | BCE Loss: 1.0505361557006836\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.630478858947754 | KNN Loss: 5.609035491943359 | BCE Loss: 1.0214433670043945\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.700708866119385 | KNN Loss: 5.64901876449585 | BCE Loss: 1.0516899824142456\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.695699691772461 | KNN Loss: 5.636763095855713 | BCE Loss: 1.0589368343353271\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.673794746398926 | KNN Loss: 5.631694793701172 | BCE Loss: 1.042100191116333\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.669854164123535 | KNN Loss: 5.615990161895752 | BCE Loss: 1.0538638830184937\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.736248016357422 | KNN Loss: 5.659145355224609 | BCE Loss: 1.0771026611328125\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.62738037109375 | KNN Loss: 5.608494281768799 | BCE Loss: 1.0188862085342407\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.756673812866211 | KNN Loss: 5.711718559265137 | BCE Loss: 1.0449551343917847\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.664157867431641 | KNN Loss: 5.633975505828857 | BCE Loss: 1.0301826000213623\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.647621154785156 | KNN Loss: 5.592929840087891 | BCE Loss: 1.0546915531158447\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.68510103225708 | KNN Loss: 5.652749061584473 | BCE Loss: 1.0323518514633179\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.648664474487305 | KNN Loss: 5.598766326904297 | BCE Loss: 1.0498982667922974\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.671127796173096 | KNN Loss: 5.6142897605896 | BCE Loss: 1.0568379163742065\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.646685600280762 | KNN Loss: 5.635777950286865 | BCE Loss: 1.0109074115753174\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.675318241119385 | KNN Loss: 5.597881317138672 | BCE Loss: 1.077436923980713\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.606116771697998 | KNN Loss: 5.59970235824585 | BCE Loss: 1.0064144134521484\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.6597371101379395 | KNN Loss: 5.617997646331787 | BCE Loss: 1.0417394638061523\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.647393703460693 | KNN Loss: 5.616033554077148 | BCE Loss: 1.031360149383545\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.698329925537109 | KNN Loss: 5.624092102050781 | BCE Loss: 1.074237585067749\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.625789642333984 | KNN Loss: 5.617541313171387 | BCE Loss: 1.0082485675811768\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.756234169006348 | KNN Loss: 5.720593452453613 | BCE Loss: 1.0356409549713135\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.623932838439941 | KNN Loss: 5.599215984344482 | BCE Loss: 1.0247169733047485\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.625329494476318 | KNN Loss: 5.600499629974365 | BCE Loss: 1.0248299837112427\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.641554355621338 | KNN Loss: 5.601069450378418 | BCE Loss: 1.04048490524292\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.662079334259033 | KNN Loss: 5.59530782699585 | BCE Loss: 1.0667715072631836\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 6.632830619812012 | KNN Loss: 5.603856563568115 | BCE Loss: 1.0289742946624756\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.714953422546387 | KNN Loss: 5.663074493408203 | BCE Loss: 1.0518789291381836\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.6443328857421875 | KNN Loss: 5.600734233856201 | BCE Loss: 1.0435984134674072\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.629477024078369 | KNN Loss: 5.602910041809082 | BCE Loss: 1.0265668630599976\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.621492385864258 | KNN Loss: 5.5977606773376465 | BCE Loss: 1.0237314701080322\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.676271438598633 | KNN Loss: 5.603487491607666 | BCE Loss: 1.0727839469909668\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.6336669921875 | KNN Loss: 5.605630874633789 | BCE Loss: 1.028036117553711\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 6.675001621246338 | KNN Loss: 5.612425327301025 | BCE Loss: 1.0625762939453125\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.743592739105225 | KNN Loss: 5.682967662811279 | BCE Loss: 1.0606250762939453\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 6.67161750793457 | KNN Loss: 5.655584335327148 | BCE Loss: 1.016033411026001\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.761987686157227 | KNN Loss: 5.687707901000977 | BCE Loss: 1.074279546737671\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.655991077423096 | KNN Loss: 5.604832649230957 | BCE Loss: 1.0511585474014282\n",
      "Epoch   385: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.686540126800537 | KNN Loss: 5.622786045074463 | BCE Loss: 1.0637542009353638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.649996757507324 | KNN Loss: 5.616557598114014 | BCE Loss: 1.0334391593933105\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.693792343139648 | KNN Loss: 5.605706214904785 | BCE Loss: 1.0880863666534424\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.647134304046631 | KNN Loss: 5.61154317855835 | BCE Loss: 1.0355911254882812\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.659626007080078 | KNN Loss: 5.60463809967041 | BCE Loss: 1.054987907409668\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.6413164138793945 | KNN Loss: 5.598004341125488 | BCE Loss: 1.0433118343353271\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.700868129730225 | KNN Loss: 5.666780948638916 | BCE Loss: 1.0340871810913086\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 6.625696182250977 | KNN Loss: 5.607847213745117 | BCE Loss: 1.0178489685058594\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.661925315856934 | KNN Loss: 5.617593765258789 | BCE Loss: 1.0443313121795654\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.732197284698486 | KNN Loss: 5.652177333831787 | BCE Loss: 1.0800200700759888\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.765115737915039 | KNN Loss: 5.71684455871582 | BCE Loss: 1.0482709407806396\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.736980438232422 | KNN Loss: 5.656400203704834 | BCE Loss: 1.0805799961090088\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 6.70564603805542 | KNN Loss: 5.643383026123047 | BCE Loss: 1.0622628927230835\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.661000728607178 | KNN Loss: 5.6178202629089355 | BCE Loss: 1.0431804656982422\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.7057061195373535 | KNN Loss: 5.640561580657959 | BCE Loss: 1.065144658088684\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.644023418426514 | KNN Loss: 5.611079216003418 | BCE Loss: 1.0329442024230957\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.645195960998535 | KNN Loss: 5.589731693267822 | BCE Loss: 1.055464506149292\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.753256797790527 | KNN Loss: 5.693598747253418 | BCE Loss: 1.0596580505371094\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.673361301422119 | KNN Loss: 5.609247207641602 | BCE Loss: 1.0641142129898071\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.637677192687988 | KNN Loss: 5.5992584228515625 | BCE Loss: 1.0384187698364258\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.685360431671143 | KNN Loss: 5.666092395782471 | BCE Loss: 1.0192679166793823\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.688023567199707 | KNN Loss: 5.610339641571045 | BCE Loss: 1.077683925628662\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.65255069732666 | KNN Loss: 5.610621929168701 | BCE Loss: 1.0419288873672485\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.632285118103027 | KNN Loss: 5.615708827972412 | BCE Loss: 1.0165765285491943\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.6919097900390625 | KNN Loss: 5.659877300262451 | BCE Loss: 1.0320322513580322\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.655757904052734 | KNN Loss: 5.609168529510498 | BCE Loss: 1.0465892553329468\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.700887680053711 | KNN Loss: 5.64294958114624 | BCE Loss: 1.0579380989074707\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.735456466674805 | KNN Loss: 5.65491247177124 | BCE Loss: 1.0805439949035645\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 6.661835193634033 | KNN Loss: 5.625208854675293 | BCE Loss: 1.0366262197494507\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 6.645758628845215 | KNN Loss: 5.627029895782471 | BCE Loss: 1.0187288522720337\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 6.6504597663879395 | KNN Loss: 5.600152969360352 | BCE Loss: 1.0503066778182983\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.6402082443237305 | KNN Loss: 5.602965354919434 | BCE Loss: 1.0372428894042969\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 6.7103376388549805 | KNN Loss: 5.645523548126221 | BCE Loss: 1.0648140907287598\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 6.67494535446167 | KNN Loss: 5.599081039428711 | BCE Loss: 1.0758644342422485\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.660418510437012 | KNN Loss: 5.60207462310791 | BCE Loss: 1.0583441257476807\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.602865219116211 | KNN Loss: 5.60137414932251 | BCE Loss: 1.0014910697937012\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.6590728759765625 | KNN Loss: 5.633267879486084 | BCE Loss: 1.0258052349090576\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.641508102416992 | KNN Loss: 5.5957112312316895 | BCE Loss: 1.0457966327667236\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.695580959320068 | KNN Loss: 5.648735523223877 | BCE Loss: 1.0468454360961914\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.661309719085693 | KNN Loss: 5.6384100914001465 | BCE Loss: 1.0228997468948364\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.725404739379883 | KNN Loss: 5.674931049346924 | BCE Loss: 1.050473928451538\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.671360492706299 | KNN Loss: 5.6435441970825195 | BCE Loss: 1.0278162956237793\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.64954137802124 | KNN Loss: 5.602952480316162 | BCE Loss: 1.0465888977050781\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.650874614715576 | KNN Loss: 5.595798492431641 | BCE Loss: 1.0550761222839355\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.71893310546875 | KNN Loss: 5.687830924987793 | BCE Loss: 1.031102180480957\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.6629228591918945 | KNN Loss: 5.597349166870117 | BCE Loss: 1.0655734539031982\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.6497578620910645 | KNN Loss: 5.659417629241943 | BCE Loss: 0.9903404116630554\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.732735633850098 | KNN Loss: 5.649625778198242 | BCE Loss: 1.0831096172332764\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.709256172180176 | KNN Loss: 5.63805627822876 | BCE Loss: 1.071199893951416\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.730245113372803 | KNN Loss: 5.673666000366211 | BCE Loss: 1.0565791130065918\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.625469207763672 | KNN Loss: 5.593024253845215 | BCE Loss: 1.0324450731277466\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.675116062164307 | KNN Loss: 5.627804756164551 | BCE Loss: 1.0473114252090454\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.687539577484131 | KNN Loss: 5.631311416625977 | BCE Loss: 1.0562280416488647\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 6.664339542388916 | KNN Loss: 5.629373550415039 | BCE Loss: 1.034965991973877\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 6.7148823738098145 | KNN Loss: 5.668882846832275 | BCE Loss: 1.0459994077682495\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 6.703519344329834 | KNN Loss: 5.638991355895996 | BCE Loss: 1.064527988433838\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.653378963470459 | KNN Loss: 5.609755039215088 | BCE Loss: 1.043623924255371\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.656688213348389 | KNN Loss: 5.61900520324707 | BCE Loss: 1.0376830101013184\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.698624134063721 | KNN Loss: 5.656367301940918 | BCE Loss: 1.0422568321228027\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.673272132873535 | KNN Loss: 5.611824035644531 | BCE Loss: 1.0614482164382935\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 6.667141437530518 | KNN Loss: 5.638887882232666 | BCE Loss: 1.0282535552978516\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.663268089294434 | KNN Loss: 5.630254745483398 | BCE Loss: 1.033013105392456\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.684065341949463 | KNN Loss: 5.614855766296387 | BCE Loss: 1.0692094564437866\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 6.672219753265381 | KNN Loss: 5.63303804397583 | BCE Loss: 1.0391817092895508\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.708425521850586 | KNN Loss: 5.656027317047119 | BCE Loss: 1.0523980855941772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 6.6920294761657715 | KNN Loss: 5.613185882568359 | BCE Loss: 1.0788434743881226\n",
      "Epoch   396: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.6530351638793945 | KNN Loss: 5.603431701660156 | BCE Loss: 1.0496032238006592\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.655535697937012 | KNN Loss: 5.617489337921143 | BCE Loss: 1.0380465984344482\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.682718276977539 | KNN Loss: 5.658532619476318 | BCE Loss: 1.0241857767105103\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.668723106384277 | KNN Loss: 5.647424221038818 | BCE Loss: 1.021298885345459\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.668506622314453 | KNN Loss: 5.598869800567627 | BCE Loss: 1.0696368217468262\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 6.645707607269287 | KNN Loss: 5.610860824584961 | BCE Loss: 1.0348467826843262\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.643596172332764 | KNN Loss: 5.612052917480469 | BCE Loss: 1.0315433740615845\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.685452938079834 | KNN Loss: 5.620508193969727 | BCE Loss: 1.0649447441101074\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.652474880218506 | KNN Loss: 5.620913982391357 | BCE Loss: 1.0315608978271484\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.639781951904297 | KNN Loss: 5.590449333190918 | BCE Loss: 1.0493323802947998\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.667730331420898 | KNN Loss: 5.633740425109863 | BCE Loss: 1.0339901447296143\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.6972503662109375 | KNN Loss: 5.630273342132568 | BCE Loss: 1.0669772624969482\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.711764335632324 | KNN Loss: 5.664801120758057 | BCE Loss: 1.0469629764556885\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.711749076843262 | KNN Loss: 5.670596599578857 | BCE Loss: 1.0411525964736938\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.640986442565918 | KNN Loss: 5.607474327087402 | BCE Loss: 1.0335121154785156\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.653773307800293 | KNN Loss: 5.59390926361084 | BCE Loss: 1.0598642826080322\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.644891738891602 | KNN Loss: 5.594719409942627 | BCE Loss: 1.0501725673675537\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.65848970413208 | KNN Loss: 5.608922958374023 | BCE Loss: 1.0495667457580566\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 6.696977615356445 | KNN Loss: 5.652804851531982 | BCE Loss: 1.0441728830337524\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 6.714709281921387 | KNN Loss: 5.672732353210449 | BCE Loss: 1.0419769287109375\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.664046287536621 | KNN Loss: 5.642395973205566 | BCE Loss: 1.0216503143310547\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.681326866149902 | KNN Loss: 5.634222984313965 | BCE Loss: 1.0471041202545166\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.767024040222168 | KNN Loss: 5.702918529510498 | BCE Loss: 1.0641053915023804\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.706297397613525 | KNN Loss: 5.65381383895874 | BCE Loss: 1.0524835586547852\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.704908847808838 | KNN Loss: 5.624454021453857 | BCE Loss: 1.08045494556427\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.650227069854736 | KNN Loss: 5.600133895874023 | BCE Loss: 1.0500932931900024\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.630188941955566 | KNN Loss: 5.605008602142334 | BCE Loss: 1.025180459022522\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.752625942230225 | KNN Loss: 5.707391262054443 | BCE Loss: 1.0452345609664917\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.6918182373046875 | KNN Loss: 5.652534008026123 | BCE Loss: 1.0392844676971436\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.679941177368164 | KNN Loss: 5.619049072265625 | BCE Loss: 1.060892105102539\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.655885696411133 | KNN Loss: 5.593872547149658 | BCE Loss: 1.0620131492614746\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.659221649169922 | KNN Loss: 5.60476541519165 | BCE Loss: 1.054456353187561\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.680489540100098 | KNN Loss: 5.647660732269287 | BCE Loss: 1.0328290462493896\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.682234764099121 | KNN Loss: 5.645290851593018 | BCE Loss: 1.0369441509246826\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.639087677001953 | KNN Loss: 5.594707012176514 | BCE Loss: 1.0443804264068604\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.76403284072876 | KNN Loss: 5.691793918609619 | BCE Loss: 1.0722390413284302\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.676102638244629 | KNN Loss: 5.610727310180664 | BCE Loss: 1.0653753280639648\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.679192543029785 | KNN Loss: 5.613264560699463 | BCE Loss: 1.0659278631210327\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.727609634399414 | KNN Loss: 5.650367259979248 | BCE Loss: 1.077242136001587\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.703421592712402 | KNN Loss: 5.647182941436768 | BCE Loss: 1.0562386512756348\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.713274002075195 | KNN Loss: 5.686748027801514 | BCE Loss: 1.026525855064392\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.6588568687438965 | KNN Loss: 5.6112518310546875 | BCE Loss: 1.047605037689209\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.6360249519348145 | KNN Loss: 5.604589462280273 | BCE Loss: 1.0314356088638306\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 6.653895854949951 | KNN Loss: 5.605590343475342 | BCE Loss: 1.0483053922653198\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.64343786239624 | KNN Loss: 5.606192588806152 | BCE Loss: 1.0372453927993774\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.68105936050415 | KNN Loss: 5.621453285217285 | BCE Loss: 1.0596059560775757\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.690421104431152 | KNN Loss: 5.641274452209473 | BCE Loss: 1.0491468906402588\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.680544853210449 | KNN Loss: 5.607930660247803 | BCE Loss: 1.0726139545440674\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.627479553222656 | KNN Loss: 5.602272987365723 | BCE Loss: 1.0252065658569336\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.7421064376831055 | KNN Loss: 5.6806135177612305 | BCE Loss: 1.061492919921875\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.681122303009033 | KNN Loss: 5.622801303863525 | BCE Loss: 1.0583209991455078\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.636590003967285 | KNN Loss: 5.594058513641357 | BCE Loss: 1.0425316095352173\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.698616981506348 | KNN Loss: 5.6347737312316895 | BCE Loss: 1.0638432502746582\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.633652687072754 | KNN Loss: 5.5895209312438965 | BCE Loss: 1.0441315174102783\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.6492204666137695 | KNN Loss: 5.602968692779541 | BCE Loss: 1.0462520122528076\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 6.633753299713135 | KNN Loss: 5.5932440757751465 | BCE Loss: 1.0405093431472778\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.681321144104004 | KNN Loss: 5.604531288146973 | BCE Loss: 1.0767898559570312\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.735248565673828 | KNN Loss: 5.679961681365967 | BCE Loss: 1.0552870035171509\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.619819164276123 | KNN Loss: 5.606577396392822 | BCE Loss: 1.0132418870925903\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.735451698303223 | KNN Loss: 5.664170742034912 | BCE Loss: 1.0712809562683105\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 6.676146507263184 | KNN Loss: 5.622955799102783 | BCE Loss: 1.05319082736969\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 6.723614692687988 | KNN Loss: 5.678127765655518 | BCE Loss: 1.0454866886138916\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.654843807220459 | KNN Loss: 5.615470886230469 | BCE Loss: 1.0393729209899902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 6.642141819000244 | KNN Loss: 5.604162216186523 | BCE Loss: 1.0379797220230103\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 6.670628070831299 | KNN Loss: 5.603306770324707 | BCE Loss: 1.0673213005065918\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.640661716461182 | KNN Loss: 5.593796253204346 | BCE Loss: 1.0468655824661255\n",
      "Epoch   407: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.66605806350708 | KNN Loss: 5.602176666259766 | BCE Loss: 1.063881278038025\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.675603866577148 | KNN Loss: 5.60789155960083 | BCE Loss: 1.0677125453948975\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.6283440589904785 | KNN Loss: 5.600160598754883 | BCE Loss: 1.0281834602355957\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.658884048461914 | KNN Loss: 5.6255292892456055 | BCE Loss: 1.0333545207977295\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.663662910461426 | KNN Loss: 5.61779260635376 | BCE Loss: 1.045870065689087\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.648443222045898 | KNN Loss: 5.607017517089844 | BCE Loss: 1.0414254665374756\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.665920257568359 | KNN Loss: 5.62013578414917 | BCE Loss: 1.0457843542099\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.70188045501709 | KNN Loss: 5.644401550292969 | BCE Loss: 1.0574790239334106\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.634411334991455 | KNN Loss: 5.603497505187988 | BCE Loss: 1.0309138298034668\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.627291679382324 | KNN Loss: 5.600093364715576 | BCE Loss: 1.027198076248169\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.625770568847656 | KNN Loss: 5.596254825592041 | BCE Loss: 1.0295155048370361\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.641924858093262 | KNN Loss: 5.626608848571777 | BCE Loss: 1.0153162479400635\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.7534098625183105 | KNN Loss: 5.708141803741455 | BCE Loss: 1.045267939567566\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.6948089599609375 | KNN Loss: 5.644467830657959 | BCE Loss: 1.0503413677215576\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.640080451965332 | KNN Loss: 5.600197792053223 | BCE Loss: 1.0398828983306885\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.671465873718262 | KNN Loss: 5.633550643920898 | BCE Loss: 1.0379154682159424\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.722728729248047 | KNN Loss: 5.646975517272949 | BCE Loss: 1.0757529735565186\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.752811908721924 | KNN Loss: 5.7025041580200195 | BCE Loss: 1.0503078699111938\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.662890434265137 | KNN Loss: 5.6171135902404785 | BCE Loss: 1.045776605606079\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 6.668060302734375 | KNN Loss: 5.598991870880127 | BCE Loss: 1.069068193435669\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.725016117095947 | KNN Loss: 5.681200981140137 | BCE Loss: 1.0438151359558105\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.780116558074951 | KNN Loss: 5.685976982116699 | BCE Loss: 1.094139575958252\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.69713020324707 | KNN Loss: 5.670161724090576 | BCE Loss: 1.0269684791564941\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.662326335906982 | KNN Loss: 5.639498710632324 | BCE Loss: 1.0228276252746582\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.657371520996094 | KNN Loss: 5.6285481452941895 | BCE Loss: 1.0288234949111938\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.658759117126465 | KNN Loss: 5.60828161239624 | BCE Loss: 1.0504775047302246\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.690191745758057 | KNN Loss: 5.650270462036133 | BCE Loss: 1.0399211645126343\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.695147514343262 | KNN Loss: 5.6500749588012695 | BCE Loss: 1.0450726747512817\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.678560256958008 | KNN Loss: 5.6213603019714355 | BCE Loss: 1.0571997165679932\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 6.697741508483887 | KNN Loss: 5.659913063049316 | BCE Loss: 1.0378282070159912\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.680376052856445 | KNN Loss: 5.618499279022217 | BCE Loss: 1.0618770122528076\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.640642166137695 | KNN Loss: 5.603633880615234 | BCE Loss: 1.037008285522461\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.680212020874023 | KNN Loss: 5.615772247314453 | BCE Loss: 1.0644400119781494\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.677641868591309 | KNN Loss: 5.637247562408447 | BCE Loss: 1.0403945446014404\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.670397758483887 | KNN Loss: 5.617329120635986 | BCE Loss: 1.0530685186386108\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.675683975219727 | KNN Loss: 5.643420219421387 | BCE Loss: 1.032263994216919\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.661681652069092 | KNN Loss: 5.612740516662598 | BCE Loss: 1.0489411354064941\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.6942057609558105 | KNN Loss: 5.634167194366455 | BCE Loss: 1.060038447380066\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.650291442871094 | KNN Loss: 5.636549949645996 | BCE Loss: 1.0137412548065186\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.6520490646362305 | KNN Loss: 5.611220359802246 | BCE Loss: 1.0408285856246948\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.689056396484375 | KNN Loss: 5.642179012298584 | BCE Loss: 1.046877384185791\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.66148567199707 | KNN Loss: 5.604349613189697 | BCE Loss: 1.057136058807373\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.658962249755859 | KNN Loss: 5.611143112182617 | BCE Loss: 1.0478191375732422\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.661162376403809 | KNN Loss: 5.612395763397217 | BCE Loss: 1.0487663745880127\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.679512977600098 | KNN Loss: 5.663585662841797 | BCE Loss: 1.0159271955490112\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.642634391784668 | KNN Loss: 5.5967888832092285 | BCE Loss: 1.0458457469940186\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.683799743652344 | KNN Loss: 5.634486198425293 | BCE Loss: 1.0493133068084717\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.643957138061523 | KNN Loss: 5.598674774169922 | BCE Loss: 1.0452826023101807\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.655879974365234 | KNN Loss: 5.597445487976074 | BCE Loss: 1.0584344863891602\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.643741607666016 | KNN Loss: 5.594745635986328 | BCE Loss: 1.0489959716796875\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.66081428527832 | KNN Loss: 5.621132850646973 | BCE Loss: 1.0396811962127686\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.638615131378174 | KNN Loss: 5.593637943267822 | BCE Loss: 1.044977068901062\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.6471076011657715 | KNN Loss: 5.594395160675049 | BCE Loss: 1.0527125597000122\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.692180156707764 | KNN Loss: 5.6496100425720215 | BCE Loss: 1.0425701141357422\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.673996925354004 | KNN Loss: 5.6202263832092285 | BCE Loss: 1.0537707805633545\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.661890983581543 | KNN Loss: 5.626418590545654 | BCE Loss: 1.0354723930358887\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.695837020874023 | KNN Loss: 5.624949932098389 | BCE Loss: 1.0708870887756348\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.703234672546387 | KNN Loss: 5.654280662536621 | BCE Loss: 1.0489540100097656\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 6.639280319213867 | KNN Loss: 5.594498157501221 | BCE Loss: 1.044782280921936\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.6642560958862305 | KNN Loss: 5.598973274230957 | BCE Loss: 1.0652828216552734\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.68303918838501 | KNN Loss: 5.6352691650390625 | BCE Loss: 1.0477700233459473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.6894378662109375 | KNN Loss: 5.614733695983887 | BCE Loss: 1.0747041702270508\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.639437675476074 | KNN Loss: 5.6038498878479 | BCE Loss: 1.0355877876281738\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.629878520965576 | KNN Loss: 5.6123948097229 | BCE Loss: 1.0174837112426758\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.733138084411621 | KNN Loss: 5.692924976348877 | BCE Loss: 1.040212869644165\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.654616355895996 | KNN Loss: 5.622399806976318 | BCE Loss: 1.0322165489196777\n",
      "Epoch   418: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.652790069580078 | KNN Loss: 5.606924533843994 | BCE Loss: 1.045865535736084\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.72524356842041 | KNN Loss: 5.679763317108154 | BCE Loss: 1.0454803705215454\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.75010871887207 | KNN Loss: 5.684408664703369 | BCE Loss: 1.065699815750122\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.6610283851623535 | KNN Loss: 5.615813732147217 | BCE Loss: 1.0452146530151367\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 6.615640163421631 | KNN Loss: 5.594524383544922 | BCE Loss: 1.021115779876709\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.654699325561523 | KNN Loss: 5.601093769073486 | BCE Loss: 1.053605318069458\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.681880950927734 | KNN Loss: 5.638498783111572 | BCE Loss: 1.0433820486068726\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.6342387199401855 | KNN Loss: 5.595921039581299 | BCE Loss: 1.0383177995681763\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.634199142456055 | KNN Loss: 5.601571559906006 | BCE Loss: 1.0326273441314697\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.721482276916504 | KNN Loss: 5.696640968322754 | BCE Loss: 1.02484130859375\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.753625869750977 | KNN Loss: 5.6788249015808105 | BCE Loss: 1.0748010873794556\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.678989410400391 | KNN Loss: 5.606651306152344 | BCE Loss: 1.0723379850387573\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.707190990447998 | KNN Loss: 5.692742824554443 | BCE Loss: 1.0144480466842651\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.65473747253418 | KNN Loss: 5.61316442489624 | BCE Loss: 1.0415728092193604\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.661705017089844 | KNN Loss: 5.6234259605407715 | BCE Loss: 1.0382791757583618\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.742108345031738 | KNN Loss: 5.676212787628174 | BCE Loss: 1.0658953189849854\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.785590171813965 | KNN Loss: 5.742757320404053 | BCE Loss: 1.0428329706192017\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.665546894073486 | KNN Loss: 5.611435890197754 | BCE Loss: 1.054111123085022\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.727225303649902 | KNN Loss: 5.688295364379883 | BCE Loss: 1.0389297008514404\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.704405307769775 | KNN Loss: 5.643649101257324 | BCE Loss: 1.0607562065124512\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.773550987243652 | KNN Loss: 5.713966369628906 | BCE Loss: 1.0595848560333252\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.666620254516602 | KNN Loss: 5.5946736335754395 | BCE Loss: 1.071946620941162\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.6584625244140625 | KNN Loss: 5.615686893463135 | BCE Loss: 1.0427753925323486\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.7060747146606445 | KNN Loss: 5.63056755065918 | BCE Loss: 1.0755071640014648\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.72928524017334 | KNN Loss: 5.684854507446289 | BCE Loss: 1.0444309711456299\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.75927209854126 | KNN Loss: 5.7133660316467285 | BCE Loss: 1.0459060668945312\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.637535572052002 | KNN Loss: 5.622304916381836 | BCE Loss: 1.015230655670166\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.687366008758545 | KNN Loss: 5.652929306030273 | BCE Loss: 1.034436583518982\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 6.7041335105896 | KNN Loss: 5.652432918548584 | BCE Loss: 1.0517005920410156\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.6154937744140625 | KNN Loss: 5.610293388366699 | BCE Loss: 1.0052005052566528\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.622020721435547 | KNN Loss: 5.606292724609375 | BCE Loss: 1.0157279968261719\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.710032939910889 | KNN Loss: 5.646482467651367 | BCE Loss: 1.0635504722595215\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.759320259094238 | KNN Loss: 5.6873674392700195 | BCE Loss: 1.0719528198242188\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.667759895324707 | KNN Loss: 5.593152046203613 | BCE Loss: 1.0746078491210938\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.692523956298828 | KNN Loss: 5.6679301261901855 | BCE Loss: 1.0245935916900635\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.65463924407959 | KNN Loss: 5.6090803146362305 | BCE Loss: 1.0455588102340698\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.6604719161987305 | KNN Loss: 5.611233711242676 | BCE Loss: 1.0492379665374756\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.715401649475098 | KNN Loss: 5.682697296142578 | BCE Loss: 1.0327045917510986\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.634733200073242 | KNN Loss: 5.604478359222412 | BCE Loss: 1.0302550792694092\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.654350280761719 | KNN Loss: 5.609804630279541 | BCE Loss: 1.0445458889007568\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.671233177185059 | KNN Loss: 5.617063999176025 | BCE Loss: 1.054168939590454\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 6.675020217895508 | KNN Loss: 5.625938415527344 | BCE Loss: 1.0490819215774536\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 6.659812927246094 | KNN Loss: 5.597966194152832 | BCE Loss: 1.0618466138839722\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.65582799911499 | KNN Loss: 5.6012654304504395 | BCE Loss: 1.0545625686645508\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.64846658706665 | KNN Loss: 5.614444255828857 | BCE Loss: 1.0340224504470825\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.661075115203857 | KNN Loss: 5.618496894836426 | BCE Loss: 1.0425782203674316\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.718208312988281 | KNN Loss: 5.6960062980651855 | BCE Loss: 1.0222022533416748\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.642812728881836 | KNN Loss: 5.6020355224609375 | BCE Loss: 1.0407774448394775\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.636241912841797 | KNN Loss: 5.595967769622803 | BCE Loss: 1.040273904800415\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.648684501647949 | KNN Loss: 5.59479284286499 | BCE Loss: 1.053891897201538\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.638372898101807 | KNN Loss: 5.594464302062988 | BCE Loss: 1.0439085960388184\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.703509330749512 | KNN Loss: 5.675406455993652 | BCE Loss: 1.0281026363372803\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.642242431640625 | KNN Loss: 5.605770111083984 | BCE Loss: 1.0364723205566406\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 6.627285003662109 | KNN Loss: 5.598550796508789 | BCE Loss: 1.0287344455718994\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.658673286437988 | KNN Loss: 5.602819442749023 | BCE Loss: 1.0558538436889648\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.706489086151123 | KNN Loss: 5.644729137420654 | BCE Loss: 1.0617599487304688\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.6761393547058105 | KNN Loss: 5.623538970947266 | BCE Loss: 1.052600383758545\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.745340347290039 | KNN Loss: 5.687159538269043 | BCE Loss: 1.0581810474395752\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.674375534057617 | KNN Loss: 5.630513668060303 | BCE Loss: 1.0438618659973145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.676879405975342 | KNN Loss: 5.637180805206299 | BCE Loss: 1.039698600769043\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.648305892944336 | KNN Loss: 5.593700408935547 | BCE Loss: 1.05460524559021\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.667458534240723 | KNN Loss: 5.640483856201172 | BCE Loss: 1.0269744396209717\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.638981819152832 | KNN Loss: 5.5918378829956055 | BCE Loss: 1.047143816947937\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.662377834320068 | KNN Loss: 5.619941711425781 | BCE Loss: 1.0424362421035767\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.639060020446777 | KNN Loss: 5.599078178405762 | BCE Loss: 1.0399816036224365\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.719330310821533 | KNN Loss: 5.666464328765869 | BCE Loss: 1.0528661012649536\n",
      "Epoch   429: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.633403301239014 | KNN Loss: 5.598230361938477 | BCE Loss: 1.0351728200912476\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.664177894592285 | KNN Loss: 5.614864826202393 | BCE Loss: 1.0493133068084717\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.644506454467773 | KNN Loss: 5.597888469696045 | BCE Loss: 1.0466177463531494\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.650887489318848 | KNN Loss: 5.619796276092529 | BCE Loss: 1.0310909748077393\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.650412082672119 | KNN Loss: 5.593839168548584 | BCE Loss: 1.0565729141235352\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.722105979919434 | KNN Loss: 5.677878379821777 | BCE Loss: 1.0442273616790771\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.772128105163574 | KNN Loss: 5.702783107757568 | BCE Loss: 1.0693449974060059\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.655208110809326 | KNN Loss: 5.592785358428955 | BCE Loss: 1.0624228715896606\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.719074249267578 | KNN Loss: 5.643190860748291 | BCE Loss: 1.075883388519287\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.64644193649292 | KNN Loss: 5.615415096282959 | BCE Loss: 1.031026840209961\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.660594940185547 | KNN Loss: 5.616419315338135 | BCE Loss: 1.0441758632659912\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.6522297859191895 | KNN Loss: 5.603901386260986 | BCE Loss: 1.0483285188674927\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.682220458984375 | KNN Loss: 5.623226642608643 | BCE Loss: 1.058993935585022\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.666664123535156 | KNN Loss: 5.6116814613342285 | BCE Loss: 1.0549826622009277\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 6.623071670532227 | KNN Loss: 5.603602886199951 | BCE Loss: 1.0194686651229858\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.704586982727051 | KNN Loss: 5.6271233558654785 | BCE Loss: 1.0774636268615723\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.6789350509643555 | KNN Loss: 5.609891414642334 | BCE Loss: 1.069043755531311\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.695926666259766 | KNN Loss: 5.622853755950928 | BCE Loss: 1.0730730295181274\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.6730852127075195 | KNN Loss: 5.6279449462890625 | BCE Loss: 1.0451405048370361\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.641396522521973 | KNN Loss: 5.602430820465088 | BCE Loss: 1.0389659404754639\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 6.699039936065674 | KNN Loss: 5.646612644195557 | BCE Loss: 1.0524272918701172\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.691298961639404 | KNN Loss: 5.635042190551758 | BCE Loss: 1.056256890296936\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.66227912902832 | KNN Loss: 5.61428689956665 | BCE Loss: 1.0479919910430908\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.681813716888428 | KNN Loss: 5.6305623054504395 | BCE Loss: 1.0512514114379883\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.674832344055176 | KNN Loss: 5.628147602081299 | BCE Loss: 1.046684741973877\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.678919792175293 | KNN Loss: 5.627110481262207 | BCE Loss: 1.051809310913086\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.671683311462402 | KNN Loss: 5.611520767211914 | BCE Loss: 1.0601627826690674\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.630729675292969 | KNN Loss: 5.594156742095947 | BCE Loss: 1.0365731716156006\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.658095359802246 | KNN Loss: 5.619359970092773 | BCE Loss: 1.0387351512908936\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.662320137023926 | KNN Loss: 5.628926753997803 | BCE Loss: 1.0333932638168335\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.6380181312561035 | KNN Loss: 5.601128578186035 | BCE Loss: 1.0368894338607788\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.651268482208252 | KNN Loss: 5.592164516448975 | BCE Loss: 1.059104084968567\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.7157883644104 | KNN Loss: 5.66060733795166 | BCE Loss: 1.0551810264587402\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.687098503112793 | KNN Loss: 5.612294673919678 | BCE Loss: 1.0748039484024048\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.651543140411377 | KNN Loss: 5.593678951263428 | BCE Loss: 1.0578640699386597\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.690451622009277 | KNN Loss: 5.631487846374512 | BCE Loss: 1.0589638948440552\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.694425582885742 | KNN Loss: 5.634066581726074 | BCE Loss: 1.060359001159668\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.690044403076172 | KNN Loss: 5.66028356552124 | BCE Loss: 1.0297610759735107\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 6.662187576293945 | KNN Loss: 5.629195690155029 | BCE Loss: 1.0329921245574951\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.682235240936279 | KNN Loss: 5.641365051269531 | BCE Loss: 1.0408700704574585\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.696228981018066 | KNN Loss: 5.643160820007324 | BCE Loss: 1.053067922592163\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.6852593421936035 | KNN Loss: 5.612475395202637 | BCE Loss: 1.0727839469909668\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.677366256713867 | KNN Loss: 5.6483235359191895 | BCE Loss: 1.0290424823760986\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.656790733337402 | KNN Loss: 5.610073566436768 | BCE Loss: 1.0467171669006348\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.687561988830566 | KNN Loss: 5.646512031555176 | BCE Loss: 1.0410500764846802\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.643561363220215 | KNN Loss: 5.613566875457764 | BCE Loss: 1.0299944877624512\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.638848304748535 | KNN Loss: 5.605841159820557 | BCE Loss: 1.0330071449279785\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.679292678833008 | KNN Loss: 5.620547771453857 | BCE Loss: 1.0587446689605713\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 6.6881608963012695 | KNN Loss: 5.6533918380737305 | BCE Loss: 1.0347689390182495\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.70377254486084 | KNN Loss: 5.650819778442383 | BCE Loss: 1.0529528856277466\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.644390106201172 | KNN Loss: 5.607763767242432 | BCE Loss: 1.0366261005401611\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.652670860290527 | KNN Loss: 5.603629112243652 | BCE Loss: 1.049041509628296\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.667935848236084 | KNN Loss: 5.635266304016113 | BCE Loss: 1.0326695442199707\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.683628559112549 | KNN Loss: 5.628700256347656 | BCE Loss: 1.0549283027648926\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.706418991088867 | KNN Loss: 5.639915466308594 | BCE Loss: 1.0665035247802734\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.692901611328125 | KNN Loss: 5.654399871826172 | BCE Loss: 1.038501501083374\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.664003372192383 | KNN Loss: 5.611622333526611 | BCE Loss: 1.0523812770843506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.673843860626221 | KNN Loss: 5.61041259765625 | BCE Loss: 1.0634312629699707\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.785172462463379 | KNN Loss: 5.7191948890686035 | BCE Loss: 1.0659778118133545\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.679688930511475 | KNN Loss: 5.632978439331055 | BCE Loss: 1.04671049118042\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.754046440124512 | KNN Loss: 5.708021640777588 | BCE Loss: 1.046025037765503\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.663088798522949 | KNN Loss: 5.595804691314697 | BCE Loss: 1.067284107208252\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.690507411956787 | KNN Loss: 5.624465465545654 | BCE Loss: 1.0660420656204224\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.637878894805908 | KNN Loss: 5.6069488525390625 | BCE Loss: 1.0309301614761353\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.65732479095459 | KNN Loss: 5.608053207397461 | BCE Loss: 1.049271583557129\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.668726444244385 | KNN Loss: 5.606747150421143 | BCE Loss: 1.0619792938232422\n",
      "Epoch   440: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.658792972564697 | KNN Loss: 5.597543239593506 | BCE Loss: 1.061249852180481\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.702714920043945 | KNN Loss: 5.676880836486816 | BCE Loss: 1.0258338451385498\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.686977386474609 | KNN Loss: 5.628453731536865 | BCE Loss: 1.0585236549377441\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.6406073570251465 | KNN Loss: 5.5981669425964355 | BCE Loss: 1.042440414428711\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.6761603355407715 | KNN Loss: 5.648530006408691 | BCE Loss: 1.0276304483413696\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.732099533081055 | KNN Loss: 5.726783275604248 | BCE Loss: 1.005316138267517\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.659297943115234 | KNN Loss: 5.59459114074707 | BCE Loss: 1.064706802368164\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.693387985229492 | KNN Loss: 5.629590034484863 | BCE Loss: 1.0637977123260498\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.682676315307617 | KNN Loss: 5.639876365661621 | BCE Loss: 1.042799711227417\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.667198181152344 | KNN Loss: 5.627040863037109 | BCE Loss: 1.0401571989059448\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.642217636108398 | KNN Loss: 5.592893123626709 | BCE Loss: 1.0493242740631104\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.69758415222168 | KNN Loss: 5.638269424438477 | BCE Loss: 1.0593147277832031\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.613643169403076 | KNN Loss: 5.590683937072754 | BCE Loss: 1.0229592323303223\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.64809513092041 | KNN Loss: 5.5926289558410645 | BCE Loss: 1.0554661750793457\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.669766426086426 | KNN Loss: 5.600916862487793 | BCE Loss: 1.0688495635986328\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.698400974273682 | KNN Loss: 5.680932521820068 | BCE Loss: 1.0174684524536133\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.671706199645996 | KNN Loss: 5.600477695465088 | BCE Loss: 1.0712287425994873\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.666691780090332 | KNN Loss: 5.647563457489014 | BCE Loss: 1.0191283226013184\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.645880699157715 | KNN Loss: 5.613123416900635 | BCE Loss: 1.032757043838501\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 6.748236656188965 | KNN Loss: 5.700681209564209 | BCE Loss: 1.047555685043335\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.617193222045898 | KNN Loss: 5.596121311187744 | BCE Loss: 1.0210716724395752\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.629408836364746 | KNN Loss: 5.604979991912842 | BCE Loss: 1.0244288444519043\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.689296722412109 | KNN Loss: 5.620892524719238 | BCE Loss: 1.0684044361114502\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.681832313537598 | KNN Loss: 5.642971992492676 | BCE Loss: 1.0388602018356323\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.639888763427734 | KNN Loss: 5.592717170715332 | BCE Loss: 1.0471713542938232\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.696628093719482 | KNN Loss: 5.652699947357178 | BCE Loss: 1.0439280271530151\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.631142616271973 | KNN Loss: 5.593085765838623 | BCE Loss: 1.0380566120147705\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.676605224609375 | KNN Loss: 5.63157844543457 | BCE Loss: 1.0450268983840942\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.659635066986084 | KNN Loss: 5.615054607391357 | BCE Loss: 1.0445804595947266\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.662210941314697 | KNN Loss: 5.642073631286621 | BCE Loss: 1.0201371908187866\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.719478130340576 | KNN Loss: 5.670314788818359 | BCE Loss: 1.0491633415222168\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.724420070648193 | KNN Loss: 5.680112361907959 | BCE Loss: 1.0443075895309448\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.711933612823486 | KNN Loss: 5.63747501373291 | BCE Loss: 1.0744587182998657\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.673851013183594 | KNN Loss: 5.6253557205200195 | BCE Loss: 1.0484952926635742\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.705966472625732 | KNN Loss: 5.648192882537842 | BCE Loss: 1.057773470878601\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.648588180541992 | KNN Loss: 5.602283477783203 | BCE Loss: 1.0463048219680786\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.724740028381348 | KNN Loss: 5.661962032318115 | BCE Loss: 1.0627779960632324\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.625927925109863 | KNN Loss: 5.590107440948486 | BCE Loss: 1.035820722579956\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.7278923988342285 | KNN Loss: 5.689971446990967 | BCE Loss: 1.0379210710525513\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.709881782531738 | KNN Loss: 5.658454418182373 | BCE Loss: 1.0514276027679443\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 6.659985542297363 | KNN Loss: 5.59941291809082 | BCE Loss: 1.0605725049972534\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.674809455871582 | KNN Loss: 5.604389190673828 | BCE Loss: 1.0704200267791748\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.712776184082031 | KNN Loss: 5.653008937835693 | BCE Loss: 1.059767246246338\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.628796100616455 | KNN Loss: 5.594950199127197 | BCE Loss: 1.0338459014892578\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.687311172485352 | KNN Loss: 5.606019020080566 | BCE Loss: 1.0812921524047852\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.675443172454834 | KNN Loss: 5.655592918395996 | BCE Loss: 1.0198503732681274\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 6.626437664031982 | KNN Loss: 5.609123706817627 | BCE Loss: 1.017314076423645\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 6.6369547843933105 | KNN Loss: 5.5942864418029785 | BCE Loss: 1.0426684617996216\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.6536688804626465 | KNN Loss: 5.598875522613525 | BCE Loss: 1.054793357849121\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.656404972076416 | KNN Loss: 5.596921920776367 | BCE Loss: 1.0594830513000488\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.736938953399658 | KNN Loss: 5.682673454284668 | BCE Loss: 1.0542654991149902\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.686569690704346 | KNN Loss: 5.639002323150635 | BCE Loss: 1.0475674867630005\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.651156902313232 | KNN Loss: 5.621383190155029 | BCE Loss: 1.0297737121582031\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.627336502075195 | KNN Loss: 5.61110258102417 | BCE Loss: 1.0162336826324463\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.711609363555908 | KNN Loss: 5.668673515319824 | BCE Loss: 1.042935848236084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.639945983886719 | KNN Loss: 5.6075263023376465 | BCE Loss: 1.0324194431304932\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.705848693847656 | KNN Loss: 5.629512786865234 | BCE Loss: 1.0763359069824219\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 6.694762229919434 | KNN Loss: 5.648298740386963 | BCE Loss: 1.0464632511138916\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 6.657317161560059 | KNN Loss: 5.600157737731934 | BCE Loss: 1.057159423828125\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 6.647641181945801 | KNN Loss: 5.602902412414551 | BCE Loss: 1.044739007949829\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.652824878692627 | KNN Loss: 5.614565849304199 | BCE Loss: 1.0382590293884277\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.627872943878174 | KNN Loss: 5.608248710632324 | BCE Loss: 1.0196242332458496\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.683035373687744 | KNN Loss: 5.635503768920898 | BCE Loss: 1.0475317239761353\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.788047790527344 | KNN Loss: 5.73062801361084 | BCE Loss: 1.0574195384979248\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 6.662835121154785 | KNN Loss: 5.603662014007568 | BCE Loss: 1.0591729879379272\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.75299072265625 | KNN Loss: 5.725677490234375 | BCE Loss: 1.027312994003296\n",
      "Epoch   451: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.769063949584961 | KNN Loss: 5.741847038269043 | BCE Loss: 1.0272166728973389\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.657426834106445 | KNN Loss: 5.60322380065918 | BCE Loss: 1.0542032718658447\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 6.6800642013549805 | KNN Loss: 5.644900798797607 | BCE Loss: 1.0351632833480835\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 6.631292343139648 | KNN Loss: 5.59066915512085 | BCE Loss: 1.040623426437378\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 6.675192832946777 | KNN Loss: 5.617323875427246 | BCE Loss: 1.0578687191009521\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.676280498504639 | KNN Loss: 5.643392562866211 | BCE Loss: 1.0328879356384277\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.690804481506348 | KNN Loss: 5.616415023803711 | BCE Loss: 1.0743892192840576\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.67154598236084 | KNN Loss: 5.618634223937988 | BCE Loss: 1.0529115200042725\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.64902400970459 | KNN Loss: 5.597203254699707 | BCE Loss: 1.0518207550048828\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.662198066711426 | KNN Loss: 5.633243083953857 | BCE Loss: 1.028955101966858\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 6.655152320861816 | KNN Loss: 5.59453010559082 | BCE Loss: 1.060621976852417\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.648470401763916 | KNN Loss: 5.597748279571533 | BCE Loss: 1.0507220029830933\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.648154258728027 | KNN Loss: 5.594420433044434 | BCE Loss: 1.0537340641021729\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.716545581817627 | KNN Loss: 5.646917819976807 | BCE Loss: 1.0696276426315308\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.634757995605469 | KNN Loss: 5.606082439422607 | BCE Loss: 1.0286755561828613\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.645341396331787 | KNN Loss: 5.607786655426025 | BCE Loss: 1.0375548601150513\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.664029121398926 | KNN Loss: 5.615422248840332 | BCE Loss: 1.0486069917678833\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.645506381988525 | KNN Loss: 5.613258361816406 | BCE Loss: 1.0322481393814087\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 6.661252021789551 | KNN Loss: 5.611400604248047 | BCE Loss: 1.049851655960083\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.657382011413574 | KNN Loss: 5.61096715927124 | BCE Loss: 1.046414852142334\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 6.64388370513916 | KNN Loss: 5.595994472503662 | BCE Loss: 1.0478894710540771\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 6.6731133460998535 | KNN Loss: 5.633159160614014 | BCE Loss: 1.0399540662765503\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.696829795837402 | KNN Loss: 5.647876262664795 | BCE Loss: 1.0489534139633179\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.696782112121582 | KNN Loss: 5.632129669189453 | BCE Loss: 1.0646525621414185\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.646450996398926 | KNN Loss: 5.598330974578857 | BCE Loss: 1.0481200218200684\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.634302616119385 | KNN Loss: 5.602561950683594 | BCE Loss: 1.031740665435791\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.709188938140869 | KNN Loss: 5.64802885055542 | BCE Loss: 1.0611599683761597\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.67787504196167 | KNN Loss: 5.612554550170898 | BCE Loss: 1.065320611000061\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.65366268157959 | KNN Loss: 5.627352237701416 | BCE Loss: 1.026310682296753\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.696590423583984 | KNN Loss: 5.664789199829102 | BCE Loss: 1.0318012237548828\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.652105808258057 | KNN Loss: 5.619521617889404 | BCE Loss: 1.0325840711593628\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.653223991394043 | KNN Loss: 5.599488258361816 | BCE Loss: 1.0537359714508057\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.684587478637695 | KNN Loss: 5.612783432006836 | BCE Loss: 1.0718038082122803\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.627211570739746 | KNN Loss: 5.605686187744141 | BCE Loss: 1.0215256214141846\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 6.665805816650391 | KNN Loss: 5.637554168701172 | BCE Loss: 1.0282516479492188\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 6.714382171630859 | KNN Loss: 5.645958423614502 | BCE Loss: 1.0684239864349365\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.64521598815918 | KNN Loss: 5.6043500900268555 | BCE Loss: 1.0408660173416138\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.708934783935547 | KNN Loss: 5.671789169311523 | BCE Loss: 1.0371453762054443\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.615847587585449 | KNN Loss: 5.596286296844482 | BCE Loss: 1.019561529159546\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.6393914222717285 | KNN Loss: 5.6006083488464355 | BCE Loss: 1.038783073425293\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.6724629402160645 | KNN Loss: 5.605483531951904 | BCE Loss: 1.0669794082641602\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.667880058288574 | KNN Loss: 5.640728950500488 | BCE Loss: 1.0271508693695068\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.643388748168945 | KNN Loss: 5.596156597137451 | BCE Loss: 1.0472322702407837\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.7331109046936035 | KNN Loss: 5.658718585968018 | BCE Loss: 1.0743924379348755\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.69480037689209 | KNN Loss: 5.640249252319336 | BCE Loss: 1.0545512437820435\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.758936882019043 | KNN Loss: 5.701911926269531 | BCE Loss: 1.0570247173309326\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.654440879821777 | KNN Loss: 5.596958637237549 | BCE Loss: 1.0574820041656494\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.673550605773926 | KNN Loss: 5.634615421295166 | BCE Loss: 1.0389354228973389\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.679267883300781 | KNN Loss: 5.607470512390137 | BCE Loss: 1.0717976093292236\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.703752517700195 | KNN Loss: 5.635403156280518 | BCE Loss: 1.0683491230010986\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.724100112915039 | KNN Loss: 5.6575775146484375 | BCE Loss: 1.0665227174758911\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.675927639007568 | KNN Loss: 5.624722957611084 | BCE Loss: 1.0512046813964844\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.660289764404297 | KNN Loss: 5.6079254150390625 | BCE Loss: 1.0523642301559448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.704185962677002 | KNN Loss: 5.65622091293335 | BCE Loss: 1.0479649305343628\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.7202558517456055 | KNN Loss: 5.668644905090332 | BCE Loss: 1.051611065864563\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.665330410003662 | KNN Loss: 5.607845306396484 | BCE Loss: 1.0574849843978882\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 6.696154594421387 | KNN Loss: 5.6513214111328125 | BCE Loss: 1.0448329448699951\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.623646259307861 | KNN Loss: 5.6089348793029785 | BCE Loss: 1.0147113800048828\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.674218654632568 | KNN Loss: 5.618333339691162 | BCE Loss: 1.0558851957321167\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.702733039855957 | KNN Loss: 5.653134346008301 | BCE Loss: 1.0495986938476562\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.70739221572876 | KNN Loss: 5.6386895179748535 | BCE Loss: 1.0687026977539062\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.645035743713379 | KNN Loss: 5.5923357009887695 | BCE Loss: 1.0527002811431885\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.71049690246582 | KNN Loss: 5.67535924911499 | BCE Loss: 1.0351375341415405\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.66840934753418 | KNN Loss: 5.627194404602051 | BCE Loss: 1.041214942932129\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.613213539123535 | KNN Loss: 5.597607612609863 | BCE Loss: 1.0156056880950928\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.6470255851745605 | KNN Loss: 5.596802234649658 | BCE Loss: 1.0502233505249023\n",
      "Epoch   462: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.731962203979492 | KNN Loss: 5.641958236694336 | BCE Loss: 1.0900037288665771\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.630268096923828 | KNN Loss: 5.593049049377441 | BCE Loss: 1.0372191667556763\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.676667213439941 | KNN Loss: 5.625077247619629 | BCE Loss: 1.0515897274017334\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.656723976135254 | KNN Loss: 5.593984603881836 | BCE Loss: 1.062739610671997\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.673262596130371 | KNN Loss: 5.639628887176514 | BCE Loss: 1.0336337089538574\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.717203140258789 | KNN Loss: 5.660553455352783 | BCE Loss: 1.0566494464874268\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 6.645382404327393 | KNN Loss: 5.625518798828125 | BCE Loss: 1.0198637247085571\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.637925148010254 | KNN Loss: 5.611056804656982 | BCE Loss: 1.0268681049346924\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.724824905395508 | KNN Loss: 5.664292812347412 | BCE Loss: 1.0605318546295166\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.6370320320129395 | KNN Loss: 5.596564292907715 | BCE Loss: 1.0404677391052246\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.648314476013184 | KNN Loss: 5.600729465484619 | BCE Loss: 1.0475847721099854\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.702305793762207 | KNN Loss: 5.611572265625 | BCE Loss: 1.090733289718628\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.635890007019043 | KNN Loss: 5.604544639587402 | BCE Loss: 1.0313453674316406\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 6.629644870758057 | KNN Loss: 5.594165325164795 | BCE Loss: 1.0354796648025513\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.665513038635254 | KNN Loss: 5.611761093139648 | BCE Loss: 1.0537517070770264\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.635334014892578 | KNN Loss: 5.60453987121582 | BCE Loss: 1.0307941436767578\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.660617828369141 | KNN Loss: 5.6008100509643555 | BCE Loss: 1.0598080158233643\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.743395805358887 | KNN Loss: 5.668529987335205 | BCE Loss: 1.0748655796051025\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.645789623260498 | KNN Loss: 5.610630035400391 | BCE Loss: 1.0351595878601074\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.634253978729248 | KNN Loss: 5.592333793640137 | BCE Loss: 1.0419201850891113\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.652097225189209 | KNN Loss: 5.631385803222656 | BCE Loss: 1.0207114219665527\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.684911727905273 | KNN Loss: 5.606720447540283 | BCE Loss: 1.0781911611557007\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.628653526306152 | KNN Loss: 5.622672080993652 | BCE Loss: 1.005981206893921\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.6574387550354 | KNN Loss: 5.602811336517334 | BCE Loss: 1.0546272993087769\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.648795127868652 | KNN Loss: 5.5983052253723145 | BCE Loss: 1.0504896640777588\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 6.629340648651123 | KNN Loss: 5.605233669281006 | BCE Loss: 1.0241069793701172\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.693694114685059 | KNN Loss: 5.61360502243042 | BCE Loss: 1.0800893306732178\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.681256294250488 | KNN Loss: 5.612192630767822 | BCE Loss: 1.069063663482666\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.728766441345215 | KNN Loss: 5.659204006195068 | BCE Loss: 1.0695626735687256\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.62862491607666 | KNN Loss: 5.608303070068359 | BCE Loss: 1.0203218460083008\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.646533966064453 | KNN Loss: 5.632774829864502 | BCE Loss: 1.0137593746185303\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.666774749755859 | KNN Loss: 5.625214099884033 | BCE Loss: 1.041560411453247\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 6.667715072631836 | KNN Loss: 5.619940757751465 | BCE Loss: 1.047774076461792\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.664017677307129 | KNN Loss: 5.609346389770508 | BCE Loss: 1.0546711683273315\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.620578765869141 | KNN Loss: 5.606278419494629 | BCE Loss: 1.0143003463745117\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.734767913818359 | KNN Loss: 5.6807169914245605 | BCE Loss: 1.054051160812378\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.6388115882873535 | KNN Loss: 5.600210666656494 | BCE Loss: 1.0386009216308594\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.702723026275635 | KNN Loss: 5.671778202056885 | BCE Loss: 1.03094482421875\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.7242350578308105 | KNN Loss: 5.6819539070129395 | BCE Loss: 1.042281150817871\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.731177806854248 | KNN Loss: 5.686952114105225 | BCE Loss: 1.044225811958313\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.6891021728515625 | KNN Loss: 5.630568027496338 | BCE Loss: 1.0585342645645142\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.639521598815918 | KNN Loss: 5.613406658172607 | BCE Loss: 1.0261149406433105\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.695895671844482 | KNN Loss: 5.630383014678955 | BCE Loss: 1.0655126571655273\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.684307098388672 | KNN Loss: 5.653925895690918 | BCE Loss: 1.0303809642791748\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.6658935546875 | KNN Loss: 5.6012678146362305 | BCE Loss: 1.06462562084198\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.682937145233154 | KNN Loss: 5.605403423309326 | BCE Loss: 1.0775337219238281\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.6950907707214355 | KNN Loss: 5.622705459594727 | BCE Loss: 1.0723851919174194\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.638856410980225 | KNN Loss: 5.59782600402832 | BCE Loss: 1.0410304069519043\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.6901702880859375 | KNN Loss: 5.620413780212402 | BCE Loss: 1.0697565078735352\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.657587051391602 | KNN Loss: 5.597729206085205 | BCE Loss: 1.0598576068878174\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.6383957862854 | KNN Loss: 5.612239360809326 | BCE Loss: 1.0261564254760742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.616541862487793 | KNN Loss: 5.59578800201416 | BCE Loss: 1.020754098892212\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.731933116912842 | KNN Loss: 5.661772727966309 | BCE Loss: 1.0701605081558228\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.6596879959106445 | KNN Loss: 5.622954845428467 | BCE Loss: 1.0367331504821777\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.687159538269043 | KNN Loss: 5.616240978240967 | BCE Loss: 1.070918321609497\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.671160697937012 | KNN Loss: 5.635196208953857 | BCE Loss: 1.0359644889831543\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.667853355407715 | KNN Loss: 5.627680778503418 | BCE Loss: 1.0401724576950073\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.660338401794434 | KNN Loss: 5.59870719909668 | BCE Loss: 1.061631441116333\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.759291648864746 | KNN Loss: 5.706948757171631 | BCE Loss: 1.0523431301116943\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.628658294677734 | KNN Loss: 5.620677947998047 | BCE Loss: 1.0079803466796875\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.713020324707031 | KNN Loss: 5.646904468536377 | BCE Loss: 1.0661157369613647\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.681766510009766 | KNN Loss: 5.666164875030518 | BCE Loss: 1.0156018733978271\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.6280412673950195 | KNN Loss: 5.60023832321167 | BCE Loss: 1.0278030633926392\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.667298316955566 | KNN Loss: 5.634698867797852 | BCE Loss: 1.0325992107391357\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.68025541305542 | KNN Loss: 5.620422840118408 | BCE Loss: 1.0598325729370117\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.709651947021484 | KNN Loss: 5.63092565536499 | BCE Loss: 1.0787262916564941\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.641994953155518 | KNN Loss: 5.605679512023926 | BCE Loss: 1.0363154411315918\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.726999282836914 | KNN Loss: 5.692253589630127 | BCE Loss: 1.034745693206787\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.688414096832275 | KNN Loss: 5.626873970031738 | BCE Loss: 1.0615402460098267\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.664845943450928 | KNN Loss: 5.628726959228516 | BCE Loss: 1.0361191034317017\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.646562576293945 | KNN Loss: 5.607901573181152 | BCE Loss: 1.0386608839035034\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.664041042327881 | KNN Loss: 5.609493255615234 | BCE Loss: 1.054547667503357\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.641863822937012 | KNN Loss: 5.596563816070557 | BCE Loss: 1.045300006866455\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.719446182250977 | KNN Loss: 5.689894676208496 | BCE Loss: 1.0295512676239014\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.7202582359313965 | KNN Loss: 5.658594131469727 | BCE Loss: 1.06166410446167\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.654951572418213 | KNN Loss: 5.63316535949707 | BCE Loss: 1.021786093711853\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.630170822143555 | KNN Loss: 5.608942985534668 | BCE Loss: 1.0212279558181763\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.695725917816162 | KNN Loss: 5.642162322998047 | BCE Loss: 1.0535635948181152\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.674005508422852 | KNN Loss: 5.6255645751953125 | BCE Loss: 1.04844069480896\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.671615123748779 | KNN Loss: 5.612894058227539 | BCE Loss: 1.0587211847305298\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.725068092346191 | KNN Loss: 5.691997051239014 | BCE Loss: 1.0330712795257568\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.693637847900391 | KNN Loss: 5.656948089599609 | BCE Loss: 1.0366899967193604\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.717552185058594 | KNN Loss: 5.6690874099731445 | BCE Loss: 1.0484645366668701\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.654440879821777 | KNN Loss: 5.625701904296875 | BCE Loss: 1.0287392139434814\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.703378200531006 | KNN Loss: 5.689988136291504 | BCE Loss: 1.013390064239502\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.637357711791992 | KNN Loss: 5.596138954162598 | BCE Loss: 1.041218638420105\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.6767425537109375 | KNN Loss: 5.615225315093994 | BCE Loss: 1.0615172386169434\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.646153450012207 | KNN Loss: 5.608349323272705 | BCE Loss: 1.037804365158081\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.733135223388672 | KNN Loss: 5.6752705574035645 | BCE Loss: 1.0578649044036865\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.712127685546875 | KNN Loss: 5.636484146118164 | BCE Loss: 1.0756433010101318\n",
      "Epoch   477: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.639118194580078 | KNN Loss: 5.598607540130615 | BCE Loss: 1.0405105352401733\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.695737361907959 | KNN Loss: 5.631054878234863 | BCE Loss: 1.0646824836730957\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.695222854614258 | KNN Loss: 5.5985517501831055 | BCE Loss: 1.0966711044311523\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.655862808227539 | KNN Loss: 5.615239143371582 | BCE Loss: 1.040623426437378\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.666311740875244 | KNN Loss: 5.599909782409668 | BCE Loss: 1.0664019584655762\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.737475395202637 | KNN Loss: 5.675936222076416 | BCE Loss: 1.0615389347076416\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.67698860168457 | KNN Loss: 5.635257244110107 | BCE Loss: 1.0417311191558838\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.663728713989258 | KNN Loss: 5.628964900970459 | BCE Loss: 1.0347638130187988\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.726555347442627 | KNN Loss: 5.681949138641357 | BCE Loss: 1.04460608959198\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.678309440612793 | KNN Loss: 5.595823287963867 | BCE Loss: 1.0824863910675049\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.615046501159668 | KNN Loss: 5.591193675994873 | BCE Loss: 1.0238525867462158\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.708601951599121 | KNN Loss: 5.636535167694092 | BCE Loss: 1.0720665454864502\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.650202751159668 | KNN Loss: 5.616420269012451 | BCE Loss: 1.0337826013565063\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 6.645805358886719 | KNN Loss: 5.590303421020508 | BCE Loss: 1.055501937866211\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.700901031494141 | KNN Loss: 5.647921562194824 | BCE Loss: 1.052979588508606\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.6321635246276855 | KNN Loss: 5.615554332733154 | BCE Loss: 1.0166091918945312\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.744846343994141 | KNN Loss: 5.7046918869018555 | BCE Loss: 1.0401546955108643\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.6493306159973145 | KNN Loss: 5.595549583435059 | BCE Loss: 1.0537811517715454\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.673300743103027 | KNN Loss: 5.616800308227539 | BCE Loss: 1.0565003156661987\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.642397880554199 | KNN Loss: 5.606734752655029 | BCE Loss: 1.035663366317749\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.647327899932861 | KNN Loss: 5.606942653656006 | BCE Loss: 1.0403852462768555\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.727148056030273 | KNN Loss: 5.682407379150391 | BCE Loss: 1.0447404384613037\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 6.628759384155273 | KNN Loss: 5.596004486083984 | BCE Loss: 1.032754898071289\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.652344226837158 | KNN Loss: 5.59592866897583 | BCE Loss: 1.0564154386520386\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.6200690269470215 | KNN Loss: 5.610501289367676 | BCE Loss: 1.0095677375793457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.665174961090088 | KNN Loss: 5.610004901885986 | BCE Loss: 1.0551700592041016\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.739755153656006 | KNN Loss: 5.687113285064697 | BCE Loss: 1.0526418685913086\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.690363883972168 | KNN Loss: 5.615386486053467 | BCE Loss: 1.0749772787094116\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.667476177215576 | KNN Loss: 5.633862495422363 | BCE Loss: 1.033613681793213\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.6948466300964355 | KNN Loss: 5.644711971282959 | BCE Loss: 1.0501346588134766\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.717164039611816 | KNN Loss: 5.660238265991211 | BCE Loss: 1.0569257736206055\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.6404266357421875 | KNN Loss: 5.60251522064209 | BCE Loss: 1.0379115343093872\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.659369468688965 | KNN Loss: 5.6508355140686035 | BCE Loss: 1.0085338354110718\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.665613651275635 | KNN Loss: 5.609306812286377 | BCE Loss: 1.0563068389892578\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.642786026000977 | KNN Loss: 5.624776363372803 | BCE Loss: 1.0180096626281738\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.649195194244385 | KNN Loss: 5.596969127655029 | BCE Loss: 1.0522260665893555\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.695345878601074 | KNN Loss: 5.628538131713867 | BCE Loss: 1.066807508468628\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.732105255126953 | KNN Loss: 5.685477256774902 | BCE Loss: 1.0466282367706299\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.655332565307617 | KNN Loss: 5.611698627471924 | BCE Loss: 1.0436339378356934\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.690832138061523 | KNN Loss: 5.660375595092773 | BCE Loss: 1.030456304550171\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.648609161376953 | KNN Loss: 5.600407600402832 | BCE Loss: 1.0482017993927002\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.666158199310303 | KNN Loss: 5.5953874588012695 | BCE Loss: 1.0707708597183228\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.691410064697266 | KNN Loss: 5.639925003051758 | BCE Loss: 1.0514849424362183\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.643885135650635 | KNN Loss: 5.607555866241455 | BCE Loss: 1.0363291501998901\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.686965465545654 | KNN Loss: 5.624527454376221 | BCE Loss: 1.0624380111694336\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.7216691970825195 | KNN Loss: 5.659010410308838 | BCE Loss: 1.0626587867736816\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.709682464599609 | KNN Loss: 5.649103164672852 | BCE Loss: 1.060579538345337\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.641454696655273 | KNN Loss: 5.594545364379883 | BCE Loss: 1.0469095706939697\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 6.664510250091553 | KNN Loss: 5.631563663482666 | BCE Loss: 1.0329464673995972\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.640940189361572 | KNN Loss: 5.606040954589844 | BCE Loss: 1.0348992347717285\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.648543357849121 | KNN Loss: 5.620522975921631 | BCE Loss: 1.0280201435089111\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.668552398681641 | KNN Loss: 5.637434005737305 | BCE Loss: 1.0311182737350464\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 6.678840637207031 | KNN Loss: 5.650052547454834 | BCE Loss: 1.0287883281707764\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.617569923400879 | KNN Loss: 5.601712703704834 | BCE Loss: 1.015857219696045\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.661659240722656 | KNN Loss: 5.627297878265381 | BCE Loss: 1.0343611240386963\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.6971940994262695 | KNN Loss: 5.667690753936768 | BCE Loss: 1.0295031070709229\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.666525840759277 | KNN Loss: 5.611185550689697 | BCE Loss: 1.05534029006958\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.665987491607666 | KNN Loss: 5.598301887512207 | BCE Loss: 1.0676854848861694\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.666909694671631 | KNN Loss: 5.623424530029297 | BCE Loss: 1.0434852838516235\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.639322757720947 | KNN Loss: 5.605992317199707 | BCE Loss: 1.0333304405212402\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.653347492218018 | KNN Loss: 5.620257377624512 | BCE Loss: 1.0330902338027954\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 6.701836585998535 | KNN Loss: 5.658132076263428 | BCE Loss: 1.0437042713165283\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.6500701904296875 | KNN Loss: 5.591256141662598 | BCE Loss: 1.0588140487670898\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.676478862762451 | KNN Loss: 5.633981704711914 | BCE Loss: 1.0424970388412476\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.654126167297363 | KNN Loss: 5.599018573760986 | BCE Loss: 1.055107593536377\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.68633508682251 | KNN Loss: 5.673994064331055 | BCE Loss: 1.0123411417007446\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 6.676040172576904 | KNN Loss: 5.637946605682373 | BCE Loss: 1.0380935668945312\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.673309326171875 | KNN Loss: 5.648521900177002 | BCE Loss: 1.0247873067855835\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.6984477043151855 | KNN Loss: 5.635727882385254 | BCE Loss: 1.062719702720642\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.641726016998291 | KNN Loss: 5.613916873931885 | BCE Loss: 1.0278091430664062\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.644923210144043 | KNN Loss: 5.600481033325195 | BCE Loss: 1.0444424152374268\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.661595821380615 | KNN Loss: 5.653810024261475 | BCE Loss: 1.0077857971191406\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.73007345199585 | KNN Loss: 5.661148548126221 | BCE Loss: 1.0689250230789185\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.734699726104736 | KNN Loss: 5.696049690246582 | BCE Loss: 1.0386501550674438\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.7866950035095215 | KNN Loss: 5.715385913848877 | BCE Loss: 1.071309208869934\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 6.649134159088135 | KNN Loss: 5.598100662231445 | BCE Loss: 1.0510333776474\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.73476505279541 | KNN Loss: 5.676924705505371 | BCE Loss: 1.0578405857086182\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.642738342285156 | KNN Loss: 5.605849742889404 | BCE Loss: 1.0368884801864624\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.7312188148498535 | KNN Loss: 5.6970601081848145 | BCE Loss: 1.0341588258743286\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.72121524810791 | KNN Loss: 5.689107418060303 | BCE Loss: 1.0321078300476074\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.658707141876221 | KNN Loss: 5.59739351272583 | BCE Loss: 1.0613136291503906\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.715610980987549 | KNN Loss: 5.664117813110352 | BCE Loss: 1.0514931678771973\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.716434478759766 | KNN Loss: 5.647396087646484 | BCE Loss: 1.0690382719039917\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.6882524490356445 | KNN Loss: 5.606444835662842 | BCE Loss: 1.0818076133728027\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.6344099044799805 | KNN Loss: 5.598155498504639 | BCE Loss: 1.0362541675567627\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.675649166107178 | KNN Loss: 5.625240325927734 | BCE Loss: 1.050408959388733\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.66139030456543 | KNN Loss: 5.60328483581543 | BCE Loss: 1.058105707168579\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.6459503173828125 | KNN Loss: 5.592368125915527 | BCE Loss: 1.053581953048706\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 6.636205673217773 | KNN Loss: 5.601589202880859 | BCE Loss: 1.0346163511276245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.654203414916992 | KNN Loss: 5.602957248687744 | BCE Loss: 1.0512464046478271\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.699536323547363 | KNN Loss: 5.646448135375977 | BCE Loss: 1.0530880689620972\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.636556625366211 | KNN Loss: 5.590782165527344 | BCE Loss: 1.0457744598388672\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.629528522491455 | KNN Loss: 5.594355583190918 | BCE Loss: 1.035172939300537\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.672906875610352 | KNN Loss: 5.615921974182129 | BCE Loss: 1.0569850206375122\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.664638042449951 | KNN Loss: 5.611959457397461 | BCE Loss: 1.0526784658432007\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 6.716658115386963 | KNN Loss: 5.670037269592285 | BCE Loss: 1.0466207265853882\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.660101413726807 | KNN Loss: 5.63767147064209 | BCE Loss: 1.0224299430847168\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.679193496704102 | KNN Loss: 5.630370140075684 | BCE Loss: 1.048823356628418\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.65786075592041 | KNN Loss: 5.599152088165283 | BCE Loss: 1.058708667755127\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.661409854888916 | KNN Loss: 5.592198848724365 | BCE Loss: 1.0692110061645508\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.674005508422852 | KNN Loss: 5.6188578605651855 | BCE Loss: 1.0551478862762451\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.6261396408081055 | KNN Loss: 5.597665786743164 | BCE Loss: 1.0284736156463623\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.733628749847412 | KNN Loss: 5.6792402267456055 | BCE Loss: 1.0543885231018066\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.650381088256836 | KNN Loss: 5.605104446411133 | BCE Loss: 1.0452765226364136\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.6269025802612305 | KNN Loss: 5.606058597564697 | BCE Loss: 1.020843744277954\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.7760820388793945 | KNN Loss: 5.700721263885498 | BCE Loss: 1.075360894203186\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 6.679483413696289 | KNN Loss: 5.619885444641113 | BCE Loss: 1.0595977306365967\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.70561408996582 | KNN Loss: 5.64573860168457 | BCE Loss: 1.059875726699829\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 6.704833030700684 | KNN Loss: 5.639303684234619 | BCE Loss: 1.0655295848846436\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.657810211181641 | KNN Loss: 5.615487098693848 | BCE Loss: 1.0423228740692139\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.648497581481934 | KNN Loss: 5.608565330505371 | BCE Loss: 1.039932370185852\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.638982772827148 | KNN Loss: 5.596584320068359 | BCE Loss: 1.04239821434021\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.664642333984375 | KNN Loss: 5.625208854675293 | BCE Loss: 1.0394337177276611\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.672381401062012 | KNN Loss: 5.616134166717529 | BCE Loss: 1.0562472343444824\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.701605796813965 | KNN Loss: 5.6422810554504395 | BCE Loss: 1.0593247413635254\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.730227470397949 | KNN Loss: 5.663248062133789 | BCE Loss: 1.0669795274734497\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.657154083251953 | KNN Loss: 5.634792804718018 | BCE Loss: 1.0223615169525146\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.628562927246094 | KNN Loss: 5.593294620513916 | BCE Loss: 1.0352685451507568\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.685903549194336 | KNN Loss: 5.6369757652282715 | BCE Loss: 1.0489277839660645\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.661885738372803 | KNN Loss: 5.633995056152344 | BCE Loss: 1.0278905630111694\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 6.691033840179443 | KNN Loss: 5.685262680053711 | BCE Loss: 1.0057711601257324\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.657576560974121 | KNN Loss: 5.610198974609375 | BCE Loss: 1.0473778247833252\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.813229084014893 | KNN Loss: 5.750095367431641 | BCE Loss: 1.0631335973739624\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.849237442016602 | KNN Loss: 5.752636909484863 | BCE Loss: 1.0966005325317383\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.726815223693848 | KNN Loss: 5.6466569900512695 | BCE Loss: 1.0801582336425781\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.619514465332031 | KNN Loss: 5.593873977661133 | BCE Loss: 1.0256404876708984\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.653232097625732 | KNN Loss: 5.60375452041626 | BCE Loss: 1.0494775772094727\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.66030216217041 | KNN Loss: 5.610569477081299 | BCE Loss: 1.0497326850891113\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.675844192504883 | KNN Loss: 5.6205644607543945 | BCE Loss: 1.0552796125411987\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.71909236907959 | KNN Loss: 5.671144485473633 | BCE Loss: 1.047947645187378\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.67110538482666 | KNN Loss: 5.637570381164551 | BCE Loss: 1.0335350036621094\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.666521072387695 | KNN Loss: 5.6305108070373535 | BCE Loss: 1.0360102653503418\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.665294647216797 | KNN Loss: 5.622879981994629 | BCE Loss: 1.0424147844314575\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.630121231079102 | KNN Loss: 5.602307319641113 | BCE Loss: 1.0278139114379883\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.7160844802856445 | KNN Loss: 5.666047096252441 | BCE Loss: 1.0500373840332031\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.628370761871338 | KNN Loss: 5.616860866546631 | BCE Loss: 1.0115097761154175\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.724198341369629 | KNN Loss: 5.692790985107422 | BCE Loss: 1.031407117843628\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.632684707641602 | KNN Loss: 5.605705738067627 | BCE Loss: 1.0269787311553955\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5823,  3.8489,  2.5081,  3.2581,  3.1805,  0.6070,  2.5824,  2.1524,\n",
      "          2.2921,  1.8750,  1.9751,  2.1245,  0.7700,  1.7622,  1.2701,  1.5015,\n",
      "          2.4763,  2.8642,  2.7307,  2.2322,  1.7439,  2.9127,  2.2424,  2.2796,\n",
      "          2.2368,  1.7092,  2.0704,  1.2598,  1.4428,  0.3424, -0.2011,  0.9850,\n",
      "          0.2514,  1.0013,  1.5070,  1.4405,  0.9859,  3.0369,  0.7450,  1.2302,\n",
      "          0.9363, -0.6310, -0.2804,  2.2822,  2.0094,  0.7214, -0.2034,  0.1042,\n",
      "          1.4171,  2.4876,  1.8115,  0.1094,  1.3559,  0.4724, -0.6058,  1.1043,\n",
      "          1.4397,  1.3207,  1.1768,  1.8007,  0.5496,  0.8158,  0.1961,  1.6543,\n",
      "          1.2789,  1.6702, -1.8123,  0.2765,  1.9163,  2.1375,  2.2309,  0.4083,\n",
      "          1.2641,  2.4029,  1.9281,  1.2849,  0.2437,  0.7234,  0.2041,  1.5392,\n",
      "          0.0822,  0.4188,  1.5643, -0.4425,  0.2419, -1.0505, -2.2799, -0.3460,\n",
      "          0.5775, -1.7547,  0.4210, -0.1554, -0.4991, -0.8217,  0.5404,  1.1915,\n",
      "         -0.6738, -0.5723,  0.4249,  1.1687,  0.6719, -1.1345,  0.8610,  1.0324,\n",
      "         -1.2207, -1.0710, -0.1228,  0.0509, -0.9901, -1.6383, -0.4317, -2.5751,\n",
      "         -0.3681,  1.5057,  1.5787, -0.2417, -0.5705,  0.0570,  1.5006, -2.4271,\n",
      "          0.1840, -0.1598,  0.4334, -0.5994,  0.0773, -0.8049, -0.9208,  0.9396,\n",
      "          0.2360, -0.5819,  0.3209, -0.6398, -1.2595, -0.3324, -0.4066,  0.7943,\n",
      "         -0.3883,  0.1509, -1.9392, -0.9079, -1.3056,  0.5922, -1.7334, -0.9013,\n",
      "         -0.9024, -0.5763, -1.4673, -1.1016, -2.2404, -0.8798, -1.2356, -0.3596,\n",
      "         -1.6843,  0.4473, -1.4033, -0.5249, -3.0515,  0.1343, -0.1368, -0.7399,\n",
      "         -2.0922, -1.5981, -1.2321, -1.2731, -2.4023, -2.2800, -3.0675]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.0675, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.8489, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b26e3e484a24e1d8dbe5d4851d3ca6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:01<00:00, 12.72it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb44c62e45aa4be0a34f886e85a88ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23385e70c29450687a5832c74fc396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a595adbcd7574dc1b18cb6432e4b66fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "Epoch: 00 | Batch: 000 / 030 | Total loss: 9.652 | Reg loss: 0.009 | Tree loss: 9.652 | Accuracy: 0.000000 | 1.009 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 030 | Total loss: 9.636 | Reg loss: 0.009 | Tree loss: 9.636 | Accuracy: 0.000000 | 0.728 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 030 | Total loss: 9.621 | Reg loss: 0.008 | Tree loss: 9.621 | Accuracy: 0.000000 | 0.6 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 030 | Total loss: 9.605 | Reg loss: 0.008 | Tree loss: 9.605 | Accuracy: 0.000000 | 0.542 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 030 | Total loss: 9.591 | Reg loss: 0.008 | Tree loss: 9.591 | Accuracy: 0.000000 | 0.509 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 030 | Total loss: 9.575 | Reg loss: 0.008 | Tree loss: 9.575 | Accuracy: 0.000000 | 0.486 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 030 | Total loss: 9.561 | Reg loss: 0.008 | Tree loss: 9.561 | Accuracy: 0.000000 | 0.469 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 030 | Total loss: 9.543 | Reg loss: 0.008 | Tree loss: 9.543 | Accuracy: 0.001953 | 0.456 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 030 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.000000 | 0.447 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 030 | Total loss: 9.516 | Reg loss: 0.008 | Tree loss: 9.516 | Accuracy: 0.003906 | 0.438 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 030 | Total loss: 9.499 | Reg loss: 0.009 | Tree loss: 9.499 | Accuracy: 0.011719 | 0.433 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 030 | Total loss: 9.482 | Reg loss: 0.009 | Tree loss: 9.482 | Accuracy: 0.011719 | 0.427 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 030 | Total loss: 9.469 | Reg loss: 0.009 | Tree loss: 9.469 | Accuracy: 0.015625 | 0.422 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 030 | Total loss: 9.455 | Reg loss: 0.009 | Tree loss: 9.455 | Accuracy: 0.050781 | 0.418 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 030 | Total loss: 9.436 | Reg loss: 0.010 | Tree loss: 9.436 | Accuracy: 0.085938 | 0.415 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 030 | Total loss: 9.428 | Reg loss: 0.010 | Tree loss: 9.428 | Accuracy: 0.085938 | 0.412 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 030 | Total loss: 9.413 | Reg loss: 0.010 | Tree loss: 9.413 | Accuracy: 0.144531 | 0.409 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 030 | Total loss: 9.398 | Reg loss: 0.011 | Tree loss: 9.398 | Accuracy: 0.195312 | 0.405 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 030 | Total loss: 9.385 | Reg loss: 0.011 | Tree loss: 9.385 | Accuracy: 0.314453 | 0.401 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 030 | Total loss: 9.368 | Reg loss: 0.012 | Tree loss: 9.368 | Accuracy: 0.466797 | 0.395 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 030 | Total loss: 9.358 | Reg loss: 0.012 | Tree loss: 9.358 | Accuracy: 0.476562 | 0.39 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 030 | Total loss: 9.335 | Reg loss: 0.012 | Tree loss: 9.335 | Accuracy: 0.550781 | 0.385 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 030 | Total loss: 9.326 | Reg loss: 0.013 | Tree loss: 9.326 | Accuracy: 0.537109 | 0.384 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 030 | Total loss: 9.315 | Reg loss: 0.013 | Tree loss: 9.315 | Accuracy: 0.566406 | 0.382 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 030 | Total loss: 9.296 | Reg loss: 0.013 | Tree loss: 9.296 | Accuracy: 0.576172 | 0.379 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 030 | Total loss: 9.277 | Reg loss: 0.014 | Tree loss: 9.277 | Accuracy: 0.564453 | 0.377 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 030 | Total loss: 9.262 | Reg loss: 0.014 | Tree loss: 9.262 | Accuracy: 0.605469 | 0.374 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 030 | Total loss: 9.257 | Reg loss: 0.015 | Tree loss: 9.257 | Accuracy: 0.560547 | 0.374 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 030 | Total loss: 9.240 | Reg loss: 0.015 | Tree loss: 9.240 | Accuracy: 0.595703 | 0.376 sec/iter\n",
      "Epoch: 00 | Batch: 029 / 030 | Total loss: 9.234 | Reg loss: 0.015 | Tree loss: 9.234 | Accuracy: 0.527778 | 0.377 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 01 | Batch: 000 / 030 | Total loss: 9.445 | Reg loss: 0.005 | Tree loss: 9.445 | Accuracy: 0.013672 | 0.383 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 030 | Total loss: 9.429 | Reg loss: 0.005 | Tree loss: 9.429 | Accuracy: 0.062500 | 0.381 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 030 | Total loss: 9.417 | Reg loss: 0.005 | Tree loss: 9.417 | Accuracy: 0.115234 | 0.379 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 030 | Total loss: 9.406 | Reg loss: 0.005 | Tree loss: 9.406 | Accuracy: 0.183594 | 0.376 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 030 | Total loss: 9.390 | Reg loss: 0.006 | Tree loss: 9.390 | Accuracy: 0.318359 | 0.373 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 030 | Total loss: 9.374 | Reg loss: 0.006 | Tree loss: 9.374 | Accuracy: 0.455078 | 0.371 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 030 | Total loss: 9.360 | Reg loss: 0.006 | Tree loss: 9.360 | Accuracy: 0.539062 | 0.37 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 030 | Total loss: 9.341 | Reg loss: 0.007 | Tree loss: 9.341 | Accuracy: 0.597656 | 0.368 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 030 | Total loss: 9.329 | Reg loss: 0.007 | Tree loss: 9.329 | Accuracy: 0.556641 | 0.367 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 030 | Total loss: 9.316 | Reg loss: 0.008 | Tree loss: 9.316 | Accuracy: 0.537109 | 0.366 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 030 | Total loss: 9.299 | Reg loss: 0.008 | Tree loss: 9.299 | Accuracy: 0.583984 | 0.365 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 030 | Total loss: 9.288 | Reg loss: 0.008 | Tree loss: 9.288 | Accuracy: 0.580078 | 0.365 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 030 | Total loss: 9.266 | Reg loss: 0.009 | Tree loss: 9.266 | Accuracy: 0.570312 | 0.364 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 030 | Total loss: 9.258 | Reg loss: 0.009 | Tree loss: 9.258 | Accuracy: 0.556641 | 0.362 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 030 | Total loss: 9.240 | Reg loss: 0.010 | Tree loss: 9.240 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 030 | Total loss: 9.227 | Reg loss: 0.010 | Tree loss: 9.227 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 030 | Total loss: 9.207 | Reg loss: 0.011 | Tree loss: 9.207 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 030 | Total loss: 9.189 | Reg loss: 0.011 | Tree loss: 9.189 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 030 | Total loss: 9.180 | Reg loss: 0.012 | Tree loss: 9.180 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 030 | Total loss: 9.167 | Reg loss: 0.012 | Tree loss: 9.167 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 030 | Total loss: 9.146 | Reg loss: 0.013 | Tree loss: 9.146 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 030 | Total loss: 9.139 | Reg loss: 0.013 | Tree loss: 9.139 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 030 | Total loss: 9.119 | Reg loss: 0.014 | Tree loss: 9.119 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 030 | Total loss: 9.101 | Reg loss: 0.014 | Tree loss: 9.101 | Accuracy: 0.607422 | 0.358 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 030 | Total loss: 9.085 | Reg loss: 0.015 | Tree loss: 9.085 | Accuracy: 0.615234 | 0.358 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 030 | Total loss: 9.072 | Reg loss: 0.015 | Tree loss: 9.072 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 030 | Total loss: 9.060 | Reg loss: 0.015 | Tree loss: 9.060 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 030 | Total loss: 9.050 | Reg loss: 0.016 | Tree loss: 9.050 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 030 | Total loss: 9.027 | Reg loss: 0.016 | Tree loss: 9.027 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 01 | Batch: 029 / 030 | Total loss: 9.012 | Reg loss: 0.017 | Tree loss: 9.012 | Accuracy: 0.555556 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 02 | Batch: 000 / 030 | Total loss: 9.263 | Reg loss: 0.008 | Tree loss: 9.263 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 030 | Total loss: 9.249 | Reg loss: 0.008 | Tree loss: 9.249 | Accuracy: 0.574219 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 002 / 030 | Total loss: 9.236 | Reg loss: 0.008 | Tree loss: 9.236 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 030 | Total loss: 9.221 | Reg loss: 0.008 | Tree loss: 9.221 | Accuracy: 0.537109 | 0.358 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 030 | Total loss: 9.206 | Reg loss: 0.008 | Tree loss: 9.206 | Accuracy: 0.587891 | 0.357 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 030 | Total loss: 9.195 | Reg loss: 0.009 | Tree loss: 9.195 | Accuracy: 0.541016 | 0.356 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 030 | Total loss: 9.180 | Reg loss: 0.009 | Tree loss: 9.180 | Accuracy: 0.589844 | 0.356 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 030 | Total loss: 9.160 | Reg loss: 0.009 | Tree loss: 9.160 | Accuracy: 0.568359 | 0.355 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 030 | Total loss: 9.148 | Reg loss: 0.010 | Tree loss: 9.148 | Accuracy: 0.539062 | 0.356 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 030 | Total loss: 9.133 | Reg loss: 0.010 | Tree loss: 9.133 | Accuracy: 0.585938 | 0.357 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 030 | Total loss: 9.110 | Reg loss: 0.010 | Tree loss: 9.110 | Accuracy: 0.603516 | 0.357 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 030 | Total loss: 9.100 | Reg loss: 0.011 | Tree loss: 9.100 | Accuracy: 0.585938 | 0.357 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 030 | Total loss: 9.082 | Reg loss: 0.011 | Tree loss: 9.082 | Accuracy: 0.564453 | 0.357 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 030 | Total loss: 9.068 | Reg loss: 0.012 | Tree loss: 9.068 | Accuracy: 0.607422 | 0.357 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 030 | Total loss: 9.049 | Reg loss: 0.012 | Tree loss: 9.049 | Accuracy: 0.601562 | 0.356 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 030 | Total loss: 9.031 | Reg loss: 0.013 | Tree loss: 9.031 | Accuracy: 0.593750 | 0.355 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 030 | Total loss: 9.025 | Reg loss: 0.013 | Tree loss: 9.025 | Accuracy: 0.560547 | 0.354 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 030 | Total loss: 9.008 | Reg loss: 0.013 | Tree loss: 9.008 | Accuracy: 0.593750 | 0.353 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 030 | Total loss: 8.995 | Reg loss: 0.014 | Tree loss: 8.995 | Accuracy: 0.566406 | 0.353 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 030 | Total loss: 8.983 | Reg loss: 0.014 | Tree loss: 8.983 | Accuracy: 0.574219 | 0.352 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 030 | Total loss: 8.958 | Reg loss: 0.015 | Tree loss: 8.958 | Accuracy: 0.607422 | 0.352 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 030 | Total loss: 8.944 | Reg loss: 0.015 | Tree loss: 8.944 | Accuracy: 0.542969 | 0.352 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 030 | Total loss: 8.923 | Reg loss: 0.016 | Tree loss: 8.923 | Accuracy: 0.615234 | 0.351 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 030 | Total loss: 8.921 | Reg loss: 0.016 | Tree loss: 8.921 | Accuracy: 0.550781 | 0.351 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 030 | Total loss: 8.913 | Reg loss: 0.017 | Tree loss: 8.913 | Accuracy: 0.542969 | 0.35 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 030 | Total loss: 8.901 | Reg loss: 0.017 | Tree loss: 8.901 | Accuracy: 0.525391 | 0.349 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 030 | Total loss: 8.865 | Reg loss: 0.018 | Tree loss: 8.865 | Accuracy: 0.585938 | 0.349 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 030 | Total loss: 8.852 | Reg loss: 0.018 | Tree loss: 8.852 | Accuracy: 0.591797 | 0.348 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 030 | Total loss: 8.837 | Reg loss: 0.018 | Tree loss: 8.837 | Accuracy: 0.546875 | 0.348 sec/iter\n",
      "Epoch: 02 | Batch: 029 / 030 | Total loss: 8.807 | Reg loss: 0.019 | Tree loss: 8.807 | Accuracy: 0.620370 | 0.348 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 03 | Batch: 000 / 030 | Total loss: 9.085 | Reg loss: 0.010 | Tree loss: 9.085 | Accuracy: 0.587891 | 0.35 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 030 | Total loss: 9.075 | Reg loss: 0.010 | Tree loss: 9.075 | Accuracy: 0.570312 | 0.35 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 030 | Total loss: 9.059 | Reg loss: 0.010 | Tree loss: 9.059 | Accuracy: 0.539062 | 0.349 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 030 | Total loss: 9.045 | Reg loss: 0.011 | Tree loss: 9.045 | Accuracy: 0.607422 | 0.349 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 030 | Total loss: 9.029 | Reg loss: 0.011 | Tree loss: 9.029 | Accuracy: 0.523438 | 0.349 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 030 | Total loss: 9.016 | Reg loss: 0.011 | Tree loss: 9.016 | Accuracy: 0.564453 | 0.349 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 030 | Total loss: 9.004 | Reg loss: 0.011 | Tree loss: 9.004 | Accuracy: 0.556641 | 0.349 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 030 | Total loss: 8.985 | Reg loss: 0.012 | Tree loss: 8.985 | Accuracy: 0.542969 | 0.35 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 030 | Total loss: 8.971 | Reg loss: 0.012 | Tree loss: 8.971 | Accuracy: 0.519531 | 0.349 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 030 | Total loss: 8.949 | Reg loss: 0.012 | Tree loss: 8.949 | Accuracy: 0.582031 | 0.349 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 030 | Total loss: 8.931 | Reg loss: 0.013 | Tree loss: 8.931 | Accuracy: 0.578125 | 0.348 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 030 | Total loss: 8.923 | Reg loss: 0.013 | Tree loss: 8.923 | Accuracy: 0.589844 | 0.348 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 030 | Total loss: 8.903 | Reg loss: 0.013 | Tree loss: 8.903 | Accuracy: 0.611328 | 0.347 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 030 | Total loss: 8.890 | Reg loss: 0.014 | Tree loss: 8.890 | Accuracy: 0.539062 | 0.347 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 030 | Total loss: 8.870 | Reg loss: 0.014 | Tree loss: 8.870 | Accuracy: 0.568359 | 0.347 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 030 | Total loss: 8.855 | Reg loss: 0.015 | Tree loss: 8.855 | Accuracy: 0.609375 | 0.346 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 030 | Total loss: 8.847 | Reg loss: 0.015 | Tree loss: 8.847 | Accuracy: 0.582031 | 0.346 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 030 | Total loss: 8.825 | Reg loss: 0.015 | Tree loss: 8.825 | Accuracy: 0.619141 | 0.346 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 030 | Total loss: 8.810 | Reg loss: 0.016 | Tree loss: 8.810 | Accuracy: 0.582031 | 0.346 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 030 | Total loss: 8.785 | Reg loss: 0.016 | Tree loss: 8.785 | Accuracy: 0.593750 | 0.346 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 030 | Total loss: 8.784 | Reg loss: 0.017 | Tree loss: 8.784 | Accuracy: 0.576172 | 0.345 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 030 | Total loss: 8.759 | Reg loss: 0.017 | Tree loss: 8.759 | Accuracy: 0.564453 | 0.345 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 030 | Total loss: 8.746 | Reg loss: 0.018 | Tree loss: 8.746 | Accuracy: 0.595703 | 0.345 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 030 | Total loss: 8.733 | Reg loss: 0.018 | Tree loss: 8.733 | Accuracy: 0.576172 | 0.344 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 030 | Total loss: 8.713 | Reg loss: 0.019 | Tree loss: 8.713 | Accuracy: 0.548828 | 0.344 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 030 | Total loss: 8.694 | Reg loss: 0.019 | Tree loss: 8.694 | Accuracy: 0.574219 | 0.344 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 030 | Total loss: 8.672 | Reg loss: 0.019 | Tree loss: 8.672 | Accuracy: 0.564453 | 0.344 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 030 | Total loss: 8.664 | Reg loss: 0.020 | Tree loss: 8.664 | Accuracy: 0.609375 | 0.344 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 030 | Total loss: 8.644 | Reg loss: 0.020 | Tree loss: 8.644 | Accuracy: 0.566406 | 0.345 sec/iter\n",
      "Epoch: 03 | Batch: 029 / 030 | Total loss: 8.647 | Reg loss: 0.021 | Tree loss: 8.647 | Accuracy: 0.490741 | 0.345 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 04 | Batch: 000 / 030 | Total loss: 8.914 | Reg loss: 0.013 | Tree loss: 8.914 | Accuracy: 0.556641 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 030 | Total loss: 8.901 | Reg loss: 0.013 | Tree loss: 8.901 | Accuracy: 0.599609 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 030 | Total loss: 8.889 | Reg loss: 0.013 | Tree loss: 8.889 | Accuracy: 0.564453 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 030 | Total loss: 8.868 | Reg loss: 0.013 | Tree loss: 8.868 | Accuracy: 0.574219 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 030 | Total loss: 8.855 | Reg loss: 0.013 | Tree loss: 8.855 | Accuracy: 0.576172 | 0.347 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 005 / 030 | Total loss: 8.836 | Reg loss: 0.013 | Tree loss: 8.836 | Accuracy: 0.580078 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 030 | Total loss: 8.829 | Reg loss: 0.014 | Tree loss: 8.829 | Accuracy: 0.539062 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 030 | Total loss: 8.806 | Reg loss: 0.014 | Tree loss: 8.806 | Accuracy: 0.570312 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 030 | Total loss: 8.791 | Reg loss: 0.014 | Tree loss: 8.791 | Accuracy: 0.585938 | 0.347 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 030 | Total loss: 8.780 | Reg loss: 0.014 | Tree loss: 8.780 | Accuracy: 0.527344 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 030 | Total loss: 8.756 | Reg loss: 0.015 | Tree loss: 8.756 | Accuracy: 0.562500 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 030 | Total loss: 8.745 | Reg loss: 0.015 | Tree loss: 8.745 | Accuracy: 0.556641 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 030 | Total loss: 8.724 | Reg loss: 0.015 | Tree loss: 8.724 | Accuracy: 0.558594 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 030 | Total loss: 8.720 | Reg loss: 0.016 | Tree loss: 8.720 | Accuracy: 0.535156 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 030 | Total loss: 8.690 | Reg loss: 0.016 | Tree loss: 8.690 | Accuracy: 0.560547 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 030 | Total loss: 8.679 | Reg loss: 0.017 | Tree loss: 8.679 | Accuracy: 0.564453 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 030 | Total loss: 8.659 | Reg loss: 0.017 | Tree loss: 8.659 | Accuracy: 0.550781 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 030 | Total loss: 8.636 | Reg loss: 0.017 | Tree loss: 8.636 | Accuracy: 0.601562 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 030 | Total loss: 8.623 | Reg loss: 0.018 | Tree loss: 8.623 | Accuracy: 0.599609 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 030 | Total loss: 8.607 | Reg loss: 0.018 | Tree loss: 8.607 | Accuracy: 0.562500 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 030 | Total loss: 8.590 | Reg loss: 0.019 | Tree loss: 8.590 | Accuracy: 0.613281 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 030 | Total loss: 8.576 | Reg loss: 0.019 | Tree loss: 8.576 | Accuracy: 0.595703 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 030 | Total loss: 8.558 | Reg loss: 0.019 | Tree loss: 8.558 | Accuracy: 0.552734 | 0.348 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 030 | Total loss: 8.539 | Reg loss: 0.020 | Tree loss: 8.539 | Accuracy: 0.587891 | 0.349 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 030 | Total loss: 8.516 | Reg loss: 0.020 | Tree loss: 8.516 | Accuracy: 0.611328 | 0.349 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 030 | Total loss: 8.502 | Reg loss: 0.021 | Tree loss: 8.502 | Accuracy: 0.583984 | 0.349 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 030 | Total loss: 8.483 | Reg loss: 0.021 | Tree loss: 8.483 | Accuracy: 0.585938 | 0.349 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 030 | Total loss: 8.480 | Reg loss: 0.022 | Tree loss: 8.480 | Accuracy: 0.566406 | 0.349 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 030 | Total loss: 8.465 | Reg loss: 0.022 | Tree loss: 8.465 | Accuracy: 0.591797 | 0.349 sec/iter\n",
      "Epoch: 04 | Batch: 029 / 030 | Total loss: 8.432 | Reg loss: 0.022 | Tree loss: 8.432 | Accuracy: 0.601852 | 0.349 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 05 | Batch: 000 / 030 | Total loss: 8.744 | Reg loss: 0.015 | Tree loss: 8.744 | Accuracy: 0.566406 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 030 | Total loss: 8.730 | Reg loss: 0.015 | Tree loss: 8.730 | Accuracy: 0.587891 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 030 | Total loss: 8.714 | Reg loss: 0.015 | Tree loss: 8.714 | Accuracy: 0.574219 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 030 | Total loss: 8.692 | Reg loss: 0.015 | Tree loss: 8.692 | Accuracy: 0.603516 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 030 | Total loss: 8.683 | Reg loss: 0.015 | Tree loss: 8.683 | Accuracy: 0.574219 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 030 | Total loss: 8.665 | Reg loss: 0.015 | Tree loss: 8.665 | Accuracy: 0.605469 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 030 | Total loss: 8.655 | Reg loss: 0.016 | Tree loss: 8.655 | Accuracy: 0.542969 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 030 | Total loss: 8.638 | Reg loss: 0.016 | Tree loss: 8.638 | Accuracy: 0.519531 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 030 | Total loss: 8.611 | Reg loss: 0.016 | Tree loss: 8.611 | Accuracy: 0.597656 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 030 | Total loss: 8.591 | Reg loss: 0.016 | Tree loss: 8.591 | Accuracy: 0.572266 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 030 | Total loss: 8.575 | Reg loss: 0.017 | Tree loss: 8.575 | Accuracy: 0.582031 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 030 | Total loss: 8.559 | Reg loss: 0.017 | Tree loss: 8.559 | Accuracy: 0.570312 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 030 | Total loss: 8.534 | Reg loss: 0.017 | Tree loss: 8.534 | Accuracy: 0.593750 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 030 | Total loss: 8.524 | Reg loss: 0.018 | Tree loss: 8.524 | Accuracy: 0.582031 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 030 | Total loss: 8.508 | Reg loss: 0.018 | Tree loss: 8.508 | Accuracy: 0.570312 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 030 | Total loss: 8.492 | Reg loss: 0.018 | Tree loss: 8.492 | Accuracy: 0.574219 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 030 | Total loss: 8.469 | Reg loss: 0.019 | Tree loss: 8.469 | Accuracy: 0.615234 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 030 | Total loss: 8.461 | Reg loss: 0.019 | Tree loss: 8.461 | Accuracy: 0.546875 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 030 | Total loss: 8.442 | Reg loss: 0.020 | Tree loss: 8.442 | Accuracy: 0.548828 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 030 | Total loss: 8.422 | Reg loss: 0.020 | Tree loss: 8.422 | Accuracy: 0.572266 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 030 | Total loss: 8.405 | Reg loss: 0.020 | Tree loss: 8.405 | Accuracy: 0.578125 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 030 | Total loss: 8.380 | Reg loss: 0.021 | Tree loss: 8.380 | Accuracy: 0.574219 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 030 | Total loss: 8.371 | Reg loss: 0.021 | Tree loss: 8.371 | Accuracy: 0.587891 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 030 | Total loss: 8.351 | Reg loss: 0.022 | Tree loss: 8.351 | Accuracy: 0.570312 | 0.35 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 030 | Total loss: 8.329 | Reg loss: 0.022 | Tree loss: 8.329 | Accuracy: 0.585938 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 030 | Total loss: 8.321 | Reg loss: 0.022 | Tree loss: 8.321 | Accuracy: 0.552734 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 030 | Total loss: 8.291 | Reg loss: 0.023 | Tree loss: 8.291 | Accuracy: 0.560547 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 030 | Total loss: 8.271 | Reg loss: 0.023 | Tree loss: 8.271 | Accuracy: 0.611328 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 030 | Total loss: 8.271 | Reg loss: 0.023 | Tree loss: 8.271 | Accuracy: 0.564453 | 0.351 sec/iter\n",
      "Epoch: 05 | Batch: 029 / 030 | Total loss: 8.261 | Reg loss: 0.024 | Tree loss: 8.261 | Accuracy: 0.620370 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 06 | Batch: 000 / 030 | Total loss: 8.566 | Reg loss: 0.017 | Tree loss: 8.566 | Accuracy: 0.601562 | 0.352 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 030 | Total loss: 8.552 | Reg loss: 0.017 | Tree loss: 8.552 | Accuracy: 0.533203 | 0.352 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 030 | Total loss: 8.530 | Reg loss: 0.017 | Tree loss: 8.530 | Accuracy: 0.591797 | 0.352 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 030 | Total loss: 8.522 | Reg loss: 0.017 | Tree loss: 8.522 | Accuracy: 0.554688 | 0.352 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 030 | Total loss: 8.502 | Reg loss: 0.017 | Tree loss: 8.502 | Accuracy: 0.583984 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 030 | Total loss: 8.488 | Reg loss: 0.017 | Tree loss: 8.488 | Accuracy: 0.548828 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 030 | Total loss: 8.467 | Reg loss: 0.018 | Tree loss: 8.467 | Accuracy: 0.566406 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 030 | Total loss: 8.452 | Reg loss: 0.018 | Tree loss: 8.452 | Accuracy: 0.556641 | 0.351 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 008 / 030 | Total loss: 8.421 | Reg loss: 0.018 | Tree loss: 8.421 | Accuracy: 0.585938 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 030 | Total loss: 8.413 | Reg loss: 0.018 | Tree loss: 8.413 | Accuracy: 0.583984 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 030 | Total loss: 8.383 | Reg loss: 0.019 | Tree loss: 8.383 | Accuracy: 0.613281 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 030 | Total loss: 8.377 | Reg loss: 0.019 | Tree loss: 8.377 | Accuracy: 0.548828 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 030 | Total loss: 8.360 | Reg loss: 0.019 | Tree loss: 8.360 | Accuracy: 0.562500 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 030 | Total loss: 8.336 | Reg loss: 0.020 | Tree loss: 8.336 | Accuracy: 0.562500 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 030 | Total loss: 8.312 | Reg loss: 0.020 | Tree loss: 8.312 | Accuracy: 0.625000 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 030 | Total loss: 8.296 | Reg loss: 0.020 | Tree loss: 8.296 | Accuracy: 0.583984 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 030 | Total loss: 8.283 | Reg loss: 0.021 | Tree loss: 8.283 | Accuracy: 0.589844 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 030 | Total loss: 8.266 | Reg loss: 0.021 | Tree loss: 8.266 | Accuracy: 0.572266 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 030 | Total loss: 8.242 | Reg loss: 0.021 | Tree loss: 8.242 | Accuracy: 0.611328 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 030 | Total loss: 8.227 | Reg loss: 0.022 | Tree loss: 8.227 | Accuracy: 0.556641 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 030 | Total loss: 8.215 | Reg loss: 0.022 | Tree loss: 8.215 | Accuracy: 0.568359 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 030 | Total loss: 8.198 | Reg loss: 0.023 | Tree loss: 8.198 | Accuracy: 0.570312 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 030 | Total loss: 8.177 | Reg loss: 0.023 | Tree loss: 8.177 | Accuracy: 0.568359 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 030 | Total loss: 8.148 | Reg loss: 0.023 | Tree loss: 8.148 | Accuracy: 0.580078 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 030 | Total loss: 8.138 | Reg loss: 0.024 | Tree loss: 8.138 | Accuracy: 0.552734 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 030 | Total loss: 8.121 | Reg loss: 0.024 | Tree loss: 8.121 | Accuracy: 0.556641 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 030 | Total loss: 8.097 | Reg loss: 0.024 | Tree loss: 8.097 | Accuracy: 0.560547 | 0.352 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 030 | Total loss: 8.073 | Reg loss: 0.025 | Tree loss: 8.073 | Accuracy: 0.595703 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 030 | Total loss: 8.068 | Reg loss: 0.025 | Tree loss: 8.068 | Accuracy: 0.548828 | 0.351 sec/iter\n",
      "Epoch: 06 | Batch: 029 / 030 | Total loss: 8.029 | Reg loss: 0.026 | Tree loss: 8.029 | Accuracy: 0.638889 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 07 | Batch: 000 / 030 | Total loss: 8.383 | Reg loss: 0.019 | Tree loss: 8.383 | Accuracy: 0.591797 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 030 | Total loss: 8.369 | Reg loss: 0.019 | Tree loss: 8.369 | Accuracy: 0.593750 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 030 | Total loss: 8.351 | Reg loss: 0.019 | Tree loss: 8.351 | Accuracy: 0.548828 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 030 | Total loss: 8.332 | Reg loss: 0.019 | Tree loss: 8.332 | Accuracy: 0.558594 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 030 | Total loss: 8.313 | Reg loss: 0.019 | Tree loss: 8.313 | Accuracy: 0.552734 | 0.353 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 030 | Total loss: 8.292 | Reg loss: 0.019 | Tree loss: 8.292 | Accuracy: 0.607422 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 030 | Total loss: 8.278 | Reg loss: 0.019 | Tree loss: 8.278 | Accuracy: 0.578125 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 030 | Total loss: 8.251 | Reg loss: 0.020 | Tree loss: 8.251 | Accuracy: 0.615234 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 030 | Total loss: 8.244 | Reg loss: 0.020 | Tree loss: 8.244 | Accuracy: 0.580078 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 030 | Total loss: 8.229 | Reg loss: 0.020 | Tree loss: 8.229 | Accuracy: 0.544922 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 030 | Total loss: 8.199 | Reg loss: 0.020 | Tree loss: 8.199 | Accuracy: 0.578125 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 030 | Total loss: 8.181 | Reg loss: 0.021 | Tree loss: 8.181 | Accuracy: 0.546875 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 030 | Total loss: 8.159 | Reg loss: 0.021 | Tree loss: 8.159 | Accuracy: 0.556641 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 030 | Total loss: 8.137 | Reg loss: 0.021 | Tree loss: 8.137 | Accuracy: 0.595703 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 030 | Total loss: 8.122 | Reg loss: 0.022 | Tree loss: 8.122 | Accuracy: 0.572266 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 030 | Total loss: 8.099 | Reg loss: 0.022 | Tree loss: 8.099 | Accuracy: 0.609375 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 030 | Total loss: 8.077 | Reg loss: 0.022 | Tree loss: 8.077 | Accuracy: 0.591797 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 030 | Total loss: 8.052 | Reg loss: 0.023 | Tree loss: 8.052 | Accuracy: 0.601562 | 0.352 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 030 | Total loss: 8.049 | Reg loss: 0.023 | Tree loss: 8.049 | Accuracy: 0.542969 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 030 | Total loss: 8.035 | Reg loss: 0.024 | Tree loss: 8.035 | Accuracy: 0.570312 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 030 | Total loss: 8.000 | Reg loss: 0.024 | Tree loss: 8.000 | Accuracy: 0.585938 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 030 | Total loss: 7.984 | Reg loss: 0.024 | Tree loss: 7.984 | Accuracy: 0.550781 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 030 | Total loss: 7.964 | Reg loss: 0.025 | Tree loss: 7.964 | Accuracy: 0.574219 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 030 | Total loss: 7.949 | Reg loss: 0.025 | Tree loss: 7.949 | Accuracy: 0.542969 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 030 | Total loss: 7.938 | Reg loss: 0.025 | Tree loss: 7.938 | Accuracy: 0.550781 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 030 | Total loss: 7.904 | Reg loss: 0.026 | Tree loss: 7.904 | Accuracy: 0.591797 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 030 | Total loss: 7.885 | Reg loss: 0.026 | Tree loss: 7.885 | Accuracy: 0.556641 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 030 | Total loss: 7.866 | Reg loss: 0.026 | Tree loss: 7.866 | Accuracy: 0.580078 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 030 | Total loss: 7.847 | Reg loss: 0.027 | Tree loss: 7.847 | Accuracy: 0.587891 | 0.351 sec/iter\n",
      "Epoch: 07 | Batch: 029 / 030 | Total loss: 7.810 | Reg loss: 0.027 | Tree loss: 7.810 | Accuracy: 0.574074 | 0.351 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 08 | Batch: 000 / 030 | Total loss: 8.196 | Reg loss: 0.021 | Tree loss: 8.196 | Accuracy: 0.562500 | 0.352 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 030 | Total loss: 8.171 | Reg loss: 0.021 | Tree loss: 8.171 | Accuracy: 0.578125 | 0.352 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 030 | Total loss: 8.157 | Reg loss: 0.021 | Tree loss: 8.157 | Accuracy: 0.570312 | 0.352 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 030 | Total loss: 8.130 | Reg loss: 0.021 | Tree loss: 8.130 | Accuracy: 0.613281 | 0.352 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 030 | Total loss: 8.119 | Reg loss: 0.021 | Tree loss: 8.119 | Accuracy: 0.560547 | 0.352 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 030 | Total loss: 8.104 | Reg loss: 0.021 | Tree loss: 8.104 | Accuracy: 0.554688 | 0.352 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 030 | Total loss: 8.084 | Reg loss: 0.021 | Tree loss: 8.084 | Accuracy: 0.529297 | 0.352 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 030 | Total loss: 8.066 | Reg loss: 0.022 | Tree loss: 8.066 | Accuracy: 0.537109 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 030 | Total loss: 8.028 | Reg loss: 0.022 | Tree loss: 8.028 | Accuracy: 0.597656 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 030 | Total loss: 8.006 | Reg loss: 0.022 | Tree loss: 8.006 | Accuracy: 0.560547 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 030 | Total loss: 7.996 | Reg loss: 0.022 | Tree loss: 7.996 | Accuracy: 0.582031 | 0.353 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 011 / 030 | Total loss: 7.980 | Reg loss: 0.023 | Tree loss: 7.980 | Accuracy: 0.591797 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 030 | Total loss: 7.944 | Reg loss: 0.023 | Tree loss: 7.944 | Accuracy: 0.566406 | 0.354 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 030 | Total loss: 7.936 | Reg loss: 0.023 | Tree loss: 7.936 | Accuracy: 0.572266 | 0.354 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 030 | Total loss: 7.921 | Reg loss: 0.024 | Tree loss: 7.921 | Accuracy: 0.576172 | 0.354 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 030 | Total loss: 7.892 | Reg loss: 0.024 | Tree loss: 7.892 | Accuracy: 0.570312 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 030 | Total loss: 7.879 | Reg loss: 0.024 | Tree loss: 7.879 | Accuracy: 0.554688 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 030 | Total loss: 7.854 | Reg loss: 0.025 | Tree loss: 7.854 | Accuracy: 0.564453 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 030 | Total loss: 7.826 | Reg loss: 0.025 | Tree loss: 7.826 | Accuracy: 0.564453 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 030 | Total loss: 7.803 | Reg loss: 0.025 | Tree loss: 7.803 | Accuracy: 0.554688 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 030 | Total loss: 7.777 | Reg loss: 0.026 | Tree loss: 7.777 | Accuracy: 0.580078 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 030 | Total loss: 7.770 | Reg loss: 0.026 | Tree loss: 7.770 | Accuracy: 0.578125 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 030 | Total loss: 7.757 | Reg loss: 0.026 | Tree loss: 7.757 | Accuracy: 0.560547 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 030 | Total loss: 7.723 | Reg loss: 0.027 | Tree loss: 7.723 | Accuracy: 0.595703 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 030 | Total loss: 7.708 | Reg loss: 0.027 | Tree loss: 7.708 | Accuracy: 0.570312 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 030 | Total loss: 7.661 | Reg loss: 0.028 | Tree loss: 7.661 | Accuracy: 0.603516 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 030 | Total loss: 7.655 | Reg loss: 0.028 | Tree loss: 7.655 | Accuracy: 0.568359 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 030 | Total loss: 7.636 | Reg loss: 0.028 | Tree loss: 7.636 | Accuracy: 0.587891 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 030 | Total loss: 7.607 | Reg loss: 0.029 | Tree loss: 7.607 | Accuracy: 0.644531 | 0.353 sec/iter\n",
      "Epoch: 08 | Batch: 029 / 030 | Total loss: 7.597 | Reg loss: 0.029 | Tree loss: 7.597 | Accuracy: 0.685185 | 0.353 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 09 | Batch: 000 / 030 | Total loss: 7.982 | Reg loss: 0.022 | Tree loss: 7.982 | Accuracy: 0.578125 | 0.353 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 030 | Total loss: 7.980 | Reg loss: 0.022 | Tree loss: 7.980 | Accuracy: 0.580078 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 030 | Total loss: 7.959 | Reg loss: 0.022 | Tree loss: 7.959 | Accuracy: 0.556641 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 030 | Total loss: 7.928 | Reg loss: 0.023 | Tree loss: 7.928 | Accuracy: 0.605469 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 030 | Total loss: 7.921 | Reg loss: 0.023 | Tree loss: 7.921 | Accuracy: 0.580078 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 030 | Total loss: 7.895 | Reg loss: 0.023 | Tree loss: 7.895 | Accuracy: 0.556641 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 030 | Total loss: 7.877 | Reg loss: 0.023 | Tree loss: 7.877 | Accuracy: 0.550781 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 030 | Total loss: 7.843 | Reg loss: 0.023 | Tree loss: 7.843 | Accuracy: 0.562500 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 030 | Total loss: 7.820 | Reg loss: 0.023 | Tree loss: 7.820 | Accuracy: 0.599609 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 030 | Total loss: 7.803 | Reg loss: 0.024 | Tree loss: 7.803 | Accuracy: 0.548828 | 0.353 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 030 | Total loss: 7.786 | Reg loss: 0.024 | Tree loss: 7.786 | Accuracy: 0.539062 | 0.353 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 030 | Total loss: 7.751 | Reg loss: 0.024 | Tree loss: 7.751 | Accuracy: 0.607422 | 0.353 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 030 | Total loss: 7.741 | Reg loss: 0.025 | Tree loss: 7.741 | Accuracy: 0.558594 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 030 | Total loss: 7.722 | Reg loss: 0.025 | Tree loss: 7.722 | Accuracy: 0.583984 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 030 | Total loss: 7.688 | Reg loss: 0.025 | Tree loss: 7.688 | Accuracy: 0.599609 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 030 | Total loss: 7.679 | Reg loss: 0.026 | Tree loss: 7.679 | Accuracy: 0.542969 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 030 | Total loss: 7.646 | Reg loss: 0.026 | Tree loss: 7.646 | Accuracy: 0.605469 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 030 | Total loss: 7.625 | Reg loss: 0.026 | Tree loss: 7.625 | Accuracy: 0.546875 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 030 | Total loss: 7.598 | Reg loss: 0.027 | Tree loss: 7.598 | Accuracy: 0.574219 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 030 | Total loss: 7.572 | Reg loss: 0.027 | Tree loss: 7.572 | Accuracy: 0.589844 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 030 | Total loss: 7.550 | Reg loss: 0.027 | Tree loss: 7.550 | Accuracy: 0.603516 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 030 | Total loss: 7.520 | Reg loss: 0.028 | Tree loss: 7.520 | Accuracy: 0.587891 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 030 | Total loss: 7.512 | Reg loss: 0.028 | Tree loss: 7.512 | Accuracy: 0.566406 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 030 | Total loss: 7.501 | Reg loss: 0.028 | Tree loss: 7.501 | Accuracy: 0.583984 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 030 | Total loss: 7.481 | Reg loss: 0.029 | Tree loss: 7.481 | Accuracy: 0.576172 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 030 | Total loss: 7.449 | Reg loss: 0.029 | Tree loss: 7.449 | Accuracy: 0.544922 | 0.355 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 030 | Total loss: 7.425 | Reg loss: 0.029 | Tree loss: 7.425 | Accuracy: 0.572266 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 030 | Total loss: 7.403 | Reg loss: 0.030 | Tree loss: 7.403 | Accuracy: 0.574219 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 030 | Total loss: 7.382 | Reg loss: 0.030 | Tree loss: 7.382 | Accuracy: 0.560547 | 0.354 sec/iter\n",
      "Epoch: 09 | Batch: 029 / 030 | Total loss: 7.366 | Reg loss: 0.030 | Tree loss: 7.366 | Accuracy: 0.555556 | 0.354 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 10 | Batch: 000 / 030 | Total loss: 7.780 | Reg loss: 0.024 | Tree loss: 7.780 | Accuracy: 0.593750 | 0.355 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 030 | Total loss: 7.764 | Reg loss: 0.024 | Tree loss: 7.764 | Accuracy: 0.574219 | 0.355 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 030 | Total loss: 7.762 | Reg loss: 0.024 | Tree loss: 7.762 | Accuracy: 0.533203 | 0.355 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 030 | Total loss: 7.722 | Reg loss: 0.024 | Tree loss: 7.722 | Accuracy: 0.583984 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 030 | Total loss: 7.705 | Reg loss: 0.024 | Tree loss: 7.705 | Accuracy: 0.558594 | 0.355 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 030 | Total loss: 7.686 | Reg loss: 0.024 | Tree loss: 7.686 | Accuracy: 0.542969 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 030 | Total loss: 7.651 | Reg loss: 0.025 | Tree loss: 7.651 | Accuracy: 0.554688 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 030 | Total loss: 7.626 | Reg loss: 0.025 | Tree loss: 7.626 | Accuracy: 0.576172 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 030 | Total loss: 7.610 | Reg loss: 0.025 | Tree loss: 7.610 | Accuracy: 0.568359 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 030 | Total loss: 7.584 | Reg loss: 0.025 | Tree loss: 7.584 | Accuracy: 0.595703 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 030 | Total loss: 7.554 | Reg loss: 0.025 | Tree loss: 7.554 | Accuracy: 0.583984 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 030 | Total loss: 7.522 | Reg loss: 0.026 | Tree loss: 7.522 | Accuracy: 0.593750 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 030 | Total loss: 7.501 | Reg loss: 0.026 | Tree loss: 7.501 | Accuracy: 0.609375 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 030 | Total loss: 7.483 | Reg loss: 0.026 | Tree loss: 7.483 | Accuracy: 0.572266 | 0.356 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 014 / 030 | Total loss: 7.449 | Reg loss: 0.027 | Tree loss: 7.449 | Accuracy: 0.601562 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 030 | Total loss: 7.456 | Reg loss: 0.027 | Tree loss: 7.456 | Accuracy: 0.562500 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 030 | Total loss: 7.412 | Reg loss: 0.027 | Tree loss: 7.412 | Accuracy: 0.583984 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 030 | Total loss: 7.404 | Reg loss: 0.028 | Tree loss: 7.404 | Accuracy: 0.542969 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 030 | Total loss: 7.379 | Reg loss: 0.028 | Tree loss: 7.379 | Accuracy: 0.562500 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 030 | Total loss: 7.342 | Reg loss: 0.028 | Tree loss: 7.342 | Accuracy: 0.550781 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 030 | Total loss: 7.313 | Reg loss: 0.029 | Tree loss: 7.313 | Accuracy: 0.582031 | 0.356 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 030 | Total loss: 7.276 | Reg loss: 0.029 | Tree loss: 7.276 | Accuracy: 0.619141 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 030 | Total loss: 7.280 | Reg loss: 0.029 | Tree loss: 7.280 | Accuracy: 0.574219 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 030 | Total loss: 7.248 | Reg loss: 0.030 | Tree loss: 7.248 | Accuracy: 0.560547 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 030 | Total loss: 7.223 | Reg loss: 0.030 | Tree loss: 7.223 | Accuracy: 0.589844 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 030 | Total loss: 7.192 | Reg loss: 0.030 | Tree loss: 7.192 | Accuracy: 0.572266 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 030 | Total loss: 7.185 | Reg loss: 0.031 | Tree loss: 7.185 | Accuracy: 0.541016 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 030 | Total loss: 7.140 | Reg loss: 0.031 | Tree loss: 7.140 | Accuracy: 0.558594 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 030 | Total loss: 7.128 | Reg loss: 0.031 | Tree loss: 7.128 | Accuracy: 0.593750 | 0.357 sec/iter\n",
      "Epoch: 10 | Batch: 029 / 030 | Total loss: 7.110 | Reg loss: 0.032 | Tree loss: 7.110 | Accuracy: 0.537037 | 0.357 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 11 | Batch: 000 / 030 | Total loss: 7.563 | Reg loss: 0.025 | Tree loss: 7.563 | Accuracy: 0.609375 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 030 | Total loss: 7.556 | Reg loss: 0.025 | Tree loss: 7.556 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 030 | Total loss: 7.535 | Reg loss: 0.025 | Tree loss: 7.535 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 030 | Total loss: 7.503 | Reg loss: 0.026 | Tree loss: 7.503 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 030 | Total loss: 7.476 | Reg loss: 0.026 | Tree loss: 7.476 | Accuracy: 0.560547 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 030 | Total loss: 7.447 | Reg loss: 0.026 | Tree loss: 7.447 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 030 | Total loss: 7.426 | Reg loss: 0.026 | Tree loss: 7.426 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 030 | Total loss: 7.414 | Reg loss: 0.026 | Tree loss: 7.414 | Accuracy: 0.546875 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 030 | Total loss: 7.376 | Reg loss: 0.026 | Tree loss: 7.376 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 030 | Total loss: 7.352 | Reg loss: 0.027 | Tree loss: 7.352 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 030 | Total loss: 7.338 | Reg loss: 0.027 | Tree loss: 7.338 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 030 | Total loss: 7.318 | Reg loss: 0.027 | Tree loss: 7.318 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 030 | Total loss: 7.266 | Reg loss: 0.027 | Tree loss: 7.266 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 030 | Total loss: 7.249 | Reg loss: 0.028 | Tree loss: 7.249 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 030 | Total loss: 7.216 | Reg loss: 0.028 | Tree loss: 7.216 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 030 | Total loss: 7.206 | Reg loss: 0.028 | Tree loss: 7.206 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 030 | Total loss: 7.172 | Reg loss: 0.029 | Tree loss: 7.172 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 030 | Total loss: 7.136 | Reg loss: 0.029 | Tree loss: 7.136 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 030 | Total loss: 7.143 | Reg loss: 0.029 | Tree loss: 7.143 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 030 | Total loss: 7.081 | Reg loss: 0.030 | Tree loss: 7.081 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 030 | Total loss: 7.060 | Reg loss: 0.030 | Tree loss: 7.060 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 030 | Total loss: 7.034 | Reg loss: 0.030 | Tree loss: 7.034 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 030 | Total loss: 7.025 | Reg loss: 0.031 | Tree loss: 7.025 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 030 | Total loss: 6.992 | Reg loss: 0.031 | Tree loss: 6.992 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 030 | Total loss: 6.959 | Reg loss: 0.031 | Tree loss: 6.959 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 030 | Total loss: 6.939 | Reg loss: 0.032 | Tree loss: 6.939 | Accuracy: 0.535156 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 030 | Total loss: 6.902 | Reg loss: 0.032 | Tree loss: 6.902 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 030 | Total loss: 6.885 | Reg loss: 0.032 | Tree loss: 6.885 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 030 | Total loss: 6.854 | Reg loss: 0.033 | Tree loss: 6.854 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 11 | Batch: 029 / 030 | Total loss: 6.802 | Reg loss: 0.033 | Tree loss: 6.802 | Accuracy: 0.620370 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 12 | Batch: 000 / 030 | Total loss: 7.359 | Reg loss: 0.026 | Tree loss: 7.359 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 030 | Total loss: 7.335 | Reg loss: 0.027 | Tree loss: 7.335 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 030 | Total loss: 7.292 | Reg loss: 0.027 | Tree loss: 7.292 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 030 | Total loss: 7.272 | Reg loss: 0.027 | Tree loss: 7.272 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 030 | Total loss: 7.233 | Reg loss: 0.027 | Tree loss: 7.233 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 030 | Total loss: 7.242 | Reg loss: 0.027 | Tree loss: 7.242 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 030 | Total loss: 7.195 | Reg loss: 0.027 | Tree loss: 7.195 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 030 | Total loss: 7.172 | Reg loss: 0.027 | Tree loss: 7.172 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 030 | Total loss: 7.151 | Reg loss: 0.027 | Tree loss: 7.151 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 030 | Total loss: 7.096 | Reg loss: 0.028 | Tree loss: 7.096 | Accuracy: 0.619141 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 030 | Total loss: 7.066 | Reg loss: 0.028 | Tree loss: 7.066 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 030 | Total loss: 7.075 | Reg loss: 0.028 | Tree loss: 7.075 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 030 | Total loss: 7.014 | Reg loss: 0.029 | Tree loss: 7.014 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 030 | Total loss: 6.987 | Reg loss: 0.029 | Tree loss: 6.987 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 030 | Total loss: 6.974 | Reg loss: 0.029 | Tree loss: 6.974 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 030 | Total loss: 6.945 | Reg loss: 0.030 | Tree loss: 6.945 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 030 | Total loss: 6.933 | Reg loss: 0.030 | Tree loss: 6.933 | Accuracy: 0.556641 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 017 / 030 | Total loss: 6.881 | Reg loss: 0.030 | Tree loss: 6.881 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 030 | Total loss: 6.865 | Reg loss: 0.031 | Tree loss: 6.865 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 030 | Total loss: 6.829 | Reg loss: 0.031 | Tree loss: 6.829 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 030 | Total loss: 6.806 | Reg loss: 0.031 | Tree loss: 6.806 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 030 | Total loss: 6.771 | Reg loss: 0.032 | Tree loss: 6.771 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 030 | Total loss: 6.777 | Reg loss: 0.032 | Tree loss: 6.777 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 030 | Total loss: 6.732 | Reg loss: 0.032 | Tree loss: 6.732 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 030 | Total loss: 6.687 | Reg loss: 0.033 | Tree loss: 6.687 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 030 | Total loss: 6.655 | Reg loss: 0.033 | Tree loss: 6.655 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 030 | Total loss: 6.638 | Reg loss: 0.034 | Tree loss: 6.638 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 030 | Total loss: 6.612 | Reg loss: 0.034 | Tree loss: 6.612 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 030 | Total loss: 6.573 | Reg loss: 0.034 | Tree loss: 6.573 | Accuracy: 0.525391 | 0.36 sec/iter\n",
      "Epoch: 12 | Batch: 029 / 030 | Total loss: 6.574 | Reg loss: 0.035 | Tree loss: 6.574 | Accuracy: 0.564815 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 13 | Batch: 000 / 030 | Total loss: 7.126 | Reg loss: 0.028 | Tree loss: 7.126 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 030 | Total loss: 7.085 | Reg loss: 0.028 | Tree loss: 7.085 | Accuracy: 0.539062 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 030 | Total loss: 7.067 | Reg loss: 0.028 | Tree loss: 7.067 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 030 | Total loss: 7.055 | Reg loss: 0.028 | Tree loss: 7.055 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 030 | Total loss: 7.022 | Reg loss: 0.028 | Tree loss: 7.022 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 030 | Total loss: 6.995 | Reg loss: 0.028 | Tree loss: 6.995 | Accuracy: 0.531250 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 030 | Total loss: 6.960 | Reg loss: 0.028 | Tree loss: 6.960 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 030 | Total loss: 6.946 | Reg loss: 0.028 | Tree loss: 6.946 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 030 | Total loss: 6.896 | Reg loss: 0.029 | Tree loss: 6.896 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 030 | Total loss: 6.866 | Reg loss: 0.029 | Tree loss: 6.866 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 030 | Total loss: 6.839 | Reg loss: 0.029 | Tree loss: 6.839 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 030 | Total loss: 6.815 | Reg loss: 0.029 | Tree loss: 6.815 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 030 | Total loss: 6.753 | Reg loss: 0.030 | Tree loss: 6.753 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 030 | Total loss: 6.748 | Reg loss: 0.030 | Tree loss: 6.748 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 030 | Total loss: 6.733 | Reg loss: 0.030 | Tree loss: 6.733 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 030 | Total loss: 6.680 | Reg loss: 0.031 | Tree loss: 6.680 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 030 | Total loss: 6.643 | Reg loss: 0.031 | Tree loss: 6.643 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 030 | Total loss: 6.644 | Reg loss: 0.032 | Tree loss: 6.644 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 030 | Total loss: 6.607 | Reg loss: 0.032 | Tree loss: 6.607 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 030 | Total loss: 6.560 | Reg loss: 0.032 | Tree loss: 6.560 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 030 | Total loss: 6.544 | Reg loss: 0.033 | Tree loss: 6.544 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 030 | Total loss: 6.514 | Reg loss: 0.033 | Tree loss: 6.514 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 030 | Total loss: 6.488 | Reg loss: 0.034 | Tree loss: 6.488 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 030 | Total loss: 6.427 | Reg loss: 0.034 | Tree loss: 6.427 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 030 | Total loss: 6.389 | Reg loss: 0.034 | Tree loss: 6.389 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 030 | Total loss: 6.380 | Reg loss: 0.035 | Tree loss: 6.380 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 030 | Total loss: 6.366 | Reg loss: 0.035 | Tree loss: 6.366 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 030 | Total loss: 6.332 | Reg loss: 0.035 | Tree loss: 6.332 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 030 | Total loss: 6.316 | Reg loss: 0.036 | Tree loss: 6.316 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 13 | Batch: 029 / 030 | Total loss: 6.271 | Reg loss: 0.036 | Tree loss: 6.271 | Accuracy: 0.611111 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 14 | Batch: 000 / 030 | Total loss: 6.894 | Reg loss: 0.029 | Tree loss: 6.894 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 030 | Total loss: 6.865 | Reg loss: 0.029 | Tree loss: 6.865 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 030 | Total loss: 6.878 | Reg loss: 0.029 | Tree loss: 6.878 | Accuracy: 0.525391 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 030 | Total loss: 6.787 | Reg loss: 0.029 | Tree loss: 6.787 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 030 | Total loss: 6.777 | Reg loss: 0.029 | Tree loss: 6.777 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 030 | Total loss: 6.749 | Reg loss: 0.029 | Tree loss: 6.749 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 030 | Total loss: 6.729 | Reg loss: 0.030 | Tree loss: 6.729 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 030 | Total loss: 6.690 | Reg loss: 0.030 | Tree loss: 6.690 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 030 | Total loss: 6.661 | Reg loss: 0.030 | Tree loss: 6.661 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 030 | Total loss: 6.616 | Reg loss: 0.030 | Tree loss: 6.616 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 030 | Total loss: 6.593 | Reg loss: 0.031 | Tree loss: 6.593 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 030 | Total loss: 6.574 | Reg loss: 0.031 | Tree loss: 6.574 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 030 | Total loss: 6.502 | Reg loss: 0.031 | Tree loss: 6.502 | Accuracy: 0.634766 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 030 | Total loss: 6.482 | Reg loss: 0.032 | Tree loss: 6.482 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 030 | Total loss: 6.460 | Reg loss: 0.032 | Tree loss: 6.460 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 030 | Total loss: 6.418 | Reg loss: 0.032 | Tree loss: 6.418 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 030 | Total loss: 6.411 | Reg loss: 0.033 | Tree loss: 6.411 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 030 | Total loss: 6.364 | Reg loss: 0.033 | Tree loss: 6.364 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 030 | Total loss: 6.307 | Reg loss: 0.033 | Tree loss: 6.307 | Accuracy: 0.619141 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 030 | Total loss: 6.286 | Reg loss: 0.034 | Tree loss: 6.286 | Accuracy: 0.570312 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 020 / 030 | Total loss: 6.270 | Reg loss: 0.034 | Tree loss: 6.270 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 030 | Total loss: 6.228 | Reg loss: 0.034 | Tree loss: 6.228 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 030 | Total loss: 6.225 | Reg loss: 0.035 | Tree loss: 6.225 | Accuracy: 0.544922 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 030 | Total loss: 6.182 | Reg loss: 0.035 | Tree loss: 6.182 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 030 | Total loss: 6.164 | Reg loss: 0.036 | Tree loss: 6.164 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 030 | Total loss: 6.115 | Reg loss: 0.036 | Tree loss: 6.115 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 030 | Total loss: 6.142 | Reg loss: 0.036 | Tree loss: 6.142 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 030 | Total loss: 6.071 | Reg loss: 0.037 | Tree loss: 6.071 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 030 | Total loss: 6.067 | Reg loss: 0.037 | Tree loss: 6.067 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 14 | Batch: 029 / 030 | Total loss: 6.044 | Reg loss: 0.037 | Tree loss: 6.044 | Accuracy: 0.574074 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 15 | Batch: 000 / 030 | Total loss: 6.678 | Reg loss: 0.030 | Tree loss: 6.678 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 030 | Total loss: 6.653 | Reg loss: 0.030 | Tree loss: 6.653 | Accuracy: 0.541016 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 030 | Total loss: 6.598 | Reg loss: 0.030 | Tree loss: 6.598 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 030 | Total loss: 6.587 | Reg loss: 0.030 | Tree loss: 6.587 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 030 | Total loss: 6.541 | Reg loss: 0.031 | Tree loss: 6.541 | Accuracy: 0.611328 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 030 | Total loss: 6.526 | Reg loss: 0.031 | Tree loss: 6.526 | Accuracy: 0.578125 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 030 | Total loss: 6.483 | Reg loss: 0.031 | Tree loss: 6.483 | Accuracy: 0.550781 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 030 | Total loss: 6.462 | Reg loss: 0.031 | Tree loss: 6.462 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 030 | Total loss: 6.438 | Reg loss: 0.031 | Tree loss: 6.438 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 030 | Total loss: 6.399 | Reg loss: 0.032 | Tree loss: 6.399 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 030 | Total loss: 6.359 | Reg loss: 0.032 | Tree loss: 6.359 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 030 | Total loss: 6.324 | Reg loss: 0.032 | Tree loss: 6.324 | Accuracy: 0.587891 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 030 | Total loss: 6.281 | Reg loss: 0.032 | Tree loss: 6.281 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 030 | Total loss: 6.244 | Reg loss: 0.033 | Tree loss: 6.244 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 030 | Total loss: 6.240 | Reg loss: 0.033 | Tree loss: 6.240 | Accuracy: 0.548828 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 030 | Total loss: 6.195 | Reg loss: 0.033 | Tree loss: 6.195 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 030 | Total loss: 6.170 | Reg loss: 0.034 | Tree loss: 6.170 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 030 | Total loss: 6.149 | Reg loss: 0.034 | Tree loss: 6.149 | Accuracy: 0.546875 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 030 | Total loss: 6.088 | Reg loss: 0.034 | Tree loss: 6.088 | Accuracy: 0.601562 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 030 | Total loss: 6.035 | Reg loss: 0.035 | Tree loss: 6.035 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 030 | Total loss: 6.011 | Reg loss: 0.035 | Tree loss: 6.011 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 030 | Total loss: 5.990 | Reg loss: 0.036 | Tree loss: 5.990 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 030 | Total loss: 5.982 | Reg loss: 0.036 | Tree loss: 5.982 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 030 | Total loss: 5.939 | Reg loss: 0.036 | Tree loss: 5.939 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 030 | Total loss: 5.884 | Reg loss: 0.037 | Tree loss: 5.884 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 030 | Total loss: 5.876 | Reg loss: 0.037 | Tree loss: 5.876 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 030 | Total loss: 5.845 | Reg loss: 0.037 | Tree loss: 5.845 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 030 | Total loss: 5.831 | Reg loss: 0.038 | Tree loss: 5.831 | Accuracy: 0.544922 | 0.361 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 030 | Total loss: 5.807 | Reg loss: 0.038 | Tree loss: 5.807 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 15 | Batch: 029 / 030 | Total loss: 5.723 | Reg loss: 0.038 | Tree loss: 5.723 | Accuracy: 0.592593 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 16 | Batch: 000 / 030 | Total loss: 6.433 | Reg loss: 0.031 | Tree loss: 6.433 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 030 | Total loss: 6.420 | Reg loss: 0.031 | Tree loss: 6.420 | Accuracy: 0.597656 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 030 | Total loss: 6.397 | Reg loss: 0.031 | Tree loss: 6.397 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 030 | Total loss: 6.366 | Reg loss: 0.031 | Tree loss: 6.366 | Accuracy: 0.593750 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 030 | Total loss: 6.346 | Reg loss: 0.032 | Tree loss: 6.346 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 030 | Total loss: 6.292 | Reg loss: 0.032 | Tree loss: 6.292 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 030 | Total loss: 6.277 | Reg loss: 0.032 | Tree loss: 6.277 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 030 | Total loss: 6.236 | Reg loss: 0.032 | Tree loss: 6.236 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 030 | Total loss: 6.197 | Reg loss: 0.032 | Tree loss: 6.197 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 030 | Total loss: 6.133 | Reg loss: 0.032 | Tree loss: 6.133 | Accuracy: 0.611328 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 030 | Total loss: 6.144 | Reg loss: 0.033 | Tree loss: 6.144 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 030 | Total loss: 6.100 | Reg loss: 0.033 | Tree loss: 6.100 | Accuracy: 0.544922 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 030 | Total loss: 6.077 | Reg loss: 0.033 | Tree loss: 6.077 | Accuracy: 0.544922 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 030 | Total loss: 6.043 | Reg loss: 0.034 | Tree loss: 6.043 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 030 | Total loss: 6.013 | Reg loss: 0.034 | Tree loss: 6.013 | Accuracy: 0.552734 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 030 | Total loss: 5.960 | Reg loss: 0.034 | Tree loss: 5.960 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 030 | Total loss: 5.911 | Reg loss: 0.035 | Tree loss: 5.911 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 030 | Total loss: 5.861 | Reg loss: 0.035 | Tree loss: 5.861 | Accuracy: 0.607422 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 030 | Total loss: 5.879 | Reg loss: 0.035 | Tree loss: 5.879 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 030 | Total loss: 5.807 | Reg loss: 0.036 | Tree loss: 5.807 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 030 | Total loss: 5.789 | Reg loss: 0.036 | Tree loss: 5.789 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 030 | Total loss: 5.775 | Reg loss: 0.036 | Tree loss: 5.775 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 030 | Total loss: 5.722 | Reg loss: 0.037 | Tree loss: 5.722 | Accuracy: 0.593750 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 023 / 030 | Total loss: 5.668 | Reg loss: 0.037 | Tree loss: 5.668 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 030 | Total loss: 5.680 | Reg loss: 0.038 | Tree loss: 5.680 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 030 | Total loss: 5.627 | Reg loss: 0.038 | Tree loss: 5.627 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 030 | Total loss: 5.610 | Reg loss: 0.038 | Tree loss: 5.610 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 030 | Total loss: 5.583 | Reg loss: 0.039 | Tree loss: 5.583 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 030 | Total loss: 5.580 | Reg loss: 0.039 | Tree loss: 5.580 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 16 | Batch: 029 / 030 | Total loss: 5.432 | Reg loss: 0.039 | Tree loss: 5.432 | Accuracy: 0.611111 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 17 | Batch: 000 / 030 | Total loss: 6.238 | Reg loss: 0.032 | Tree loss: 6.238 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 030 | Total loss: 6.220 | Reg loss: 0.032 | Tree loss: 6.220 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 030 | Total loss: 6.209 | Reg loss: 0.032 | Tree loss: 6.209 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 030 | Total loss: 6.175 | Reg loss: 0.032 | Tree loss: 6.175 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 030 | Total loss: 6.126 | Reg loss: 0.032 | Tree loss: 6.126 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 030 | Total loss: 6.090 | Reg loss: 0.032 | Tree loss: 6.090 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 030 | Total loss: 6.060 | Reg loss: 0.033 | Tree loss: 6.060 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 030 | Total loss: 6.032 | Reg loss: 0.033 | Tree loss: 6.032 | Accuracy: 0.541016 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 030 | Total loss: 5.962 | Reg loss: 0.033 | Tree loss: 5.962 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 030 | Total loss: 5.968 | Reg loss: 0.033 | Tree loss: 5.968 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 030 | Total loss: 5.925 | Reg loss: 0.033 | Tree loss: 5.925 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 030 | Total loss: 5.849 | Reg loss: 0.034 | Tree loss: 5.849 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 030 | Total loss: 5.839 | Reg loss: 0.034 | Tree loss: 5.839 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 030 | Total loss: 5.795 | Reg loss: 0.034 | Tree loss: 5.795 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 030 | Total loss: 5.738 | Reg loss: 0.035 | Tree loss: 5.738 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 030 | Total loss: 5.743 | Reg loss: 0.035 | Tree loss: 5.743 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 030 | Total loss: 5.715 | Reg loss: 0.035 | Tree loss: 5.715 | Accuracy: 0.531250 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 030 | Total loss: 5.614 | Reg loss: 0.036 | Tree loss: 5.614 | Accuracy: 0.615234 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 030 | Total loss: 5.652 | Reg loss: 0.036 | Tree loss: 5.652 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 030 | Total loss: 5.599 | Reg loss: 0.036 | Tree loss: 5.599 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 030 | Total loss: 5.536 | Reg loss: 0.037 | Tree loss: 5.536 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 030 | Total loss: 5.556 | Reg loss: 0.037 | Tree loss: 5.556 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 030 | Total loss: 5.497 | Reg loss: 0.038 | Tree loss: 5.497 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 030 | Total loss: 5.431 | Reg loss: 0.038 | Tree loss: 5.431 | Accuracy: 0.625000 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 030 | Total loss: 5.446 | Reg loss: 0.038 | Tree loss: 5.446 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 030 | Total loss: 5.368 | Reg loss: 0.039 | Tree loss: 5.368 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 030 | Total loss: 5.379 | Reg loss: 0.039 | Tree loss: 5.379 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 030 | Total loss: 5.370 | Reg loss: 0.039 | Tree loss: 5.370 | Accuracy: 0.529297 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 030 | Total loss: 5.266 | Reg loss: 0.040 | Tree loss: 5.266 | Accuracy: 0.603516 | 0.36 sec/iter\n",
      "Epoch: 17 | Batch: 029 / 030 | Total loss: 5.310 | Reg loss: 0.040 | Tree loss: 5.310 | Accuracy: 0.574074 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 18 | Batch: 000 / 030 | Total loss: 6.042 | Reg loss: 0.033 | Tree loss: 6.042 | Accuracy: 0.599609 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 030 | Total loss: 5.997 | Reg loss: 0.033 | Tree loss: 5.997 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 030 | Total loss: 5.970 | Reg loss: 0.033 | Tree loss: 5.970 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 030 | Total loss: 5.959 | Reg loss: 0.033 | Tree loss: 5.959 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 030 | Total loss: 5.923 | Reg loss: 0.033 | Tree loss: 5.923 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 030 | Total loss: 5.895 | Reg loss: 0.033 | Tree loss: 5.895 | Accuracy: 0.546875 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 030 | Total loss: 5.849 | Reg loss: 0.033 | Tree loss: 5.849 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 030 | Total loss: 5.814 | Reg loss: 0.033 | Tree loss: 5.814 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 030 | Total loss: 5.750 | Reg loss: 0.034 | Tree loss: 5.750 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 030 | Total loss: 5.725 | Reg loss: 0.034 | Tree loss: 5.725 | Accuracy: 0.607422 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 030 | Total loss: 5.702 | Reg loss: 0.034 | Tree loss: 5.702 | Accuracy: 0.542969 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 030 | Total loss: 5.676 | Reg loss: 0.034 | Tree loss: 5.676 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 030 | Total loss: 5.638 | Reg loss: 0.035 | Tree loss: 5.638 | Accuracy: 0.542969 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 030 | Total loss: 5.585 | Reg loss: 0.035 | Tree loss: 5.585 | Accuracy: 0.597656 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 030 | Total loss: 5.553 | Reg loss: 0.035 | Tree loss: 5.553 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 030 | Total loss: 5.514 | Reg loss: 0.036 | Tree loss: 5.514 | Accuracy: 0.539062 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 030 | Total loss: 5.472 | Reg loss: 0.036 | Tree loss: 5.472 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 030 | Total loss: 5.407 | Reg loss: 0.036 | Tree loss: 5.407 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 030 | Total loss: 5.391 | Reg loss: 0.037 | Tree loss: 5.391 | Accuracy: 0.623047 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 030 | Total loss: 5.296 | Reg loss: 0.037 | Tree loss: 5.296 | Accuracy: 0.638672 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 030 | Total loss: 5.328 | Reg loss: 0.038 | Tree loss: 5.328 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 030 | Total loss: 5.303 | Reg loss: 0.038 | Tree loss: 5.303 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 030 | Total loss: 5.257 | Reg loss: 0.038 | Tree loss: 5.257 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 030 | Total loss: 5.250 | Reg loss: 0.039 | Tree loss: 5.250 | Accuracy: 0.542969 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 030 | Total loss: 5.181 | Reg loss: 0.039 | Tree loss: 5.181 | Accuracy: 0.595703 | 0.362 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 030 | Total loss: 5.150 | Reg loss: 0.040 | Tree loss: 5.150 | Accuracy: 0.582031 | 0.362 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 026 / 030 | Total loss: 5.164 | Reg loss: 0.040 | Tree loss: 5.164 | Accuracy: 0.542969 | 0.361 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 030 | Total loss: 5.097 | Reg loss: 0.040 | Tree loss: 5.097 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 030 | Total loss: 5.013 | Reg loss: 0.041 | Tree loss: 5.013 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 18 | Batch: 029 / 030 | Total loss: 5.049 | Reg loss: 0.041 | Tree loss: 5.049 | Accuracy: 0.509259 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 19 | Batch: 000 / 030 | Total loss: 5.853 | Reg loss: 0.034 | Tree loss: 5.853 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 030 | Total loss: 5.809 | Reg loss: 0.034 | Tree loss: 5.809 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 030 | Total loss: 5.794 | Reg loss: 0.034 | Tree loss: 5.794 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 030 | Total loss: 5.746 | Reg loss: 0.034 | Tree loss: 5.746 | Accuracy: 0.595703 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 030 | Total loss: 5.708 | Reg loss: 0.034 | Tree loss: 5.708 | Accuracy: 0.552734 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 030 | Total loss: 5.651 | Reg loss: 0.034 | Tree loss: 5.651 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 030 | Total loss: 5.620 | Reg loss: 0.034 | Tree loss: 5.620 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 030 | Total loss: 5.566 | Reg loss: 0.034 | Tree loss: 5.566 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 030 | Total loss: 5.557 | Reg loss: 0.035 | Tree loss: 5.557 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 030 | Total loss: 5.528 | Reg loss: 0.035 | Tree loss: 5.528 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 030 | Total loss: 5.500 | Reg loss: 0.035 | Tree loss: 5.500 | Accuracy: 0.521484 | 0.361 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 030 | Total loss: 5.462 | Reg loss: 0.035 | Tree loss: 5.462 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 030 | Total loss: 5.387 | Reg loss: 0.036 | Tree loss: 5.387 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 030 | Total loss: 5.342 | Reg loss: 0.036 | Tree loss: 5.342 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 030 | Total loss: 5.340 | Reg loss: 0.036 | Tree loss: 5.340 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 030 | Total loss: 5.260 | Reg loss: 0.037 | Tree loss: 5.260 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 030 | Total loss: 5.295 | Reg loss: 0.037 | Tree loss: 5.295 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 030 | Total loss: 5.202 | Reg loss: 0.038 | Tree loss: 5.202 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 030 | Total loss: 5.148 | Reg loss: 0.038 | Tree loss: 5.148 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 030 | Total loss: 5.131 | Reg loss: 0.038 | Tree loss: 5.131 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 030 | Total loss: 5.132 | Reg loss: 0.039 | Tree loss: 5.132 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 030 | Total loss: 5.001 | Reg loss: 0.039 | Tree loss: 5.001 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 030 | Total loss: 4.997 | Reg loss: 0.040 | Tree loss: 4.997 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 030 | Total loss: 5.001 | Reg loss: 0.040 | Tree loss: 5.001 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 030 | Total loss: 4.992 | Reg loss: 0.040 | Tree loss: 4.992 | Accuracy: 0.556641 | 0.359 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 030 | Total loss: 4.925 | Reg loss: 0.041 | Tree loss: 4.925 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 030 | Total loss: 4.875 | Reg loss: 0.041 | Tree loss: 4.875 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 030 | Total loss: 4.806 | Reg loss: 0.042 | Tree loss: 4.806 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 030 | Total loss: 4.832 | Reg loss: 0.042 | Tree loss: 4.832 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 19 | Batch: 029 / 030 | Total loss: 4.896 | Reg loss: 0.042 | Tree loss: 4.896 | Accuracy: 0.518519 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 20 | Batch: 000 / 030 | Total loss: 5.596 | Reg loss: 0.035 | Tree loss: 5.596 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 030 | Total loss: 5.616 | Reg loss: 0.035 | Tree loss: 5.616 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 030 | Total loss: 5.545 | Reg loss: 0.035 | Tree loss: 5.545 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 030 | Total loss: 5.555 | Reg loss: 0.035 | Tree loss: 5.555 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 030 | Total loss: 5.490 | Reg loss: 0.035 | Tree loss: 5.490 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 030 | Total loss: 5.434 | Reg loss: 0.035 | Tree loss: 5.434 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 030 | Total loss: 5.437 | Reg loss: 0.035 | Tree loss: 5.437 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 030 | Total loss: 5.353 | Reg loss: 0.036 | Tree loss: 5.353 | Accuracy: 0.617188 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 030 | Total loss: 5.374 | Reg loss: 0.036 | Tree loss: 5.374 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 030 | Total loss: 5.321 | Reg loss: 0.036 | Tree loss: 5.321 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 030 | Total loss: 5.247 | Reg loss: 0.036 | Tree loss: 5.247 | Accuracy: 0.605469 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 030 | Total loss: 5.218 | Reg loss: 0.037 | Tree loss: 5.218 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 030 | Total loss: 5.224 | Reg loss: 0.037 | Tree loss: 5.224 | Accuracy: 0.541016 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 030 | Total loss: 5.206 | Reg loss: 0.037 | Tree loss: 5.206 | Accuracy: 0.527344 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 030 | Total loss: 5.102 | Reg loss: 0.038 | Tree loss: 5.102 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 030 | Total loss: 5.066 | Reg loss: 0.038 | Tree loss: 5.066 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 030 | Total loss: 4.975 | Reg loss: 0.038 | Tree loss: 4.975 | Accuracy: 0.613281 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 030 | Total loss: 5.001 | Reg loss: 0.039 | Tree loss: 5.001 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 030 | Total loss: 4.970 | Reg loss: 0.039 | Tree loss: 4.970 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 030 | Total loss: 4.909 | Reg loss: 0.040 | Tree loss: 4.909 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 030 | Total loss: 4.810 | Reg loss: 0.040 | Tree loss: 4.810 | Accuracy: 0.630859 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 030 | Total loss: 4.846 | Reg loss: 0.040 | Tree loss: 4.846 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 030 | Total loss: 4.854 | Reg loss: 0.041 | Tree loss: 4.854 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 030 | Total loss: 4.753 | Reg loss: 0.041 | Tree loss: 4.753 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 030 | Total loss: 4.772 | Reg loss: 0.042 | Tree loss: 4.772 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 030 | Total loss: 4.661 | Reg loss: 0.042 | Tree loss: 4.661 | Accuracy: 0.632812 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 030 | Total loss: 4.719 | Reg loss: 0.042 | Tree loss: 4.719 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 030 | Total loss: 4.639 | Reg loss: 0.043 | Tree loss: 4.639 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 030 | Total loss: 4.564 | Reg loss: 0.043 | Tree loss: 4.564 | Accuracy: 0.597656 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 029 / 030 | Total loss: 4.683 | Reg loss: 0.044 | Tree loss: 4.683 | Accuracy: 0.518519 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 21 | Batch: 000 / 030 | Total loss: 5.437 | Reg loss: 0.036 | Tree loss: 5.437 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 030 | Total loss: 5.452 | Reg loss: 0.036 | Tree loss: 5.452 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 030 | Total loss: 5.372 | Reg loss: 0.036 | Tree loss: 5.372 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 030 | Total loss: 5.335 | Reg loss: 0.036 | Tree loss: 5.335 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 030 | Total loss: 5.274 | Reg loss: 0.036 | Tree loss: 5.274 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 030 | Total loss: 5.253 | Reg loss: 0.037 | Tree loss: 5.253 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 030 | Total loss: 5.227 | Reg loss: 0.037 | Tree loss: 5.227 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 030 | Total loss: 5.166 | Reg loss: 0.037 | Tree loss: 5.166 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 030 | Total loss: 5.207 | Reg loss: 0.037 | Tree loss: 5.207 | Accuracy: 0.529297 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 030 | Total loss: 5.069 | Reg loss: 0.037 | Tree loss: 5.069 | Accuracy: 0.605469 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 030 | Total loss: 5.052 | Reg loss: 0.038 | Tree loss: 5.052 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 030 | Total loss: 5.013 | Reg loss: 0.038 | Tree loss: 5.013 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 030 | Total loss: 5.010 | Reg loss: 0.038 | Tree loss: 5.010 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 030 | Total loss: 4.905 | Reg loss: 0.039 | Tree loss: 4.905 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 030 | Total loss: 4.917 | Reg loss: 0.039 | Tree loss: 4.917 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 030 | Total loss: 4.840 | Reg loss: 0.039 | Tree loss: 4.840 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 030 | Total loss: 4.836 | Reg loss: 0.040 | Tree loss: 4.836 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 030 | Total loss: 4.764 | Reg loss: 0.040 | Tree loss: 4.764 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 030 | Total loss: 4.742 | Reg loss: 0.040 | Tree loss: 4.742 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 030 | Total loss: 4.721 | Reg loss: 0.041 | Tree loss: 4.721 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 030 | Total loss: 4.680 | Reg loss: 0.041 | Tree loss: 4.680 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 030 | Total loss: 4.668 | Reg loss: 0.042 | Tree loss: 4.668 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 030 | Total loss: 4.588 | Reg loss: 0.042 | Tree loss: 4.588 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 030 | Total loss: 4.552 | Reg loss: 0.042 | Tree loss: 4.552 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 030 | Total loss: 4.511 | Reg loss: 0.043 | Tree loss: 4.511 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 030 | Total loss: 4.486 | Reg loss: 0.043 | Tree loss: 4.486 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 030 | Total loss: 4.452 | Reg loss: 0.043 | Tree loss: 4.452 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 030 | Total loss: 4.446 | Reg loss: 0.044 | Tree loss: 4.446 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 030 | Total loss: 4.352 | Reg loss: 0.044 | Tree loss: 4.352 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 21 | Batch: 029 / 030 | Total loss: 4.276 | Reg loss: 0.045 | Tree loss: 4.276 | Accuracy: 0.611111 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 22 | Batch: 000 / 030 | Total loss: 5.182 | Reg loss: 0.038 | Tree loss: 5.182 | Accuracy: 0.613281 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 030 | Total loss: 5.156 | Reg loss: 0.038 | Tree loss: 5.156 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 030 | Total loss: 5.133 | Reg loss: 0.038 | Tree loss: 5.133 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 030 | Total loss: 5.087 | Reg loss: 0.038 | Tree loss: 5.087 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 030 | Total loss: 5.043 | Reg loss: 0.038 | Tree loss: 5.043 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 030 | Total loss: 5.011 | Reg loss: 0.038 | Tree loss: 5.011 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 030 | Total loss: 5.001 | Reg loss: 0.038 | Tree loss: 5.001 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 030 | Total loss: 4.898 | Reg loss: 0.038 | Tree loss: 4.898 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 030 | Total loss: 4.916 | Reg loss: 0.039 | Tree loss: 4.916 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 030 | Total loss: 4.824 | Reg loss: 0.039 | Tree loss: 4.824 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 030 | Total loss: 4.825 | Reg loss: 0.039 | Tree loss: 4.825 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 030 | Total loss: 4.734 | Reg loss: 0.039 | Tree loss: 4.734 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 030 | Total loss: 4.702 | Reg loss: 0.040 | Tree loss: 4.702 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 030 | Total loss: 4.680 | Reg loss: 0.040 | Tree loss: 4.680 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 030 | Total loss: 4.573 | Reg loss: 0.040 | Tree loss: 4.573 | Accuracy: 0.619141 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 030 | Total loss: 4.574 | Reg loss: 0.041 | Tree loss: 4.574 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 030 | Total loss: 4.528 | Reg loss: 0.041 | Tree loss: 4.528 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 030 | Total loss: 4.498 | Reg loss: 0.041 | Tree loss: 4.498 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 030 | Total loss: 4.488 | Reg loss: 0.042 | Tree loss: 4.488 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 030 | Total loss: 4.405 | Reg loss: 0.042 | Tree loss: 4.405 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 030 | Total loss: 4.335 | Reg loss: 0.042 | Tree loss: 4.335 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 030 | Total loss: 4.394 | Reg loss: 0.043 | Tree loss: 4.394 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 030 | Total loss: 4.289 | Reg loss: 0.043 | Tree loss: 4.289 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 030 | Total loss: 4.264 | Reg loss: 0.043 | Tree loss: 4.264 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 030 | Total loss: 4.236 | Reg loss: 0.044 | Tree loss: 4.236 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 030 | Total loss: 4.227 | Reg loss: 0.044 | Tree loss: 4.227 | Accuracy: 0.544922 | 0.358 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 030 | Total loss: 4.108 | Reg loss: 0.044 | Tree loss: 4.108 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 030 | Total loss: 4.128 | Reg loss: 0.045 | Tree loss: 4.128 | Accuracy: 0.562500 | 0.358 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 030 | Total loss: 4.033 | Reg loss: 0.045 | Tree loss: 4.033 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 22 | Batch: 029 / 030 | Total loss: 4.110 | Reg loss: 0.045 | Tree loss: 4.110 | Accuracy: 0.564815 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 000 / 030 | Total loss: 4.918 | Reg loss: 0.039 | Tree loss: 4.918 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 030 | Total loss: 4.899 | Reg loss: 0.039 | Tree loss: 4.899 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 030 | Total loss: 4.910 | Reg loss: 0.039 | Tree loss: 4.910 | Accuracy: 0.560547 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 030 | Total loss: 4.789 | Reg loss: 0.039 | Tree loss: 4.789 | Accuracy: 0.587891 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 030 | Total loss: 4.780 | Reg loss: 0.039 | Tree loss: 4.780 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 030 | Total loss: 4.760 | Reg loss: 0.039 | Tree loss: 4.760 | Accuracy: 0.593750 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 030 | Total loss: 4.729 | Reg loss: 0.039 | Tree loss: 4.729 | Accuracy: 0.542969 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 030 | Total loss: 4.625 | Reg loss: 0.039 | Tree loss: 4.625 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 030 | Total loss: 4.597 | Reg loss: 0.040 | Tree loss: 4.597 | Accuracy: 0.587891 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 030 | Total loss: 4.560 | Reg loss: 0.040 | Tree loss: 4.560 | Accuracy: 0.599609 | 0.358 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 030 | Total loss: 4.471 | Reg loss: 0.040 | Tree loss: 4.471 | Accuracy: 0.595703 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 030 | Total loss: 4.481 | Reg loss: 0.040 | Tree loss: 4.481 | Accuracy: 0.572266 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 030 | Total loss: 4.445 | Reg loss: 0.041 | Tree loss: 4.445 | Accuracy: 0.560547 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 030 | Total loss: 4.397 | Reg loss: 0.041 | Tree loss: 4.397 | Accuracy: 0.572266 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 030 | Total loss: 4.320 | Reg loss: 0.041 | Tree loss: 4.320 | Accuracy: 0.582031 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 030 | Total loss: 4.337 | Reg loss: 0.041 | Tree loss: 4.337 | Accuracy: 0.562500 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 030 | Total loss: 4.266 | Reg loss: 0.042 | Tree loss: 4.266 | Accuracy: 0.576172 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 030 | Total loss: 4.203 | Reg loss: 0.042 | Tree loss: 4.203 | Accuracy: 0.580078 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 030 | Total loss: 4.189 | Reg loss: 0.042 | Tree loss: 4.189 | Accuracy: 0.580078 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 030 | Total loss: 4.146 | Reg loss: 0.043 | Tree loss: 4.146 | Accuracy: 0.568359 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 030 | Total loss: 4.103 | Reg loss: 0.043 | Tree loss: 4.103 | Accuracy: 0.566406 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 030 | Total loss: 4.053 | Reg loss: 0.043 | Tree loss: 4.053 | Accuracy: 0.580078 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 030 | Total loss: 4.022 | Reg loss: 0.043 | Tree loss: 4.022 | Accuracy: 0.578125 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 030 | Total loss: 3.980 | Reg loss: 0.044 | Tree loss: 3.980 | Accuracy: 0.570312 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 030 | Total loss: 3.905 | Reg loss: 0.044 | Tree loss: 3.905 | Accuracy: 0.585938 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 030 | Total loss: 3.923 | Reg loss: 0.044 | Tree loss: 3.923 | Accuracy: 0.580078 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 030 | Total loss: 3.882 | Reg loss: 0.045 | Tree loss: 3.882 | Accuracy: 0.564453 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 030 | Total loss: 3.895 | Reg loss: 0.045 | Tree loss: 3.895 | Accuracy: 0.556641 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 030 | Total loss: 3.786 | Reg loss: 0.045 | Tree loss: 3.786 | Accuracy: 0.587891 | 0.357 sec/iter\n",
      "Epoch: 23 | Batch: 029 / 030 | Total loss: 3.912 | Reg loss: 0.046 | Tree loss: 3.912 | Accuracy: 0.509259 | 0.357 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 24 | Batch: 000 / 030 | Total loss: 4.723 | Reg loss: 0.040 | Tree loss: 4.723 | Accuracy: 0.560547 | 0.358 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 030 | Total loss: 4.668 | Reg loss: 0.040 | Tree loss: 4.668 | Accuracy: 0.572266 | 0.358 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 030 | Total loss: 4.621 | Reg loss: 0.040 | Tree loss: 4.621 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 030 | Total loss: 4.539 | Reg loss: 0.040 | Tree loss: 4.539 | Accuracy: 0.619141 | 0.358 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 030 | Total loss: 4.543 | Reg loss: 0.040 | Tree loss: 4.543 | Accuracy: 0.541016 | 0.358 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 030 | Total loss: 4.440 | Reg loss: 0.040 | Tree loss: 4.440 | Accuracy: 0.587891 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 030 | Total loss: 4.507 | Reg loss: 0.040 | Tree loss: 4.507 | Accuracy: 0.566406 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 030 | Total loss: 4.378 | Reg loss: 0.040 | Tree loss: 4.378 | Accuracy: 0.560547 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 030 | Total loss: 4.288 | Reg loss: 0.040 | Tree loss: 4.288 | Accuracy: 0.589844 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 030 | Total loss: 4.247 | Reg loss: 0.041 | Tree loss: 4.247 | Accuracy: 0.613281 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 030 | Total loss: 4.262 | Reg loss: 0.041 | Tree loss: 4.262 | Accuracy: 0.597656 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 030 | Total loss: 4.225 | Reg loss: 0.041 | Tree loss: 4.225 | Accuracy: 0.558594 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 030 | Total loss: 4.147 | Reg loss: 0.041 | Tree loss: 4.147 | Accuracy: 0.591797 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 030 | Total loss: 4.052 | Reg loss: 0.041 | Tree loss: 4.052 | Accuracy: 0.593750 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 030 | Total loss: 4.105 | Reg loss: 0.042 | Tree loss: 4.105 | Accuracy: 0.550781 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 030 | Total loss: 4.052 | Reg loss: 0.042 | Tree loss: 4.052 | Accuracy: 0.574219 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 030 | Total loss: 3.983 | Reg loss: 0.042 | Tree loss: 3.983 | Accuracy: 0.589844 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 030 | Total loss: 3.975 | Reg loss: 0.042 | Tree loss: 3.975 | Accuracy: 0.552734 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 030 | Total loss: 3.871 | Reg loss: 0.043 | Tree loss: 3.871 | Accuracy: 0.574219 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 030 | Total loss: 3.897 | Reg loss: 0.043 | Tree loss: 3.897 | Accuracy: 0.556641 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 030 | Total loss: 3.831 | Reg loss: 0.043 | Tree loss: 3.831 | Accuracy: 0.564453 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 030 | Total loss: 3.787 | Reg loss: 0.044 | Tree loss: 3.787 | Accuracy: 0.572266 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 030 | Total loss: 3.757 | Reg loss: 0.044 | Tree loss: 3.757 | Accuracy: 0.595703 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 030 | Total loss: 3.697 | Reg loss: 0.044 | Tree loss: 3.697 | Accuracy: 0.572266 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 030 | Total loss: 3.655 | Reg loss: 0.044 | Tree loss: 3.655 | Accuracy: 0.578125 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 030 | Total loss: 3.671 | Reg loss: 0.045 | Tree loss: 3.671 | Accuracy: 0.552734 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 030 | Total loss: 3.613 | Reg loss: 0.045 | Tree loss: 3.613 | Accuracy: 0.572266 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 030 | Total loss: 3.612 | Reg loss: 0.045 | Tree loss: 3.612 | Accuracy: 0.550781 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 030 | Total loss: 3.491 | Reg loss: 0.045 | Tree loss: 3.491 | Accuracy: 0.617188 | 0.357 sec/iter\n",
      "Epoch: 24 | Batch: 029 / 030 | Total loss: 3.472 | Reg loss: 0.046 | Tree loss: 3.472 | Accuracy: 0.611111 | 0.357 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 25 | Batch: 000 / 030 | Total loss: 4.395 | Reg loss: 0.040 | Tree loss: 4.395 | Accuracy: 0.613281 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 030 | Total loss: 4.398 | Reg loss: 0.040 | Tree loss: 4.398 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 030 | Total loss: 4.365 | Reg loss: 0.040 | Tree loss: 4.365 | Accuracy: 0.580078 | 0.358 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 003 / 030 | Total loss: 4.357 | Reg loss: 0.040 | Tree loss: 4.357 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 030 | Total loss: 4.244 | Reg loss: 0.041 | Tree loss: 4.244 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 030 | Total loss: 4.178 | Reg loss: 0.041 | Tree loss: 4.178 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 030 | Total loss: 4.171 | Reg loss: 0.041 | Tree loss: 4.171 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 030 | Total loss: 4.137 | Reg loss: 0.041 | Tree loss: 4.137 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 030 | Total loss: 4.087 | Reg loss: 0.041 | Tree loss: 4.087 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 030 | Total loss: 4.063 | Reg loss: 0.041 | Tree loss: 4.063 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 030 | Total loss: 3.987 | Reg loss: 0.041 | Tree loss: 3.987 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 030 | Total loss: 3.935 | Reg loss: 0.041 | Tree loss: 3.935 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 030 | Total loss: 3.941 | Reg loss: 0.042 | Tree loss: 3.941 | Accuracy: 0.517578 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 030 | Total loss: 3.817 | Reg loss: 0.042 | Tree loss: 3.817 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 030 | Total loss: 3.767 | Reg loss: 0.042 | Tree loss: 3.767 | Accuracy: 0.613281 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 030 | Total loss: 3.734 | Reg loss: 0.042 | Tree loss: 3.734 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 030 | Total loss: 3.735 | Reg loss: 0.043 | Tree loss: 3.735 | Accuracy: 0.537109 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 030 | Total loss: 3.719 | Reg loss: 0.043 | Tree loss: 3.719 | Accuracy: 0.562500 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 030 | Total loss: 3.620 | Reg loss: 0.043 | Tree loss: 3.620 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 030 | Total loss: 3.546 | Reg loss: 0.043 | Tree loss: 3.546 | Accuracy: 0.603516 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 030 | Total loss: 3.516 | Reg loss: 0.044 | Tree loss: 3.516 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 030 | Total loss: 3.541 | Reg loss: 0.044 | Tree loss: 3.541 | Accuracy: 0.572266 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 030 | Total loss: 3.526 | Reg loss: 0.044 | Tree loss: 3.526 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 030 | Total loss: 3.484 | Reg loss: 0.044 | Tree loss: 3.484 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 030 | Total loss: 3.432 | Reg loss: 0.044 | Tree loss: 3.432 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 030 | Total loss: 3.385 | Reg loss: 0.045 | Tree loss: 3.385 | Accuracy: 0.587891 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 030 | Total loss: 3.338 | Reg loss: 0.045 | Tree loss: 3.338 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 030 | Total loss: 3.358 | Reg loss: 0.045 | Tree loss: 3.358 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 030 | Total loss: 3.304 | Reg loss: 0.045 | Tree loss: 3.304 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 25 | Batch: 029 / 030 | Total loss: 3.265 | Reg loss: 0.046 | Tree loss: 3.265 | Accuracy: 0.574074 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 26 | Batch: 000 / 030 | Total loss: 4.163 | Reg loss: 0.041 | Tree loss: 4.163 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 030 | Total loss: 4.210 | Reg loss: 0.041 | Tree loss: 4.210 | Accuracy: 0.539062 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 030 | Total loss: 4.036 | Reg loss: 0.041 | Tree loss: 4.036 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 030 | Total loss: 4.034 | Reg loss: 0.041 | Tree loss: 4.034 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 030 | Total loss: 4.006 | Reg loss: 0.041 | Tree loss: 4.006 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 030 | Total loss: 3.926 | Reg loss: 0.041 | Tree loss: 3.926 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 030 | Total loss: 3.923 | Reg loss: 0.041 | Tree loss: 3.923 | Accuracy: 0.562500 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 030 | Total loss: 3.850 | Reg loss: 0.041 | Tree loss: 3.850 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 030 | Total loss: 3.819 | Reg loss: 0.041 | Tree loss: 3.819 | Accuracy: 0.593750 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 030 | Total loss: 3.748 | Reg loss: 0.041 | Tree loss: 3.748 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 030 | Total loss: 3.680 | Reg loss: 0.042 | Tree loss: 3.680 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 030 | Total loss: 3.678 | Reg loss: 0.042 | Tree loss: 3.678 | Accuracy: 0.593750 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 030 | Total loss: 3.702 | Reg loss: 0.042 | Tree loss: 3.702 | Accuracy: 0.535156 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 030 | Total loss: 3.535 | Reg loss: 0.042 | Tree loss: 3.535 | Accuracy: 0.605469 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 030 | Total loss: 3.577 | Reg loss: 0.042 | Tree loss: 3.577 | Accuracy: 0.566406 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 030 | Total loss: 3.548 | Reg loss: 0.043 | Tree loss: 3.548 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 030 | Total loss: 3.417 | Reg loss: 0.043 | Tree loss: 3.417 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 030 | Total loss: 3.493 | Reg loss: 0.043 | Tree loss: 3.493 | Accuracy: 0.533203 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 030 | Total loss: 3.353 | Reg loss: 0.043 | Tree loss: 3.353 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 030 | Total loss: 3.342 | Reg loss: 0.043 | Tree loss: 3.342 | Accuracy: 0.599609 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 030 | Total loss: 3.348 | Reg loss: 0.044 | Tree loss: 3.348 | Accuracy: 0.562500 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 030 | Total loss: 3.308 | Reg loss: 0.044 | Tree loss: 3.308 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 030 | Total loss: 3.257 | Reg loss: 0.044 | Tree loss: 3.257 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 030 | Total loss: 3.316 | Reg loss: 0.044 | Tree loss: 3.316 | Accuracy: 0.523438 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 030 | Total loss: 3.235 | Reg loss: 0.044 | Tree loss: 3.235 | Accuracy: 0.550781 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 030 | Total loss: 3.088 | Reg loss: 0.045 | Tree loss: 3.088 | Accuracy: 0.611328 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 030 | Total loss: 3.082 | Reg loss: 0.045 | Tree loss: 3.082 | Accuracy: 0.613281 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 030 | Total loss: 3.134 | Reg loss: 0.045 | Tree loss: 3.134 | Accuracy: 0.566406 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 030 | Total loss: 3.083 | Reg loss: 0.045 | Tree loss: 3.083 | Accuracy: 0.566406 | 0.358 sec/iter\n",
      "Epoch: 26 | Batch: 029 / 030 | Total loss: 2.909 | Reg loss: 0.045 | Tree loss: 2.909 | Accuracy: 0.638889 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 27 | Batch: 000 / 030 | Total loss: 3.920 | Reg loss: 0.041 | Tree loss: 3.920 | Accuracy: 0.544922 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 030 | Total loss: 3.875 | Reg loss: 0.041 | Tree loss: 3.875 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 030 | Total loss: 3.826 | Reg loss: 0.041 | Tree loss: 3.826 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 030 | Total loss: 3.826 | Reg loss: 0.041 | Tree loss: 3.826 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 030 | Total loss: 3.746 | Reg loss: 0.041 | Tree loss: 3.746 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 030 | Total loss: 3.675 | Reg loss: 0.041 | Tree loss: 3.675 | Accuracy: 0.583984 | 0.358 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 006 / 030 | Total loss: 3.629 | Reg loss: 0.041 | Tree loss: 3.629 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 030 | Total loss: 3.616 | Reg loss: 0.041 | Tree loss: 3.616 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 030 | Total loss: 3.573 | Reg loss: 0.042 | Tree loss: 3.573 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 030 | Total loss: 3.518 | Reg loss: 0.042 | Tree loss: 3.518 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 030 | Total loss: 3.494 | Reg loss: 0.042 | Tree loss: 3.494 | Accuracy: 0.546875 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 030 | Total loss: 3.467 | Reg loss: 0.042 | Tree loss: 3.467 | Accuracy: 0.552734 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 030 | Total loss: 3.419 | Reg loss: 0.042 | Tree loss: 3.419 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 030 | Total loss: 3.336 | Reg loss: 0.042 | Tree loss: 3.336 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 030 | Total loss: 3.249 | Reg loss: 0.042 | Tree loss: 3.249 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 030 | Total loss: 3.300 | Reg loss: 0.043 | Tree loss: 3.300 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 030 | Total loss: 3.194 | Reg loss: 0.043 | Tree loss: 3.194 | Accuracy: 0.611328 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 030 | Total loss: 3.227 | Reg loss: 0.043 | Tree loss: 3.227 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 030 | Total loss: 3.200 | Reg loss: 0.043 | Tree loss: 3.200 | Accuracy: 0.552734 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 030 | Total loss: 3.120 | Reg loss: 0.043 | Tree loss: 3.120 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 030 | Total loss: 3.039 | Reg loss: 0.044 | Tree loss: 3.039 | Accuracy: 0.611328 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 030 | Total loss: 3.005 | Reg loss: 0.044 | Tree loss: 3.005 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 030 | Total loss: 3.001 | Reg loss: 0.044 | Tree loss: 3.001 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 030 | Total loss: 2.992 | Reg loss: 0.044 | Tree loss: 2.992 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 030 | Total loss: 3.022 | Reg loss: 0.044 | Tree loss: 3.022 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 030 | Total loss: 3.003 | Reg loss: 0.045 | Tree loss: 3.003 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 030 | Total loss: 2.914 | Reg loss: 0.045 | Tree loss: 2.914 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 030 | Total loss: 2.960 | Reg loss: 0.045 | Tree loss: 2.960 | Accuracy: 0.550781 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 030 | Total loss: 2.891 | Reg loss: 0.045 | Tree loss: 2.891 | Accuracy: 0.548828 | 0.358 sec/iter\n",
      "Epoch: 27 | Batch: 029 / 030 | Total loss: 2.810 | Reg loss: 0.045 | Tree loss: 2.810 | Accuracy: 0.564815 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 28 | Batch: 000 / 030 | Total loss: 3.633 | Reg loss: 0.041 | Tree loss: 3.633 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 030 | Total loss: 3.645 | Reg loss: 0.041 | Tree loss: 3.645 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 030 | Total loss: 3.541 | Reg loss: 0.041 | Tree loss: 3.541 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 030 | Total loss: 3.584 | Reg loss: 0.041 | Tree loss: 3.584 | Accuracy: 0.544922 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 030 | Total loss: 3.465 | Reg loss: 0.041 | Tree loss: 3.465 | Accuracy: 0.613281 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 030 | Total loss: 3.479 | Reg loss: 0.041 | Tree loss: 3.479 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 030 | Total loss: 3.468 | Reg loss: 0.042 | Tree loss: 3.468 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 030 | Total loss: 3.401 | Reg loss: 0.042 | Tree loss: 3.401 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 030 | Total loss: 3.332 | Reg loss: 0.042 | Tree loss: 3.332 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 030 | Total loss: 3.313 | Reg loss: 0.042 | Tree loss: 3.313 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 030 | Total loss: 3.208 | Reg loss: 0.042 | Tree loss: 3.208 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 030 | Total loss: 3.235 | Reg loss: 0.042 | Tree loss: 3.235 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 030 | Total loss: 3.170 | Reg loss: 0.042 | Tree loss: 3.170 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 030 | Total loss: 3.188 | Reg loss: 0.042 | Tree loss: 3.188 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 030 | Total loss: 3.124 | Reg loss: 0.043 | Tree loss: 3.124 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 030 | Total loss: 3.046 | Reg loss: 0.043 | Tree loss: 3.046 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 030 | Total loss: 3.048 | Reg loss: 0.043 | Tree loss: 3.048 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 030 | Total loss: 2.951 | Reg loss: 0.043 | Tree loss: 2.951 | Accuracy: 0.613281 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 030 | Total loss: 2.968 | Reg loss: 0.043 | Tree loss: 2.968 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 030 | Total loss: 2.918 | Reg loss: 0.043 | Tree loss: 2.918 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 030 | Total loss: 2.929 | Reg loss: 0.044 | Tree loss: 2.929 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 030 | Total loss: 2.871 | Reg loss: 0.044 | Tree loss: 2.871 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 030 | Total loss: 2.842 | Reg loss: 0.044 | Tree loss: 2.842 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 030 | Total loss: 2.806 | Reg loss: 0.044 | Tree loss: 2.806 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 030 | Total loss: 2.714 | Reg loss: 0.044 | Tree loss: 2.714 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 030 | Total loss: 2.780 | Reg loss: 0.044 | Tree loss: 2.780 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 030 | Total loss: 2.701 | Reg loss: 0.045 | Tree loss: 2.701 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 030 | Total loss: 2.678 | Reg loss: 0.045 | Tree loss: 2.678 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 030 | Total loss: 2.664 | Reg loss: 0.045 | Tree loss: 2.664 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 28 | Batch: 029 / 030 | Total loss: 2.655 | Reg loss: 0.045 | Tree loss: 2.655 | Accuracy: 0.546296 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 29 | Batch: 000 / 030 | Total loss: 3.489 | Reg loss: 0.041 | Tree loss: 3.489 | Accuracy: 0.531250 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 030 | Total loss: 3.411 | Reg loss: 0.041 | Tree loss: 3.411 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 030 | Total loss: 3.329 | Reg loss: 0.041 | Tree loss: 3.329 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 030 | Total loss: 3.301 | Reg loss: 0.041 | Tree loss: 3.301 | Accuracy: 0.609375 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 030 | Total loss: 3.273 | Reg loss: 0.041 | Tree loss: 3.273 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 030 | Total loss: 3.154 | Reg loss: 0.042 | Tree loss: 3.154 | Accuracy: 0.626953 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 030 | Total loss: 3.190 | Reg loss: 0.042 | Tree loss: 3.190 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 030 | Total loss: 3.137 | Reg loss: 0.042 | Tree loss: 3.137 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 030 | Total loss: 3.159 | Reg loss: 0.042 | Tree loss: 3.159 | Accuracy: 0.570312 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Batch: 009 / 030 | Total loss: 3.068 | Reg loss: 0.042 | Tree loss: 3.068 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 030 | Total loss: 3.081 | Reg loss: 0.042 | Tree loss: 3.081 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 030 | Total loss: 3.018 | Reg loss: 0.042 | Tree loss: 3.018 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 030 | Total loss: 2.999 | Reg loss: 0.042 | Tree loss: 2.999 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 030 | Total loss: 2.898 | Reg loss: 0.042 | Tree loss: 2.898 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 030 | Total loss: 2.936 | Reg loss: 0.042 | Tree loss: 2.936 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 030 | Total loss: 2.918 | Reg loss: 0.043 | Tree loss: 2.918 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 030 | Total loss: 2.915 | Reg loss: 0.043 | Tree loss: 2.915 | Accuracy: 0.505859 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 030 | Total loss: 2.785 | Reg loss: 0.043 | Tree loss: 2.785 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 030 | Total loss: 2.779 | Reg loss: 0.043 | Tree loss: 2.779 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 030 | Total loss: 2.743 | Reg loss: 0.043 | Tree loss: 2.743 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 030 | Total loss: 2.710 | Reg loss: 0.043 | Tree loss: 2.710 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 030 | Total loss: 2.671 | Reg loss: 0.044 | Tree loss: 2.671 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 030 | Total loss: 2.700 | Reg loss: 0.044 | Tree loss: 2.700 | Accuracy: 0.556641 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 030 | Total loss: 2.627 | Reg loss: 0.044 | Tree loss: 2.627 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 030 | Total loss: 2.550 | Reg loss: 0.044 | Tree loss: 2.550 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 030 | Total loss: 2.590 | Reg loss: 0.044 | Tree loss: 2.590 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 030 | Total loss: 2.487 | Reg loss: 0.044 | Tree loss: 2.487 | Accuracy: 0.621094 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 030 | Total loss: 2.526 | Reg loss: 0.044 | Tree loss: 2.526 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 030 | Total loss: 2.535 | Reg loss: 0.045 | Tree loss: 2.535 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 29 | Batch: 029 / 030 | Total loss: 2.553 | Reg loss: 0.045 | Tree loss: 2.553 | Accuracy: 0.527778 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 30 | Batch: 000 / 030 | Total loss: 3.179 | Reg loss: 0.041 | Tree loss: 3.179 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 030 | Total loss: 3.197 | Reg loss: 0.041 | Tree loss: 3.197 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 030 | Total loss: 3.209 | Reg loss: 0.041 | Tree loss: 3.209 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 030 | Total loss: 3.149 | Reg loss: 0.041 | Tree loss: 3.149 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 030 | Total loss: 3.092 | Reg loss: 0.041 | Tree loss: 3.092 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 030 | Total loss: 3.049 | Reg loss: 0.042 | Tree loss: 3.049 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 030 | Total loss: 3.001 | Reg loss: 0.042 | Tree loss: 3.001 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 030 | Total loss: 2.928 | Reg loss: 0.042 | Tree loss: 2.928 | Accuracy: 0.605469 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 030 | Total loss: 2.861 | Reg loss: 0.042 | Tree loss: 2.861 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 030 | Total loss: 3.007 | Reg loss: 0.042 | Tree loss: 3.007 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 030 | Total loss: 2.855 | Reg loss: 0.042 | Tree loss: 2.855 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 030 | Total loss: 2.841 | Reg loss: 0.042 | Tree loss: 2.841 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 030 | Total loss: 2.765 | Reg loss: 0.042 | Tree loss: 2.765 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 030 | Total loss: 2.757 | Reg loss: 0.042 | Tree loss: 2.757 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 030 | Total loss: 2.720 | Reg loss: 0.042 | Tree loss: 2.720 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 030 | Total loss: 2.766 | Reg loss: 0.042 | Tree loss: 2.766 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 030 | Total loss: 2.676 | Reg loss: 0.043 | Tree loss: 2.676 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 030 | Total loss: 2.652 | Reg loss: 0.043 | Tree loss: 2.652 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 030 | Total loss: 2.581 | Reg loss: 0.043 | Tree loss: 2.581 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 030 | Total loss: 2.581 | Reg loss: 0.043 | Tree loss: 2.581 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 030 | Total loss: 2.561 | Reg loss: 0.043 | Tree loss: 2.561 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 030 | Total loss: 2.514 | Reg loss: 0.043 | Tree loss: 2.514 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 030 | Total loss: 2.471 | Reg loss: 0.043 | Tree loss: 2.471 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 030 | Total loss: 2.454 | Reg loss: 0.044 | Tree loss: 2.454 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 030 | Total loss: 2.453 | Reg loss: 0.044 | Tree loss: 2.453 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 030 | Total loss: 2.428 | Reg loss: 0.044 | Tree loss: 2.428 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 030 | Total loss: 2.377 | Reg loss: 0.044 | Tree loss: 2.377 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 030 | Total loss: 2.307 | Reg loss: 0.044 | Tree loss: 2.307 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 030 | Total loss: 2.353 | Reg loss: 0.044 | Tree loss: 2.353 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 30 | Batch: 029 / 030 | Total loss: 2.397 | Reg loss: 0.044 | Tree loss: 2.397 | Accuracy: 0.537037 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 31 | Batch: 000 / 030 | Total loss: 3.061 | Reg loss: 0.041 | Tree loss: 3.061 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 030 | Total loss: 2.983 | Reg loss: 0.041 | Tree loss: 2.983 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 030 | Total loss: 2.993 | Reg loss: 0.041 | Tree loss: 2.993 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 030 | Total loss: 2.922 | Reg loss: 0.041 | Tree loss: 2.922 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 030 | Total loss: 2.903 | Reg loss: 0.041 | Tree loss: 2.903 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 030 | Total loss: 2.887 | Reg loss: 0.041 | Tree loss: 2.887 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 030 | Total loss: 2.951 | Reg loss: 0.041 | Tree loss: 2.951 | Accuracy: 0.523438 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 030 | Total loss: 2.752 | Reg loss: 0.042 | Tree loss: 2.752 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 030 | Total loss: 2.793 | Reg loss: 0.042 | Tree loss: 2.793 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 030 | Total loss: 2.754 | Reg loss: 0.042 | Tree loss: 2.754 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 030 | Total loss: 2.632 | Reg loss: 0.042 | Tree loss: 2.632 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 030 | Total loss: 2.685 | Reg loss: 0.042 | Tree loss: 2.685 | Accuracy: 0.554688 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 012 / 030 | Total loss: 2.653 | Reg loss: 0.042 | Tree loss: 2.653 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 030 | Total loss: 2.548 | Reg loss: 0.042 | Tree loss: 2.548 | Accuracy: 0.623047 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 030 | Total loss: 2.590 | Reg loss: 0.042 | Tree loss: 2.590 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 030 | Total loss: 2.540 | Reg loss: 0.042 | Tree loss: 2.540 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 030 | Total loss: 2.460 | Reg loss: 0.042 | Tree loss: 2.460 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 030 | Total loss: 2.477 | Reg loss: 0.042 | Tree loss: 2.477 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 030 | Total loss: 2.444 | Reg loss: 0.043 | Tree loss: 2.444 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 030 | Total loss: 2.445 | Reg loss: 0.043 | Tree loss: 2.445 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 030 | Total loss: 2.396 | Reg loss: 0.043 | Tree loss: 2.396 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 030 | Total loss: 2.315 | Reg loss: 0.043 | Tree loss: 2.315 | Accuracy: 0.603516 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 030 | Total loss: 2.329 | Reg loss: 0.043 | Tree loss: 2.329 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 030 | Total loss: 2.359 | Reg loss: 0.043 | Tree loss: 2.359 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 030 | Total loss: 2.304 | Reg loss: 0.043 | Tree loss: 2.304 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 030 | Total loss: 2.229 | Reg loss: 0.043 | Tree loss: 2.229 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 030 | Total loss: 2.277 | Reg loss: 0.044 | Tree loss: 2.277 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 030 | Total loss: 2.230 | Reg loss: 0.044 | Tree loss: 2.230 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 028 / 030 | Total loss: 2.168 | Reg loss: 0.044 | Tree loss: 2.168 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 31 | Batch: 029 / 030 | Total loss: 2.205 | Reg loss: 0.044 | Tree loss: 2.205 | Accuracy: 0.555556 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 32 | Batch: 000 / 030 | Total loss: 2.876 | Reg loss: 0.041 | Tree loss: 2.876 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 030 | Total loss: 2.817 | Reg loss: 0.041 | Tree loss: 2.817 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 030 | Total loss: 2.800 | Reg loss: 0.041 | Tree loss: 2.800 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 030 | Total loss: 2.766 | Reg loss: 0.041 | Tree loss: 2.766 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 030 | Total loss: 2.663 | Reg loss: 0.041 | Tree loss: 2.663 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 030 | Total loss: 2.692 | Reg loss: 0.041 | Tree loss: 2.692 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 030 | Total loss: 2.660 | Reg loss: 0.041 | Tree loss: 2.660 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 030 | Total loss: 2.662 | Reg loss: 0.041 | Tree loss: 2.662 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 030 | Total loss: 2.676 | Reg loss: 0.041 | Tree loss: 2.676 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 030 | Total loss: 2.575 | Reg loss: 0.041 | Tree loss: 2.575 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 030 | Total loss: 2.548 | Reg loss: 0.042 | Tree loss: 2.548 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 030 | Total loss: 2.514 | Reg loss: 0.042 | Tree loss: 2.514 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 030 | Total loss: 2.459 | Reg loss: 0.042 | Tree loss: 2.459 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 030 | Total loss: 2.416 | Reg loss: 0.042 | Tree loss: 2.416 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 030 | Total loss: 2.444 | Reg loss: 0.042 | Tree loss: 2.444 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 030 | Total loss: 2.343 | Reg loss: 0.042 | Tree loss: 2.343 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 030 | Total loss: 2.379 | Reg loss: 0.042 | Tree loss: 2.379 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 030 | Total loss: 2.320 | Reg loss: 0.042 | Tree loss: 2.320 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 030 | Total loss: 2.321 | Reg loss: 0.042 | Tree loss: 2.321 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 030 | Total loss: 2.278 | Reg loss: 0.042 | Tree loss: 2.278 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 030 | Total loss: 2.241 | Reg loss: 0.043 | Tree loss: 2.241 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 030 | Total loss: 2.247 | Reg loss: 0.043 | Tree loss: 2.247 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 030 | Total loss: 2.254 | Reg loss: 0.043 | Tree loss: 2.254 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 030 | Total loss: 2.198 | Reg loss: 0.043 | Tree loss: 2.198 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 030 | Total loss: 2.187 | Reg loss: 0.043 | Tree loss: 2.187 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 030 | Total loss: 2.188 | Reg loss: 0.043 | Tree loss: 2.188 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 030 | Total loss: 2.104 | Reg loss: 0.043 | Tree loss: 2.104 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 030 | Total loss: 2.148 | Reg loss: 0.043 | Tree loss: 2.148 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 030 | Total loss: 2.047 | Reg loss: 0.043 | Tree loss: 2.047 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 32 | Batch: 029 / 030 | Total loss: 2.099 | Reg loss: 0.043 | Tree loss: 2.099 | Accuracy: 0.564815 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 33 | Batch: 000 / 030 | Total loss: 2.747 | Reg loss: 0.041 | Tree loss: 2.747 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 030 | Total loss: 2.698 | Reg loss: 0.041 | Tree loss: 2.698 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 030 | Total loss: 2.639 | Reg loss: 0.041 | Tree loss: 2.639 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 030 | Total loss: 2.597 | Reg loss: 0.041 | Tree loss: 2.597 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 030 | Total loss: 2.602 | Reg loss: 0.041 | Tree loss: 2.602 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 030 | Total loss: 2.581 | Reg loss: 0.041 | Tree loss: 2.581 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 030 | Total loss: 2.471 | Reg loss: 0.041 | Tree loss: 2.471 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 030 | Total loss: 2.485 | Reg loss: 0.041 | Tree loss: 2.485 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 030 | Total loss: 2.480 | Reg loss: 0.041 | Tree loss: 2.480 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 030 | Total loss: 2.427 | Reg loss: 0.041 | Tree loss: 2.427 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 030 | Total loss: 2.408 | Reg loss: 0.041 | Tree loss: 2.408 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 030 | Total loss: 2.300 | Reg loss: 0.041 | Tree loss: 2.300 | Accuracy: 0.613281 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 030 | Total loss: 2.329 | Reg loss: 0.041 | Tree loss: 2.329 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 030 | Total loss: 2.258 | Reg loss: 0.041 | Tree loss: 2.258 | Accuracy: 0.636719 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 030 | Total loss: 2.328 | Reg loss: 0.042 | Tree loss: 2.328 | Accuracy: 0.541016 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 015 / 030 | Total loss: 2.259 | Reg loss: 0.042 | Tree loss: 2.259 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 030 | Total loss: 2.203 | Reg loss: 0.042 | Tree loss: 2.203 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 030 | Total loss: 2.221 | Reg loss: 0.042 | Tree loss: 2.221 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 030 | Total loss: 2.263 | Reg loss: 0.042 | Tree loss: 2.263 | Accuracy: 0.517578 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 030 | Total loss: 2.203 | Reg loss: 0.042 | Tree loss: 2.203 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 030 | Total loss: 2.181 | Reg loss: 0.042 | Tree loss: 2.181 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 030 | Total loss: 2.119 | Reg loss: 0.042 | Tree loss: 2.119 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 030 | Total loss: 2.090 | Reg loss: 0.042 | Tree loss: 2.090 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 030 | Total loss: 2.057 | Reg loss: 0.042 | Tree loss: 2.057 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 030 | Total loss: 2.021 | Reg loss: 0.042 | Tree loss: 2.021 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 030 | Total loss: 1.938 | Reg loss: 0.043 | Tree loss: 1.938 | Accuracy: 0.621094 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 030 | Total loss: 2.083 | Reg loss: 0.043 | Tree loss: 2.083 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 030 | Total loss: 2.009 | Reg loss: 0.043 | Tree loss: 2.009 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 030 | Total loss: 1.922 | Reg loss: 0.043 | Tree loss: 1.922 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 33 | Batch: 029 / 030 | Total loss: 1.926 | Reg loss: 0.043 | Tree loss: 1.926 | Accuracy: 0.629630 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 34 | Batch: 000 / 030 | Total loss: 2.538 | Reg loss: 0.041 | Tree loss: 2.538 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 030 | Total loss: 2.577 | Reg loss: 0.041 | Tree loss: 2.577 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 030 | Total loss: 2.466 | Reg loss: 0.041 | Tree loss: 2.466 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 030 | Total loss: 2.497 | Reg loss: 0.041 | Tree loss: 2.497 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 030 | Total loss: 2.450 | Reg loss: 0.041 | Tree loss: 2.450 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 030 | Total loss: 2.444 | Reg loss: 0.041 | Tree loss: 2.444 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 030 | Total loss: 2.369 | Reg loss: 0.041 | Tree loss: 2.369 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 030 | Total loss: 2.311 | Reg loss: 0.041 | Tree loss: 2.311 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 030 | Total loss: 2.305 | Reg loss: 0.041 | Tree loss: 2.305 | Accuracy: 0.617188 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 030 | Total loss: 2.307 | Reg loss: 0.041 | Tree loss: 2.307 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 030 | Total loss: 2.272 | Reg loss: 0.041 | Tree loss: 2.272 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 030 | Total loss: 2.267 | Reg loss: 0.041 | Tree loss: 2.267 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 030 | Total loss: 2.232 | Reg loss: 0.041 | Tree loss: 2.232 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 030 | Total loss: 2.183 | Reg loss: 0.041 | Tree loss: 2.183 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 030 | Total loss: 2.161 | Reg loss: 0.041 | Tree loss: 2.161 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 030 | Total loss: 2.108 | Reg loss: 0.041 | Tree loss: 2.108 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 030 | Total loss: 2.144 | Reg loss: 0.041 | Tree loss: 2.144 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 030 | Total loss: 2.104 | Reg loss: 0.041 | Tree loss: 2.104 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 030 | Total loss: 2.139 | Reg loss: 0.041 | Tree loss: 2.139 | Accuracy: 0.529297 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 030 | Total loss: 2.021 | Reg loss: 0.042 | Tree loss: 2.021 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 030 | Total loss: 2.021 | Reg loss: 0.042 | Tree loss: 2.021 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 030 | Total loss: 1.943 | Reg loss: 0.042 | Tree loss: 1.943 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 030 | Total loss: 2.013 | Reg loss: 0.042 | Tree loss: 2.013 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 030 | Total loss: 2.003 | Reg loss: 0.042 | Tree loss: 2.003 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 030 | Total loss: 1.951 | Reg loss: 0.042 | Tree loss: 1.951 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 030 | Total loss: 1.912 | Reg loss: 0.042 | Tree loss: 1.912 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 030 | Total loss: 1.928 | Reg loss: 0.042 | Tree loss: 1.928 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 030 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.632812 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 030 | Total loss: 1.868 | Reg loss: 0.042 | Tree loss: 1.868 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 34 | Batch: 029 / 030 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.574074 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 35 | Batch: 000 / 030 | Total loss: 2.388 | Reg loss: 0.040 | Tree loss: 2.388 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 030 | Total loss: 2.354 | Reg loss: 0.040 | Tree loss: 2.354 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 030 | Total loss: 2.434 | Reg loss: 0.040 | Tree loss: 2.434 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 030 | Total loss: 2.327 | Reg loss: 0.040 | Tree loss: 2.327 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 030 | Total loss: 2.317 | Reg loss: 0.040 | Tree loss: 2.317 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 030 | Total loss: 2.280 | Reg loss: 0.040 | Tree loss: 2.280 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 030 | Total loss: 2.263 | Reg loss: 0.040 | Tree loss: 2.263 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 030 | Total loss: 2.291 | Reg loss: 0.040 | Tree loss: 2.291 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 030 | Total loss: 2.180 | Reg loss: 0.040 | Tree loss: 2.180 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 030 | Total loss: 2.200 | Reg loss: 0.040 | Tree loss: 2.200 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 030 | Total loss: 2.170 | Reg loss: 0.040 | Tree loss: 2.170 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 030 | Total loss: 2.124 | Reg loss: 0.041 | Tree loss: 2.124 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 030 | Total loss: 2.104 | Reg loss: 0.041 | Tree loss: 2.104 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 030 | Total loss: 2.081 | Reg loss: 0.041 | Tree loss: 2.081 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 030 | Total loss: 2.025 | Reg loss: 0.041 | Tree loss: 2.025 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 030 | Total loss: 2.040 | Reg loss: 0.041 | Tree loss: 2.040 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 030 | Total loss: 2.023 | Reg loss: 0.041 | Tree loss: 2.023 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 030 | Total loss: 1.945 | Reg loss: 0.041 | Tree loss: 1.945 | Accuracy: 0.580078 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 018 / 030 | Total loss: 2.003 | Reg loss: 0.041 | Tree loss: 2.003 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 030 | Total loss: 1.979 | Reg loss: 0.041 | Tree loss: 1.979 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 030 | Total loss: 1.905 | Reg loss: 0.041 | Tree loss: 1.905 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 030 | Total loss: 1.886 | Reg loss: 0.041 | Tree loss: 1.886 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 030 | Total loss: 1.943 | Reg loss: 0.041 | Tree loss: 1.943 | Accuracy: 0.527344 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 030 | Total loss: 1.855 | Reg loss: 0.041 | Tree loss: 1.855 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 030 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.613281 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 030 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 030 | Total loss: 1.804 | Reg loss: 0.042 | Tree loss: 1.804 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 030 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 030 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 35 | Batch: 029 / 030 | Total loss: 1.735 | Reg loss: 0.042 | Tree loss: 1.735 | Accuracy: 0.583333 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 36 | Batch: 000 / 030 | Total loss: 2.246 | Reg loss: 0.040 | Tree loss: 2.246 | Accuracy: 0.605469 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 030 | Total loss: 2.318 | Reg loss: 0.040 | Tree loss: 2.318 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 030 | Total loss: 2.252 | Reg loss: 0.040 | Tree loss: 2.252 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 030 | Total loss: 2.219 | Reg loss: 0.040 | Tree loss: 2.219 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 030 | Total loss: 2.204 | Reg loss: 0.040 | Tree loss: 2.204 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 030 | Total loss: 2.149 | Reg loss: 0.040 | Tree loss: 2.149 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 030 | Total loss: 2.173 | Reg loss: 0.040 | Tree loss: 2.173 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 030 | Total loss: 2.142 | Reg loss: 0.040 | Tree loss: 2.142 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 030 | Total loss: 2.156 | Reg loss: 0.040 | Tree loss: 2.156 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 030 | Total loss: 2.059 | Reg loss: 0.040 | Tree loss: 2.059 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 030 | Total loss: 2.059 | Reg loss: 0.040 | Tree loss: 2.059 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 030 | Total loss: 1.991 | Reg loss: 0.040 | Tree loss: 1.991 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 030 | Total loss: 2.010 | Reg loss: 0.040 | Tree loss: 2.010 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 030 | Total loss: 1.959 | Reg loss: 0.040 | Tree loss: 1.959 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 030 | Total loss: 1.925 | Reg loss: 0.040 | Tree loss: 1.925 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 030 | Total loss: 1.927 | Reg loss: 0.040 | Tree loss: 1.927 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 030 | Total loss: 1.979 | Reg loss: 0.040 | Tree loss: 1.979 | Accuracy: 0.527344 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 030 | Total loss: 1.849 | Reg loss: 0.041 | Tree loss: 1.849 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 030 | Total loss: 1.915 | Reg loss: 0.041 | Tree loss: 1.915 | Accuracy: 0.519531 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 030 | Total loss: 1.864 | Reg loss: 0.041 | Tree loss: 1.864 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 030 | Total loss: 1.823 | Reg loss: 0.041 | Tree loss: 1.823 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 030 | Total loss: 1.786 | Reg loss: 0.041 | Tree loss: 1.786 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 030 | Total loss: 1.790 | Reg loss: 0.041 | Tree loss: 1.790 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 030 | Total loss: 1.769 | Reg loss: 0.041 | Tree loss: 1.769 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 030 | Total loss: 1.818 | Reg loss: 0.041 | Tree loss: 1.818 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 030 | Total loss: 1.710 | Reg loss: 0.041 | Tree loss: 1.710 | Accuracy: 0.605469 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 030 | Total loss: 1.715 | Reg loss: 0.041 | Tree loss: 1.715 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 030 | Total loss: 1.703 | Reg loss: 0.041 | Tree loss: 1.703 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 030 | Total loss: 1.682 | Reg loss: 0.041 | Tree loss: 1.682 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 36 | Batch: 029 / 030 | Total loss: 1.628 | Reg loss: 0.041 | Tree loss: 1.628 | Accuracy: 0.648148 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 37 | Batch: 000 / 030 | Total loss: 2.159 | Reg loss: 0.040 | Tree loss: 2.159 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 030 | Total loss: 2.175 | Reg loss: 0.040 | Tree loss: 2.175 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 030 | Total loss: 2.149 | Reg loss: 0.040 | Tree loss: 2.149 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 030 | Total loss: 2.167 | Reg loss: 0.040 | Tree loss: 2.167 | Accuracy: 0.541016 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 030 | Total loss: 2.081 | Reg loss: 0.040 | Tree loss: 2.081 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 030 | Total loss: 2.064 | Reg loss: 0.040 | Tree loss: 2.064 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 030 | Total loss: 2.076 | Reg loss: 0.040 | Tree loss: 2.076 | Accuracy: 0.531250 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 030 | Total loss: 2.035 | Reg loss: 0.040 | Tree loss: 2.035 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 030 | Total loss: 2.001 | Reg loss: 0.040 | Tree loss: 2.001 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 030 | Total loss: 1.941 | Reg loss: 0.040 | Tree loss: 1.941 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 030 | Total loss: 1.952 | Reg loss: 0.040 | Tree loss: 1.952 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 030 | Total loss: 1.918 | Reg loss: 0.040 | Tree loss: 1.918 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 030 | Total loss: 1.892 | Reg loss: 0.040 | Tree loss: 1.892 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 030 | Total loss: 1.876 | Reg loss: 0.040 | Tree loss: 1.876 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 030 | Total loss: 1.816 | Reg loss: 0.040 | Tree loss: 1.816 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 030 | Total loss: 1.807 | Reg loss: 0.040 | Tree loss: 1.807 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 030 | Total loss: 1.843 | Reg loss: 0.040 | Tree loss: 1.843 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 030 | Total loss: 1.800 | Reg loss: 0.040 | Tree loss: 1.800 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 030 | Total loss: 1.804 | Reg loss: 0.040 | Tree loss: 1.804 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 030 | Total loss: 1.689 | Reg loss: 0.040 | Tree loss: 1.689 | Accuracy: 0.634766 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 030 | Total loss: 1.730 | Reg loss: 0.040 | Tree loss: 1.730 | Accuracy: 0.597656 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 021 / 030 | Total loss: 1.726 | Reg loss: 0.040 | Tree loss: 1.726 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 030 | Total loss: 1.704 | Reg loss: 0.040 | Tree loss: 1.704 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 030 | Total loss: 1.707 | Reg loss: 0.040 | Tree loss: 1.707 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 030 | Total loss: 1.673 | Reg loss: 0.040 | Tree loss: 1.673 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 030 | Total loss: 1.730 | Reg loss: 0.041 | Tree loss: 1.730 | Accuracy: 0.539062 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 030 | Total loss: 1.640 | Reg loss: 0.041 | Tree loss: 1.640 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 030 | Total loss: 1.616 | Reg loss: 0.041 | Tree loss: 1.616 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 030 | Total loss: 1.681 | Reg loss: 0.041 | Tree loss: 1.681 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 37 | Batch: 029 / 030 | Total loss: 1.638 | Reg loss: 0.041 | Tree loss: 1.638 | Accuracy: 0.555556 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 38 | Batch: 000 / 030 | Total loss: 2.094 | Reg loss: 0.039 | Tree loss: 2.094 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 030 | Total loss: 2.066 | Reg loss: 0.039 | Tree loss: 2.066 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 030 | Total loss: 2.037 | Reg loss: 0.039 | Tree loss: 2.037 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 030 | Total loss: 2.006 | Reg loss: 0.039 | Tree loss: 2.006 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 030 | Total loss: 1.996 | Reg loss: 0.039 | Tree loss: 1.996 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 030 | Total loss: 1.930 | Reg loss: 0.039 | Tree loss: 1.930 | Accuracy: 0.638672 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 030 | Total loss: 1.922 | Reg loss: 0.039 | Tree loss: 1.922 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 030 | Total loss: 1.945 | Reg loss: 0.039 | Tree loss: 1.945 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 030 | Total loss: 1.905 | Reg loss: 0.039 | Tree loss: 1.905 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 030 | Total loss: 1.917 | Reg loss: 0.039 | Tree loss: 1.917 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 030 | Total loss: 1.849 | Reg loss: 0.039 | Tree loss: 1.849 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 030 | Total loss: 1.859 | Reg loss: 0.039 | Tree loss: 1.859 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 030 | Total loss: 1.766 | Reg loss: 0.039 | Tree loss: 1.766 | Accuracy: 0.613281 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 030 | Total loss: 1.754 | Reg loss: 0.039 | Tree loss: 1.754 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 030 | Total loss: 1.811 | Reg loss: 0.039 | Tree loss: 1.811 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 030 | Total loss: 1.785 | Reg loss: 0.039 | Tree loss: 1.785 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 030 | Total loss: 1.763 | Reg loss: 0.040 | Tree loss: 1.763 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 030 | Total loss: 1.689 | Reg loss: 0.040 | Tree loss: 1.689 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 030 | Total loss: 1.699 | Reg loss: 0.040 | Tree loss: 1.699 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 030 | Total loss: 1.673 | Reg loss: 0.040 | Tree loss: 1.673 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 030 | Total loss: 1.641 | Reg loss: 0.040 | Tree loss: 1.641 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 030 | Total loss: 1.655 | Reg loss: 0.040 | Tree loss: 1.655 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 030 | Total loss: 1.638 | Reg loss: 0.040 | Tree loss: 1.638 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 030 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 030 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 030 | Total loss: 1.632 | Reg loss: 0.040 | Tree loss: 1.632 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 030 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.541016 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 030 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 030 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 38 | Batch: 029 / 030 | Total loss: 1.645 | Reg loss: 0.040 | Tree loss: 1.645 | Accuracy: 0.500000 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 39 | Batch: 000 / 030 | Total loss: 2.002 | Reg loss: 0.039 | Tree loss: 2.002 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 030 | Total loss: 1.956 | Reg loss: 0.039 | Tree loss: 1.956 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 030 | Total loss: 2.016 | Reg loss: 0.039 | Tree loss: 2.016 | Accuracy: 0.539062 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 030 | Total loss: 1.956 | Reg loss: 0.039 | Tree loss: 1.956 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 030 | Total loss: 1.907 | Reg loss: 0.039 | Tree loss: 1.907 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 030 | Total loss: 1.902 | Reg loss: 0.039 | Tree loss: 1.902 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 030 | Total loss: 1.869 | Reg loss: 0.039 | Tree loss: 1.869 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 030 | Total loss: 1.832 | Reg loss: 0.039 | Tree loss: 1.832 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 030 | Total loss: 1.811 | Reg loss: 0.039 | Tree loss: 1.811 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 030 | Total loss: 1.773 | Reg loss: 0.039 | Tree loss: 1.773 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 030 | Total loss: 1.803 | Reg loss: 0.039 | Tree loss: 1.803 | Accuracy: 0.544922 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 030 | Total loss: 1.779 | Reg loss: 0.039 | Tree loss: 1.779 | Accuracy: 0.544922 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 030 | Total loss: 1.722 | Reg loss: 0.039 | Tree loss: 1.722 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 030 | Total loss: 1.681 | Reg loss: 0.039 | Tree loss: 1.681 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 030 | Total loss: 1.676 | Reg loss: 0.039 | Tree loss: 1.676 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 030 | Total loss: 1.669 | Reg loss: 0.039 | Tree loss: 1.669 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 030 | Total loss: 1.644 | Reg loss: 0.039 | Tree loss: 1.644 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 030 | Total loss: 1.638 | Reg loss: 0.039 | Tree loss: 1.638 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 030 | Total loss: 1.636 | Reg loss: 0.039 | Tree loss: 1.636 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 030 | Total loss: 1.590 | Reg loss: 0.039 | Tree loss: 1.590 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 030 | Total loss: 1.577 | Reg loss: 0.039 | Tree loss: 1.577 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 030 | Total loss: 1.570 | Reg loss: 0.039 | Tree loss: 1.570 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 030 | Total loss: 1.538 | Reg loss: 0.039 | Tree loss: 1.538 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 030 | Total loss: 1.583 | Reg loss: 0.039 | Tree loss: 1.583 | Accuracy: 0.560547 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 024 / 030 | Total loss: 1.536 | Reg loss: 0.039 | Tree loss: 1.536 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 030 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 030 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 030 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.542969 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 030 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 39 | Batch: 029 / 030 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.629630 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 40 | Batch: 000 / 030 | Total loss: 1.894 | Reg loss: 0.038 | Tree loss: 1.894 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 030 | Total loss: 1.917 | Reg loss: 0.038 | Tree loss: 1.917 | Accuracy: 0.537109 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 030 | Total loss: 1.878 | Reg loss: 0.038 | Tree loss: 1.878 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 030 | Total loss: 1.847 | Reg loss: 0.038 | Tree loss: 1.847 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 030 | Total loss: 1.773 | Reg loss: 0.038 | Tree loss: 1.773 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 030 | Total loss: 1.784 | Reg loss: 0.038 | Tree loss: 1.784 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 030 | Total loss: 1.795 | Reg loss: 0.038 | Tree loss: 1.795 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 030 | Total loss: 1.791 | Reg loss: 0.038 | Tree loss: 1.791 | Accuracy: 0.531250 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 030 | Total loss: 1.731 | Reg loss: 0.038 | Tree loss: 1.731 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 030 | Total loss: 1.760 | Reg loss: 0.038 | Tree loss: 1.760 | Accuracy: 0.533203 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 030 | Total loss: 1.734 | Reg loss: 0.038 | Tree loss: 1.734 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 030 | Total loss: 1.717 | Reg loss: 0.038 | Tree loss: 1.717 | Accuracy: 0.527344 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 030 | Total loss: 1.673 | Reg loss: 0.038 | Tree loss: 1.673 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 030 | Total loss: 1.657 | Reg loss: 0.038 | Tree loss: 1.657 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 030 | Total loss: 1.650 | Reg loss: 0.038 | Tree loss: 1.650 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 030 | Total loss: 1.608 | Reg loss: 0.039 | Tree loss: 1.608 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 030 | Total loss: 1.591 | Reg loss: 0.039 | Tree loss: 1.591 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 030 | Total loss: 1.584 | Reg loss: 0.039 | Tree loss: 1.584 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 030 | Total loss: 1.567 | Reg loss: 0.039 | Tree loss: 1.567 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 030 | Total loss: 1.546 | Reg loss: 0.039 | Tree loss: 1.546 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 030 | Total loss: 1.499 | Reg loss: 0.039 | Tree loss: 1.499 | Accuracy: 0.623047 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 030 | Total loss: 1.507 | Reg loss: 0.039 | Tree loss: 1.507 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 030 | Total loss: 1.518 | Reg loss: 0.039 | Tree loss: 1.518 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 030 | Total loss: 1.468 | Reg loss: 0.039 | Tree loss: 1.468 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 030 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.613281 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 030 | Total loss: 1.437 | Reg loss: 0.039 | Tree loss: 1.437 | Accuracy: 0.613281 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 030 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 030 | Total loss: 1.493 | Reg loss: 0.039 | Tree loss: 1.493 | Accuracy: 0.542969 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 030 | Total loss: 1.422 | Reg loss: 0.039 | Tree loss: 1.422 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 40 | Batch: 029 / 030 | Total loss: 1.367 | Reg loss: 0.039 | Tree loss: 1.367 | Accuracy: 0.620370 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 41 | Batch: 000 / 030 | Total loss: 1.842 | Reg loss: 0.038 | Tree loss: 1.842 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 030 | Total loss: 1.817 | Reg loss: 0.038 | Tree loss: 1.817 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 030 | Total loss: 1.815 | Reg loss: 0.038 | Tree loss: 1.815 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 030 | Total loss: 1.795 | Reg loss: 0.038 | Tree loss: 1.795 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 030 | Total loss: 1.731 | Reg loss: 0.038 | Tree loss: 1.731 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 030 | Total loss: 1.740 | Reg loss: 0.038 | Tree loss: 1.740 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 030 | Total loss: 1.727 | Reg loss: 0.038 | Tree loss: 1.727 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 030 | Total loss: 1.700 | Reg loss: 0.038 | Tree loss: 1.700 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 030 | Total loss: 1.679 | Reg loss: 0.038 | Tree loss: 1.679 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 030 | Total loss: 1.658 | Reg loss: 0.038 | Tree loss: 1.658 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 030 | Total loss: 1.632 | Reg loss: 0.038 | Tree loss: 1.632 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 030 | Total loss: 1.621 | Reg loss: 0.038 | Tree loss: 1.621 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 030 | Total loss: 1.579 | Reg loss: 0.038 | Tree loss: 1.579 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 030 | Total loss: 1.568 | Reg loss: 0.038 | Tree loss: 1.568 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 030 | Total loss: 1.549 | Reg loss: 0.038 | Tree loss: 1.549 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 030 | Total loss: 1.545 | Reg loss: 0.038 | Tree loss: 1.545 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 030 | Total loss: 1.555 | Reg loss: 0.038 | Tree loss: 1.555 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 030 | Total loss: 1.485 | Reg loss: 0.038 | Tree loss: 1.485 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 030 | Total loss: 1.486 | Reg loss: 0.038 | Tree loss: 1.486 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 030 | Total loss: 1.481 | Reg loss: 0.038 | Tree loss: 1.481 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 030 | Total loss: 1.454 | Reg loss: 0.038 | Tree loss: 1.454 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 030 | Total loss: 1.447 | Reg loss: 0.038 | Tree loss: 1.447 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 030 | Total loss: 1.424 | Reg loss: 0.038 | Tree loss: 1.424 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 030 | Total loss: 1.457 | Reg loss: 0.038 | Tree loss: 1.457 | Accuracy: 0.539062 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 030 | Total loss: 1.448 | Reg loss: 0.038 | Tree loss: 1.448 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 030 | Total loss: 1.416 | Reg loss: 0.039 | Tree loss: 1.416 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 030 | Total loss: 1.399 | Reg loss: 0.039 | Tree loss: 1.399 | Accuracy: 0.583984 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 027 / 030 | Total loss: 1.409 | Reg loss: 0.039 | Tree loss: 1.409 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 030 | Total loss: 1.409 | Reg loss: 0.039 | Tree loss: 1.409 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 41 | Batch: 029 / 030 | Total loss: 1.370 | Reg loss: 0.039 | Tree loss: 1.370 | Accuracy: 0.564815 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 42 | Batch: 000 / 030 | Total loss: 1.723 | Reg loss: 0.037 | Tree loss: 1.723 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 030 | Total loss: 1.749 | Reg loss: 0.037 | Tree loss: 1.749 | Accuracy: 0.539062 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 030 | Total loss: 1.702 | Reg loss: 0.037 | Tree loss: 1.702 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 030 | Total loss: 1.689 | Reg loss: 0.037 | Tree loss: 1.689 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 030 | Total loss: 1.722 | Reg loss: 0.037 | Tree loss: 1.722 | Accuracy: 0.537109 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 030 | Total loss: 1.683 | Reg loss: 0.037 | Tree loss: 1.683 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 030 | Total loss: 1.625 | Reg loss: 0.037 | Tree loss: 1.625 | Accuracy: 0.615234 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 030 | Total loss: 1.621 | Reg loss: 0.037 | Tree loss: 1.621 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 030 | Total loss: 1.663 | Reg loss: 0.037 | Tree loss: 1.663 | Accuracy: 0.529297 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 030 | Total loss: 1.600 | Reg loss: 0.037 | Tree loss: 1.600 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 030 | Total loss: 1.570 | Reg loss: 0.037 | Tree loss: 1.570 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 030 | Total loss: 1.552 | Reg loss: 0.037 | Tree loss: 1.552 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 030 | Total loss: 1.540 | Reg loss: 0.037 | Tree loss: 1.540 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 030 | Total loss: 1.508 | Reg loss: 0.037 | Tree loss: 1.508 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 030 | Total loss: 1.533 | Reg loss: 0.038 | Tree loss: 1.533 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 030 | Total loss: 1.486 | Reg loss: 0.038 | Tree loss: 1.486 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 030 | Total loss: 1.486 | Reg loss: 0.038 | Tree loss: 1.486 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 030 | Total loss: 1.502 | Reg loss: 0.038 | Tree loss: 1.502 | Accuracy: 0.523438 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 030 | Total loss: 1.430 | Reg loss: 0.038 | Tree loss: 1.430 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 030 | Total loss: 1.461 | Reg loss: 0.038 | Tree loss: 1.461 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 030 | Total loss: 1.394 | Reg loss: 0.038 | Tree loss: 1.394 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 030 | Total loss: 1.398 | Reg loss: 0.038 | Tree loss: 1.398 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 030 | Total loss: 1.392 | Reg loss: 0.038 | Tree loss: 1.392 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 030 | Total loss: 1.395 | Reg loss: 0.038 | Tree loss: 1.395 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 030 | Total loss: 1.356 | Reg loss: 0.038 | Tree loss: 1.356 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 030 | Total loss: 1.349 | Reg loss: 0.038 | Tree loss: 1.349 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 030 | Total loss: 1.393 | Reg loss: 0.038 | Tree loss: 1.393 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 030 | Total loss: 1.336 | Reg loss: 0.038 | Tree loss: 1.336 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 030 | Total loss: 1.321 | Reg loss: 0.038 | Tree loss: 1.321 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 42 | Batch: 029 / 030 | Total loss: 1.360 | Reg loss: 0.038 | Tree loss: 1.360 | Accuracy: 0.564815 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 43 | Batch: 000 / 030 | Total loss: 1.688 | Reg loss: 0.037 | Tree loss: 1.688 | Accuracy: 0.578125 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 030 | Total loss: 1.660 | Reg loss: 0.037 | Tree loss: 1.660 | Accuracy: 0.597656 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 030 | Total loss: 1.668 | Reg loss: 0.037 | Tree loss: 1.668 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 030 | Total loss: 1.644 | Reg loss: 0.037 | Tree loss: 1.644 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 030 | Total loss: 1.602 | Reg loss: 0.037 | Tree loss: 1.602 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 030 | Total loss: 1.603 | Reg loss: 0.037 | Tree loss: 1.603 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 030 | Total loss: 1.590 | Reg loss: 0.037 | Tree loss: 1.590 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 030 | Total loss: 1.546 | Reg loss: 0.037 | Tree loss: 1.546 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 030 | Total loss: 1.519 | Reg loss: 0.037 | Tree loss: 1.519 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 030 | Total loss: 1.508 | Reg loss: 0.037 | Tree loss: 1.508 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 030 | Total loss: 1.529 | Reg loss: 0.037 | Tree loss: 1.529 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 030 | Total loss: 1.474 | Reg loss: 0.037 | Tree loss: 1.474 | Accuracy: 0.582031 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 030 | Total loss: 1.512 | Reg loss: 0.037 | Tree loss: 1.512 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 030 | Total loss: 1.498 | Reg loss: 0.037 | Tree loss: 1.498 | Accuracy: 0.548828 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 030 | Total loss: 1.439 | Reg loss: 0.037 | Tree loss: 1.439 | Accuracy: 0.605469 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 030 | Total loss: 1.425 | Reg loss: 0.037 | Tree loss: 1.425 | Accuracy: 0.578125 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 030 | Total loss: 1.432 | Reg loss: 0.037 | Tree loss: 1.432 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 030 | Total loss: 1.405 | Reg loss: 0.037 | Tree loss: 1.405 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 030 | Total loss: 1.424 | Reg loss: 0.037 | Tree loss: 1.424 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 030 | Total loss: 1.351 | Reg loss: 0.037 | Tree loss: 1.351 | Accuracy: 0.630859 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 030 | Total loss: 1.373 | Reg loss: 0.037 | Tree loss: 1.373 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 030 | Total loss: 1.376 | Reg loss: 0.037 | Tree loss: 1.376 | Accuracy: 0.601562 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 030 | Total loss: 1.347 | Reg loss: 0.037 | Tree loss: 1.347 | Accuracy: 0.578125 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 030 | Total loss: 1.346 | Reg loss: 0.038 | Tree loss: 1.346 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 030 | Total loss: 1.330 | Reg loss: 0.038 | Tree loss: 1.330 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 030 | Total loss: 1.349 | Reg loss: 0.038 | Tree loss: 1.349 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 030 | Total loss: 1.357 | Reg loss: 0.038 | Tree loss: 1.357 | Accuracy: 0.533203 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 030 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 030 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 43 | Batch: 029 / 030 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.564815 | 0.362 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 44 | Batch: 000 / 030 | Total loss: 1.622 | Reg loss: 0.037 | Tree loss: 1.622 | Accuracy: 0.599609 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 030 | Total loss: 1.616 | Reg loss: 0.037 | Tree loss: 1.616 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 030 | Total loss: 1.597 | Reg loss: 0.036 | Tree loss: 1.597 | Accuracy: 0.601562 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 030 | Total loss: 1.596 | Reg loss: 0.036 | Tree loss: 1.596 | Accuracy: 0.542969 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 030 | Total loss: 1.578 | Reg loss: 0.036 | Tree loss: 1.578 | Accuracy: 0.533203 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 030 | Total loss: 1.495 | Reg loss: 0.036 | Tree loss: 1.495 | Accuracy: 0.634766 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 030 | Total loss: 1.535 | Reg loss: 0.036 | Tree loss: 1.535 | Accuracy: 0.574219 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 030 | Total loss: 1.529 | Reg loss: 0.037 | Tree loss: 1.529 | Accuracy: 0.539062 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 030 | Total loss: 1.483 | Reg loss: 0.037 | Tree loss: 1.483 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 030 | Total loss: 1.480 | Reg loss: 0.037 | Tree loss: 1.480 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 030 | Total loss: 1.448 | Reg loss: 0.037 | Tree loss: 1.448 | Accuracy: 0.599609 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 030 | Total loss: 1.430 | Reg loss: 0.037 | Tree loss: 1.430 | Accuracy: 0.593750 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 030 | Total loss: 1.431 | Reg loss: 0.037 | Tree loss: 1.431 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 030 | Total loss: 1.432 | Reg loss: 0.037 | Tree loss: 1.432 | Accuracy: 0.556641 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 030 | Total loss: 1.409 | Reg loss: 0.037 | Tree loss: 1.409 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 030 | Total loss: 1.398 | Reg loss: 0.037 | Tree loss: 1.398 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 030 | Total loss: 1.373 | Reg loss: 0.037 | Tree loss: 1.373 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 030 | Total loss: 1.414 | Reg loss: 0.037 | Tree loss: 1.414 | Accuracy: 0.525391 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 030 | Total loss: 1.375 | Reg loss: 0.037 | Tree loss: 1.375 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 030 | Total loss: 1.328 | Reg loss: 0.037 | Tree loss: 1.328 | Accuracy: 0.597656 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 030 | Total loss: 1.332 | Reg loss: 0.037 | Tree loss: 1.332 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 030 | Total loss: 1.330 | Reg loss: 0.037 | Tree loss: 1.330 | Accuracy: 0.568359 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 030 | Total loss: 1.325 | Reg loss: 0.037 | Tree loss: 1.325 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 030 | Total loss: 1.326 | Reg loss: 0.037 | Tree loss: 1.326 | Accuracy: 0.542969 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 030 | Total loss: 1.293 | Reg loss: 0.037 | Tree loss: 1.293 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 030 | Total loss: 1.247 | Reg loss: 0.037 | Tree loss: 1.247 | Accuracy: 0.609375 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 030 | Total loss: 1.286 | Reg loss: 0.037 | Tree loss: 1.286 | Accuracy: 0.552734 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 030 | Total loss: 1.271 | Reg loss: 0.037 | Tree loss: 1.271 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 030 | Total loss: 1.237 | Reg loss: 0.037 | Tree loss: 1.237 | Accuracy: 0.619141 | 0.362 sec/iter\n",
      "Epoch: 44 | Batch: 029 / 030 | Total loss: 1.301 | Reg loss: 0.037 | Tree loss: 1.301 | Accuracy: 0.527778 | 0.362 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 45 | Batch: 000 / 030 | Total loss: 1.582 | Reg loss: 0.036 | Tree loss: 1.582 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 030 | Total loss: 1.574 | Reg loss: 0.036 | Tree loss: 1.574 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 030 | Total loss: 1.539 | Reg loss: 0.036 | Tree loss: 1.539 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 030 | Total loss: 1.495 | Reg loss: 0.036 | Tree loss: 1.495 | Accuracy: 0.625000 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 030 | Total loss: 1.502 | Reg loss: 0.036 | Tree loss: 1.502 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 030 | Total loss: 1.503 | Reg loss: 0.036 | Tree loss: 1.503 | Accuracy: 0.582031 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 030 | Total loss: 1.480 | Reg loss: 0.036 | Tree loss: 1.480 | Accuracy: 0.548828 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 030 | Total loss: 1.472 | Reg loss: 0.036 | Tree loss: 1.472 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 030 | Total loss: 1.446 | Reg loss: 0.036 | Tree loss: 1.446 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 030 | Total loss: 1.451 | Reg loss: 0.036 | Tree loss: 1.451 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 030 | Total loss: 1.396 | Reg loss: 0.036 | Tree loss: 1.396 | Accuracy: 0.615234 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 030 | Total loss: 1.418 | Reg loss: 0.036 | Tree loss: 1.418 | Accuracy: 0.552734 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 030 | Total loss: 1.374 | Reg loss: 0.036 | Tree loss: 1.374 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 030 | Total loss: 1.391 | Reg loss: 0.036 | Tree loss: 1.391 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 030 | Total loss: 1.383 | Reg loss: 0.036 | Tree loss: 1.383 | Accuracy: 0.574219 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 030 | Total loss: 1.356 | Reg loss: 0.036 | Tree loss: 1.356 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 030 | Total loss: 1.362 | Reg loss: 0.036 | Tree loss: 1.362 | Accuracy: 0.548828 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 030 | Total loss: 1.329 | Reg loss: 0.036 | Tree loss: 1.329 | Accuracy: 0.597656 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 030 | Total loss: 1.283 | Reg loss: 0.036 | Tree loss: 1.283 | Accuracy: 0.605469 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 030 | Total loss: 1.314 | Reg loss: 0.036 | Tree loss: 1.314 | Accuracy: 0.521484 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 030 | Total loss: 1.286 | Reg loss: 0.037 | Tree loss: 1.286 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 030 | Total loss: 1.252 | Reg loss: 0.037 | Tree loss: 1.252 | Accuracy: 0.605469 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 030 | Total loss: 1.272 | Reg loss: 0.037 | Tree loss: 1.272 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 030 | Total loss: 1.259 | Reg loss: 0.037 | Tree loss: 1.259 | Accuracy: 0.574219 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 030 | Total loss: 1.248 | Reg loss: 0.037 | Tree loss: 1.248 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 030 | Total loss: 1.266 | Reg loss: 0.037 | Tree loss: 1.266 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 030 | Total loss: 1.236 | Reg loss: 0.037 | Tree loss: 1.236 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 030 | Total loss: 1.247 | Reg loss: 0.037 | Tree loss: 1.247 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 030 | Total loss: 1.253 | Reg loss: 0.037 | Tree loss: 1.253 | Accuracy: 0.552734 | 0.362 sec/iter\n",
      "Epoch: 45 | Batch: 029 / 030 | Total loss: 1.203 | Reg loss: 0.037 | Tree loss: 1.203 | Accuracy: 0.574074 | 0.362 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 46 | Batch: 000 / 030 | Total loss: 1.564 | Reg loss: 0.036 | Tree loss: 1.564 | Accuracy: 0.562500 | 0.362 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 001 / 030 | Total loss: 1.500 | Reg loss: 0.036 | Tree loss: 1.500 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 030 | Total loss: 1.480 | Reg loss: 0.036 | Tree loss: 1.480 | Accuracy: 0.615234 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 030 | Total loss: 1.492 | Reg loss: 0.036 | Tree loss: 1.492 | Accuracy: 0.542969 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 030 | Total loss: 1.487 | Reg loss: 0.036 | Tree loss: 1.487 | Accuracy: 0.548828 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 030 | Total loss: 1.459 | Reg loss: 0.036 | Tree loss: 1.459 | Accuracy: 0.587891 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 030 | Total loss: 1.443 | Reg loss: 0.036 | Tree loss: 1.443 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 030 | Total loss: 1.450 | Reg loss: 0.036 | Tree loss: 1.450 | Accuracy: 0.568359 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 030 | Total loss: 1.421 | Reg loss: 0.036 | Tree loss: 1.421 | Accuracy: 0.546875 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 030 | Total loss: 1.415 | Reg loss: 0.036 | Tree loss: 1.415 | Accuracy: 0.541016 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 030 | Total loss: 1.353 | Reg loss: 0.036 | Tree loss: 1.353 | Accuracy: 0.605469 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 030 | Total loss: 1.357 | Reg loss: 0.036 | Tree loss: 1.357 | Accuracy: 0.548828 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 030 | Total loss: 1.330 | Reg loss: 0.036 | Tree loss: 1.330 | Accuracy: 0.599609 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 030 | Total loss: 1.343 | Reg loss: 0.036 | Tree loss: 1.343 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 030 | Total loss: 1.310 | Reg loss: 0.036 | Tree loss: 1.310 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 030 | Total loss: 1.299 | Reg loss: 0.036 | Tree loss: 1.299 | Accuracy: 0.587891 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 030 | Total loss: 1.293 | Reg loss: 0.036 | Tree loss: 1.293 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 030 | Total loss: 1.286 | Reg loss: 0.036 | Tree loss: 1.286 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 030 | Total loss: 1.258 | Reg loss: 0.036 | Tree loss: 1.258 | Accuracy: 0.601562 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 030 | Total loss: 1.247 | Reg loss: 0.036 | Tree loss: 1.247 | Accuracy: 0.611328 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 030 | Total loss: 1.258 | Reg loss: 0.036 | Tree loss: 1.258 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 030 | Total loss: 1.240 | Reg loss: 0.036 | Tree loss: 1.240 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 030 | Total loss: 1.257 | Reg loss: 0.036 | Tree loss: 1.257 | Accuracy: 0.554688 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 030 | Total loss: 1.243 | Reg loss: 0.036 | Tree loss: 1.243 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 030 | Total loss: 1.199 | Reg loss: 0.036 | Tree loss: 1.199 | Accuracy: 0.597656 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 030 | Total loss: 1.234 | Reg loss: 0.036 | Tree loss: 1.234 | Accuracy: 0.554688 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 030 | Total loss: 1.186 | Reg loss: 0.036 | Tree loss: 1.186 | Accuracy: 0.593750 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 030 | Total loss: 1.233 | Reg loss: 0.036 | Tree loss: 1.233 | Accuracy: 0.537109 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 030 | Total loss: 1.202 | Reg loss: 0.036 | Tree loss: 1.202 | Accuracy: 0.554688 | 0.362 sec/iter\n",
      "Epoch: 46 | Batch: 029 / 030 | Total loss: 1.199 | Reg loss: 0.036 | Tree loss: 1.199 | Accuracy: 0.564815 | 0.362 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 47 | Batch: 000 / 030 | Total loss: 1.503 | Reg loss: 0.035 | Tree loss: 1.503 | Accuracy: 0.548828 | 0.363 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 030 | Total loss: 1.494 | Reg loss: 0.035 | Tree loss: 1.494 | Accuracy: 0.576172 | 0.363 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 030 | Total loss: 1.472 | Reg loss: 0.035 | Tree loss: 1.472 | Accuracy: 0.554688 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 030 | Total loss: 1.428 | Reg loss: 0.035 | Tree loss: 1.428 | Accuracy: 0.582031 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 030 | Total loss: 1.426 | Reg loss: 0.035 | Tree loss: 1.426 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 030 | Total loss: 1.405 | Reg loss: 0.035 | Tree loss: 1.405 | Accuracy: 0.568359 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 030 | Total loss: 1.381 | Reg loss: 0.035 | Tree loss: 1.381 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 030 | Total loss: 1.380 | Reg loss: 0.035 | Tree loss: 1.380 | Accuracy: 0.578125 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 030 | Total loss: 1.338 | Reg loss: 0.035 | Tree loss: 1.338 | Accuracy: 0.595703 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 030 | Total loss: 1.364 | Reg loss: 0.035 | Tree loss: 1.364 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 030 | Total loss: 1.344 | Reg loss: 0.035 | Tree loss: 1.344 | Accuracy: 0.568359 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 030 | Total loss: 1.361 | Reg loss: 0.035 | Tree loss: 1.361 | Accuracy: 0.537109 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 030 | Total loss: 1.286 | Reg loss: 0.035 | Tree loss: 1.286 | Accuracy: 0.630859 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 030 | Total loss: 1.323 | Reg loss: 0.036 | Tree loss: 1.323 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 030 | Total loss: 1.263 | Reg loss: 0.036 | Tree loss: 1.263 | Accuracy: 0.611328 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 030 | Total loss: 1.275 | Reg loss: 0.036 | Tree loss: 1.275 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 030 | Total loss: 1.276 | Reg loss: 0.036 | Tree loss: 1.276 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 030 | Total loss: 1.263 | Reg loss: 0.036 | Tree loss: 1.263 | Accuracy: 0.556641 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 030 | Total loss: 1.260 | Reg loss: 0.036 | Tree loss: 1.260 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 030 | Total loss: 1.233 | Reg loss: 0.036 | Tree loss: 1.233 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 030 | Total loss: 1.225 | Reg loss: 0.036 | Tree loss: 1.225 | Accuracy: 0.599609 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 030 | Total loss: 1.199 | Reg loss: 0.036 | Tree loss: 1.199 | Accuracy: 0.605469 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 030 | Total loss: 1.208 | Reg loss: 0.036 | Tree loss: 1.208 | Accuracy: 0.601562 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 030 | Total loss: 1.203 | Reg loss: 0.036 | Tree loss: 1.203 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 030 | Total loss: 1.194 | Reg loss: 0.036 | Tree loss: 1.194 | Accuracy: 0.574219 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 030 | Total loss: 1.211 | Reg loss: 0.036 | Tree loss: 1.211 | Accuracy: 0.529297 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 030 | Total loss: 1.183 | Reg loss: 0.036 | Tree loss: 1.183 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 030 | Total loss: 1.191 | Reg loss: 0.036 | Tree loss: 1.191 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 030 | Total loss: 1.145 | Reg loss: 0.036 | Tree loss: 1.145 | Accuracy: 0.603516 | 0.362 sec/iter\n",
      "Epoch: 47 | Batch: 029 / 030 | Total loss: 1.124 | Reg loss: 0.036 | Tree loss: 1.124 | Accuracy: 0.657407 | 0.362 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 48 | Batch: 000 / 030 | Total loss: 1.432 | Reg loss: 0.035 | Tree loss: 1.432 | Accuracy: 0.597656 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 030 | Total loss: 1.461 | Reg loss: 0.035 | Tree loss: 1.461 | Accuracy: 0.546875 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 030 | Total loss: 1.406 | Reg loss: 0.035 | Tree loss: 1.406 | Accuracy: 0.613281 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 030 | Total loss: 1.450 | Reg loss: 0.035 | Tree loss: 1.450 | Accuracy: 0.500000 | 0.363 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 004 / 030 | Total loss: 1.373 | Reg loss: 0.035 | Tree loss: 1.373 | Accuracy: 0.582031 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 030 | Total loss: 1.362 | Reg loss: 0.035 | Tree loss: 1.362 | Accuracy: 0.580078 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 030 | Total loss: 1.392 | Reg loss: 0.035 | Tree loss: 1.392 | Accuracy: 0.546875 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 030 | Total loss: 1.346 | Reg loss: 0.035 | Tree loss: 1.346 | Accuracy: 0.578125 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 030 | Total loss: 1.323 | Reg loss: 0.035 | Tree loss: 1.323 | Accuracy: 0.609375 | 0.363 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 030 | Total loss: 1.319 | Reg loss: 0.035 | Tree loss: 1.319 | Accuracy: 0.603516 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 030 | Total loss: 1.306 | Reg loss: 0.035 | Tree loss: 1.306 | Accuracy: 0.568359 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 030 | Total loss: 1.310 | Reg loss: 0.035 | Tree loss: 1.310 | Accuracy: 0.550781 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 030 | Total loss: 1.276 | Reg loss: 0.035 | Tree loss: 1.276 | Accuracy: 0.593750 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 030 | Total loss: 1.273 | Reg loss: 0.035 | Tree loss: 1.273 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 030 | Total loss: 1.243 | Reg loss: 0.035 | Tree loss: 1.243 | Accuracy: 0.613281 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 030 | Total loss: 1.238 | Reg loss: 0.035 | Tree loss: 1.238 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 030 | Total loss: 1.223 | Reg loss: 0.035 | Tree loss: 1.223 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 030 | Total loss: 1.219 | Reg loss: 0.035 | Tree loss: 1.219 | Accuracy: 0.582031 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 030 | Total loss: 1.215 | Reg loss: 0.035 | Tree loss: 1.215 | Accuracy: 0.574219 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 030 | Total loss: 1.195 | Reg loss: 0.035 | Tree loss: 1.195 | Accuracy: 0.599609 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 030 | Total loss: 1.200 | Reg loss: 0.035 | Tree loss: 1.200 | Accuracy: 0.568359 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 030 | Total loss: 1.199 | Reg loss: 0.035 | Tree loss: 1.199 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 030 | Total loss: 1.196 | Reg loss: 0.035 | Tree loss: 1.196 | Accuracy: 0.552734 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 030 | Total loss: 1.165 | Reg loss: 0.036 | Tree loss: 1.165 | Accuracy: 0.585938 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 030 | Total loss: 1.153 | Reg loss: 0.036 | Tree loss: 1.153 | Accuracy: 0.582031 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 030 | Total loss: 1.160 | Reg loss: 0.036 | Tree loss: 1.160 | Accuracy: 0.578125 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 030 | Total loss: 1.170 | Reg loss: 0.036 | Tree loss: 1.170 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 030 | Total loss: 1.163 | Reg loss: 0.036 | Tree loss: 1.163 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 030 | Total loss: 1.132 | Reg loss: 0.036 | Tree loss: 1.132 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 48 | Batch: 029 / 030 | Total loss: 1.128 | Reg loss: 0.036 | Tree loss: 1.128 | Accuracy: 0.564815 | 0.362 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 49 | Batch: 000 / 030 | Total loss: 1.401 | Reg loss: 0.035 | Tree loss: 1.401 | Accuracy: 0.582031 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 030 | Total loss: 1.388 | Reg loss: 0.035 | Tree loss: 1.388 | Accuracy: 0.558594 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 030 | Total loss: 1.385 | Reg loss: 0.035 | Tree loss: 1.385 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 030 | Total loss: 1.371 | Reg loss: 0.035 | Tree loss: 1.371 | Accuracy: 0.552734 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 030 | Total loss: 1.340 | Reg loss: 0.035 | Tree loss: 1.340 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 030 | Total loss: 1.328 | Reg loss: 0.035 | Tree loss: 1.328 | Accuracy: 0.605469 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 030 | Total loss: 1.323 | Reg loss: 0.035 | Tree loss: 1.323 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 030 | Total loss: 1.345 | Reg loss: 0.035 | Tree loss: 1.345 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 030 | Total loss: 1.339 | Reg loss: 0.035 | Tree loss: 1.339 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 030 | Total loss: 1.284 | Reg loss: 0.035 | Tree loss: 1.284 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 030 | Total loss: 1.280 | Reg loss: 0.035 | Tree loss: 1.280 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 030 | Total loss: 1.250 | Reg loss: 0.035 | Tree loss: 1.250 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 030 | Total loss: 1.245 | Reg loss: 0.035 | Tree loss: 1.245 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 030 | Total loss: 1.244 | Reg loss: 0.035 | Tree loss: 1.244 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 030 | Total loss: 1.241 | Reg loss: 0.035 | Tree loss: 1.241 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 030 | Total loss: 1.205 | Reg loss: 0.035 | Tree loss: 1.205 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 030 | Total loss: 1.222 | Reg loss: 0.035 | Tree loss: 1.222 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 030 | Total loss: 1.187 | Reg loss: 0.035 | Tree loss: 1.187 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 030 | Total loss: 1.199 | Reg loss: 0.035 | Tree loss: 1.199 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 030 | Total loss: 1.177 | Reg loss: 0.035 | Tree loss: 1.177 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 030 | Total loss: 1.167 | Reg loss: 0.035 | Tree loss: 1.167 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 030 | Total loss: 1.168 | Reg loss: 0.035 | Tree loss: 1.168 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 030 | Total loss: 1.175 | Reg loss: 0.035 | Tree loss: 1.175 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 030 | Total loss: 1.149 | Reg loss: 0.035 | Tree loss: 1.149 | Accuracy: 0.574219 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 030 | Total loss: 1.138 | Reg loss: 0.035 | Tree loss: 1.138 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 030 | Total loss: 1.148 | Reg loss: 0.035 | Tree loss: 1.148 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 030 | Total loss: 1.131 | Reg loss: 0.035 | Tree loss: 1.131 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 030 | Total loss: 1.123 | Reg loss: 0.035 | Tree loss: 1.123 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 030 | Total loss: 1.135 | Reg loss: 0.035 | Tree loss: 1.135 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 49 | Batch: 029 / 030 | Total loss: 1.131 | Reg loss: 0.035 | Tree loss: 1.131 | Accuracy: 0.583333 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 50 | Batch: 000 / 030 | Total loss: 1.370 | Reg loss: 0.034 | Tree loss: 1.370 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 030 | Total loss: 1.332 | Reg loss: 0.034 | Tree loss: 1.332 | Accuracy: 0.593750 | 0.362 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 030 | Total loss: 1.385 | Reg loss: 0.034 | Tree loss: 1.385 | Accuracy: 0.529297 | 0.362 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 030 | Total loss: 1.341 | Reg loss: 0.034 | Tree loss: 1.341 | Accuracy: 0.562500 | 0.362 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 030 | Total loss: 1.329 | Reg loss: 0.034 | Tree loss: 1.329 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 030 | Total loss: 1.354 | Reg loss: 0.034 | Tree loss: 1.354 | Accuracy: 0.527344 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 030 | Total loss: 1.271 | Reg loss: 0.034 | Tree loss: 1.271 | Accuracy: 0.582031 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 007 / 030 | Total loss: 1.280 | Reg loss: 0.034 | Tree loss: 1.280 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 030 | Total loss: 1.289 | Reg loss: 0.034 | Tree loss: 1.289 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 030 | Total loss: 1.242 | Reg loss: 0.034 | Tree loss: 1.242 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 030 | Total loss: 1.262 | Reg loss: 0.034 | Tree loss: 1.262 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 030 | Total loss: 1.244 | Reg loss: 0.034 | Tree loss: 1.244 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 030 | Total loss: 1.222 | Reg loss: 0.034 | Tree loss: 1.222 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 030 | Total loss: 1.235 | Reg loss: 0.034 | Tree loss: 1.235 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 030 | Total loss: 1.182 | Reg loss: 0.035 | Tree loss: 1.182 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 030 | Total loss: 1.197 | Reg loss: 0.035 | Tree loss: 1.197 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 030 | Total loss: 1.191 | Reg loss: 0.035 | Tree loss: 1.191 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 030 | Total loss: 1.171 | Reg loss: 0.035 | Tree loss: 1.171 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 030 | Total loss: 1.159 | Reg loss: 0.035 | Tree loss: 1.159 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 030 | Total loss: 1.185 | Reg loss: 0.035 | Tree loss: 1.185 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 030 | Total loss: 1.164 | Reg loss: 0.035 | Tree loss: 1.164 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 030 | Total loss: 1.138 | Reg loss: 0.035 | Tree loss: 1.138 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 030 | Total loss: 1.100 | Reg loss: 0.035 | Tree loss: 1.100 | Accuracy: 0.623047 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 030 | Total loss: 1.141 | Reg loss: 0.035 | Tree loss: 1.141 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 030 | Total loss: 1.122 | Reg loss: 0.035 | Tree loss: 1.122 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 030 | Total loss: 1.104 | Reg loss: 0.035 | Tree loss: 1.104 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 030 | Total loss: 1.099 | Reg loss: 0.035 | Tree loss: 1.099 | Accuracy: 0.609375 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 030 | Total loss: 1.080 | Reg loss: 0.035 | Tree loss: 1.080 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 030 | Total loss: 1.124 | Reg loss: 0.035 | Tree loss: 1.124 | Accuracy: 0.537109 | 0.361 sec/iter\n",
      "Epoch: 50 | Batch: 029 / 030 | Total loss: 1.106 | Reg loss: 0.035 | Tree loss: 1.106 | Accuracy: 0.555556 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 51 | Batch: 000 / 030 | Total loss: 1.348 | Reg loss: 0.034 | Tree loss: 1.348 | Accuracy: 0.572266 | 0.362 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 030 | Total loss: 1.359 | Reg loss: 0.034 | Tree loss: 1.359 | Accuracy: 0.564453 | 0.362 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 030 | Total loss: 1.337 | Reg loss: 0.034 | Tree loss: 1.337 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 030 | Total loss: 1.320 | Reg loss: 0.034 | Tree loss: 1.320 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 030 | Total loss: 1.301 | Reg loss: 0.034 | Tree loss: 1.301 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 030 | Total loss: 1.287 | Reg loss: 0.034 | Tree loss: 1.287 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 030 | Total loss: 1.292 | Reg loss: 0.034 | Tree loss: 1.292 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 030 | Total loss: 1.268 | Reg loss: 0.034 | Tree loss: 1.268 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 030 | Total loss: 1.225 | Reg loss: 0.034 | Tree loss: 1.225 | Accuracy: 0.613281 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 030 | Total loss: 1.240 | Reg loss: 0.034 | Tree loss: 1.240 | Accuracy: 0.535156 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 030 | Total loss: 1.213 | Reg loss: 0.034 | Tree loss: 1.213 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 030 | Total loss: 1.193 | Reg loss: 0.034 | Tree loss: 1.193 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 030 | Total loss: 1.182 | Reg loss: 0.034 | Tree loss: 1.182 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 030 | Total loss: 1.207 | Reg loss: 0.034 | Tree loss: 1.207 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 030 | Total loss: 1.182 | Reg loss: 0.034 | Tree loss: 1.182 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 030 | Total loss: 1.197 | Reg loss: 0.034 | Tree loss: 1.197 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 030 | Total loss: 1.161 | Reg loss: 0.034 | Tree loss: 1.161 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 030 | Total loss: 1.148 | Reg loss: 0.034 | Tree loss: 1.148 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 030 | Total loss: 1.138 | Reg loss: 0.034 | Tree loss: 1.138 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 030 | Total loss: 1.157 | Reg loss: 0.034 | Tree loss: 1.157 | Accuracy: 0.544922 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 030 | Total loss: 1.108 | Reg loss: 0.034 | Tree loss: 1.108 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 030 | Total loss: 1.122 | Reg loss: 0.034 | Tree loss: 1.122 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 030 | Total loss: 1.110 | Reg loss: 0.034 | Tree loss: 1.110 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 030 | Total loss: 1.111 | Reg loss: 0.034 | Tree loss: 1.111 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 030 | Total loss: 1.099 | Reg loss: 0.034 | Tree loss: 1.099 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 030 | Total loss: 1.090 | Reg loss: 0.035 | Tree loss: 1.090 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 030 | Total loss: 1.088 | Reg loss: 0.035 | Tree loss: 1.088 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 030 | Total loss: 1.074 | Reg loss: 0.035 | Tree loss: 1.074 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 030 | Total loss: 1.085 | Reg loss: 0.035 | Tree loss: 1.085 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 51 | Batch: 029 / 030 | Total loss: 1.124 | Reg loss: 0.035 | Tree loss: 1.124 | Accuracy: 0.527778 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 52 | Batch: 000 / 030 | Total loss: 1.334 | Reg loss: 0.034 | Tree loss: 1.334 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 030 | Total loss: 1.321 | Reg loss: 0.034 | Tree loss: 1.321 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 030 | Total loss: 1.282 | Reg loss: 0.034 | Tree loss: 1.282 | Accuracy: 0.611328 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 030 | Total loss: 1.266 | Reg loss: 0.034 | Tree loss: 1.266 | Accuracy: 0.615234 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 030 | Total loss: 1.265 | Reg loss: 0.034 | Tree loss: 1.265 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 030 | Total loss: 1.260 | Reg loss: 0.034 | Tree loss: 1.260 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 030 | Total loss: 1.281 | Reg loss: 0.034 | Tree loss: 1.281 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 030 | Total loss: 1.257 | Reg loss: 0.034 | Tree loss: 1.257 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 030 | Total loss: 1.238 | Reg loss: 0.034 | Tree loss: 1.238 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 030 | Total loss: 1.228 | Reg loss: 0.034 | Tree loss: 1.228 | Accuracy: 0.560547 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Batch: 010 / 030 | Total loss: 1.203 | Reg loss: 0.034 | Tree loss: 1.203 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 030 | Total loss: 1.191 | Reg loss: 0.034 | Tree loss: 1.191 | Accuracy: 0.583984 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 030 | Total loss: 1.193 | Reg loss: 0.034 | Tree loss: 1.193 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 030 | Total loss: 1.174 | Reg loss: 0.034 | Tree loss: 1.174 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 030 | Total loss: 1.129 | Reg loss: 0.034 | Tree loss: 1.129 | Accuracy: 0.611328 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 030 | Total loss: 1.162 | Reg loss: 0.034 | Tree loss: 1.162 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 030 | Total loss: 1.137 | Reg loss: 0.034 | Tree loss: 1.137 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 030 | Total loss: 1.127 | Reg loss: 0.034 | Tree loss: 1.127 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 030 | Total loss: 1.120 | Reg loss: 0.034 | Tree loss: 1.120 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 030 | Total loss: 1.124 | Reg loss: 0.034 | Tree loss: 1.124 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 030 | Total loss: 1.111 | Reg loss: 0.034 | Tree loss: 1.111 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 030 | Total loss: 1.078 | Reg loss: 0.034 | Tree loss: 1.078 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 030 | Total loss: 1.088 | Reg loss: 0.034 | Tree loss: 1.088 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 030 | Total loss: 1.101 | Reg loss: 0.034 | Tree loss: 1.101 | Accuracy: 0.541016 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 030 | Total loss: 1.077 | Reg loss: 0.034 | Tree loss: 1.077 | Accuracy: 0.595703 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 030 | Total loss: 1.100 | Reg loss: 0.034 | Tree loss: 1.100 | Accuracy: 0.550781 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 030 | Total loss: 1.085 | Reg loss: 0.034 | Tree loss: 1.085 | Accuracy: 0.556641 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 030 | Total loss: 1.060 | Reg loss: 0.034 | Tree loss: 1.060 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 030 | Total loss: 1.042 | Reg loss: 0.034 | Tree loss: 1.042 | Accuracy: 0.601562 | 0.362 sec/iter\n",
      "Epoch: 52 | Batch: 029 / 030 | Total loss: 1.062 | Reg loss: 0.034 | Tree loss: 1.062 | Accuracy: 0.555556 | 0.362 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 53 | Batch: 000 / 030 | Total loss: 1.283 | Reg loss: 0.033 | Tree loss: 1.283 | Accuracy: 0.601562 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 030 | Total loss: 1.284 | Reg loss: 0.033 | Tree loss: 1.284 | Accuracy: 0.599609 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 030 | Total loss: 1.265 | Reg loss: 0.033 | Tree loss: 1.265 | Accuracy: 0.574219 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 030 | Total loss: 1.244 | Reg loss: 0.033 | Tree loss: 1.244 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 030 | Total loss: 1.247 | Reg loss: 0.033 | Tree loss: 1.247 | Accuracy: 0.589844 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 030 | Total loss: 1.223 | Reg loss: 0.033 | Tree loss: 1.223 | Accuracy: 0.591797 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 030 | Total loss: 1.238 | Reg loss: 0.033 | Tree loss: 1.238 | Accuracy: 0.537109 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 030 | Total loss: 1.212 | Reg loss: 0.033 | Tree loss: 1.212 | Accuracy: 0.566406 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 030 | Total loss: 1.207 | Reg loss: 0.033 | Tree loss: 1.207 | Accuracy: 0.578125 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 030 | Total loss: 1.199 | Reg loss: 0.033 | Tree loss: 1.199 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 030 | Total loss: 1.200 | Reg loss: 0.033 | Tree loss: 1.200 | Accuracy: 0.576172 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 030 | Total loss: 1.190 | Reg loss: 0.033 | Tree loss: 1.190 | Accuracy: 0.539062 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 030 | Total loss: 1.145 | Reg loss: 0.033 | Tree loss: 1.145 | Accuracy: 0.617188 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 030 | Total loss: 1.154 | Reg loss: 0.034 | Tree loss: 1.154 | Accuracy: 0.568359 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 030 | Total loss: 1.136 | Reg loss: 0.034 | Tree loss: 1.136 | Accuracy: 0.570312 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 030 | Total loss: 1.156 | Reg loss: 0.034 | Tree loss: 1.156 | Accuracy: 0.560547 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 030 | Total loss: 1.136 | Reg loss: 0.034 | Tree loss: 1.136 | Accuracy: 0.580078 | 0.362 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 030 | Total loss: 1.107 | Reg loss: 0.034 | Tree loss: 1.107 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 030 | Total loss: 1.126 | Reg loss: 0.034 | Tree loss: 1.126 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 030 | Total loss: 1.103 | Reg loss: 0.034 | Tree loss: 1.103 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 030 | Total loss: 1.097 | Reg loss: 0.034 | Tree loss: 1.097 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 030 | Total loss: 1.093 | Reg loss: 0.034 | Tree loss: 1.093 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 030 | Total loss: 1.084 | Reg loss: 0.034 | Tree loss: 1.084 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 030 | Total loss: 1.055 | Reg loss: 0.034 | Tree loss: 1.055 | Accuracy: 0.625000 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 030 | Total loss: 1.086 | Reg loss: 0.034 | Tree loss: 1.086 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 030 | Total loss: 1.071 | Reg loss: 0.034 | Tree loss: 1.071 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 030 | Total loss: 1.047 | Reg loss: 0.034 | Tree loss: 1.047 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 030 | Total loss: 1.050 | Reg loss: 0.034 | Tree loss: 1.050 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 030 | Total loss: 1.049 | Reg loss: 0.034 | Tree loss: 1.049 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 53 | Batch: 029 / 030 | Total loss: 1.026 | Reg loss: 0.034 | Tree loss: 1.026 | Accuracy: 0.629630 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 54 | Batch: 000 / 030 | Total loss: 1.273 | Reg loss: 0.033 | Tree loss: 1.273 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 030 | Total loss: 1.288 | Reg loss: 0.033 | Tree loss: 1.288 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 030 | Total loss: 1.253 | Reg loss: 0.033 | Tree loss: 1.253 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 030 | Total loss: 1.237 | Reg loss: 0.033 | Tree loss: 1.237 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 030 | Total loss: 1.256 | Reg loss: 0.033 | Tree loss: 1.256 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 030 | Total loss: 1.215 | Reg loss: 0.033 | Tree loss: 1.215 | Accuracy: 0.611328 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 030 | Total loss: 1.202 | Reg loss: 0.033 | Tree loss: 1.202 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 030 | Total loss: 1.202 | Reg loss: 0.033 | Tree loss: 1.202 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 030 | Total loss: 1.195 | Reg loss: 0.033 | Tree loss: 1.195 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 030 | Total loss: 1.175 | Reg loss: 0.033 | Tree loss: 1.175 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 030 | Total loss: 1.203 | Reg loss: 0.033 | Tree loss: 1.203 | Accuracy: 0.527344 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 030 | Total loss: 1.153 | Reg loss: 0.033 | Tree loss: 1.153 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 030 | Total loss: 1.133 | Reg loss: 0.033 | Tree loss: 1.133 | Accuracy: 0.576172 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 013 / 030 | Total loss: 1.122 | Reg loss: 0.033 | Tree loss: 1.122 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 030 | Total loss: 1.104 | Reg loss: 0.033 | Tree loss: 1.104 | Accuracy: 0.615234 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 030 | Total loss: 1.109 | Reg loss: 0.033 | Tree loss: 1.109 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 030 | Total loss: 1.095 | Reg loss: 0.033 | Tree loss: 1.095 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 030 | Total loss: 1.084 | Reg loss: 0.033 | Tree loss: 1.084 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 030 | Total loss: 1.104 | Reg loss: 0.033 | Tree loss: 1.104 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 030 | Total loss: 1.078 | Reg loss: 0.033 | Tree loss: 1.078 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 030 | Total loss: 1.081 | Reg loss: 0.033 | Tree loss: 1.081 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 030 | Total loss: 1.088 | Reg loss: 0.033 | Tree loss: 1.088 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 030 | Total loss: 1.060 | Reg loss: 0.033 | Tree loss: 1.060 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 030 | Total loss: 1.035 | Reg loss: 0.034 | Tree loss: 1.035 | Accuracy: 0.619141 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 030 | Total loss: 1.044 | Reg loss: 0.034 | Tree loss: 1.044 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 030 | Total loss: 1.071 | Reg loss: 0.034 | Tree loss: 1.071 | Accuracy: 0.527344 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 030 | Total loss: 1.046 | Reg loss: 0.034 | Tree loss: 1.046 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 030 | Total loss: 1.047 | Reg loss: 0.034 | Tree loss: 1.047 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 030 | Total loss: 1.015 | Reg loss: 0.034 | Tree loss: 1.015 | Accuracy: 0.615234 | 0.36 sec/iter\n",
      "Epoch: 54 | Batch: 029 / 030 | Total loss: 1.032 | Reg loss: 0.034 | Tree loss: 1.032 | Accuracy: 0.546296 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 55 | Batch: 000 / 030 | Total loss: 1.251 | Reg loss: 0.033 | Tree loss: 1.251 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 030 | Total loss: 1.237 | Reg loss: 0.033 | Tree loss: 1.237 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 030 | Total loss: 1.230 | Reg loss: 0.033 | Tree loss: 1.230 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 030 | Total loss: 1.231 | Reg loss: 0.033 | Tree loss: 1.231 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 030 | Total loss: 1.210 | Reg loss: 0.033 | Tree loss: 1.210 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 030 | Total loss: 1.203 | Reg loss: 0.033 | Tree loss: 1.203 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 030 | Total loss: 1.169 | Reg loss: 0.033 | Tree loss: 1.169 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 030 | Total loss: 1.196 | Reg loss: 0.033 | Tree loss: 1.196 | Accuracy: 0.525391 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 030 | Total loss: 1.164 | Reg loss: 0.033 | Tree loss: 1.164 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 030 | Total loss: 1.160 | Reg loss: 0.033 | Tree loss: 1.160 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 030 | Total loss: 1.138 | Reg loss: 0.033 | Tree loss: 1.138 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 030 | Total loss: 1.141 | Reg loss: 0.033 | Tree loss: 1.141 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 030 | Total loss: 1.129 | Reg loss: 0.033 | Tree loss: 1.129 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 030 | Total loss: 1.107 | Reg loss: 0.033 | Tree loss: 1.107 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 030 | Total loss: 1.122 | Reg loss: 0.033 | Tree loss: 1.122 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 030 | Total loss: 1.093 | Reg loss: 0.033 | Tree loss: 1.093 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 030 | Total loss: 1.110 | Reg loss: 0.033 | Tree loss: 1.110 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 030 | Total loss: 1.111 | Reg loss: 0.033 | Tree loss: 1.111 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 030 | Total loss: 1.075 | Reg loss: 0.033 | Tree loss: 1.075 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 030 | Total loss: 1.067 | Reg loss: 0.033 | Tree loss: 1.067 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 030 | Total loss: 1.073 | Reg loss: 0.033 | Tree loss: 1.073 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 030 | Total loss: 1.045 | Reg loss: 0.033 | Tree loss: 1.045 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 030 | Total loss: 1.022 | Reg loss: 0.033 | Tree loss: 1.022 | Accuracy: 0.648438 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 030 | Total loss: 1.042 | Reg loss: 0.033 | Tree loss: 1.042 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 030 | Total loss: 1.039 | Reg loss: 0.033 | Tree loss: 1.039 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 030 | Total loss: 1.052 | Reg loss: 0.033 | Tree loss: 1.052 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 030 | Total loss: 1.028 | Reg loss: 0.033 | Tree loss: 1.028 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 030 | Total loss: 1.030 | Reg loss: 0.033 | Tree loss: 1.030 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 030 | Total loss: 1.017 | Reg loss: 0.033 | Tree loss: 1.017 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 55 | Batch: 029 / 030 | Total loss: 1.015 | Reg loss: 0.033 | Tree loss: 1.015 | Accuracy: 0.574074 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 56 | Batch: 000 / 030 | Total loss: 1.220 | Reg loss: 0.033 | Tree loss: 1.220 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 030 | Total loss: 1.205 | Reg loss: 0.033 | Tree loss: 1.205 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 030 | Total loss: 1.238 | Reg loss: 0.033 | Tree loss: 1.238 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 030 | Total loss: 1.231 | Reg loss: 0.033 | Tree loss: 1.231 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 030 | Total loss: 1.188 | Reg loss: 0.033 | Tree loss: 1.188 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 030 | Total loss: 1.192 | Reg loss: 0.033 | Tree loss: 1.192 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 030 | Total loss: 1.161 | Reg loss: 0.033 | Tree loss: 1.161 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 030 | Total loss: 1.159 | Reg loss: 0.033 | Tree loss: 1.159 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 030 | Total loss: 1.155 | Reg loss: 0.033 | Tree loss: 1.155 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 030 | Total loss: 1.135 | Reg loss: 0.033 | Tree loss: 1.135 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 030 | Total loss: 1.135 | Reg loss: 0.033 | Tree loss: 1.135 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 030 | Total loss: 1.115 | Reg loss: 0.033 | Tree loss: 1.115 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 030 | Total loss: 1.107 | Reg loss: 0.033 | Tree loss: 1.107 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 030 | Total loss: 1.092 | Reg loss: 0.033 | Tree loss: 1.092 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 030 | Total loss: 1.111 | Reg loss: 0.033 | Tree loss: 1.111 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 030 | Total loss: 1.109 | Reg loss: 0.033 | Tree loss: 1.109 | Accuracy: 0.523438 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 016 / 030 | Total loss: 1.080 | Reg loss: 0.033 | Tree loss: 1.080 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 030 | Total loss: 1.080 | Reg loss: 0.033 | Tree loss: 1.080 | Accuracy: 0.546875 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 030 | Total loss: 1.053 | Reg loss: 0.033 | Tree loss: 1.053 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 030 | Total loss: 1.066 | Reg loss: 0.033 | Tree loss: 1.066 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 030 | Total loss: 1.039 | Reg loss: 0.033 | Tree loss: 1.039 | Accuracy: 0.595703 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 030 | Total loss: 1.038 | Reg loss: 0.033 | Tree loss: 1.038 | Accuracy: 0.609375 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 030 | Total loss: 1.044 | Reg loss: 0.033 | Tree loss: 1.044 | Accuracy: 0.546875 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 030 | Total loss: 1.032 | Reg loss: 0.033 | Tree loss: 1.032 | Accuracy: 0.566406 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 030 | Total loss: 1.030 | Reg loss: 0.033 | Tree loss: 1.030 | Accuracy: 0.587891 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 030 | Total loss: 1.024 | Reg loss: 0.033 | Tree loss: 1.024 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 030 | Total loss: 1.016 | Reg loss: 0.033 | Tree loss: 1.016 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 030 | Total loss: 1.009 | Reg loss: 0.033 | Tree loss: 1.009 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 030 | Total loss: 1.001 | Reg loss: 0.033 | Tree loss: 1.001 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 56 | Batch: 029 / 030 | Total loss: 0.984 | Reg loss: 0.033 | Tree loss: 0.984 | Accuracy: 0.611111 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 57 | Batch: 000 / 030 | Total loss: 1.226 | Reg loss: 0.032 | Tree loss: 1.226 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 030 | Total loss: 1.229 | Reg loss: 0.032 | Tree loss: 1.229 | Accuracy: 0.599609 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 030 | Total loss: 1.181 | Reg loss: 0.032 | Tree loss: 1.181 | Accuracy: 0.595703 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 030 | Total loss: 1.176 | Reg loss: 0.032 | Tree loss: 1.176 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 030 | Total loss: 1.196 | Reg loss: 0.032 | Tree loss: 1.196 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 030 | Total loss: 1.153 | Reg loss: 0.032 | Tree loss: 1.153 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 030 | Total loss: 1.151 | Reg loss: 0.032 | Tree loss: 1.151 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 030 | Total loss: 1.110 | Reg loss: 0.032 | Tree loss: 1.110 | Accuracy: 0.623047 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 030 | Total loss: 1.153 | Reg loss: 0.032 | Tree loss: 1.153 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 030 | Total loss: 1.120 | Reg loss: 0.032 | Tree loss: 1.120 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 030 | Total loss: 1.119 | Reg loss: 0.032 | Tree loss: 1.119 | Accuracy: 0.599609 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 030 | Total loss: 1.127 | Reg loss: 0.032 | Tree loss: 1.127 | Accuracy: 0.537109 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 030 | Total loss: 1.093 | Reg loss: 0.032 | Tree loss: 1.093 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 030 | Total loss: 1.079 | Reg loss: 0.032 | Tree loss: 1.079 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 030 | Total loss: 1.085 | Reg loss: 0.032 | Tree loss: 1.085 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 030 | Total loss: 1.079 | Reg loss: 0.032 | Tree loss: 1.079 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 030 | Total loss: 1.034 | Reg loss: 0.032 | Tree loss: 1.034 | Accuracy: 0.626953 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 030 | Total loss: 1.061 | Reg loss: 0.033 | Tree loss: 1.061 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 030 | Total loss: 1.058 | Reg loss: 0.033 | Tree loss: 1.058 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 030 | Total loss: 1.042 | Reg loss: 0.033 | Tree loss: 1.042 | Accuracy: 0.601562 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 030 | Total loss: 1.042 | Reg loss: 0.033 | Tree loss: 1.042 | Accuracy: 0.562500 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 030 | Total loss: 1.032 | Reg loss: 0.033 | Tree loss: 1.032 | Accuracy: 0.564453 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 030 | Total loss: 1.047 | Reg loss: 0.033 | Tree loss: 1.047 | Accuracy: 0.539062 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 030 | Total loss: 1.025 | Reg loss: 0.033 | Tree loss: 1.025 | Accuracy: 0.562500 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 030 | Total loss: 1.008 | Reg loss: 0.033 | Tree loss: 1.008 | Accuracy: 0.605469 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 030 | Total loss: 1.010 | Reg loss: 0.033 | Tree loss: 1.010 | Accuracy: 0.568359 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 030 | Total loss: 1.001 | Reg loss: 0.033 | Tree loss: 1.001 | Accuracy: 0.580078 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 030 | Total loss: 1.040 | Reg loss: 0.033 | Tree loss: 1.040 | Accuracy: 0.505859 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 030 | Total loss: 1.005 | Reg loss: 0.033 | Tree loss: 1.005 | Accuracy: 0.558594 | 0.357 sec/iter\n",
      "Epoch: 57 | Batch: 029 / 030 | Total loss: 0.999 | Reg loss: 0.033 | Tree loss: 0.999 | Accuracy: 0.592593 | 0.357 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 58 | Batch: 000 / 030 | Total loss: 1.211 | Reg loss: 0.032 | Tree loss: 1.211 | Accuracy: 0.546875 | 0.358 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 030 | Total loss: 1.183 | Reg loss: 0.032 | Tree loss: 1.183 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 030 | Total loss: 1.178 | Reg loss: 0.032 | Tree loss: 1.178 | Accuracy: 0.603516 | 0.358 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 030 | Total loss: 1.169 | Reg loss: 0.032 | Tree loss: 1.169 | Accuracy: 0.572266 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 030 | Total loss: 1.154 | Reg loss: 0.032 | Tree loss: 1.154 | Accuracy: 0.603516 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 030 | Total loss: 1.165 | Reg loss: 0.032 | Tree loss: 1.165 | Accuracy: 0.574219 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 030 | Total loss: 1.133 | Reg loss: 0.032 | Tree loss: 1.133 | Accuracy: 0.605469 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 030 | Total loss: 1.125 | Reg loss: 0.032 | Tree loss: 1.125 | Accuracy: 0.582031 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 030 | Total loss: 1.131 | Reg loss: 0.032 | Tree loss: 1.131 | Accuracy: 0.570312 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 030 | Total loss: 1.116 | Reg loss: 0.032 | Tree loss: 1.116 | Accuracy: 0.562500 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 030 | Total loss: 1.100 | Reg loss: 0.032 | Tree loss: 1.100 | Accuracy: 0.582031 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 030 | Total loss: 1.101 | Reg loss: 0.032 | Tree loss: 1.101 | Accuracy: 0.583984 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 030 | Total loss: 1.078 | Reg loss: 0.032 | Tree loss: 1.078 | Accuracy: 0.605469 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 030 | Total loss: 1.071 | Reg loss: 0.032 | Tree loss: 1.071 | Accuracy: 0.605469 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 030 | Total loss: 1.050 | Reg loss: 0.032 | Tree loss: 1.050 | Accuracy: 0.597656 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 030 | Total loss: 1.055 | Reg loss: 0.032 | Tree loss: 1.055 | Accuracy: 0.576172 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 030 | Total loss: 1.062 | Reg loss: 0.032 | Tree loss: 1.062 | Accuracy: 0.582031 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 030 | Total loss: 1.038 | Reg loss: 0.032 | Tree loss: 1.038 | Accuracy: 0.578125 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 030 | Total loss: 1.044 | Reg loss: 0.032 | Tree loss: 1.044 | Accuracy: 0.554688 | 0.357 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 019 / 030 | Total loss: 1.014 | Reg loss: 0.032 | Tree loss: 1.014 | Accuracy: 0.589844 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 030 | Total loss: 1.054 | Reg loss: 0.032 | Tree loss: 1.054 | Accuracy: 0.541016 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 030 | Total loss: 1.032 | Reg loss: 0.032 | Tree loss: 1.032 | Accuracy: 0.550781 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 030 | Total loss: 1.014 | Reg loss: 0.032 | Tree loss: 1.014 | Accuracy: 0.574219 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 030 | Total loss: 1.012 | Reg loss: 0.032 | Tree loss: 1.012 | Accuracy: 0.568359 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 030 | Total loss: 0.998 | Reg loss: 0.032 | Tree loss: 0.998 | Accuracy: 0.585938 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 030 | Total loss: 1.006 | Reg loss: 0.032 | Tree loss: 1.006 | Accuracy: 0.576172 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 030 | Total loss: 1.004 | Reg loss: 0.033 | Tree loss: 1.004 | Accuracy: 0.554688 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 030 | Total loss: 1.019 | Reg loss: 0.033 | Tree loss: 1.019 | Accuracy: 0.527344 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 030 | Total loss: 1.009 | Reg loss: 0.033 | Tree loss: 1.009 | Accuracy: 0.548828 | 0.357 sec/iter\n",
      "Epoch: 58 | Batch: 029 / 030 | Total loss: 0.992 | Reg loss: 0.033 | Tree loss: 0.992 | Accuracy: 0.583333 | 0.357 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 59 | Batch: 000 / 030 | Total loss: 1.192 | Reg loss: 0.032 | Tree loss: 1.192 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 030 | Total loss: 1.189 | Reg loss: 0.032 | Tree loss: 1.189 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 030 | Total loss: 1.185 | Reg loss: 0.032 | Tree loss: 1.185 | Accuracy: 0.560547 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 030 | Total loss: 1.173 | Reg loss: 0.032 | Tree loss: 1.173 | Accuracy: 0.593750 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 030 | Total loss: 1.155 | Reg loss: 0.032 | Tree loss: 1.155 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 030 | Total loss: 1.138 | Reg loss: 0.032 | Tree loss: 1.138 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 030 | Total loss: 1.146 | Reg loss: 0.032 | Tree loss: 1.146 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 030 | Total loss: 1.146 | Reg loss: 0.032 | Tree loss: 1.146 | Accuracy: 0.541016 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 030 | Total loss: 1.120 | Reg loss: 0.032 | Tree loss: 1.120 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 030 | Total loss: 1.084 | Reg loss: 0.032 | Tree loss: 1.084 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 030 | Total loss: 1.106 | Reg loss: 0.032 | Tree loss: 1.106 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 030 | Total loss: 1.097 | Reg loss: 0.032 | Tree loss: 1.097 | Accuracy: 0.550781 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 030 | Total loss: 1.057 | Reg loss: 0.032 | Tree loss: 1.057 | Accuracy: 0.609375 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 030 | Total loss: 1.042 | Reg loss: 0.032 | Tree loss: 1.042 | Accuracy: 0.621094 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 030 | Total loss: 1.032 | Reg loss: 0.032 | Tree loss: 1.032 | Accuracy: 0.605469 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 030 | Total loss: 1.047 | Reg loss: 0.032 | Tree loss: 1.047 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 030 | Total loss: 1.041 | Reg loss: 0.032 | Tree loss: 1.041 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 030 | Total loss: 1.032 | Reg loss: 0.032 | Tree loss: 1.032 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 030 | Total loss: 1.027 | Reg loss: 0.032 | Tree loss: 1.027 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 030 | Total loss: 1.025 | Reg loss: 0.032 | Tree loss: 1.025 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 030 | Total loss: 1.014 | Reg loss: 0.032 | Tree loss: 1.014 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 030 | Total loss: 1.022 | Reg loss: 0.032 | Tree loss: 1.022 | Accuracy: 0.552734 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 030 | Total loss: 1.015 | Reg loss: 0.032 | Tree loss: 1.015 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 030 | Total loss: 1.007 | Reg loss: 0.032 | Tree loss: 1.007 | Accuracy: 0.572266 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 030 | Total loss: 0.991 | Reg loss: 0.032 | Tree loss: 0.991 | Accuracy: 0.546875 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 030 | Total loss: 0.967 | Reg loss: 0.032 | Tree loss: 0.967 | Accuracy: 0.607422 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 030 | Total loss: 0.979 | Reg loss: 0.032 | Tree loss: 0.979 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 030 | Total loss: 0.986 | Reg loss: 0.032 | Tree loss: 0.986 | Accuracy: 0.544922 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 030 | Total loss: 0.983 | Reg loss: 0.032 | Tree loss: 0.983 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 59 | Batch: 029 / 030 | Total loss: 0.947 | Reg loss: 0.032 | Tree loss: 0.947 | Accuracy: 0.611111 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 60 | Batch: 000 / 030 | Total loss: 1.181 | Reg loss: 0.031 | Tree loss: 1.181 | Accuracy: 0.550781 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 030 | Total loss: 1.165 | Reg loss: 0.031 | Tree loss: 1.165 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 030 | Total loss: 1.173 | Reg loss: 0.031 | Tree loss: 1.173 | Accuracy: 0.562500 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 030 | Total loss: 1.162 | Reg loss: 0.031 | Tree loss: 1.162 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 030 | Total loss: 1.170 | Reg loss: 0.031 | Tree loss: 1.170 | Accuracy: 0.533203 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 030 | Total loss: 1.125 | Reg loss: 0.031 | Tree loss: 1.125 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 030 | Total loss: 1.115 | Reg loss: 0.031 | Tree loss: 1.115 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 030 | Total loss: 1.113 | Reg loss: 0.031 | Tree loss: 1.113 | Accuracy: 0.593750 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 030 | Total loss: 1.113 | Reg loss: 0.032 | Tree loss: 1.113 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 030 | Total loss: 1.089 | Reg loss: 0.032 | Tree loss: 1.089 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 030 | Total loss: 1.085 | Reg loss: 0.032 | Tree loss: 1.085 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 030 | Total loss: 1.072 | Reg loss: 0.032 | Tree loss: 1.072 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 030 | Total loss: 1.058 | Reg loss: 0.032 | Tree loss: 1.058 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 030 | Total loss: 1.052 | Reg loss: 0.032 | Tree loss: 1.052 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 030 | Total loss: 1.043 | Reg loss: 0.032 | Tree loss: 1.043 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 030 | Total loss: 1.049 | Reg loss: 0.032 | Tree loss: 1.049 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 030 | Total loss: 1.021 | Reg loss: 0.032 | Tree loss: 1.021 | Accuracy: 0.595703 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 030 | Total loss: 1.044 | Reg loss: 0.032 | Tree loss: 1.044 | Accuracy: 0.546875 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 030 | Total loss: 1.022 | Reg loss: 0.032 | Tree loss: 1.022 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 030 | Total loss: 1.006 | Reg loss: 0.032 | Tree loss: 1.006 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 030 | Total loss: 1.014 | Reg loss: 0.032 | Tree loss: 1.014 | Accuracy: 0.550781 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 030 | Total loss: 1.006 | Reg loss: 0.032 | Tree loss: 1.006 | Accuracy: 0.566406 | 0.358 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 022 / 030 | Total loss: 0.977 | Reg loss: 0.032 | Tree loss: 0.977 | Accuracy: 0.621094 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 030 | Total loss: 1.001 | Reg loss: 0.032 | Tree loss: 1.001 | Accuracy: 0.562500 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 030 | Total loss: 1.003 | Reg loss: 0.032 | Tree loss: 1.003 | Accuracy: 0.542969 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 030 | Total loss: 0.977 | Reg loss: 0.032 | Tree loss: 0.977 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 030 | Total loss: 0.944 | Reg loss: 0.032 | Tree loss: 0.944 | Accuracy: 0.630859 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 030 | Total loss: 0.959 | Reg loss: 0.032 | Tree loss: 0.959 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 030 | Total loss: 0.966 | Reg loss: 0.032 | Tree loss: 0.966 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 60 | Batch: 029 / 030 | Total loss: 0.938 | Reg loss: 0.032 | Tree loss: 0.938 | Accuracy: 0.638889 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 61 | Batch: 000 / 030 | Total loss: 1.164 | Reg loss: 0.031 | Tree loss: 1.164 | Accuracy: 0.560547 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 030 | Total loss: 1.166 | Reg loss: 0.031 | Tree loss: 1.166 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 030 | Total loss: 1.166 | Reg loss: 0.031 | Tree loss: 1.166 | Accuracy: 0.560547 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 030 | Total loss: 1.141 | Reg loss: 0.031 | Tree loss: 1.141 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 030 | Total loss: 1.140 | Reg loss: 0.031 | Tree loss: 1.140 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 030 | Total loss: 1.117 | Reg loss: 0.031 | Tree loss: 1.117 | Accuracy: 0.595703 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 030 | Total loss: 1.120 | Reg loss: 0.031 | Tree loss: 1.120 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 030 | Total loss: 1.114 | Reg loss: 0.031 | Tree loss: 1.114 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 030 | Total loss: 1.082 | Reg loss: 0.031 | Tree loss: 1.082 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 030 | Total loss: 1.083 | Reg loss: 0.031 | Tree loss: 1.083 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 030 | Total loss: 1.071 | Reg loss: 0.031 | Tree loss: 1.071 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 030 | Total loss: 1.055 | Reg loss: 0.031 | Tree loss: 1.055 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 030 | Total loss: 1.054 | Reg loss: 0.031 | Tree loss: 1.054 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 030 | Total loss: 1.059 | Reg loss: 0.031 | Tree loss: 1.059 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 030 | Total loss: 1.036 | Reg loss: 0.031 | Tree loss: 1.036 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 030 | Total loss: 1.021 | Reg loss: 0.031 | Tree loss: 1.021 | Accuracy: 0.593750 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 030 | Total loss: 1.016 | Reg loss: 0.032 | Tree loss: 1.016 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 030 | Total loss: 1.034 | Reg loss: 0.032 | Tree loss: 1.034 | Accuracy: 0.535156 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 030 | Total loss: 1.009 | Reg loss: 0.032 | Tree loss: 1.009 | Accuracy: 0.572266 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 030 | Total loss: 1.006 | Reg loss: 0.032 | Tree loss: 1.006 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 030 | Total loss: 0.989 | Reg loss: 0.032 | Tree loss: 0.989 | Accuracy: 0.587891 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 030 | Total loss: 0.990 | Reg loss: 0.032 | Tree loss: 0.990 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 030 | Total loss: 0.986 | Reg loss: 0.032 | Tree loss: 0.986 | Accuracy: 0.566406 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 030 | Total loss: 0.985 | Reg loss: 0.032 | Tree loss: 0.985 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 030 | Total loss: 0.961 | Reg loss: 0.032 | Tree loss: 0.961 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 030 | Total loss: 0.963 | Reg loss: 0.032 | Tree loss: 0.963 | Accuracy: 0.593750 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 030 | Total loss: 0.962 | Reg loss: 0.032 | Tree loss: 0.962 | Accuracy: 0.599609 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 030 | Total loss: 0.976 | Reg loss: 0.032 | Tree loss: 0.976 | Accuracy: 0.544922 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 030 | Total loss: 0.953 | Reg loss: 0.032 | Tree loss: 0.953 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 61 | Batch: 029 / 030 | Total loss: 0.950 | Reg loss: 0.032 | Tree loss: 0.950 | Accuracy: 0.564815 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 62 | Batch: 000 / 030 | Total loss: 1.135 | Reg loss: 0.031 | Tree loss: 1.135 | Accuracy: 0.605469 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 030 | Total loss: 1.150 | Reg loss: 0.031 | Tree loss: 1.150 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 030 | Total loss: 1.146 | Reg loss: 0.031 | Tree loss: 1.146 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 030 | Total loss: 1.108 | Reg loss: 0.031 | Tree loss: 1.108 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 030 | Total loss: 1.137 | Reg loss: 0.031 | Tree loss: 1.137 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 030 | Total loss: 1.137 | Reg loss: 0.031 | Tree loss: 1.137 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 030 | Total loss: 1.110 | Reg loss: 0.031 | Tree loss: 1.110 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 030 | Total loss: 1.110 | Reg loss: 0.031 | Tree loss: 1.110 | Accuracy: 0.539062 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 030 | Total loss: 1.078 | Reg loss: 0.031 | Tree loss: 1.078 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 030 | Total loss: 1.051 | Reg loss: 0.031 | Tree loss: 1.051 | Accuracy: 0.609375 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 030 | Total loss: 1.046 | Reg loss: 0.031 | Tree loss: 1.046 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 030 | Total loss: 1.061 | Reg loss: 0.031 | Tree loss: 1.061 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 030 | Total loss: 1.054 | Reg loss: 0.031 | Tree loss: 1.054 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 030 | Total loss: 1.060 | Reg loss: 0.031 | Tree loss: 1.060 | Accuracy: 0.531250 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 030 | Total loss: 1.040 | Reg loss: 0.031 | Tree loss: 1.040 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 030 | Total loss: 1.027 | Reg loss: 0.031 | Tree loss: 1.027 | Accuracy: 0.568359 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 030 | Total loss: 1.012 | Reg loss: 0.031 | Tree loss: 1.012 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 030 | Total loss: 1.005 | Reg loss: 0.031 | Tree loss: 1.005 | Accuracy: 0.572266 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 030 | Total loss: 0.992 | Reg loss: 0.031 | Tree loss: 0.992 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 030 | Total loss: 0.977 | Reg loss: 0.031 | Tree loss: 0.977 | Accuracy: 0.621094 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 030 | Total loss: 0.991 | Reg loss: 0.031 | Tree loss: 0.991 | Accuracy: 0.572266 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 030 | Total loss: 0.980 | Reg loss: 0.031 | Tree loss: 0.980 | Accuracy: 0.595703 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 030 | Total loss: 0.967 | Reg loss: 0.031 | Tree loss: 0.967 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 030 | Total loss: 0.987 | Reg loss: 0.032 | Tree loss: 0.987 | Accuracy: 0.550781 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 030 | Total loss: 0.968 | Reg loss: 0.032 | Tree loss: 0.968 | Accuracy: 0.583984 | 0.358 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 025 / 030 | Total loss: 0.964 | Reg loss: 0.032 | Tree loss: 0.964 | Accuracy: 0.572266 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 030 | Total loss: 0.947 | Reg loss: 0.032 | Tree loss: 0.947 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 030 | Total loss: 0.946 | Reg loss: 0.032 | Tree loss: 0.946 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 030 | Total loss: 0.967 | Reg loss: 0.032 | Tree loss: 0.967 | Accuracy: 0.548828 | 0.358 sec/iter\n",
      "Epoch: 62 | Batch: 029 / 030 | Total loss: 0.959 | Reg loss: 0.032 | Tree loss: 0.959 | Accuracy: 0.583333 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 63 | Batch: 000 / 030 | Total loss: 1.136 | Reg loss: 0.031 | Tree loss: 1.136 | Accuracy: 0.595703 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 030 | Total loss: 1.163 | Reg loss: 0.031 | Tree loss: 1.163 | Accuracy: 0.539062 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 030 | Total loss: 1.131 | Reg loss: 0.031 | Tree loss: 1.131 | Accuracy: 0.597656 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 030 | Total loss: 1.106 | Reg loss: 0.031 | Tree loss: 1.106 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 030 | Total loss: 1.112 | Reg loss: 0.031 | Tree loss: 1.112 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 030 | Total loss: 1.108 | Reg loss: 0.031 | Tree loss: 1.108 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 030 | Total loss: 1.081 | Reg loss: 0.031 | Tree loss: 1.081 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 030 | Total loss: 1.074 | Reg loss: 0.031 | Tree loss: 1.074 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 030 | Total loss: 1.071 | Reg loss: 0.031 | Tree loss: 1.071 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 030 | Total loss: 1.066 | Reg loss: 0.031 | Tree loss: 1.066 | Accuracy: 0.587891 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 030 | Total loss: 1.070 | Reg loss: 0.031 | Tree loss: 1.070 | Accuracy: 0.566406 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 030 | Total loss: 1.041 | Reg loss: 0.031 | Tree loss: 1.041 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 030 | Total loss: 1.042 | Reg loss: 0.031 | Tree loss: 1.042 | Accuracy: 0.566406 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 030 | Total loss: 1.035 | Reg loss: 0.031 | Tree loss: 1.035 | Accuracy: 0.552734 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 030 | Total loss: 1.034 | Reg loss: 0.031 | Tree loss: 1.034 | Accuracy: 0.541016 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 030 | Total loss: 1.002 | Reg loss: 0.031 | Tree loss: 1.002 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 030 | Total loss: 1.017 | Reg loss: 0.031 | Tree loss: 1.017 | Accuracy: 0.531250 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 030 | Total loss: 0.993 | Reg loss: 0.031 | Tree loss: 0.993 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 030 | Total loss: 0.986 | Reg loss: 0.031 | Tree loss: 0.986 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 030 | Total loss: 0.977 | Reg loss: 0.031 | Tree loss: 0.977 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 030 | Total loss: 0.985 | Reg loss: 0.031 | Tree loss: 0.985 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 030 | Total loss: 0.986 | Reg loss: 0.031 | Tree loss: 0.986 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 030 | Total loss: 0.982 | Reg loss: 0.031 | Tree loss: 0.982 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 030 | Total loss: 0.960 | Reg loss: 0.031 | Tree loss: 0.960 | Accuracy: 0.603516 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 030 | Total loss: 0.954 | Reg loss: 0.031 | Tree loss: 0.954 | Accuracy: 0.599609 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 030 | Total loss: 0.962 | Reg loss: 0.031 | Tree loss: 0.962 | Accuracy: 0.554688 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 030 | Total loss: 0.949 | Reg loss: 0.031 | Tree loss: 0.949 | Accuracy: 0.583984 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 030 | Total loss: 0.946 | Reg loss: 0.031 | Tree loss: 0.946 | Accuracy: 0.576172 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 030 | Total loss: 0.949 | Reg loss: 0.031 | Tree loss: 0.949 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 63 | Batch: 029 / 030 | Total loss: 0.910 | Reg loss: 0.031 | Tree loss: 0.910 | Accuracy: 0.629630 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 64 | Batch: 000 / 030 | Total loss: 1.123 | Reg loss: 0.031 | Tree loss: 1.123 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 030 | Total loss: 1.123 | Reg loss: 0.031 | Tree loss: 1.123 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 030 | Total loss: 1.139 | Reg loss: 0.031 | Tree loss: 1.139 | Accuracy: 0.544922 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 030 | Total loss: 1.101 | Reg loss: 0.031 | Tree loss: 1.101 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 030 | Total loss: 1.102 | Reg loss: 0.031 | Tree loss: 1.102 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 030 | Total loss: 1.117 | Reg loss: 0.031 | Tree loss: 1.117 | Accuracy: 0.531250 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 030 | Total loss: 1.092 | Reg loss: 0.031 | Tree loss: 1.092 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 030 | Total loss: 1.061 | Reg loss: 0.031 | Tree loss: 1.061 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 030 | Total loss: 1.055 | Reg loss: 0.031 | Tree loss: 1.055 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 030 | Total loss: 1.054 | Reg loss: 0.031 | Tree loss: 1.054 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 030 | Total loss: 1.032 | Reg loss: 0.031 | Tree loss: 1.032 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 030 | Total loss: 1.039 | Reg loss: 0.031 | Tree loss: 1.039 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 030 | Total loss: 1.033 | Reg loss: 0.031 | Tree loss: 1.033 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 030 | Total loss: 1.019 | Reg loss: 0.031 | Tree loss: 1.019 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 030 | Total loss: 1.011 | Reg loss: 0.031 | Tree loss: 1.011 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 030 | Total loss: 0.989 | Reg loss: 0.031 | Tree loss: 0.989 | Accuracy: 0.611328 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 030 | Total loss: 1.012 | Reg loss: 0.031 | Tree loss: 1.012 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 030 | Total loss: 0.995 | Reg loss: 0.031 | Tree loss: 0.995 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 030 | Total loss: 0.987 | Reg loss: 0.031 | Tree loss: 0.987 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 030 | Total loss: 0.975 | Reg loss: 0.031 | Tree loss: 0.975 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 030 | Total loss: 0.967 | Reg loss: 0.031 | Tree loss: 0.967 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 030 | Total loss: 0.982 | Reg loss: 0.031 | Tree loss: 0.982 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 030 | Total loss: 0.954 | Reg loss: 0.031 | Tree loss: 0.954 | Accuracy: 0.609375 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 030 | Total loss: 0.976 | Reg loss: 0.031 | Tree loss: 0.976 | Accuracy: 0.523438 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 030 | Total loss: 0.949 | Reg loss: 0.031 | Tree loss: 0.949 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 030 | Total loss: 0.963 | Reg loss: 0.031 | Tree loss: 0.963 | Accuracy: 0.544922 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 030 | Total loss: 0.965 | Reg loss: 0.031 | Tree loss: 0.965 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 030 | Total loss: 0.927 | Reg loss: 0.031 | Tree loss: 0.927 | Accuracy: 0.605469 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Batch: 028 / 030 | Total loss: 0.961 | Reg loss: 0.031 | Tree loss: 0.961 | Accuracy: 0.546875 | 0.359 sec/iter\n",
      "Epoch: 64 | Batch: 029 / 030 | Total loss: 0.903 | Reg loss: 0.031 | Tree loss: 0.903 | Accuracy: 0.648148 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 65 | Batch: 000 / 030 | Total loss: 1.142 | Reg loss: 0.030 | Tree loss: 1.142 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 030 | Total loss: 1.120 | Reg loss: 0.030 | Tree loss: 1.120 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 030 | Total loss: 1.109 | Reg loss: 0.030 | Tree loss: 1.109 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 030 | Total loss: 1.120 | Reg loss: 0.030 | Tree loss: 1.120 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 030 | Total loss: 1.076 | Reg loss: 0.030 | Tree loss: 1.076 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 030 | Total loss: 1.078 | Reg loss: 0.030 | Tree loss: 1.078 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 030 | Total loss: 1.083 | Reg loss: 0.030 | Tree loss: 1.083 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 030 | Total loss: 1.058 | Reg loss: 0.030 | Tree loss: 1.058 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 030 | Total loss: 1.058 | Reg loss: 0.030 | Tree loss: 1.058 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 030 | Total loss: 1.060 | Reg loss: 0.031 | Tree loss: 1.060 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 030 | Total loss: 1.037 | Reg loss: 0.031 | Tree loss: 1.037 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 030 | Total loss: 1.038 | Reg loss: 0.031 | Tree loss: 1.038 | Accuracy: 0.556641 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 030 | Total loss: 1.022 | Reg loss: 0.031 | Tree loss: 1.022 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 030 | Total loss: 1.018 | Reg loss: 0.031 | Tree loss: 1.018 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 030 | Total loss: 1.011 | Reg loss: 0.031 | Tree loss: 1.011 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 030 | Total loss: 1.000 | Reg loss: 0.031 | Tree loss: 1.000 | Accuracy: 0.558594 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 030 | Total loss: 0.973 | Reg loss: 0.031 | Tree loss: 0.973 | Accuracy: 0.603516 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 030 | Total loss: 0.976 | Reg loss: 0.031 | Tree loss: 0.976 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 030 | Total loss: 0.971 | Reg loss: 0.031 | Tree loss: 0.971 | Accuracy: 0.574219 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 030 | Total loss: 0.986 | Reg loss: 0.031 | Tree loss: 0.986 | Accuracy: 0.541016 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 030 | Total loss: 0.966 | Reg loss: 0.031 | Tree loss: 0.966 | Accuracy: 0.589844 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 030 | Total loss: 0.957 | Reg loss: 0.031 | Tree loss: 0.957 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 030 | Total loss: 0.970 | Reg loss: 0.031 | Tree loss: 0.970 | Accuracy: 0.548828 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 030 | Total loss: 0.945 | Reg loss: 0.031 | Tree loss: 0.945 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 030 | Total loss: 0.963 | Reg loss: 0.031 | Tree loss: 0.963 | Accuracy: 0.539062 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 030 | Total loss: 0.925 | Reg loss: 0.031 | Tree loss: 0.925 | Accuracy: 0.617188 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 030 | Total loss: 0.942 | Reg loss: 0.031 | Tree loss: 0.942 | Accuracy: 0.578125 | 0.358 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 030 | Total loss: 0.933 | Reg loss: 0.031 | Tree loss: 0.933 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 030 | Total loss: 0.943 | Reg loss: 0.031 | Tree loss: 0.943 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 65 | Batch: 029 / 030 | Total loss: 0.939 | Reg loss: 0.031 | Tree loss: 0.939 | Accuracy: 0.527778 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 66 | Batch: 000 / 030 | Total loss: 1.116 | Reg loss: 0.030 | Tree loss: 1.116 | Accuracy: 0.605469 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 030 | Total loss: 1.127 | Reg loss: 0.030 | Tree loss: 1.127 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 030 | Total loss: 1.105 | Reg loss: 0.030 | Tree loss: 1.105 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 030 | Total loss: 1.091 | Reg loss: 0.030 | Tree loss: 1.091 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 030 | Total loss: 1.095 | Reg loss: 0.030 | Tree loss: 1.095 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 030 | Total loss: 1.077 | Reg loss: 0.030 | Tree loss: 1.077 | Accuracy: 0.607422 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 030 | Total loss: 1.086 | Reg loss: 0.030 | Tree loss: 1.086 | Accuracy: 0.527344 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 030 | Total loss: 1.063 | Reg loss: 0.030 | Tree loss: 1.063 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 030 | Total loss: 1.028 | Reg loss: 0.030 | Tree loss: 1.028 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 030 | Total loss: 1.018 | Reg loss: 0.030 | Tree loss: 1.018 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 030 | Total loss: 1.014 | Reg loss: 0.030 | Tree loss: 1.014 | Accuracy: 0.609375 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 030 | Total loss: 1.040 | Reg loss: 0.030 | Tree loss: 1.040 | Accuracy: 0.556641 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 030 | Total loss: 1.004 | Reg loss: 0.030 | Tree loss: 1.004 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 030 | Total loss: 1.008 | Reg loss: 0.030 | Tree loss: 1.008 | Accuracy: 0.556641 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 030 | Total loss: 0.988 | Reg loss: 0.030 | Tree loss: 0.988 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 030 | Total loss: 1.006 | Reg loss: 0.030 | Tree loss: 1.006 | Accuracy: 0.531250 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 030 | Total loss: 0.999 | Reg loss: 0.031 | Tree loss: 0.999 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 030 | Total loss: 0.975 | Reg loss: 0.031 | Tree loss: 0.975 | Accuracy: 0.591797 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 030 | Total loss: 0.981 | Reg loss: 0.031 | Tree loss: 0.981 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 030 | Total loss: 0.972 | Reg loss: 0.031 | Tree loss: 0.972 | Accuracy: 0.580078 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 030 | Total loss: 0.960 | Reg loss: 0.031 | Tree loss: 0.960 | Accuracy: 0.582031 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 030 | Total loss: 0.944 | Reg loss: 0.031 | Tree loss: 0.944 | Accuracy: 0.601562 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 030 | Total loss: 0.951 | Reg loss: 0.031 | Tree loss: 0.951 | Accuracy: 0.585938 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 030 | Total loss: 0.967 | Reg loss: 0.031 | Tree loss: 0.967 | Accuracy: 0.541016 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 030 | Total loss: 0.942 | Reg loss: 0.031 | Tree loss: 0.942 | Accuracy: 0.564453 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 030 | Total loss: 0.929 | Reg loss: 0.031 | Tree loss: 0.929 | Accuracy: 0.595703 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 030 | Total loss: 0.937 | Reg loss: 0.031 | Tree loss: 0.937 | Accuracy: 0.556641 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 030 | Total loss: 0.919 | Reg loss: 0.031 | Tree loss: 0.919 | Accuracy: 0.605469 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 030 | Total loss: 0.945 | Reg loss: 0.031 | Tree loss: 0.945 | Accuracy: 0.542969 | 0.358 sec/iter\n",
      "Epoch: 66 | Batch: 029 / 030 | Total loss: 0.958 | Reg loss: 0.031 | Tree loss: 0.958 | Accuracy: 0.481481 | 0.358 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 000 / 030 | Total loss: 1.121 | Reg loss: 0.030 | Tree loss: 1.121 | Accuracy: 0.613281 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 030 | Total loss: 1.113 | Reg loss: 0.030 | Tree loss: 1.113 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 030 | Total loss: 1.120 | Reg loss: 0.030 | Tree loss: 1.120 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 030 | Total loss: 1.088 | Reg loss: 0.030 | Tree loss: 1.088 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 030 | Total loss: 1.080 | Reg loss: 0.030 | Tree loss: 1.080 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 030 | Total loss: 1.052 | Reg loss: 0.030 | Tree loss: 1.052 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 030 | Total loss: 1.058 | Reg loss: 0.030 | Tree loss: 1.058 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 030 | Total loss: 1.053 | Reg loss: 0.030 | Tree loss: 1.053 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 030 | Total loss: 1.041 | Reg loss: 0.030 | Tree loss: 1.041 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 030 | Total loss: 1.021 | Reg loss: 0.030 | Tree loss: 1.021 | Accuracy: 0.617188 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 030 | Total loss: 1.015 | Reg loss: 0.030 | Tree loss: 1.015 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 030 | Total loss: 0.995 | Reg loss: 0.030 | Tree loss: 0.995 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 030 | Total loss: 1.014 | Reg loss: 0.030 | Tree loss: 1.014 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 030 | Total loss: 0.998 | Reg loss: 0.030 | Tree loss: 0.998 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 030 | Total loss: 0.989 | Reg loss: 0.030 | Tree loss: 0.989 | Accuracy: 0.570312 | 0.358 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 030 | Total loss: 0.990 | Reg loss: 0.030 | Tree loss: 0.990 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 030 | Total loss: 1.000 | Reg loss: 0.030 | Tree loss: 1.000 | Accuracy: 0.546875 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 030 | Total loss: 0.982 | Reg loss: 0.030 | Tree loss: 0.982 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 030 | Total loss: 0.974 | Reg loss: 0.030 | Tree loss: 0.974 | Accuracy: 0.556641 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 030 | Total loss: 0.959 | Reg loss: 0.030 | Tree loss: 0.959 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 030 | Total loss: 0.951 | Reg loss: 0.030 | Tree loss: 0.951 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 030 | Total loss: 0.950 | Reg loss: 0.031 | Tree loss: 0.950 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 030 | Total loss: 0.941 | Reg loss: 0.031 | Tree loss: 0.941 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 030 | Total loss: 0.963 | Reg loss: 0.031 | Tree loss: 0.963 | Accuracy: 0.539062 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 030 | Total loss: 0.925 | Reg loss: 0.031 | Tree loss: 0.925 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 030 | Total loss: 0.936 | Reg loss: 0.031 | Tree loss: 0.936 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 030 | Total loss: 0.923 | Reg loss: 0.031 | Tree loss: 0.923 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 030 | Total loss: 0.915 | Reg loss: 0.031 | Tree loss: 0.915 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 030 | Total loss: 0.926 | Reg loss: 0.031 | Tree loss: 0.926 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 67 | Batch: 029 / 030 | Total loss: 0.950 | Reg loss: 0.031 | Tree loss: 0.950 | Accuracy: 0.500000 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 68 | Batch: 000 / 030 | Total loss: 1.111 | Reg loss: 0.030 | Tree loss: 1.111 | Accuracy: 0.605469 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 030 | Total loss: 1.111 | Reg loss: 0.030 | Tree loss: 1.111 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 030 | Total loss: 1.112 | Reg loss: 0.030 | Tree loss: 1.112 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 030 | Total loss: 1.093 | Reg loss: 0.030 | Tree loss: 1.093 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 030 | Total loss: 1.088 | Reg loss: 0.030 | Tree loss: 1.088 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 030 | Total loss: 1.070 | Reg loss: 0.030 | Tree loss: 1.070 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 030 | Total loss: 1.055 | Reg loss: 0.030 | Tree loss: 1.055 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 030 | Total loss: 1.057 | Reg loss: 0.030 | Tree loss: 1.057 | Accuracy: 0.537109 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 030 | Total loss: 1.023 | Reg loss: 0.030 | Tree loss: 1.023 | Accuracy: 0.621094 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 030 | Total loss: 1.024 | Reg loss: 0.030 | Tree loss: 1.024 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 030 | Total loss: 1.031 | Reg loss: 0.030 | Tree loss: 1.031 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 030 | Total loss: 1.013 | Reg loss: 0.030 | Tree loss: 1.013 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 030 | Total loss: 0.988 | Reg loss: 0.030 | Tree loss: 0.988 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 030 | Total loss: 0.979 | Reg loss: 0.030 | Tree loss: 0.979 | Accuracy: 0.611328 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 030 | Total loss: 0.983 | Reg loss: 0.030 | Tree loss: 0.983 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 030 | Total loss: 0.979 | Reg loss: 0.030 | Tree loss: 0.979 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 030 | Total loss: 0.958 | Reg loss: 0.030 | Tree loss: 0.958 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 030 | Total loss: 0.964 | Reg loss: 0.030 | Tree loss: 0.964 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 030 | Total loss: 0.964 | Reg loss: 0.030 | Tree loss: 0.964 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 030 | Total loss: 0.960 | Reg loss: 0.030 | Tree loss: 0.960 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 030 | Total loss: 0.948 | Reg loss: 0.030 | Tree loss: 0.948 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 030 | Total loss: 0.942 | Reg loss: 0.030 | Tree loss: 0.942 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 030 | Total loss: 0.942 | Reg loss: 0.030 | Tree loss: 0.942 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 030 | Total loss: 0.918 | Reg loss: 0.030 | Tree loss: 0.918 | Accuracy: 0.619141 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 030 | Total loss: 0.932 | Reg loss: 0.030 | Tree loss: 0.932 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 030 | Total loss: 0.934 | Reg loss: 0.030 | Tree loss: 0.934 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 030 | Total loss: 0.929 | Reg loss: 0.031 | Tree loss: 0.929 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 030 | Total loss: 0.907 | Reg loss: 0.031 | Tree loss: 0.907 | Accuracy: 0.605469 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 030 | Total loss: 0.923 | Reg loss: 0.031 | Tree loss: 0.923 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 68 | Batch: 029 / 030 | Total loss: 0.919 | Reg loss: 0.031 | Tree loss: 0.919 | Accuracy: 0.555556 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 69 | Batch: 000 / 030 | Total loss: 1.091 | Reg loss: 0.030 | Tree loss: 1.091 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 030 | Total loss: 1.115 | Reg loss: 0.030 | Tree loss: 1.115 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 030 | Total loss: 1.083 | Reg loss: 0.030 | Tree loss: 1.083 | Accuracy: 0.541016 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 003 / 030 | Total loss: 1.096 | Reg loss: 0.030 | Tree loss: 1.096 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 030 | Total loss: 1.100 | Reg loss: 0.030 | Tree loss: 1.100 | Accuracy: 0.529297 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 030 | Total loss: 1.046 | Reg loss: 0.030 | Tree loss: 1.046 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 030 | Total loss: 1.046 | Reg loss: 0.030 | Tree loss: 1.046 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 030 | Total loss: 1.047 | Reg loss: 0.030 | Tree loss: 1.047 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 030 | Total loss: 1.029 | Reg loss: 0.030 | Tree loss: 1.029 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 030 | Total loss: 1.030 | Reg loss: 0.030 | Tree loss: 1.030 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 030 | Total loss: 1.029 | Reg loss: 0.030 | Tree loss: 1.029 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 030 | Total loss: 0.990 | Reg loss: 0.030 | Tree loss: 0.990 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 030 | Total loss: 0.982 | Reg loss: 0.030 | Tree loss: 0.982 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 030 | Total loss: 1.002 | Reg loss: 0.030 | Tree loss: 1.002 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 030 | Total loss: 0.993 | Reg loss: 0.030 | Tree loss: 0.993 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 030 | Total loss: 0.970 | Reg loss: 0.030 | Tree loss: 0.970 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 030 | Total loss: 0.976 | Reg loss: 0.030 | Tree loss: 0.976 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 030 | Total loss: 0.970 | Reg loss: 0.030 | Tree loss: 0.970 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 030 | Total loss: 0.956 | Reg loss: 0.030 | Tree loss: 0.956 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 030 | Total loss: 0.937 | Reg loss: 0.030 | Tree loss: 0.937 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 030 | Total loss: 0.932 | Reg loss: 0.030 | Tree loss: 0.932 | Accuracy: 0.607422 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 030 | Total loss: 0.931 | Reg loss: 0.030 | Tree loss: 0.931 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 030 | Total loss: 0.939 | Reg loss: 0.030 | Tree loss: 0.939 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 030 | Total loss: 0.939 | Reg loss: 0.030 | Tree loss: 0.939 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 030 | Total loss: 0.920 | Reg loss: 0.030 | Tree loss: 0.920 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 030 | Total loss: 0.923 | Reg loss: 0.030 | Tree loss: 0.923 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 030 | Total loss: 0.911 | Reg loss: 0.030 | Tree loss: 0.911 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 030 | Total loss: 0.902 | Reg loss: 0.030 | Tree loss: 0.902 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 030 | Total loss: 0.895 | Reg loss: 0.030 | Tree loss: 0.895 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 69 | Batch: 029 / 030 | Total loss: 0.901 | Reg loss: 0.030 | Tree loss: 0.901 | Accuracy: 0.583333 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 70 | Batch: 000 / 030 | Total loss: 1.110 | Reg loss: 0.030 | Tree loss: 1.110 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 030 | Total loss: 1.103 | Reg loss: 0.030 | Tree loss: 1.103 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 030 | Total loss: 1.083 | Reg loss: 0.030 | Tree loss: 1.083 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 030 | Total loss: 1.071 | Reg loss: 0.030 | Tree loss: 1.071 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 030 | Total loss: 1.054 | Reg loss: 0.030 | Tree loss: 1.054 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 030 | Total loss: 1.057 | Reg loss: 0.030 | Tree loss: 1.057 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 030 | Total loss: 1.074 | Reg loss: 0.030 | Tree loss: 1.074 | Accuracy: 0.525391 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 030 | Total loss: 1.027 | Reg loss: 0.030 | Tree loss: 1.027 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 030 | Total loss: 1.034 | Reg loss: 0.030 | Tree loss: 1.034 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 030 | Total loss: 1.012 | Reg loss: 0.030 | Tree loss: 1.012 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 030 | Total loss: 0.994 | Reg loss: 0.030 | Tree loss: 0.994 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 030 | Total loss: 1.015 | Reg loss: 0.030 | Tree loss: 1.015 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 030 | Total loss: 1.003 | Reg loss: 0.030 | Tree loss: 1.003 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 030 | Total loss: 0.993 | Reg loss: 0.030 | Tree loss: 0.993 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 030 | Total loss: 0.961 | Reg loss: 0.030 | Tree loss: 0.961 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 030 | Total loss: 0.957 | Reg loss: 0.030 | Tree loss: 0.957 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 030 | Total loss: 0.958 | Reg loss: 0.030 | Tree loss: 0.958 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 030 | Total loss: 0.957 | Reg loss: 0.030 | Tree loss: 0.957 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 030 | Total loss: 0.950 | Reg loss: 0.030 | Tree loss: 0.950 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 030 | Total loss: 0.927 | Reg loss: 0.030 | Tree loss: 0.927 | Accuracy: 0.607422 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 030 | Total loss: 0.927 | Reg loss: 0.030 | Tree loss: 0.927 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 030 | Total loss: 0.954 | Reg loss: 0.030 | Tree loss: 0.954 | Accuracy: 0.529297 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 030 | Total loss: 0.939 | Reg loss: 0.030 | Tree loss: 0.939 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 030 | Total loss: 0.917 | Reg loss: 0.030 | Tree loss: 0.917 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 030 | Total loss: 0.914 | Reg loss: 0.030 | Tree loss: 0.914 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 030 | Total loss: 0.907 | Reg loss: 0.030 | Tree loss: 0.907 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 030 | Total loss: 0.908 | Reg loss: 0.030 | Tree loss: 0.908 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 030 | Total loss: 0.907 | Reg loss: 0.030 | Tree loss: 0.907 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 030 | Total loss: 0.900 | Reg loss: 0.030 | Tree loss: 0.900 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 70 | Batch: 029 / 030 | Total loss: 0.909 | Reg loss: 0.030 | Tree loss: 0.909 | Accuracy: 0.601852 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 71 | Batch: 000 / 030 | Total loss: 1.087 | Reg loss: 0.029 | Tree loss: 1.087 | Accuracy: 0.605469 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 030 | Total loss: 1.075 | Reg loss: 0.029 | Tree loss: 1.075 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 030 | Total loss: 1.068 | Reg loss: 0.029 | Tree loss: 1.068 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 030 | Total loss: 1.081 | Reg loss: 0.029 | Tree loss: 1.081 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 030 | Total loss: 1.044 | Reg loss: 0.029 | Tree loss: 1.044 | Accuracy: 0.609375 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 030 | Total loss: 1.067 | Reg loss: 0.029 | Tree loss: 1.067 | Accuracy: 0.550781 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 006 / 030 | Total loss: 1.046 | Reg loss: 0.029 | Tree loss: 1.046 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 030 | Total loss: 1.024 | Reg loss: 0.029 | Tree loss: 1.024 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 030 | Total loss: 1.024 | Reg loss: 0.030 | Tree loss: 1.024 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 030 | Total loss: 1.014 | Reg loss: 0.030 | Tree loss: 1.014 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 030 | Total loss: 1.026 | Reg loss: 0.030 | Tree loss: 1.026 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 030 | Total loss: 1.005 | Reg loss: 0.030 | Tree loss: 1.005 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 030 | Total loss: 0.975 | Reg loss: 0.030 | Tree loss: 0.975 | Accuracy: 0.607422 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 030 | Total loss: 0.988 | Reg loss: 0.030 | Tree loss: 0.988 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 030 | Total loss: 0.978 | Reg loss: 0.030 | Tree loss: 0.978 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 030 | Total loss: 0.960 | Reg loss: 0.030 | Tree loss: 0.960 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 030 | Total loss: 0.945 | Reg loss: 0.030 | Tree loss: 0.945 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 030 | Total loss: 0.949 | Reg loss: 0.030 | Tree loss: 0.949 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 030 | Total loss: 0.955 | Reg loss: 0.030 | Tree loss: 0.955 | Accuracy: 0.552734 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 030 | Total loss: 0.940 | Reg loss: 0.030 | Tree loss: 0.940 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 030 | Total loss: 0.925 | Reg loss: 0.030 | Tree loss: 0.925 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 030 | Total loss: 0.925 | Reg loss: 0.030 | Tree loss: 0.925 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 030 | Total loss: 0.923 | Reg loss: 0.030 | Tree loss: 0.923 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 030 | Total loss: 0.908 | Reg loss: 0.030 | Tree loss: 0.908 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 030 | Total loss: 0.896 | Reg loss: 0.030 | Tree loss: 0.896 | Accuracy: 0.623047 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 030 | Total loss: 0.919 | Reg loss: 0.030 | Tree loss: 0.919 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 030 | Total loss: 0.903 | Reg loss: 0.030 | Tree loss: 0.903 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 030 | Total loss: 0.916 | Reg loss: 0.030 | Tree loss: 0.916 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 030 | Total loss: 0.899 | Reg loss: 0.030 | Tree loss: 0.899 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 71 | Batch: 029 / 030 | Total loss: 0.920 | Reg loss: 0.030 | Tree loss: 0.920 | Accuracy: 0.537037 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 72 | Batch: 000 / 030 | Total loss: 1.124 | Reg loss: 0.029 | Tree loss: 1.124 | Accuracy: 0.537109 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 030 | Total loss: 1.086 | Reg loss: 0.029 | Tree loss: 1.086 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 030 | Total loss: 1.072 | Reg loss: 0.029 | Tree loss: 1.072 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 030 | Total loss: 1.087 | Reg loss: 0.029 | Tree loss: 1.087 | Accuracy: 0.527344 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 030 | Total loss: 1.048 | Reg loss: 0.029 | Tree loss: 1.048 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 030 | Total loss: 1.038 | Reg loss: 0.029 | Tree loss: 1.038 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 030 | Total loss: 1.024 | Reg loss: 0.029 | Tree loss: 1.024 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 030 | Total loss: 1.021 | Reg loss: 0.029 | Tree loss: 1.021 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 030 | Total loss: 1.031 | Reg loss: 0.029 | Tree loss: 1.031 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 030 | Total loss: 0.983 | Reg loss: 0.029 | Tree loss: 0.983 | Accuracy: 0.613281 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 030 | Total loss: 0.995 | Reg loss: 0.029 | Tree loss: 0.995 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 030 | Total loss: 1.002 | Reg loss: 0.029 | Tree loss: 1.002 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 030 | Total loss: 0.991 | Reg loss: 0.029 | Tree loss: 0.991 | Accuracy: 0.546875 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 030 | Total loss: 0.971 | Reg loss: 0.030 | Tree loss: 0.971 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 030 | Total loss: 0.967 | Reg loss: 0.030 | Tree loss: 0.967 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 030 | Total loss: 0.950 | Reg loss: 0.030 | Tree loss: 0.950 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 030 | Total loss: 0.948 | Reg loss: 0.030 | Tree loss: 0.948 | Accuracy: 0.605469 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 030 | Total loss: 0.942 | Reg loss: 0.030 | Tree loss: 0.942 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 030 | Total loss: 0.923 | Reg loss: 0.030 | Tree loss: 0.923 | Accuracy: 0.607422 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 030 | Total loss: 0.933 | Reg loss: 0.030 | Tree loss: 0.933 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 030 | Total loss: 0.949 | Reg loss: 0.030 | Tree loss: 0.949 | Accuracy: 0.539062 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 030 | Total loss: 0.927 | Reg loss: 0.030 | Tree loss: 0.927 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 030 | Total loss: 0.925 | Reg loss: 0.030 | Tree loss: 0.925 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 030 | Total loss: 0.899 | Reg loss: 0.030 | Tree loss: 0.899 | Accuracy: 0.619141 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 030 | Total loss: 0.890 | Reg loss: 0.030 | Tree loss: 0.890 | Accuracy: 0.607422 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 030 | Total loss: 0.910 | Reg loss: 0.030 | Tree loss: 0.910 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 030 | Total loss: 0.897 | Reg loss: 0.030 | Tree loss: 0.897 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 030 | Total loss: 0.898 | Reg loss: 0.030 | Tree loss: 0.898 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 030 | Total loss: 0.898 | Reg loss: 0.030 | Tree loss: 0.898 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 72 | Batch: 029 / 030 | Total loss: 0.927 | Reg loss: 0.030 | Tree loss: 0.927 | Accuracy: 0.490741 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 73 | Batch: 000 / 030 | Total loss: 1.089 | Reg loss: 0.029 | Tree loss: 1.089 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 030 | Total loss: 1.088 | Reg loss: 0.029 | Tree loss: 1.088 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 030 | Total loss: 1.060 | Reg loss: 0.029 | Tree loss: 1.060 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 030 | Total loss: 1.072 | Reg loss: 0.029 | Tree loss: 1.072 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 030 | Total loss: 1.053 | Reg loss: 0.029 | Tree loss: 1.053 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 030 | Total loss: 1.021 | Reg loss: 0.029 | Tree loss: 1.021 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 030 | Total loss: 1.038 | Reg loss: 0.029 | Tree loss: 1.038 | Accuracy: 0.544922 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 030 | Total loss: 1.018 | Reg loss: 0.029 | Tree loss: 1.018 | Accuracy: 0.609375 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 030 | Total loss: 1.021 | Reg loss: 0.029 | Tree loss: 1.021 | Accuracy: 0.548828 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 009 / 030 | Total loss: 0.996 | Reg loss: 0.029 | Tree loss: 0.996 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 030 | Total loss: 0.999 | Reg loss: 0.029 | Tree loss: 0.999 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 030 | Total loss: 0.997 | Reg loss: 0.029 | Tree loss: 0.997 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 030 | Total loss: 0.972 | Reg loss: 0.029 | Tree loss: 0.972 | Accuracy: 0.595703 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 030 | Total loss: 0.975 | Reg loss: 0.029 | Tree loss: 0.975 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 030 | Total loss: 0.958 | Reg loss: 0.029 | Tree loss: 0.958 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 030 | Total loss: 0.961 | Reg loss: 0.029 | Tree loss: 0.961 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 030 | Total loss: 0.934 | Reg loss: 0.029 | Tree loss: 0.934 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 030 | Total loss: 0.942 | Reg loss: 0.030 | Tree loss: 0.942 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 030 | Total loss: 0.933 | Reg loss: 0.030 | Tree loss: 0.933 | Accuracy: 0.583984 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 030 | Total loss: 0.935 | Reg loss: 0.030 | Tree loss: 0.935 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 030 | Total loss: 0.932 | Reg loss: 0.030 | Tree loss: 0.932 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 030 | Total loss: 0.925 | Reg loss: 0.030 | Tree loss: 0.925 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 030 | Total loss: 0.896 | Reg loss: 0.030 | Tree loss: 0.896 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 030 | Total loss: 0.904 | Reg loss: 0.030 | Tree loss: 0.904 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 030 | Total loss: 0.903 | Reg loss: 0.030 | Tree loss: 0.903 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 030 | Total loss: 0.889 | Reg loss: 0.030 | Tree loss: 0.889 | Accuracy: 0.607422 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 030 | Total loss: 0.885 | Reg loss: 0.030 | Tree loss: 0.885 | Accuracy: 0.605469 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 030 | Total loss: 0.922 | Reg loss: 0.030 | Tree loss: 0.922 | Accuracy: 0.525391 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 030 | Total loss: 0.887 | Reg loss: 0.030 | Tree loss: 0.887 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 73 | Batch: 029 / 030 | Total loss: 0.901 | Reg loss: 0.030 | Tree loss: 0.901 | Accuracy: 0.564815 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 74 | Batch: 000 / 030 | Total loss: 1.091 | Reg loss: 0.029 | Tree loss: 1.091 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 030 | Total loss: 1.055 | Reg loss: 0.029 | Tree loss: 1.055 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 030 | Total loss: 1.061 | Reg loss: 0.029 | Tree loss: 1.061 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 030 | Total loss: 1.050 | Reg loss: 0.029 | Tree loss: 1.050 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 030 | Total loss: 1.045 | Reg loss: 0.029 | Tree loss: 1.045 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 030 | Total loss: 1.037 | Reg loss: 0.029 | Tree loss: 1.037 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 030 | Total loss: 1.016 | Reg loss: 0.029 | Tree loss: 1.016 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 030 | Total loss: 1.024 | Reg loss: 0.029 | Tree loss: 1.024 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 030 | Total loss: 1.006 | Reg loss: 0.029 | Tree loss: 1.006 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 030 | Total loss: 1.000 | Reg loss: 0.029 | Tree loss: 1.000 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 030 | Total loss: 0.990 | Reg loss: 0.029 | Tree loss: 0.990 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 030 | Total loss: 0.986 | Reg loss: 0.029 | Tree loss: 0.986 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 030 | Total loss: 0.985 | Reg loss: 0.029 | Tree loss: 0.985 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 030 | Total loss: 0.955 | Reg loss: 0.029 | Tree loss: 0.955 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 030 | Total loss: 0.959 | Reg loss: 0.029 | Tree loss: 0.959 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 030 | Total loss: 0.955 | Reg loss: 0.029 | Tree loss: 0.955 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 030 | Total loss: 0.946 | Reg loss: 0.029 | Tree loss: 0.946 | Accuracy: 0.572266 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 030 | Total loss: 0.925 | Reg loss: 0.029 | Tree loss: 0.925 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 030 | Total loss: 0.934 | Reg loss: 0.029 | Tree loss: 0.934 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 030 | Total loss: 0.928 | Reg loss: 0.029 | Tree loss: 0.928 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 030 | Total loss: 0.941 | Reg loss: 0.030 | Tree loss: 0.941 | Accuracy: 0.525391 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 030 | Total loss: 0.940 | Reg loss: 0.030 | Tree loss: 0.940 | Accuracy: 0.521484 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 030 | Total loss: 0.903 | Reg loss: 0.030 | Tree loss: 0.903 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 030 | Total loss: 0.903 | Reg loss: 0.030 | Tree loss: 0.903 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 030 | Total loss: 0.893 | Reg loss: 0.030 | Tree loss: 0.893 | Accuracy: 0.587891 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 030 | Total loss: 0.886 | Reg loss: 0.030 | Tree loss: 0.886 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 030 | Total loss: 0.883 | Reg loss: 0.030 | Tree loss: 0.883 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 030 | Total loss: 0.889 | Reg loss: 0.030 | Tree loss: 0.889 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 030 | Total loss: 0.882 | Reg loss: 0.030 | Tree loss: 0.882 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 74 | Batch: 029 / 030 | Total loss: 0.923 | Reg loss: 0.030 | Tree loss: 0.923 | Accuracy: 0.490741 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 75 | Batch: 000 / 030 | Total loss: 1.083 | Reg loss: 0.029 | Tree loss: 1.083 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 030 | Total loss: 1.083 | Reg loss: 0.029 | Tree loss: 1.083 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 030 | Total loss: 1.058 | Reg loss: 0.029 | Tree loss: 1.058 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 030 | Total loss: 1.045 | Reg loss: 0.029 | Tree loss: 1.045 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 030 | Total loss: 1.058 | Reg loss: 0.029 | Tree loss: 1.058 | Accuracy: 0.546875 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 030 | Total loss: 1.014 | Reg loss: 0.029 | Tree loss: 1.014 | Accuracy: 0.617188 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 030 | Total loss: 1.031 | Reg loss: 0.029 | Tree loss: 1.031 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 030 | Total loss: 1.030 | Reg loss: 0.029 | Tree loss: 1.030 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 030 | Total loss: 0.998 | Reg loss: 0.029 | Tree loss: 0.998 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 030 | Total loss: 0.990 | Reg loss: 0.029 | Tree loss: 0.990 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 030 | Total loss: 0.966 | Reg loss: 0.029 | Tree loss: 0.966 | Accuracy: 0.609375 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 030 | Total loss: 0.976 | Reg loss: 0.029 | Tree loss: 0.976 | Accuracy: 0.580078 | 0.359 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 012 / 030 | Total loss: 0.974 | Reg loss: 0.029 | Tree loss: 0.974 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 030 | Total loss: 0.977 | Reg loss: 0.029 | Tree loss: 0.977 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 030 | Total loss: 0.954 | Reg loss: 0.029 | Tree loss: 0.954 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 030 | Total loss: 0.936 | Reg loss: 0.029 | Tree loss: 0.936 | Accuracy: 0.589844 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 030 | Total loss: 0.947 | Reg loss: 0.029 | Tree loss: 0.947 | Accuracy: 0.548828 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 030 | Total loss: 0.936 | Reg loss: 0.029 | Tree loss: 0.936 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 030 | Total loss: 0.891 | Reg loss: 0.029 | Tree loss: 0.891 | Accuracy: 0.648438 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 030 | Total loss: 0.925 | Reg loss: 0.029 | Tree loss: 0.925 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 030 | Total loss: 0.894 | Reg loss: 0.029 | Tree loss: 0.894 | Accuracy: 0.619141 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 030 | Total loss: 0.906 | Reg loss: 0.029 | Tree loss: 0.906 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 030 | Total loss: 0.911 | Reg loss: 0.029 | Tree loss: 0.911 | Accuracy: 0.546875 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 030 | Total loss: 0.917 | Reg loss: 0.030 | Tree loss: 0.917 | Accuracy: 0.541016 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 030 | Total loss: 0.908 | Reg loss: 0.030 | Tree loss: 0.908 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 030 | Total loss: 0.900 | Reg loss: 0.030 | Tree loss: 0.900 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 030 | Total loss: 0.886 | Reg loss: 0.030 | Tree loss: 0.886 | Accuracy: 0.593750 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 030 | Total loss: 0.897 | Reg loss: 0.030 | Tree loss: 0.897 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 030 | Total loss: 0.881 | Reg loss: 0.030 | Tree loss: 0.881 | Accuracy: 0.582031 | 0.359 sec/iter\n",
      "Epoch: 75 | Batch: 029 / 030 | Total loss: 0.868 | Reg loss: 0.030 | Tree loss: 0.868 | Accuracy: 0.583333 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 76 | Batch: 000 / 030 | Total loss: 1.077 | Reg loss: 0.029 | Tree loss: 1.077 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 030 | Total loss: 1.066 | Reg loss: 0.029 | Tree loss: 1.066 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 030 | Total loss: 1.062 | Reg loss: 0.029 | Tree loss: 1.062 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 030 | Total loss: 1.069 | Reg loss: 0.029 | Tree loss: 1.069 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 030 | Total loss: 1.044 | Reg loss: 0.029 | Tree loss: 1.044 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 030 | Total loss: 1.043 | Reg loss: 0.029 | Tree loss: 1.043 | Accuracy: 0.542969 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 030 | Total loss: 1.025 | Reg loss: 0.029 | Tree loss: 1.025 | Accuracy: 0.570312 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 030 | Total loss: 1.014 | Reg loss: 0.029 | Tree loss: 1.014 | Accuracy: 0.574219 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 030 | Total loss: 1.015 | Reg loss: 0.029 | Tree loss: 1.015 | Accuracy: 0.560547 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 030 | Total loss: 0.990 | Reg loss: 0.029 | Tree loss: 0.990 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 030 | Total loss: 0.970 | Reg loss: 0.029 | Tree loss: 0.970 | Accuracy: 0.603516 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 030 | Total loss: 0.974 | Reg loss: 0.029 | Tree loss: 0.974 | Accuracy: 0.550781 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 030 | Total loss: 0.963 | Reg loss: 0.029 | Tree loss: 0.963 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 030 | Total loss: 0.949 | Reg loss: 0.029 | Tree loss: 0.949 | Accuracy: 0.585938 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 030 | Total loss: 0.936 | Reg loss: 0.029 | Tree loss: 0.936 | Accuracy: 0.597656 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 030 | Total loss: 0.947 | Reg loss: 0.029 | Tree loss: 0.947 | Accuracy: 0.566406 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 030 | Total loss: 0.928 | Reg loss: 0.029 | Tree loss: 0.928 | Accuracy: 0.578125 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 030 | Total loss: 0.925 | Reg loss: 0.029 | Tree loss: 0.925 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 030 | Total loss: 0.931 | Reg loss: 0.029 | Tree loss: 0.931 | Accuracy: 0.554688 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 030 | Total loss: 0.918 | Reg loss: 0.029 | Tree loss: 0.918 | Accuracy: 0.576172 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 030 | Total loss: 0.903 | Reg loss: 0.029 | Tree loss: 0.903 | Accuracy: 0.601562 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 030 | Total loss: 0.907 | Reg loss: 0.029 | Tree loss: 0.907 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 030 | Total loss: 0.891 | Reg loss: 0.029 | Tree loss: 0.891 | Accuracy: 0.591797 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 030 | Total loss: 0.898 | Reg loss: 0.029 | Tree loss: 0.898 | Accuracy: 0.580078 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 030 | Total loss: 0.875 | Reg loss: 0.029 | Tree loss: 0.875 | Accuracy: 0.617188 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 030 | Total loss: 0.892 | Reg loss: 0.029 | Tree loss: 0.892 | Accuracy: 0.558594 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 030 | Total loss: 0.892 | Reg loss: 0.029 | Tree loss: 0.892 | Accuracy: 0.562500 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 030 | Total loss: 0.891 | Reg loss: 0.030 | Tree loss: 0.891 | Accuracy: 0.564453 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 030 | Total loss: 0.865 | Reg loss: 0.030 | Tree loss: 0.865 | Accuracy: 0.599609 | 0.359 sec/iter\n",
      "Epoch: 76 | Batch: 029 / 030 | Total loss: 0.879 | Reg loss: 0.030 | Tree loss: 0.879 | Accuracy: 0.574074 | 0.359 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 77 | Batch: 000 / 030 | Total loss: 1.075 | Reg loss: 0.029 | Tree loss: 1.075 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 030 | Total loss: 1.062 | Reg loss: 0.029 | Tree loss: 1.062 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 030 | Total loss: 1.058 | Reg loss: 0.029 | Tree loss: 1.058 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 030 | Total loss: 1.030 | Reg loss: 0.029 | Tree loss: 1.030 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 030 | Total loss: 1.038 | Reg loss: 0.029 | Tree loss: 1.038 | Accuracy: 0.568359 | 0.359 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 030 | Total loss: 1.037 | Reg loss: 0.029 | Tree loss: 1.037 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 030 | Total loss: 1.012 | Reg loss: 0.029 | Tree loss: 1.012 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 030 | Total loss: 1.009 | Reg loss: 0.029 | Tree loss: 1.009 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 030 | Total loss: 0.976 | Reg loss: 0.029 | Tree loss: 0.976 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 030 | Total loss: 0.992 | Reg loss: 0.029 | Tree loss: 0.992 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 030 | Total loss: 0.975 | Reg loss: 0.029 | Tree loss: 0.975 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 030 | Total loss: 0.969 | Reg loss: 0.029 | Tree loss: 0.969 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 030 | Total loss: 0.968 | Reg loss: 0.029 | Tree loss: 0.968 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 030 | Total loss: 0.963 | Reg loss: 0.029 | Tree loss: 0.963 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 030 | Total loss: 0.950 | Reg loss: 0.029 | Tree loss: 0.950 | Accuracy: 0.572266 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 015 / 030 | Total loss: 0.928 | Reg loss: 0.029 | Tree loss: 0.928 | Accuracy: 0.617188 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 030 | Total loss: 0.933 | Reg loss: 0.029 | Tree loss: 0.933 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 030 | Total loss: 0.906 | Reg loss: 0.029 | Tree loss: 0.906 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 030 | Total loss: 0.915 | Reg loss: 0.029 | Tree loss: 0.915 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 030 | Total loss: 0.929 | Reg loss: 0.029 | Tree loss: 0.929 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 030 | Total loss: 0.896 | Reg loss: 0.029 | Tree loss: 0.896 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 030 | Total loss: 0.894 | Reg loss: 0.029 | Tree loss: 0.894 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 030 | Total loss: 0.912 | Reg loss: 0.029 | Tree loss: 0.912 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 030 | Total loss: 0.908 | Reg loss: 0.029 | Tree loss: 0.908 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 030 | Total loss: 0.885 | Reg loss: 0.029 | Tree loss: 0.885 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 030 | Total loss: 0.877 | Reg loss: 0.029 | Tree loss: 0.877 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 030 | Total loss: 0.895 | Reg loss: 0.029 | Tree loss: 0.895 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 030 | Total loss: 0.887 | Reg loss: 0.029 | Tree loss: 0.887 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 030 | Total loss: 0.878 | Reg loss: 0.029 | Tree loss: 0.878 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 77 | Batch: 029 / 030 | Total loss: 0.890 | Reg loss: 0.029 | Tree loss: 0.890 | Accuracy: 0.500000 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 78 | Batch: 000 / 030 | Total loss: 1.066 | Reg loss: 0.029 | Tree loss: 1.066 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 030 | Total loss: 1.084 | Reg loss: 0.029 | Tree loss: 1.084 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 030 | Total loss: 1.059 | Reg loss: 0.029 | Tree loss: 1.059 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 030 | Total loss: 1.021 | Reg loss: 0.029 | Tree loss: 1.021 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 030 | Total loss: 1.034 | Reg loss: 0.029 | Tree loss: 1.034 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 030 | Total loss: 1.000 | Reg loss: 0.029 | Tree loss: 1.000 | Accuracy: 0.619141 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 030 | Total loss: 0.993 | Reg loss: 0.029 | Tree loss: 0.993 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 030 | Total loss: 1.000 | Reg loss: 0.029 | Tree loss: 1.000 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 030 | Total loss: 0.970 | Reg loss: 0.029 | Tree loss: 0.970 | Accuracy: 0.625000 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 030 | Total loss: 0.980 | Reg loss: 0.029 | Tree loss: 0.980 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 030 | Total loss: 0.977 | Reg loss: 0.029 | Tree loss: 0.977 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 030 | Total loss: 0.965 | Reg loss: 0.029 | Tree loss: 0.965 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 030 | Total loss: 0.953 | Reg loss: 0.029 | Tree loss: 0.953 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 030 | Total loss: 0.950 | Reg loss: 0.029 | Tree loss: 0.950 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 030 | Total loss: 0.949 | Reg loss: 0.029 | Tree loss: 0.949 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 030 | Total loss: 0.940 | Reg loss: 0.029 | Tree loss: 0.940 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 030 | Total loss: 0.931 | Reg loss: 0.029 | Tree loss: 0.931 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 030 | Total loss: 0.920 | Reg loss: 0.029 | Tree loss: 0.920 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 030 | Total loss: 0.916 | Reg loss: 0.029 | Tree loss: 0.916 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 030 | Total loss: 0.901 | Reg loss: 0.029 | Tree loss: 0.901 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 030 | Total loss: 0.925 | Reg loss: 0.029 | Tree loss: 0.925 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 030 | Total loss: 0.910 | Reg loss: 0.029 | Tree loss: 0.910 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 030 | Total loss: 0.895 | Reg loss: 0.029 | Tree loss: 0.895 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 030 | Total loss: 0.902 | Reg loss: 0.029 | Tree loss: 0.902 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 030 | Total loss: 0.885 | Reg loss: 0.029 | Tree loss: 0.885 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 030 | Total loss: 0.895 | Reg loss: 0.029 | Tree loss: 0.895 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 030 | Total loss: 0.886 | Reg loss: 0.029 | Tree loss: 0.886 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 030 | Total loss: 0.889 | Reg loss: 0.029 | Tree loss: 0.889 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 030 | Total loss: 0.882 | Reg loss: 0.029 | Tree loss: 0.882 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 78 | Batch: 029 / 030 | Total loss: 0.849 | Reg loss: 0.029 | Tree loss: 0.849 | Accuracy: 0.611111 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 79 | Batch: 000 / 030 | Total loss: 1.068 | Reg loss: 0.029 | Tree loss: 1.068 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 030 | Total loss: 1.056 | Reg loss: 0.029 | Tree loss: 1.056 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 030 | Total loss: 1.038 | Reg loss: 0.029 | Tree loss: 1.038 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 030 | Total loss: 1.055 | Reg loss: 0.029 | Tree loss: 1.055 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 030 | Total loss: 1.013 | Reg loss: 0.029 | Tree loss: 1.013 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 030 | Total loss: 1.005 | Reg loss: 0.029 | Tree loss: 1.005 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 030 | Total loss: 0.997 | Reg loss: 0.029 | Tree loss: 0.997 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 030 | Total loss: 1.012 | Reg loss: 0.029 | Tree loss: 1.012 | Accuracy: 0.539062 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 030 | Total loss: 0.984 | Reg loss: 0.029 | Tree loss: 0.984 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 030 | Total loss: 1.005 | Reg loss: 0.029 | Tree loss: 1.005 | Accuracy: 0.533203 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 030 | Total loss: 0.987 | Reg loss: 0.029 | Tree loss: 0.987 | Accuracy: 0.529297 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 030 | Total loss: 0.964 | Reg loss: 0.029 | Tree loss: 0.964 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 030 | Total loss: 0.965 | Reg loss: 0.029 | Tree loss: 0.965 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 030 | Total loss: 0.947 | Reg loss: 0.029 | Tree loss: 0.947 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 030 | Total loss: 0.922 | Reg loss: 0.029 | Tree loss: 0.922 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 030 | Total loss: 0.936 | Reg loss: 0.029 | Tree loss: 0.936 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 030 | Total loss: 0.912 | Reg loss: 0.029 | Tree loss: 0.912 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 030 | Total loss: 0.929 | Reg loss: 0.029 | Tree loss: 0.929 | Accuracy: 0.568359 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 018 / 030 | Total loss: 0.913 | Reg loss: 0.029 | Tree loss: 0.913 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 030 | Total loss: 0.899 | Reg loss: 0.029 | Tree loss: 0.899 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 030 | Total loss: 0.904 | Reg loss: 0.029 | Tree loss: 0.904 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 030 | Total loss: 0.916 | Reg loss: 0.029 | Tree loss: 0.916 | Accuracy: 0.527344 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 030 | Total loss: 0.888 | Reg loss: 0.029 | Tree loss: 0.888 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 030 | Total loss: 0.879 | Reg loss: 0.029 | Tree loss: 0.879 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 030 | Total loss: 0.880 | Reg loss: 0.029 | Tree loss: 0.880 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 030 | Total loss: 0.877 | Reg loss: 0.029 | Tree loss: 0.877 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 030 | Total loss: 0.875 | Reg loss: 0.029 | Tree loss: 0.875 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 030 | Total loss: 0.875 | Reg loss: 0.029 | Tree loss: 0.875 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 030 | Total loss: 0.863 | Reg loss: 0.029 | Tree loss: 0.863 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 79 | Batch: 029 / 030 | Total loss: 0.860 | Reg loss: 0.029 | Tree loss: 0.860 | Accuracy: 0.611111 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 80 | Batch: 000 / 030 | Total loss: 1.057 | Reg loss: 0.028 | Tree loss: 1.057 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 030 | Total loss: 1.055 | Reg loss: 0.028 | Tree loss: 1.055 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 030 | Total loss: 1.026 | Reg loss: 0.028 | Tree loss: 1.026 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 030 | Total loss: 1.052 | Reg loss: 0.028 | Tree loss: 1.052 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 030 | Total loss: 1.014 | Reg loss: 0.028 | Tree loss: 1.014 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 030 | Total loss: 1.027 | Reg loss: 0.028 | Tree loss: 1.027 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 030 | Total loss: 1.005 | Reg loss: 0.028 | Tree loss: 1.005 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 030 | Total loss: 1.005 | Reg loss: 0.028 | Tree loss: 1.005 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 030 | Total loss: 0.980 | Reg loss: 0.028 | Tree loss: 0.980 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 030 | Total loss: 0.969 | Reg loss: 0.029 | Tree loss: 0.969 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 030 | Total loss: 0.964 | Reg loss: 0.029 | Tree loss: 0.964 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 030 | Total loss: 0.952 | Reg loss: 0.029 | Tree loss: 0.952 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 030 | Total loss: 0.966 | Reg loss: 0.029 | Tree loss: 0.966 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 030 | Total loss: 0.955 | Reg loss: 0.029 | Tree loss: 0.955 | Accuracy: 0.539062 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 030 | Total loss: 0.936 | Reg loss: 0.029 | Tree loss: 0.936 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 030 | Total loss: 0.919 | Reg loss: 0.029 | Tree loss: 0.919 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 030 | Total loss: 0.921 | Reg loss: 0.029 | Tree loss: 0.921 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 030 | Total loss: 0.923 | Reg loss: 0.029 | Tree loss: 0.923 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 030 | Total loss: 0.904 | Reg loss: 0.029 | Tree loss: 0.904 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 030 | Total loss: 0.900 | Reg loss: 0.029 | Tree loss: 0.900 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 030 | Total loss: 0.892 | Reg loss: 0.029 | Tree loss: 0.892 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 030 | Total loss: 0.890 | Reg loss: 0.029 | Tree loss: 0.890 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 030 | Total loss: 0.905 | Reg loss: 0.029 | Tree loss: 0.905 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 030 | Total loss: 0.887 | Reg loss: 0.029 | Tree loss: 0.887 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 030 | Total loss: 0.865 | Reg loss: 0.029 | Tree loss: 0.865 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 030 | Total loss: 0.894 | Reg loss: 0.029 | Tree loss: 0.894 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 030 | Total loss: 0.862 | Reg loss: 0.029 | Tree loss: 0.862 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 030 | Total loss: 0.859 | Reg loss: 0.029 | Tree loss: 0.859 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 028 / 030 | Total loss: 0.879 | Reg loss: 0.029 | Tree loss: 0.879 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 80 | Batch: 029 / 030 | Total loss: 0.860 | Reg loss: 0.029 | Tree loss: 0.860 | Accuracy: 0.601852 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 81 | Batch: 000 / 030 | Total loss: 1.052 | Reg loss: 0.028 | Tree loss: 1.052 | Accuracy: 0.625000 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 030 | Total loss: 1.058 | Reg loss: 0.028 | Tree loss: 1.058 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 030 | Total loss: 1.053 | Reg loss: 0.028 | Tree loss: 1.053 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 030 | Total loss: 1.034 | Reg loss: 0.028 | Tree loss: 1.034 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 030 | Total loss: 1.026 | Reg loss: 0.028 | Tree loss: 1.026 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 030 | Total loss: 1.006 | Reg loss: 0.028 | Tree loss: 1.006 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 030 | Total loss: 1.003 | Reg loss: 0.028 | Tree loss: 1.003 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 030 | Total loss: 0.987 | Reg loss: 0.028 | Tree loss: 0.987 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 030 | Total loss: 0.973 | Reg loss: 0.028 | Tree loss: 0.973 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 030 | Total loss: 0.984 | Reg loss: 0.028 | Tree loss: 0.984 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 030 | Total loss: 0.963 | Reg loss: 0.028 | Tree loss: 0.963 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 030 | Total loss: 0.954 | Reg loss: 0.028 | Tree loss: 0.954 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 030 | Total loss: 0.963 | Reg loss: 0.029 | Tree loss: 0.963 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 030 | Total loss: 0.948 | Reg loss: 0.029 | Tree loss: 0.948 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 030 | Total loss: 0.928 | Reg loss: 0.029 | Tree loss: 0.928 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 030 | Total loss: 0.947 | Reg loss: 0.029 | Tree loss: 0.947 | Accuracy: 0.525391 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 030 | Total loss: 0.918 | Reg loss: 0.029 | Tree loss: 0.918 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 030 | Total loss: 0.913 | Reg loss: 0.029 | Tree loss: 0.913 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 030 | Total loss: 0.909 | Reg loss: 0.029 | Tree loss: 0.909 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 030 | Total loss: 0.887 | Reg loss: 0.029 | Tree loss: 0.887 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 030 | Total loss: 0.893 | Reg loss: 0.029 | Tree loss: 0.893 | Accuracy: 0.578125 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Batch: 021 / 030 | Total loss: 0.886 | Reg loss: 0.029 | Tree loss: 0.886 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 030 | Total loss: 0.893 | Reg loss: 0.029 | Tree loss: 0.893 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 030 | Total loss: 0.878 | Reg loss: 0.029 | Tree loss: 0.878 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 030 | Total loss: 0.878 | Reg loss: 0.029 | Tree loss: 0.878 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 030 | Total loss: 0.857 | Reg loss: 0.029 | Tree loss: 0.857 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 030 | Total loss: 0.867 | Reg loss: 0.029 | Tree loss: 0.867 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 030 | Total loss: 0.868 | Reg loss: 0.029 | Tree loss: 0.868 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 030 | Total loss: 0.858 | Reg loss: 0.029 | Tree loss: 0.858 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 81 | Batch: 029 / 030 | Total loss: 0.831 | Reg loss: 0.029 | Tree loss: 0.831 | Accuracy: 0.638889 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 82 | Batch: 000 / 030 | Total loss: 1.048 | Reg loss: 0.028 | Tree loss: 1.048 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 030 | Total loss: 1.034 | Reg loss: 0.028 | Tree loss: 1.034 | Accuracy: 0.626953 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 030 | Total loss: 1.027 | Reg loss: 0.028 | Tree loss: 1.027 | Accuracy: 0.603516 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 030 | Total loss: 1.035 | Reg loss: 0.028 | Tree loss: 1.035 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 030 | Total loss: 1.017 | Reg loss: 0.028 | Tree loss: 1.017 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 030 | Total loss: 1.008 | Reg loss: 0.028 | Tree loss: 1.008 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 030 | Total loss: 1.005 | Reg loss: 0.028 | Tree loss: 1.005 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 030 | Total loss: 0.975 | Reg loss: 0.028 | Tree loss: 0.975 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 030 | Total loss: 0.964 | Reg loss: 0.028 | Tree loss: 0.964 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 030 | Total loss: 0.968 | Reg loss: 0.028 | Tree loss: 0.968 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 030 | Total loss: 0.950 | Reg loss: 0.028 | Tree loss: 0.950 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 030 | Total loss: 0.960 | Reg loss: 0.028 | Tree loss: 0.960 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 030 | Total loss: 0.935 | Reg loss: 0.028 | Tree loss: 0.935 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 030 | Total loss: 0.959 | Reg loss: 0.028 | Tree loss: 0.959 | Accuracy: 0.521484 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 030 | Total loss: 0.933 | Reg loss: 0.029 | Tree loss: 0.933 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 030 | Total loss: 0.920 | Reg loss: 0.029 | Tree loss: 0.920 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 030 | Total loss: 0.898 | Reg loss: 0.029 | Tree loss: 0.898 | Accuracy: 0.626953 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 030 | Total loss: 0.923 | Reg loss: 0.029 | Tree loss: 0.923 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 030 | Total loss: 0.917 | Reg loss: 0.029 | Tree loss: 0.917 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 030 | Total loss: 0.914 | Reg loss: 0.029 | Tree loss: 0.914 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 030 | Total loss: 0.903 | Reg loss: 0.029 | Tree loss: 0.903 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 030 | Total loss: 0.882 | Reg loss: 0.029 | Tree loss: 0.882 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 030 | Total loss: 0.865 | Reg loss: 0.029 | Tree loss: 0.865 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 030 | Total loss: 0.881 | Reg loss: 0.029 | Tree loss: 0.881 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 030 | Total loss: 0.894 | Reg loss: 0.029 | Tree loss: 0.894 | Accuracy: 0.525391 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 030 | Total loss: 0.883 | Reg loss: 0.029 | Tree loss: 0.883 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 030 | Total loss: 0.862 | Reg loss: 0.029 | Tree loss: 0.862 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 030 | Total loss: 0.872 | Reg loss: 0.029 | Tree loss: 0.872 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 030 | Total loss: 0.867 | Reg loss: 0.029 | Tree loss: 0.867 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 82 | Batch: 029 / 030 | Total loss: 0.857 | Reg loss: 0.029 | Tree loss: 0.857 | Accuracy: 0.564815 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 83 | Batch: 000 / 030 | Total loss: 1.060 | Reg loss: 0.028 | Tree loss: 1.060 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 030 | Total loss: 1.027 | Reg loss: 0.028 | Tree loss: 1.027 | Accuracy: 0.619141 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 030 | Total loss: 1.039 | Reg loss: 0.028 | Tree loss: 1.039 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 030 | Total loss: 1.051 | Reg loss: 0.028 | Tree loss: 1.051 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 030 | Total loss: 1.007 | Reg loss: 0.028 | Tree loss: 1.007 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 030 | Total loss: 0.991 | Reg loss: 0.028 | Tree loss: 0.991 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 030 | Total loss: 0.990 | Reg loss: 0.028 | Tree loss: 0.990 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 030 | Total loss: 0.975 | Reg loss: 0.028 | Tree loss: 0.975 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 030 | Total loss: 0.982 | Reg loss: 0.028 | Tree loss: 0.982 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 030 | Total loss: 0.971 | Reg loss: 0.028 | Tree loss: 0.971 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 030 | Total loss: 0.946 | Reg loss: 0.028 | Tree loss: 0.946 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 030 | Total loss: 0.967 | Reg loss: 0.028 | Tree loss: 0.967 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 030 | Total loss: 0.939 | Reg loss: 0.028 | Tree loss: 0.939 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 030 | Total loss: 0.929 | Reg loss: 0.028 | Tree loss: 0.929 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 030 | Total loss: 0.935 | Reg loss: 0.028 | Tree loss: 0.935 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 030 | Total loss: 0.912 | Reg loss: 0.028 | Tree loss: 0.912 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 030 | Total loss: 0.927 | Reg loss: 0.029 | Tree loss: 0.927 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 030 | Total loss: 0.898 | Reg loss: 0.029 | Tree loss: 0.898 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 030 | Total loss: 0.912 | Reg loss: 0.029 | Tree loss: 0.912 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 030 | Total loss: 0.891 | Reg loss: 0.029 | Tree loss: 0.891 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 030 | Total loss: 0.894 | Reg loss: 0.029 | Tree loss: 0.894 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 030 | Total loss: 0.881 | Reg loss: 0.029 | Tree loss: 0.881 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 030 | Total loss: 0.891 | Reg loss: 0.029 | Tree loss: 0.891 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 030 | Total loss: 0.877 | Reg loss: 0.029 | Tree loss: 0.877 | Accuracy: 0.580078 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 024 / 030 | Total loss: 0.866 | Reg loss: 0.029 | Tree loss: 0.866 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 030 | Total loss: 0.872 | Reg loss: 0.029 | Tree loss: 0.872 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 030 | Total loss: 0.875 | Reg loss: 0.029 | Tree loss: 0.875 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 030 | Total loss: 0.881 | Reg loss: 0.029 | Tree loss: 0.881 | Accuracy: 0.535156 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 030 | Total loss: 0.847 | Reg loss: 0.029 | Tree loss: 0.847 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 83 | Batch: 029 / 030 | Total loss: 0.820 | Reg loss: 0.029 | Tree loss: 0.820 | Accuracy: 0.648148 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 84 | Batch: 000 / 030 | Total loss: 1.062 | Reg loss: 0.028 | Tree loss: 1.062 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 030 | Total loss: 1.052 | Reg loss: 0.028 | Tree loss: 1.052 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 030 | Total loss: 1.043 | Reg loss: 0.028 | Tree loss: 1.043 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 030 | Total loss: 1.025 | Reg loss: 0.028 | Tree loss: 1.025 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 030 | Total loss: 1.018 | Reg loss: 0.028 | Tree loss: 1.018 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 030 | Total loss: 0.987 | Reg loss: 0.028 | Tree loss: 0.987 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 030 | Total loss: 0.997 | Reg loss: 0.028 | Tree loss: 0.997 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 030 | Total loss: 0.974 | Reg loss: 0.028 | Tree loss: 0.974 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 030 | Total loss: 0.973 | Reg loss: 0.028 | Tree loss: 0.973 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 030 | Total loss: 0.965 | Reg loss: 0.028 | Tree loss: 0.965 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 030 | Total loss: 0.957 | Reg loss: 0.028 | Tree loss: 0.957 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 030 | Total loss: 0.931 | Reg loss: 0.028 | Tree loss: 0.931 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 030 | Total loss: 0.950 | Reg loss: 0.028 | Tree loss: 0.950 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 030 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.619141 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 030 | Total loss: 0.923 | Reg loss: 0.028 | Tree loss: 0.923 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 030 | Total loss: 0.923 | Reg loss: 0.028 | Tree loss: 0.923 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 030 | Total loss: 0.903 | Reg loss: 0.028 | Tree loss: 0.903 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 030 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 030 | Total loss: 0.906 | Reg loss: 0.029 | Tree loss: 0.906 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 030 | Total loss: 0.901 | Reg loss: 0.029 | Tree loss: 0.901 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 030 | Total loss: 0.901 | Reg loss: 0.029 | Tree loss: 0.901 | Accuracy: 0.539062 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 030 | Total loss: 0.883 | Reg loss: 0.029 | Tree loss: 0.883 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 030 | Total loss: 0.885 | Reg loss: 0.029 | Tree loss: 0.885 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 030 | Total loss: 0.871 | Reg loss: 0.029 | Tree loss: 0.871 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 030 | Total loss: 0.873 | Reg loss: 0.029 | Tree loss: 0.873 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 030 | Total loss: 0.870 | Reg loss: 0.029 | Tree loss: 0.870 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 030 | Total loss: 0.850 | Reg loss: 0.029 | Tree loss: 0.850 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 030 | Total loss: 0.853 | Reg loss: 0.029 | Tree loss: 0.853 | Accuracy: 0.595703 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 030 | Total loss: 0.858 | Reg loss: 0.029 | Tree loss: 0.858 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 84 | Batch: 029 / 030 | Total loss: 0.854 | Reg loss: 0.029 | Tree loss: 0.854 | Accuracy: 0.546296 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 85 | Batch: 000 / 030 | Total loss: 1.022 | Reg loss: 0.028 | Tree loss: 1.022 | Accuracy: 0.634766 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 030 | Total loss: 1.025 | Reg loss: 0.028 | Tree loss: 1.025 | Accuracy: 0.617188 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 030 | Total loss: 1.041 | Reg loss: 0.028 | Tree loss: 1.041 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 030 | Total loss: 1.022 | Reg loss: 0.028 | Tree loss: 1.022 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 030 | Total loss: 1.010 | Reg loss: 0.028 | Tree loss: 1.010 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 030 | Total loss: 1.012 | Reg loss: 0.028 | Tree loss: 1.012 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 030 | Total loss: 0.990 | Reg loss: 0.028 | Tree loss: 0.990 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 030 | Total loss: 0.968 | Reg loss: 0.028 | Tree loss: 0.968 | Accuracy: 0.605469 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 030 | Total loss: 0.969 | Reg loss: 0.028 | Tree loss: 0.969 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 030 | Total loss: 0.967 | Reg loss: 0.028 | Tree loss: 0.967 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 030 | Total loss: 0.957 | Reg loss: 0.028 | Tree loss: 0.957 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 030 | Total loss: 0.945 | Reg loss: 0.028 | Tree loss: 0.945 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 030 | Total loss: 0.942 | Reg loss: 0.028 | Tree loss: 0.942 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 030 | Total loss: 0.938 | Reg loss: 0.028 | Tree loss: 0.938 | Accuracy: 0.554688 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 030 | Total loss: 0.912 | Reg loss: 0.028 | Tree loss: 0.912 | Accuracy: 0.589844 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 030 | Total loss: 0.910 | Reg loss: 0.028 | Tree loss: 0.910 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 030 | Total loss: 0.929 | Reg loss: 0.028 | Tree loss: 0.929 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 030 | Total loss: 0.912 | Reg loss: 0.028 | Tree loss: 0.912 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 030 | Total loss: 0.901 | Reg loss: 0.028 | Tree loss: 0.901 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 030 | Total loss: 0.910 | Reg loss: 0.028 | Tree loss: 0.910 | Accuracy: 0.539062 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 030 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 030 | Total loss: 0.879 | Reg loss: 0.029 | Tree loss: 0.879 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 030 | Total loss: 0.893 | Reg loss: 0.029 | Tree loss: 0.893 | Accuracy: 0.529297 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 030 | Total loss: 0.879 | Reg loss: 0.029 | Tree loss: 0.879 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 030 | Total loss: 0.870 | Reg loss: 0.029 | Tree loss: 0.870 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 030 | Total loss: 0.854 | Reg loss: 0.029 | Tree loss: 0.854 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 030 | Total loss: 0.845 | Reg loss: 0.029 | Tree loss: 0.845 | Accuracy: 0.595703 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 027 / 030 | Total loss: 0.844 | Reg loss: 0.029 | Tree loss: 0.844 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 030 | Total loss: 0.853 | Reg loss: 0.029 | Tree loss: 0.853 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 85 | Batch: 029 / 030 | Total loss: 0.833 | Reg loss: 0.029 | Tree loss: 0.833 | Accuracy: 0.629630 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 86 | Batch: 000 / 030 | Total loss: 1.046 | Reg loss: 0.028 | Tree loss: 1.046 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 030 | Total loss: 1.037 | Reg loss: 0.028 | Tree loss: 1.037 | Accuracy: 0.603516 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 030 | Total loss: 1.030 | Reg loss: 0.028 | Tree loss: 1.030 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 030 | Total loss: 0.998 | Reg loss: 0.028 | Tree loss: 0.998 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 030 | Total loss: 0.989 | Reg loss: 0.028 | Tree loss: 0.989 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 030 | Total loss: 1.003 | Reg loss: 0.028 | Tree loss: 1.003 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 030 | Total loss: 1.001 | Reg loss: 0.028 | Tree loss: 1.001 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 030 | Total loss: 0.983 | Reg loss: 0.028 | Tree loss: 0.983 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 030 | Total loss: 0.965 | Reg loss: 0.028 | Tree loss: 0.965 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 030 | Total loss: 0.967 | Reg loss: 0.028 | Tree loss: 0.967 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 030 | Total loss: 0.950 | Reg loss: 0.028 | Tree loss: 0.950 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 030 | Total loss: 0.948 | Reg loss: 0.028 | Tree loss: 0.948 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 030 | Total loss: 0.923 | Reg loss: 0.028 | Tree loss: 0.923 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 030 | Total loss: 0.934 | Reg loss: 0.028 | Tree loss: 0.934 | Accuracy: 0.542969 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 030 | Total loss: 0.927 | Reg loss: 0.028 | Tree loss: 0.927 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 030 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.621094 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 030 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 030 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 030 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.599609 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 030 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 030 | Total loss: 0.910 | Reg loss: 0.028 | Tree loss: 0.910 | Accuracy: 0.539062 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 030 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 030 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 030 | Total loss: 0.877 | Reg loss: 0.029 | Tree loss: 0.877 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 030 | Total loss: 0.852 | Reg loss: 0.029 | Tree loss: 0.852 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 030 | Total loss: 0.854 | Reg loss: 0.029 | Tree loss: 0.854 | Accuracy: 0.583984 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 030 | Total loss: 0.858 | Reg loss: 0.029 | Tree loss: 0.858 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 030 | Total loss: 0.859 | Reg loss: 0.029 | Tree loss: 0.859 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 030 | Total loss: 0.859 | Reg loss: 0.029 | Tree loss: 0.859 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 86 | Batch: 029 / 030 | Total loss: 0.808 | Reg loss: 0.029 | Tree loss: 0.808 | Accuracy: 0.657407 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 87 | Batch: 000 / 030 | Total loss: 1.040 | Reg loss: 0.028 | Tree loss: 1.040 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 030 | Total loss: 1.036 | Reg loss: 0.028 | Tree loss: 1.036 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 030 | Total loss: 1.020 | Reg loss: 0.028 | Tree loss: 1.020 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 030 | Total loss: 1.015 | Reg loss: 0.028 | Tree loss: 1.015 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 030 | Total loss: 1.012 | Reg loss: 0.028 | Tree loss: 1.012 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 030 | Total loss: 1.003 | Reg loss: 0.028 | Tree loss: 1.003 | Accuracy: 0.558594 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 030 | Total loss: 1.001 | Reg loss: 0.028 | Tree loss: 1.001 | Accuracy: 0.552734 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 030 | Total loss: 0.968 | Reg loss: 0.028 | Tree loss: 0.968 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 030 | Total loss: 0.960 | Reg loss: 0.028 | Tree loss: 0.960 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 030 | Total loss: 0.965 | Reg loss: 0.028 | Tree loss: 0.965 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 030 | Total loss: 0.956 | Reg loss: 0.028 | Tree loss: 0.956 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 030 | Total loss: 0.939 | Reg loss: 0.028 | Tree loss: 0.939 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 030 | Total loss: 0.935 | Reg loss: 0.028 | Tree loss: 0.935 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 030 | Total loss: 0.954 | Reg loss: 0.028 | Tree loss: 0.954 | Accuracy: 0.517578 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 030 | Total loss: 0.923 | Reg loss: 0.028 | Tree loss: 0.923 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 030 | Total loss: 0.911 | Reg loss: 0.028 | Tree loss: 0.911 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 030 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 030 | Total loss: 0.913 | Reg loss: 0.028 | Tree loss: 0.913 | Accuracy: 0.544922 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 030 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 030 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 030 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 030 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 030 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 030 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 030 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 030 | Total loss: 0.860 | Reg loss: 0.029 | Tree loss: 0.860 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 030 | Total loss: 0.845 | Reg loss: 0.029 | Tree loss: 0.845 | Accuracy: 0.603516 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 030 | Total loss: 0.834 | Reg loss: 0.029 | Tree loss: 0.834 | Accuracy: 0.611328 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 030 | Total loss: 0.844 | Reg loss: 0.029 | Tree loss: 0.844 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 87 | Batch: 029 / 030 | Total loss: 0.862 | Reg loss: 0.029 | Tree loss: 0.862 | Accuracy: 0.564815 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 88 | Batch: 000 / 030 | Total loss: 1.041 | Reg loss: 0.028 | Tree loss: 1.041 | Accuracy: 0.537109 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 030 | Total loss: 1.040 | Reg loss: 0.028 | Tree loss: 1.040 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 030 | Total loss: 1.023 | Reg loss: 0.028 | Tree loss: 1.023 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 030 | Total loss: 1.007 | Reg loss: 0.028 | Tree loss: 1.007 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 030 | Total loss: 1.027 | Reg loss: 0.028 | Tree loss: 1.027 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 030 | Total loss: 0.997 | Reg loss: 0.028 | Tree loss: 0.997 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 030 | Total loss: 0.988 | Reg loss: 0.028 | Tree loss: 0.988 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 030 | Total loss: 0.972 | Reg loss: 0.028 | Tree loss: 0.972 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 030 | Total loss: 0.972 | Reg loss: 0.028 | Tree loss: 0.972 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 030 | Total loss: 0.971 | Reg loss: 0.028 | Tree loss: 0.971 | Accuracy: 0.525391 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 030 | Total loss: 0.942 | Reg loss: 0.028 | Tree loss: 0.942 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 030 | Total loss: 0.922 | Reg loss: 0.028 | Tree loss: 0.922 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 030 | Total loss: 0.923 | Reg loss: 0.028 | Tree loss: 0.923 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 030 | Total loss: 0.920 | Reg loss: 0.028 | Tree loss: 0.920 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 030 | Total loss: 0.908 | Reg loss: 0.028 | Tree loss: 0.908 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 030 | Total loss: 0.908 | Reg loss: 0.028 | Tree loss: 0.908 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 030 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 030 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 030 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 030 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 030 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.548828 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 030 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 030 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 030 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 030 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.613281 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 030 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 030 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.568359 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 030 | Total loss: 0.851 | Reg loss: 0.029 | Tree loss: 0.851 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 030 | Total loss: 0.849 | Reg loss: 0.029 | Tree loss: 0.849 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 88 | Batch: 029 / 030 | Total loss: 0.825 | Reg loss: 0.029 | Tree loss: 0.825 | Accuracy: 0.629630 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 89 | Batch: 000 / 030 | Total loss: 1.050 | Reg loss: 0.028 | Tree loss: 1.050 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 030 | Total loss: 1.016 | Reg loss: 0.028 | Tree loss: 1.016 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 030 | Total loss: 1.032 | Reg loss: 0.028 | Tree loss: 1.032 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 030 | Total loss: 1.007 | Reg loss: 0.028 | Tree loss: 1.007 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 030 | Total loss: 1.008 | Reg loss: 0.028 | Tree loss: 1.008 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 030 | Total loss: 0.981 | Reg loss: 0.028 | Tree loss: 0.981 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 030 | Total loss: 0.984 | Reg loss: 0.028 | Tree loss: 0.984 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 030 | Total loss: 0.985 | Reg loss: 0.028 | Tree loss: 0.985 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 030 | Total loss: 0.969 | Reg loss: 0.028 | Tree loss: 0.969 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 030 | Total loss: 0.955 | Reg loss: 0.028 | Tree loss: 0.955 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 030 | Total loss: 0.942 | Reg loss: 0.028 | Tree loss: 0.942 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 030 | Total loss: 0.932 | Reg loss: 0.028 | Tree loss: 0.932 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 030 | Total loss: 0.934 | Reg loss: 0.028 | Tree loss: 0.934 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 030 | Total loss: 0.915 | Reg loss: 0.028 | Tree loss: 0.915 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 030 | Total loss: 0.908 | Reg loss: 0.028 | Tree loss: 0.908 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 030 | Total loss: 0.906 | Reg loss: 0.028 | Tree loss: 0.906 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 030 | Total loss: 0.902 | Reg loss: 0.028 | Tree loss: 0.902 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 030 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 030 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 030 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 030 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 030 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.550781 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 030 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 030 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 030 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.593750 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 030 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.607422 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 030 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.541016 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 030 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 030 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 89 | Batch: 029 / 030 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.648148 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 90 | Batch: 000 / 030 | Total loss: 1.042 | Reg loss: 0.028 | Tree loss: 1.042 | Accuracy: 0.552734 | 0.36 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 001 / 030 | Total loss: 1.019 | Reg loss: 0.028 | Tree loss: 1.019 | Accuracy: 0.585938 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 030 | Total loss: 0.999 | Reg loss: 0.028 | Tree loss: 0.999 | Accuracy: 0.609375 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 030 | Total loss: 1.019 | Reg loss: 0.028 | Tree loss: 1.019 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 030 | Total loss: 0.997 | Reg loss: 0.028 | Tree loss: 0.997 | Accuracy: 0.580078 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 030 | Total loss: 0.997 | Reg loss: 0.028 | Tree loss: 0.997 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 030 | Total loss: 0.991 | Reg loss: 0.028 | Tree loss: 0.991 | Accuracy: 0.556641 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 030 | Total loss: 0.982 | Reg loss: 0.028 | Tree loss: 0.982 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 030 | Total loss: 0.947 | Reg loss: 0.028 | Tree loss: 0.947 | Accuracy: 0.601562 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 030 | Total loss: 0.961 | Reg loss: 0.028 | Tree loss: 0.961 | Accuracy: 0.560547 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 030 | Total loss: 0.943 | Reg loss: 0.028 | Tree loss: 0.943 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 030 | Total loss: 0.933 | Reg loss: 0.028 | Tree loss: 0.933 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 030 | Total loss: 0.921 | Reg loss: 0.028 | Tree loss: 0.921 | Accuracy: 0.576172 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 030 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 030 | Total loss: 0.895 | Reg loss: 0.028 | Tree loss: 0.895 | Accuracy: 0.605469 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 030 | Total loss: 0.904 | Reg loss: 0.028 | Tree loss: 0.904 | Accuracy: 0.562500 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 030 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.587891 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 030 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.574219 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 030 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.566406 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 030 | Total loss: 0.903 | Reg loss: 0.028 | Tree loss: 0.903 | Accuracy: 0.531250 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 030 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.570312 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 030 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.591797 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 030 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.546875 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 030 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.582031 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 030 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.572266 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 030 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 030 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.597656 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 030 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.564453 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 030 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.578125 | 0.36 sec/iter\n",
      "Epoch: 90 | Batch: 029 / 030 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.611111 | 0.36 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 91 | Batch: 000 / 030 | Total loss: 1.057 | Reg loss: 0.028 | Tree loss: 1.057 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 030 | Total loss: 1.013 | Reg loss: 0.028 | Tree loss: 1.013 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 030 | Total loss: 1.046 | Reg loss: 0.028 | Tree loss: 1.046 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 030 | Total loss: 1.029 | Reg loss: 0.028 | Tree loss: 1.029 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 030 | Total loss: 1.007 | Reg loss: 0.028 | Tree loss: 1.007 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 030 | Total loss: 0.980 | Reg loss: 0.028 | Tree loss: 0.980 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 030 | Total loss: 0.974 | Reg loss: 0.028 | Tree loss: 0.974 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 030 | Total loss: 0.949 | Reg loss: 0.028 | Tree loss: 0.949 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 030 | Total loss: 0.967 | Reg loss: 0.028 | Tree loss: 0.967 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 030 | Total loss: 0.947 | Reg loss: 0.028 | Tree loss: 0.947 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 030 | Total loss: 0.936 | Reg loss: 0.028 | Tree loss: 0.936 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 030 | Total loss: 0.930 | Reg loss: 0.028 | Tree loss: 0.930 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 030 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 030 | Total loss: 0.923 | Reg loss: 0.028 | Tree loss: 0.923 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 030 | Total loss: 0.905 | Reg loss: 0.028 | Tree loss: 0.905 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 030 | Total loss: 0.911 | Reg loss: 0.028 | Tree loss: 0.911 | Accuracy: 0.539062 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 030 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 030 | Total loss: 0.902 | Reg loss: 0.028 | Tree loss: 0.902 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 030 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 030 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 030 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 030 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 030 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 030 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 030 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 030 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 030 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.609375 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 030 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 030 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 91 | Batch: 029 / 030 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.574074 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 92 | Batch: 000 / 030 | Total loss: 1.038 | Reg loss: 0.027 | Tree loss: 1.038 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 030 | Total loss: 1.030 | Reg loss: 0.027 | Tree loss: 1.030 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 030 | Total loss: 1.024 | Reg loss: 0.027 | Tree loss: 1.024 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 030 | Total loss: 0.997 | Reg loss: 0.027 | Tree loss: 0.997 | Accuracy: 0.587891 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 004 / 030 | Total loss: 1.004 | Reg loss: 0.027 | Tree loss: 1.004 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 030 | Total loss: 0.987 | Reg loss: 0.027 | Tree loss: 0.987 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 030 | Total loss: 0.972 | Reg loss: 0.028 | Tree loss: 0.972 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 030 | Total loss: 0.968 | Reg loss: 0.028 | Tree loss: 0.968 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 030 | Total loss: 0.941 | Reg loss: 0.028 | Tree loss: 0.941 | Accuracy: 0.613281 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 030 | Total loss: 0.936 | Reg loss: 0.028 | Tree loss: 0.936 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 030 | Total loss: 0.922 | Reg loss: 0.028 | Tree loss: 0.922 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 030 | Total loss: 0.926 | Reg loss: 0.028 | Tree loss: 0.926 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 030 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 030 | Total loss: 0.916 | Reg loss: 0.028 | Tree loss: 0.916 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 030 | Total loss: 0.900 | Reg loss: 0.028 | Tree loss: 0.900 | Accuracy: 0.615234 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 030 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 030 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 030 | Total loss: 0.907 | Reg loss: 0.028 | Tree loss: 0.907 | Accuracy: 0.531250 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 030 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 030 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.628906 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 030 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 030 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 030 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 030 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 030 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 030 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.525391 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 030 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 030 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 030 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 92 | Batch: 029 / 030 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.666667 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 93 | Batch: 000 / 030 | Total loss: 1.042 | Reg loss: 0.027 | Tree loss: 1.042 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 030 | Total loss: 1.026 | Reg loss: 0.027 | Tree loss: 1.026 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 030 | Total loss: 1.010 | Reg loss: 0.027 | Tree loss: 1.010 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 030 | Total loss: 0.996 | Reg loss: 0.027 | Tree loss: 0.996 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 030 | Total loss: 0.987 | Reg loss: 0.027 | Tree loss: 0.987 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 030 | Total loss: 0.993 | Reg loss: 0.027 | Tree loss: 0.993 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 030 | Total loss: 0.978 | Reg loss: 0.027 | Tree loss: 0.978 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 030 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 0.617188 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 030 | Total loss: 0.932 | Reg loss: 0.028 | Tree loss: 0.932 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 030 | Total loss: 0.957 | Reg loss: 0.028 | Tree loss: 0.957 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 030 | Total loss: 0.948 | Reg loss: 0.028 | Tree loss: 0.948 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 030 | Total loss: 0.916 | Reg loss: 0.028 | Tree loss: 0.916 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 030 | Total loss: 0.934 | Reg loss: 0.028 | Tree loss: 0.934 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 030 | Total loss: 0.925 | Reg loss: 0.028 | Tree loss: 0.925 | Accuracy: 0.535156 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 030 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 030 | Total loss: 0.908 | Reg loss: 0.028 | Tree loss: 0.908 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 030 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 030 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 030 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 030 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 030 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 030 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 030 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 030 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 030 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 030 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 030 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 030 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 030 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 93 | Batch: 029 / 030 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.629630 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 94 | Batch: 000 / 030 | Total loss: 1.033 | Reg loss: 0.027 | Tree loss: 1.033 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 030 | Total loss: 1.013 | Reg loss: 0.027 | Tree loss: 1.013 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 030 | Total loss: 1.031 | Reg loss: 0.027 | Tree loss: 1.031 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 030 | Total loss: 1.012 | Reg loss: 0.027 | Tree loss: 1.012 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 030 | Total loss: 0.997 | Reg loss: 0.027 | Tree loss: 0.997 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 030 | Total loss: 0.987 | Reg loss: 0.027 | Tree loss: 0.987 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 030 | Total loss: 0.975 | Reg loss: 0.027 | Tree loss: 0.975 | Accuracy: 0.558594 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 007 / 030 | Total loss: 0.966 | Reg loss: 0.027 | Tree loss: 0.966 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 030 | Total loss: 0.953 | Reg loss: 0.027 | Tree loss: 0.953 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 030 | Total loss: 0.932 | Reg loss: 0.027 | Tree loss: 0.932 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 030 | Total loss: 0.930 | Reg loss: 0.028 | Tree loss: 0.930 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 030 | Total loss: 0.909 | Reg loss: 0.028 | Tree loss: 0.909 | Accuracy: 0.619141 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 030 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 030 | Total loss: 0.904 | Reg loss: 0.028 | Tree loss: 0.904 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 030 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 030 | Total loss: 0.905 | Reg loss: 0.028 | Tree loss: 0.905 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 030 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 030 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.625000 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 030 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 030 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 030 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 030 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.617188 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 030 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 030 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.613281 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 030 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 030 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 030 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 030 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.521484 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 030 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 94 | Batch: 029 / 030 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.592593 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 95 | Batch: 000 / 030 | Total loss: 1.011 | Reg loss: 0.027 | Tree loss: 1.011 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 030 | Total loss: 1.025 | Reg loss: 0.027 | Tree loss: 1.025 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 030 | Total loss: 1.002 | Reg loss: 0.027 | Tree loss: 1.002 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 030 | Total loss: 0.998 | Reg loss: 0.027 | Tree loss: 0.998 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 030 | Total loss: 0.976 | Reg loss: 0.027 | Tree loss: 0.976 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 030 | Total loss: 0.987 | Reg loss: 0.027 | Tree loss: 0.987 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 030 | Total loss: 0.975 | Reg loss: 0.027 | Tree loss: 0.975 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 030 | Total loss: 0.961 | Reg loss: 0.027 | Tree loss: 0.961 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 030 | Total loss: 0.954 | Reg loss: 0.027 | Tree loss: 0.954 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 030 | Total loss: 0.950 | Reg loss: 0.027 | Tree loss: 0.950 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 030 | Total loss: 0.949 | Reg loss: 0.027 | Tree loss: 0.949 | Accuracy: 0.533203 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 030 | Total loss: 0.928 | Reg loss: 0.027 | Tree loss: 0.928 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 030 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 030 | Total loss: 0.908 | Reg loss: 0.028 | Tree loss: 0.908 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 030 | Total loss: 0.903 | Reg loss: 0.028 | Tree loss: 0.903 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 030 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 030 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 030 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 030 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 030 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 030 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.613281 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 030 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 030 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 030 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 030 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 030 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 030 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 030 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 030 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.525391 | 0.361 sec/iter\n",
      "Epoch: 95 | Batch: 029 / 030 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.620370 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 96 | Batch: 000 / 030 | Total loss: 1.045 | Reg loss: 0.027 | Tree loss: 1.045 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 030 | Total loss: 1.019 | Reg loss: 0.027 | Tree loss: 1.019 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 030 | Total loss: 1.001 | Reg loss: 0.027 | Tree loss: 1.001 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 030 | Total loss: 1.006 | Reg loss: 0.027 | Tree loss: 1.006 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 030 | Total loss: 0.990 | Reg loss: 0.027 | Tree loss: 0.990 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 030 | Total loss: 0.975 | Reg loss: 0.027 | Tree loss: 0.975 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 030 | Total loss: 0.986 | Reg loss: 0.027 | Tree loss: 0.986 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 030 | Total loss: 0.949 | Reg loss: 0.027 | Tree loss: 0.949 | Accuracy: 0.619141 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 030 | Total loss: 0.962 | Reg loss: 0.027 | Tree loss: 0.962 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 030 | Total loss: 0.916 | Reg loss: 0.027 | Tree loss: 0.916 | Accuracy: 0.611328 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Batch: 010 / 030 | Total loss: 0.919 | Reg loss: 0.027 | Tree loss: 0.919 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 030 | Total loss: 0.922 | Reg loss: 0.027 | Tree loss: 0.922 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 030 | Total loss: 0.916 | Reg loss: 0.027 | Tree loss: 0.916 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 030 | Total loss: 0.900 | Reg loss: 0.028 | Tree loss: 0.900 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 030 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 030 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 030 | Total loss: 0.910 | Reg loss: 0.028 | Tree loss: 0.910 | Accuracy: 0.517578 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 030 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.636719 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 030 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 030 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.611328 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 030 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 030 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 030 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 030 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 030 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.544922 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 030 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 030 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 030 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 030 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 96 | Batch: 029 / 030 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.592593 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 97 | Batch: 000 / 030 | Total loss: 1.042 | Reg loss: 0.027 | Tree loss: 1.042 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 030 | Total loss: 1.007 | Reg loss: 0.027 | Tree loss: 1.007 | Accuracy: 0.625000 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 030 | Total loss: 1.033 | Reg loss: 0.027 | Tree loss: 1.033 | Accuracy: 0.529297 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 030 | Total loss: 0.995 | Reg loss: 0.027 | Tree loss: 0.995 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 030 | Total loss: 0.996 | Reg loss: 0.027 | Tree loss: 0.996 | Accuracy: 0.552734 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 030 | Total loss: 0.983 | Reg loss: 0.027 | Tree loss: 0.983 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 030 | Total loss: 0.970 | Reg loss: 0.027 | Tree loss: 0.970 | Accuracy: 0.554688 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 030 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 030 | Total loss: 0.963 | Reg loss: 0.027 | Tree loss: 0.963 | Accuracy: 0.533203 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 030 | Total loss: 0.932 | Reg loss: 0.027 | Tree loss: 0.932 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 030 | Total loss: 0.924 | Reg loss: 0.027 | Tree loss: 0.924 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 030 | Total loss: 0.910 | Reg loss: 0.027 | Tree loss: 0.910 | Accuracy: 0.611328 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 030 | Total loss: 0.925 | Reg loss: 0.027 | Tree loss: 0.925 | Accuracy: 0.537109 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 030 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.576172 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 030 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.609375 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 030 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 030 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 030 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 030 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 030 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.537109 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 030 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 030 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 030 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 030 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.556641 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 030 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 030 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 030 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.548828 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 030 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 030 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.597656 | 0.361 sec/iter\n",
      "Epoch: 97 | Batch: 029 / 030 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.583333 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 98 | Batch: 000 / 030 | Total loss: 1.026 | Reg loss: 0.027 | Tree loss: 1.026 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 030 | Total loss: 1.018 | Reg loss: 0.027 | Tree loss: 1.018 | Accuracy: 0.589844 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 030 | Total loss: 1.021 | Reg loss: 0.027 | Tree loss: 1.021 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 030 | Total loss: 0.993 | Reg loss: 0.027 | Tree loss: 0.993 | Accuracy: 0.605469 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 030 | Total loss: 0.995 | Reg loss: 0.027 | Tree loss: 0.995 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 030 | Total loss: 0.981 | Reg loss: 0.027 | Tree loss: 0.981 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 030 | Total loss: 0.975 | Reg loss: 0.027 | Tree loss: 0.975 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 030 | Total loss: 0.948 | Reg loss: 0.027 | Tree loss: 0.948 | Accuracy: 0.593750 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 030 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 0.566406 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 030 | Total loss: 0.926 | Reg loss: 0.027 | Tree loss: 0.926 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 030 | Total loss: 0.928 | Reg loss: 0.027 | Tree loss: 0.928 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 030 | Total loss: 0.927 | Reg loss: 0.027 | Tree loss: 0.927 | Accuracy: 0.542969 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 030 | Total loss: 0.902 | Reg loss: 0.027 | Tree loss: 0.902 | Accuracy: 0.580078 | 0.361 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Batch: 013 / 030 | Total loss: 0.917 | Reg loss: 0.027 | Tree loss: 0.917 | Accuracy: 0.541016 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 030 | Total loss: 0.913 | Reg loss: 0.027 | Tree loss: 0.913 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 030 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 030 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 030 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.544922 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 030 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.546875 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 030 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 030 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 030 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 030 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 030 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.607422 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 030 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.574219 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 030 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.582031 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 030 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.601562 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 030 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 030 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.570312 | 0.361 sec/iter\n",
      "Epoch: 98 | Batch: 029 / 030 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.462963 | 0.361 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 99 | Batch: 000 / 030 | Total loss: 1.019 | Reg loss: 0.027 | Tree loss: 1.019 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 030 | Total loss: 1.029 | Reg loss: 0.027 | Tree loss: 1.029 | Accuracy: 0.535156 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 030 | Total loss: 1.018 | Reg loss: 0.027 | Tree loss: 1.018 | Accuracy: 0.568359 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 030 | Total loss: 1.009 | Reg loss: 0.027 | Tree loss: 1.009 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 030 | Total loss: 0.991 | Reg loss: 0.027 | Tree loss: 0.991 | Accuracy: 0.542969 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 030 | Total loss: 0.987 | Reg loss: 0.027 | Tree loss: 0.987 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 030 | Total loss: 0.961 | Reg loss: 0.027 | Tree loss: 0.961 | Accuracy: 0.621094 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 030 | Total loss: 0.961 | Reg loss: 0.027 | Tree loss: 0.961 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 030 | Total loss: 0.953 | Reg loss: 0.027 | Tree loss: 0.953 | Accuracy: 0.562500 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 030 | Total loss: 0.919 | Reg loss: 0.027 | Tree loss: 0.919 | Accuracy: 0.595703 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 030 | Total loss: 0.919 | Reg loss: 0.027 | Tree loss: 0.919 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 030 | Total loss: 0.917 | Reg loss: 0.027 | Tree loss: 0.917 | Accuracy: 0.572266 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 030 | Total loss: 0.909 | Reg loss: 0.027 | Tree loss: 0.909 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 030 | Total loss: 0.897 | Reg loss: 0.027 | Tree loss: 0.897 | Accuracy: 0.603516 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 030 | Total loss: 0.876 | Reg loss: 0.027 | Tree loss: 0.876 | Accuracy: 0.580078 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 030 | Total loss: 0.875 | Reg loss: 0.027 | Tree loss: 0.875 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 030 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.578125 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 030 | Total loss: 0.895 | Reg loss: 0.028 | Tree loss: 0.895 | Accuracy: 0.537109 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 030 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.560547 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 030 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.585938 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 030 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.587891 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 030 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.550781 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 030 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.583984 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 030 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.564453 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 030 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.558594 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 030 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.542969 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 030 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.615234 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 030 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.599609 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 030 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.591797 | 0.361 sec/iter\n",
      "Epoch: 99 | Batch: 029 / 030 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.592593 | 0.361 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6df44a05804ba79d8e963f3bc35cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc262bc0ceb04146bef1170850c11dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb6e3cb686b42c78f4a7f30f1cef8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafea094cfa844dcba3d0d7d9bfdba46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.083333333333333\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 96\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/miniconda3/envs/rambo/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "11646\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "1668\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "1642\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "Average comprehensibility: 35.4375\n",
      "std comprehensibility: 4.887936894368966\n",
      "var comprehensibility: 23.891927083333332\n",
      "minimum comprehensibility: 20\n",
      "maximum comprehensibility: 42\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
