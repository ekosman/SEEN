{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 128\n",
    "tree_depth = 6\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.180225372314453 | KNN Loss: 6.229950904846191 | BCE Loss: 1.9502747058868408\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.183635711669922 | KNN Loss: 6.229683876037598 | BCE Loss: 1.9539520740509033\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.158685684204102 | KNN Loss: 6.2299699783325195 | BCE Loss: 1.928715467453003\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.198995590209961 | KNN Loss: 6.229693412780762 | BCE Loss: 1.9693019390106201\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.157793998718262 | KNN Loss: 6.229772567749023 | BCE Loss: 1.9280214309692383\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.0819673538208 | KNN Loss: 6.229701042175293 | BCE Loss: 1.8522663116455078\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.16765022277832 | KNN Loss: 6.229369640350342 | BCE Loss: 1.9382810592651367\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.128780364990234 | KNN Loss: 6.229136943817139 | BCE Loss: 1.8996435403823853\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.132796287536621 | KNN Loss: 6.228820323944092 | BCE Loss: 1.9039757251739502\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.113788604736328 | KNN Loss: 6.229063510894775 | BCE Loss: 1.8847246170043945\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.110812187194824 | KNN Loss: 6.228487014770508 | BCE Loss: 1.882325530052185\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.137700080871582 | KNN Loss: 6.228706359863281 | BCE Loss: 1.9089934825897217\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.117679595947266 | KNN Loss: 6.228760719299316 | BCE Loss: 1.8889187574386597\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.098260879516602 | KNN Loss: 6.228057384490967 | BCE Loss: 1.8702030181884766\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.07375431060791 | KNN Loss: 6.228106498718262 | BCE Loss: 1.8456480503082275\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.05919361114502 | KNN Loss: 6.227556228637695 | BCE Loss: 1.8316373825073242\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.084758758544922 | KNN Loss: 6.227345943450928 | BCE Loss: 1.8574128150939941\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.013344764709473 | KNN Loss: 6.22724723815918 | BCE Loss: 1.7860972881317139\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.075628280639648 | KNN Loss: 6.226711273193359 | BCE Loss: 1.8489174842834473\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.047163963317871 | KNN Loss: 6.2265424728393555 | BCE Loss: 1.8206217288970947\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.018184661865234 | KNN Loss: 6.226302146911621 | BCE Loss: 1.7918827533721924\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.105605125427246 | KNN Loss: 6.226337432861328 | BCE Loss: 1.879267692565918\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.053670883178711 | KNN Loss: 6.22534704208374 | BCE Loss: 1.8283236026763916\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.002121925354004 | KNN Loss: 6.225225925445557 | BCE Loss: 1.7768962383270264\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.993780136108398 | KNN Loss: 6.224924087524414 | BCE Loss: 1.768856167793274\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.977456569671631 | KNN Loss: 6.224950313568115 | BCE Loss: 1.7525062561035156\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.965802192687988 | KNN Loss: 6.223926544189453 | BCE Loss: 1.7418756484985352\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 8.0079345703125 | KNN Loss: 6.224411964416504 | BCE Loss: 1.7835230827331543\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.976837158203125 | KNN Loss: 6.222712993621826 | BCE Loss: 1.7541242837905884\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.9558024406433105 | KNN Loss: 6.222167491912842 | BCE Loss: 1.7336349487304688\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.953529357910156 | KNN Loss: 6.2212934494018555 | BCE Loss: 1.7322356700897217\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.924829959869385 | KNN Loss: 6.220674514770508 | BCE Loss: 1.704155445098877\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.916565895080566 | KNN Loss: 6.219700336456299 | BCE Loss: 1.6968657970428467\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.889915466308594 | KNN Loss: 6.220381736755371 | BCE Loss: 1.6695339679718018\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.901002407073975 | KNN Loss: 6.218313217163086 | BCE Loss: 1.6826890707015991\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.885530471801758 | KNN Loss: 6.216968059539795 | BCE Loss: 1.6685621738433838\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.885725021362305 | KNN Loss: 6.215542316436768 | BCE Loss: 1.670182466506958\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.888225555419922 | KNN Loss: 6.214858055114746 | BCE Loss: 1.6733677387237549\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.872860431671143 | KNN Loss: 6.214630126953125 | BCE Loss: 1.6582303047180176\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.812445640563965 | KNN Loss: 6.211764812469482 | BCE Loss: 1.6006807088851929\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.817808151245117 | KNN Loss: 6.209829807281494 | BCE Loss: 1.607978343963623\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.795560836791992 | KNN Loss: 6.208458423614502 | BCE Loss: 1.5871024131774902\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.791805267333984 | KNN Loss: 6.202757835388184 | BCE Loss: 1.5890474319458008\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.769377708435059 | KNN Loss: 6.203227996826172 | BCE Loss: 1.5661494731903076\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.764760971069336 | KNN Loss: 6.198914051055908 | BCE Loss: 1.5658469200134277\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.7387261390686035 | KNN Loss: 6.198803901672363 | BCE Loss: 1.5399221181869507\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.69614315032959 | KNN Loss: 6.193129062652588 | BCE Loss: 1.5030138492584229\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.704531192779541 | KNN Loss: 6.184452533721924 | BCE Loss: 1.5200786590576172\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.6623125076293945 | KNN Loss: 6.184164047241211 | BCE Loss: 1.4781486988067627\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.660201072692871 | KNN Loss: 6.175795555114746 | BCE Loss: 1.484405517578125\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.5863494873046875 | KNN Loss: 6.16657829284668 | BCE Loss: 1.4197709560394287\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.553379058837891 | KNN Loss: 6.1598591804504395 | BCE Loss: 1.3935198783874512\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.520147323608398 | KNN Loss: 6.153952121734619 | BCE Loss: 1.3661950826644897\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.551074981689453 | KNN Loss: 6.149037837982178 | BCE Loss: 1.4020373821258545\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.47991943359375 | KNN Loss: 6.136030197143555 | BCE Loss: 1.3438892364501953\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.464669227600098 | KNN Loss: 6.122841835021973 | BCE Loss: 1.341827630996704\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.436382293701172 | KNN Loss: 6.108124732971191 | BCE Loss: 1.32825767993927\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.435862064361572 | KNN Loss: 6.098511695861816 | BCE Loss: 1.3373503684997559\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.370490074157715 | KNN Loss: 6.076130390167236 | BCE Loss: 1.294359564781189\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.360875129699707 | KNN Loss: 6.0590949058532715 | BCE Loss: 1.3017802238464355\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.340692520141602 | KNN Loss: 6.048898696899414 | BCE Loss: 1.2917935848236084\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.278305530548096 | KNN Loss: 6.026393890380859 | BCE Loss: 1.2519117593765259\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.20713996887207 | KNN Loss: 5.987210273742676 | BCE Loss: 1.2199294567108154\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.12870979309082 | KNN Loss: 5.971230506896973 | BCE Loss: 1.1574790477752686\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.12025260925293 | KNN Loss: 5.934870719909668 | BCE Loss: 1.1853816509246826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.07936954498291 | KNN Loss: 5.899815082550049 | BCE Loss: 1.1795545816421509\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.034160614013672 | KNN Loss: 5.883945465087891 | BCE Loss: 1.1502151489257812\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 6.996936798095703 | KNN Loss: 5.832756519317627 | BCE Loss: 1.1641805171966553\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.9601149559021 | KNN Loss: 5.7869720458984375 | BCE Loss: 1.1731430292129517\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 6.9008636474609375 | KNN Loss: 5.745500564575195 | BCE Loss: 1.155362844467163\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 6.829466819763184 | KNN Loss: 5.708059787750244 | BCE Loss: 1.1214070320129395\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 6.806700706481934 | KNN Loss: 5.70042610168457 | BCE Loss: 1.1062748432159424\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 6.7193603515625 | KNN Loss: 5.615592002868652 | BCE Loss: 1.1037683486938477\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 6.700746536254883 | KNN Loss: 5.582955360412598 | BCE Loss: 1.1177910566329956\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 6.628149509429932 | KNN Loss: 5.538040637969971 | BCE Loss: 1.090108871459961\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 6.564499378204346 | KNN Loss: 5.468009948730469 | BCE Loss: 1.0964895486831665\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 6.523926258087158 | KNN Loss: 5.441880702972412 | BCE Loss: 1.082045555114746\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 6.39146614074707 | KNN Loss: 5.35217809677124 | BCE Loss: 1.0392879247665405\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 6.446927547454834 | KNN Loss: 5.373844623565674 | BCE Loss: 1.0730829238891602\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 6.428518772125244 | KNN Loss: 5.321076393127441 | BCE Loss: 1.1074422597885132\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 6.397078037261963 | KNN Loss: 5.320683479309082 | BCE Loss: 1.0763946771621704\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 6.416656494140625 | KNN Loss: 5.324889183044434 | BCE Loss: 1.091767430305481\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 6.390960693359375 | KNN Loss: 5.288867950439453 | BCE Loss: 1.1020927429199219\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 6.332846641540527 | KNN Loss: 5.242414474487305 | BCE Loss: 1.0904319286346436\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 6.320873260498047 | KNN Loss: 5.223685264587402 | BCE Loss: 1.0971877574920654\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 6.332085609436035 | KNN Loss: 5.243373870849609 | BCE Loss: 1.0887118577957153\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 6.277420520782471 | KNN Loss: 5.2147603034973145 | BCE Loss: 1.0626600980758667\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 6.299489974975586 | KNN Loss: 5.215629577636719 | BCE Loss: 1.0838605165481567\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 6.304501056671143 | KNN Loss: 5.201016902923584 | BCE Loss: 1.1034841537475586\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 6.271246433258057 | KNN Loss: 5.2025980949401855 | BCE Loss: 1.068648338317871\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 6.227912902832031 | KNN Loss: 5.15399694442749 | BCE Loss: 1.073915719985962\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 6.2789998054504395 | KNN Loss: 5.18376350402832 | BCE Loss: 1.0952364206314087\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 6.255244255065918 | KNN Loss: 5.181829929351807 | BCE Loss: 1.0734143257141113\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 6.241855621337891 | KNN Loss: 5.163817882537842 | BCE Loss: 1.0780376195907593\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 6.258833885192871 | KNN Loss: 5.158679008483887 | BCE Loss: 1.1001547574996948\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 6.231557846069336 | KNN Loss: 5.1682257652282715 | BCE Loss: 1.0633318424224854\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 6.203394412994385 | KNN Loss: 5.139767646789551 | BCE Loss: 1.0636268854141235\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 6.242658615112305 | KNN Loss: 5.150420188903809 | BCE Loss: 1.092238187789917\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 6.279939651489258 | KNN Loss: 5.172244548797607 | BCE Loss: 1.1076951026916504\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 6.233096122741699 | KNN Loss: 5.123298168182373 | BCE Loss: 1.1097981929779053\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 6.200766563415527 | KNN Loss: 5.1407575607299805 | BCE Loss: 1.060009241104126\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 6.186079025268555 | KNN Loss: 5.129373073577881 | BCE Loss: 1.0567059516906738\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 6.182884693145752 | KNN Loss: 5.123985767364502 | BCE Loss: 1.0588990449905396\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.185357570648193 | KNN Loss: 5.13380765914917 | BCE Loss: 1.0515499114990234\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.220752716064453 | KNN Loss: 5.134659767150879 | BCE Loss: 1.0860927104949951\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.206066131591797 | KNN Loss: 5.1209282875061035 | BCE Loss: 1.0851380825042725\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.219359397888184 | KNN Loss: 5.163321018218994 | BCE Loss: 1.0560386180877686\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.197737216949463 | KNN Loss: 5.1335883140563965 | BCE Loss: 1.0641489028930664\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.168415069580078 | KNN Loss: 5.126105785369873 | BCE Loss: 1.042309045791626\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.180375576019287 | KNN Loss: 5.107351779937744 | BCE Loss: 1.073023796081543\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.213286399841309 | KNN Loss: 5.145791053771973 | BCE Loss: 1.067495346069336\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.187130928039551 | KNN Loss: 5.1154913902282715 | BCE Loss: 1.0716395378112793\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.173989295959473 | KNN Loss: 5.131969451904297 | BCE Loss: 1.0420198440551758\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.154674530029297 | KNN Loss: 5.0982770919799805 | BCE Loss: 1.0563971996307373\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.162970542907715 | KNN Loss: 5.098371505737305 | BCE Loss: 1.0645992755889893\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.166840553283691 | KNN Loss: 5.116245269775391 | BCE Loss: 1.0505955219268799\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.1679582595825195 | KNN Loss: 5.119629383087158 | BCE Loss: 1.0483289957046509\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.179287910461426 | KNN Loss: 5.120948314666748 | BCE Loss: 1.0583398342132568\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.155289173126221 | KNN Loss: 5.099067211151123 | BCE Loss: 1.0562220811843872\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.13906192779541 | KNN Loss: 5.0921149253845215 | BCE Loss: 1.0469468832015991\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.15583610534668 | KNN Loss: 5.103235244750977 | BCE Loss: 1.0526008605957031\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.180739402770996 | KNN Loss: 5.115198612213135 | BCE Loss: 1.0655410289764404\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.200737953186035 | KNN Loss: 5.125856876373291 | BCE Loss: 1.074880838394165\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.156040191650391 | KNN Loss: 5.093240261077881 | BCE Loss: 1.0627999305725098\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.163468360900879 | KNN Loss: 5.107223987579346 | BCE Loss: 1.0562446117401123\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.114456653594971 | KNN Loss: 5.080164432525635 | BCE Loss: 1.0342923402786255\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.139012336730957 | KNN Loss: 5.083566665649414 | BCE Loss: 1.055445671081543\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.142327308654785 | KNN Loss: 5.099246501922607 | BCE Loss: 1.0430808067321777\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.173503398895264 | KNN Loss: 5.107046604156494 | BCE Loss: 1.0664567947387695\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.15169095993042 | KNN Loss: 5.097936630249023 | BCE Loss: 1.053754448890686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.136683464050293 | KNN Loss: 5.08859395980835 | BCE Loss: 1.0480892658233643\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.154345512390137 | KNN Loss: 5.095767498016357 | BCE Loss: 1.0585782527923584\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.227887153625488 | KNN Loss: 5.146585464477539 | BCE Loss: 1.0813019275665283\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.109425067901611 | KNN Loss: 5.08043098449707 | BCE Loss: 1.028994083404541\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.174731731414795 | KNN Loss: 5.090341091156006 | BCE Loss: 1.084390640258789\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.132198333740234 | KNN Loss: 5.077462673187256 | BCE Loss: 1.054735779762268\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.128279209136963 | KNN Loss: 5.067727565765381 | BCE Loss: 1.060551643371582\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.169456958770752 | KNN Loss: 5.112816333770752 | BCE Loss: 1.056640625\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.157802581787109 | KNN Loss: 5.115970611572266 | BCE Loss: 1.0418322086334229\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.156014442443848 | KNN Loss: 5.081886291503906 | BCE Loss: 1.0741281509399414\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.174720287322998 | KNN Loss: 5.098170280456543 | BCE Loss: 1.0765501260757446\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.169008255004883 | KNN Loss: 5.103361129760742 | BCE Loss: 1.0656471252441406\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.1821746826171875 | KNN Loss: 5.112651824951172 | BCE Loss: 1.0695230960845947\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.117879390716553 | KNN Loss: 5.085373401641846 | BCE Loss: 1.0325061082839966\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.148560047149658 | KNN Loss: 5.088863849639893 | BCE Loss: 1.0596961975097656\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.127948760986328 | KNN Loss: 5.071659564971924 | BCE Loss: 1.0562894344329834\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.151051044464111 | KNN Loss: 5.07488489151001 | BCE Loss: 1.0761661529541016\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.146206855773926 | KNN Loss: 5.082147598266602 | BCE Loss: 1.0640591382980347\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.133700370788574 | KNN Loss: 5.092734336853027 | BCE Loss: 1.040966272354126\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.145099639892578 | KNN Loss: 5.083350658416748 | BCE Loss: 1.061748743057251\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.107271671295166 | KNN Loss: 5.068999290466309 | BCE Loss: 1.0382723808288574\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.1384077072143555 | KNN Loss: 5.08241081237793 | BCE Loss: 1.0559970140457153\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.124101638793945 | KNN Loss: 5.08123779296875 | BCE Loss: 1.0428636074066162\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.127479076385498 | KNN Loss: 5.072807312011719 | BCE Loss: 1.0546716451644897\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.082890033721924 | KNN Loss: 5.068117618560791 | BCE Loss: 1.0147724151611328\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.140246868133545 | KNN Loss: 5.094795227050781 | BCE Loss: 1.0454516410827637\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.13699197769165 | KNN Loss: 5.056999683380127 | BCE Loss: 1.0799922943115234\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.152504920959473 | KNN Loss: 5.086485862731934 | BCE Loss: 1.066019058227539\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.121847152709961 | KNN Loss: 5.0675177574157715 | BCE Loss: 1.0543296337127686\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.1362786293029785 | KNN Loss: 5.097921848297119 | BCE Loss: 1.0383567810058594\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.090903282165527 | KNN Loss: 5.077025413513184 | BCE Loss: 1.0138777494430542\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.200986862182617 | KNN Loss: 5.095851421356201 | BCE Loss: 1.105135440826416\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.113361358642578 | KNN Loss: 5.059798240661621 | BCE Loss: 1.0535633563995361\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.123497486114502 | KNN Loss: 5.081655502319336 | BCE Loss: 1.041841983795166\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.119586944580078 | KNN Loss: 5.073862075805664 | BCE Loss: 1.045724630355835\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.126645565032959 | KNN Loss: 5.066347599029541 | BCE Loss: 1.0602980852127075\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.146932601928711 | KNN Loss: 5.082521915435791 | BCE Loss: 1.06441068649292\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.102827072143555 | KNN Loss: 5.062495708465576 | BCE Loss: 1.0403311252593994\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.16566276550293 | KNN Loss: 5.076319694519043 | BCE Loss: 1.0893433094024658\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.131324291229248 | KNN Loss: 5.081592559814453 | BCE Loss: 1.0497318506240845\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.140711784362793 | KNN Loss: 5.0810866355896 | BCE Loss: 1.0596250295639038\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.138113975524902 | KNN Loss: 5.0789794921875 | BCE Loss: 1.0591344833374023\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.122750282287598 | KNN Loss: 5.07178258895874 | BCE Loss: 1.0509674549102783\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.100873947143555 | KNN Loss: 5.060865879058838 | BCE Loss: 1.0400078296661377\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.117135047912598 | KNN Loss: 5.064767837524414 | BCE Loss: 1.052367091178894\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.127018928527832 | KNN Loss: 5.06269645690918 | BCE Loss: 1.0643227100372314\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.12562370300293 | KNN Loss: 5.068925380706787 | BCE Loss: 1.0566985607147217\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.102222442626953 | KNN Loss: 5.0637125968933105 | BCE Loss: 1.0385100841522217\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.140061378479004 | KNN Loss: 5.072681427001953 | BCE Loss: 1.0673797130584717\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.143004417419434 | KNN Loss: 5.083244323730469 | BCE Loss: 1.0597598552703857\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.118442535400391 | KNN Loss: 5.049360275268555 | BCE Loss: 1.0690820217132568\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.116466522216797 | KNN Loss: 5.061853885650635 | BCE Loss: 1.054612398147583\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.126296043395996 | KNN Loss: 5.0684967041015625 | BCE Loss: 1.0577991008758545\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.096712112426758 | KNN Loss: 5.060662746429443 | BCE Loss: 1.0360496044158936\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.144698143005371 | KNN Loss: 5.115890979766846 | BCE Loss: 1.0288069248199463\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.153651237487793 | KNN Loss: 5.064384460449219 | BCE Loss: 1.0892665386199951\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.151902198791504 | KNN Loss: 5.073665618896484 | BCE Loss: 1.0782368183135986\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.119502067565918 | KNN Loss: 5.057319641113281 | BCE Loss: 1.0621821880340576\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.103655815124512 | KNN Loss: 5.064795017242432 | BCE Loss: 1.0388610363006592\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.12246036529541 | KNN Loss: 5.061855316162109 | BCE Loss: 1.0606052875518799\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.09839391708374 | KNN Loss: 5.054737567901611 | BCE Loss: 1.043656349182129\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.13282585144043 | KNN Loss: 5.056495666503906 | BCE Loss: 1.0763300657272339\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.153950214385986 | KNN Loss: 5.098958492279053 | BCE Loss: 1.0549917221069336\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.111991882324219 | KNN Loss: 5.055169582366943 | BCE Loss: 1.0568225383758545\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.116893768310547 | KNN Loss: 5.062922477722168 | BCE Loss: 1.053971290588379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.130435943603516 | KNN Loss: 5.046717166900635 | BCE Loss: 1.0837187767028809\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.0779924392700195 | KNN Loss: 5.037525653839111 | BCE Loss: 1.0404667854309082\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.108156204223633 | KNN Loss: 5.0492377281188965 | BCE Loss: 1.0589187145233154\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.203396797180176 | KNN Loss: 5.093068599700928 | BCE Loss: 1.1103284358978271\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.112232208251953 | KNN Loss: 5.073187351226807 | BCE Loss: 1.0390450954437256\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.130401611328125 | KNN Loss: 5.056979179382324 | BCE Loss: 1.0734226703643799\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.106178283691406 | KNN Loss: 5.061697006225586 | BCE Loss: 1.0444811582565308\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.100916862487793 | KNN Loss: 5.063270568847656 | BCE Loss: 1.0376462936401367\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.1197686195373535 | KNN Loss: 5.069879055023193 | BCE Loss: 1.0498895645141602\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.106204509735107 | KNN Loss: 5.0774970054626465 | BCE Loss: 1.028707504272461\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.137667179107666 | KNN Loss: 5.060163974761963 | BCE Loss: 1.0775033235549927\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.098223686218262 | KNN Loss: 5.052712440490723 | BCE Loss: 1.045511245727539\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.1504292488098145 | KNN Loss: 5.088371276855469 | BCE Loss: 1.0620578527450562\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.136327743530273 | KNN Loss: 5.063368797302246 | BCE Loss: 1.072959065437317\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.102393627166748 | KNN Loss: 5.046746730804443 | BCE Loss: 1.0556467771530151\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.152454376220703 | KNN Loss: 5.062948226928711 | BCE Loss: 1.0895063877105713\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.069788932800293 | KNN Loss: 5.042383670806885 | BCE Loss: 1.0274052619934082\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.08579683303833 | KNN Loss: 5.054740905761719 | BCE Loss: 1.0310559272766113\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.121164798736572 | KNN Loss: 5.06424617767334 | BCE Loss: 1.0569186210632324\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.083149433135986 | KNN Loss: 5.043429374694824 | BCE Loss: 1.0397201776504517\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.101962566375732 | KNN Loss: 5.052108287811279 | BCE Loss: 1.0498543977737427\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.083741664886475 | KNN Loss: 5.0698323249816895 | BCE Loss: 1.0139093399047852\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.0989274978637695 | KNN Loss: 5.042011260986328 | BCE Loss: 1.0569159984588623\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.117852687835693 | KNN Loss: 5.082019805908203 | BCE Loss: 1.0358328819274902\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.114628791809082 | KNN Loss: 5.063649654388428 | BCE Loss: 1.0509791374206543\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.1034650802612305 | KNN Loss: 5.049789905548096 | BCE Loss: 1.0536751747131348\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.1185407638549805 | KNN Loss: 5.061726093292236 | BCE Loss: 1.0568146705627441\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.108756065368652 | KNN Loss: 5.058809280395508 | BCE Loss: 1.0499470233917236\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.126516819000244 | KNN Loss: 5.0473761558532715 | BCE Loss: 1.0791407823562622\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.200838088989258 | KNN Loss: 5.119029521942139 | BCE Loss: 1.0818084478378296\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.088499069213867 | KNN Loss: 5.050299644470215 | BCE Loss: 1.0381994247436523\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.080203056335449 | KNN Loss: 5.048811912536621 | BCE Loss: 1.0313913822174072\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.088008403778076 | KNN Loss: 5.047427654266357 | BCE Loss: 1.0405807495117188\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.079570770263672 | KNN Loss: 5.039655685424805 | BCE Loss: 1.0399150848388672\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.102685928344727 | KNN Loss: 5.041851043701172 | BCE Loss: 1.0608346462249756\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.107419013977051 | KNN Loss: 5.058949947357178 | BCE Loss: 1.0484691858291626\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.119943618774414 | KNN Loss: 5.057377815246582 | BCE Loss: 1.0625659227371216\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.110952377319336 | KNN Loss: 5.075348854064941 | BCE Loss: 1.0356035232543945\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.116945266723633 | KNN Loss: 5.067384243011475 | BCE Loss: 1.0495612621307373\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.083728790283203 | KNN Loss: 5.041870594024658 | BCE Loss: 1.0418579578399658\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.091151237487793 | KNN Loss: 5.046903133392334 | BCE Loss: 1.0442478656768799\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.146556854248047 | KNN Loss: 5.080314636230469 | BCE Loss: 1.066241979598999\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.132294178009033 | KNN Loss: 5.046367168426514 | BCE Loss: 1.085927128791809\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.103204250335693 | KNN Loss: 5.057560920715332 | BCE Loss: 1.0456433296203613\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.0626325607299805 | KNN Loss: 5.0381999015808105 | BCE Loss: 1.0244325399398804\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.112056732177734 | KNN Loss: 5.058350086212158 | BCE Loss: 1.0537065267562866\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.1038079261779785 | KNN Loss: 5.057991027832031 | BCE Loss: 1.0458168983459473\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.107704162597656 | KNN Loss: 5.051075458526611 | BCE Loss: 1.0566285848617554\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.134638786315918 | KNN Loss: 5.067511081695557 | BCE Loss: 1.0671277046203613\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.120941162109375 | KNN Loss: 5.04692268371582 | BCE Loss: 1.0740182399749756\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.08822774887085 | KNN Loss: 5.035315036773682 | BCE Loss: 1.052912712097168\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.09934663772583 | KNN Loss: 5.051716327667236 | BCE Loss: 1.0476303100585938\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.1049699783325195 | KNN Loss: 5.05493688583374 | BCE Loss: 1.0500330924987793\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.127046585083008 | KNN Loss: 5.054991245269775 | BCE Loss: 1.0720553398132324\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.1232757568359375 | KNN Loss: 5.046867847442627 | BCE Loss: 1.0764081478118896\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.088395118713379 | KNN Loss: 5.0412678718566895 | BCE Loss: 1.0471271276474\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.100994110107422 | KNN Loss: 5.04941463470459 | BCE Loss: 1.051579475402832\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.123968124389648 | KNN Loss: 5.056229591369629 | BCE Loss: 1.0677382946014404\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.111396789550781 | KNN Loss: 5.051597595214844 | BCE Loss: 1.0597989559173584\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.086613655090332 | KNN Loss: 5.0412187576293945 | BCE Loss: 1.045395016670227\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.087772369384766 | KNN Loss: 5.047492504119873 | BCE Loss: 1.0402796268463135\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.1216912269592285 | KNN Loss: 5.044045925140381 | BCE Loss: 1.0776454210281372\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.106481552124023 | KNN Loss: 5.056840419769287 | BCE Loss: 1.0496411323547363\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.090188980102539 | KNN Loss: 5.037344932556152 | BCE Loss: 1.0528442859649658\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.151512145996094 | KNN Loss: 5.069396495819092 | BCE Loss: 1.082115650177002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.0896077156066895 | KNN Loss: 5.037966728210449 | BCE Loss: 1.0516409873962402\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.083768844604492 | KNN Loss: 5.039661884307861 | BCE Loss: 1.04410719871521\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.115143775939941 | KNN Loss: 5.054600238800049 | BCE Loss: 1.0605437755584717\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.11208438873291 | KNN Loss: 5.058661937713623 | BCE Loss: 1.0534226894378662\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.074542999267578 | KNN Loss: 5.037545680999756 | BCE Loss: 1.0369970798492432\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.1285080909729 | KNN Loss: 5.04819917678833 | BCE Loss: 1.0803089141845703\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.118646621704102 | KNN Loss: 5.072786331176758 | BCE Loss: 1.0458600521087646\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.0933332443237305 | KNN Loss: 5.0483551025390625 | BCE Loss: 1.0449780225753784\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.085073947906494 | KNN Loss: 5.031048774719238 | BCE Loss: 1.0540251731872559\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.092771530151367 | KNN Loss: 5.039377689361572 | BCE Loss: 1.0533936023712158\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.1211323738098145 | KNN Loss: 5.047356128692627 | BCE Loss: 1.073776364326477\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.111654758453369 | KNN Loss: 5.077425003051758 | BCE Loss: 1.0342296361923218\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.091555118560791 | KNN Loss: 5.051105499267578 | BCE Loss: 1.0404495000839233\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.102035045623779 | KNN Loss: 5.035642623901367 | BCE Loss: 1.066392421722412\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.10964298248291 | KNN Loss: 5.050591945648193 | BCE Loss: 1.059051275253296\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.075052738189697 | KNN Loss: 5.041487216949463 | BCE Loss: 1.0335655212402344\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.128190994262695 | KNN Loss: 5.0586838722229 | BCE Loss: 1.069507360458374\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.1178669929504395 | KNN Loss: 5.054563045501709 | BCE Loss: 1.06330406665802\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.070746421813965 | KNN Loss: 5.030906677246094 | BCE Loss: 1.039839744567871\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.111616134643555 | KNN Loss: 5.0479583740234375 | BCE Loss: 1.0636579990386963\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.10684871673584 | KNN Loss: 5.058741092681885 | BCE Loss: 1.048107624053955\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.100419998168945 | KNN Loss: 5.039523601531982 | BCE Loss: 1.0608961582183838\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.0412797927856445 | KNN Loss: 5.037659168243408 | BCE Loss: 1.0036208629608154\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.088815212249756 | KNN Loss: 5.055199146270752 | BCE Loss: 1.033616065979004\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.086999893188477 | KNN Loss: 5.043177127838135 | BCE Loss: 1.0438225269317627\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.097306251525879 | KNN Loss: 5.0390729904174805 | BCE Loss: 1.0582330226898193\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.090208530426025 | KNN Loss: 5.043694496154785 | BCE Loss: 1.0465139150619507\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.12437105178833 | KNN Loss: 5.04092264175415 | BCE Loss: 1.0834485292434692\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.100850582122803 | KNN Loss: 5.037128448486328 | BCE Loss: 1.0637221336364746\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.078369140625 | KNN Loss: 5.037517070770264 | BCE Loss: 1.0408520698547363\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.101812839508057 | KNN Loss: 5.038898468017578 | BCE Loss: 1.062914252281189\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.104049205780029 | KNN Loss: 5.042508125305176 | BCE Loss: 1.0615410804748535\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.08499813079834 | KNN Loss: 5.036540508270264 | BCE Loss: 1.0484577417373657\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.095224380493164 | KNN Loss: 5.037688732147217 | BCE Loss: 1.0575354099273682\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.094951629638672 | KNN Loss: 5.029280662536621 | BCE Loss: 1.0656710863113403\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.09760856628418 | KNN Loss: 5.046636581420898 | BCE Loss: 1.0509719848632812\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.132287502288818 | KNN Loss: 5.068325519561768 | BCE Loss: 1.0639619827270508\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.075295448303223 | KNN Loss: 5.044013977050781 | BCE Loss: 1.031281590461731\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.081596374511719 | KNN Loss: 5.023219108581543 | BCE Loss: 1.0583775043487549\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.0969014167785645 | KNN Loss: 5.04340934753418 | BCE Loss: 1.0534921884536743\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.081351280212402 | KNN Loss: 5.026415824890137 | BCE Loss: 1.0549356937408447\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.061223983764648 | KNN Loss: 5.028318881988525 | BCE Loss: 1.0329053401947021\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.149522304534912 | KNN Loss: 5.089661598205566 | BCE Loss: 1.0598607063293457\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.0799407958984375 | KNN Loss: 5.04368257522583 | BCE Loss: 1.0362584590911865\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.087737560272217 | KNN Loss: 5.044397830963135 | BCE Loss: 1.043339729309082\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.083401679992676 | KNN Loss: 5.038678169250488 | BCE Loss: 1.0447235107421875\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.135900974273682 | KNN Loss: 5.089564800262451 | BCE Loss: 1.0463361740112305\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.092848300933838 | KNN Loss: 5.041295051574707 | BCE Loss: 1.0515532493591309\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.0634660720825195 | KNN Loss: 5.037899494171143 | BCE Loss: 1.025566816329956\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.104429721832275 | KNN Loss: 5.052832126617432 | BCE Loss: 1.0515974760055542\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.152942657470703 | KNN Loss: 5.09185266494751 | BCE Loss: 1.0610899925231934\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.092922210693359 | KNN Loss: 5.021554470062256 | BCE Loss: 1.0713677406311035\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.088675498962402 | KNN Loss: 5.043511867523193 | BCE Loss: 1.0451635122299194\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.123407363891602 | KNN Loss: 5.038837909698486 | BCE Loss: 1.0845695734024048\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.105424404144287 | KNN Loss: 5.038737773895264 | BCE Loss: 1.0666866302490234\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.068930149078369 | KNN Loss: 5.035200595855713 | BCE Loss: 1.0337294340133667\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.079866409301758 | KNN Loss: 5.043324947357178 | BCE Loss: 1.03654146194458\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.070225238800049 | KNN Loss: 5.03886079788208 | BCE Loss: 1.0313643217086792\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.045945167541504 | KNN Loss: 5.017217636108398 | BCE Loss: 1.028727650642395\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.103617191314697 | KNN Loss: 5.044296741485596 | BCE Loss: 1.059320330619812\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.078949928283691 | KNN Loss: 5.031852722167969 | BCE Loss: 1.0470974445343018\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.090651512145996 | KNN Loss: 5.034398555755615 | BCE Loss: 1.05625319480896\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.097908973693848 | KNN Loss: 5.05234956741333 | BCE Loss: 1.0455595254898071\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.088438034057617 | KNN Loss: 5.052734851837158 | BCE Loss: 1.035703182220459\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.09207820892334 | KNN Loss: 5.040394306182861 | BCE Loss: 1.0516841411590576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.0696024894714355 | KNN Loss: 5.0296831130981445 | BCE Loss: 1.039919376373291\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.080321788787842 | KNN Loss: 5.031465530395508 | BCE Loss: 1.048856258392334\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.100520610809326 | KNN Loss: 5.021414756774902 | BCE Loss: 1.0791059732437134\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.1001458168029785 | KNN Loss: 5.057011127471924 | BCE Loss: 1.0431345701217651\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.1169891357421875 | KNN Loss: 5.054841041564941 | BCE Loss: 1.062147855758667\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.090317726135254 | KNN Loss: 5.046672821044922 | BCE Loss: 1.0436450242996216\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.137221813201904 | KNN Loss: 5.048509120941162 | BCE Loss: 1.0887126922607422\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.166006088256836 | KNN Loss: 5.112795352935791 | BCE Loss: 1.0532104969024658\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.023922920227051 | KNN Loss: 5.009631156921387 | BCE Loss: 1.014291763305664\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.079681396484375 | KNN Loss: 5.03499698638916 | BCE Loss: 1.044684648513794\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.095331192016602 | KNN Loss: 5.026525020599365 | BCE Loss: 1.0688064098358154\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.109541893005371 | KNN Loss: 5.049293518066406 | BCE Loss: 1.060248613357544\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.050630569458008 | KNN Loss: 5.025986671447754 | BCE Loss: 1.024643898010254\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.048379898071289 | KNN Loss: 5.017277717590332 | BCE Loss: 1.0311024188995361\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.07475471496582 | KNN Loss: 5.027434825897217 | BCE Loss: 1.0473201274871826\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.067389488220215 | KNN Loss: 5.027130603790283 | BCE Loss: 1.0402591228485107\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.084529876708984 | KNN Loss: 5.044654846191406 | BCE Loss: 1.039874792098999\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.065500259399414 | KNN Loss: 5.029568195343018 | BCE Loss: 1.0359318256378174\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.0971174240112305 | KNN Loss: 5.041667938232422 | BCE Loss: 1.0554494857788086\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.133767127990723 | KNN Loss: 5.073526859283447 | BCE Loss: 1.0602405071258545\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.106078624725342 | KNN Loss: 5.060443878173828 | BCE Loss: 1.0456346273422241\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.095125198364258 | KNN Loss: 5.017641067504883 | BCE Loss: 1.077483892440796\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.057261943817139 | KNN Loss: 5.027462959289551 | BCE Loss: 1.029798984527588\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.120426177978516 | KNN Loss: 5.090888977050781 | BCE Loss: 1.0295374393463135\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.104764938354492 | KNN Loss: 5.060480117797852 | BCE Loss: 1.044284701347351\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.099772930145264 | KNN Loss: 5.0555100440979 | BCE Loss: 1.0442627668380737\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.091418266296387 | KNN Loss: 5.050583839416504 | BCE Loss: 1.0408341884613037\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.079634666442871 | KNN Loss: 5.043616771697998 | BCE Loss: 1.0360180139541626\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.065579891204834 | KNN Loss: 5.025833606719971 | BCE Loss: 1.0397462844848633\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.065988063812256 | KNN Loss: 5.027125835418701 | BCE Loss: 1.0388622283935547\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.090010643005371 | KNN Loss: 5.05919885635376 | BCE Loss: 1.0308120250701904\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.064366340637207 | KNN Loss: 5.011359214782715 | BCE Loss: 1.0530070066452026\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.087582111358643 | KNN Loss: 5.02886962890625 | BCE Loss: 1.0587124824523926\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.077210426330566 | KNN Loss: 5.029077529907227 | BCE Loss: 1.0481326580047607\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.062276363372803 | KNN Loss: 5.018486976623535 | BCE Loss: 1.0437893867492676\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.093448638916016 | KNN Loss: 5.0426201820373535 | BCE Loss: 1.050828456878662\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.064818382263184 | KNN Loss: 5.024409294128418 | BCE Loss: 1.0404092073440552\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.065953731536865 | KNN Loss: 5.020742893218994 | BCE Loss: 1.0452109575271606\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.063912391662598 | KNN Loss: 5.021744251251221 | BCE Loss: 1.0421680212020874\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.102741241455078 | KNN Loss: 5.07073450088501 | BCE Loss: 1.032006859779358\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.090155601501465 | KNN Loss: 5.0518951416015625 | BCE Loss: 1.0382606983184814\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.080270767211914 | KNN Loss: 5.023888111114502 | BCE Loss: 1.056382656097412\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.083308696746826 | KNN Loss: 5.029265403747559 | BCE Loss: 1.0540434122085571\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.063373565673828 | KNN Loss: 5.019614219665527 | BCE Loss: 1.0437595844268799\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.0433149337768555 | KNN Loss: 5.024476528167725 | BCE Loss: 1.0188381671905518\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.066814422607422 | KNN Loss: 5.022801399230957 | BCE Loss: 1.0440127849578857\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.097392559051514 | KNN Loss: 5.0192179679870605 | BCE Loss: 1.0781745910644531\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.049139022827148 | KNN Loss: 5.020881175994873 | BCE Loss: 1.0282577276229858\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.1121978759765625 | KNN Loss: 5.046342849731445 | BCE Loss: 1.0658552646636963\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.104033470153809 | KNN Loss: 5.042883396148682 | BCE Loss: 1.061150312423706\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.081264495849609 | KNN Loss: 5.026618480682373 | BCE Loss: 1.0546460151672363\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.053044319152832 | KNN Loss: 5.021272659301758 | BCE Loss: 1.0317714214324951\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.088620185852051 | KNN Loss: 5.0532073974609375 | BCE Loss: 1.0354127883911133\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.136291980743408 | KNN Loss: 5.0866241455078125 | BCE Loss: 1.0496678352355957\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.074328422546387 | KNN Loss: 5.039281845092773 | BCE Loss: 1.0350465774536133\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.081543922424316 | KNN Loss: 5.038050174713135 | BCE Loss: 1.0434937477111816\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.083104133605957 | KNN Loss: 5.019130229949951 | BCE Loss: 1.063974142074585\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.073232173919678 | KNN Loss: 5.0354084968566895 | BCE Loss: 1.0378236770629883\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.080350875854492 | KNN Loss: 5.03956413269043 | BCE Loss: 1.0407865047454834\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.045005798339844 | KNN Loss: 5.017625331878662 | BCE Loss: 1.027380347251892\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.114627838134766 | KNN Loss: 5.036055564880371 | BCE Loss: 1.078572154045105\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.110823631286621 | KNN Loss: 5.059170722961426 | BCE Loss: 1.0516529083251953\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.078015327453613 | KNN Loss: 5.0187201499938965 | BCE Loss: 1.0592950582504272\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.074652671813965 | KNN Loss: 5.021722793579102 | BCE Loss: 1.0529296398162842\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.072447776794434 | KNN Loss: 5.024720191955566 | BCE Loss: 1.0477277040481567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.041706085205078 | KNN Loss: 5.017374515533447 | BCE Loss: 1.0243315696716309\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.0404157638549805 | KNN Loss: 5.0175700187683105 | BCE Loss: 1.02284574508667\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.049448013305664 | KNN Loss: 5.01605224609375 | BCE Loss: 1.033395528793335\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.062939643859863 | KNN Loss: 5.035002708435059 | BCE Loss: 1.0279369354248047\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.055649757385254 | KNN Loss: 5.013498306274414 | BCE Loss: 1.0421514511108398\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.091758728027344 | KNN Loss: 5.031163692474365 | BCE Loss: 1.0605950355529785\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.097441673278809 | KNN Loss: 5.0313920974731445 | BCE Loss: 1.066049337387085\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.106651306152344 | KNN Loss: 5.041915416717529 | BCE Loss: 1.064735770225525\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.066173553466797 | KNN Loss: 5.033342361450195 | BCE Loss: 1.0328309535980225\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.023449420928955 | KNN Loss: 5.015679359436035 | BCE Loss: 1.00777006149292\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.089715957641602 | KNN Loss: 5.023884296417236 | BCE Loss: 1.0658316612243652\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.073619842529297 | KNN Loss: 5.029465675354004 | BCE Loss: 1.044154405593872\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.090987682342529 | KNN Loss: 5.028761386871338 | BCE Loss: 1.0622261762619019\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.094520568847656 | KNN Loss: 5.028744220733643 | BCE Loss: 1.0657763481140137\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.091577529907227 | KNN Loss: 5.02756404876709 | BCE Loss: 1.0640132427215576\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.077731609344482 | KNN Loss: 4.999492168426514 | BCE Loss: 1.0782394409179688\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.089499473571777 | KNN Loss: 5.042788505554199 | BCE Loss: 1.0467112064361572\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.066040515899658 | KNN Loss: 5.013916015625 | BCE Loss: 1.0521243810653687\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.078452110290527 | KNN Loss: 5.033718109130859 | BCE Loss: 1.044734001159668\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.05886173248291 | KNN Loss: 5.022231578826904 | BCE Loss: 1.036630392074585\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.065951347351074 | KNN Loss: 5.017849922180176 | BCE Loss: 1.048101544380188\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.09080696105957 | KNN Loss: 5.01052713394165 | BCE Loss: 1.0802799463272095\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.066780090332031 | KNN Loss: 5.002111911773682 | BCE Loss: 1.06466805934906\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.040670871734619 | KNN Loss: 5.021624565124512 | BCE Loss: 1.019046425819397\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.086060047149658 | KNN Loss: 5.020740985870361 | BCE Loss: 1.0653190612792969\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.038967132568359 | KNN Loss: 4.998076438903809 | BCE Loss: 1.0408906936645508\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.084891319274902 | KNN Loss: 5.029909610748291 | BCE Loss: 1.0549819469451904\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.042654991149902 | KNN Loss: 5.009788513183594 | BCE Loss: 1.0328665971755981\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.045050621032715 | KNN Loss: 5.008147239685059 | BCE Loss: 1.0369031429290771\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.076650619506836 | KNN Loss: 5.025434970855713 | BCE Loss: 1.051215648651123\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.088404178619385 | KNN Loss: 5.029531478881836 | BCE Loss: 1.0588726997375488\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.069555282592773 | KNN Loss: 5.03959846496582 | BCE Loss: 1.0299569368362427\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.068359851837158 | KNN Loss: 5.007239818572998 | BCE Loss: 1.0611200332641602\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.071602821350098 | KNN Loss: 5.033830165863037 | BCE Loss: 1.03777277469635\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.0884504318237305 | KNN Loss: 5.01573371887207 | BCE Loss: 1.0727167129516602\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.054797649383545 | KNN Loss: 5.0270586013793945 | BCE Loss: 1.0277390480041504\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.069516181945801 | KNN Loss: 5.02484130859375 | BCE Loss: 1.0446751117706299\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.079026222229004 | KNN Loss: 5.035315990447998 | BCE Loss: 1.0437103509902954\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.081287860870361 | KNN Loss: 5.039260387420654 | BCE Loss: 1.042027473449707\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.054432392120361 | KNN Loss: 5.018455505371094 | BCE Loss: 1.035976767539978\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.036811828613281 | KNN Loss: 5.014220714569092 | BCE Loss: 1.0225913524627686\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.049459457397461 | KNN Loss: 5.012561321258545 | BCE Loss: 1.0368983745574951\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.087310791015625 | KNN Loss: 5.042322635650635 | BCE Loss: 1.0449881553649902\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.089054107666016 | KNN Loss: 5.04970645904541 | BCE Loss: 1.0393476486206055\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.048459053039551 | KNN Loss: 5.006147384643555 | BCE Loss: 1.042311668395996\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.026998043060303 | KNN Loss: 5.0074639320373535 | BCE Loss: 1.0195341110229492\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.058341026306152 | KNN Loss: 5.0225605964660645 | BCE Loss: 1.035780429840088\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.076653957366943 | KNN Loss: 5.005500316619873 | BCE Loss: 1.0711536407470703\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.036941051483154 | KNN Loss: 5.003459930419922 | BCE Loss: 1.0334810018539429\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.029298782348633 | KNN Loss: 5.019711017608643 | BCE Loss: 1.0095875263214111\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.065535068511963 | KNN Loss: 5.034545421600342 | BCE Loss: 1.030989646911621\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.055349349975586 | KNN Loss: 5.030635356903076 | BCE Loss: 1.0247139930725098\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.096704006195068 | KNN Loss: 5.049410820007324 | BCE Loss: 1.0472931861877441\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.075256824493408 | KNN Loss: 5.048104286193848 | BCE Loss: 1.027152419090271\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.060358047485352 | KNN Loss: 5.00337028503418 | BCE Loss: 1.0569875240325928\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.034061908721924 | KNN Loss: 5.00446081161499 | BCE Loss: 1.0296010971069336\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.089088439941406 | KNN Loss: 5.007826805114746 | BCE Loss: 1.0812615156173706\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.103485584259033 | KNN Loss: 5.0364227294921875 | BCE Loss: 1.0670627355575562\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.06136417388916 | KNN Loss: 5.017154693603516 | BCE Loss: 1.0442094802856445\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.094983100891113 | KNN Loss: 5.025426387786865 | BCE Loss: 1.069556713104248\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.052000522613525 | KNN Loss: 5.004608631134033 | BCE Loss: 1.0473918914794922\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.053892612457275 | KNN Loss: 5.007105350494385 | BCE Loss: 1.0467873811721802\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.042026519775391 | KNN Loss: 5.002701759338379 | BCE Loss: 1.0393248796463013\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.075791358947754 | KNN Loss: 5.010148048400879 | BCE Loss: 1.065643310546875\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.084071636199951 | KNN Loss: 5.061971187591553 | BCE Loss: 1.0221003293991089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.06343936920166 | KNN Loss: 5.036379337310791 | BCE Loss: 1.02705979347229\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.0555315017700195 | KNN Loss: 5.015176773071289 | BCE Loss: 1.0403549671173096\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.082566261291504 | KNN Loss: 5.032643795013428 | BCE Loss: 1.049922227859497\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.067405700683594 | KNN Loss: 5.012475490570068 | BCE Loss: 1.0549304485321045\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.104670524597168 | KNN Loss: 5.027459621429443 | BCE Loss: 1.0772110223770142\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.140169143676758 | KNN Loss: 5.109612464904785 | BCE Loss: 1.0305566787719727\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.049584865570068 | KNN Loss: 5.01343297958374 | BCE Loss: 1.0361520051956177\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.075631141662598 | KNN Loss: 5.02353572845459 | BCE Loss: 1.052095651626587\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.078474998474121 | KNN Loss: 5.039807319641113 | BCE Loss: 1.0386677980422974\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.085259437561035 | KNN Loss: 5.049436092376709 | BCE Loss: 1.0358233451843262\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.033435821533203 | KNN Loss: 4.998382568359375 | BCE Loss: 1.0350534915924072\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.095631122589111 | KNN Loss: 5.0494818687438965 | BCE Loss: 1.0461492538452148\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.084906578063965 | KNN Loss: 5.02811861038208 | BCE Loss: 1.0567877292633057\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.071102619171143 | KNN Loss: 5.041277885437012 | BCE Loss: 1.0298247337341309\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.092177867889404 | KNN Loss: 5.02937126159668 | BCE Loss: 1.0628067255020142\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.049915790557861 | KNN Loss: 5.016460418701172 | BCE Loss: 1.0334552526474\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.03822135925293 | KNN Loss: 5.012277126312256 | BCE Loss: 1.0259441137313843\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.044522762298584 | KNN Loss: 5.000091075897217 | BCE Loss: 1.0444316864013672\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.082408905029297 | KNN Loss: 5.0026960372924805 | BCE Loss: 1.0797126293182373\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.039913177490234 | KNN Loss: 5.00955057144165 | BCE Loss: 1.0303623676300049\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.063575744628906 | KNN Loss: 5.022056579589844 | BCE Loss: 1.0415191650390625\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.029421806335449 | KNN Loss: 5.006512641906738 | BCE Loss: 1.0229089260101318\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.064977645874023 | KNN Loss: 5.020017623901367 | BCE Loss: 1.0449597835540771\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.061225891113281 | KNN Loss: 5.035405158996582 | BCE Loss: 1.0258209705352783\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.063236236572266 | KNN Loss: 5.010140419006348 | BCE Loss: 1.053096055984497\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.0622053146362305 | KNN Loss: 5.031905651092529 | BCE Loss: 1.0302995443344116\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.083468914031982 | KNN Loss: 5.022121906280518 | BCE Loss: 1.0613470077514648\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.080654621124268 | KNN Loss: 5.0524444580078125 | BCE Loss: 1.0282100439071655\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.085018157958984 | KNN Loss: 5.018229007720947 | BCE Loss: 1.066788911819458\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.036068439483643 | KNN Loss: 5.0131330490112305 | BCE Loss: 1.0229352712631226\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.052736282348633 | KNN Loss: 4.989218235015869 | BCE Loss: 1.0635179281234741\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.047471046447754 | KNN Loss: 5.002281665802002 | BCE Loss: 1.045189619064331\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.11289644241333 | KNN Loss: 5.06195068359375 | BCE Loss: 1.0509456396102905\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.034282207489014 | KNN Loss: 5.022333145141602 | BCE Loss: 1.0119491815567017\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.051858901977539 | KNN Loss: 5.023509979248047 | BCE Loss: 1.0283489227294922\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.064151763916016 | KNN Loss: 5.013676166534424 | BCE Loss: 1.0504755973815918\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.1336164474487305 | KNN Loss: 5.0983781814575195 | BCE Loss: 1.035238265991211\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.095648765563965 | KNN Loss: 5.037383556365967 | BCE Loss: 1.058264970779419\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.042602062225342 | KNN Loss: 5.015052795410156 | BCE Loss: 1.0275492668151855\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.039668083190918 | KNN Loss: 5.008784294128418 | BCE Loss: 1.030884027481079\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.042852878570557 | KNN Loss: 5.001491069793701 | BCE Loss: 1.041361689567566\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.051332473754883 | KNN Loss: 5.000741958618164 | BCE Loss: 1.0505907535552979\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.0663065910339355 | KNN Loss: 5.018209934234619 | BCE Loss: 1.0480966567993164\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.026085376739502 | KNN Loss: 5.005379676818848 | BCE Loss: 1.0207056999206543\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.055431365966797 | KNN Loss: 5.026878833770752 | BCE Loss: 1.0285522937774658\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.038529872894287 | KNN Loss: 5.029591083526611 | BCE Loss: 1.0089389085769653\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.059675216674805 | KNN Loss: 5.002511501312256 | BCE Loss: 1.057163953781128\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.037993907928467 | KNN Loss: 4.997184753417969 | BCE Loss: 1.040809154510498\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.049009323120117 | KNN Loss: 5.00803279876709 | BCE Loss: 1.0409767627716064\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.0587053298950195 | KNN Loss: 5.013277530670166 | BCE Loss: 1.0454277992248535\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.016587257385254 | KNN Loss: 5.002884387969971 | BCE Loss: 1.0137031078338623\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.056805610656738 | KNN Loss: 5.020657539367676 | BCE Loss: 1.0361478328704834\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.082595348358154 | KNN Loss: 5.025815010070801 | BCE Loss: 1.0567803382873535\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.07847785949707 | KNN Loss: 5.018798828125 | BCE Loss: 1.0596790313720703\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.029743194580078 | KNN Loss: 4.998544216156006 | BCE Loss: 1.0311989784240723\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.035955429077148 | KNN Loss: 5.006370544433594 | BCE Loss: 1.0295846462249756\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.050222873687744 | KNN Loss: 5.008069038391113 | BCE Loss: 1.0421539545059204\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.087526321411133 | KNN Loss: 5.066181659698486 | BCE Loss: 1.0213444232940674\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.038341522216797 | KNN Loss: 5.002918243408203 | BCE Loss: 1.0354230403900146\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.073834419250488 | KNN Loss: 5.009101867675781 | BCE Loss: 1.0647327899932861\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.093419551849365 | KNN Loss: 5.020501613616943 | BCE Loss: 1.0729178190231323\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.048623561859131 | KNN Loss: 5.005519866943359 | BCE Loss: 1.043103575706482\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.052360534667969 | KNN Loss: 5.020111083984375 | BCE Loss: 1.0322494506835938\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.062490940093994 | KNN Loss: 5.007091045379639 | BCE Loss: 1.0553998947143555\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.057929039001465 | KNN Loss: 5.024499893188477 | BCE Loss: 1.0334293842315674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.059650421142578 | KNN Loss: 5.025633811950684 | BCE Loss: 1.0340168476104736\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.03624153137207 | KNN Loss: 5.004266738891602 | BCE Loss: 1.0319749116897583\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.02328634262085 | KNN Loss: 5.006771087646484 | BCE Loss: 1.0165152549743652\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.0826520919799805 | KNN Loss: 5.031464099884033 | BCE Loss: 1.0511879920959473\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.055439472198486 | KNN Loss: 5.00681734085083 | BCE Loss: 1.0486220121383667\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.092889308929443 | KNN Loss: 5.05412483215332 | BCE Loss: 1.0387643575668335\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.040262222290039 | KNN Loss: 5.015324592590332 | BCE Loss: 1.024937629699707\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.037534713745117 | KNN Loss: 4.999297618865967 | BCE Loss: 1.0382370948791504\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.019761562347412 | KNN Loss: 5.010334491729736 | BCE Loss: 1.0094270706176758\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.0396881103515625 | KNN Loss: 5.022215843200684 | BCE Loss: 1.017472267150879\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.0629730224609375 | KNN Loss: 5.011967182159424 | BCE Loss: 1.0510060787200928\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.032242298126221 | KNN Loss: 5.015812873840332 | BCE Loss: 1.0164293050765991\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.036347389221191 | KNN Loss: 5.000174045562744 | BCE Loss: 1.0361735820770264\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.030579090118408 | KNN Loss: 5.004902362823486 | BCE Loss: 1.0256766080856323\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.057711601257324 | KNN Loss: 5.001837730407715 | BCE Loss: 1.0558738708496094\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.055427074432373 | KNN Loss: 5.005669116973877 | BCE Loss: 1.049757957458496\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.026432037353516 | KNN Loss: 5.005129814147949 | BCE Loss: 1.0213022232055664\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.035996437072754 | KNN Loss: 4.998350143432617 | BCE Loss: 1.0376460552215576\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.023622989654541 | KNN Loss: 5.003864765167236 | BCE Loss: 1.0197581052780151\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.082841396331787 | KNN Loss: 5.046623706817627 | BCE Loss: 1.0362175703048706\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.0773539543151855 | KNN Loss: 5.033280849456787 | BCE Loss: 1.044073224067688\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.041074275970459 | KNN Loss: 4.993054389953613 | BCE Loss: 1.0480197668075562\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.063928604125977 | KNN Loss: 5.0180206298828125 | BCE Loss: 1.045907735824585\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.0668230056762695 | KNN Loss: 5.029918193817139 | BCE Loss: 1.0369045734405518\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.026020526885986 | KNN Loss: 4.9982709884643555 | BCE Loss: 1.0277496576309204\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.033569812774658 | KNN Loss: 4.996279239654541 | BCE Loss: 1.0372905731201172\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.03287935256958 | KNN Loss: 4.989334583282471 | BCE Loss: 1.043544888496399\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.072619915008545 | KNN Loss: 5.0240864753723145 | BCE Loss: 1.04853355884552\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.050731658935547 | KNN Loss: 5.022939205169678 | BCE Loss: 1.0277924537658691\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.067455768585205 | KNN Loss: 4.999663352966309 | BCE Loss: 1.0677924156188965\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.027006149291992 | KNN Loss: 5.010054588317871 | BCE Loss: 1.016951560974121\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 5.997881889343262 | KNN Loss: 4.988572597503662 | BCE Loss: 1.0093090534210205\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.0184478759765625 | KNN Loss: 4.99183464050293 | BCE Loss: 1.0266132354736328\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.039167404174805 | KNN Loss: 5.0258893966674805 | BCE Loss: 1.0132778882980347\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.023913383483887 | KNN Loss: 5.0157341957092285 | BCE Loss: 1.0081791877746582\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.050804138183594 | KNN Loss: 4.999253273010254 | BCE Loss: 1.0515508651733398\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.077419281005859 | KNN Loss: 5.004892826080322 | BCE Loss: 1.0725263357162476\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.053835391998291 | KNN Loss: 5.01406717300415 | BCE Loss: 1.039768099784851\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.044072151184082 | KNN Loss: 5.009585857391357 | BCE Loss: 1.0344862937927246\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.0215654373168945 | KNN Loss: 5.015422821044922 | BCE Loss: 1.006142497062683\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.077750205993652 | KNN Loss: 5.021884441375732 | BCE Loss: 1.05586576461792\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.058638572692871 | KNN Loss: 5.030925273895264 | BCE Loss: 1.0277132987976074\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.039669513702393 | KNN Loss: 4.997560501098633 | BCE Loss: 1.0421090126037598\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.0150675773620605 | KNN Loss: 4.994300365447998 | BCE Loss: 1.0207672119140625\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.054977893829346 | KNN Loss: 5.028609275817871 | BCE Loss: 1.0263687372207642\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.063513278961182 | KNN Loss: 5.013739109039307 | BCE Loss: 1.049774169921875\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.056451797485352 | KNN Loss: 4.998904228210449 | BCE Loss: 1.0575478076934814\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.061191082000732 | KNN Loss: 5.051872253417969 | BCE Loss: 1.0093188285827637\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.035240173339844 | KNN Loss: 5.015872478485107 | BCE Loss: 1.0193676948547363\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.078174591064453 | KNN Loss: 5.023849010467529 | BCE Loss: 1.0543256998062134\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.017086505889893 | KNN Loss: 4.993227481842041 | BCE Loss: 1.0238590240478516\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.071423530578613 | KNN Loss: 5.030222415924072 | BCE Loss: 1.041200876235962\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.031789779663086 | KNN Loss: 5.0172200202941895 | BCE Loss: 1.0145697593688965\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.016270637512207 | KNN Loss: 4.99069881439209 | BCE Loss: 1.0255720615386963\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.017142295837402 | KNN Loss: 5.003752708435059 | BCE Loss: 1.0133894681930542\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.07835578918457 | KNN Loss: 5.049745559692383 | BCE Loss: 1.0286102294921875\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.03514289855957 | KNN Loss: 4.9981513023376465 | BCE Loss: 1.0369913578033447\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.016750335693359 | KNN Loss: 5.000899314880371 | BCE Loss: 1.0158512592315674\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.057458877563477 | KNN Loss: 5.01978063583374 | BCE Loss: 1.0376784801483154\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.074012756347656 | KNN Loss: 5.033060550689697 | BCE Loss: 1.040952444076538\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.029433250427246 | KNN Loss: 4.989340305328369 | BCE Loss: 1.040092945098877\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.062539100646973 | KNN Loss: 5.0311760902404785 | BCE Loss: 1.0313630104064941\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.01002311706543 | KNN Loss: 4.997439384460449 | BCE Loss: 1.0125839710235596\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.036137580871582 | KNN Loss: 4.989814281463623 | BCE Loss: 1.0463234186172485\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.085745334625244 | KNN Loss: 5.021681785583496 | BCE Loss: 1.064063549041748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.028935432434082 | KNN Loss: 5.014415264129639 | BCE Loss: 1.0145201683044434\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.0476765632629395 | KNN Loss: 5.018375396728516 | BCE Loss: 1.0293011665344238\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.0189313888549805 | KNN Loss: 4.999255180358887 | BCE Loss: 1.0196764469146729\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.04156494140625 | KNN Loss: 5.020716667175293 | BCE Loss: 1.0208485126495361\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.042229652404785 | KNN Loss: 5.020939350128174 | BCE Loss: 1.0212905406951904\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.038056373596191 | KNN Loss: 5.0220537185668945 | BCE Loss: 1.016002893447876\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.033816814422607 | KNN Loss: 5.031329154968262 | BCE Loss: 1.0024877786636353\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.054437637329102 | KNN Loss: 5.027155876159668 | BCE Loss: 1.0272817611694336\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.040500640869141 | KNN Loss: 5.021287441253662 | BCE Loss: 1.0192129611968994\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.037497520446777 | KNN Loss: 4.998826503753662 | BCE Loss: 1.0386712551116943\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.085446357727051 | KNN Loss: 5.048160076141357 | BCE Loss: 1.0372865200042725\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.026500701904297 | KNN Loss: 5.0155229568481445 | BCE Loss: 1.0109775066375732\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.044064044952393 | KNN Loss: 4.999520778656006 | BCE Loss: 1.0445433855056763\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.0110859870910645 | KNN Loss: 4.996123790740967 | BCE Loss: 1.0149621963500977\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.065616607666016 | KNN Loss: 5.0287909507751465 | BCE Loss: 1.0368256568908691\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.03512716293335 | KNN Loss: 4.995786666870117 | BCE Loss: 1.0393404960632324\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.02451753616333 | KNN Loss: 5.022473335266113 | BCE Loss: 1.0020443201065063\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.039797782897949 | KNN Loss: 5.033586502075195 | BCE Loss: 1.006211280822754\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.057405471801758 | KNN Loss: 5.022045135498047 | BCE Loss: 1.0353604555130005\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.022618770599365 | KNN Loss: 4.993948936462402 | BCE Loss: 1.028669834136963\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.05828857421875 | KNN Loss: 5.012144565582275 | BCE Loss: 1.0461442470550537\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.0431928634643555 | KNN Loss: 4.994785308837891 | BCE Loss: 1.0484075546264648\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.068320274353027 | KNN Loss: 5.0463948249816895 | BCE Loss: 1.021925687789917\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.037541389465332 | KNN Loss: 5.00769567489624 | BCE Loss: 1.0298455953598022\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.1177263259887695 | KNN Loss: 5.073242664337158 | BCE Loss: 1.0444839000701904\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.0486836433410645 | KNN Loss: 5.022820949554443 | BCE Loss: 1.025862693786621\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.08621072769165 | KNN Loss: 5.030555248260498 | BCE Loss: 1.0556554794311523\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.042070388793945 | KNN Loss: 5.010601997375488 | BCE Loss: 1.0314682722091675\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.060513019561768 | KNN Loss: 5.007209777832031 | BCE Loss: 1.0533031225204468\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.035398483276367 | KNN Loss: 5.0036845207214355 | BCE Loss: 1.0317137241363525\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.026628017425537 | KNN Loss: 5.014608383178711 | BCE Loss: 1.0120195150375366\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.017245292663574 | KNN Loss: 5.003528118133545 | BCE Loss: 1.0137171745300293\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.042633056640625 | KNN Loss: 5.0092244148254395 | BCE Loss: 1.033408761024475\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.016840934753418 | KNN Loss: 4.993303298950195 | BCE Loss: 1.0235378742218018\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.048758029937744 | KNN Loss: 5.011382102966309 | BCE Loss: 1.0373759269714355\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.040803909301758 | KNN Loss: 4.9902825355529785 | BCE Loss: 1.0505216121673584\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.054030895233154 | KNN Loss: 5.016973972320557 | BCE Loss: 1.0370569229125977\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.064360618591309 | KNN Loss: 5.023491859436035 | BCE Loss: 1.0408685207366943\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.007598876953125 | KNN Loss: 4.996194839477539 | BCE Loss: 1.011404275894165\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.071629524230957 | KNN Loss: 5.008091926574707 | BCE Loss: 1.0635374784469604\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.074423789978027 | KNN Loss: 5.021731853485107 | BCE Loss: 1.0526916980743408\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.04727840423584 | KNN Loss: 5.014908313751221 | BCE Loss: 1.0323703289031982\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.059684753417969 | KNN Loss: 5.023850917816162 | BCE Loss: 1.0358335971832275\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.042222023010254 | KNN Loss: 5.017537593841553 | BCE Loss: 1.0246843099594116\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.072714805603027 | KNN Loss: 5.034271717071533 | BCE Loss: 1.0384430885314941\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.014151096343994 | KNN Loss: 4.995804786682129 | BCE Loss: 1.0183463096618652\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.033513069152832 | KNN Loss: 4.996016979217529 | BCE Loss: 1.0374963283538818\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.078563213348389 | KNN Loss: 5.046689987182617 | BCE Loss: 1.031873106956482\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.023244857788086 | KNN Loss: 4.9850873947143555 | BCE Loss: 1.038157343864441\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.048935413360596 | KNN Loss: 5.022597789764404 | BCE Loss: 1.0263375043869019\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.034151077270508 | KNN Loss: 4.993901252746582 | BCE Loss: 1.0402497053146362\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.061756134033203 | KNN Loss: 5.006965160369873 | BCE Loss: 1.05479097366333\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.030898571014404 | KNN Loss: 4.9994659423828125 | BCE Loss: 1.0314326286315918\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.092668533325195 | KNN Loss: 5.046597003936768 | BCE Loss: 1.0460714101791382\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.072839260101318 | KNN Loss: 5.042929172515869 | BCE Loss: 1.0299102067947388\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.028162956237793 | KNN Loss: 5.009297847747803 | BCE Loss: 1.0188653469085693\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.043429851531982 | KNN Loss: 5.005238056182861 | BCE Loss: 1.0381916761398315\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.020204544067383 | KNN Loss: 4.9873504638671875 | BCE Loss: 1.0328543186187744\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.039920806884766 | KNN Loss: 4.981204509735107 | BCE Loss: 1.058716058731079\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.054471969604492 | KNN Loss: 5.026160717010498 | BCE Loss: 1.0283113718032837\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.057657241821289 | KNN Loss: 5.02473258972168 | BCE Loss: 1.0329244136810303\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.049453258514404 | KNN Loss: 5.014737129211426 | BCE Loss: 1.0347161293029785\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.0268144607543945 | KNN Loss: 5.0101752281188965 | BCE Loss: 1.0166394710540771\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.048061370849609 | KNN Loss: 5.017923355102539 | BCE Loss: 1.0301380157470703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.0429534912109375 | KNN Loss: 5.013554573059082 | BCE Loss: 1.0293986797332764\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.009341239929199 | KNN Loss: 4.989260673522949 | BCE Loss: 1.020080804824829\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.020467281341553 | KNN Loss: 5.001977920532227 | BCE Loss: 1.0184894800186157\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.085610866546631 | KNN Loss: 5.029135704040527 | BCE Loss: 1.056475281715393\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.028777599334717 | KNN Loss: 4.985438346862793 | BCE Loss: 1.0433392524719238\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.029018878936768 | KNN Loss: 5.047170162200928 | BCE Loss: 0.9818486571311951\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.033347129821777 | KNN Loss: 5.006369590759277 | BCE Loss: 1.0269775390625\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.041639804840088 | KNN Loss: 4.99087381362915 | BCE Loss: 1.0507659912109375\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.044112205505371 | KNN Loss: 5.021937370300293 | BCE Loss: 1.0221749544143677\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.02534294128418 | KNN Loss: 5.000778675079346 | BCE Loss: 1.0245640277862549\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.041988372802734 | KNN Loss: 5.0049309730529785 | BCE Loss: 1.0370572805404663\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.02645206451416 | KNN Loss: 5.016460418701172 | BCE Loss: 1.0099914073944092\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.034692764282227 | KNN Loss: 4.993129730224609 | BCE Loss: 1.0415630340576172\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.040012359619141 | KNN Loss: 5.003376007080078 | BCE Loss: 1.0366365909576416\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.073153018951416 | KNN Loss: 5.037101745605469 | BCE Loss: 1.0360512733459473\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.055004596710205 | KNN Loss: 5.012148857116699 | BCE Loss: 1.0428557395935059\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.067692756652832 | KNN Loss: 5.053818702697754 | BCE Loss: 1.0138740539550781\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.038129806518555 | KNN Loss: 5.000429630279541 | BCE Loss: 1.0377001762390137\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.064059257507324 | KNN Loss: 5.01315975189209 | BCE Loss: 1.0508992671966553\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.033729553222656 | KNN Loss: 5.022670269012451 | BCE Loss: 1.0110595226287842\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.044164180755615 | KNN Loss: 5.009509086608887 | BCE Loss: 1.0346550941467285\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.019665241241455 | KNN Loss: 5.023509502410889 | BCE Loss: 0.9961556196212769\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.03326416015625 | KNN Loss: 5.015091419219971 | BCE Loss: 1.0181729793548584\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.07798957824707 | KNN Loss: 5.028075218200684 | BCE Loss: 1.0499143600463867\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.065722465515137 | KNN Loss: 5.011907577514648 | BCE Loss: 1.0538148880004883\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.094345569610596 | KNN Loss: 5.047728538513184 | BCE Loss: 1.0466171503067017\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.089229106903076 | KNN Loss: 5.04642915725708 | BCE Loss: 1.042799949645996\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.055943489074707 | KNN Loss: 5.014678478240967 | BCE Loss: 1.0412652492523193\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.007256031036377 | KNN Loss: 5.007708549499512 | BCE Loss: 0.9995474815368652\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.039871692657471 | KNN Loss: 5.027129650115967 | BCE Loss: 1.012742042541504\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.004706382751465 | KNN Loss: 4.995538711547852 | BCE Loss: 1.0091676712036133\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.05185079574585 | KNN Loss: 5.008859157562256 | BCE Loss: 1.0429916381835938\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.024902820587158 | KNN Loss: 5.0366315841674805 | BCE Loss: 0.9882711172103882\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 5.99370002746582 | KNN Loss: 4.975837230682373 | BCE Loss: 1.0178630352020264\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.022347450256348 | KNN Loss: 4.982151508331299 | BCE Loss: 1.040196180343628\n",
      "Epoch   114: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.039170265197754 | KNN Loss: 4.998373508453369 | BCE Loss: 1.0407969951629639\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.035505771636963 | KNN Loss: 4.987566947937012 | BCE Loss: 1.0479387044906616\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.031586170196533 | KNN Loss: 5.018113136291504 | BCE Loss: 1.0134730339050293\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.010528564453125 | KNN Loss: 4.998795509338379 | BCE Loss: 1.0117332935333252\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.095187187194824 | KNN Loss: 5.051384449005127 | BCE Loss: 1.0438027381896973\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.105528354644775 | KNN Loss: 5.083079814910889 | BCE Loss: 1.0224485397338867\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.061431884765625 | KNN Loss: 5.03485631942749 | BCE Loss: 1.0265758037567139\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 5.997446060180664 | KNN Loss: 4.994385719299316 | BCE Loss: 1.0030605792999268\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.036547660827637 | KNN Loss: 5.018642902374268 | BCE Loss: 1.0179047584533691\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.000187873840332 | KNN Loss: 4.9800825119018555 | BCE Loss: 1.0201053619384766\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.05028772354126 | KNN Loss: 5.012080192565918 | BCE Loss: 1.0382076501846313\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.068270683288574 | KNN Loss: 5.022448539733887 | BCE Loss: 1.0458223819732666\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.051855087280273 | KNN Loss: 5.032263278961182 | BCE Loss: 1.0195918083190918\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.019330024719238 | KNN Loss: 4.979660511016846 | BCE Loss: 1.039669394493103\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 5.993579864501953 | KNN Loss: 4.994485378265381 | BCE Loss: 0.9990943670272827\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.044438362121582 | KNN Loss: 5.0218682289123535 | BCE Loss: 1.0225701332092285\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.006111145019531 | KNN Loss: 4.990139007568359 | BCE Loss: 1.0159718990325928\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 5.997531414031982 | KNN Loss: 4.986039638519287 | BCE Loss: 1.0114916563034058\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.012270450592041 | KNN Loss: 5.003403186798096 | BCE Loss: 1.0088672637939453\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.120222568511963 | KNN Loss: 5.050808429718018 | BCE Loss: 1.0694141387939453\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.014686584472656 | KNN Loss: 4.988022327423096 | BCE Loss: 1.0266644954681396\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.046820163726807 | KNN Loss: 5.01743221282959 | BCE Loss: 1.0293878316879272\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.034811496734619 | KNN Loss: 5.027263164520264 | BCE Loss: 1.007548451423645\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.003807067871094 | KNN Loss: 4.991125583648682 | BCE Loss: 1.0126813650131226\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.018698692321777 | KNN Loss: 4.975203037261963 | BCE Loss: 1.0434954166412354\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 5.9956841468811035 | KNN Loss: 4.999244689941406 | BCE Loss: 0.9964394569396973\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.038149833679199 | KNN Loss: 5.017609119415283 | BCE Loss: 1.020540475845337\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.046167850494385 | KNN Loss: 5.00016975402832 | BCE Loss: 1.045998215675354\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.04886531829834 | KNN Loss: 5.007660865783691 | BCE Loss: 1.0412042140960693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.039344787597656 | KNN Loss: 5.014034748077393 | BCE Loss: 1.0253098011016846\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.056617259979248 | KNN Loss: 4.993488788604736 | BCE Loss: 1.0631283521652222\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.022832870483398 | KNN Loss: 4.99938440322876 | BCE Loss: 1.0234485864639282\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.041475296020508 | KNN Loss: 5.001172065734863 | BCE Loss: 1.040303349494934\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.025002479553223 | KNN Loss: 4.995077133178711 | BCE Loss: 1.0299252271652222\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.082763671875 | KNN Loss: 5.066988468170166 | BCE Loss: 1.0157749652862549\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.021142482757568 | KNN Loss: 5.0023579597473145 | BCE Loss: 1.0187846422195435\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.016135215759277 | KNN Loss: 4.994772911071777 | BCE Loss: 1.021362543106079\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.005899906158447 | KNN Loss: 4.980459690093994 | BCE Loss: 1.0254402160644531\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.080739974975586 | KNN Loss: 5.015785217285156 | BCE Loss: 1.0649545192718506\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.031229496002197 | KNN Loss: 4.997437000274658 | BCE Loss: 1.0337923765182495\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.033029079437256 | KNN Loss: 4.9872965812683105 | BCE Loss: 1.0457324981689453\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.005642890930176 | KNN Loss: 4.990357875823975 | BCE Loss: 1.015284776687622\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.066849231719971 | KNN Loss: 5.0571675300598145 | BCE Loss: 1.0096815824508667\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.071869373321533 | KNN Loss: 5.000281810760498 | BCE Loss: 1.0715876817703247\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.075153350830078 | KNN Loss: 5.036028861999512 | BCE Loss: 1.0391243696212769\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.05588436126709 | KNN Loss: 5.033112525939941 | BCE Loss: 1.0227715969085693\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.014942169189453 | KNN Loss: 4.996671676635742 | BCE Loss: 1.0182702541351318\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.035440921783447 | KNN Loss: 4.980490207672119 | BCE Loss: 1.0549507141113281\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.05988073348999 | KNN Loss: 5.02622652053833 | BCE Loss: 1.0336543321609497\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.045231819152832 | KNN Loss: 5.01898193359375 | BCE Loss: 1.0262501239776611\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.053003311157227 | KNN Loss: 5.005312919616699 | BCE Loss: 1.0476901531219482\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.058881759643555 | KNN Loss: 5.035539627075195 | BCE Loss: 1.0233421325683594\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.056411266326904 | KNN Loss: 5.021603107452393 | BCE Loss: 1.0348081588745117\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.011104583740234 | KNN Loss: 4.98409366607666 | BCE Loss: 1.0270111560821533\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.079967498779297 | KNN Loss: 5.038492202758789 | BCE Loss: 1.041475534439087\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.033545970916748 | KNN Loss: 5.004513263702393 | BCE Loss: 1.029032588005066\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.0178961753845215 | KNN Loss: 5.008190155029297 | BCE Loss: 1.009705901145935\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.028841495513916 | KNN Loss: 4.996067047119141 | BCE Loss: 1.0327743291854858\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.056029319763184 | KNN Loss: 5.022702217102051 | BCE Loss: 1.0333268642425537\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.03837776184082 | KNN Loss: 5.018743515014648 | BCE Loss: 1.0196340084075928\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.073873043060303 | KNN Loss: 5.012789249420166 | BCE Loss: 1.0610837936401367\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.0330095291137695 | KNN Loss: 5.007477283477783 | BCE Loss: 1.0255322456359863\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.020477771759033 | KNN Loss: 4.995169639587402 | BCE Loss: 1.0253081321716309\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.0732340812683105 | KNN Loss: 5.035107612609863 | BCE Loss: 1.0381265878677368\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.014836311340332 | KNN Loss: 4.986538410186768 | BCE Loss: 1.0282981395721436\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.069680213928223 | KNN Loss: 5.024419784545898 | BCE Loss: 1.0452606678009033\n",
      "Epoch   125: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.015070915222168 | KNN Loss: 4.984715938568115 | BCE Loss: 1.0303547382354736\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.033443450927734 | KNN Loss: 4.998143672943115 | BCE Loss: 1.0353000164031982\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.051196098327637 | KNN Loss: 5.022713661193848 | BCE Loss: 1.0284826755523682\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.038897514343262 | KNN Loss: 4.997384548187256 | BCE Loss: 1.0415127277374268\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.058744430541992 | KNN Loss: 5.035225868225098 | BCE Loss: 1.0235183238983154\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.00899076461792 | KNN Loss: 4.9769978523254395 | BCE Loss: 1.0319929122924805\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.04341459274292 | KNN Loss: 5.000967502593994 | BCE Loss: 1.0424472093582153\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.02020788192749 | KNN Loss: 5.02608585357666 | BCE Loss: 0.9941220283508301\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.000238418579102 | KNN Loss: 5.025450229644775 | BCE Loss: 0.9747881889343262\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.036286354064941 | KNN Loss: 4.9851579666137695 | BCE Loss: 1.0511283874511719\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.019177436828613 | KNN Loss: 4.998752117156982 | BCE Loss: 1.0204253196716309\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.053208351135254 | KNN Loss: 5.013707637786865 | BCE Loss: 1.0395009517669678\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.04102897644043 | KNN Loss: 4.998190402984619 | BCE Loss: 1.0428388118743896\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.040680885314941 | KNN Loss: 5.0070037841796875 | BCE Loss: 1.0336768627166748\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.060154438018799 | KNN Loss: 4.9866943359375 | BCE Loss: 1.0734602212905884\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.066194534301758 | KNN Loss: 5.024292469024658 | BCE Loss: 1.0419018268585205\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.094881057739258 | KNN Loss: 5.0025129318237305 | BCE Loss: 1.0923681259155273\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.07217264175415 | KNN Loss: 5.056538105010986 | BCE Loss: 1.0156346559524536\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.040942192077637 | KNN Loss: 5.000932216644287 | BCE Loss: 1.0400100946426392\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.020574569702148 | KNN Loss: 4.976171970367432 | BCE Loss: 1.0444025993347168\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.058684349060059 | KNN Loss: 5.0001726150512695 | BCE Loss: 1.058511734008789\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.071022033691406 | KNN Loss: 5.024496555328369 | BCE Loss: 1.046525239944458\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.04389762878418 | KNN Loss: 5.022506237030029 | BCE Loss: 1.02139151096344\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.0319414138793945 | KNN Loss: 4.996911525726318 | BCE Loss: 1.0350300073623657\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.047121524810791 | KNN Loss: 5.011315822601318 | BCE Loss: 1.0358058214187622\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.019223213195801 | KNN Loss: 5.0106611251831055 | BCE Loss: 1.0085618495941162\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.022342205047607 | KNN Loss: 5.014849662780762 | BCE Loss: 1.0074925422668457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.042799472808838 | KNN Loss: 4.97893762588501 | BCE Loss: 1.0638618469238281\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.04741096496582 | KNN Loss: 5.011135578155518 | BCE Loss: 1.0362751483917236\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 5.997302055358887 | KNN Loss: 4.984949588775635 | BCE Loss: 1.0123522281646729\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.0244340896606445 | KNN Loss: 4.98615026473999 | BCE Loss: 1.0382840633392334\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.0276079177856445 | KNN Loss: 4.988617897033691 | BCE Loss: 1.0389901399612427\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.055008888244629 | KNN Loss: 5.041882514953613 | BCE Loss: 1.0131261348724365\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.05958366394043 | KNN Loss: 5.048610687255859 | BCE Loss: 1.0109727382659912\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.0307393074035645 | KNN Loss: 5.018115520477295 | BCE Loss: 1.012623906135559\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.034761428833008 | KNN Loss: 5.014577865600586 | BCE Loss: 1.020183801651001\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.057298183441162 | KNN Loss: 5.018360137939453 | BCE Loss: 1.038938045501709\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.0214152336120605 | KNN Loss: 5.008846282958984 | BCE Loss: 1.0125688314437866\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.0355072021484375 | KNN Loss: 5.031599998474121 | BCE Loss: 1.003907322883606\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.001370906829834 | KNN Loss: 4.991722106933594 | BCE Loss: 1.0096486806869507\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.058874130249023 | KNN Loss: 5.011713027954102 | BCE Loss: 1.0471608638763428\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.047698974609375 | KNN Loss: 5.00460958480835 | BCE Loss: 1.043089509010315\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.086670398712158 | KNN Loss: 5.039008140563965 | BCE Loss: 1.0476621389389038\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.00885534286499 | KNN Loss: 5.0152435302734375 | BCE Loss: 0.9936118721961975\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.013596534729004 | KNN Loss: 4.9936041831970215 | BCE Loss: 1.0199925899505615\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.084798812866211 | KNN Loss: 5.055956840515137 | BCE Loss: 1.0288419723510742\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.039816856384277 | KNN Loss: 5.021389007568359 | BCE Loss: 1.018428087234497\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.045223712921143 | KNN Loss: 4.9929327964782715 | BCE Loss: 1.052290916442871\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.001165390014648 | KNN Loss: 4.988783836364746 | BCE Loss: 1.0123813152313232\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.021827697753906 | KNN Loss: 5.0123090744018555 | BCE Loss: 1.0095186233520508\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.045633316040039 | KNN Loss: 5.002593517303467 | BCE Loss: 1.0430395603179932\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.023792266845703 | KNN Loss: 4.98129415512085 | BCE Loss: 1.0424983501434326\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.053311347961426 | KNN Loss: 5.009753704071045 | BCE Loss: 1.0435576438903809\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.02110481262207 | KNN Loss: 4.998137474060059 | BCE Loss: 1.0229675769805908\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.068170547485352 | KNN Loss: 5.0142621994018555 | BCE Loss: 1.0539085865020752\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.059688568115234 | KNN Loss: 5.042654037475586 | BCE Loss: 1.0170342922210693\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.078063011169434 | KNN Loss: 5.044109344482422 | BCE Loss: 1.0339537858963013\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.035594463348389 | KNN Loss: 5.007424354553223 | BCE Loss: 1.028170108795166\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.014848709106445 | KNN Loss: 4.994787216186523 | BCE Loss: 1.0200612545013428\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.047317028045654 | KNN Loss: 5.009280204772949 | BCE Loss: 1.0380367040634155\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.069425582885742 | KNN Loss: 5.060640335083008 | BCE Loss: 1.0087851285934448\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.061740398406982 | KNN Loss: 5.021045684814453 | BCE Loss: 1.0406947135925293\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.0489821434021 | KNN Loss: 5.010605335235596 | BCE Loss: 1.0383766889572144\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.042106628417969 | KNN Loss: 5.027931213378906 | BCE Loss: 1.014175534248352\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.0250139236450195 | KNN Loss: 4.98634147644043 | BCE Loss: 1.0386724472045898\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.043111324310303 | KNN Loss: 5.013379096984863 | BCE Loss: 1.029732346534729\n",
      "Epoch   136: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.083133697509766 | KNN Loss: 5.045487403869629 | BCE Loss: 1.0376461744308472\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.041166305541992 | KNN Loss: 5.005865573883057 | BCE Loss: 1.0353009700775146\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.012914657592773 | KNN Loss: 5.002622604370117 | BCE Loss: 1.0102919340133667\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.043316841125488 | KNN Loss: 5.002627372741699 | BCE Loss: 1.040689468383789\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.056388854980469 | KNN Loss: 4.998241424560547 | BCE Loss: 1.0581474304199219\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.074875354766846 | KNN Loss: 5.039478778839111 | BCE Loss: 1.0353964567184448\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.030850410461426 | KNN Loss: 5.012948036193848 | BCE Loss: 1.0179026126861572\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.048327445983887 | KNN Loss: 5.007324695587158 | BCE Loss: 1.0410025119781494\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.065682888031006 | KNN Loss: 5.029047966003418 | BCE Loss: 1.036634922027588\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.025105953216553 | KNN Loss: 4.979234218597412 | BCE Loss: 1.0458717346191406\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.085618019104004 | KNN Loss: 5.042654514312744 | BCE Loss: 1.0429633855819702\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.004347801208496 | KNN Loss: 4.979358196258545 | BCE Loss: 1.0249896049499512\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 5.991234302520752 | KNN Loss: 4.979238033294678 | BCE Loss: 1.0119962692260742\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.0169477462768555 | KNN Loss: 4.9998555183410645 | BCE Loss: 1.0170921087265015\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.022819995880127 | KNN Loss: 5.027337551116943 | BCE Loss: 0.995482325553894\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.047316551208496 | KNN Loss: 5.007156848907471 | BCE Loss: 1.040159821510315\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.037837982177734 | KNN Loss: 5.020846366882324 | BCE Loss: 1.0169914960861206\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.023211479187012 | KNN Loss: 4.990974426269531 | BCE Loss: 1.0322372913360596\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.0576958656311035 | KNN Loss: 5.04487419128418 | BCE Loss: 1.0128216743469238\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.027490615844727 | KNN Loss: 5.00030517578125 | BCE Loss: 1.0271854400634766\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.037308692932129 | KNN Loss: 4.9939727783203125 | BCE Loss: 1.0433361530303955\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.039729118347168 | KNN Loss: 5.004682540893555 | BCE Loss: 1.0350465774536133\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.049210548400879 | KNN Loss: 5.037998199462891 | BCE Loss: 1.0112124681472778\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.042457580566406 | KNN Loss: 4.98874568939209 | BCE Loss: 1.053712010383606\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.045293807983398 | KNN Loss: 5.010376930236816 | BCE Loss: 1.034916877746582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.061967849731445 | KNN Loss: 5.016207218170166 | BCE Loss: 1.0457608699798584\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.068056106567383 | KNN Loss: 5.016519069671631 | BCE Loss: 1.051537275314331\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.016406059265137 | KNN Loss: 4.98006534576416 | BCE Loss: 1.0363404750823975\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.089438438415527 | KNN Loss: 5.06364631652832 | BCE Loss: 1.0257923603057861\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.032982349395752 | KNN Loss: 5.0310492515563965 | BCE Loss: 1.001933217048645\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.099852561950684 | KNN Loss: 5.047641277313232 | BCE Loss: 1.0522111654281616\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.1062188148498535 | KNN Loss: 5.032506942749023 | BCE Loss: 1.07371187210083\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.0432586669921875 | KNN Loss: 5.015885353088379 | BCE Loss: 1.0273735523223877\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.113668918609619 | KNN Loss: 5.029752254486084 | BCE Loss: 1.0839166641235352\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.033801555633545 | KNN Loss: 5.00295877456665 | BCE Loss: 1.030842900276184\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 5.997159004211426 | KNN Loss: 4.990349769592285 | BCE Loss: 1.0068089962005615\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.072656631469727 | KNN Loss: 5.020522594451904 | BCE Loss: 1.0521337985992432\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.0605549812316895 | KNN Loss: 5.007861614227295 | BCE Loss: 1.0526933670043945\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.087545394897461 | KNN Loss: 5.069774627685547 | BCE Loss: 1.017770528793335\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.0455851554870605 | KNN Loss: 5.016438961029053 | BCE Loss: 1.0291463136672974\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.0391998291015625 | KNN Loss: 5.020754337310791 | BCE Loss: 1.0184457302093506\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.01491641998291 | KNN Loss: 4.970383644104004 | BCE Loss: 1.0445330142974854\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.024419784545898 | KNN Loss: 5.016280651092529 | BCE Loss: 1.0081391334533691\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.035938262939453 | KNN Loss: 5.048074722290039 | BCE Loss: 0.9878635406494141\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 6.0423688888549805 | KNN Loss: 5.007060527801514 | BCE Loss: 1.0353082418441772\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.035362720489502 | KNN Loss: 5.019041061401367 | BCE Loss: 1.0163217782974243\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.066085338592529 | KNN Loss: 5.030392646789551 | BCE Loss: 1.035692811012268\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.016998767852783 | KNN Loss: 5.011768341064453 | BCE Loss: 1.00523042678833\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.071869373321533 | KNN Loss: 5.0401482582092285 | BCE Loss: 1.0317211151123047\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 5.989060401916504 | KNN Loss: 4.976669788360596 | BCE Loss: 1.0123908519744873\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.031398773193359 | KNN Loss: 4.998937129974365 | BCE Loss: 1.0324618816375732\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.000458717346191 | KNN Loss: 4.997602462768555 | BCE Loss: 1.0028563737869263\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.015300750732422 | KNN Loss: 4.987631320953369 | BCE Loss: 1.0276695489883423\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.057216644287109 | KNN Loss: 5.035594940185547 | BCE Loss: 1.0216217041015625\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.041461944580078 | KNN Loss: 5.004373073577881 | BCE Loss: 1.0370888710021973\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.023702621459961 | KNN Loss: 5.003722667694092 | BCE Loss: 1.0199798345565796\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.046558380126953 | KNN Loss: 4.988935947418213 | BCE Loss: 1.0576221942901611\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.032266616821289 | KNN Loss: 4.9940505027771 | BCE Loss: 1.0382158756256104\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.040769577026367 | KNN Loss: 4.994032382965088 | BCE Loss: 1.0467370748519897\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 5.996064186096191 | KNN Loss: 4.978891372680664 | BCE Loss: 1.0171726942062378\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.060927391052246 | KNN Loss: 4.996585369110107 | BCE Loss: 1.0643420219421387\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.072824954986572 | KNN Loss: 5.056139945983887 | BCE Loss: 1.016684889793396\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.039007186889648 | KNN Loss: 5.015953063964844 | BCE Loss: 1.0230540037155151\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.0245232582092285 | KNN Loss: 5.001471996307373 | BCE Loss: 1.023051381111145\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.033050537109375 | KNN Loss: 5.004955768585205 | BCE Loss: 1.02809476852417\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.0264363288879395 | KNN Loss: 4.9972243309021 | BCE Loss: 1.0292121171951294\n",
      "Epoch   147: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.014861106872559 | KNN Loss: 4.995702743530273 | BCE Loss: 1.019158124923706\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 5.976825714111328 | KNN Loss: 4.980435848236084 | BCE Loss: 0.9963898658752441\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.0431671142578125 | KNN Loss: 5.003347873687744 | BCE Loss: 1.0398192405700684\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.063467025756836 | KNN Loss: 5.046507835388184 | BCE Loss: 1.0169591903686523\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.0673933029174805 | KNN Loss: 5.048823356628418 | BCE Loss: 1.018569827079773\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.037927150726318 | KNN Loss: 5.011631011962891 | BCE Loss: 1.0262960195541382\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.003780364990234 | KNN Loss: 4.9971604347229 | BCE Loss: 1.0066200494766235\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.0561323165893555 | KNN Loss: 5.02485990524292 | BCE Loss: 1.0312726497650146\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.062237739562988 | KNN Loss: 5.017904281616211 | BCE Loss: 1.0443332195281982\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.047504425048828 | KNN Loss: 5.005109786987305 | BCE Loss: 1.0423946380615234\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.081174850463867 | KNN Loss: 5.042372226715088 | BCE Loss: 1.0388028621673584\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.029691219329834 | KNN Loss: 4.9988274574279785 | BCE Loss: 1.0308637619018555\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.022260665893555 | KNN Loss: 5.006634712219238 | BCE Loss: 1.0156259536743164\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.010289192199707 | KNN Loss: 5.0025224685668945 | BCE Loss: 1.0077669620513916\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.021554946899414 | KNN Loss: 5.011778354644775 | BCE Loss: 1.0097767114639282\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.019236087799072 | KNN Loss: 5.01492166519165 | BCE Loss: 1.0043145418167114\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.0681681632995605 | KNN Loss: 5.00907039642334 | BCE Loss: 1.0590977668762207\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.078024864196777 | KNN Loss: 5.050642490386963 | BCE Loss: 1.0273821353912354\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.063568115234375 | KNN Loss: 5.0295820236206055 | BCE Loss: 1.0339860916137695\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.000456809997559 | KNN Loss: 4.994584560394287 | BCE Loss: 1.0058724880218506\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.004068851470947 | KNN Loss: 4.997548580169678 | BCE Loss: 1.006520390510559\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.141467571258545 | KNN Loss: 5.075111389160156 | BCE Loss: 1.0663560628890991\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.039466857910156 | KNN Loss: 5.028534889221191 | BCE Loss: 1.0109319686889648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.00905179977417 | KNN Loss: 4.994162559509277 | BCE Loss: 1.0148893594741821\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.037932395935059 | KNN Loss: 5.019835948944092 | BCE Loss: 1.0180965662002563\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.0169267654418945 | KNN Loss: 5.010152339935303 | BCE Loss: 1.0067743062973022\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.08650016784668 | KNN Loss: 5.043859004974365 | BCE Loss: 1.0426409244537354\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.03225564956665 | KNN Loss: 5.002765655517578 | BCE Loss: 1.0294899940490723\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.038332939147949 | KNN Loss: 5.00814151763916 | BCE Loss: 1.0301915407180786\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.0367889404296875 | KNN Loss: 5.026343822479248 | BCE Loss: 1.0104448795318604\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.032766819000244 | KNN Loss: 5.008983135223389 | BCE Loss: 1.0237836837768555\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.019044876098633 | KNN Loss: 5.00230598449707 | BCE Loss: 1.0167391300201416\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.0718231201171875 | KNN Loss: 5.028801918029785 | BCE Loss: 1.0430210828781128\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 5.998104095458984 | KNN Loss: 4.9866943359375 | BCE Loss: 1.0114095211029053\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.07369327545166 | KNN Loss: 5.043877124786377 | BCE Loss: 1.0298161506652832\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.028799533843994 | KNN Loss: 5.009746551513672 | BCE Loss: 1.0190531015396118\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.02620267868042 | KNN Loss: 4.991467475891113 | BCE Loss: 1.0347352027893066\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.100994110107422 | KNN Loss: 5.04233455657959 | BCE Loss: 1.058659315109253\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.0394206047058105 | KNN Loss: 5.023910045623779 | BCE Loss: 1.0155105590820312\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.064021110534668 | KNN Loss: 5.038808822631836 | BCE Loss: 1.025212287902832\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.0221333503723145 | KNN Loss: 4.99710750579834 | BCE Loss: 1.0250259637832642\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.038352012634277 | KNN Loss: 5.004234313964844 | BCE Loss: 1.0341176986694336\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.0418548583984375 | KNN Loss: 5.006821632385254 | BCE Loss: 1.0350329875946045\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.051467418670654 | KNN Loss: 5.032901287078857 | BCE Loss: 1.0185660123825073\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.055103302001953 | KNN Loss: 5.0116400718688965 | BCE Loss: 1.0434633493423462\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.0255937576293945 | KNN Loss: 5.001394271850586 | BCE Loss: 1.0241997241973877\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.020110130310059 | KNN Loss: 5.004738807678223 | BCE Loss: 1.0153714418411255\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.006914138793945 | KNN Loss: 4.990154266357422 | BCE Loss: 1.0167596340179443\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.049980640411377 | KNN Loss: 5.020970344543457 | BCE Loss: 1.02901029586792\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.043656826019287 | KNN Loss: 5.018075942993164 | BCE Loss: 1.0255807638168335\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.012875556945801 | KNN Loss: 4.97747802734375 | BCE Loss: 1.0353972911834717\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.058588981628418 | KNN Loss: 5.020102024078369 | BCE Loss: 1.0384869575500488\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.078835964202881 | KNN Loss: 5.007655143737793 | BCE Loss: 1.0711807012557983\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.02253532409668 | KNN Loss: 4.989636421203613 | BCE Loss: 1.0328989028930664\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.042180061340332 | KNN Loss: 4.9950127601623535 | BCE Loss: 1.0471670627593994\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.0452680587768555 | KNN Loss: 4.999011039733887 | BCE Loss: 1.0462567806243896\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.016518592834473 | KNN Loss: 4.98521614074707 | BCE Loss: 1.031302571296692\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.063870429992676 | KNN Loss: 5.03201150894165 | BCE Loss: 1.0318591594696045\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.017326354980469 | KNN Loss: 4.973804950714111 | BCE Loss: 1.0435216426849365\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.023421287536621 | KNN Loss: 5.006386756896973 | BCE Loss: 1.017034649848938\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.014446258544922 | KNN Loss: 5.010919570922852 | BCE Loss: 1.0035264492034912\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.047591209411621 | KNN Loss: 5.003537654876709 | BCE Loss: 1.0440537929534912\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.067154884338379 | KNN Loss: 5.013818740844727 | BCE Loss: 1.0533361434936523\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 5.975177764892578 | KNN Loss: 4.971087455749512 | BCE Loss: 1.0040903091430664\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.0516357421875 | KNN Loss: 5.027113437652588 | BCE Loss: 1.0245224237442017\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.045790672302246 | KNN Loss: 5.017316818237305 | BCE Loss: 1.0284736156463623\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.036144256591797 | KNN Loss: 4.99519157409668 | BCE Loss: 1.0409529209136963\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.048688888549805 | KNN Loss: 5.010419845581055 | BCE Loss: 1.038268804550171\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.063662528991699 | KNN Loss: 5.034749984741211 | BCE Loss: 1.0289123058319092\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.038364410400391 | KNN Loss: 5.011476993560791 | BCE Loss: 1.0268874168395996\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.042719841003418 | KNN Loss: 5.013668060302734 | BCE Loss: 1.0290515422821045\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.0322675704956055 | KNN Loss: 5.003180503845215 | BCE Loss: 1.0290870666503906\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.03618860244751 | KNN Loss: 4.990569114685059 | BCE Loss: 1.0456194877624512\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.057415008544922 | KNN Loss: 5.031895160675049 | BCE Loss: 1.025519847869873\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.010523796081543 | KNN Loss: 5.004526615142822 | BCE Loss: 1.0059973001480103\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.014773845672607 | KNN Loss: 4.993041515350342 | BCE Loss: 1.0217324495315552\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.0727105140686035 | KNN Loss: 5.032535076141357 | BCE Loss: 1.040175437927246\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.05344820022583 | KNN Loss: 5.012150764465332 | BCE Loss: 1.041297435760498\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.016523838043213 | KNN Loss: 4.990708351135254 | BCE Loss: 1.0258153676986694\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.014352321624756 | KNN Loss: 5.006391525268555 | BCE Loss: 1.0079607963562012\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 5.99894905090332 | KNN Loss: 4.984150409698486 | BCE Loss: 1.014798879623413\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.049820423126221 | KNN Loss: 5.029684066772461 | BCE Loss: 1.0201364755630493\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.0389556884765625 | KNN Loss: 5.012756824493408 | BCE Loss: 1.0261986255645752\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.029596328735352 | KNN Loss: 4.990365028381348 | BCE Loss: 1.0392310619354248\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.053427696228027 | KNN Loss: 5.020336627960205 | BCE Loss: 1.0330913066864014\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.0298357009887695 | KNN Loss: 5.026487827301025 | BCE Loss: 1.0033479928970337\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.0049543380737305 | KNN Loss: 4.984150409698486 | BCE Loss: 1.0208039283752441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.049095630645752 | KNN Loss: 5.015454292297363 | BCE Loss: 1.0336413383483887\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.022940635681152 | KNN Loss: 4.9958696365356445 | BCE Loss: 1.0270711183547974\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.0734710693359375 | KNN Loss: 5.02890682220459 | BCE Loss: 1.0445644855499268\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.045058250427246 | KNN Loss: 5.035076141357422 | BCE Loss: 1.0099823474884033\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.112523555755615 | KNN Loss: 5.0693359375 | BCE Loss: 1.0431876182556152\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.032015323638916 | KNN Loss: 4.998532772064209 | BCE Loss: 1.033482551574707\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.052684783935547 | KNN Loss: 5.023055553436279 | BCE Loss: 1.0296292304992676\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.057159423828125 | KNN Loss: 5.026776313781738 | BCE Loss: 1.0303828716278076\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.017592430114746 | KNN Loss: 5.023108005523682 | BCE Loss: 0.9944842457771301\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.1180419921875 | KNN Loss: 5.088564395904541 | BCE Loss: 1.029477834701538\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.0719313621521 | KNN Loss: 5.017651081085205 | BCE Loss: 1.0542802810668945\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.018004894256592 | KNN Loss: 5.009971618652344 | BCE Loss: 1.0080333948135376\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.056092262268066 | KNN Loss: 5.034680366516113 | BCE Loss: 1.0214121341705322\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.0587921142578125 | KNN Loss: 5.03451681137085 | BCE Loss: 1.024275302886963\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.023960113525391 | KNN Loss: 5.00436544418335 | BCE Loss: 1.0195945501327515\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.0681939125061035 | KNN Loss: 5.016393661499023 | BCE Loss: 1.05180025100708\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 5.987574100494385 | KNN Loss: 4.98284387588501 | BCE Loss: 1.004730224609375\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.028895854949951 | KNN Loss: 5.010202407836914 | BCE Loss: 1.0186935663223267\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.100514888763428 | KNN Loss: 5.043832778930664 | BCE Loss: 1.0566822290420532\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 5.989020347595215 | KNN Loss: 4.990967750549316 | BCE Loss: 0.9980523586273193\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.022302627563477 | KNN Loss: 4.9949541091918945 | BCE Loss: 1.0273483991622925\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.077232360839844 | KNN Loss: 5.066896438598633 | BCE Loss: 1.010335922241211\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.050210952758789 | KNN Loss: 5.030501365661621 | BCE Loss: 1.019709587097168\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.000473499298096 | KNN Loss: 4.997255325317383 | BCE Loss: 1.003218173980713\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.034882068634033 | KNN Loss: 4.99393367767334 | BCE Loss: 1.0409483909606934\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.095929145812988 | KNN Loss: 5.067284107208252 | BCE Loss: 1.0286450386047363\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.036051273345947 | KNN Loss: 5.011402130126953 | BCE Loss: 1.0246491432189941\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.0118865966796875 | KNN Loss: 4.979053974151611 | BCE Loss: 1.032832384109497\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.0212202072143555 | KNN Loss: 4.991433620452881 | BCE Loss: 1.0297865867614746\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.036942958831787 | KNN Loss: 4.998528480529785 | BCE Loss: 1.038414478302002\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.043063640594482 | KNN Loss: 4.9882426261901855 | BCE Loss: 1.0548210144042969\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.009316444396973 | KNN Loss: 5.002266883850098 | BCE Loss: 1.007049560546875\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.05265998840332 | KNN Loss: 5.042945384979248 | BCE Loss: 1.0097148418426514\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.055160999298096 | KNN Loss: 5.00786828994751 | BCE Loss: 1.0472925901412964\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 5.9890336990356445 | KNN Loss: 4.972518444061279 | BCE Loss: 1.0165154933929443\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.000249862670898 | KNN Loss: 4.970892906188965 | BCE Loss: 1.0293570756912231\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.0266571044921875 | KNN Loss: 4.995719909667969 | BCE Loss: 1.0309370756149292\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.040681838989258 | KNN Loss: 4.9901628494262695 | BCE Loss: 1.0505192279815674\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.092846870422363 | KNN Loss: 5.029664039611816 | BCE Loss: 1.0631828308105469\n",
      "Epoch   168: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.0023298263549805 | KNN Loss: 4.991007328033447 | BCE Loss: 1.0113224983215332\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.066372871398926 | KNN Loss: 5.019238471984863 | BCE Loss: 1.047134280204773\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.016263961791992 | KNN Loss: 4.989727973937988 | BCE Loss: 1.026535987854004\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.032454490661621 | KNN Loss: 4.9929046630859375 | BCE Loss: 1.0395495891571045\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.057966232299805 | KNN Loss: 5.029759883880615 | BCE Loss: 1.0282062292099\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.03437614440918 | KNN Loss: 4.999237060546875 | BCE Loss: 1.0351392030715942\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.0019707679748535 | KNN Loss: 4.988050937652588 | BCE Loss: 1.0139199495315552\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.077939987182617 | KNN Loss: 5.045019149780273 | BCE Loss: 1.0329207181930542\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.037843704223633 | KNN Loss: 5.024364948272705 | BCE Loss: 1.0134789943695068\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.033724784851074 | KNN Loss: 5.0236496925354 | BCE Loss: 1.0100749731063843\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.048405170440674 | KNN Loss: 5.025362968444824 | BCE Loss: 1.0230422019958496\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.026448726654053 | KNN Loss: 4.982456684112549 | BCE Loss: 1.0439921617507935\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.011241912841797 | KNN Loss: 4.997391700744629 | BCE Loss: 1.0138499736785889\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.0367913246154785 | KNN Loss: 5.019612789154053 | BCE Loss: 1.0171784162521362\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.016875743865967 | KNN Loss: 5.005921363830566 | BCE Loss: 1.0109542608261108\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 5.997406959533691 | KNN Loss: 4.978142738342285 | BCE Loss: 1.0192642211914062\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.048652172088623 | KNN Loss: 5.007740020751953 | BCE Loss: 1.04091215133667\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 5.998302936553955 | KNN Loss: 4.979116439819336 | BCE Loss: 1.0191864967346191\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.043605804443359 | KNN Loss: 5.016423225402832 | BCE Loss: 1.0271823406219482\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.050361633300781 | KNN Loss: 5.011307716369629 | BCE Loss: 1.0390539169311523\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.010451316833496 | KNN Loss: 4.97926139831543 | BCE Loss: 1.0311897993087769\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.003522872924805 | KNN Loss: 4.994712829589844 | BCE Loss: 1.0088098049163818\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.026810646057129 | KNN Loss: 5.000094413757324 | BCE Loss: 1.0267164707183838\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.054437160491943 | KNN Loss: 5.004170894622803 | BCE Loss: 1.050266146659851\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 5.989914894104004 | KNN Loss: 4.983376502990723 | BCE Loss: 1.0065381526947021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.031352996826172 | KNN Loss: 5.0040106773376465 | BCE Loss: 1.0273425579071045\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.039177894592285 | KNN Loss: 5.012136459350586 | BCE Loss: 1.0270415544509888\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.042013168334961 | KNN Loss: 5.027540683746338 | BCE Loss: 1.014472484588623\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.011489391326904 | KNN Loss: 4.990205764770508 | BCE Loss: 1.021283745765686\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.043184280395508 | KNN Loss: 5.000129699707031 | BCE Loss: 1.0430548191070557\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.073986053466797 | KNN Loss: 5.0400872230529785 | BCE Loss: 1.0338985919952393\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.0549421310424805 | KNN Loss: 5.002638339996338 | BCE Loss: 1.0523035526275635\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.084356784820557 | KNN Loss: 5.032279968261719 | BCE Loss: 1.052076816558838\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.018961429595947 | KNN Loss: 4.993305683135986 | BCE Loss: 1.0256558656692505\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.039722919464111 | KNN Loss: 5.00911808013916 | BCE Loss: 1.0306047201156616\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.048349857330322 | KNN Loss: 4.99389123916626 | BCE Loss: 1.0544586181640625\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.028411865234375 | KNN Loss: 5.020264625549316 | BCE Loss: 1.0081472396850586\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 5.9872260093688965 | KNN Loss: 4.976847171783447 | BCE Loss: 1.0103787183761597\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.026876449584961 | KNN Loss: 4.995968818664551 | BCE Loss: 1.030907392501831\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.036864757537842 | KNN Loss: 5.018607139587402 | BCE Loss: 1.01825749874115\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.040972709655762 | KNN Loss: 5.016969680786133 | BCE Loss: 1.024003267288208\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 5.995609760284424 | KNN Loss: 4.994708061218262 | BCE Loss: 1.0009015798568726\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.051159858703613 | KNN Loss: 4.996277332305908 | BCE Loss: 1.054882287979126\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.067863464355469 | KNN Loss: 5.037980079650879 | BCE Loss: 1.0298833847045898\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.058090686798096 | KNN Loss: 4.994810104370117 | BCE Loss: 1.0632805824279785\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.053073883056641 | KNN Loss: 5.0056233406066895 | BCE Loss: 1.0474504232406616\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.074165344238281 | KNN Loss: 5.043394565582275 | BCE Loss: 1.0307705402374268\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.013640403747559 | KNN Loss: 5.009038925170898 | BCE Loss: 1.0046014785766602\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.0654706954956055 | KNN Loss: 5.0222907066345215 | BCE Loss: 1.043179988861084\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.031589031219482 | KNN Loss: 4.9998369216918945 | BCE Loss: 1.031752109527588\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.0314812660217285 | KNN Loss: 4.999685287475586 | BCE Loss: 1.0317959785461426\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.05042028427124 | KNN Loss: 4.9969868659973145 | BCE Loss: 1.0534335374832153\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.018978118896484 | KNN Loss: 4.996591567993164 | BCE Loss: 1.0223865509033203\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.011350631713867 | KNN Loss: 4.998466968536377 | BCE Loss: 1.0128836631774902\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.047657489776611 | KNN Loss: 5.019046306610107 | BCE Loss: 1.028611183166504\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.000820636749268 | KNN Loss: 5.0097808837890625 | BCE Loss: 0.9910396933555603\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.024745464324951 | KNN Loss: 4.986911773681641 | BCE Loss: 1.0378336906433105\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.069633483886719 | KNN Loss: 5.030452728271484 | BCE Loss: 1.039180874824524\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.03762674331665 | KNN Loss: 4.988802433013916 | BCE Loss: 1.0488241910934448\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 5.990869998931885 | KNN Loss: 4.986971378326416 | BCE Loss: 1.0038987398147583\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 5.976363182067871 | KNN Loss: 4.992323875427246 | BCE Loss: 0.9840390682220459\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.048335075378418 | KNN Loss: 5.006567001342773 | BCE Loss: 1.0417683124542236\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.108741283416748 | KNN Loss: 5.073289394378662 | BCE Loss: 1.035451889038086\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.027710437774658 | KNN Loss: 5.004368305206299 | BCE Loss: 1.0233421325683594\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 6.041469097137451 | KNN Loss: 4.999118328094482 | BCE Loss: 1.0423508882522583\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.049715995788574 | KNN Loss: 5.011442184448242 | BCE Loss: 1.0382740497589111\n",
      "Epoch   179: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.012051582336426 | KNN Loss: 4.992908477783203 | BCE Loss: 1.0191433429718018\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.039409637451172 | KNN Loss: 5.017639636993408 | BCE Loss: 1.0217698812484741\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.01262092590332 | KNN Loss: 5.003165245056152 | BCE Loss: 1.0094558000564575\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.04711389541626 | KNN Loss: 5.012757301330566 | BCE Loss: 1.034356713294983\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.054647922515869 | KNN Loss: 5.017025947570801 | BCE Loss: 1.0376219749450684\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 5.975473403930664 | KNN Loss: 4.991271018981934 | BCE Loss: 0.98420250415802\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 6.039920330047607 | KNN Loss: 5.006126880645752 | BCE Loss: 1.0337934494018555\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.039791107177734 | KNN Loss: 4.99833345413208 | BCE Loss: 1.0414577722549438\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.062736988067627 | KNN Loss: 5.020824909210205 | BCE Loss: 1.0419121980667114\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.033832550048828 | KNN Loss: 5.001731872558594 | BCE Loss: 1.0321006774902344\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.023726463317871 | KNN Loss: 5.004696846008301 | BCE Loss: 1.0190298557281494\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.000739097595215 | KNN Loss: 4.984893798828125 | BCE Loss: 1.0158450603485107\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.062884330749512 | KNN Loss: 5.013585567474365 | BCE Loss: 1.0492990016937256\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.038613319396973 | KNN Loss: 5.008659839630127 | BCE Loss: 1.0299537181854248\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.00571346282959 | KNN Loss: 4.995005130767822 | BCE Loss: 1.010708212852478\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.04606294631958 | KNN Loss: 5.0207953453063965 | BCE Loss: 1.0252676010131836\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.04851770401001 | KNN Loss: 5.017523765563965 | BCE Loss: 1.030993938446045\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.032571792602539 | KNN Loss: 4.983994483947754 | BCE Loss: 1.0485775470733643\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.076885223388672 | KNN Loss: 5.053526401519775 | BCE Loss: 1.0233588218688965\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.028575897216797 | KNN Loss: 5.034596920013428 | BCE Loss: 0.9939790368080139\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 6.031644344329834 | KNN Loss: 5.00952672958374 | BCE Loss: 1.0221176147460938\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.08863639831543 | KNN Loss: 5.012042045593262 | BCE Loss: 1.076594352722168\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 5.999292373657227 | KNN Loss: 4.985050201416016 | BCE Loss: 1.01424241065979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.055254936218262 | KNN Loss: 5.032163619995117 | BCE Loss: 1.0230915546417236\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.007072925567627 | KNN Loss: 4.979960918426514 | BCE Loss: 1.0271121263504028\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.043704509735107 | KNN Loss: 5.002006530761719 | BCE Loss: 1.0416978597640991\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.087887763977051 | KNN Loss: 5.046228408813477 | BCE Loss: 1.0416591167449951\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.014974117279053 | KNN Loss: 4.987212657928467 | BCE Loss: 1.027761459350586\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.049522399902344 | KNN Loss: 5.01785945892334 | BCE Loss: 1.0316628217697144\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.043483734130859 | KNN Loss: 5.009125709533691 | BCE Loss: 1.0343579053878784\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.023841857910156 | KNN Loss: 4.994759559631348 | BCE Loss: 1.0290822982788086\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.01015567779541 | KNN Loss: 4.980922698974609 | BCE Loss: 1.0292332172393799\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.044656753540039 | KNN Loss: 5.012517929077148 | BCE Loss: 1.0321385860443115\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.059732437133789 | KNN Loss: 5.014671325683594 | BCE Loss: 1.0450609922409058\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.051814079284668 | KNN Loss: 5.006471157073975 | BCE Loss: 1.0453428030014038\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.014868259429932 | KNN Loss: 4.976736545562744 | BCE Loss: 1.0381317138671875\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 5.991272449493408 | KNN Loss: 4.983455181121826 | BCE Loss: 1.007817268371582\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 5.999820232391357 | KNN Loss: 4.993244171142578 | BCE Loss: 1.0065760612487793\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 5.99544620513916 | KNN Loss: 4.997048377990723 | BCE Loss: 0.9983978867530823\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.030930995941162 | KNN Loss: 4.998613357543945 | BCE Loss: 1.0323177576065063\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.0458292961120605 | KNN Loss: 5.011502742767334 | BCE Loss: 1.0343266725540161\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.010673999786377 | KNN Loss: 4.9738264083862305 | BCE Loss: 1.036847472190857\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 5.99290657043457 | KNN Loss: 4.980003356933594 | BCE Loss: 1.0129029750823975\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.034730434417725 | KNN Loss: 5.012811183929443 | BCE Loss: 1.0219192504882812\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 6.0212626457214355 | KNN Loss: 4.988826751708984 | BCE Loss: 1.0324358940124512\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.164657115936279 | KNN Loss: 5.128331184387207 | BCE Loss: 1.0363258123397827\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.014654159545898 | KNN Loss: 5.016395568847656 | BCE Loss: 0.9982588291168213\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.0562286376953125 | KNN Loss: 5.02468204498291 | BCE Loss: 1.0315463542938232\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.028902530670166 | KNN Loss: 4.995235919952393 | BCE Loss: 1.033666729927063\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 5.9872589111328125 | KNN Loss: 4.986230373382568 | BCE Loss: 1.0010285377502441\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.020652770996094 | KNN Loss: 4.989809036254883 | BCE Loss: 1.0308434963226318\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.000219821929932 | KNN Loss: 5.007465839385986 | BCE Loss: 0.9927538633346558\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.063591480255127 | KNN Loss: 5.017007827758789 | BCE Loss: 1.046583652496338\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.0667901039123535 | KNN Loss: 5.012737274169922 | BCE Loss: 1.0540529489517212\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.024219512939453 | KNN Loss: 5.019233226776123 | BCE Loss: 1.004986047744751\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.01501989364624 | KNN Loss: 4.978509426116943 | BCE Loss: 1.0365103483200073\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.1178483963012695 | KNN Loss: 5.096151828765869 | BCE Loss: 1.0216968059539795\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.057004451751709 | KNN Loss: 5.03289794921875 | BCE Loss: 1.0241063833236694\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.074419021606445 | KNN Loss: 5.010108947753906 | BCE Loss: 1.0643103122711182\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.030637741088867 | KNN Loss: 4.995786666870117 | BCE Loss: 1.034851312637329\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.092833995819092 | KNN Loss: 5.027810573577881 | BCE Loss: 1.0650235414505005\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.031580924987793 | KNN Loss: 5.001968860626221 | BCE Loss: 1.0296118259429932\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.026566028594971 | KNN Loss: 5.024702548980713 | BCE Loss: 1.0018634796142578\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.086483001708984 | KNN Loss: 5.056111812591553 | BCE Loss: 1.0303711891174316\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.0232462882995605 | KNN Loss: 5.00603723526001 | BCE Loss: 1.0172089338302612\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.024733543395996 | KNN Loss: 5.004030704498291 | BCE Loss: 1.0207030773162842\n",
      "Epoch   190: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.0507612228393555 | KNN Loss: 5.021335124969482 | BCE Loss: 1.029426097869873\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.006147384643555 | KNN Loss: 5.006088733673096 | BCE Loss: 1.0000587701797485\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.028594017028809 | KNN Loss: 4.999776363372803 | BCE Loss: 1.028817892074585\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.022824287414551 | KNN Loss: 4.990171432495117 | BCE Loss: 1.0326526165008545\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.032958984375 | KNN Loss: 5.000828742980957 | BCE Loss: 1.0321300029754639\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.038448810577393 | KNN Loss: 5.044904708862305 | BCE Loss: 0.9935439825057983\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.095794677734375 | KNN Loss: 5.058358192443848 | BCE Loss: 1.0374367237091064\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.024014949798584 | KNN Loss: 5.008325099945068 | BCE Loss: 1.015689730644226\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.030970573425293 | KNN Loss: 5.011364459991455 | BCE Loss: 1.0196058750152588\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.061594486236572 | KNN Loss: 5.027134418487549 | BCE Loss: 1.0344600677490234\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.036866188049316 | KNN Loss: 4.982104301452637 | BCE Loss: 1.0547621250152588\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.034191131591797 | KNN Loss: 4.997349262237549 | BCE Loss: 1.036841630935669\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.024502754211426 | KNN Loss: 4.996160984039307 | BCE Loss: 1.0283420085906982\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 6.028400421142578 | KNN Loss: 5.000186443328857 | BCE Loss: 1.0282139778137207\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.046169281005859 | KNN Loss: 5.009313106536865 | BCE Loss: 1.0368561744689941\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.038737773895264 | KNN Loss: 4.984063625335693 | BCE Loss: 1.0546741485595703\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.109076023101807 | KNN Loss: 5.087520122528076 | BCE Loss: 1.02155601978302\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.022964000701904 | KNN Loss: 4.996578216552734 | BCE Loss: 1.0263856649398804\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.029989719390869 | KNN Loss: 5.015641689300537 | BCE Loss: 1.0143481492996216\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.01730489730835 | KNN Loss: 4.995344161987305 | BCE Loss: 1.0219608545303345\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.051095962524414 | KNN Loss: 5.0139031410217285 | BCE Loss: 1.0371930599212646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.008994102478027 | KNN Loss: 4.99589204788208 | BCE Loss: 1.0131018161773682\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.047043323516846 | KNN Loss: 4.9933037757873535 | BCE Loss: 1.0537395477294922\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.0696306228637695 | KNN Loss: 5.016899108886719 | BCE Loss: 1.0527315139770508\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.021917343139648 | KNN Loss: 5.024557590484619 | BCE Loss: 0.9973598122596741\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.0365800857543945 | KNN Loss: 5.05091667175293 | BCE Loss: 0.9856632947921753\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.030801296234131 | KNN Loss: 5.0106706619262695 | BCE Loss: 1.0201306343078613\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.0315937995910645 | KNN Loss: 5.011815547943115 | BCE Loss: 1.0197783708572388\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.042860984802246 | KNN Loss: 4.992047309875488 | BCE Loss: 1.050813913345337\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.051830768585205 | KNN Loss: 5.007595062255859 | BCE Loss: 1.0442357063293457\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.0762553215026855 | KNN Loss: 5.048514366149902 | BCE Loss: 1.0277409553527832\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.023533344268799 | KNN Loss: 4.990478515625 | BCE Loss: 1.0330548286437988\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.052924156188965 | KNN Loss: 5.018498420715332 | BCE Loss: 1.0344256162643433\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.025676727294922 | KNN Loss: 5.024693012237549 | BCE Loss: 1.0009838342666626\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.028278350830078 | KNN Loss: 4.985299587249756 | BCE Loss: 1.0429787635803223\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.04047966003418 | KNN Loss: 4.996554851531982 | BCE Loss: 1.0439249277114868\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 5.979564189910889 | KNN Loss: 4.982381343841553 | BCE Loss: 0.9971827268600464\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.028355598449707 | KNN Loss: 4.999850749969482 | BCE Loss: 1.028504729270935\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.032407760620117 | KNN Loss: 4.988848686218262 | BCE Loss: 1.0435593128204346\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.025757789611816 | KNN Loss: 4.996176242828369 | BCE Loss: 1.0295813083648682\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 5.995362758636475 | KNN Loss: 4.974681854248047 | BCE Loss: 1.0206809043884277\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.013571739196777 | KNN Loss: 4.995327949523926 | BCE Loss: 1.0182439088821411\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 5.9900383949279785 | KNN Loss: 4.983221054077148 | BCE Loss: 1.0068174600601196\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.054154872894287 | KNN Loss: 5.0136260986328125 | BCE Loss: 1.040528655052185\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.057161331176758 | KNN Loss: 5.007289409637451 | BCE Loss: 1.0498716831207275\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.078104496002197 | KNN Loss: 5.017472743988037 | BCE Loss: 1.0606317520141602\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.012729644775391 | KNN Loss: 4.975367069244385 | BCE Loss: 1.0373625755310059\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.014636516571045 | KNN Loss: 5.012123107910156 | BCE Loss: 1.0025134086608887\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.006045341491699 | KNN Loss: 4.9912004470825195 | BCE Loss: 1.0148446559906006\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.017969131469727 | KNN Loss: 5.00618314743042 | BCE Loss: 1.0117859840393066\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.070858955383301 | KNN Loss: 5.007528781890869 | BCE Loss: 1.0633304119110107\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.053625583648682 | KNN Loss: 5.024713516235352 | BCE Loss: 1.02891206741333\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.080082416534424 | KNN Loss: 5.015484809875488 | BCE Loss: 1.064597487449646\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.027339458465576 | KNN Loss: 4.989674091339111 | BCE Loss: 1.0376654863357544\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.098898410797119 | KNN Loss: 5.05794620513916 | BCE Loss: 1.040952205657959\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.0534186363220215 | KNN Loss: 5.003896713256836 | BCE Loss: 1.0495219230651855\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 5.9964141845703125 | KNN Loss: 4.985461711883545 | BCE Loss: 1.0109527111053467\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 5.993344783782959 | KNN Loss: 4.986217021942139 | BCE Loss: 1.0071278810501099\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.056332588195801 | KNN Loss: 5.023970127105713 | BCE Loss: 1.0323622226715088\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.039935111999512 | KNN Loss: 5.032054901123047 | BCE Loss: 1.007880449295044\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.068882942199707 | KNN Loss: 5.020832538604736 | BCE Loss: 1.0480502843856812\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.025629043579102 | KNN Loss: 5.021755695343018 | BCE Loss: 1.0038734674453735\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.087273597717285 | KNN Loss: 5.0389838218688965 | BCE Loss: 1.0482896566390991\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.029867172241211 | KNN Loss: 5.014072418212891 | BCE Loss: 1.0157946348190308\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.056173324584961 | KNN Loss: 5.006543159484863 | BCE Loss: 1.0496304035186768\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.0353288650512695 | KNN Loss: 4.997824668884277 | BCE Loss: 1.0375044345855713\n",
      "Epoch   201: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.0437235832214355 | KNN Loss: 5.023683547973633 | BCE Loss: 1.0200400352478027\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.043707370758057 | KNN Loss: 5.003906726837158 | BCE Loss: 1.0398006439208984\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 5.9975152015686035 | KNN Loss: 4.992213726043701 | BCE Loss: 1.005301594734192\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.067651271820068 | KNN Loss: 5.027488708496094 | BCE Loss: 1.040162444114685\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.075100898742676 | KNN Loss: 5.035948276519775 | BCE Loss: 1.0391525030136108\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.028246879577637 | KNN Loss: 4.999567031860352 | BCE Loss: 1.0286800861358643\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 5.987726211547852 | KNN Loss: 4.999495029449463 | BCE Loss: 0.988231360912323\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.0119781494140625 | KNN Loss: 4.998488426208496 | BCE Loss: 1.0134894847869873\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.0367655754089355 | KNN Loss: 4.9958391189575195 | BCE Loss: 1.040926456451416\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.029979705810547 | KNN Loss: 5.0124616622924805 | BCE Loss: 1.017518162727356\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.025521278381348 | KNN Loss: 5.019374847412109 | BCE Loss: 1.0061461925506592\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.039584159851074 | KNN Loss: 4.994938373565674 | BCE Loss: 1.0446460247039795\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.09437370300293 | KNN Loss: 5.061252117156982 | BCE Loss: 1.0331217050552368\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.016003131866455 | KNN Loss: 4.995692253112793 | BCE Loss: 1.0203109979629517\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 5.99999475479126 | KNN Loss: 4.982169151306152 | BCE Loss: 1.0178256034851074\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.011978626251221 | KNN Loss: 4.996275424957275 | BCE Loss: 1.0157032012939453\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 5.991405010223389 | KNN Loss: 4.974501609802246 | BCE Loss: 1.0169034004211426\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 5.9989423751831055 | KNN Loss: 4.9859514236450195 | BCE Loss: 1.0129907131195068\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.038210868835449 | KNN Loss: 4.998687744140625 | BCE Loss: 1.0395231246948242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.036676406860352 | KNN Loss: 5.009092807769775 | BCE Loss: 1.027583360671997\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.128673553466797 | KNN Loss: 5.068080902099609 | BCE Loss: 1.060592770576477\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.012908935546875 | KNN Loss: 4.996050834655762 | BCE Loss: 1.0168578624725342\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 5.999847888946533 | KNN Loss: 4.9896135330200195 | BCE Loss: 1.0102342367172241\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.040980339050293 | KNN Loss: 5.001227378845215 | BCE Loss: 1.0397531986236572\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.069392204284668 | KNN Loss: 5.024964809417725 | BCE Loss: 1.0444276332855225\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.0454912185668945 | KNN Loss: 5.018773555755615 | BCE Loss: 1.0267175436019897\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.00527286529541 | KNN Loss: 5.00943660736084 | BCE Loss: 0.9958361983299255\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.040396690368652 | KNN Loss: 5.001791000366211 | BCE Loss: 1.0386056900024414\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.096817493438721 | KNN Loss: 5.025896072387695 | BCE Loss: 1.0709214210510254\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.034006595611572 | KNN Loss: 5.011753559112549 | BCE Loss: 1.0222529172897339\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.048467636108398 | KNN Loss: 5.005045413970947 | BCE Loss: 1.0434222221374512\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 5.993147373199463 | KNN Loss: 4.984992980957031 | BCE Loss: 1.0081543922424316\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 5.987250328063965 | KNN Loss: 4.981544017791748 | BCE Loss: 1.0057064294815063\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.034064292907715 | KNN Loss: 4.997350215911865 | BCE Loss: 1.0367143154144287\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.014126777648926 | KNN Loss: 4.999485015869141 | BCE Loss: 1.0146420001983643\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.059756278991699 | KNN Loss: 5.019280433654785 | BCE Loss: 1.0404759645462036\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.051935195922852 | KNN Loss: 4.9949798583984375 | BCE Loss: 1.0569554567337036\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.006772994995117 | KNN Loss: 4.992461204528809 | BCE Loss: 1.0143120288848877\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.029501914978027 | KNN Loss: 4.993762969970703 | BCE Loss: 1.0357388257980347\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.007724761962891 | KNN Loss: 4.9979963302612305 | BCE Loss: 1.0097286701202393\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.052365779876709 | KNN Loss: 5.01037073135376 | BCE Loss: 1.0419950485229492\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.0298919677734375 | KNN Loss: 5.031716823577881 | BCE Loss: 0.9981751441955566\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.049596786499023 | KNN Loss: 5.0489630699157715 | BCE Loss: 1.000633716583252\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.025689601898193 | KNN Loss: 5.000916004180908 | BCE Loss: 1.0247735977172852\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.000973224639893 | KNN Loss: 4.987420082092285 | BCE Loss: 1.0135530233383179\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.066736698150635 | KNN Loss: 5.04662561416626 | BCE Loss: 1.020111083984375\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.038733959197998 | KNN Loss: 5.044829368591309 | BCE Loss: 0.9939044713973999\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.071728229522705 | KNN Loss: 5.004634380340576 | BCE Loss: 1.0670937299728394\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.0306172370910645 | KNN Loss: 5.000296592712402 | BCE Loss: 1.030320644378662\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.040656089782715 | KNN Loss: 5.020172119140625 | BCE Loss: 1.0204837322235107\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.06266975402832 | KNN Loss: 5.013564586639404 | BCE Loss: 1.0491054058074951\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.017304420471191 | KNN Loss: 5.013928413391113 | BCE Loss: 1.0033758878707886\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.128926753997803 | KNN Loss: 5.075257301330566 | BCE Loss: 1.0536694526672363\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.032916069030762 | KNN Loss: 5.0016770362854 | BCE Loss: 1.0312391519546509\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.028454780578613 | KNN Loss: 4.996993541717529 | BCE Loss: 1.031461477279663\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 5.996781826019287 | KNN Loss: 4.978989601135254 | BCE Loss: 1.0177922248840332\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.054189682006836 | KNN Loss: 5.025896072387695 | BCE Loss: 1.0282937288284302\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.0317888259887695 | KNN Loss: 5.022141933441162 | BCE Loss: 1.0096468925476074\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.084675312042236 | KNN Loss: 5.062826633453369 | BCE Loss: 1.0218487977981567\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.104609966278076 | KNN Loss: 5.0792951583862305 | BCE Loss: 1.0253148078918457\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.050693035125732 | KNN Loss: 5.005064487457275 | BCE Loss: 1.045628547668457\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.007898330688477 | KNN Loss: 4.982795238494873 | BCE Loss: 1.025103211402893\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.0091233253479 | KNN Loss: 4.991750240325928 | BCE Loss: 1.017372965812683\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.0452728271484375 | KNN Loss: 4.981568813323975 | BCE Loss: 1.0637038946151733\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.0299482345581055 | KNN Loss: 5.017246723175049 | BCE Loss: 1.0127016305923462\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.003187656402588 | KNN Loss: 4.999200344085693 | BCE Loss: 1.0039873123168945\n",
      "Epoch   212: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.015944480895996 | KNN Loss: 4.991029739379883 | BCE Loss: 1.0249146223068237\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.024038314819336 | KNN Loss: 4.9947509765625 | BCE Loss: 1.029287576675415\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.017693042755127 | KNN Loss: 4.99931526184082 | BCE Loss: 1.0183779001235962\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.064321517944336 | KNN Loss: 5.041125774383545 | BCE Loss: 1.023195743560791\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.029011249542236 | KNN Loss: 4.987414836883545 | BCE Loss: 1.0415964126586914\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.008729934692383 | KNN Loss: 5.002111434936523 | BCE Loss: 1.0066182613372803\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.035190582275391 | KNN Loss: 5.002712726593018 | BCE Loss: 1.0324779748916626\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.033673286437988 | KNN Loss: 5.012521266937256 | BCE Loss: 1.0211522579193115\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.034650802612305 | KNN Loss: 5.015890121459961 | BCE Loss: 1.0187606811523438\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.126920223236084 | KNN Loss: 5.069082736968994 | BCE Loss: 1.0578374862670898\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.021090507507324 | KNN Loss: 4.977308750152588 | BCE Loss: 1.0437819957733154\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.027453422546387 | KNN Loss: 5.002890586853027 | BCE Loss: 1.024562954902649\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 6.028438091278076 | KNN Loss: 4.99443244934082 | BCE Loss: 1.0340056419372559\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.030520915985107 | KNN Loss: 5.000540733337402 | BCE Loss: 1.029980182647705\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.043383598327637 | KNN Loss: 5.003490447998047 | BCE Loss: 1.0398930311203003\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.022161960601807 | KNN Loss: 5.013670921325684 | BCE Loss: 1.0084909200668335\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.013424873352051 | KNN Loss: 4.990861415863037 | BCE Loss: 1.0225634574890137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.027600288391113 | KNN Loss: 5.015091896057129 | BCE Loss: 1.0125083923339844\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.012761116027832 | KNN Loss: 4.984920978546143 | BCE Loss: 1.0278403759002686\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.075695991516113 | KNN Loss: 5.048672676086426 | BCE Loss: 1.027023434638977\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.060834884643555 | KNN Loss: 5.039547920227051 | BCE Loss: 1.021287202835083\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.066418170928955 | KNN Loss: 5.037097454071045 | BCE Loss: 1.0293205976486206\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.041723251342773 | KNN Loss: 5.031862735748291 | BCE Loss: 1.0098605155944824\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.034419536590576 | KNN Loss: 5.002009391784668 | BCE Loss: 1.0324100255966187\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.119030952453613 | KNN Loss: 5.082966327667236 | BCE Loss: 1.036064624786377\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.046290397644043 | KNN Loss: 5.0055766105651855 | BCE Loss: 1.0407137870788574\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.024723529815674 | KNN Loss: 5.014831066131592 | BCE Loss: 1.009892463684082\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.034281253814697 | KNN Loss: 5.006375312805176 | BCE Loss: 1.0279059410095215\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.074156761169434 | KNN Loss: 5.0290656089782715 | BCE Loss: 1.045090913772583\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.0347394943237305 | KNN Loss: 4.996173858642578 | BCE Loss: 1.0385656356811523\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 5.991178512573242 | KNN Loss: 4.993112087249756 | BCE Loss: 0.9980666637420654\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.032935619354248 | KNN Loss: 4.995067596435547 | BCE Loss: 1.0378681421279907\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.027441024780273 | KNN Loss: 4.990659713745117 | BCE Loss: 1.0367815494537354\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.0302324295043945 | KNN Loss: 4.998339653015137 | BCE Loss: 1.0318928956985474\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.053844451904297 | KNN Loss: 5.030637264251709 | BCE Loss: 1.023207187652588\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.021280288696289 | KNN Loss: 5.000871658325195 | BCE Loss: 1.0204088687896729\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.061960697174072 | KNN Loss: 5.003365993499756 | BCE Loss: 1.0585947036743164\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.063793182373047 | KNN Loss: 4.996185779571533 | BCE Loss: 1.0676075220108032\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.0733866691589355 | KNN Loss: 5.049945831298828 | BCE Loss: 1.023440957069397\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.036078453063965 | KNN Loss: 4.985945224761963 | BCE Loss: 1.0501333475112915\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.037358283996582 | KNN Loss: 5.006530284881592 | BCE Loss: 1.0308277606964111\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.019097328186035 | KNN Loss: 4.979129791259766 | BCE Loss: 1.03996741771698\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.062443733215332 | KNN Loss: 5.023565292358398 | BCE Loss: 1.0388784408569336\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.0368242263793945 | KNN Loss: 5.006137847900391 | BCE Loss: 1.0306861400604248\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.121408462524414 | KNN Loss: 5.059686183929443 | BCE Loss: 1.0617225170135498\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.0328369140625 | KNN Loss: 5.013453960418701 | BCE Loss: 1.019383192062378\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.00277042388916 | KNN Loss: 4.993539810180664 | BCE Loss: 1.009230375289917\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.00403356552124 | KNN Loss: 4.987628936767578 | BCE Loss: 1.0164047479629517\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.006902694702148 | KNN Loss: 4.997250556945801 | BCE Loss: 1.0096518993377686\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 6.023777484893799 | KNN Loss: 5.001269340515137 | BCE Loss: 1.022508144378662\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.0249481201171875 | KNN Loss: 5.030416011810303 | BCE Loss: 0.9945319890975952\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.011262893676758 | KNN Loss: 4.9914937019348145 | BCE Loss: 1.0197694301605225\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.028181076049805 | KNN Loss: 4.995080471038818 | BCE Loss: 1.0331008434295654\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.069789886474609 | KNN Loss: 5.007421970367432 | BCE Loss: 1.0623679161071777\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.050153732299805 | KNN Loss: 5.029258728027344 | BCE Loss: 1.02089524269104\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.042171001434326 | KNN Loss: 5.028136253356934 | BCE Loss: 1.0140347480773926\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.026079177856445 | KNN Loss: 5.026876449584961 | BCE Loss: 0.9992027282714844\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 5.98917293548584 | KNN Loss: 4.9904561042785645 | BCE Loss: 0.9987165927886963\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.0369791984558105 | KNN Loss: 5.026012897491455 | BCE Loss: 1.010966420173645\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.0295820236206055 | KNN Loss: 4.985795021057129 | BCE Loss: 1.0437872409820557\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.005336761474609 | KNN Loss: 5.003374099731445 | BCE Loss: 1.0019627809524536\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.079897880554199 | KNN Loss: 5.0451226234436035 | BCE Loss: 1.0347752571105957\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.037726402282715 | KNN Loss: 5.032050132751465 | BCE Loss: 1.005676507949829\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.044209003448486 | KNN Loss: 5.02786111831665 | BCE Loss: 1.0163477659225464\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.033477306365967 | KNN Loss: 5.008629322052002 | BCE Loss: 1.0248478651046753\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.0381903648376465 | KNN Loss: 5.001516819000244 | BCE Loss: 1.0366735458374023\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.062918663024902 | KNN Loss: 5.0488104820251465 | BCE Loss: 1.0141079425811768\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.029639720916748 | KNN Loss: 4.999823570251465 | BCE Loss: 1.0298160314559937\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.039334297180176 | KNN Loss: 5.017373561859131 | BCE Loss: 1.021960973739624\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 5.995785236358643 | KNN Loss: 4.989538669586182 | BCE Loss: 1.006246566772461\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.02921199798584 | KNN Loss: 4.98756742477417 | BCE Loss: 1.04164457321167\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 5.992185592651367 | KNN Loss: 4.986913204193115 | BCE Loss: 1.005272626876831\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.01954984664917 | KNN Loss: 4.991359710693359 | BCE Loss: 1.0281901359558105\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.025954246520996 | KNN Loss: 4.9947710037231445 | BCE Loss: 1.0311834812164307\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.037036418914795 | KNN Loss: 4.999114036560059 | BCE Loss: 1.0379225015640259\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.044017314910889 | KNN Loss: 5.019084453582764 | BCE Loss: 1.024932861328125\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.000986099243164 | KNN Loss: 4.986917972564697 | BCE Loss: 1.014068365097046\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.029623985290527 | KNN Loss: 5.015527248382568 | BCE Loss: 1.0140964984893799\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.02971076965332 | KNN Loss: 4.980445861816406 | BCE Loss: 1.0492647886276245\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.002647399902344 | KNN Loss: 4.990671157836914 | BCE Loss: 1.0119762420654297\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.04044771194458 | KNN Loss: 5.023294925689697 | BCE Loss: 1.0171527862548828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.042793273925781 | KNN Loss: 5.020571231842041 | BCE Loss: 1.0222220420837402\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.069212913513184 | KNN Loss: 5.0487470626831055 | BCE Loss: 1.020465612411499\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.078712463378906 | KNN Loss: 5.047940254211426 | BCE Loss: 1.0307719707489014\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.048078536987305 | KNN Loss: 4.995428562164307 | BCE Loss: 1.0526502132415771\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 5.992036819458008 | KNN Loss: 4.984274864196777 | BCE Loss: 1.0077621936798096\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 5.9871320724487305 | KNN Loss: 4.971126079559326 | BCE Loss: 1.0160062313079834\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.040619373321533 | KNN Loss: 5.026799201965332 | BCE Loss: 1.0138201713562012\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.0364556312561035 | KNN Loss: 5.006229877471924 | BCE Loss: 1.0302257537841797\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 5.98886775970459 | KNN Loss: 5.002676486968994 | BCE Loss: 0.9861913919448853\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.139083385467529 | KNN Loss: 5.096497058868408 | BCE Loss: 1.042586326599121\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.0207319259643555 | KNN Loss: 5.004732131958008 | BCE Loss: 1.015999674797058\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.035870552062988 | KNN Loss: 4.995362281799316 | BCE Loss: 1.040508508682251\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.006963729858398 | KNN Loss: 5.002826690673828 | BCE Loss: 1.0041369199752808\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.041804313659668 | KNN Loss: 5.013648509979248 | BCE Loss: 1.0281555652618408\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.009376525878906 | KNN Loss: 4.981614589691162 | BCE Loss: 1.027761697769165\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.037878036499023 | KNN Loss: 4.9904866218566895 | BCE Loss: 1.047391414642334\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.020845413208008 | KNN Loss: 4.994801044464111 | BCE Loss: 1.0260441303253174\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 5.995670318603516 | KNN Loss: 4.9990949630737305 | BCE Loss: 0.9965752363204956\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.01010274887085 | KNN Loss: 4.996222019195557 | BCE Loss: 1.013880729675293\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.035650253295898 | KNN Loss: 4.9763569831848145 | BCE Loss: 1.0592930316925049\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.0491790771484375 | KNN Loss: 5.013777732849121 | BCE Loss: 1.0354011058807373\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.064931392669678 | KNN Loss: 4.994577407836914 | BCE Loss: 1.0703539848327637\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.002822399139404 | KNN Loss: 4.99619722366333 | BCE Loss: 1.0066252946853638\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.046211242675781 | KNN Loss: 5.040807723999023 | BCE Loss: 1.0054032802581787\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 5.9899725914001465 | KNN Loss: 4.989962100982666 | BCE Loss: 1.0000104904174805\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.0122761726379395 | KNN Loss: 5.012993812561035 | BCE Loss: 0.9992824792861938\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.00629186630249 | KNN Loss: 4.989173889160156 | BCE Loss: 1.017117977142334\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.045510292053223 | KNN Loss: 5.001276969909668 | BCE Loss: 1.0442330837249756\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.014230251312256 | KNN Loss: 5.003047943115234 | BCE Loss: 1.0111823081970215\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.034829616546631 | KNN Loss: 5.025450229644775 | BCE Loss: 1.0093793869018555\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.0104451179504395 | KNN Loss: 4.990243434906006 | BCE Loss: 1.0202016830444336\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.0332417488098145 | KNN Loss: 4.9900665283203125 | BCE Loss: 1.043175220489502\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.018856048583984 | KNN Loss: 4.993978977203369 | BCE Loss: 1.0248770713806152\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.045570373535156 | KNN Loss: 4.99931001663208 | BCE Loss: 1.0462603569030762\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.002429962158203 | KNN Loss: 5.005073547363281 | BCE Loss: 0.9973564147949219\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.021247386932373 | KNN Loss: 5.015079498291016 | BCE Loss: 1.0061678886413574\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.05518913269043 | KNN Loss: 5.060949325561523 | BCE Loss: 0.9942400455474854\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.029447555541992 | KNN Loss: 5.0052809715271 | BCE Loss: 1.0241668224334717\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 5.997180938720703 | KNN Loss: 4.992016792297363 | BCE Loss: 1.0051639080047607\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.027313232421875 | KNN Loss: 4.991135597229004 | BCE Loss: 1.0361778736114502\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.027315139770508 | KNN Loss: 4.992173194885254 | BCE Loss: 1.035141944885254\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.049459457397461 | KNN Loss: 5.010556697845459 | BCE Loss: 1.0389028787612915\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.004117965698242 | KNN Loss: 4.974251747131348 | BCE Loss: 1.0298664569854736\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.035943508148193 | KNN Loss: 5.003493785858154 | BCE Loss: 1.0324496030807495\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.043118000030518 | KNN Loss: 5.008298873901367 | BCE Loss: 1.03481924533844\n",
      "Epoch   233: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.013339042663574 | KNN Loss: 4.9938201904296875 | BCE Loss: 1.0195189714431763\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.022269248962402 | KNN Loss: 4.977030277252197 | BCE Loss: 1.0452390909194946\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.039703369140625 | KNN Loss: 5.025386810302734 | BCE Loss: 1.0143167972564697\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.0309343338012695 | KNN Loss: 5.002743721008301 | BCE Loss: 1.0281903743743896\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.00556755065918 | KNN Loss: 5.0171799659729 | BCE Loss: 0.9883878231048584\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.0643720626831055 | KNN Loss: 5.016661643981934 | BCE Loss: 1.047710657119751\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.017735481262207 | KNN Loss: 4.994243621826172 | BCE Loss: 1.0234918594360352\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.025696754455566 | KNN Loss: 4.999608993530273 | BCE Loss: 1.026087760925293\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.05595588684082 | KNN Loss: 5.018588542938232 | BCE Loss: 1.0373672246932983\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.0137529373168945 | KNN Loss: 5.010648727416992 | BCE Loss: 1.0031044483184814\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.030232906341553 | KNN Loss: 5.0077433586120605 | BCE Loss: 1.0224895477294922\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.025835990905762 | KNN Loss: 4.991876125335693 | BCE Loss: 1.0339596271514893\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.0363054275512695 | KNN Loss: 5.003817558288574 | BCE Loss: 1.0324881076812744\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.084113121032715 | KNN Loss: 5.0547075271606445 | BCE Loss: 1.0294058322906494\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.088244438171387 | KNN Loss: 5.071173667907715 | BCE Loss: 1.0170707702636719\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.044961929321289 | KNN Loss: 5.021089553833008 | BCE Loss: 1.0238722562789917\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.049635887145996 | KNN Loss: 5.022883892059326 | BCE Loss: 1.0267518758773804\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.056126594543457 | KNN Loss: 5.00610876083374 | BCE Loss: 1.0500175952911377\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.068549156188965 | KNN Loss: 5.040858268737793 | BCE Loss: 1.0276908874511719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.05882453918457 | KNN Loss: 5.016063690185547 | BCE Loss: 1.0427608489990234\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.0089006423950195 | KNN Loss: 4.986953258514404 | BCE Loss: 1.0219471454620361\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.0318474769592285 | KNN Loss: 5.017027854919434 | BCE Loss: 1.014819622039795\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.040718078613281 | KNN Loss: 5.023330211639404 | BCE Loss: 1.017388105392456\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 5.992644786834717 | KNN Loss: 4.9713521003723145 | BCE Loss: 1.021292805671692\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.056281089782715 | KNN Loss: 5.027825355529785 | BCE Loss: 1.0284559726715088\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 5.999690532684326 | KNN Loss: 4.989825248718262 | BCE Loss: 1.009865164756775\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.0180463790893555 | KNN Loss: 4.979657173156738 | BCE Loss: 1.0383894443511963\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.040311813354492 | KNN Loss: 4.998032569885254 | BCE Loss: 1.0422791242599487\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.042767524719238 | KNN Loss: 5.011931896209717 | BCE Loss: 1.0308358669281006\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.043850898742676 | KNN Loss: 5.019376277923584 | BCE Loss: 1.0244746208190918\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.007988929748535 | KNN Loss: 5.005876064300537 | BCE Loss: 1.0021129846572876\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.032768249511719 | KNN Loss: 5.001199722290039 | BCE Loss: 1.0315685272216797\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.006131172180176 | KNN Loss: 4.989655017852783 | BCE Loss: 1.0164761543273926\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.008237838745117 | KNN Loss: 4.9836201667785645 | BCE Loss: 1.0246177911758423\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.0196332931518555 | KNN Loss: 4.991757392883301 | BCE Loss: 1.0278756618499756\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.036776542663574 | KNN Loss: 4.9898223876953125 | BCE Loss: 1.0469540357589722\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.056819915771484 | KNN Loss: 5.0458245277404785 | BCE Loss: 1.0109955072402954\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.029189586639404 | KNN Loss: 4.994561672210693 | BCE Loss: 1.034627914428711\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.048202037811279 | KNN Loss: 5.010039806365967 | BCE Loss: 1.038162112236023\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.072792053222656 | KNN Loss: 5.045356750488281 | BCE Loss: 1.027435064315796\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.023464202880859 | KNN Loss: 5.0286946296691895 | BCE Loss: 0.994769811630249\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.057362079620361 | KNN Loss: 5.008206844329834 | BCE Loss: 1.0491552352905273\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.029711723327637 | KNN Loss: 5.02026891708374 | BCE Loss: 1.0094425678253174\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.0410614013671875 | KNN Loss: 5.021878719329834 | BCE Loss: 1.0191829204559326\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.098600387573242 | KNN Loss: 5.077483177185059 | BCE Loss: 1.0211174488067627\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.046258926391602 | KNN Loss: 5.000488758087158 | BCE Loss: 1.0457704067230225\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.005624771118164 | KNN Loss: 4.994958400726318 | BCE Loss: 1.0106661319732666\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.054241180419922 | KNN Loss: 4.999693870544434 | BCE Loss: 1.0545474290847778\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.032097816467285 | KNN Loss: 5.011861324310303 | BCE Loss: 1.0202364921569824\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 5.998065948486328 | KNN Loss: 4.978896141052246 | BCE Loss: 1.0191699266433716\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.021478176116943 | KNN Loss: 4.9864726066589355 | BCE Loss: 1.0350055694580078\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.003507614135742 | KNN Loss: 4.990990161895752 | BCE Loss: 1.0125174522399902\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.006731033325195 | KNN Loss: 5.007155895233154 | BCE Loss: 0.9995753765106201\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.047031402587891 | KNN Loss: 5.0094404220581055 | BCE Loss: 1.0375909805297852\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.065063953399658 | KNN Loss: 5.027426242828369 | BCE Loss: 1.0376375913619995\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.046537399291992 | KNN Loss: 4.988444805145264 | BCE Loss: 1.058092713356018\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.052100658416748 | KNN Loss: 5.000319480895996 | BCE Loss: 1.051781177520752\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.086329460144043 | KNN Loss: 5.059640407562256 | BCE Loss: 1.0266891717910767\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.043546676635742 | KNN Loss: 5.013428211212158 | BCE Loss: 1.0301182270050049\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.03274393081665 | KNN Loss: 5.020407199859619 | BCE Loss: 1.0123367309570312\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.097715854644775 | KNN Loss: 5.074821949005127 | BCE Loss: 1.0228939056396484\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.049892425537109 | KNN Loss: 5.040876388549805 | BCE Loss: 1.0090159177780151\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.061042785644531 | KNN Loss: 5.008584022521973 | BCE Loss: 1.0524590015411377\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.099453926086426 | KNN Loss: 5.028122901916504 | BCE Loss: 1.0713310241699219\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.063125133514404 | KNN Loss: 5.019384860992432 | BCE Loss: 1.0437402725219727\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.005448341369629 | KNN Loss: 4.988007545471191 | BCE Loss: 1.0174410343170166\n",
      "Epoch   244: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.060996055603027 | KNN Loss: 5.032353401184082 | BCE Loss: 1.0286426544189453\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.046021461486816 | KNN Loss: 4.998583793640137 | BCE Loss: 1.0474377870559692\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.054440498352051 | KNN Loss: 4.9889326095581055 | BCE Loss: 1.0655081272125244\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.036101818084717 | KNN Loss: 5.015990257263184 | BCE Loss: 1.0201115608215332\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.032763481140137 | KNN Loss: 5.005842208862305 | BCE Loss: 1.0269213914871216\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.020595550537109 | KNN Loss: 5.001905918121338 | BCE Loss: 1.0186893939971924\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.027587890625 | KNN Loss: 5.008705139160156 | BCE Loss: 1.0188827514648438\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.027676105499268 | KNN Loss: 5.0179667472839355 | BCE Loss: 1.009709358215332\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.0107879638671875 | KNN Loss: 4.981178283691406 | BCE Loss: 1.0296097993850708\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.053203582763672 | KNN Loss: 5.026605606079102 | BCE Loss: 1.0265982151031494\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.032920837402344 | KNN Loss: 4.989596843719482 | BCE Loss: 1.0433238744735718\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.044010162353516 | KNN Loss: 4.998020172119141 | BCE Loss: 1.0459901094436646\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.045291900634766 | KNN Loss: 4.998953819274902 | BCE Loss: 1.0463379621505737\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.037148475646973 | KNN Loss: 4.9903788566589355 | BCE Loss: 1.0467698574066162\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.085385322570801 | KNN Loss: 5.009040832519531 | BCE Loss: 1.07634437084198\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.045116424560547 | KNN Loss: 5.029782295227051 | BCE Loss: 1.015333890914917\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.009493827819824 | KNN Loss: 5.003634929656982 | BCE Loss: 1.0058590173721313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.013554096221924 | KNN Loss: 5.013326168060303 | BCE Loss: 1.000227928161621\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.02556037902832 | KNN Loss: 4.976141452789307 | BCE Loss: 1.0494186878204346\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.031200408935547 | KNN Loss: 5.0092549324035645 | BCE Loss: 1.0219454765319824\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 5.988860130310059 | KNN Loss: 4.977952003479004 | BCE Loss: 1.0109078884124756\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.054637908935547 | KNN Loss: 5.026487350463867 | BCE Loss: 1.0281503200531006\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.046933174133301 | KNN Loss: 5.024504661560059 | BCE Loss: 1.0224287509918213\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.0405097007751465 | KNN Loss: 4.99690055847168 | BCE Loss: 1.0436090230941772\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.052636623382568 | KNN Loss: 5.0313286781311035 | BCE Loss: 1.0213078260421753\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.047703742980957 | KNN Loss: 5.011551856994629 | BCE Loss: 1.0361518859863281\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.07918119430542 | KNN Loss: 5.025505065917969 | BCE Loss: 1.0536761283874512\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.037364959716797 | KNN Loss: 5.010509490966797 | BCE Loss: 1.026855707168579\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.084794044494629 | KNN Loss: 5.0493645668029785 | BCE Loss: 1.0354294776916504\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.073563575744629 | KNN Loss: 5.043936729431152 | BCE Loss: 1.0296269655227661\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.065720081329346 | KNN Loss: 5.02291202545166 | BCE Loss: 1.042808175086975\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.003294944763184 | KNN Loss: 4.993874549865723 | BCE Loss: 1.00942063331604\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.039538860321045 | KNN Loss: 5.00144624710083 | BCE Loss: 1.0380926132202148\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.0571675300598145 | KNN Loss: 5.010564804077148 | BCE Loss: 1.0466028451919556\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.037746906280518 | KNN Loss: 5.003905296325684 | BCE Loss: 1.033841609954834\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.026976585388184 | KNN Loss: 5.006527423858643 | BCE Loss: 1.0204490423202515\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.126311302185059 | KNN Loss: 5.083981513977051 | BCE Loss: 1.0423297882080078\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.002262592315674 | KNN Loss: 5.007933139801025 | BCE Loss: 0.9943292737007141\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.068325996398926 | KNN Loss: 5.032923221588135 | BCE Loss: 1.035402774810791\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.092467308044434 | KNN Loss: 5.065021991729736 | BCE Loss: 1.0274454355239868\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.029106140136719 | KNN Loss: 5.015360355377197 | BCE Loss: 1.013745665550232\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.038936614990234 | KNN Loss: 4.983418941497803 | BCE Loss: 1.0555174350738525\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.023845672607422 | KNN Loss: 5.015145778656006 | BCE Loss: 1.0087001323699951\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.023406982421875 | KNN Loss: 4.992460250854492 | BCE Loss: 1.030946969985962\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.025000095367432 | KNN Loss: 5.007055759429932 | BCE Loss: 1.0179443359375\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.039522171020508 | KNN Loss: 5.0130085945129395 | BCE Loss: 1.0265135765075684\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.068525791168213 | KNN Loss: 5.048220634460449 | BCE Loss: 1.0203050374984741\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.041831016540527 | KNN Loss: 4.979633808135986 | BCE Loss: 1.062196969985962\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.039417266845703 | KNN Loss: 4.998164176940918 | BCE Loss: 1.0412530899047852\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.067954063415527 | KNN Loss: 5.026745796203613 | BCE Loss: 1.041208267211914\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.064268589019775 | KNN Loss: 5.036754608154297 | BCE Loss: 1.027513861656189\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.014084339141846 | KNN Loss: 5.014461040496826 | BCE Loss: 0.9996234178543091\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.0628509521484375 | KNN Loss: 5.002892971038818 | BCE Loss: 1.05995774269104\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 5.994225978851318 | KNN Loss: 5.010243892669678 | BCE Loss: 0.9839821457862854\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.049711227416992 | KNN Loss: 5.016722679138184 | BCE Loss: 1.0329887866973877\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.029064178466797 | KNN Loss: 5.002926826477051 | BCE Loss: 1.0261375904083252\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.056336402893066 | KNN Loss: 5.011831283569336 | BCE Loss: 1.0445051193237305\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.0192952156066895 | KNN Loss: 5.000589370727539 | BCE Loss: 1.01870596408844\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.044743061065674 | KNN Loss: 5.0179972648620605 | BCE Loss: 1.0267457962036133\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.031676292419434 | KNN Loss: 5.00104284286499 | BCE Loss: 1.0306334495544434\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.118370532989502 | KNN Loss: 5.07562255859375 | BCE Loss: 1.0427478551864624\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.027143478393555 | KNN Loss: 5.002016544342041 | BCE Loss: 1.0251271724700928\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.052740573883057 | KNN Loss: 5.013330936431885 | BCE Loss: 1.0394096374511719\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.028717041015625 | KNN Loss: 5.013935089111328 | BCE Loss: 1.0147817134857178\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.022036075592041 | KNN Loss: 4.9929351806640625 | BCE Loss: 1.0291008949279785\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.031052589416504 | KNN Loss: 5.021646499633789 | BCE Loss: 1.0094060897827148\n",
      "Epoch   255: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.002597332000732 | KNN Loss: 4.992592811584473 | BCE Loss: 1.0100045204162598\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.034728050231934 | KNN Loss: 5.021728992462158 | BCE Loss: 1.0129992961883545\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.065943717956543 | KNN Loss: 5.033003807067871 | BCE Loss: 1.0329397916793823\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.029879570007324 | KNN Loss: 5.023428440093994 | BCE Loss: 1.006450891494751\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.026144981384277 | KNN Loss: 4.993671894073486 | BCE Loss: 1.032472848892212\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.0344438552856445 | KNN Loss: 4.997256278991699 | BCE Loss: 1.0371874570846558\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.088120937347412 | KNN Loss: 5.092045307159424 | BCE Loss: 0.9960757493972778\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.0088300704956055 | KNN Loss: 4.9785051345825195 | BCE Loss: 1.0303250551223755\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.011841297149658 | KNN Loss: 4.995645999908447 | BCE Loss: 1.016195297241211\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.045687675476074 | KNN Loss: 5.0194172859191895 | BCE Loss: 1.0262702703475952\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.041685581207275 | KNN Loss: 5.011056900024414 | BCE Loss: 1.0306286811828613\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.028288841247559 | KNN Loss: 4.985161781311035 | BCE Loss: 1.0431268215179443\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.086376667022705 | KNN Loss: 5.009049892425537 | BCE Loss: 1.077326774597168\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.018516540527344 | KNN Loss: 5.003599166870117 | BCE Loss: 1.0149176120758057\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.026989936828613 | KNN Loss: 5.048959255218506 | BCE Loss: 0.9780309200286865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.024074554443359 | KNN Loss: 5.001955986022949 | BCE Loss: 1.0221185684204102\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.058073043823242 | KNN Loss: 5.005248069763184 | BCE Loss: 1.0528247356414795\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.004766464233398 | KNN Loss: 4.99049711227417 | BCE Loss: 1.014269471168518\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.052371501922607 | KNN Loss: 4.997718334197998 | BCE Loss: 1.0546531677246094\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.034512519836426 | KNN Loss: 5.0196533203125 | BCE Loss: 1.0148591995239258\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.039517879486084 | KNN Loss: 5.013401985168457 | BCE Loss: 1.026115894317627\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.064043998718262 | KNN Loss: 5.0011491775512695 | BCE Loss: 1.0628949403762817\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.037271022796631 | KNN Loss: 5.024835586547852 | BCE Loss: 1.0124355554580688\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.065378665924072 | KNN Loss: 5.040850639343262 | BCE Loss: 1.024527907371521\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.052824974060059 | KNN Loss: 5.0544514656066895 | BCE Loss: 0.9983735084533691\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.093865394592285 | KNN Loss: 5.057333469390869 | BCE Loss: 1.0365321636199951\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.0197601318359375 | KNN Loss: 5.012660503387451 | BCE Loss: 1.0070996284484863\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.076089859008789 | KNN Loss: 5.028953552246094 | BCE Loss: 1.0471360683441162\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.063675403594971 | KNN Loss: 5.016502380371094 | BCE Loss: 1.0471731424331665\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.031008720397949 | KNN Loss: 4.997176647186279 | BCE Loss: 1.033832311630249\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.028777122497559 | KNN Loss: 5.020771026611328 | BCE Loss: 1.0080063343048096\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.095213890075684 | KNN Loss: 5.069579601287842 | BCE Loss: 1.025634527206421\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.017603874206543 | KNN Loss: 5.019261360168457 | BCE Loss: 0.9983425140380859\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.0521321296691895 | KNN Loss: 5.001187801361084 | BCE Loss: 1.050944209098816\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.038967609405518 | KNN Loss: 5.009644508361816 | BCE Loss: 1.0293231010437012\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.010046005249023 | KNN Loss: 4.982276916503906 | BCE Loss: 1.0277689695358276\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.033762454986572 | KNN Loss: 5.036391735076904 | BCE Loss: 0.9973706007003784\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.0297770500183105 | KNN Loss: 4.98887300491333 | BCE Loss: 1.0409040451049805\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.037937164306641 | KNN Loss: 5.017862796783447 | BCE Loss: 1.0200741291046143\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.058744430541992 | KNN Loss: 5.017213344573975 | BCE Loss: 1.041530966758728\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.022378921508789 | KNN Loss: 4.987112998962402 | BCE Loss: 1.0352660417556763\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.044441223144531 | KNN Loss: 5.006979942321777 | BCE Loss: 1.0374611616134644\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.041934490203857 | KNN Loss: 4.977407455444336 | BCE Loss: 1.064527153968811\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.062407970428467 | KNN Loss: 5.060214519500732 | BCE Loss: 1.0021933317184448\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.012578010559082 | KNN Loss: 4.9807634353637695 | BCE Loss: 1.0318145751953125\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.012721061706543 | KNN Loss: 4.995383262634277 | BCE Loss: 1.0173380374908447\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.0587968826293945 | KNN Loss: 5.008706569671631 | BCE Loss: 1.0500903129577637\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.019890785217285 | KNN Loss: 4.992306232452393 | BCE Loss: 1.0275843143463135\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.025452613830566 | KNN Loss: 4.992742538452148 | BCE Loss: 1.0327099561691284\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.0328497886657715 | KNN Loss: 4.983376502990723 | BCE Loss: 1.0494732856750488\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.029778480529785 | KNN Loss: 5.003875732421875 | BCE Loss: 1.0259027481079102\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.0106096267700195 | KNN Loss: 4.980615139007568 | BCE Loss: 1.0299947261810303\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.026313781738281 | KNN Loss: 5.015325546264648 | BCE Loss: 1.0109883546829224\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.06503438949585 | KNN Loss: 5.025106906890869 | BCE Loss: 1.0399274826049805\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.056465148925781 | KNN Loss: 5.007391452789307 | BCE Loss: 1.0490739345550537\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.016420364379883 | KNN Loss: 5.0015716552734375 | BCE Loss: 1.0148484706878662\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.000397682189941 | KNN Loss: 4.9952712059021 | BCE Loss: 1.005126714706421\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.001333236694336 | KNN Loss: 5.001251220703125 | BCE Loss: 1.0000817775726318\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.047858238220215 | KNN Loss: 5.000819683074951 | BCE Loss: 1.0470387935638428\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.09547233581543 | KNN Loss: 5.067862033843994 | BCE Loss: 1.0276105403900146\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.036179542541504 | KNN Loss: 4.984503269195557 | BCE Loss: 1.0516765117645264\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.068098068237305 | KNN Loss: 5.045253276824951 | BCE Loss: 1.0228447914123535\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.023550033569336 | KNN Loss: 5.007707118988037 | BCE Loss: 1.0158429145812988\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.043768405914307 | KNN Loss: 5.0098981857299805 | BCE Loss: 1.0338702201843262\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.045171737670898 | KNN Loss: 5.010122776031494 | BCE Loss: 1.0350487232208252\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.065476417541504 | KNN Loss: 5.018462181091309 | BCE Loss: 1.0470143556594849\n",
      "Epoch   266: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.023199081420898 | KNN Loss: 4.99235200881958 | BCE Loss: 1.030847191810608\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.012311935424805 | KNN Loss: 5.004190921783447 | BCE Loss: 1.0081208944320679\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.013875961303711 | KNN Loss: 4.995203018188477 | BCE Loss: 1.0186731815338135\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.055317401885986 | KNN Loss: 5.037500858306885 | BCE Loss: 1.0178165435791016\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.018207550048828 | KNN Loss: 4.981723785400391 | BCE Loss: 1.0364837646484375\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.067543029785156 | KNN Loss: 5.017673969268799 | BCE Loss: 1.0498690605163574\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.0173773765563965 | KNN Loss: 5.008290767669678 | BCE Loss: 1.0090866088867188\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.020504951477051 | KNN Loss: 4.996132850646973 | BCE Loss: 1.0243723392486572\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.057816505432129 | KNN Loss: 5.008311748504639 | BCE Loss: 1.0495046377182007\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.014489650726318 | KNN Loss: 5.012800216674805 | BCE Loss: 1.0016895532608032\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.0412983894348145 | KNN Loss: 4.992082595825195 | BCE Loss: 1.0492157936096191\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.045584678649902 | KNN Loss: 5.0181450843811035 | BCE Loss: 1.027439832687378\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 5.997422695159912 | KNN Loss: 4.98000955581665 | BCE Loss: 1.0174130201339722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.013460636138916 | KNN Loss: 4.999653339385986 | BCE Loss: 1.0138071775436401\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.032060623168945 | KNN Loss: 5.018365383148193 | BCE Loss: 1.013695478439331\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.06089973449707 | KNN Loss: 5.012787342071533 | BCE Loss: 1.0481125116348267\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.036672592163086 | KNN Loss: 5.012868881225586 | BCE Loss: 1.0238037109375\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.066684722900391 | KNN Loss: 5.035377502441406 | BCE Loss: 1.0313074588775635\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 6.0298357009887695 | KNN Loss: 5.021240711212158 | BCE Loss: 1.0085949897766113\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.070206642150879 | KNN Loss: 5.028445720672607 | BCE Loss: 1.0417611598968506\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 5.967853546142578 | KNN Loss: 4.977558135986328 | BCE Loss: 0.9902951717376709\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.030579566955566 | KNN Loss: 4.985508441925049 | BCE Loss: 1.0450712442398071\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.073821067810059 | KNN Loss: 5.031788349151611 | BCE Loss: 1.0420328378677368\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 5.996784210205078 | KNN Loss: 4.986321926116943 | BCE Loss: 1.0104620456695557\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.019619941711426 | KNN Loss: 4.989675045013428 | BCE Loss: 1.029944658279419\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.021155834197998 | KNN Loss: 4.986263751983643 | BCE Loss: 1.034891963005066\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.034782409667969 | KNN Loss: 5.013391971588135 | BCE Loss: 1.021390438079834\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.033079147338867 | KNN Loss: 5.004240036010742 | BCE Loss: 1.028839111328125\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 5.99520206451416 | KNN Loss: 4.978085041046143 | BCE Loss: 1.0171172618865967\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.021416664123535 | KNN Loss: 5.004020690917969 | BCE Loss: 1.0173958539962769\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.04656982421875 | KNN Loss: 5.034108638763428 | BCE Loss: 1.0124610662460327\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.018848419189453 | KNN Loss: 5.001890659332275 | BCE Loss: 1.0169576406478882\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.003632068634033 | KNN Loss: 4.987213611602783 | BCE Loss: 1.0164183378219604\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.050239562988281 | KNN Loss: 5.0126633644104 | BCE Loss: 1.0375760793685913\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.0078349113464355 | KNN Loss: 4.9850263595581055 | BCE Loss: 1.02280855178833\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.065781593322754 | KNN Loss: 5.035741329193115 | BCE Loss: 1.0300400257110596\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 5.992325782775879 | KNN Loss: 4.99021053314209 | BCE Loss: 1.0021153688430786\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.052674293518066 | KNN Loss: 5.039376258850098 | BCE Loss: 1.0132982730865479\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.057371616363525 | KNN Loss: 5.008332252502441 | BCE Loss: 1.049039363861084\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.121492385864258 | KNN Loss: 5.0613932609558105 | BCE Loss: 1.0600993633270264\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.032326698303223 | KNN Loss: 5.029921531677246 | BCE Loss: 1.0024054050445557\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.031885147094727 | KNN Loss: 5.017805576324463 | BCE Loss: 1.0140794515609741\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.037588119506836 | KNN Loss: 5.034087181091309 | BCE Loss: 1.0035008192062378\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.019416809082031 | KNN Loss: 4.996732234954834 | BCE Loss: 1.0226843357086182\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.026756286621094 | KNN Loss: 4.989376068115234 | BCE Loss: 1.037380337715149\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 5.989859580993652 | KNN Loss: 4.979060649871826 | BCE Loss: 1.010798692703247\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 5.998321533203125 | KNN Loss: 5.001075267791748 | BCE Loss: 0.9972461462020874\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.001576900482178 | KNN Loss: 4.994187831878662 | BCE Loss: 1.0073890686035156\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 5.98670768737793 | KNN Loss: 4.994999408721924 | BCE Loss: 0.991708517074585\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.043087005615234 | KNN Loss: 5.022521018981934 | BCE Loss: 1.0205659866333008\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.043244361877441 | KNN Loss: 5.015401840209961 | BCE Loss: 1.02784264087677\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.044869422912598 | KNN Loss: 5.016129493713379 | BCE Loss: 1.0287399291992188\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.02239990234375 | KNN Loss: 4.983990669250488 | BCE Loss: 1.0384089946746826\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.019093990325928 | KNN Loss: 4.98577356338501 | BCE Loss: 1.0333203077316284\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.031016826629639 | KNN Loss: 5.007616996765137 | BCE Loss: 1.023399829864502\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.038567066192627 | KNN Loss: 5.005664825439453 | BCE Loss: 1.0329022407531738\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.009927749633789 | KNN Loss: 4.99553108215332 | BCE Loss: 1.0143964290618896\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.078953742980957 | KNN Loss: 5.074909210205078 | BCE Loss: 1.004044532775879\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 5.991015911102295 | KNN Loss: 4.982575416564941 | BCE Loss: 1.008440375328064\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.016744136810303 | KNN Loss: 4.9839935302734375 | BCE Loss: 1.0327506065368652\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.037165641784668 | KNN Loss: 5.031135082244873 | BCE Loss: 1.0060303211212158\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 5.985326766967773 | KNN Loss: 4.97422981262207 | BCE Loss: 1.0110970735549927\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.062207221984863 | KNN Loss: 5.00499153137207 | BCE Loss: 1.0572154521942139\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.038898944854736 | KNN Loss: 4.990076065063477 | BCE Loss: 1.0488228797912598\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.025613784790039 | KNN Loss: 5.003255844116211 | BCE Loss: 1.0223581790924072\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.0511956214904785 | KNN Loss: 5.001542091369629 | BCE Loss: 1.0496535301208496\n",
      "Epoch   277: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.0619635581970215 | KNN Loss: 5.0018815994262695 | BCE Loss: 1.0600820779800415\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.0677385330200195 | KNN Loss: 4.997289180755615 | BCE Loss: 1.0704491138458252\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.052306175231934 | KNN Loss: 5.0196990966796875 | BCE Loss: 1.032606840133667\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.014505386352539 | KNN Loss: 4.983606338500977 | BCE Loss: 1.030898928642273\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.051303863525391 | KNN Loss: 4.993561744689941 | BCE Loss: 1.0577421188354492\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.013896942138672 | KNN Loss: 4.992720603942871 | BCE Loss: 1.0211764574050903\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.080820560455322 | KNN Loss: 5.022937774658203 | BCE Loss: 1.0578827857971191\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.072818756103516 | KNN Loss: 5.0108466148376465 | BCE Loss: 1.06197190284729\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.011417388916016 | KNN Loss: 5.002064228057861 | BCE Loss: 1.0093530416488647\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.079553127288818 | KNN Loss: 5.033370494842529 | BCE Loss: 1.046182632446289\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.064858913421631 | KNN Loss: 5.035041809082031 | BCE Loss: 1.02981698513031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.003467559814453 | KNN Loss: 4.980100154876709 | BCE Loss: 1.0233672857284546\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.0261688232421875 | KNN Loss: 5.010121822357178 | BCE Loss: 1.0160467624664307\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.041759490966797 | KNN Loss: 5.025346279144287 | BCE Loss: 1.0164132118225098\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.015995979309082 | KNN Loss: 5.003365993499756 | BCE Loss: 1.0126302242279053\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.063911437988281 | KNN Loss: 5.007852077484131 | BCE Loss: 1.0560591220855713\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.055912971496582 | KNN Loss: 5.036261558532715 | BCE Loss: 1.019651174545288\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.029735088348389 | KNN Loss: 5.008279323577881 | BCE Loss: 1.0214556455612183\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.022757053375244 | KNN Loss: 4.9821696281433105 | BCE Loss: 1.0405874252319336\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.015614032745361 | KNN Loss: 5.005152702331543 | BCE Loss: 1.0104613304138184\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.069240093231201 | KNN Loss: 5.040781497955322 | BCE Loss: 1.028458595275879\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.046027660369873 | KNN Loss: 5.0179443359375 | BCE Loss: 1.028083324432373\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.067002296447754 | KNN Loss: 5.023567199707031 | BCE Loss: 1.0434350967407227\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.01631498336792 | KNN Loss: 4.988645076751709 | BCE Loss: 1.027669906616211\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 6.0972208976745605 | KNN Loss: 5.022776126861572 | BCE Loss: 1.0744447708129883\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.047605514526367 | KNN Loss: 5.005889892578125 | BCE Loss: 1.0417158603668213\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.067634105682373 | KNN Loss: 5.0536417961120605 | BCE Loss: 1.0139923095703125\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.056751728057861 | KNN Loss: 5.037010192871094 | BCE Loss: 1.0197415351867676\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.016441822052002 | KNN Loss: 5.023049354553223 | BCE Loss: 0.9933925867080688\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.05629825592041 | KNN Loss: 4.99495792388916 | BCE Loss: 1.061340570449829\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.041475296020508 | KNN Loss: 4.99763822555542 | BCE Loss: 1.043837308883667\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.0052876472473145 | KNN Loss: 4.982199192047119 | BCE Loss: 1.0230884552001953\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 5.999678134918213 | KNN Loss: 4.994912147521973 | BCE Loss: 1.0047659873962402\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.015752792358398 | KNN Loss: 4.996689319610596 | BCE Loss: 1.0190637111663818\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.002237796783447 | KNN Loss: 4.994135856628418 | BCE Loss: 1.0081019401550293\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.048859596252441 | KNN Loss: 5.007945537567139 | BCE Loss: 1.0409138202667236\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.027569770812988 | KNN Loss: 4.9982991218566895 | BCE Loss: 1.0292706489562988\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.045204162597656 | KNN Loss: 4.993659973144531 | BCE Loss: 1.051544427871704\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.023502349853516 | KNN Loss: 4.993551254272461 | BCE Loss: 1.0299510955810547\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.003636360168457 | KNN Loss: 4.996389865875244 | BCE Loss: 1.007246494293213\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.007961273193359 | KNN Loss: 4.982850551605225 | BCE Loss: 1.0251106023788452\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.072517395019531 | KNN Loss: 5.039490222930908 | BCE Loss: 1.033027172088623\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.125966548919678 | KNN Loss: 5.064981460571289 | BCE Loss: 1.0609849691390991\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.0527167320251465 | KNN Loss: 5.0074687004089355 | BCE Loss: 1.045248031616211\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.019609451293945 | KNN Loss: 5.0039963722229 | BCE Loss: 1.0156128406524658\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.030910968780518 | KNN Loss: 4.985781192779541 | BCE Loss: 1.0451297760009766\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.056122779846191 | KNN Loss: 5.022688865661621 | BCE Loss: 1.0334340333938599\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.061659812927246 | KNN Loss: 5.031425952911377 | BCE Loss: 1.03023362159729\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.014382362365723 | KNN Loss: 4.977129936218262 | BCE Loss: 1.0372523069381714\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.066465377807617 | KNN Loss: 5.0223870277404785 | BCE Loss: 1.0440785884857178\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.076972007751465 | KNN Loss: 5.018827438354492 | BCE Loss: 1.0581448078155518\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.031400203704834 | KNN Loss: 4.9966230392456055 | BCE Loss: 1.034777283668518\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.0598225593566895 | KNN Loss: 5.0170111656188965 | BCE Loss: 1.0428115129470825\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.055039405822754 | KNN Loss: 5.030577182769775 | BCE Loss: 1.0244619846343994\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.046870708465576 | KNN Loss: 5.016636371612549 | BCE Loss: 1.0302343368530273\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 5.984971046447754 | KNN Loss: 4.989421844482422 | BCE Loss: 0.9955494403839111\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.056776523590088 | KNN Loss: 5.000551700592041 | BCE Loss: 1.0562248229980469\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.021181583404541 | KNN Loss: 4.9979658126831055 | BCE Loss: 1.0232157707214355\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.011960029602051 | KNN Loss: 5.007190704345703 | BCE Loss: 1.0047695636749268\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.0601911544799805 | KNN Loss: 5.0150675773620605 | BCE Loss: 1.0451234579086304\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.026669979095459 | KNN Loss: 4.996346950531006 | BCE Loss: 1.0303230285644531\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.032482147216797 | KNN Loss: 5.006654739379883 | BCE Loss: 1.0258276462554932\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.030912399291992 | KNN Loss: 4.985964298248291 | BCE Loss: 1.0449483394622803\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 5.993458271026611 | KNN Loss: 5.002525329589844 | BCE Loss: 0.9909330010414124\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.0225114822387695 | KNN Loss: 4.994751453399658 | BCE Loss: 1.0277601480484009\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.019504547119141 | KNN Loss: 4.999614715576172 | BCE Loss: 1.0198898315429688\n",
      "Epoch   288: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 5.999131202697754 | KNN Loss: 4.985067367553711 | BCE Loss: 1.0140639543533325\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.017202377319336 | KNN Loss: 4.995499610900879 | BCE Loss: 1.0217030048370361\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.019413948059082 | KNN Loss: 4.992765426635742 | BCE Loss: 1.026648759841919\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 5.9993486404418945 | KNN Loss: 4.995922088623047 | BCE Loss: 1.0034265518188477\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.051967620849609 | KNN Loss: 5.000442028045654 | BCE Loss: 1.0515254735946655\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.034745693206787 | KNN Loss: 5.023032188415527 | BCE Loss: 1.0117136240005493\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.023403167724609 | KNN Loss: 4.976211071014404 | BCE Loss: 1.047191858291626\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.012555122375488 | KNN Loss: 4.997439384460449 | BCE Loss: 1.015115737915039\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.052446365356445 | KNN Loss: 5.019297122955322 | BCE Loss: 1.033149003982544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.03170108795166 | KNN Loss: 5.0049967765808105 | BCE Loss: 1.0267040729522705\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.0596513748168945 | KNN Loss: 5.0432939529418945 | BCE Loss: 1.0163575410842896\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 5.988691806793213 | KNN Loss: 4.971010208129883 | BCE Loss: 1.01768159866333\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.040393352508545 | KNN Loss: 5.01356315612793 | BCE Loss: 1.0268300771713257\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.044535160064697 | KNN Loss: 5.017277717590332 | BCE Loss: 1.0272573232650757\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.014586925506592 | KNN Loss: 5.008232593536377 | BCE Loss: 1.0063544511795044\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.0018463134765625 | KNN Loss: 4.9893293380737305 | BCE Loss: 1.012516975402832\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.024381637573242 | KNN Loss: 4.995099067687988 | BCE Loss: 1.029282808303833\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.126033782958984 | KNN Loss: 5.06837272644043 | BCE Loss: 1.0576609373092651\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.023364067077637 | KNN Loss: 4.996476650238037 | BCE Loss: 1.0268871784210205\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.04522705078125 | KNN Loss: 5.013283729553223 | BCE Loss: 1.0319433212280273\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.047170639038086 | KNN Loss: 5.001795291900635 | BCE Loss: 1.0453752279281616\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.03037166595459 | KNN Loss: 4.985415935516357 | BCE Loss: 1.0449554920196533\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 5.98491907119751 | KNN Loss: 4.977255821228027 | BCE Loss: 1.0076631307601929\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.034201622009277 | KNN Loss: 4.994341850280762 | BCE Loss: 1.0398600101470947\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.009734153747559 | KNN Loss: 4.977329730987549 | BCE Loss: 1.0324046611785889\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.029172897338867 | KNN Loss: 5.020244598388672 | BCE Loss: 1.0089281797409058\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.066746711730957 | KNN Loss: 5.03001594543457 | BCE Loss: 1.0367307662963867\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.040926933288574 | KNN Loss: 4.9868245124816895 | BCE Loss: 1.0541025400161743\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.001458644866943 | KNN Loss: 4.999628067016602 | BCE Loss: 1.0018305778503418\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.029413223266602 | KNN Loss: 4.9935712814331055 | BCE Loss: 1.0358418226242065\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.010961055755615 | KNN Loss: 4.98899507522583 | BCE Loss: 1.0219660997390747\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.092931747436523 | KNN Loss: 5.049988746643066 | BCE Loss: 1.0429431200027466\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.0168232917785645 | KNN Loss: 4.977028846740723 | BCE Loss: 1.0397944450378418\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.018486499786377 | KNN Loss: 4.994713306427002 | BCE Loss: 1.023773193359375\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.066429138183594 | KNN Loss: 5.034237384796143 | BCE Loss: 1.0321919918060303\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.031560897827148 | KNN Loss: 5.024231910705566 | BCE Loss: 1.0073288679122925\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.025010108947754 | KNN Loss: 4.991644859313965 | BCE Loss: 1.03336501121521\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.050716876983643 | KNN Loss: 5.015316009521484 | BCE Loss: 1.0354009866714478\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.021013259887695 | KNN Loss: 4.999542236328125 | BCE Loss: 1.0214710235595703\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 5.995454788208008 | KNN Loss: 4.981551647186279 | BCE Loss: 1.0139031410217285\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.013681411743164 | KNN Loss: 4.977828502655029 | BCE Loss: 1.0358529090881348\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.031050205230713 | KNN Loss: 5.004940986633301 | BCE Loss: 1.026109218597412\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.044010162353516 | KNN Loss: 5.035971641540527 | BCE Loss: 1.0080385208129883\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.050027847290039 | KNN Loss: 5.028941631317139 | BCE Loss: 1.0210860967636108\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.024531364440918 | KNN Loss: 4.991700172424316 | BCE Loss: 1.0328309535980225\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.019937515258789 | KNN Loss: 4.9823899269104 | BCE Loss: 1.0375475883483887\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.034014701843262 | KNN Loss: 5.022937297821045 | BCE Loss: 1.0110774040222168\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.083978176116943 | KNN Loss: 5.062345504760742 | BCE Loss: 1.0216326713562012\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.0236101150512695 | KNN Loss: 4.99018669128418 | BCE Loss: 1.0334234237670898\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.056252479553223 | KNN Loss: 5.063835620880127 | BCE Loss: 0.9924167990684509\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.049963474273682 | KNN Loss: 5.005545139312744 | BCE Loss: 1.0444183349609375\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.0154218673706055 | KNN Loss: 5.0168776512146 | BCE Loss: 0.9985443353652954\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.058675765991211 | KNN Loss: 5.00852632522583 | BCE Loss: 1.0501493215560913\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 5.995040416717529 | KNN Loss: 4.985291957855225 | BCE Loss: 1.0097484588623047\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.043290138244629 | KNN Loss: 5.002771377563477 | BCE Loss: 1.0405187606811523\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.039495468139648 | KNN Loss: 5.023370265960693 | BCE Loss: 1.0161254405975342\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.022204875946045 | KNN Loss: 4.986381530761719 | BCE Loss: 1.0358233451843262\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.02174186706543 | KNN Loss: 5.0229291915893555 | BCE Loss: 0.9988127946853638\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.0168962478637695 | KNN Loss: 5.0093793869018555 | BCE Loss: 1.007516860961914\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.041491508483887 | KNN Loss: 5.031179428100586 | BCE Loss: 1.0103123188018799\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.079980850219727 | KNN Loss: 5.043240070343018 | BCE Loss: 1.036741018295288\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.025051593780518 | KNN Loss: 5.014997482299805 | BCE Loss: 1.0100542306900024\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.007767677307129 | KNN Loss: 4.976638317108154 | BCE Loss: 1.0311293601989746\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.021179676055908 | KNN Loss: 5.024631977081299 | BCE Loss: 0.9965475797653198\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.070740699768066 | KNN Loss: 5.031043529510498 | BCE Loss: 1.0396974086761475\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.027137756347656 | KNN Loss: 5.010298728942871 | BCE Loss: 1.0168390274047852\n",
      "Epoch   299: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.029813766479492 | KNN Loss: 5.009803771972656 | BCE Loss: 1.0200101137161255\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.037717819213867 | KNN Loss: 4.999081611633301 | BCE Loss: 1.0386359691619873\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.086215972900391 | KNN Loss: 5.0308403968811035 | BCE Loss: 1.055375337600708\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.060567855834961 | KNN Loss: 5.02030086517334 | BCE Loss: 1.040266752243042\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.041692733764648 | KNN Loss: 5.037294864654541 | BCE Loss: 1.0043981075286865\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.000965118408203 | KNN Loss: 5.005749702453613 | BCE Loss: 0.9952152967453003\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.020842552185059 | KNN Loss: 4.998764514923096 | BCE Loss: 1.022078037261963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.046483993530273 | KNN Loss: 5.026795387268066 | BCE Loss: 1.019688367843628\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.0946784019470215 | KNN Loss: 5.087503433227539 | BCE Loss: 1.0071749687194824\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.039053916931152 | KNN Loss: 4.999898433685303 | BCE Loss: 1.0391556024551392\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.017491817474365 | KNN Loss: 5.005316257476807 | BCE Loss: 1.0121755599975586\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.046480655670166 | KNN Loss: 5.0136260986328125 | BCE Loss: 1.0328545570373535\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.039165019989014 | KNN Loss: 5.008707046508789 | BCE Loss: 1.0304579734802246\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.046754837036133 | KNN Loss: 5.010242938995361 | BCE Loss: 1.0365121364593506\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.0189924240112305 | KNN Loss: 4.981760025024414 | BCE Loss: 1.0372322797775269\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.021465301513672 | KNN Loss: 4.993875980377197 | BCE Loss: 1.0275895595550537\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.003932476043701 | KNN Loss: 5.019169807434082 | BCE Loss: 0.9847626090049744\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.019046783447266 | KNN Loss: 4.9860005378723145 | BCE Loss: 1.0330464839935303\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 5.997706890106201 | KNN Loss: 4.9694061279296875 | BCE Loss: 1.0283006429672241\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.046342372894287 | KNN Loss: 5.026288986206055 | BCE Loss: 1.0200533866882324\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.037740707397461 | KNN Loss: 4.9932990074157715 | BCE Loss: 1.0444419384002686\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.003531455993652 | KNN Loss: 4.986917018890381 | BCE Loss: 1.0166141986846924\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.05064582824707 | KNN Loss: 5.029834747314453 | BCE Loss: 1.0208110809326172\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.043802261352539 | KNN Loss: 5.025094509124756 | BCE Loss: 1.0187077522277832\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.014915466308594 | KNN Loss: 5.003389358520508 | BCE Loss: 1.0115258693695068\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.044513702392578 | KNN Loss: 5.029139518737793 | BCE Loss: 1.0153740644454956\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.014439105987549 | KNN Loss: 5.006997585296631 | BCE Loss: 1.0074416399002075\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.004125595092773 | KNN Loss: 4.9957475662231445 | BCE Loss: 1.0083779096603394\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.051965236663818 | KNN Loss: 5.011893272399902 | BCE Loss: 1.0400720834732056\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.110803604125977 | KNN Loss: 5.0818562507629395 | BCE Loss: 1.0289475917816162\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.022331237792969 | KNN Loss: 5.004456996917725 | BCE Loss: 1.0178744792938232\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.03871488571167 | KNN Loss: 4.995281219482422 | BCE Loss: 1.0434335470199585\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.022416114807129 | KNN Loss: 5.01096773147583 | BCE Loss: 1.011448621749878\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.026228904724121 | KNN Loss: 4.987565994262695 | BCE Loss: 1.0386626720428467\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.018727779388428 | KNN Loss: 5.004334926605225 | BCE Loss: 1.0143927335739136\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.041031360626221 | KNN Loss: 5.016690731048584 | BCE Loss: 1.0243406295776367\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.039776802062988 | KNN Loss: 5.030759334564209 | BCE Loss: 1.0090173482894897\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.028990745544434 | KNN Loss: 5.002627372741699 | BCE Loss: 1.026363492012024\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.0808258056640625 | KNN Loss: 5.059457778930664 | BCE Loss: 1.0213677883148193\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.002267360687256 | KNN Loss: 4.992012023925781 | BCE Loss: 1.010255217552185\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.026329040527344 | KNN Loss: 5.01137638092041 | BCE Loss: 1.0149526596069336\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.042784690856934 | KNN Loss: 4.9987640380859375 | BCE Loss: 1.0440207719802856\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.055020809173584 | KNN Loss: 5.0439581871032715 | BCE Loss: 1.011062502861023\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.032291412353516 | KNN Loss: 5.000412464141846 | BCE Loss: 1.03187894821167\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.07833194732666 | KNN Loss: 5.053660869598389 | BCE Loss: 1.0246710777282715\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 5.994051456451416 | KNN Loss: 4.981743335723877 | BCE Loss: 1.012308120727539\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.089939117431641 | KNN Loss: 5.057750225067139 | BCE Loss: 1.032188892364502\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.073144912719727 | KNN Loss: 5.054265022277832 | BCE Loss: 1.0188796520233154\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.005266189575195 | KNN Loss: 4.976220607757568 | BCE Loss: 1.0290453433990479\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.009653091430664 | KNN Loss: 4.991888999938965 | BCE Loss: 1.0177643299102783\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.030995845794678 | KNN Loss: 5.010575294494629 | BCE Loss: 1.0204206705093384\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.035226821899414 | KNN Loss: 4.995452404022217 | BCE Loss: 1.0397746562957764\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.075598239898682 | KNN Loss: 5.034845352172852 | BCE Loss: 1.04075288772583\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.039989471435547 | KNN Loss: 5.009852409362793 | BCE Loss: 1.0301368236541748\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 5.99591064453125 | KNN Loss: 4.997039794921875 | BCE Loss: 0.9988706111907959\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.046325206756592 | KNN Loss: 5.0200629234313965 | BCE Loss: 1.0262622833251953\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.031870365142822 | KNN Loss: 5.000086784362793 | BCE Loss: 1.0317835807800293\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.051917552947998 | KNN Loss: 5.020394802093506 | BCE Loss: 1.0315227508544922\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.057602882385254 | KNN Loss: 4.991407871246338 | BCE Loss: 1.0661951303482056\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.042350769042969 | KNN Loss: 5.0311784744262695 | BCE Loss: 1.0111720561981201\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.007925510406494 | KNN Loss: 4.996341705322266 | BCE Loss: 1.011583685874939\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.062948703765869 | KNN Loss: 5.0430707931518555 | BCE Loss: 1.0198780298233032\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.007809162139893 | KNN Loss: 4.992824077606201 | BCE Loss: 1.0149850845336914\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 5.9964599609375 | KNN Loss: 4.98543119430542 | BCE Loss: 1.01102876663208\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.002778053283691 | KNN Loss: 5.010031223297119 | BCE Loss: 0.9927468299865723\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.038130760192871 | KNN Loss: 5.0231499671936035 | BCE Loss: 1.0149810314178467\n",
      "Epoch   310: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.037646770477295 | KNN Loss: 5.018958568572998 | BCE Loss: 1.0186882019042969\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.041987895965576 | KNN Loss: 5.032406806945801 | BCE Loss: 1.0095810890197754\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.039152145385742 | KNN Loss: 5.002252101898193 | BCE Loss: 1.0368998050689697\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.090402603149414 | KNN Loss: 5.034448623657227 | BCE Loss: 1.055953860282898\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.001481056213379 | KNN Loss: 4.980600357055664 | BCE Loss: 1.020880937576294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.019783020019531 | KNN Loss: 5.008553504943848 | BCE Loss: 1.0112295150756836\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.032553672790527 | KNN Loss: 4.996654510498047 | BCE Loss: 1.0358991622924805\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.059474468231201 | KNN Loss: 5.0266218185424805 | BCE Loss: 1.0328525304794312\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.071877479553223 | KNN Loss: 5.011009216308594 | BCE Loss: 1.060868501663208\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.030112266540527 | KNN Loss: 5.020129680633545 | BCE Loss: 1.0099824666976929\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.060830116271973 | KNN Loss: 5.029746055603027 | BCE Loss: 1.0310842990875244\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.013570308685303 | KNN Loss: 4.995748996734619 | BCE Loss: 1.0178213119506836\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.033949851989746 | KNN Loss: 5.020389080047607 | BCE Loss: 1.0135606527328491\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.00249719619751 | KNN Loss: 4.98803186416626 | BCE Loss: 1.0144654512405396\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.070387363433838 | KNN Loss: 5.006707191467285 | BCE Loss: 1.0636801719665527\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 5.986395835876465 | KNN Loss: 4.986527919769287 | BCE Loss: 0.9998679161071777\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.087644577026367 | KNN Loss: 5.038417339324951 | BCE Loss: 1.0492274761199951\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.032778263092041 | KNN Loss: 5.010492324829102 | BCE Loss: 1.022286057472229\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.023743629455566 | KNN Loss: 4.978770732879639 | BCE Loss: 1.0449728965759277\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.032261371612549 | KNN Loss: 4.99148416519165 | BCE Loss: 1.0407772064208984\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.017731189727783 | KNN Loss: 5.012295246124268 | BCE Loss: 1.0054359436035156\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.016438961029053 | KNN Loss: 4.999648094177246 | BCE Loss: 1.0167909860610962\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.048620223999023 | KNN Loss: 5.003093719482422 | BCE Loss: 1.0455265045166016\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.018449306488037 | KNN Loss: 4.996331214904785 | BCE Loss: 1.0221182107925415\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 5.987403392791748 | KNN Loss: 4.980523109436035 | BCE Loss: 1.006880283355713\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.071051597595215 | KNN Loss: 5.027030944824219 | BCE Loss: 1.0440205335617065\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.021457195281982 | KNN Loss: 5.0089311599731445 | BCE Loss: 1.012526035308838\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.059624671936035 | KNN Loss: 5.018887519836426 | BCE Loss: 1.0407369136810303\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.035019874572754 | KNN Loss: 5.010737419128418 | BCE Loss: 1.024282693862915\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.079746723175049 | KNN Loss: 5.03818416595459 | BCE Loss: 1.0415626764297485\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.071168899536133 | KNN Loss: 5.017613410949707 | BCE Loss: 1.0535557270050049\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.035794258117676 | KNN Loss: 5.008556365966797 | BCE Loss: 1.0272376537322998\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.012696266174316 | KNN Loss: 5.002038478851318 | BCE Loss: 1.0106576681137085\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 5.99502420425415 | KNN Loss: 4.975142002105713 | BCE Loss: 1.019882321357727\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.011451721191406 | KNN Loss: 4.993992328643799 | BCE Loss: 1.0174593925476074\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.026697635650635 | KNN Loss: 5.018167972564697 | BCE Loss: 1.008529543876648\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.035406112670898 | KNN Loss: 4.973947525024414 | BCE Loss: 1.061458706855774\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.001601219177246 | KNN Loss: 4.986025810241699 | BCE Loss: 1.0155755281448364\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.014612197875977 | KNN Loss: 5.008944988250732 | BCE Loss: 1.0056672096252441\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.0302863121032715 | KNN Loss: 4.998226165771484 | BCE Loss: 1.032060146331787\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 5.993879795074463 | KNN Loss: 4.997184753417969 | BCE Loss: 0.9966951608657837\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.030988693237305 | KNN Loss: 4.998047828674316 | BCE Loss: 1.0329407453536987\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.067054748535156 | KNN Loss: 5.022846221923828 | BCE Loss: 1.044208288192749\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.002449989318848 | KNN Loss: 5.002467155456543 | BCE Loss: 0.9999826550483704\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.042520523071289 | KNN Loss: 5.011935710906982 | BCE Loss: 1.0305850505828857\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.02803897857666 | KNN Loss: 4.994523525238037 | BCE Loss: 1.033515214920044\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 5.9784088134765625 | KNN Loss: 4.9668402671813965 | BCE Loss: 1.011568546295166\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.0774641036987305 | KNN Loss: 5.049463272094727 | BCE Loss: 1.0280005931854248\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.022360324859619 | KNN Loss: 4.998322486877441 | BCE Loss: 1.0240377187728882\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.023690223693848 | KNN Loss: 4.994198322296143 | BCE Loss: 1.0294917821884155\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.043006896972656 | KNN Loss: 4.999541282653809 | BCE Loss: 1.0434656143188477\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.038784503936768 | KNN Loss: 5.000512599945068 | BCE Loss: 1.0382720232009888\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.02061653137207 | KNN Loss: 5.005339622497559 | BCE Loss: 1.0152766704559326\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 5.99452543258667 | KNN Loss: 4.981712341308594 | BCE Loss: 1.0128130912780762\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.02314567565918 | KNN Loss: 5.013987064361572 | BCE Loss: 1.0091586112976074\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.047809600830078 | KNN Loss: 5.005318641662598 | BCE Loss: 1.0424907207489014\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.051884651184082 | KNN Loss: 5.027337074279785 | BCE Loss: 1.0245473384857178\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.012171745300293 | KNN Loss: 4.99259090423584 | BCE Loss: 1.0195810794830322\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.068633079528809 | KNN Loss: 5.051034927368164 | BCE Loss: 1.0175979137420654\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.038330554962158 | KNN Loss: 4.994317054748535 | BCE Loss: 1.044013500213623\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.034648418426514 | KNN Loss: 5.01741361618042 | BCE Loss: 1.0172348022460938\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.039825439453125 | KNN Loss: 4.9975266456604 | BCE Loss: 1.0422990322113037\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.012571811676025 | KNN Loss: 4.982500076293945 | BCE Loss: 1.03007173538208\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.078065395355225 | KNN Loss: 5.026317119598389 | BCE Loss: 1.051748275756836\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.057323455810547 | KNN Loss: 5.01746129989624 | BCE Loss: 1.0398621559143066\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.049415111541748 | KNN Loss: 5.0013275146484375 | BCE Loss: 1.048087477684021\n",
      "Epoch   321: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.007439613342285 | KNN Loss: 4.989407539367676 | BCE Loss: 1.0180320739746094\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.03430700302124 | KNN Loss: 4.9784932136535645 | BCE Loss: 1.0558139085769653\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.0585527420043945 | KNN Loss: 5.040785312652588 | BCE Loss: 1.0177675485610962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.021280288696289 | KNN Loss: 5.013329029083252 | BCE Loss: 1.0079514980316162\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.030664443969727 | KNN Loss: 5.000585556030273 | BCE Loss: 1.0300791263580322\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 5.997589111328125 | KNN Loss: 4.997008323669434 | BCE Loss: 1.0005810260772705\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.016063213348389 | KNN Loss: 4.985025405883789 | BCE Loss: 1.03103768825531\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.033472061157227 | KNN Loss: 4.96937370300293 | BCE Loss: 1.0640981197357178\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.0244293212890625 | KNN Loss: 4.991325378417969 | BCE Loss: 1.0331039428710938\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.00136661529541 | KNN Loss: 4.997824192047119 | BCE Loss: 1.0035425424575806\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 5.994441509246826 | KNN Loss: 4.995545864105225 | BCE Loss: 0.998895525932312\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.004535675048828 | KNN Loss: 4.993924140930176 | BCE Loss: 1.0106117725372314\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.043675422668457 | KNN Loss: 5.017692565917969 | BCE Loss: 1.0259830951690674\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.048578262329102 | KNN Loss: 5.024372577667236 | BCE Loss: 1.0242059230804443\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.043761730194092 | KNN Loss: 4.9946160316467285 | BCE Loss: 1.0491458177566528\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.0481157302856445 | KNN Loss: 5.020109176635742 | BCE Loss: 1.0280067920684814\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.096419334411621 | KNN Loss: 5.045080184936523 | BCE Loss: 1.0513393878936768\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 5.990974426269531 | KNN Loss: 4.985358715057373 | BCE Loss: 1.0056158304214478\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.077757835388184 | KNN Loss: 5.055319786071777 | BCE Loss: 1.0224380493164062\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 5.987273216247559 | KNN Loss: 4.984115123748779 | BCE Loss: 1.0031578540802002\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.0454301834106445 | KNN Loss: 5.003920078277588 | BCE Loss: 1.041509985923767\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.032126426696777 | KNN Loss: 5.007733345031738 | BCE Loss: 1.0243933200836182\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.058574199676514 | KNN Loss: 5.016244411468506 | BCE Loss: 1.0423297882080078\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.045361042022705 | KNN Loss: 5.011993885040283 | BCE Loss: 1.0333672761917114\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.037837028503418 | KNN Loss: 5.0172600746154785 | BCE Loss: 1.0205767154693604\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.044962406158447 | KNN Loss: 5.016192436218262 | BCE Loss: 1.0287699699401855\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.055217742919922 | KNN Loss: 5.021847724914551 | BCE Loss: 1.033369779586792\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.050497055053711 | KNN Loss: 5.027721881866455 | BCE Loss: 1.022775411605835\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.044198036193848 | KNN Loss: 5.018019676208496 | BCE Loss: 1.0261785984039307\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.014158248901367 | KNN Loss: 5.0015549659729 | BCE Loss: 1.012603521347046\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.037838935852051 | KNN Loss: 4.998998641967773 | BCE Loss: 1.0388405323028564\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.033439636230469 | KNN Loss: 5.0022406578063965 | BCE Loss: 1.0311987400054932\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.001921653747559 | KNN Loss: 4.980049133300781 | BCE Loss: 1.0218722820281982\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.006312847137451 | KNN Loss: 5.005830764770508 | BCE Loss: 1.0004819631576538\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.030808448791504 | KNN Loss: 4.995676040649414 | BCE Loss: 1.0351322889328003\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.017999649047852 | KNN Loss: 5.002757549285889 | BCE Loss: 1.0152418613433838\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.032773971557617 | KNN Loss: 5.018400192260742 | BCE Loss: 1.0143738985061646\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.136781692504883 | KNN Loss: 5.101480960845947 | BCE Loss: 1.035300612449646\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.028202533721924 | KNN Loss: 4.985231876373291 | BCE Loss: 1.0429706573486328\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.041313171386719 | KNN Loss: 5.019331455230713 | BCE Loss: 1.0219815969467163\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.020219802856445 | KNN Loss: 4.993658065795898 | BCE Loss: 1.026561975479126\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.001410007476807 | KNN Loss: 5.018883228302002 | BCE Loss: 0.9825266003608704\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.011812210083008 | KNN Loss: 5.000069618225098 | BCE Loss: 1.0117424726486206\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.025010108947754 | KNN Loss: 4.985086441040039 | BCE Loss: 1.0399236679077148\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.023896217346191 | KNN Loss: 5.004633903503418 | BCE Loss: 1.0192623138427734\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.029108047485352 | KNN Loss: 4.988598346710205 | BCE Loss: 1.040509581565857\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 5.9997100830078125 | KNN Loss: 4.993185043334961 | BCE Loss: 1.0065252780914307\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 5.992404937744141 | KNN Loss: 4.984189510345459 | BCE Loss: 1.0082156658172607\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.112419128417969 | KNN Loss: 5.048451900482178 | BCE Loss: 1.063967227935791\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.000804424285889 | KNN Loss: 4.987724304199219 | BCE Loss: 1.01308012008667\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.003485679626465 | KNN Loss: 4.992037773132324 | BCE Loss: 1.0114479064941406\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.037076950073242 | KNN Loss: 5.000809669494629 | BCE Loss: 1.0362670421600342\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.051234245300293 | KNN Loss: 5.039637088775635 | BCE Loss: 1.0115971565246582\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.07557487487793 | KNN Loss: 5.013722896575928 | BCE Loss: 1.061852216720581\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.008781433105469 | KNN Loss: 5.011270523071289 | BCE Loss: 0.9975106716156006\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.0390543937683105 | KNN Loss: 4.997892379760742 | BCE Loss: 1.0411620140075684\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.044557571411133 | KNN Loss: 5.029735565185547 | BCE Loss: 1.014822244644165\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.020041465759277 | KNN Loss: 5.012686252593994 | BCE Loss: 1.0073552131652832\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.060318946838379 | KNN Loss: 5.010624885559082 | BCE Loss: 1.049694299697876\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.035573959350586 | KNN Loss: 4.99070405960083 | BCE Loss: 1.044870138168335\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.016145706176758 | KNN Loss: 5.001785755157471 | BCE Loss: 1.0143601894378662\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.046367168426514 | KNN Loss: 4.994504451751709 | BCE Loss: 1.0518627166748047\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.036096096038818 | KNN Loss: 5.026846885681152 | BCE Loss: 1.0092490911483765\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.031439781188965 | KNN Loss: 4.990538597106934 | BCE Loss: 1.0409011840820312\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.0858354568481445 | KNN Loss: 5.05322265625 | BCE Loss: 1.0326125621795654\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.0509138107299805 | KNN Loss: 5.0038323402404785 | BCE Loss: 1.047081708908081\n",
      "Epoch   332: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.04598331451416 | KNN Loss: 4.98954963684082 | BCE Loss: 1.0564335584640503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.077615737915039 | KNN Loss: 5.045828819274902 | BCE Loss: 1.0317871570587158\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.036493301391602 | KNN Loss: 5.000519275665283 | BCE Loss: 1.0359737873077393\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.121911525726318 | KNN Loss: 5.084228038787842 | BCE Loss: 1.037683367729187\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.02011251449585 | KNN Loss: 5.001737594604492 | BCE Loss: 1.018375039100647\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.065311431884766 | KNN Loss: 5.0178117752075195 | BCE Loss: 1.0474998950958252\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.021064758300781 | KNN Loss: 4.995584011077881 | BCE Loss: 1.0254809856414795\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.030372619628906 | KNN Loss: 5.007797718048096 | BCE Loss: 1.0225751399993896\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.0334367752075195 | KNN Loss: 5.005491256713867 | BCE Loss: 1.0279452800750732\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.012862205505371 | KNN Loss: 5.006289482116699 | BCE Loss: 1.0065724849700928\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.000312805175781 | KNN Loss: 4.981411933898926 | BCE Loss: 1.0189011096954346\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.077284812927246 | KNN Loss: 5.015329360961914 | BCE Loss: 1.0619556903839111\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.048097610473633 | KNN Loss: 5.041926860809326 | BCE Loss: 1.0061707496643066\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.049990653991699 | KNN Loss: 5.045022010803223 | BCE Loss: 1.0049686431884766\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.0410027503967285 | KNN Loss: 4.989348411560059 | BCE Loss: 1.05165433883667\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.030570030212402 | KNN Loss: 5.018707752227783 | BCE Loss: 1.0118625164031982\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.0386643409729 | KNN Loss: 5.020947456359863 | BCE Loss: 1.0177167654037476\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.0267014503479 | KNN Loss: 4.992511749267578 | BCE Loss: 1.0341898202896118\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.07758903503418 | KNN Loss: 5.037568092346191 | BCE Loss: 1.0400208234786987\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.024907112121582 | KNN Loss: 4.991358757019043 | BCE Loss: 1.0335482358932495\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.063577651977539 | KNN Loss: 5.035214424133301 | BCE Loss: 1.0283633470535278\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.010248184204102 | KNN Loss: 4.980963706970215 | BCE Loss: 1.0292844772338867\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.043037414550781 | KNN Loss: 5.007756233215332 | BCE Loss: 1.0352814197540283\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.041722774505615 | KNN Loss: 5.0207343101501465 | BCE Loss: 1.0209884643554688\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.029840469360352 | KNN Loss: 4.998826503753662 | BCE Loss: 1.0310138463974\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.053009033203125 | KNN Loss: 5.021629810333252 | BCE Loss: 1.031378984451294\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.049728870391846 | KNN Loss: 5.0115485191345215 | BCE Loss: 1.0381803512573242\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.0047712326049805 | KNN Loss: 5.000234127044678 | BCE Loss: 1.0045371055603027\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.045995712280273 | KNN Loss: 5.021088600158691 | BCE Loss: 1.024907112121582\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.0477800369262695 | KNN Loss: 5.009265899658203 | BCE Loss: 1.0385141372680664\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.025355815887451 | KNN Loss: 4.989472389221191 | BCE Loss: 1.0358834266662598\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.130953788757324 | KNN Loss: 5.050004005432129 | BCE Loss: 1.0809499025344849\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.084438323974609 | KNN Loss: 5.064953327178955 | BCE Loss: 1.0194847583770752\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 5.987735271453857 | KNN Loss: 4.98343563079834 | BCE Loss: 1.0042996406555176\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.01142692565918 | KNN Loss: 4.9881086349487305 | BCE Loss: 1.0233185291290283\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.001500129699707 | KNN Loss: 5.005203723907471 | BCE Loss: 0.9962963461875916\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.064474105834961 | KNN Loss: 5.017421245574951 | BCE Loss: 1.0470528602600098\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.067867279052734 | KNN Loss: 5.019893169403076 | BCE Loss: 1.047973871231079\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.0353288650512695 | KNN Loss: 5.0009050369262695 | BCE Loss: 1.0344237089157104\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.0067267417907715 | KNN Loss: 4.991387367248535 | BCE Loss: 1.0153393745422363\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 5.985175609588623 | KNN Loss: 4.977038383483887 | BCE Loss: 1.0081372261047363\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 5.998071193695068 | KNN Loss: 4.971508502960205 | BCE Loss: 1.0265626907348633\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.058274745941162 | KNN Loss: 5.017450332641602 | BCE Loss: 1.0408244132995605\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.028628349304199 | KNN Loss: 4.9883317947387695 | BCE Loss: 1.0402963161468506\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.029918193817139 | KNN Loss: 5.002463340759277 | BCE Loss: 1.0274548530578613\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.000492572784424 | KNN Loss: 4.985232830047607 | BCE Loss: 1.0152597427368164\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.028512477874756 | KNN Loss: 5.005722522735596 | BCE Loss: 1.0227899551391602\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.015847206115723 | KNN Loss: 4.9761528968811035 | BCE Loss: 1.03969407081604\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.015927791595459 | KNN Loss: 4.985657691955566 | BCE Loss: 1.030269980430603\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 5.995154857635498 | KNN Loss: 4.984067916870117 | BCE Loss: 1.0110869407653809\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.062788009643555 | KNN Loss: 5.017991065979004 | BCE Loss: 1.0447969436645508\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.034529209136963 | KNN Loss: 5.036888599395752 | BCE Loss: 0.9976407885551453\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.04024600982666 | KNN Loss: 5.0092949867248535 | BCE Loss: 1.0309511423110962\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.012707710266113 | KNN Loss: 4.992323875427246 | BCE Loss: 1.0203840732574463\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.041545391082764 | KNN Loss: 5.021111488342285 | BCE Loss: 1.020434021949768\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.023606300354004 | KNN Loss: 5.007785797119141 | BCE Loss: 1.0158205032348633\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.065303325653076 | KNN Loss: 5.030355930328369 | BCE Loss: 1.034947395324707\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.012598037719727 | KNN Loss: 4.984228610992432 | BCE Loss: 1.0283695459365845\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.032705783843994 | KNN Loss: 5.009189605712891 | BCE Loss: 1.023516297340393\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.074702262878418 | KNN Loss: 5.0465264320373535 | BCE Loss: 1.0281758308410645\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.034025192260742 | KNN Loss: 5.005488872528076 | BCE Loss: 1.028536319732666\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.029755592346191 | KNN Loss: 5.012399196624756 | BCE Loss: 1.0173563957214355\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.001613616943359 | KNN Loss: 4.986499309539795 | BCE Loss: 1.0151140689849854\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.029066562652588 | KNN Loss: 5.009756088256836 | BCE Loss: 1.0193103551864624\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.038382053375244 | KNN Loss: 5.004465103149414 | BCE Loss: 1.0339168310165405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.004743576049805 | KNN Loss: 4.9858174324035645 | BCE Loss: 1.0189261436462402\n",
      "Epoch   343: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.0359086990356445 | KNN Loss: 5.0205888748168945 | BCE Loss: 1.015319585800171\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.067837238311768 | KNN Loss: 5.06294059753418 | BCE Loss: 1.004896640777588\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.021771430969238 | KNN Loss: 4.9852142333984375 | BCE Loss: 1.0365574359893799\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.072725296020508 | KNN Loss: 5.056077480316162 | BCE Loss: 1.0166479349136353\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.002506732940674 | KNN Loss: 4.998241424560547 | BCE Loss: 1.0042654275894165\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.047629356384277 | KNN Loss: 4.996552467346191 | BCE Loss: 1.0510767698287964\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.063973903656006 | KNN Loss: 5.011728763580322 | BCE Loss: 1.0522451400756836\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.062795162200928 | KNN Loss: 5.043489456176758 | BCE Loss: 1.0193055868148804\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.0744476318359375 | KNN Loss: 5.033621788024902 | BCE Loss: 1.0408260822296143\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.01310396194458 | KNN Loss: 4.988958835601807 | BCE Loss: 1.0241450071334839\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 6.025528907775879 | KNN Loss: 5.007053375244141 | BCE Loss: 1.0184752941131592\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.010830402374268 | KNN Loss: 5.003513813018799 | BCE Loss: 1.0073164701461792\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.015454292297363 | KNN Loss: 4.989550590515137 | BCE Loss: 1.0259039402008057\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.050899982452393 | KNN Loss: 5.01301908493042 | BCE Loss: 1.0378810167312622\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.084810256958008 | KNN Loss: 5.039767742156982 | BCE Loss: 1.0450422763824463\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.029046535491943 | KNN Loss: 4.9987993240356445 | BCE Loss: 1.0302472114562988\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.104526996612549 | KNN Loss: 5.0594096183776855 | BCE Loss: 1.0451173782348633\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.012090682983398 | KNN Loss: 4.980337619781494 | BCE Loss: 1.0317533016204834\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.033246994018555 | KNN Loss: 5.009349346160889 | BCE Loss: 1.0238975286483765\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 5.994852066040039 | KNN Loss: 4.997538089752197 | BCE Loss: 0.9973140954971313\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.040298938751221 | KNN Loss: 5.006989002227783 | BCE Loss: 1.033310055732727\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.028550148010254 | KNN Loss: 5.014084339141846 | BCE Loss: 1.0144659280776978\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.01866340637207 | KNN Loss: 5.000314712524414 | BCE Loss: 1.0183486938476562\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.007676601409912 | KNN Loss: 5.004856586456299 | BCE Loss: 1.0028200149536133\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 5.996497631072998 | KNN Loss: 4.9983110427856445 | BCE Loss: 0.9981865286827087\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.040757179260254 | KNN Loss: 5.01239013671875 | BCE Loss: 1.028367042541504\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.0347900390625 | KNN Loss: 5.037814140319824 | BCE Loss: 0.9969757795333862\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.061173915863037 | KNN Loss: 5.0045976638793945 | BCE Loss: 1.0565763711929321\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.008297443389893 | KNN Loss: 4.98881721496582 | BCE Loss: 1.0194802284240723\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.01207971572876 | KNN Loss: 4.9830322265625 | BCE Loss: 1.0290476083755493\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.003974437713623 | KNN Loss: 5.004211902618408 | BCE Loss: 0.9997626543045044\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.004632949829102 | KNN Loss: 4.974883556365967 | BCE Loss: 1.0297492742538452\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.0574235916137695 | KNN Loss: 5.02037239074707 | BCE Loss: 1.0370509624481201\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.0201416015625 | KNN Loss: 4.994087219238281 | BCE Loss: 1.0260541439056396\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.03514289855957 | KNN Loss: 4.996710300445557 | BCE Loss: 1.0384328365325928\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.045377731323242 | KNN Loss: 5.014376163482666 | BCE Loss: 1.031001329421997\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.003596782684326 | KNN Loss: 4.99698543548584 | BCE Loss: 1.0066113471984863\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.112037658691406 | KNN Loss: 5.059753894805908 | BCE Loss: 1.0522838830947876\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.020373821258545 | KNN Loss: 4.982089996337891 | BCE Loss: 1.0382837057113647\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.034650802612305 | KNN Loss: 5.024798393249512 | BCE Loss: 1.0098525285720825\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 5.991580963134766 | KNN Loss: 4.959352493286133 | BCE Loss: 1.0322283506393433\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.019542694091797 | KNN Loss: 5.001581192016602 | BCE Loss: 1.0179613828659058\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.057730197906494 | KNN Loss: 5.032474994659424 | BCE Loss: 1.0252550840377808\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.081060409545898 | KNN Loss: 5.019087791442871 | BCE Loss: 1.061972737312317\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.078411102294922 | KNN Loss: 5.038192272186279 | BCE Loss: 1.0402189493179321\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.044853210449219 | KNN Loss: 5.003381252288818 | BCE Loss: 1.0414721965789795\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.048130989074707 | KNN Loss: 5.01120662689209 | BCE Loss: 1.0369243621826172\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.012127876281738 | KNN Loss: 4.987753391265869 | BCE Loss: 1.0243746042251587\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.104546546936035 | KNN Loss: 5.047201156616211 | BCE Loss: 1.0573451519012451\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.0501484870910645 | KNN Loss: 5.010825157165527 | BCE Loss: 1.0393234491348267\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.066531658172607 | KNN Loss: 5.03080940246582 | BCE Loss: 1.035722255706787\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.030936241149902 | KNN Loss: 5.019989967346191 | BCE Loss: 1.010946273803711\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.02854061126709 | KNN Loss: 5.001915454864502 | BCE Loss: 1.026625394821167\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.0529069900512695 | KNN Loss: 5.010083198547363 | BCE Loss: 1.0428239107131958\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.022241592407227 | KNN Loss: 5.024537563323975 | BCE Loss: 0.9977042078971863\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.0000901222229 | KNN Loss: 4.984638214111328 | BCE Loss: 1.0154519081115723\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.008512496948242 | KNN Loss: 4.987630367279053 | BCE Loss: 1.0208818912506104\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.04859733581543 | KNN Loss: 5.00300407409668 | BCE Loss: 1.045593500137329\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.061351299285889 | KNN Loss: 5.011170387268066 | BCE Loss: 1.0501809120178223\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.004190921783447 | KNN Loss: 4.990821838378906 | BCE Loss: 1.0133689641952515\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.040919303894043 | KNN Loss: 4.991741180419922 | BCE Loss: 1.0491783618927002\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 5.97367525100708 | KNN Loss: 4.973997592926025 | BCE Loss: 0.999677836894989\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.072928428649902 | KNN Loss: 5.029601573944092 | BCE Loss: 1.0433266162872314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.092792510986328 | KNN Loss: 5.040063381195068 | BCE Loss: 1.0527291297912598\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.007585048675537 | KNN Loss: 5.013785362243652 | BCE Loss: 0.9937997460365295\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.024809837341309 | KNN Loss: 5.006701469421387 | BCE Loss: 1.0181083679199219\n",
      "Epoch   354: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.022641181945801 | KNN Loss: 4.999680995941162 | BCE Loss: 1.0229599475860596\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.03481912612915 | KNN Loss: 4.995436191558838 | BCE Loss: 1.039382815361023\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.034597873687744 | KNN Loss: 4.994095325469971 | BCE Loss: 1.040502667427063\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.003665924072266 | KNN Loss: 4.987577438354492 | BCE Loss: 1.0160882472991943\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.04669189453125 | KNN Loss: 5.026375770568848 | BCE Loss: 1.0203158855438232\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.024474143981934 | KNN Loss: 4.993419170379639 | BCE Loss: 1.0310548543930054\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.033497333526611 | KNN Loss: 5.0270233154296875 | BCE Loss: 1.0064738988876343\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.034839630126953 | KNN Loss: 5.001525402069092 | BCE Loss: 1.0333143472671509\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.001245975494385 | KNN Loss: 4.977883815765381 | BCE Loss: 1.0233620405197144\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.038803577423096 | KNN Loss: 5.025941848754883 | BCE Loss: 1.012861728668213\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.065464019775391 | KNN Loss: 4.9998273849487305 | BCE Loss: 1.0656368732452393\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.029070854187012 | KNN Loss: 5.005783557891846 | BCE Loss: 1.0232875347137451\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.082492828369141 | KNN Loss: 5.001653671264648 | BCE Loss: 1.0808393955230713\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.029926300048828 | KNN Loss: 4.984420299530029 | BCE Loss: 1.0455057621002197\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.025966167449951 | KNN Loss: 4.992557525634766 | BCE Loss: 1.033408761024475\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.045553684234619 | KNN Loss: 5.014415740966797 | BCE Loss: 1.0311379432678223\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.013131141662598 | KNN Loss: 4.988288402557373 | BCE Loss: 1.0248429775238037\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.0049285888671875 | KNN Loss: 5.0020904541015625 | BCE Loss: 1.002837896347046\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.044936180114746 | KNN Loss: 5.017853260040283 | BCE Loss: 1.0270826816558838\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.049900054931641 | KNN Loss: 4.987850666046143 | BCE Loss: 1.062049388885498\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.083535194396973 | KNN Loss: 5.00451135635376 | BCE Loss: 1.079024076461792\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 5.99655818939209 | KNN Loss: 4.988582611083984 | BCE Loss: 1.0079755783081055\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.055146217346191 | KNN Loss: 4.99514627456665 | BCE Loss: 1.059999704360962\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.016670227050781 | KNN Loss: 5.0015339851379395 | BCE Loss: 1.015136480331421\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.011232376098633 | KNN Loss: 4.994015693664551 | BCE Loss: 1.017216444015503\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.021913528442383 | KNN Loss: 5.000802040100098 | BCE Loss: 1.0211116075515747\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.060635089874268 | KNN Loss: 5.017352104187012 | BCE Loss: 1.0432829856872559\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.088644027709961 | KNN Loss: 5.075735569000244 | BCE Loss: 1.0129083395004272\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.022417068481445 | KNN Loss: 5.020208358764648 | BCE Loss: 1.0022087097167969\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.0230560302734375 | KNN Loss: 4.998194217681885 | BCE Loss: 1.0248620510101318\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.032732009887695 | KNN Loss: 5.004217147827148 | BCE Loss: 1.028515100479126\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.0773773193359375 | KNN Loss: 5.022768020629883 | BCE Loss: 1.0546095371246338\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.05789852142334 | KNN Loss: 5.029115676879883 | BCE Loss: 1.0287830829620361\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 5.998421669006348 | KNN Loss: 5.005234241485596 | BCE Loss: 0.9931874871253967\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.039066791534424 | KNN Loss: 5.001021862030029 | BCE Loss: 1.0380449295043945\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.004106521606445 | KNN Loss: 4.9825873374938965 | BCE Loss: 1.021519422531128\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.114497184753418 | KNN Loss: 5.081146240234375 | BCE Loss: 1.033351182937622\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.056413173675537 | KNN Loss: 5.042600154876709 | BCE Loss: 1.0138131380081177\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.0331315994262695 | KNN Loss: 4.995619297027588 | BCE Loss: 1.0375125408172607\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.035591125488281 | KNN Loss: 5.008549213409424 | BCE Loss: 1.0270419120788574\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.017104148864746 | KNN Loss: 4.993941783905029 | BCE Loss: 1.0231624841690063\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 6.004488468170166 | KNN Loss: 4.9819746017456055 | BCE Loss: 1.022513747215271\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.0132975578308105 | KNN Loss: 4.9900312423706055 | BCE Loss: 1.023266315460205\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 5.980525970458984 | KNN Loss: 4.980116367340088 | BCE Loss: 1.0004096031188965\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.041391372680664 | KNN Loss: 5.024114608764648 | BCE Loss: 1.0172770023345947\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.086330413818359 | KNN Loss: 5.037861347198486 | BCE Loss: 1.048468828201294\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.015380859375 | KNN Loss: 4.98474645614624 | BCE Loss: 1.0306346416473389\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.015179634094238 | KNN Loss: 4.993530750274658 | BCE Loss: 1.021648645401001\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.030508041381836 | KNN Loss: 5.0080461502075195 | BCE Loss: 1.0224616527557373\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.0532307624816895 | KNN Loss: 5.016453266143799 | BCE Loss: 1.036777377128601\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.0184760093688965 | KNN Loss: 4.988392353057861 | BCE Loss: 1.0300836563110352\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.004963397979736 | KNN Loss: 4.9839043617248535 | BCE Loss: 1.0210589170455933\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.009233474731445 | KNN Loss: 4.997908592224121 | BCE Loss: 1.0113250017166138\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.005788326263428 | KNN Loss: 4.988387584686279 | BCE Loss: 1.017400860786438\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.010720729827881 | KNN Loss: 5.005730628967285 | BCE Loss: 1.0049901008605957\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.036441802978516 | KNN Loss: 5.014177322387695 | BCE Loss: 1.0222644805908203\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.022188186645508 | KNN Loss: 5.008384704589844 | BCE Loss: 1.0138036012649536\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.036032676696777 | KNN Loss: 5.017697811126709 | BCE Loss: 1.0183351039886475\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.028838634490967 | KNN Loss: 4.99551248550415 | BCE Loss: 1.0333261489868164\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.099604606628418 | KNN Loss: 5.083958148956299 | BCE Loss: 1.01564621925354\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.0347113609313965 | KNN Loss: 5.0320143699646 | BCE Loss: 1.0026969909667969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.010943412780762 | KNN Loss: 4.977572917938232 | BCE Loss: 1.0333702564239502\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.026951789855957 | KNN Loss: 4.998095989227295 | BCE Loss: 1.028855562210083\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.016975402832031 | KNN Loss: 4.996631622314453 | BCE Loss: 1.0203437805175781\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.097758769989014 | KNN Loss: 5.06287956237793 | BCE Loss: 1.0348793268203735\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.00969123840332 | KNN Loss: 4.987896919250488 | BCE Loss: 1.0217944383621216\n",
      "Epoch   365: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.023990631103516 | KNN Loss: 5.000502586364746 | BCE Loss: 1.0234878063201904\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.050355911254883 | KNN Loss: 4.991152286529541 | BCE Loss: 1.0592033863067627\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.061166763305664 | KNN Loss: 5.026088714599609 | BCE Loss: 1.0350782871246338\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.048058986663818 | KNN Loss: 5.0329976081848145 | BCE Loss: 1.015061378479004\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 5.989551544189453 | KNN Loss: 4.987800598144531 | BCE Loss: 1.0017507076263428\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.114287853240967 | KNN Loss: 5.075069904327393 | BCE Loss: 1.0392179489135742\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.060709476470947 | KNN Loss: 5.009891033172607 | BCE Loss: 1.0508184432983398\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.037529468536377 | KNN Loss: 5.033689498901367 | BCE Loss: 1.0038399696350098\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.079197883605957 | KNN Loss: 5.037865161895752 | BCE Loss: 1.041332721710205\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 5.995589256286621 | KNN Loss: 4.999610424041748 | BCE Loss: 0.995978593826294\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.057924270629883 | KNN Loss: 5.019432544708252 | BCE Loss: 1.0384917259216309\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.031628608703613 | KNN Loss: 5.0055317878723145 | BCE Loss: 1.026097059249878\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.049180030822754 | KNN Loss: 5.001504898071289 | BCE Loss: 1.0476751327514648\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.027578353881836 | KNN Loss: 5.0202202796936035 | BCE Loss: 1.0073579549789429\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.033934593200684 | KNN Loss: 5.032561779022217 | BCE Loss: 1.0013726949691772\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.118953227996826 | KNN Loss: 5.074919700622559 | BCE Loss: 1.0440336465835571\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.005465030670166 | KNN Loss: 4.988429546356201 | BCE Loss: 1.0170354843139648\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.0104875564575195 | KNN Loss: 4.993160247802734 | BCE Loss: 1.0173271894454956\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.047526836395264 | KNN Loss: 4.995473384857178 | BCE Loss: 1.052053451538086\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.076706886291504 | KNN Loss: 5.062654972076416 | BCE Loss: 1.0140520334243774\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.02024507522583 | KNN Loss: 4.996289253234863 | BCE Loss: 1.0239559412002563\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.041058540344238 | KNN Loss: 5.01563024520874 | BCE Loss: 1.025428056716919\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.03715705871582 | KNN Loss: 5.005815505981445 | BCE Loss: 1.031341791152954\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.056488990783691 | KNN Loss: 5.022615909576416 | BCE Loss: 1.0338733196258545\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.039931774139404 | KNN Loss: 5.045090198516846 | BCE Loss: 0.9948416352272034\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.016295909881592 | KNN Loss: 4.973914623260498 | BCE Loss: 1.0423812866210938\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.0344061851501465 | KNN Loss: 5.014133453369141 | BCE Loss: 1.0202727317810059\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.070379257202148 | KNN Loss: 4.989846706390381 | BCE Loss: 1.0805325508117676\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.030732154846191 | KNN Loss: 5.03261137008667 | BCE Loss: 0.998120903968811\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.046710968017578 | KNN Loss: 5.01057767868042 | BCE Loss: 1.0361335277557373\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.005784511566162 | KNN Loss: 4.9993896484375 | BCE Loss: 1.006394863128662\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.007417678833008 | KNN Loss: 4.988948345184326 | BCE Loss: 1.0184690952301025\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.010200023651123 | KNN Loss: 4.997216701507568 | BCE Loss: 1.0129833221435547\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.0586981773376465 | KNN Loss: 5.024104595184326 | BCE Loss: 1.0345935821533203\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.02205228805542 | KNN Loss: 4.9942498207092285 | BCE Loss: 1.0278024673461914\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.053715229034424 | KNN Loss: 5.008616924285889 | BCE Loss: 1.0450981855392456\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.049589157104492 | KNN Loss: 4.9929399490356445 | BCE Loss: 1.0566494464874268\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.105038166046143 | KNN Loss: 5.076842784881592 | BCE Loss: 1.0281955003738403\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.0260114669799805 | KNN Loss: 5.001641273498535 | BCE Loss: 1.0243703126907349\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.023189067840576 | KNN Loss: 4.993022918701172 | BCE Loss: 1.0301661491394043\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 5.979679107666016 | KNN Loss: 4.981903076171875 | BCE Loss: 0.9977761507034302\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.014931678771973 | KNN Loss: 4.99091911315918 | BCE Loss: 1.0240126848220825\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.054810047149658 | KNN Loss: 4.998910903930664 | BCE Loss: 1.0558992624282837\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.012181758880615 | KNN Loss: 5.0189948081970215 | BCE Loss: 0.9931870698928833\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.042543888092041 | KNN Loss: 4.981922626495361 | BCE Loss: 1.0606212615966797\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.038925647735596 | KNN Loss: 5.015357971191406 | BCE Loss: 1.0235676765441895\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.069460868835449 | KNN Loss: 5.048651695251465 | BCE Loss: 1.0208089351654053\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.0578813552856445 | KNN Loss: 5.037294387817383 | BCE Loss: 1.0205867290496826\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.075945854187012 | KNN Loss: 5.027718544006348 | BCE Loss: 1.048227071762085\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.029581069946289 | KNN Loss: 4.9992289543151855 | BCE Loss: 1.0303518772125244\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.049654006958008 | KNN Loss: 5.046678066253662 | BCE Loss: 1.0029758214950562\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 5.991578578948975 | KNN Loss: 4.992918968200684 | BCE Loss: 0.998659610748291\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.0631818771362305 | KNN Loss: 5.001709461212158 | BCE Loss: 1.0614721775054932\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.030960559844971 | KNN Loss: 5.010997295379639 | BCE Loss: 1.0199631452560425\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.016887664794922 | KNN Loss: 5.002294063568115 | BCE Loss: 1.0145938396453857\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.022172451019287 | KNN Loss: 4.994792461395264 | BCE Loss: 1.0273799896240234\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.07430362701416 | KNN Loss: 5.019806861877441 | BCE Loss: 1.0544968843460083\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.024879455566406 | KNN Loss: 4.9844865798950195 | BCE Loss: 1.0403931140899658\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 5.997227668762207 | KNN Loss: 4.989114761352539 | BCE Loss: 1.008113145828247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.023799419403076 | KNN Loss: 4.982768535614014 | BCE Loss: 1.0410308837890625\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.081629753112793 | KNN Loss: 5.032063961029053 | BCE Loss: 1.0495657920837402\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 6.037136077880859 | KNN Loss: 5.00532865524292 | BCE Loss: 1.0318071842193604\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.009263038635254 | KNN Loss: 5.007815837860107 | BCE Loss: 1.0014472007751465\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.007588863372803 | KNN Loss: 4.988133907318115 | BCE Loss: 1.0194549560546875\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.011289596557617 | KNN Loss: 5.004359245300293 | BCE Loss: 1.0069303512573242\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.03176212310791 | KNN Loss: 4.974371433258057 | BCE Loss: 1.0573906898498535\n",
      "Epoch   376: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.06162166595459 | KNN Loss: 5.023324966430664 | BCE Loss: 1.0382969379425049\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.004122734069824 | KNN Loss: 4.981124401092529 | BCE Loss: 1.022998571395874\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.103531360626221 | KNN Loss: 5.051584243774414 | BCE Loss: 1.0519471168518066\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.004070281982422 | KNN Loss: 5.005081653594971 | BCE Loss: 0.9989883899688721\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.012109756469727 | KNN Loss: 4.978664398193359 | BCE Loss: 1.033445119857788\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.073822021484375 | KNN Loss: 5.040862560272217 | BCE Loss: 1.0329593420028687\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.084178447723389 | KNN Loss: 5.016568660736084 | BCE Loss: 1.0676097869873047\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.014778137207031 | KNN Loss: 5.010613441467285 | BCE Loss: 1.0041649341583252\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.019876480102539 | KNN Loss: 5.009528636932373 | BCE Loss: 1.010347604751587\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.0295000076293945 | KNN Loss: 4.995804309844971 | BCE Loss: 1.033695936203003\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.023308753967285 | KNN Loss: 5.035399436950684 | BCE Loss: 0.9879094362258911\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.009403705596924 | KNN Loss: 4.980694770812988 | BCE Loss: 1.028708815574646\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.071335792541504 | KNN Loss: 5.021442413330078 | BCE Loss: 1.0498933792114258\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.032097816467285 | KNN Loss: 4.989974021911621 | BCE Loss: 1.0421239137649536\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.047722339630127 | KNN Loss: 5.01572847366333 | BCE Loss: 1.0319938659667969\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.059353828430176 | KNN Loss: 5.028746128082275 | BCE Loss: 1.03060781955719\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.020939826965332 | KNN Loss: 5.017176151275635 | BCE Loss: 1.0037637948989868\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.050634384155273 | KNN Loss: 5.004739284515381 | BCE Loss: 1.0458948612213135\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.038994312286377 | KNN Loss: 5.010379314422607 | BCE Loss: 1.0286149978637695\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.001683712005615 | KNN Loss: 4.993374347686768 | BCE Loss: 1.0083094835281372\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.019732475280762 | KNN Loss: 4.995251655578613 | BCE Loss: 1.0244810581207275\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.021461009979248 | KNN Loss: 4.998907566070557 | BCE Loss: 1.022553563117981\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.070862293243408 | KNN Loss: 4.998832702636719 | BCE Loss: 1.0720295906066895\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.014371871948242 | KNN Loss: 4.985270023345947 | BCE Loss: 1.0291016101837158\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.073991775512695 | KNN Loss: 5.037782669067383 | BCE Loss: 1.036209225654602\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.009631633758545 | KNN Loss: 4.975856781005859 | BCE Loss: 1.0337748527526855\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.0529046058654785 | KNN Loss: 5.0055623054504395 | BCE Loss: 1.0473424196243286\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.075282096862793 | KNN Loss: 5.045273780822754 | BCE Loss: 1.0300085544586182\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.093355655670166 | KNN Loss: 5.042271137237549 | BCE Loss: 1.0510843992233276\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.1082282066345215 | KNN Loss: 5.048910617828369 | BCE Loss: 1.0593175888061523\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.062176704406738 | KNN Loss: 5.060547351837158 | BCE Loss: 1.00162935256958\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.017740249633789 | KNN Loss: 5.014468193054199 | BCE Loss: 1.0032718181610107\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.09115743637085 | KNN Loss: 5.063364505767822 | BCE Loss: 1.0277929306030273\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.025731086730957 | KNN Loss: 4.995670795440674 | BCE Loss: 1.0300602912902832\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.041608810424805 | KNN Loss: 5.003246307373047 | BCE Loss: 1.0383626222610474\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.029630661010742 | KNN Loss: 4.9888787269592285 | BCE Loss: 1.0407521724700928\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.039167404174805 | KNN Loss: 5.02435302734375 | BCE Loss: 1.0148144960403442\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.0028910636901855 | KNN Loss: 4.9845476150512695 | BCE Loss: 1.0183433294296265\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.030972003936768 | KNN Loss: 4.995658874511719 | BCE Loss: 1.0353130102157593\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.001145362854004 | KNN Loss: 4.990724563598633 | BCE Loss: 1.0104210376739502\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.045459747314453 | KNN Loss: 5.007481098175049 | BCE Loss: 1.0379786491394043\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.033356189727783 | KNN Loss: 5.014703273773193 | BCE Loss: 1.0186529159545898\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 5.992982387542725 | KNN Loss: 4.999283790588379 | BCE Loss: 0.9936987161636353\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.024282455444336 | KNN Loss: 5.004361152648926 | BCE Loss: 1.0199213027954102\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.007580280303955 | KNN Loss: 4.993732929229736 | BCE Loss: 1.0138473510742188\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.056873321533203 | KNN Loss: 5.007828712463379 | BCE Loss: 1.0490443706512451\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.045912742614746 | KNN Loss: 5.010368347167969 | BCE Loss: 1.0355441570281982\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.05073356628418 | KNN Loss: 5.022675037384033 | BCE Loss: 1.0280585289001465\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.014858245849609 | KNN Loss: 4.9967427253723145 | BCE Loss: 1.018115758895874\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 5.974433898925781 | KNN Loss: 4.998875141143799 | BCE Loss: 0.9755586385726929\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.010647773742676 | KNN Loss: 4.9786505699157715 | BCE Loss: 1.0319969654083252\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 5.995691299438477 | KNN Loss: 4.986609935760498 | BCE Loss: 1.0090816020965576\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.006030082702637 | KNN Loss: 4.9955153465271 | BCE Loss: 1.010514736175537\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.108713150024414 | KNN Loss: 5.073265075683594 | BCE Loss: 1.0354478359222412\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.026095390319824 | KNN Loss: 4.995194435119629 | BCE Loss: 1.0309009552001953\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.011800289154053 | KNN Loss: 5.020090579986572 | BCE Loss: 0.9917096495628357\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.043018341064453 | KNN Loss: 5.013389587402344 | BCE Loss: 1.0296287536621094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.037484169006348 | KNN Loss: 5.0340471267700195 | BCE Loss: 1.0034370422363281\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.028408050537109 | KNN Loss: 5.0018391609191895 | BCE Loss: 1.026569128036499\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.029114723205566 | KNN Loss: 5.038174152374268 | BCE Loss: 0.9909406900405884\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.024418830871582 | KNN Loss: 4.993710994720459 | BCE Loss: 1.0307080745697021\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 5.996101379394531 | KNN Loss: 4.991487979888916 | BCE Loss: 1.0046131610870361\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.047181129455566 | KNN Loss: 5.006996154785156 | BCE Loss: 1.0401849746704102\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.061966419219971 | KNN Loss: 5.043574333190918 | BCE Loss: 1.0183922052383423\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.0193986892700195 | KNN Loss: 4.989124774932861 | BCE Loss: 1.0302739143371582\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.026915073394775 | KNN Loss: 5.012675762176514 | BCE Loss: 1.0142394304275513\n",
      "Epoch   387: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 5.993268966674805 | KNN Loss: 4.99888801574707 | BCE Loss: 0.9943811297416687\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.065841197967529 | KNN Loss: 5.010137557983398 | BCE Loss: 1.0557036399841309\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.0379133224487305 | KNN Loss: 5.001642227172852 | BCE Loss: 1.0362712144851685\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.006405830383301 | KNN Loss: 5.001306056976318 | BCE Loss: 1.0050995349884033\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.0391154289245605 | KNN Loss: 5.029132843017578 | BCE Loss: 1.0099824666976929\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.035594940185547 | KNN Loss: 5.010286808013916 | BCE Loss: 1.0253078937530518\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.075179576873779 | KNN Loss: 5.02946138381958 | BCE Loss: 1.0457181930541992\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.031364440917969 | KNN Loss: 4.989745616912842 | BCE Loss: 1.041619062423706\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.014118671417236 | KNN Loss: 4.987551689147949 | BCE Loss: 1.026566982269287\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.0147624015808105 | KNN Loss: 4.987226963043213 | BCE Loss: 1.027535319328308\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.061559200286865 | KNN Loss: 5.024122714996338 | BCE Loss: 1.0374364852905273\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.079077243804932 | KNN Loss: 5.0185651779174805 | BCE Loss: 1.0605119466781616\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.024970054626465 | KNN Loss: 4.97857666015625 | BCE Loss: 1.046393632888794\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.048308849334717 | KNN Loss: 4.996685028076172 | BCE Loss: 1.051623821258545\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.057868480682373 | KNN Loss: 5.019912242889404 | BCE Loss: 1.0379562377929688\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.05582857131958 | KNN Loss: 5.016383171081543 | BCE Loss: 1.039445400238037\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 5.998435020446777 | KNN Loss: 5.011015892028809 | BCE Loss: 0.9874190092086792\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 5.997198581695557 | KNN Loss: 4.971611499786377 | BCE Loss: 1.0255870819091797\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 6.053706169128418 | KNN Loss: 5.014858245849609 | BCE Loss: 1.038847804069519\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.0336503982543945 | KNN Loss: 4.99587345123291 | BCE Loss: 1.0377769470214844\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 5.972421169281006 | KNN Loss: 4.965206146240234 | BCE Loss: 1.007214903831482\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 5.999608039855957 | KNN Loss: 4.98751974105835 | BCE Loss: 1.0120882987976074\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.040978908538818 | KNN Loss: 5.022617816925049 | BCE Loss: 1.0183610916137695\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.015146255493164 | KNN Loss: 4.989129066467285 | BCE Loss: 1.026017427444458\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.0299577713012695 | KNN Loss: 5.0049285888671875 | BCE Loss: 1.025029182434082\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.033817768096924 | KNN Loss: 4.987603187561035 | BCE Loss: 1.0462145805358887\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.078604698181152 | KNN Loss: 5.01508092880249 | BCE Loss: 1.063523530960083\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.022163391113281 | KNN Loss: 4.990909099578857 | BCE Loss: 1.0312540531158447\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.010885715484619 | KNN Loss: 5.012421607971191 | BCE Loss: 0.998464047908783\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.068617820739746 | KNN Loss: 5.0288825035095215 | BCE Loss: 1.0397350788116455\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.010708808898926 | KNN Loss: 4.989517688751221 | BCE Loss: 1.0211913585662842\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.0511627197265625 | KNN Loss: 4.998202323913574 | BCE Loss: 1.0529602766036987\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.03591775894165 | KNN Loss: 4.994500637054443 | BCE Loss: 1.0414170026779175\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.057879447937012 | KNN Loss: 5.04924201965332 | BCE Loss: 1.0086374282836914\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.052847862243652 | KNN Loss: 5.003970623016357 | BCE Loss: 1.048877239227295\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.007725238800049 | KNN Loss: 5.019689083099365 | BCE Loss: 0.9880361557006836\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.0160675048828125 | KNN Loss: 5.004909992218018 | BCE Loss: 1.0111573934555054\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.045915603637695 | KNN Loss: 4.999671459197998 | BCE Loss: 1.0462439060211182\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.029447078704834 | KNN Loss: 4.990759372711182 | BCE Loss: 1.0386877059936523\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.047904014587402 | KNN Loss: 5.03541898727417 | BCE Loss: 1.012485146522522\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.095220565795898 | KNN Loss: 5.056571006774902 | BCE Loss: 1.038649559020996\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 5.994531631469727 | KNN Loss: 4.983171463012695 | BCE Loss: 1.0113599300384521\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 6.033413887023926 | KNN Loss: 5.007798671722412 | BCE Loss: 1.0256152153015137\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 6.046778202056885 | KNN Loss: 5.00740909576416 | BCE Loss: 1.0393692255020142\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.02957010269165 | KNN Loss: 5.015957832336426 | BCE Loss: 1.0136123895645142\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.011491775512695 | KNN Loss: 4.98799467086792 | BCE Loss: 1.0234968662261963\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.020619869232178 | KNN Loss: 5.007760524749756 | BCE Loss: 1.0128594636917114\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.041825294494629 | KNN Loss: 4.997007369995117 | BCE Loss: 1.0448181629180908\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 5.998857498168945 | KNN Loss: 4.990065574645996 | BCE Loss: 1.0087919235229492\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.017078399658203 | KNN Loss: 4.987390995025635 | BCE Loss: 1.0296874046325684\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.020284652709961 | KNN Loss: 4.996933937072754 | BCE Loss: 1.0233509540557861\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 5.99677038192749 | KNN Loss: 5.004991054534912 | BCE Loss: 0.9917791485786438\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.0206756591796875 | KNN Loss: 5.0108747482299805 | BCE Loss: 1.009800672531128\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 5.9746551513671875 | KNN Loss: 4.989356517791748 | BCE Loss: 0.9852983951568604\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.092700004577637 | KNN Loss: 5.039811134338379 | BCE Loss: 1.0528888702392578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.026158332824707 | KNN Loss: 4.99375581741333 | BCE Loss: 1.0324022769927979\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.023304462432861 | KNN Loss: 5.019600868225098 | BCE Loss: 1.0037037134170532\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.030067443847656 | KNN Loss: 5.000393390655518 | BCE Loss: 1.0296742916107178\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.047136306762695 | KNN Loss: 5.0218186378479 | BCE Loss: 1.0253174304962158\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 5.991802215576172 | KNN Loss: 4.976236343383789 | BCE Loss: 1.0155659914016724\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.072108268737793 | KNN Loss: 5.007017612457275 | BCE Loss: 1.065090537071228\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.001628875732422 | KNN Loss: 4.997427463531494 | BCE Loss: 1.0042016506195068\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.043091297149658 | KNN Loss: 5.0255022048950195 | BCE Loss: 1.0175890922546387\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.015387535095215 | KNN Loss: 5.019172191619873 | BCE Loss: 0.9962153434753418\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.040619850158691 | KNN Loss: 4.9906110763549805 | BCE Loss: 1.0500088930130005\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.055584907531738 | KNN Loss: 5.026946544647217 | BCE Loss: 1.0286383628845215\n",
      "Epoch   398: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.038608551025391 | KNN Loss: 5.006927013397217 | BCE Loss: 1.0316814184188843\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.043277740478516 | KNN Loss: 5.0299482345581055 | BCE Loss: 1.0133297443389893\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.004262447357178 | KNN Loss: 4.993040561676025 | BCE Loss: 1.0112218856811523\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.026711463928223 | KNN Loss: 5.004558563232422 | BCE Loss: 1.0221529006958008\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.017834663391113 | KNN Loss: 5.00676965713501 | BCE Loss: 1.0110652446746826\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.082261085510254 | KNN Loss: 5.048400402069092 | BCE Loss: 1.0338609218597412\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 5.999327659606934 | KNN Loss: 4.986313819885254 | BCE Loss: 1.0130137205123901\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 5.997216701507568 | KNN Loss: 4.9807515144348145 | BCE Loss: 1.016465187072754\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.036324501037598 | KNN Loss: 5.009671211242676 | BCE Loss: 1.0266531705856323\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.050574779510498 | KNN Loss: 5.031225681304932 | BCE Loss: 1.0193490982055664\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.1130876541137695 | KNN Loss: 5.0647077560424805 | BCE Loss: 1.0483797788619995\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.107382297515869 | KNN Loss: 5.082607746124268 | BCE Loss: 1.0247745513916016\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.093958377838135 | KNN Loss: 5.053835868835449 | BCE Loss: 1.0401225090026855\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.066317081451416 | KNN Loss: 5.038519859313965 | BCE Loss: 1.0277972221374512\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.001579284667969 | KNN Loss: 4.992387771606445 | BCE Loss: 1.009191632270813\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.091767311096191 | KNN Loss: 5.039602756500244 | BCE Loss: 1.0521645545959473\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.017475128173828 | KNN Loss: 5.015359401702881 | BCE Loss: 1.0021158456802368\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.0390729904174805 | KNN Loss: 5.015777587890625 | BCE Loss: 1.0232951641082764\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.017172336578369 | KNN Loss: 5.002011775970459 | BCE Loss: 1.0151606798171997\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.048223495483398 | KNN Loss: 5.013453960418701 | BCE Loss: 1.0347697734832764\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.05128288269043 | KNN Loss: 5.008114814758301 | BCE Loss: 1.043168067932129\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.031108856201172 | KNN Loss: 4.992730617523193 | BCE Loss: 1.0383780002593994\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.053339004516602 | KNN Loss: 4.998202800750732 | BCE Loss: 1.0551362037658691\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.034112930297852 | KNN Loss: 5.005320072174072 | BCE Loss: 1.0287930965423584\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.058784484863281 | KNN Loss: 4.999772548675537 | BCE Loss: 1.0590121746063232\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.0269575119018555 | KNN Loss: 4.996467113494873 | BCE Loss: 1.0304906368255615\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.088961601257324 | KNN Loss: 5.0352020263671875 | BCE Loss: 1.0537595748901367\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.096251487731934 | KNN Loss: 5.054510116577148 | BCE Loss: 1.0417413711547852\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.002607345581055 | KNN Loss: 4.98816442489624 | BCE Loss: 1.0144426822662354\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.047286033630371 | KNN Loss: 5.010763168334961 | BCE Loss: 1.036522626876831\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.005568981170654 | KNN Loss: 4.979955673217773 | BCE Loss: 1.0256131887435913\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 5.993430137634277 | KNN Loss: 4.989588260650635 | BCE Loss: 1.003841757774353\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.041454315185547 | KNN Loss: 5.012102127075195 | BCE Loss: 1.0293523073196411\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.037022113800049 | KNN Loss: 5.011183738708496 | BCE Loss: 1.0258382558822632\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.051759719848633 | KNN Loss: 5.009521961212158 | BCE Loss: 1.0422377586364746\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.005436897277832 | KNN Loss: 4.988929748535156 | BCE Loss: 1.0165073871612549\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.023604393005371 | KNN Loss: 5.00984001159668 | BCE Loss: 1.013764500617981\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.008249282836914 | KNN Loss: 4.998165607452393 | BCE Loss: 1.0100834369659424\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.038547515869141 | KNN Loss: 5.011566162109375 | BCE Loss: 1.0269815921783447\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.008790016174316 | KNN Loss: 4.9758501052856445 | BCE Loss: 1.0329396724700928\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.0350236892700195 | KNN Loss: 4.998983383178711 | BCE Loss: 1.0360405445098877\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.019915580749512 | KNN Loss: 4.991431713104248 | BCE Loss: 1.0284838676452637\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.0593767166137695 | KNN Loss: 5.0150980949401855 | BCE Loss: 1.0442783832550049\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 5.9996466636657715 | KNN Loss: 4.986713886260986 | BCE Loss: 1.0129326581954956\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.032052040100098 | KNN Loss: 5.001314640045166 | BCE Loss: 1.0307375192642212\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.038378715515137 | KNN Loss: 5.034319877624512 | BCE Loss: 1.004058837890625\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.021390438079834 | KNN Loss: 4.983719825744629 | BCE Loss: 1.0376707315444946\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.05980920791626 | KNN Loss: 5.026364326477051 | BCE Loss: 1.033444881439209\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 5.998034477233887 | KNN Loss: 4.9977030754089355 | BCE Loss: 1.0003312826156616\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 6.056300163269043 | KNN Loss: 5.028541564941406 | BCE Loss: 1.0277585983276367\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.019155502319336 | KNN Loss: 4.988912582397461 | BCE Loss: 1.0302428007125854\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 6.08078145980835 | KNN Loss: 5.052602291107178 | BCE Loss: 1.0281790494918823\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 5.999999046325684 | KNN Loss: 4.985453128814697 | BCE Loss: 1.0145459175109863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.00821590423584 | KNN Loss: 4.98282527923584 | BCE Loss: 1.0253905057907104\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.069305419921875 | KNN Loss: 5.0304341316223145 | BCE Loss: 1.038871169090271\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.032289505004883 | KNN Loss: 5.043704986572266 | BCE Loss: 0.9885842800140381\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.060688018798828 | KNN Loss: 5.041126728057861 | BCE Loss: 1.0195614099502563\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.018387794494629 | KNN Loss: 4.992721080780029 | BCE Loss: 1.02566659450531\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.046230316162109 | KNN Loss: 5.019248008728027 | BCE Loss: 1.0269824266433716\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.075963973999023 | KNN Loss: 5.055668354034424 | BCE Loss: 1.0202953815460205\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.033123016357422 | KNN Loss: 5.0040483474731445 | BCE Loss: 1.0290746688842773\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.016972064971924 | KNN Loss: 5.009374141693115 | BCE Loss: 1.007597804069519\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.011466026306152 | KNN Loss: 4.985842227935791 | BCE Loss: 1.0256239175796509\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.036797523498535 | KNN Loss: 5.023865699768066 | BCE Loss: 1.0129318237304688\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.0428266525268555 | KNN Loss: 5.020012378692627 | BCE Loss: 1.0228140354156494\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.044149398803711 | KNN Loss: 4.990961074829102 | BCE Loss: 1.0531883239746094\n",
      "Epoch   409: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.0619354248046875 | KNN Loss: 4.990255832672119 | BCE Loss: 1.0716798305511475\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.047553539276123 | KNN Loss: 5.023432731628418 | BCE Loss: 1.0241206884384155\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.038496017456055 | KNN Loss: 5.028746128082275 | BCE Loss: 1.0097496509552002\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.053228855133057 | KNN Loss: 5.007318496704102 | BCE Loss: 1.0459104776382446\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.0330119132995605 | KNN Loss: 5.036691188812256 | BCE Loss: 0.996320903301239\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.0177812576293945 | KNN Loss: 5.000433921813965 | BCE Loss: 1.0173472166061401\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.038661956787109 | KNN Loss: 5.0232367515563965 | BCE Loss: 1.015425443649292\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 5.973165512084961 | KNN Loss: 4.986469268798828 | BCE Loss: 0.9866964817047119\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.062816619873047 | KNN Loss: 5.014642238616943 | BCE Loss: 1.0481743812561035\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.072810649871826 | KNN Loss: 5.011128902435303 | BCE Loss: 1.061681866645813\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.062684059143066 | KNN Loss: 5.012451648712158 | BCE Loss: 1.0502324104309082\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.020873069763184 | KNN Loss: 4.987316608428955 | BCE Loss: 1.0335562229156494\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.081119537353516 | KNN Loss: 5.057453632354736 | BCE Loss: 1.0236656665802002\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.055345058441162 | KNN Loss: 5.044979095458984 | BCE Loss: 1.0103658437728882\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.066697597503662 | KNN Loss: 5.024919509887695 | BCE Loss: 1.0417782068252563\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.027856826782227 | KNN Loss: 4.985795497894287 | BCE Loss: 1.0420615673065186\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.020267486572266 | KNN Loss: 4.99611234664917 | BCE Loss: 1.0241553783416748\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 5.987858772277832 | KNN Loss: 4.960198879241943 | BCE Loss: 1.0276598930358887\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.042381763458252 | KNN Loss: 4.986570358276367 | BCE Loss: 1.0558115243911743\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.015556335449219 | KNN Loss: 4.999486446380615 | BCE Loss: 1.0160696506500244\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.013500213623047 | KNN Loss: 4.994372844696045 | BCE Loss: 1.0191271305084229\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.032861709594727 | KNN Loss: 5.009594917297363 | BCE Loss: 1.0232667922973633\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.03269624710083 | KNN Loss: 5.0312700271606445 | BCE Loss: 1.0014262199401855\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.041046142578125 | KNN Loss: 5.01671838760376 | BCE Loss: 1.0243276357650757\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.052423477172852 | KNN Loss: 5.005463600158691 | BCE Loss: 1.046959638595581\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.050756454467773 | KNN Loss: 5.00748348236084 | BCE Loss: 1.0432729721069336\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.08427619934082 | KNN Loss: 5.026915073394775 | BCE Loss: 1.0573610067367554\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.062194347381592 | KNN Loss: 5.01173734664917 | BCE Loss: 1.0504570007324219\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.049962997436523 | KNN Loss: 5.01190185546875 | BCE Loss: 1.0380613803863525\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.018072605133057 | KNN Loss: 5.002641677856445 | BCE Loss: 1.0154309272766113\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.079821586608887 | KNN Loss: 5.006261825561523 | BCE Loss: 1.0735596418380737\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.042712211608887 | KNN Loss: 5.006743431091309 | BCE Loss: 1.0359690189361572\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.039704322814941 | KNN Loss: 5.000657558441162 | BCE Loss: 1.0390470027923584\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.061997413635254 | KNN Loss: 5.006060600280762 | BCE Loss: 1.0559370517730713\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.076240539550781 | KNN Loss: 5.052160263061523 | BCE Loss: 1.0240802764892578\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.038999557495117 | KNN Loss: 4.999212265014648 | BCE Loss: 1.0397870540618896\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.014183521270752 | KNN Loss: 4.992772102355957 | BCE Loss: 1.0214112997055054\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.036405563354492 | KNN Loss: 4.995577812194824 | BCE Loss: 1.0408275127410889\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.033178329467773 | KNN Loss: 5.016960144042969 | BCE Loss: 1.0162184238433838\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.0284247398376465 | KNN Loss: 4.996458530426025 | BCE Loss: 1.031966209411621\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.044519424438477 | KNN Loss: 5.023787498474121 | BCE Loss: 1.020732045173645\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.056684494018555 | KNN Loss: 5.019562721252441 | BCE Loss: 1.0371218919754028\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.098786354064941 | KNN Loss: 5.045877933502197 | BCE Loss: 1.0529086589813232\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.123372554779053 | KNN Loss: 5.0642852783203125 | BCE Loss: 1.0590872764587402\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.039076805114746 | KNN Loss: 5.020717620849609 | BCE Loss: 1.0183589458465576\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.0127997398376465 | KNN Loss: 4.990364074707031 | BCE Loss: 1.0224355459213257\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 5.989316940307617 | KNN Loss: 4.9863080978393555 | BCE Loss: 1.0030090808868408\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.046483039855957 | KNN Loss: 5.008868217468262 | BCE Loss: 1.0376150608062744\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.052420616149902 | KNN Loss: 5.017869472503662 | BCE Loss: 1.0345511436462402\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.054890155792236 | KNN Loss: 5.035979270935059 | BCE Loss: 1.0189108848571777\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.037823677062988 | KNN Loss: 5.0055623054504395 | BCE Loss: 1.032261610031128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.099130630493164 | KNN Loss: 5.0404582023620605 | BCE Loss: 1.0586724281311035\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.067590713500977 | KNN Loss: 5.033631801605225 | BCE Loss: 1.033959150314331\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.040345191955566 | KNN Loss: 5.0184431076049805 | BCE Loss: 1.0219018459320068\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.008139610290527 | KNN Loss: 4.979063034057617 | BCE Loss: 1.029076337814331\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.053720951080322 | KNN Loss: 5.012526988983154 | BCE Loss: 1.041193962097168\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.072053909301758 | KNN Loss: 5.031579971313477 | BCE Loss: 1.0404741764068604\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.044543743133545 | KNN Loss: 5.027968883514404 | BCE Loss: 1.0165748596191406\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 6.020480155944824 | KNN Loss: 5.001270771026611 | BCE Loss: 1.0192091464996338\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.012220859527588 | KNN Loss: 4.987576007843018 | BCE Loss: 1.0246449708938599\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.028964996337891 | KNN Loss: 4.998392105102539 | BCE Loss: 1.0305728912353516\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.055735111236572 | KNN Loss: 5.04551362991333 | BCE Loss: 1.0102214813232422\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.035462379455566 | KNN Loss: 4.993155002593994 | BCE Loss: 1.0423073768615723\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.035111427307129 | KNN Loss: 5.000475883483887 | BCE Loss: 1.0346355438232422\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.060070991516113 | KNN Loss: 5.0300798416137695 | BCE Loss: 1.0299913883209229\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.057492256164551 | KNN Loss: 5.028180122375488 | BCE Loss: 1.029312014579773\n",
      "Epoch   420: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.069832801818848 | KNN Loss: 5.030065059661865 | BCE Loss: 1.0397679805755615\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.067241191864014 | KNN Loss: 5.032552719116211 | BCE Loss: 1.0346884727478027\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.009462356567383 | KNN Loss: 4.984532356262207 | BCE Loss: 1.0249301195144653\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.071371078491211 | KNN Loss: 5.030313491821289 | BCE Loss: 1.0410577058792114\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.08873176574707 | KNN Loss: 5.034445285797119 | BCE Loss: 1.0542867183685303\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.048964500427246 | KNN Loss: 4.993906497955322 | BCE Loss: 1.055058240890503\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.041054725646973 | KNN Loss: 5.015377521514893 | BCE Loss: 1.0256774425506592\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.067282199859619 | KNN Loss: 5.030508041381836 | BCE Loss: 1.0367740392684937\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.059689998626709 | KNN Loss: 5.026844501495361 | BCE Loss: 1.0328454971313477\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.061145782470703 | KNN Loss: 5.02602481842041 | BCE Loss: 1.035121202468872\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.020536422729492 | KNN Loss: 4.994387149810791 | BCE Loss: 1.026149034500122\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.040009498596191 | KNN Loss: 5.008886337280273 | BCE Loss: 1.0311229228973389\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.03489875793457 | KNN Loss: 4.994531631469727 | BCE Loss: 1.0403673648834229\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.036112308502197 | KNN Loss: 4.979895114898682 | BCE Loss: 1.0562173128128052\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.0393385887146 | KNN Loss: 4.9965925216674805 | BCE Loss: 1.0427460670471191\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.019052028656006 | KNN Loss: 5.000492572784424 | BCE Loss: 1.0185593366622925\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 5.977427959442139 | KNN Loss: 4.984775543212891 | BCE Loss: 0.9926522970199585\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.020596504211426 | KNN Loss: 4.9908127784729 | BCE Loss: 1.029783844947815\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.046126365661621 | KNN Loss: 5.037457466125488 | BCE Loss: 1.0086688995361328\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.025202751159668 | KNN Loss: 4.98757266998291 | BCE Loss: 1.0376298427581787\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.056434631347656 | KNN Loss: 5.024607181549072 | BCE Loss: 1.031827688217163\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.053457736968994 | KNN Loss: 4.99552583694458 | BCE Loss: 1.057931900024414\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.019031047821045 | KNN Loss: 4.994581699371338 | BCE Loss: 1.0244492292404175\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.062459468841553 | KNN Loss: 5.0435638427734375 | BCE Loss: 1.0188956260681152\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.090121746063232 | KNN Loss: 5.036857604980469 | BCE Loss: 1.0532641410827637\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.016422271728516 | KNN Loss: 5.004405975341797 | BCE Loss: 1.0120165348052979\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.048516750335693 | KNN Loss: 4.9953155517578125 | BCE Loss: 1.0532011985778809\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.017482757568359 | KNN Loss: 5.019521713256836 | BCE Loss: 0.9979612231254578\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.054555892944336 | KNN Loss: 5.019791126251221 | BCE Loss: 1.0347646474838257\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 6.019794940948486 | KNN Loss: 4.986624240875244 | BCE Loss: 1.0331708192825317\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 5.993710517883301 | KNN Loss: 4.9778594970703125 | BCE Loss: 1.0158507823944092\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.046041488647461 | KNN Loss: 5.005832195281982 | BCE Loss: 1.0402090549468994\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.030954360961914 | KNN Loss: 5.0163984298706055 | BCE Loss: 1.0145556926727295\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.07605504989624 | KNN Loss: 5.03411340713501 | BCE Loss: 1.0419416427612305\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.0307297706604 | KNN Loss: 5.04134464263916 | BCE Loss: 0.9893851280212402\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.016457557678223 | KNN Loss: 5.002786159515381 | BCE Loss: 1.0136711597442627\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.02761697769165 | KNN Loss: 4.997311592102051 | BCE Loss: 1.0303053855895996\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.026352882385254 | KNN Loss: 4.999342918395996 | BCE Loss: 1.0270097255706787\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.0606794357299805 | KNN Loss: 4.993386745452881 | BCE Loss: 1.0672924518585205\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.026739120483398 | KNN Loss: 4.9895124435424805 | BCE Loss: 1.037226676940918\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.054986000061035 | KNN Loss: 5.02906608581543 | BCE Loss: 1.025919795036316\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 5.99739408493042 | KNN Loss: 4.990320205688477 | BCE Loss: 1.0070737600326538\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.032997131347656 | KNN Loss: 4.983259201049805 | BCE Loss: 1.0497376918792725\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.0144524574279785 | KNN Loss: 4.976622104644775 | BCE Loss: 1.0378302335739136\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.021486282348633 | KNN Loss: 5.0059309005737305 | BCE Loss: 1.0155551433563232\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.043700218200684 | KNN Loss: 5.040768623352051 | BCE Loss: 1.002931833267212\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.039794921875 | KNN Loss: 5.013698101043701 | BCE Loss: 1.0260969400405884\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.067506790161133 | KNN Loss: 5.01438045501709 | BCE Loss: 1.053126573562622\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.0438337326049805 | KNN Loss: 4.9952778816223145 | BCE Loss: 1.048555850982666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.044663429260254 | KNN Loss: 5.004051208496094 | BCE Loss: 1.040611982345581\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.009640216827393 | KNN Loss: 4.9978179931640625 | BCE Loss: 1.0118221044540405\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.015079498291016 | KNN Loss: 5.002893447875977 | BCE Loss: 1.0121862888336182\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.072293281555176 | KNN Loss: 5.046955585479736 | BCE Loss: 1.0253374576568604\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.008981704711914 | KNN Loss: 4.986256122589111 | BCE Loss: 1.0227253437042236\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.040383815765381 | KNN Loss: 5.014544486999512 | BCE Loss: 1.0258394479751587\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.039428234100342 | KNN Loss: 5.008208274841309 | BCE Loss: 1.0312198400497437\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.046314716339111 | KNN Loss: 5.020675182342529 | BCE Loss: 1.0256394147872925\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.007549285888672 | KNN Loss: 4.972429275512695 | BCE Loss: 1.0351200103759766\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.068100929260254 | KNN Loss: 5.067009925842285 | BCE Loss: 1.0010910034179688\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.038036346435547 | KNN Loss: 4.994469165802002 | BCE Loss: 1.043567180633545\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.060009002685547 | KNN Loss: 5.011855125427246 | BCE Loss: 1.0481541156768799\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.062758445739746 | KNN Loss: 4.987190246582031 | BCE Loss: 1.0755681991577148\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.002364635467529 | KNN Loss: 5.008730411529541 | BCE Loss: 0.9936341047286987\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.125651836395264 | KNN Loss: 5.061860084533691 | BCE Loss: 1.0637918710708618\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.026601791381836 | KNN Loss: 4.998963832855225 | BCE Loss: 1.0276379585266113\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.058732509613037 | KNN Loss: 5.047499179840088 | BCE Loss: 1.0112333297729492\n",
      "Epoch   431: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.054672718048096 | KNN Loss: 5.024719715118408 | BCE Loss: 1.029952883720398\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.062912464141846 | KNN Loss: 5.043708801269531 | BCE Loss: 1.0192036628723145\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 5.99976110458374 | KNN Loss: 4.983520984649658 | BCE Loss: 1.016240119934082\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.055843830108643 | KNN Loss: 5.021398544311523 | BCE Loss: 1.0344452857971191\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.053919792175293 | KNN Loss: 5.0348687171936035 | BCE Loss: 1.0190508365631104\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.003426551818848 | KNN Loss: 4.9891438484191895 | BCE Loss: 1.0142829418182373\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.03562593460083 | KNN Loss: 5.0103325843811035 | BCE Loss: 1.0252933502197266\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.003969192504883 | KNN Loss: 4.987679481506348 | BCE Loss: 1.016289472579956\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 5.983872413635254 | KNN Loss: 4.988648414611816 | BCE Loss: 0.9952241778373718\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.035435199737549 | KNN Loss: 5.016686916351318 | BCE Loss: 1.018748164176941\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.018654823303223 | KNN Loss: 5.012278079986572 | BCE Loss: 1.0063766241073608\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.055933952331543 | KNN Loss: 5.033801555633545 | BCE Loss: 1.022132158279419\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.0015692710876465 | KNN Loss: 4.9907402992248535 | BCE Loss: 1.010828971862793\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.0493693351745605 | KNN Loss: 5.0111565589904785 | BCE Loss: 1.0382126569747925\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.000967979431152 | KNN Loss: 4.996459484100342 | BCE Loss: 1.0045084953308105\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.028134822845459 | KNN Loss: 4.984401702880859 | BCE Loss: 1.0437331199645996\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.042418956756592 | KNN Loss: 5.009610652923584 | BCE Loss: 1.0328083038330078\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.04638671875 | KNN Loss: 5.022839546203613 | BCE Loss: 1.0235474109649658\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.028495788574219 | KNN Loss: 4.9984211921691895 | BCE Loss: 1.0300748348236084\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.048064231872559 | KNN Loss: 5.0050153732299805 | BCE Loss: 1.0430489778518677\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.0293378829956055 | KNN Loss: 5.006753444671631 | BCE Loss: 1.0225844383239746\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.0156965255737305 | KNN Loss: 4.990340232849121 | BCE Loss: 1.0253562927246094\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.012171745300293 | KNN Loss: 5.008767604827881 | BCE Loss: 1.003403902053833\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.040717601776123 | KNN Loss: 4.991573333740234 | BCE Loss: 1.0491443872451782\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.008923530578613 | KNN Loss: 4.988414287567139 | BCE Loss: 1.0205093622207642\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.097984313964844 | KNN Loss: 5.033743381500244 | BCE Loss: 1.0642410516738892\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 6.055256366729736 | KNN Loss: 5.0398101806640625 | BCE Loss: 1.0154461860656738\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.018400192260742 | KNN Loss: 4.981071949005127 | BCE Loss: 1.0373280048370361\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.027224540710449 | KNN Loss: 5.021900177001953 | BCE Loss: 1.005324363708496\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.037347793579102 | KNN Loss: 5.014456272125244 | BCE Loss: 1.0228912830352783\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.046756744384766 | KNN Loss: 4.9956374168396 | BCE Loss: 1.051119089126587\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.0350799560546875 | KNN Loss: 5.0135626792907715 | BCE Loss: 1.021517276763916\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.028939247131348 | KNN Loss: 4.9990410804748535 | BCE Loss: 1.0298980474472046\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.046041011810303 | KNN Loss: 4.998275279998779 | BCE Loss: 1.047765851020813\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.028556823730469 | KNN Loss: 4.990924835205078 | BCE Loss: 1.0376319885253906\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.021183013916016 | KNN Loss: 5.010406970977783 | BCE Loss: 1.0107762813568115\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 5.988687515258789 | KNN Loss: 4.974881172180176 | BCE Loss: 1.0138061046600342\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.048069477081299 | KNN Loss: 4.989373207092285 | BCE Loss: 1.0586961507797241\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.013747215270996 | KNN Loss: 5.000789642333984 | BCE Loss: 1.0129574537277222\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.025323867797852 | KNN Loss: 4.99543571472168 | BCE Loss: 1.0298881530761719\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.0087056159973145 | KNN Loss: 5.004514694213867 | BCE Loss: 1.0041908025741577\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.016175746917725 | KNN Loss: 4.976110935211182 | BCE Loss: 1.040064811706543\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.053225994110107 | KNN Loss: 4.9962158203125 | BCE Loss: 1.0570101737976074\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.081655502319336 | KNN Loss: 5.058678150177002 | BCE Loss: 1.022977352142334\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.008876800537109 | KNN Loss: 4.998433589935303 | BCE Loss: 1.0104434490203857\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.001898765563965 | KNN Loss: 4.995326995849609 | BCE Loss: 1.0065715312957764\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.035800933837891 | KNN Loss: 4.998582363128662 | BCE Loss: 1.0372183322906494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.029807090759277 | KNN Loss: 5.002346515655518 | BCE Loss: 1.0274605751037598\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.013274192810059 | KNN Loss: 5.011310577392578 | BCE Loss: 1.0019633769989014\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.071589469909668 | KNN Loss: 5.042746067047119 | BCE Loss: 1.0288431644439697\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.000012397766113 | KNN Loss: 4.99124813079834 | BCE Loss: 1.0087642669677734\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.06846809387207 | KNN Loss: 5.021121501922607 | BCE Loss: 1.047346591949463\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.0055084228515625 | KNN Loss: 4.982028007507324 | BCE Loss: 1.0234801769256592\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.038318634033203 | KNN Loss: 4.984135627746582 | BCE Loss: 1.0541828870773315\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.065736770629883 | KNN Loss: 5.024531841278076 | BCE Loss: 1.041204810142517\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.072824001312256 | KNN Loss: 5.039011478424072 | BCE Loss: 1.0338125228881836\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.047158241271973 | KNN Loss: 5.006053924560547 | BCE Loss: 1.0411043167114258\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.030656814575195 | KNN Loss: 5.003775596618652 | BCE Loss: 1.0268809795379639\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.083856582641602 | KNN Loss: 5.04234504699707 | BCE Loss: 1.0415114164352417\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.030877590179443 | KNN Loss: 5.007122993469238 | BCE Loss: 1.023754596710205\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.051614761352539 | KNN Loss: 5.022157192230225 | BCE Loss: 1.0294575691223145\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.041453838348389 | KNN Loss: 5.004849433898926 | BCE Loss: 1.036604404449463\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.050134181976318 | KNN Loss: 5.006974697113037 | BCE Loss: 1.0431596040725708\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.039927005767822 | KNN Loss: 4.989902496337891 | BCE Loss: 1.0500246286392212\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.036181449890137 | KNN Loss: 5.006933212280273 | BCE Loss: 1.0292479991912842\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.0203142166137695 | KNN Loss: 4.985930442810059 | BCE Loss: 1.034383773803711\n",
      "Epoch   442: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.034475326538086 | KNN Loss: 4.9983720779418945 | BCE Loss: 1.0361034870147705\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.027214527130127 | KNN Loss: 4.9965500831604 | BCE Loss: 1.0306644439697266\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.028024196624756 | KNN Loss: 5.0136284828186035 | BCE Loss: 1.0143957138061523\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.030167579650879 | KNN Loss: 4.980960369110107 | BCE Loss: 1.0492074489593506\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.024051189422607 | KNN Loss: 4.996336460113525 | BCE Loss: 1.027714729309082\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.020359039306641 | KNN Loss: 4.989681243896484 | BCE Loss: 1.0306779146194458\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.057655334472656 | KNN Loss: 5.032308578491211 | BCE Loss: 1.0253466367721558\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 5.985830783843994 | KNN Loss: 4.974010467529297 | BCE Loss: 1.0118203163146973\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.01592493057251 | KNN Loss: 5.001251220703125 | BCE Loss: 1.0146737098693848\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.141604423522949 | KNN Loss: 5.075870037078857 | BCE Loss: 1.0657343864440918\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.062787055969238 | KNN Loss: 5.039349555969238 | BCE Loss: 1.0234376192092896\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.086091041564941 | KNN Loss: 5.057053089141846 | BCE Loss: 1.0290381908416748\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.014040946960449 | KNN Loss: 4.998082637786865 | BCE Loss: 1.0159581899642944\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.027493476867676 | KNN Loss: 4.974287033081055 | BCE Loss: 1.053206205368042\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.059922218322754 | KNN Loss: 5.028850555419922 | BCE Loss: 1.0310717821121216\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.008754730224609 | KNN Loss: 4.992484092712402 | BCE Loss: 1.0162707567214966\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.0387678146362305 | KNN Loss: 5.03837776184082 | BCE Loss: 1.0003901720046997\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.007251739501953 | KNN Loss: 5.0023651123046875 | BCE Loss: 1.0048863887786865\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.041663646697998 | KNN Loss: 5.01078987121582 | BCE Loss: 1.0308736562728882\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.0012359619140625 | KNN Loss: 4.99276876449585 | BCE Loss: 1.008467197418213\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.100940704345703 | KNN Loss: 5.045535087585449 | BCE Loss: 1.0554053783416748\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.026525497436523 | KNN Loss: 5.025825500488281 | BCE Loss: 1.0006999969482422\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.033916473388672 | KNN Loss: 4.999236106872559 | BCE Loss: 1.0346806049346924\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.020232677459717 | KNN Loss: 4.9823079109191895 | BCE Loss: 1.0379247665405273\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.030264854431152 | KNN Loss: 5.008689880371094 | BCE Loss: 1.0215752124786377\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.041561126708984 | KNN Loss: 5.017385482788086 | BCE Loss: 1.0241756439208984\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.0232625007629395 | KNN Loss: 4.986817836761475 | BCE Loss: 1.0364446640014648\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.042230129241943 | KNN Loss: 5.0131330490112305 | BCE Loss: 1.0290971994400024\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 5.980650901794434 | KNN Loss: 4.980997085571289 | BCE Loss: 0.9996540546417236\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.024951457977295 | KNN Loss: 5.008541584014893 | BCE Loss: 1.0164098739624023\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.068820953369141 | KNN Loss: 5.050014972686768 | BCE Loss: 1.0188062191009521\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.0738749504089355 | KNN Loss: 5.017685413360596 | BCE Loss: 1.0561895370483398\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.060148239135742 | KNN Loss: 5.0256476402282715 | BCE Loss: 1.0345008373260498\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.018512725830078 | KNN Loss: 4.987250328063965 | BCE Loss: 1.0312621593475342\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 5.9913787841796875 | KNN Loss: 4.994065761566162 | BCE Loss: 0.9973130226135254\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 5.993687629699707 | KNN Loss: 5.0032734870910645 | BCE Loss: 0.9904141426086426\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.047475337982178 | KNN Loss: 5.009900093078613 | BCE Loss: 1.037575125694275\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.0226640701293945 | KNN Loss: 4.979084014892578 | BCE Loss: 1.0435798168182373\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.016371726989746 | KNN Loss: 4.975602626800537 | BCE Loss: 1.0407692193984985\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.0973052978515625 | KNN Loss: 5.055739879608154 | BCE Loss: 1.041565179824829\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.050258636474609 | KNN Loss: 5.044920444488525 | BCE Loss: 1.005338430404663\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.026177883148193 | KNN Loss: 5.011073589324951 | BCE Loss: 1.0151041746139526\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.0160017013549805 | KNN Loss: 4.995184898376465 | BCE Loss: 1.0208168029785156\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.002201080322266 | KNN Loss: 4.9913649559021 | BCE Loss: 1.010835886001587\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.0630011558532715 | KNN Loss: 4.998529434204102 | BCE Loss: 1.06447172164917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 5.998149394989014 | KNN Loss: 4.985795974731445 | BCE Loss: 1.0123534202575684\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 5.988480567932129 | KNN Loss: 4.9815239906311035 | BCE Loss: 1.0069568157196045\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 5.9784064292907715 | KNN Loss: 4.990254878997803 | BCE Loss: 0.9881516098976135\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.009145259857178 | KNN Loss: 4.9896345138549805 | BCE Loss: 1.0195107460021973\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.014015197753906 | KNN Loss: 5.000187873840332 | BCE Loss: 1.0138274431228638\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.029138565063477 | KNN Loss: 4.995615482330322 | BCE Loss: 1.0335230827331543\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.070472717285156 | KNN Loss: 5.054712772369385 | BCE Loss: 1.0157597064971924\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 6.025610446929932 | KNN Loss: 5.004109859466553 | BCE Loss: 1.021500587463379\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.047528266906738 | KNN Loss: 5.021854400634766 | BCE Loss: 1.0256741046905518\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.0385050773620605 | KNN Loss: 5.006199836730957 | BCE Loss: 1.0323052406311035\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.008847713470459 | KNN Loss: 5.044172763824463 | BCE Loss: 0.9646749496459961\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 5.999538898468018 | KNN Loss: 4.984777927398682 | BCE Loss: 1.0147608518600464\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 5.998765468597412 | KNN Loss: 4.980469226837158 | BCE Loss: 1.0182961225509644\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 5.986352920532227 | KNN Loss: 4.957646369934082 | BCE Loss: 1.0287063121795654\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.029672145843506 | KNN Loss: 5.005150318145752 | BCE Loss: 1.024521827697754\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.014400959014893 | KNN Loss: 5.0034003257751465 | BCE Loss: 1.0110005140304565\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.027029991149902 | KNN Loss: 5.007058620452881 | BCE Loss: 1.0199716091156006\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.024195194244385 | KNN Loss: 4.986729621887207 | BCE Loss: 1.0374656915664673\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.052763938903809 | KNN Loss: 5.024355411529541 | BCE Loss: 1.0284085273742676\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 6.063623428344727 | KNN Loss: 5.006719589233398 | BCE Loss: 1.0569038391113281\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.030719757080078 | KNN Loss: 5.010629653930664 | BCE Loss: 1.020089864730835\n",
      "Epoch   453: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.0231218338012695 | KNN Loss: 5.005624294281006 | BCE Loss: 1.0174973011016846\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.037252426147461 | KNN Loss: 4.9995317459106445 | BCE Loss: 1.0377205610275269\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.042346954345703 | KNN Loss: 5.026455402374268 | BCE Loss: 1.0158917903900146\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.048069000244141 | KNN Loss: 5.0403313636779785 | BCE Loss: 1.0077378749847412\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.004367351531982 | KNN Loss: 5.004786014556885 | BCE Loss: 0.9995813369750977\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.026204586029053 | KNN Loss: 5.008202075958252 | BCE Loss: 1.0180025100708008\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 5.981887340545654 | KNN Loss: 4.981621742248535 | BCE Loss: 1.0002655982971191\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.038557529449463 | KNN Loss: 4.991995811462402 | BCE Loss: 1.0465617179870605\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 5.997810363769531 | KNN Loss: 4.980988025665283 | BCE Loss: 1.0168225765228271\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 5.998614311218262 | KNN Loss: 4.982846736907959 | BCE Loss: 1.0157675743103027\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.061838150024414 | KNN Loss: 5.044337272644043 | BCE Loss: 1.017500638961792\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.059456825256348 | KNN Loss: 5.011984825134277 | BCE Loss: 1.0474717617034912\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.069048881530762 | KNN Loss: 5.019872188568115 | BCE Loss: 1.0491764545440674\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.021772384643555 | KNN Loss: 4.999425888061523 | BCE Loss: 1.0223462581634521\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.02059268951416 | KNN Loss: 5.001291751861572 | BCE Loss: 1.019300937652588\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.015491485595703 | KNN Loss: 5.004424095153809 | BCE Loss: 1.0110673904418945\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.042719841003418 | KNN Loss: 5.015102386474609 | BCE Loss: 1.0276172161102295\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.023934364318848 | KNN Loss: 4.9950175285339355 | BCE Loss: 1.0289167165756226\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.047088623046875 | KNN Loss: 5.018754959106445 | BCE Loss: 1.0283339023590088\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.025503158569336 | KNN Loss: 4.980494499206543 | BCE Loss: 1.0450084209442139\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.052975654602051 | KNN Loss: 5.017497539520264 | BCE Loss: 1.035477876663208\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.060028553009033 | KNN Loss: 5.013354301452637 | BCE Loss: 1.0466742515563965\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 5.989101409912109 | KNN Loss: 4.980031967163086 | BCE Loss: 1.009069561958313\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 5.999727725982666 | KNN Loss: 4.9854912757873535 | BCE Loss: 1.014236330986023\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.000404357910156 | KNN Loss: 5.0107502937316895 | BCE Loss: 0.9896540641784668\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.028491020202637 | KNN Loss: 5.00630521774292 | BCE Loss: 1.022186040878296\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.02001428604126 | KNN Loss: 4.988160610198975 | BCE Loss: 1.0318537950515747\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.045621871948242 | KNN Loss: 5.013083457946777 | BCE Loss: 1.0325384140014648\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.068601131439209 | KNN Loss: 5.028789520263672 | BCE Loss: 1.039811611175537\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.058938980102539 | KNN Loss: 4.9919753074646 | BCE Loss: 1.0669636726379395\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.049880027770996 | KNN Loss: 5.041736125946045 | BCE Loss: 1.0081440210342407\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.079166889190674 | KNN Loss: 5.056724548339844 | BCE Loss: 1.02244234085083\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.0492753982543945 | KNN Loss: 4.990543365478516 | BCE Loss: 1.058732032775879\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.020221710205078 | KNN Loss: 5.006005764007568 | BCE Loss: 1.0142160654067993\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.041669845581055 | KNN Loss: 5.003633975982666 | BCE Loss: 1.0380356311798096\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.021999359130859 | KNN Loss: 4.987277984619141 | BCE Loss: 1.0347216129302979\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.038848876953125 | KNN Loss: 5.011190891265869 | BCE Loss: 1.027658224105835\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.02580451965332 | KNN Loss: 5.0134148597717285 | BCE Loss: 1.0123894214630127\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.000840663909912 | KNN Loss: 4.994163513183594 | BCE Loss: 1.0066771507263184\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.033745765686035 | KNN Loss: 4.987779140472412 | BCE Loss: 1.045966386795044\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.091229438781738 | KNN Loss: 5.028491973876953 | BCE Loss: 1.0627375841140747\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.055163383483887 | KNN Loss: 5.016414165496826 | BCE Loss: 1.0387492179870605\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.087900161743164 | KNN Loss: 5.038468837738037 | BCE Loss: 1.049431562423706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.076190948486328 | KNN Loss: 5.036505699157715 | BCE Loss: 1.0396854877471924\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 5.98680305480957 | KNN Loss: 4.9798102378845215 | BCE Loss: 1.0069929361343384\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.058855056762695 | KNN Loss: 5.041995048522949 | BCE Loss: 1.016859769821167\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.000916481018066 | KNN Loss: 4.994308948516846 | BCE Loss: 1.0066072940826416\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.026651859283447 | KNN Loss: 5.011651992797852 | BCE Loss: 1.0149997472763062\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.0110392570495605 | KNN Loss: 4.977228164672852 | BCE Loss: 1.033811092376709\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.058467864990234 | KNN Loss: 5.020451545715332 | BCE Loss: 1.0380160808563232\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.073695182800293 | KNN Loss: 5.017346382141113 | BCE Loss: 1.0563490390777588\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.032209396362305 | KNN Loss: 4.978550434112549 | BCE Loss: 1.0536589622497559\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.039710998535156 | KNN Loss: 5.012131214141846 | BCE Loss: 1.0275797843933105\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.035175323486328 | KNN Loss: 4.992028713226318 | BCE Loss: 1.0431467294692993\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.061674118041992 | KNN Loss: 5.016476154327393 | BCE Loss: 1.0451982021331787\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.009296417236328 | KNN Loss: 5.010680198669434 | BCE Loss: 0.9986163377761841\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.03823184967041 | KNN Loss: 5.019217014312744 | BCE Loss: 1.0190150737762451\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.018031597137451 | KNN Loss: 4.983508586883545 | BCE Loss: 1.0345231294631958\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.067166328430176 | KNN Loss: 5.0339460372924805 | BCE Loss: 1.0332204103469849\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.017873287200928 | KNN Loss: 5.005303382873535 | BCE Loss: 1.0125699043273926\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 5.998776435852051 | KNN Loss: 4.994577407836914 | BCE Loss: 1.0041992664337158\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.052752494812012 | KNN Loss: 5.000668525695801 | BCE Loss: 1.05208420753479\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.009417533874512 | KNN Loss: 4.997730731964111 | BCE Loss: 1.01168692111969\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.0443434715271 | KNN Loss: 5.019942283630371 | BCE Loss: 1.0244011878967285\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.025750160217285 | KNN Loss: 5.000185966491699 | BCE Loss: 1.025564432144165\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.054881572723389 | KNN Loss: 5.046453952789307 | BCE Loss: 1.008427619934082\n",
      "Epoch   464: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.0231547355651855 | KNN Loss: 5.006657123565674 | BCE Loss: 1.0164976119995117\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 5.983413219451904 | KNN Loss: 5.009576320648193 | BCE Loss: 0.9738367199897766\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.003016471862793 | KNN Loss: 4.983685493469238 | BCE Loss: 1.0193310976028442\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.0634026527404785 | KNN Loss: 4.9867753982543945 | BCE Loss: 1.0766271352767944\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.044251441955566 | KNN Loss: 5.015076160430908 | BCE Loss: 1.0291755199432373\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.034466743469238 | KNN Loss: 4.986131191253662 | BCE Loss: 1.0483357906341553\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.0347161293029785 | KNN Loss: 5.001500606536865 | BCE Loss: 1.0332154035568237\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.070956230163574 | KNN Loss: 5.0151519775390625 | BCE Loss: 1.0558044910430908\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.036267280578613 | KNN Loss: 5.014636039733887 | BCE Loss: 1.0216310024261475\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.058320045471191 | KNN Loss: 4.99868106842041 | BCE Loss: 1.0596392154693604\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.0373125076293945 | KNN Loss: 4.995086669921875 | BCE Loss: 1.0422258377075195\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.034552574157715 | KNN Loss: 5.002373695373535 | BCE Loss: 1.0321788787841797\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.008328437805176 | KNN Loss: 5.001636505126953 | BCE Loss: 1.0066920518875122\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 5.977158546447754 | KNN Loss: 4.995700359344482 | BCE Loss: 0.9814581871032715\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.045022964477539 | KNN Loss: 5.020492076873779 | BCE Loss: 1.0245311260223389\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.036193370819092 | KNN Loss: 4.992825031280518 | BCE Loss: 1.0433683395385742\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.0598039627075195 | KNN Loss: 5.015820026397705 | BCE Loss: 1.0439839363098145\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.031252861022949 | KNN Loss: 5.006937503814697 | BCE Loss: 1.024315357208252\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.049201965332031 | KNN Loss: 5.029471397399902 | BCE Loss: 1.0197304487228394\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.0674333572387695 | KNN Loss: 5.026392459869385 | BCE Loss: 1.0410411357879639\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 5.984921932220459 | KNN Loss: 4.9857497215271 | BCE Loss: 0.9991723895072937\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.022012710571289 | KNN Loss: 5.01420783996582 | BCE Loss: 1.0078046321868896\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.044961452484131 | KNN Loss: 5.0575737953186035 | BCE Loss: 0.9873878359794617\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.090884685516357 | KNN Loss: 5.043032169342041 | BCE Loss: 1.0478525161743164\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.064389228820801 | KNN Loss: 5.024327278137207 | BCE Loss: 1.0400621891021729\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.0136027336120605 | KNN Loss: 4.995756149291992 | BCE Loss: 1.0178465843200684\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.040419101715088 | KNN Loss: 5.021193981170654 | BCE Loss: 1.0192251205444336\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.034407138824463 | KNN Loss: 4.998265743255615 | BCE Loss: 1.0361413955688477\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.0263214111328125 | KNN Loss: 5.005155086517334 | BCE Loss: 1.0211663246154785\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.016188621520996 | KNN Loss: 5.009136199951172 | BCE Loss: 1.0070524215698242\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.038164138793945 | KNN Loss: 5.0024495124816895 | BCE Loss: 1.0357147455215454\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.029451370239258 | KNN Loss: 4.9996018409729 | BCE Loss: 1.0298494100570679\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.061221122741699 | KNN Loss: 5.010280609130859 | BCE Loss: 1.0509405136108398\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.00395393371582 | KNN Loss: 4.9826884269714355 | BCE Loss: 1.0212652683258057\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.041668891906738 | KNN Loss: 5.027335166931152 | BCE Loss: 1.0143336057662964\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.053073883056641 | KNN Loss: 5.041632175445557 | BCE Loss: 1.011441946029663\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.029177188873291 | KNN Loss: 5.009815216064453 | BCE Loss: 1.0193618535995483\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.0905609130859375 | KNN Loss: 5.0370941162109375 | BCE Loss: 1.053466796875\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.059516429901123 | KNN Loss: 5.048496246337891 | BCE Loss: 1.011020302772522\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.040600776672363 | KNN Loss: 5.004795551300049 | BCE Loss: 1.0358049869537354\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.072451591491699 | KNN Loss: 5.026244640350342 | BCE Loss: 1.0462067127227783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.022159099578857 | KNN Loss: 4.992762088775635 | BCE Loss: 1.0293970108032227\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.073005676269531 | KNN Loss: 5.009349822998047 | BCE Loss: 1.0636560916900635\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.052948951721191 | KNN Loss: 5.012661933898926 | BCE Loss: 1.0402872562408447\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.033650875091553 | KNN Loss: 5.002420425415039 | BCE Loss: 1.0312303304672241\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.046316623687744 | KNN Loss: 5.022641181945801 | BCE Loss: 1.0236754417419434\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.02301025390625 | KNN Loss: 5.001128196716309 | BCE Loss: 1.0218818187713623\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.0301618576049805 | KNN Loss: 5.008731365203857 | BCE Loss: 1.021430253982544\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.019319534301758 | KNN Loss: 5.002521991729736 | BCE Loss: 1.0167975425720215\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.055677890777588 | KNN Loss: 5.014616966247559 | BCE Loss: 1.0410610437393188\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.0157790184021 | KNN Loss: 5.007387638092041 | BCE Loss: 1.0083913803100586\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.026369094848633 | KNN Loss: 5.017322540283203 | BCE Loss: 1.0090463161468506\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.057007312774658 | KNN Loss: 5.00061559677124 | BCE Loss: 1.0563918352127075\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.050252437591553 | KNN Loss: 5.021337509155273 | BCE Loss: 1.0289149284362793\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.0548095703125 | KNN Loss: 5.034934997558594 | BCE Loss: 1.0198746919631958\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.040755271911621 | KNN Loss: 5.00623893737793 | BCE Loss: 1.0345163345336914\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.053083419799805 | KNN Loss: 5.022204875946045 | BCE Loss: 1.0308785438537598\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.019575119018555 | KNN Loss: 5.005818843841553 | BCE Loss: 1.0137561559677124\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.006216526031494 | KNN Loss: 4.986288070678711 | BCE Loss: 1.0199283361434937\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.072876453399658 | KNN Loss: 5.01664924621582 | BCE Loss: 1.0562273263931274\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.022757053375244 | KNN Loss: 4.998505115509033 | BCE Loss: 1.024251937866211\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.082642078399658 | KNN Loss: 5.052916049957275 | BCE Loss: 1.0297260284423828\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.011120796203613 | KNN Loss: 4.996647357940674 | BCE Loss: 1.0144731998443604\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.021780967712402 | KNN Loss: 5.011481761932373 | BCE Loss: 1.0102992057800293\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.047182083129883 | KNN Loss: 5.033010005950928 | BCE Loss: 1.014171838760376\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.059722900390625 | KNN Loss: 5.007421493530273 | BCE Loss: 1.0523011684417725\n",
      "Epoch   475: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.024529457092285 | KNN Loss: 5.016501426696777 | BCE Loss: 1.008028268814087\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.067683219909668 | KNN Loss: 5.046327590942383 | BCE Loss: 1.0213555097579956\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.031772613525391 | KNN Loss: 4.989750385284424 | BCE Loss: 1.042022466659546\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.019491195678711 | KNN Loss: 4.9922685623168945 | BCE Loss: 1.0272228717803955\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.082221984863281 | KNN Loss: 5.043504238128662 | BCE Loss: 1.0387179851531982\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.042793273925781 | KNN Loss: 5.037757873535156 | BCE Loss: 1.005035400390625\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.0454607009887695 | KNN Loss: 5.003235816955566 | BCE Loss: 1.042224645614624\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.015859603881836 | KNN Loss: 4.989193916320801 | BCE Loss: 1.0266659259796143\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.03917932510376 | KNN Loss: 4.994771957397461 | BCE Loss: 1.0444073677062988\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.056553840637207 | KNN Loss: 5.029364585876465 | BCE Loss: 1.0271892547607422\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.036933898925781 | KNN Loss: 5.006380081176758 | BCE Loss: 1.0305536985397339\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.018808364868164 | KNN Loss: 4.988058567047119 | BCE Loss: 1.030749797821045\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.049132347106934 | KNN Loss: 5.008039951324463 | BCE Loss: 1.0410921573638916\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.01658821105957 | KNN Loss: 4.982595443725586 | BCE Loss: 1.0339930057525635\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.0124640464782715 | KNN Loss: 4.990297317504883 | BCE Loss: 1.0221666097640991\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.034808158874512 | KNN Loss: 4.983327388763428 | BCE Loss: 1.0514808893203735\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.046915531158447 | KNN Loss: 4.99717903137207 | BCE Loss: 1.0497363805770874\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.027606010437012 | KNN Loss: 5.011723518371582 | BCE Loss: 1.0158827304840088\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.069437026977539 | KNN Loss: 5.033442974090576 | BCE Loss: 1.0359938144683838\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.049639701843262 | KNN Loss: 5.029272556304932 | BCE Loss: 1.0203672647476196\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.042935371398926 | KNN Loss: 4.998661994934082 | BCE Loss: 1.0442733764648438\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.001957893371582 | KNN Loss: 5.002042770385742 | BCE Loss: 0.9999150633811951\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.052563667297363 | KNN Loss: 5.005786418914795 | BCE Loss: 1.0467771291732788\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.031220436096191 | KNN Loss: 4.985227584838867 | BCE Loss: 1.0459930896759033\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.037301063537598 | KNN Loss: 5.000488758087158 | BCE Loss: 1.0368120670318604\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 5.962515830993652 | KNN Loss: 4.966092109680176 | BCE Loss: 0.9964234828948975\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.028542995452881 | KNN Loss: 5.024922847747803 | BCE Loss: 1.0036201477050781\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.034783363342285 | KNN Loss: 5.020384788513184 | BCE Loss: 1.014398455619812\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.061147689819336 | KNN Loss: 5.028228759765625 | BCE Loss: 1.032918930053711\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.059900283813477 | KNN Loss: 4.9995293617248535 | BCE Loss: 1.060370683670044\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.0056939125061035 | KNN Loss: 4.9822564125061035 | BCE Loss: 1.0234375\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.048635005950928 | KNN Loss: 5.004608154296875 | BCE Loss: 1.0440268516540527\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.0444016456604 | KNN Loss: 5.020066261291504 | BCE Loss: 1.024335503578186\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.061480522155762 | KNN Loss: 5.052858352661133 | BCE Loss: 1.008622407913208\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 5.99632453918457 | KNN Loss: 4.989823341369629 | BCE Loss: 1.0065011978149414\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.028444290161133 | KNN Loss: 5.004209995269775 | BCE Loss: 1.0242342948913574\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.043456554412842 | KNN Loss: 5.003625869750977 | BCE Loss: 1.0398306846618652\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.037435531616211 | KNN Loss: 5.009673595428467 | BCE Loss: 1.027761697769165\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.043526649475098 | KNN Loss: 5.030189514160156 | BCE Loss: 1.0133368968963623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.055817127227783 | KNN Loss: 5.0468010902404785 | BCE Loss: 1.0090160369873047\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.0202507972717285 | KNN Loss: 4.999390125274658 | BCE Loss: 1.0208606719970703\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.018528461456299 | KNN Loss: 5.000139236450195 | BCE Loss: 1.018389344215393\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.071181774139404 | KNN Loss: 5.0631022453308105 | BCE Loss: 1.0080796480178833\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.069654941558838 | KNN Loss: 5.0339765548706055 | BCE Loss: 1.0356782674789429\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.013039588928223 | KNN Loss: 5.013042449951172 | BCE Loss: 0.9999969005584717\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.068719863891602 | KNN Loss: 5.020889759063721 | BCE Loss: 1.0478301048278809\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.03743839263916 | KNN Loss: 4.987823009490967 | BCE Loss: 1.0496156215667725\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.057336807250977 | KNN Loss: 5.013262748718262 | BCE Loss: 1.0440738201141357\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.028088092803955 | KNN Loss: 4.999853610992432 | BCE Loss: 1.0282344818115234\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.137941360473633 | KNN Loss: 5.126216888427734 | BCE Loss: 1.011724591255188\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.055112838745117 | KNN Loss: 4.993352890014648 | BCE Loss: 1.0617601871490479\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.047390937805176 | KNN Loss: 5.032649517059326 | BCE Loss: 1.0147416591644287\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.032079219818115 | KNN Loss: 5.006839275360107 | BCE Loss: 1.0252400636672974\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.050143241882324 | KNN Loss: 4.993780612945557 | BCE Loss: 1.0563628673553467\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.0255608558654785 | KNN Loss: 5.012146472930908 | BCE Loss: 1.0134143829345703\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.043474197387695 | KNN Loss: 5.0114426612854 | BCE Loss: 1.0320316553115845\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.042485237121582 | KNN Loss: 4.983251571655273 | BCE Loss: 1.0592334270477295\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.029522895812988 | KNN Loss: 5.030070781707764 | BCE Loss: 0.9994518756866455\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.081517219543457 | KNN Loss: 5.016590595245361 | BCE Loss: 1.0649267435073853\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.023157119750977 | KNN Loss: 5.0001654624938965 | BCE Loss: 1.02299165725708\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 5.989438533782959 | KNN Loss: 4.97205114364624 | BCE Loss: 1.0173873901367188\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.053042411804199 | KNN Loss: 4.995595932006836 | BCE Loss: 1.0574467182159424\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.021050930023193 | KNN Loss: 4.981788158416748 | BCE Loss: 1.0392627716064453\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.027158737182617 | KNN Loss: 5.017291069030762 | BCE Loss: 1.0098679065704346\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 5.9948883056640625 | KNN Loss: 5.005326271057129 | BCE Loss: 0.9895621538162231\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.00399112701416 | KNN Loss: 4.994290351867676 | BCE Loss: 1.0097005367279053\n",
      "Epoch   486: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.044229030609131 | KNN Loss: 5.014149188995361 | BCE Loss: 1.0300798416137695\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.067874431610107 | KNN Loss: 5.002532005310059 | BCE Loss: 1.0653424263000488\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.042635440826416 | KNN Loss: 5.018957614898682 | BCE Loss: 1.0236777067184448\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.010739803314209 | KNN Loss: 4.995666027069092 | BCE Loss: 1.0150736570358276\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.028144836425781 | KNN Loss: 5.0145039558410645 | BCE Loss: 1.013641119003296\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.011104583740234 | KNN Loss: 5.007755756378174 | BCE Loss: 1.0033490657806396\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.053155422210693 | KNN Loss: 4.997864246368408 | BCE Loss: 1.0552912950515747\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 6.030261039733887 | KNN Loss: 4.993450164794922 | BCE Loss: 1.036811113357544\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.099283218383789 | KNN Loss: 5.057575702667236 | BCE Loss: 1.0417072772979736\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.0769805908203125 | KNN Loss: 5.018570423126221 | BCE Loss: 1.0584100484848022\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.039654731750488 | KNN Loss: 5.018463611602783 | BCE Loss: 1.021190881729126\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.037640571594238 | KNN Loss: 5.00065279006958 | BCE Loss: 1.0369880199432373\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 5.995913505554199 | KNN Loss: 4.982172012329102 | BCE Loss: 1.0137412548065186\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.007267951965332 | KNN Loss: 4.981592655181885 | BCE Loss: 1.0256755352020264\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.016770839691162 | KNN Loss: 5.010476112365723 | BCE Loss: 1.00629460811615\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.027369022369385 | KNN Loss: 5.004921913146973 | BCE Loss: 1.0224469900131226\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.027299880981445 | KNN Loss: 5.0015716552734375 | BCE Loss: 1.0257281064987183\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.037013053894043 | KNN Loss: 5.015259265899658 | BCE Loss: 1.0217537879943848\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.028999328613281 | KNN Loss: 4.9830827713012695 | BCE Loss: 1.0459166765213013\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.02850341796875 | KNN Loss: 5.001890659332275 | BCE Loss: 1.0266125202178955\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.053436279296875 | KNN Loss: 5.03319787979126 | BCE Loss: 1.0202383995056152\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 5.9924702644348145 | KNN Loss: 4.982757568359375 | BCE Loss: 1.00971257686615\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.0435872077941895 | KNN Loss: 5.005377769470215 | BCE Loss: 1.038209319114685\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.116798400878906 | KNN Loss: 5.033003807067871 | BCE Loss: 1.083794355392456\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.082030773162842 | KNN Loss: 5.031447887420654 | BCE Loss: 1.0505828857421875\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.033168792724609 | KNN Loss: 5.016657829284668 | BCE Loss: 1.0165112018585205\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.0150322914123535 | KNN Loss: 4.992044925689697 | BCE Loss: 1.0229873657226562\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.04635763168335 | KNN Loss: 5.007187366485596 | BCE Loss: 1.0391701459884644\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.031795024871826 | KNN Loss: 4.99194860458374 | BCE Loss: 1.039846420288086\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.016935348510742 | KNN Loss: 4.996984481811523 | BCE Loss: 1.0199507474899292\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.015883445739746 | KNN Loss: 4.984623432159424 | BCE Loss: 1.0312598943710327\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.012359619140625 | KNN Loss: 5.001194477081299 | BCE Loss: 1.011164903640747\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.030885696411133 | KNN Loss: 5.007260799407959 | BCE Loss: 1.0236246585845947\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.003504753112793 | KNN Loss: 4.979146957397461 | BCE Loss: 1.0243580341339111\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 6.078772068023682 | KNN Loss: 5.053615570068359 | BCE Loss: 1.0251564979553223\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.055530548095703 | KNN Loss: 5.021256446838379 | BCE Loss: 1.0342742204666138\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.051910400390625 | KNN Loss: 5.014316082000732 | BCE Loss: 1.0375940799713135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.004672527313232 | KNN Loss: 5.000941276550293 | BCE Loss: 1.003731369972229\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.00396728515625 | KNN Loss: 4.997598648071289 | BCE Loss: 1.00636887550354\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.017963886260986 | KNN Loss: 5.013737201690674 | BCE Loss: 1.004226803779602\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.02263069152832 | KNN Loss: 5.001904487609863 | BCE Loss: 1.020725965499878\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 6.031940460205078 | KNN Loss: 5.015169143676758 | BCE Loss: 1.0167710781097412\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.021288871765137 | KNN Loss: 4.998803615570068 | BCE Loss: 1.0224854946136475\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.0339555740356445 | KNN Loss: 4.987903118133545 | BCE Loss: 1.0460525751113892\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.079273223876953 | KNN Loss: 5.01887845993042 | BCE Loss: 1.0603947639465332\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.017971038818359 | KNN Loss: 4.990634918212891 | BCE Loss: 1.0273361206054688\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.008121967315674 | KNN Loss: 4.9887776374816895 | BCE Loss: 1.0193442106246948\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.0222625732421875 | KNN Loss: 5.001865386962891 | BCE Loss: 1.0203970670700073\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.0168375968933105 | KNN Loss: 4.996054172515869 | BCE Loss: 1.0207834243774414\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.044719696044922 | KNN Loss: 4.989786624908447 | BCE Loss: 1.0549328327178955\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.043097972869873 | KNN Loss: 5.02437686920166 | BCE Loss: 1.018721103668213\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.006063461303711 | KNN Loss: 4.994292736053467 | BCE Loss: 1.0117707252502441\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 5.981277942657471 | KNN Loss: 4.998906135559082 | BCE Loss: 0.9823718070983887\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.023775577545166 | KNN Loss: 4.985803604125977 | BCE Loss: 1.0379718542099\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 5.994590759277344 | KNN Loss: 4.990293502807617 | BCE Loss: 1.0042970180511475\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.077596187591553 | KNN Loss: 5.051324844360352 | BCE Loss: 1.0262712240219116\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.032255172729492 | KNN Loss: 4.986739635467529 | BCE Loss: 1.045515775680542\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.068480491638184 | KNN Loss: 5.038690090179443 | BCE Loss: 1.0297905206680298\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.07075309753418 | KNN Loss: 5.033545970916748 | BCE Loss: 1.037207007408142\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.069357872009277 | KNN Loss: 5.051669597625732 | BCE Loss: 1.017688512802124\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.0128936767578125 | KNN Loss: 4.987426280975342 | BCE Loss: 1.0254671573638916\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.0627546310424805 | KNN Loss: 5.009090423583984 | BCE Loss: 1.0536644458770752\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.020916938781738 | KNN Loss: 5.0175395011901855 | BCE Loss: 1.0033774375915527\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.120269298553467 | KNN Loss: 5.080853462219238 | BCE Loss: 1.039415717124939\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.043012619018555 | KNN Loss: 5.016713619232178 | BCE Loss: 1.026298999786377\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.027425765991211 | KNN Loss: 4.975336074829102 | BCE Loss: 1.052089810371399\n",
      "Epoch   497: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 5.979714393615723 | KNN Loss: 4.984037399291992 | BCE Loss: 0.9956767559051514\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.014683723449707 | KNN Loss: 4.993086338043213 | BCE Loss: 1.0215972661972046\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.0202789306640625 | KNN Loss: 5.003040313720703 | BCE Loss: 1.017238736152649\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.025040149688721 | KNN Loss: 5.004665851593018 | BCE Loss: 1.0203742980957031\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.058919906616211 | KNN Loss: 5.0325727462768555 | BCE Loss: 1.026347041130066\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.075996398925781 | KNN Loss: 5.006533622741699 | BCE Loss: 1.069462537765503\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.049043655395508 | KNN Loss: 5.0266618728637695 | BCE Loss: 1.0223815441131592\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.0130510330200195 | KNN Loss: 4.9978108406066895 | BCE Loss: 1.0152404308319092\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.068587779998779 | KNN Loss: 5.034456253051758 | BCE Loss: 1.034131646156311\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.084201335906982 | KNN Loss: 5.037696361541748 | BCE Loss: 1.0465049743652344\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.027687072753906 | KNN Loss: 5.000835418701172 | BCE Loss: 1.0268518924713135\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.020425319671631 | KNN Loss: 5.008389472961426 | BCE Loss: 1.012035846710205\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.068970680236816 | KNN Loss: 5.020045280456543 | BCE Loss: 1.0489253997802734\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.034004211425781 | KNN Loss: 5.027812957763672 | BCE Loss: 1.006191372871399\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.041601181030273 | KNN Loss: 5.015910625457764 | BCE Loss: 1.0256904363632202\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.024416923522949 | KNN Loss: 4.988677501678467 | BCE Loss: 1.0357393026351929\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.029252052307129 | KNN Loss: 5.004091262817383 | BCE Loss: 1.025160789489746\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.054859638214111 | KNN Loss: 5.015527248382568 | BCE Loss: 1.039332389831543\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2265,  3.8615,  2.5736,  2.9090,  2.8241,  0.6991,  2.6035,  1.6462,\n",
      "          2.3869,  2.0602,  2.1964,  2.0600,  0.6111,  1.8932,  1.3220,  1.4583,\n",
      "          2.9208,  2.6505,  2.8708,  2.3284,  1.7294,  3.1018,  2.3613,  2.6003,\n",
      "          1.9957,  1.8294,  2.0026,  1.2849,  1.4417,  0.4059, -0.1320,  1.0266,\n",
      "          0.2938,  0.9174,  1.6701,  1.4917,  1.0858,  3.2999,  0.7243,  1.4092,\n",
      "          0.8491, -0.9957, -0.2098,  2.2923,  2.1144,  0.7912, -0.1759,  0.0579,\n",
      "          1.4154,  1.9824,  1.9156,  0.2031,  1.2618,  0.4512, -0.5825,  1.0971,\n",
      "          1.3924,  1.4737,  1.2272,  1.9045,  0.5839,  0.9296,  0.1225,  1.7981,\n",
      "          1.3353,  1.7348, -1.8844,  0.4145,  2.3761,  2.0298,  2.5776,  0.4516,\n",
      "          1.4844,  2.5514,  1.8596,  1.3033,  0.3155,  0.7763,  0.1432,  1.6417,\n",
      "          0.0722,  0.5226,  1.9932, -0.3415,  0.2397, -1.0558, -2.4648, -0.2253,\n",
      "          0.5830, -1.8698,  0.5544, -0.1350, -0.5117, -0.9409,  0.6519,  1.3184,\n",
      "         -0.6455, -0.7293,  0.3989,  1.1207,  0.5522, -1.2155,  0.9544,  0.9939,\n",
      "         -1.2768, -1.1880, -0.1337,  0.0368, -0.9415, -1.6573, -0.3247, -2.9666,\n",
      "         -0.3451,  1.7694,  1.6686, -0.2881, -0.5617,  0.1027,  1.4871, -2.6136,\n",
      "          0.0907, -0.1112,  0.4473, -0.8124,  0.1153, -0.7696, -0.9495,  0.9662,\n",
      "          0.2578, -0.5430,  0.3683, -0.7620, -1.4391, -0.2963, -0.4374,  0.9573,\n",
      "         -0.5478,  0.2241, -1.9388, -0.9853, -1.3062,  0.6011, -1.9035, -0.9636,\n",
      "         -0.9747, -0.5707, -1.7136, -1.1931, -2.4906, -0.9994, -1.2386, -0.3225,\n",
      "         -1.8880,  0.5027, -1.4628, -0.6174, -3.4429,  0.0991, -0.1950, -0.7208,\n",
      "         -2.2619, -1.7637, -1.1644, -1.3070, -2.4089, -2.4275, -3.4108]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.4429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.8615, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14dd128da0648ec8cb32fb8fa0f5ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].to('cpu') for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 88.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().to('cpu')\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc75db205f54cff989647181743471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc2eb58b435412c9130dd13a662feeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.1, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b1f9ca1e92425c812007ee464c3646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "Epoch: 00 | Batch: 000 / 029 | Total loss: 9.575 | Reg loss: 0.007 | Tree loss: 9.575 | Accuracy: 0.000000 | 0.098 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 029 | Total loss: 9.570 | Reg loss: 0.007 | Tree loss: 9.570 | Accuracy: 0.000000 | 0.086 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 029 | Total loss: 9.548 | Reg loss: 0.007 | Tree loss: 9.548 | Accuracy: 0.000000 | 0.079 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 029 | Total loss: 9.531 | Reg loss: 0.007 | Tree loss: 9.531 | Accuracy: 0.000000 | 0.076 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 029 | Total loss: 9.536 | Reg loss: 0.007 | Tree loss: 9.536 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 029 | Total loss: 9.522 | Reg loss: 0.007 | Tree loss: 9.522 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 029 | Total loss: 9.507 | Reg loss: 0.007 | Tree loss: 9.507 | Accuracy: 0.000000 | 0.072 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 029 | Total loss: 9.487 | Reg loss: 0.007 | Tree loss: 9.487 | Accuracy: 0.000000 | 0.071 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 029 | Total loss: 9.491 | Reg loss: 0.007 | Tree loss: 9.491 | Accuracy: 0.000000 | 0.07 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 029 | Total loss: 9.484 | Reg loss: 0.007 | Tree loss: 9.484 | Accuracy: 0.000000 | 0.07 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 029 | Total loss: 9.473 | Reg loss: 0.007 | Tree loss: 9.473 | Accuracy: 0.001953 | 0.07 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 029 | Total loss: 9.448 | Reg loss: 0.007 | Tree loss: 9.448 | Accuracy: 0.001953 | 0.07 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 029 | Total loss: 9.438 | Reg loss: 0.008 | Tree loss: 9.438 | Accuracy: 0.003906 | 0.07 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 029 | Total loss: 9.432 | Reg loss: 0.008 | Tree loss: 9.432 | Accuracy: 0.009766 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 029 | Total loss: 9.428 | Reg loss: 0.008 | Tree loss: 9.428 | Accuracy: 0.031250 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 029 | Total loss: 9.399 | Reg loss: 0.008 | Tree loss: 9.399 | Accuracy: 0.042969 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 029 | Total loss: 9.396 | Reg loss: 0.009 | Tree loss: 9.396 | Accuracy: 0.060547 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 029 | Total loss: 9.383 | Reg loss: 0.009 | Tree loss: 9.383 | Accuracy: 0.126953 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 029 | Total loss: 9.388 | Reg loss: 0.009 | Tree loss: 9.388 | Accuracy: 0.179688 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 029 | Total loss: 9.359 | Reg loss: 0.009 | Tree loss: 9.359 | Accuracy: 0.259766 | 0.067 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 029 | Total loss: 9.349 | Reg loss: 0.010 | Tree loss: 9.349 | Accuracy: 0.279297 | 0.067 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 029 | Total loss: 9.333 | Reg loss: 0.010 | Tree loss: 9.333 | Accuracy: 0.291016 | 0.067 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 029 | Total loss: 9.321 | Reg loss: 0.010 | Tree loss: 9.321 | Accuracy: 0.306641 | 0.066 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 029 | Total loss: 9.329 | Reg loss: 0.010 | Tree loss: 9.329 | Accuracy: 0.277344 | 0.066 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 029 | Total loss: 9.305 | Reg loss: 0.011 | Tree loss: 9.305 | Accuracy: 0.316406 | 0.066 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 029 | Total loss: 9.304 | Reg loss: 0.011 | Tree loss: 9.304 | Accuracy: 0.296875 | 0.066 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 029 | Total loss: 9.288 | Reg loss: 0.011 | Tree loss: 9.288 | Accuracy: 0.300781 | 0.067 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 029 | Total loss: 9.269 | Reg loss: 0.012 | Tree loss: 9.269 | Accuracy: 0.308594 | 0.067 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 029 | Total loss: 9.289 | Reg loss: 0.012 | Tree loss: 9.289 | Accuracy: 0.272727 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 01 | Batch: 000 / 029 | Total loss: 9.397 | Reg loss: 0.004 | Tree loss: 9.397 | Accuracy: 0.207031 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 029 | Total loss: 9.375 | Reg loss: 0.004 | Tree loss: 9.375 | Accuracy: 0.316406 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 029 | Total loss: 9.367 | Reg loss: 0.004 | Tree loss: 9.367 | Accuracy: 0.304688 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 029 | Total loss: 9.377 | Reg loss: 0.004 | Tree loss: 9.377 | Accuracy: 0.267578 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 029 | Total loss: 9.338 | Reg loss: 0.004 | Tree loss: 9.338 | Accuracy: 0.328125 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 029 | Total loss: 9.338 | Reg loss: 0.005 | Tree loss: 9.338 | Accuracy: 0.328125 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 029 | Total loss: 9.332 | Reg loss: 0.005 | Tree loss: 9.332 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 029 | Total loss: 9.323 | Reg loss: 0.005 | Tree loss: 9.323 | Accuracy: 0.296875 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 029 | Total loss: 9.310 | Reg loss: 0.005 | Tree loss: 9.310 | Accuracy: 0.306641 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 029 | Total loss: 9.315 | Reg loss: 0.006 | Tree loss: 9.315 | Accuracy: 0.257812 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 029 | Total loss: 9.299 | Reg loss: 0.006 | Tree loss: 9.299 | Accuracy: 0.281250 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 029 | Total loss: 9.260 | Reg loss: 0.006 | Tree loss: 9.260 | Accuracy: 0.318359 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 029 | Total loss: 9.282 | Reg loss: 0.007 | Tree loss: 9.282 | Accuracy: 0.236328 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 029 | Total loss: 9.245 | Reg loss: 0.007 | Tree loss: 9.245 | Accuracy: 0.289062 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 029 | Total loss: 9.236 | Reg loss: 0.007 | Tree loss: 9.236 | Accuracy: 0.302734 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 029 | Total loss: 9.235 | Reg loss: 0.008 | Tree loss: 9.235 | Accuracy: 0.273438 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 029 | Total loss: 9.213 | Reg loss: 0.008 | Tree loss: 9.213 | Accuracy: 0.300781 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 029 | Total loss: 9.213 | Reg loss: 0.008 | Tree loss: 9.213 | Accuracy: 0.279297 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 029 | Total loss: 9.201 | Reg loss: 0.009 | Tree loss: 9.201 | Accuracy: 0.298828 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 029 | Total loss: 9.194 | Reg loss: 0.009 | Tree loss: 9.194 | Accuracy: 0.287109 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 029 | Total loss: 9.194 | Reg loss: 0.009 | Tree loss: 9.194 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 029 | Total loss: 9.175 | Reg loss: 0.010 | Tree loss: 9.175 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 029 | Total loss: 9.151 | Reg loss: 0.010 | Tree loss: 9.151 | Accuracy: 0.308594 | 0.065 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 029 | Total loss: 9.132 | Reg loss: 0.010 | Tree loss: 9.132 | Accuracy: 0.320312 | 0.065 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 029 | Total loss: 9.137 | Reg loss: 0.011 | Tree loss: 9.137 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 029 | Total loss: 9.110 | Reg loss: 0.011 | Tree loss: 9.110 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 029 | Total loss: 9.126 | Reg loss: 0.011 | Tree loss: 9.126 | Accuracy: 0.263672 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 029 | Total loss: 9.111 | Reg loss: 0.012 | Tree loss: 9.111 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 029 | Total loss: 9.097 | Reg loss: 0.012 | Tree loss: 9.097 | Accuracy: 0.298701 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 02 | Batch: 000 / 029 | Total loss: 9.234 | Reg loss: 0.006 | Tree loss: 9.234 | Accuracy: 0.283203 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 029 | Total loss: 9.221 | Reg loss: 0.006 | Tree loss: 9.221 | Accuracy: 0.310547 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 029 | Total loss: 9.224 | Reg loss: 0.006 | Tree loss: 9.224 | Accuracy: 0.267578 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 029 | Total loss: 9.194 | Reg loss: 0.006 | Tree loss: 9.194 | Accuracy: 0.314453 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 029 | Total loss: 9.194 | Reg loss: 0.006 | Tree loss: 9.194 | Accuracy: 0.291016 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 029 | Total loss: 9.174 | Reg loss: 0.006 | Tree loss: 9.174 | Accuracy: 0.306641 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 029 | Total loss: 9.175 | Reg loss: 0.007 | Tree loss: 9.175 | Accuracy: 0.285156 | 0.066 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 007 / 029 | Total loss: 9.179 | Reg loss: 0.007 | Tree loss: 9.179 | Accuracy: 0.238281 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 029 | Total loss: 9.143 | Reg loss: 0.007 | Tree loss: 9.143 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 029 | Total loss: 9.134 | Reg loss: 0.007 | Tree loss: 9.134 | Accuracy: 0.271484 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 029 | Total loss: 9.140 | Reg loss: 0.008 | Tree loss: 9.140 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 029 | Total loss: 9.093 | Reg loss: 0.008 | Tree loss: 9.093 | Accuracy: 0.355469 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 029 | Total loss: 9.109 | Reg loss: 0.008 | Tree loss: 9.109 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 029 | Total loss: 9.074 | Reg loss: 0.008 | Tree loss: 9.074 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 029 | Total loss: 9.088 | Reg loss: 0.009 | Tree loss: 9.088 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 029 | Total loss: 9.083 | Reg loss: 0.009 | Tree loss: 9.083 | Accuracy: 0.263672 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 029 | Total loss: 9.036 | Reg loss: 0.009 | Tree loss: 9.036 | Accuracy: 0.347656 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 029 | Total loss: 9.033 | Reg loss: 0.010 | Tree loss: 9.033 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 029 | Total loss: 9.024 | Reg loss: 0.010 | Tree loss: 9.024 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 029 | Total loss: 9.033 | Reg loss: 0.010 | Tree loss: 9.033 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 029 | Total loss: 9.007 | Reg loss: 0.011 | Tree loss: 9.007 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 029 | Total loss: 8.999 | Reg loss: 0.011 | Tree loss: 8.999 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 029 | Total loss: 8.974 | Reg loss: 0.011 | Tree loss: 8.974 | Accuracy: 0.320312 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 029 | Total loss: 8.984 | Reg loss: 0.012 | Tree loss: 8.984 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 029 | Total loss: 8.971 | Reg loss: 0.012 | Tree loss: 8.971 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 029 | Total loss: 8.950 | Reg loss: 0.012 | Tree loss: 8.950 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 029 | Total loss: 8.967 | Reg loss: 0.013 | Tree loss: 8.967 | Accuracy: 0.250000 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 029 | Total loss: 8.947 | Reg loss: 0.013 | Tree loss: 8.947 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 029 | Total loss: 8.902 | Reg loss: 0.013 | Tree loss: 8.902 | Accuracy: 0.298701 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 03 | Batch: 000 / 029 | Total loss: 9.069 | Reg loss: 0.008 | Tree loss: 9.069 | Accuracy: 0.316406 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 029 | Total loss: 9.068 | Reg loss: 0.008 | Tree loss: 9.068 | Accuracy: 0.292969 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 029 | Total loss: 9.057 | Reg loss: 0.008 | Tree loss: 9.057 | Accuracy: 0.287109 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 029 | Total loss: 9.033 | Reg loss: 0.008 | Tree loss: 9.033 | Accuracy: 0.306641 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 029 | Total loss: 9.042 | Reg loss: 0.008 | Tree loss: 9.042 | Accuracy: 0.275391 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 029 | Total loss: 9.023 | Reg loss: 0.008 | Tree loss: 9.023 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 029 | Total loss: 8.992 | Reg loss: 0.008 | Tree loss: 8.992 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 029 | Total loss: 9.002 | Reg loss: 0.009 | Tree loss: 9.002 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 029 | Total loss: 8.981 | Reg loss: 0.009 | Tree loss: 8.981 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 029 | Total loss: 8.972 | Reg loss: 0.009 | Tree loss: 8.972 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 029 | Total loss: 8.939 | Reg loss: 0.009 | Tree loss: 8.939 | Accuracy: 0.322266 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 029 | Total loss: 8.942 | Reg loss: 0.010 | Tree loss: 8.942 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 029 | Total loss: 8.931 | Reg loss: 0.010 | Tree loss: 8.931 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 029 | Total loss: 8.919 | Reg loss: 0.010 | Tree loss: 8.919 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 029 | Total loss: 8.906 | Reg loss: 0.011 | Tree loss: 8.906 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 029 | Total loss: 8.910 | Reg loss: 0.011 | Tree loss: 8.910 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 029 | Total loss: 8.882 | Reg loss: 0.011 | Tree loss: 8.882 | Accuracy: 0.318359 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 029 | Total loss: 8.881 | Reg loss: 0.011 | Tree loss: 8.881 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 029 | Total loss: 8.866 | Reg loss: 0.012 | Tree loss: 8.866 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 029 | Total loss: 8.870 | Reg loss: 0.012 | Tree loss: 8.870 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 029 | Total loss: 8.866 | Reg loss: 0.012 | Tree loss: 8.866 | Accuracy: 0.261719 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 029 | Total loss: 8.856 | Reg loss: 0.013 | Tree loss: 8.856 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 029 | Total loss: 8.822 | Reg loss: 0.013 | Tree loss: 8.822 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 029 | Total loss: 8.820 | Reg loss: 0.013 | Tree loss: 8.820 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 029 | Total loss: 8.807 | Reg loss: 0.014 | Tree loss: 8.807 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 029 | Total loss: 8.778 | Reg loss: 0.014 | Tree loss: 8.778 | Accuracy: 0.316406 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 029 | Total loss: 8.787 | Reg loss: 0.014 | Tree loss: 8.787 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 029 | Total loss: 8.758 | Reg loss: 0.015 | Tree loss: 8.758 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 029 | Total loss: 8.787 | Reg loss: 0.015 | Tree loss: 8.787 | Accuracy: 0.227273 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 04 | Batch: 000 / 029 | Total loss: 8.915 | Reg loss: 0.010 | Tree loss: 8.915 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 029 | Total loss: 8.907 | Reg loss: 0.010 | Tree loss: 8.907 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 029 | Total loss: 8.888 | Reg loss: 0.010 | Tree loss: 8.888 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 029 | Total loss: 8.904 | Reg loss: 0.010 | Tree loss: 8.904 | Accuracy: 0.257812 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 029 | Total loss: 8.861 | Reg loss: 0.010 | Tree loss: 8.861 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 029 | Total loss: 8.857 | Reg loss: 0.010 | Tree loss: 8.857 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 029 | Total loss: 8.825 | Reg loss: 0.010 | Tree loss: 8.825 | Accuracy: 0.316406 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 029 | Total loss: 8.846 | Reg loss: 0.010 | Tree loss: 8.846 | Accuracy: 0.244141 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 029 | Total loss: 8.809 | Reg loss: 0.011 | Tree loss: 8.809 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 029 | Total loss: 8.815 | Reg loss: 0.011 | Tree loss: 8.815 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 029 | Total loss: 8.805 | Reg loss: 0.011 | Tree loss: 8.805 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 029 | Total loss: 8.772 | Reg loss: 0.011 | Tree loss: 8.772 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 029 | Total loss: 8.756 | Reg loss: 0.012 | Tree loss: 8.756 | Accuracy: 0.322266 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 013 / 029 | Total loss: 8.747 | Reg loss: 0.012 | Tree loss: 8.747 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 029 | Total loss: 8.735 | Reg loss: 0.012 | Tree loss: 8.735 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 029 | Total loss: 8.733 | Reg loss: 0.013 | Tree loss: 8.733 | Accuracy: 0.318359 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 029 | Total loss: 8.729 | Reg loss: 0.013 | Tree loss: 8.729 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 029 | Total loss: 8.744 | Reg loss: 0.013 | Tree loss: 8.744 | Accuracy: 0.250000 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 029 | Total loss: 8.720 | Reg loss: 0.013 | Tree loss: 8.720 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 029 | Total loss: 8.696 | Reg loss: 0.014 | Tree loss: 8.696 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 029 | Total loss: 8.695 | Reg loss: 0.014 | Tree loss: 8.695 | Accuracy: 0.263672 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 029 | Total loss: 8.684 | Reg loss: 0.014 | Tree loss: 8.684 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 029 | Total loss: 8.644 | Reg loss: 0.015 | Tree loss: 8.644 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 029 | Total loss: 8.649 | Reg loss: 0.015 | Tree loss: 8.649 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 029 | Total loss: 8.641 | Reg loss: 0.015 | Tree loss: 8.641 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 029 | Total loss: 8.592 | Reg loss: 0.016 | Tree loss: 8.592 | Accuracy: 0.316406 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 029 | Total loss: 8.601 | Reg loss: 0.016 | Tree loss: 8.601 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 029 | Total loss: 8.580 | Reg loss: 0.016 | Tree loss: 8.580 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 029 | Total loss: 8.577 | Reg loss: 0.017 | Tree loss: 8.577 | Accuracy: 0.324675 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 05 | Batch: 000 / 029 | Total loss: 8.743 | Reg loss: 0.011 | Tree loss: 8.743 | Accuracy: 0.330078 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 029 | Total loss: 8.732 | Reg loss: 0.012 | Tree loss: 8.732 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 029 | Total loss: 8.738 | Reg loss: 0.012 | Tree loss: 8.738 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 029 | Total loss: 8.719 | Reg loss: 0.012 | Tree loss: 8.719 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 029 | Total loss: 8.719 | Reg loss: 0.012 | Tree loss: 8.719 | Accuracy: 0.253906 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 029 | Total loss: 8.710 | Reg loss: 0.012 | Tree loss: 8.710 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 029 | Total loss: 8.644 | Reg loss: 0.012 | Tree loss: 8.644 | Accuracy: 0.351562 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 029 | Total loss: 8.675 | Reg loss: 0.012 | Tree loss: 8.675 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 029 | Total loss: 8.650 | Reg loss: 0.013 | Tree loss: 8.650 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 029 | Total loss: 8.653 | Reg loss: 0.013 | Tree loss: 8.653 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 029 | Total loss: 8.630 | Reg loss: 0.013 | Tree loss: 8.630 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 029 | Total loss: 8.614 | Reg loss: 0.013 | Tree loss: 8.614 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 029 | Total loss: 8.596 | Reg loss: 0.014 | Tree loss: 8.596 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 029 | Total loss: 8.574 | Reg loss: 0.014 | Tree loss: 8.574 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 029 | Total loss: 8.576 | Reg loss: 0.014 | Tree loss: 8.576 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 029 | Total loss: 8.583 | Reg loss: 0.015 | Tree loss: 8.583 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 029 | Total loss: 8.573 | Reg loss: 0.015 | Tree loss: 8.573 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 029 | Total loss: 8.516 | Reg loss: 0.015 | Tree loss: 8.516 | Accuracy: 0.318359 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 029 | Total loss: 8.556 | Reg loss: 0.015 | Tree loss: 8.556 | Accuracy: 0.240234 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 029 | Total loss: 8.516 | Reg loss: 0.016 | Tree loss: 8.516 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 029 | Total loss: 8.492 | Reg loss: 0.016 | Tree loss: 8.492 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 029 | Total loss: 8.501 | Reg loss: 0.016 | Tree loss: 8.501 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 029 | Total loss: 8.481 | Reg loss: 0.017 | Tree loss: 8.481 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 029 | Total loss: 8.460 | Reg loss: 0.017 | Tree loss: 8.460 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 029 | Total loss: 8.424 | Reg loss: 0.017 | Tree loss: 8.424 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 029 | Total loss: 8.419 | Reg loss: 0.018 | Tree loss: 8.419 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 029 | Total loss: 8.403 | Reg loss: 0.018 | Tree loss: 8.403 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 029 | Total loss: 8.414 | Reg loss: 0.018 | Tree loss: 8.414 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 029 | Total loss: 8.373 | Reg loss: 0.019 | Tree loss: 8.373 | Accuracy: 0.311688 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 06 | Batch: 000 / 029 | Total loss: 8.577 | Reg loss: 0.013 | Tree loss: 8.577 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 029 | Total loss: 8.572 | Reg loss: 0.014 | Tree loss: 8.572 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 029 | Total loss: 8.570 | Reg loss: 0.014 | Tree loss: 8.570 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 029 | Total loss: 8.533 | Reg loss: 0.014 | Tree loss: 8.533 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 029 | Total loss: 8.547 | Reg loss: 0.014 | Tree loss: 8.547 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 029 | Total loss: 8.515 | Reg loss: 0.014 | Tree loss: 8.515 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 029 | Total loss: 8.529 | Reg loss: 0.014 | Tree loss: 8.529 | Accuracy: 0.271484 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 029 | Total loss: 8.517 | Reg loss: 0.014 | Tree loss: 8.517 | Accuracy: 0.261719 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 029 | Total loss: 8.499 | Reg loss: 0.015 | Tree loss: 8.499 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 029 | Total loss: 8.466 | Reg loss: 0.015 | Tree loss: 8.466 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 029 | Total loss: 8.423 | Reg loss: 0.015 | Tree loss: 8.423 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 029 | Total loss: 8.452 | Reg loss: 0.015 | Tree loss: 8.452 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 029 | Total loss: 8.400 | Reg loss: 0.016 | Tree loss: 8.400 | Accuracy: 0.326172 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 029 | Total loss: 8.426 | Reg loss: 0.016 | Tree loss: 8.426 | Accuracy: 0.277344 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 029 | Total loss: 8.385 | Reg loss: 0.016 | Tree loss: 8.385 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 029 | Total loss: 8.397 | Reg loss: 0.017 | Tree loss: 8.397 | Accuracy: 0.263672 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 029 | Total loss: 8.358 | Reg loss: 0.017 | Tree loss: 8.358 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 029 | Total loss: 8.355 | Reg loss: 0.017 | Tree loss: 8.355 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 029 | Total loss: 8.322 | Reg loss: 0.018 | Tree loss: 8.322 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 029 | Total loss: 8.321 | Reg loss: 0.018 | Tree loss: 8.321 | Accuracy: 0.291016 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 020 / 029 | Total loss: 8.309 | Reg loss: 0.018 | Tree loss: 8.309 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 029 | Total loss: 8.285 | Reg loss: 0.019 | Tree loss: 8.285 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 029 | Total loss: 8.269 | Reg loss: 0.019 | Tree loss: 8.269 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 029 | Total loss: 8.266 | Reg loss: 0.019 | Tree loss: 8.266 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 029 | Total loss: 8.240 | Reg loss: 0.020 | Tree loss: 8.240 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 029 | Total loss: 8.235 | Reg loss: 0.020 | Tree loss: 8.235 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 029 | Total loss: 8.226 | Reg loss: 0.020 | Tree loss: 8.226 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 029 | Total loss: 8.193 | Reg loss: 0.021 | Tree loss: 8.193 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 029 | Total loss: 8.207 | Reg loss: 0.021 | Tree loss: 8.207 | Accuracy: 0.279221 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 07 | Batch: 000 / 029 | Total loss: 8.412 | Reg loss: 0.015 | Tree loss: 8.412 | Accuracy: 0.308594 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 029 | Total loss: 8.398 | Reg loss: 0.016 | Tree loss: 8.398 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 029 | Total loss: 8.401 | Reg loss: 0.016 | Tree loss: 8.401 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 029 | Total loss: 8.359 | Reg loss: 0.016 | Tree loss: 8.359 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 029 | Total loss: 8.350 | Reg loss: 0.016 | Tree loss: 8.350 | Accuracy: 0.320312 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 029 | Total loss: 8.372 | Reg loss: 0.016 | Tree loss: 8.372 | Accuracy: 0.255859 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 029 | Total loss: 8.321 | Reg loss: 0.016 | Tree loss: 8.321 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 029 | Total loss: 8.303 | Reg loss: 0.016 | Tree loss: 8.303 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 029 | Total loss: 8.319 | Reg loss: 0.017 | Tree loss: 8.319 | Accuracy: 0.277344 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 029 | Total loss: 8.272 | Reg loss: 0.017 | Tree loss: 8.272 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 029 | Total loss: 8.306 | Reg loss: 0.017 | Tree loss: 8.306 | Accuracy: 0.251953 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 029 | Total loss: 8.256 | Reg loss: 0.017 | Tree loss: 8.256 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 029 | Total loss: 8.215 | Reg loss: 0.018 | Tree loss: 8.215 | Accuracy: 0.318359 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 029 | Total loss: 8.207 | Reg loss: 0.018 | Tree loss: 8.207 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 029 | Total loss: 8.209 | Reg loss: 0.018 | Tree loss: 8.209 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 029 | Total loss: 8.175 | Reg loss: 0.019 | Tree loss: 8.175 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 029 | Total loss: 8.163 | Reg loss: 0.019 | Tree loss: 8.163 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 029 | Total loss: 8.153 | Reg loss: 0.019 | Tree loss: 8.153 | Accuracy: 0.277344 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 029 | Total loss: 8.120 | Reg loss: 0.020 | Tree loss: 8.120 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 029 | Total loss: 8.147 | Reg loss: 0.020 | Tree loss: 8.147 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 029 | Total loss: 8.101 | Reg loss: 0.020 | Tree loss: 8.101 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 029 | Total loss: 8.057 | Reg loss: 0.021 | Tree loss: 8.057 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 029 | Total loss: 8.083 | Reg loss: 0.021 | Tree loss: 8.083 | Accuracy: 0.263672 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 029 | Total loss: 8.041 | Reg loss: 0.021 | Tree loss: 8.041 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 029 | Total loss: 8.034 | Reg loss: 0.022 | Tree loss: 8.034 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 029 | Total loss: 8.018 | Reg loss: 0.022 | Tree loss: 8.018 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 029 | Total loss: 7.988 | Reg loss: 0.022 | Tree loss: 7.988 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 029 | Total loss: 8.007 | Reg loss: 0.023 | Tree loss: 8.007 | Accuracy: 0.257812 | 0.065 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 029 | Total loss: 8.006 | Reg loss: 0.023 | Tree loss: 8.006 | Accuracy: 0.285714 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 08 | Batch: 000 / 029 | Total loss: 8.243 | Reg loss: 0.017 | Tree loss: 8.243 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 029 | Total loss: 8.229 | Reg loss: 0.017 | Tree loss: 8.229 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 029 | Total loss: 8.217 | Reg loss: 0.018 | Tree loss: 8.217 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 029 | Total loss: 8.185 | Reg loss: 0.018 | Tree loss: 8.185 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 029 | Total loss: 8.175 | Reg loss: 0.018 | Tree loss: 8.175 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 029 | Total loss: 8.182 | Reg loss: 0.018 | Tree loss: 8.182 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 029 | Total loss: 8.147 | Reg loss: 0.018 | Tree loss: 8.147 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 029 | Total loss: 8.097 | Reg loss: 0.018 | Tree loss: 8.097 | Accuracy: 0.308594 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 029 | Total loss: 8.110 | Reg loss: 0.019 | Tree loss: 8.110 | Accuracy: 0.271484 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 029 | Total loss: 8.085 | Reg loss: 0.019 | Tree loss: 8.085 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 029 | Total loss: 8.099 | Reg loss: 0.019 | Tree loss: 8.099 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 029 | Total loss: 8.059 | Reg loss: 0.019 | Tree loss: 8.059 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 029 | Total loss: 8.057 | Reg loss: 0.020 | Tree loss: 8.057 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 029 | Total loss: 7.990 | Reg loss: 0.020 | Tree loss: 7.990 | Accuracy: 0.333984 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 029 | Total loss: 8.012 | Reg loss: 0.020 | Tree loss: 8.012 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 029 | Total loss: 7.979 | Reg loss: 0.020 | Tree loss: 7.979 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 029 | Total loss: 7.948 | Reg loss: 0.021 | Tree loss: 7.948 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 029 | Total loss: 7.930 | Reg loss: 0.021 | Tree loss: 7.930 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 029 | Total loss: 7.960 | Reg loss: 0.021 | Tree loss: 7.960 | Accuracy: 0.271484 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 029 | Total loss: 7.947 | Reg loss: 0.022 | Tree loss: 7.947 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 029 | Total loss: 7.909 | Reg loss: 0.022 | Tree loss: 7.909 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 029 | Total loss: 7.820 | Reg loss: 0.022 | Tree loss: 7.820 | Accuracy: 0.339844 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 029 | Total loss: 7.802 | Reg loss: 0.023 | Tree loss: 7.802 | Accuracy: 0.335938 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 029 | Total loss: 7.892 | Reg loss: 0.023 | Tree loss: 7.892 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 029 | Total loss: 7.808 | Reg loss: 0.024 | Tree loss: 7.808 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 029 | Total loss: 7.792 | Reg loss: 0.024 | Tree loss: 7.792 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 029 | Total loss: 7.797 | Reg loss: 0.024 | Tree loss: 7.797 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 029 | Total loss: 7.770 | Reg loss: 0.025 | Tree loss: 7.770 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 029 | Total loss: 7.803 | Reg loss: 0.025 | Tree loss: 7.803 | Accuracy: 0.253247 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 09 | Batch: 000 / 029 | Total loss: 8.103 | Reg loss: 0.019 | Tree loss: 8.103 | Accuracy: 0.267578 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 029 | Total loss: 8.030 | Reg loss: 0.019 | Tree loss: 8.030 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 029 | Total loss: 8.023 | Reg loss: 0.019 | Tree loss: 8.023 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 029 | Total loss: 8.029 | Reg loss: 0.020 | Tree loss: 8.029 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 029 | Total loss: 7.997 | Reg loss: 0.020 | Tree loss: 7.997 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 029 | Total loss: 7.975 | Reg loss: 0.020 | Tree loss: 7.975 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 029 | Total loss: 7.964 | Reg loss: 0.020 | Tree loss: 7.964 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 029 | Total loss: 7.944 | Reg loss: 0.020 | Tree loss: 7.944 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 029 | Total loss: 7.911 | Reg loss: 0.020 | Tree loss: 7.911 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 029 | Total loss: 7.905 | Reg loss: 0.021 | Tree loss: 7.905 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 029 | Total loss: 7.839 | Reg loss: 0.021 | Tree loss: 7.839 | Accuracy: 0.324219 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 029 | Total loss: 7.844 | Reg loss: 0.021 | Tree loss: 7.844 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 029 | Total loss: 7.820 | Reg loss: 0.021 | Tree loss: 7.820 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 029 | Total loss: 7.831 | Reg loss: 0.022 | Tree loss: 7.831 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 029 | Total loss: 7.792 | Reg loss: 0.022 | Tree loss: 7.792 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 029 | Total loss: 7.793 | Reg loss: 0.022 | Tree loss: 7.793 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 029 | Total loss: 7.805 | Reg loss: 0.022 | Tree loss: 7.805 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 029 | Total loss: 7.763 | Reg loss: 0.023 | Tree loss: 7.763 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 029 | Total loss: 7.726 | Reg loss: 0.023 | Tree loss: 7.726 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 029 | Total loss: 7.709 | Reg loss: 0.023 | Tree loss: 7.709 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 029 | Total loss: 7.640 | Reg loss: 0.024 | Tree loss: 7.640 | Accuracy: 0.333984 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 029 | Total loss: 7.658 | Reg loss: 0.024 | Tree loss: 7.658 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 029 | Total loss: 7.632 | Reg loss: 0.024 | Tree loss: 7.632 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 029 | Total loss: 7.657 | Reg loss: 0.025 | Tree loss: 7.657 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 029 | Total loss: 7.603 | Reg loss: 0.025 | Tree loss: 7.603 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 029 | Total loss: 7.598 | Reg loss: 0.025 | Tree loss: 7.598 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 029 | Total loss: 7.608 | Reg loss: 0.026 | Tree loss: 7.608 | Accuracy: 0.253906 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 029 | Total loss: 7.478 | Reg loss: 0.026 | Tree loss: 7.478 | Accuracy: 0.332031 | 0.065 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 029 | Total loss: 7.570 | Reg loss: 0.026 | Tree loss: 7.570 | Accuracy: 0.266234 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 10 | Batch: 000 / 029 | Total loss: 7.891 | Reg loss: 0.021 | Tree loss: 7.891 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 029 | Total loss: 7.856 | Reg loss: 0.021 | Tree loss: 7.856 | Accuracy: 0.332031 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 029 | Total loss: 7.862 | Reg loss: 0.021 | Tree loss: 7.862 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 029 | Total loss: 7.803 | Reg loss: 0.021 | Tree loss: 7.803 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 029 | Total loss: 7.814 | Reg loss: 0.021 | Tree loss: 7.814 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 029 | Total loss: 7.797 | Reg loss: 0.021 | Tree loss: 7.797 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 029 | Total loss: 7.732 | Reg loss: 0.022 | Tree loss: 7.732 | Accuracy: 0.324219 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 029 | Total loss: 7.768 | Reg loss: 0.022 | Tree loss: 7.768 | Accuracy: 0.261719 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 029 | Total loss: 7.713 | Reg loss: 0.022 | Tree loss: 7.713 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 029 | Total loss: 7.723 | Reg loss: 0.022 | Tree loss: 7.723 | Accuracy: 0.261719 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 029 | Total loss: 7.698 | Reg loss: 0.022 | Tree loss: 7.698 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 029 | Total loss: 7.716 | Reg loss: 0.023 | Tree loss: 7.716 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 029 | Total loss: 7.638 | Reg loss: 0.023 | Tree loss: 7.638 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 029 | Total loss: 7.659 | Reg loss: 0.023 | Tree loss: 7.659 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 029 | Total loss: 7.561 | Reg loss: 0.023 | Tree loss: 7.561 | Accuracy: 0.341797 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 029 | Total loss: 7.577 | Reg loss: 0.024 | Tree loss: 7.577 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 029 | Total loss: 7.529 | Reg loss: 0.024 | Tree loss: 7.529 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 029 | Total loss: 7.555 | Reg loss: 0.024 | Tree loss: 7.555 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 029 | Total loss: 7.476 | Reg loss: 0.025 | Tree loss: 7.476 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 029 | Total loss: 7.475 | Reg loss: 0.025 | Tree loss: 7.475 | Accuracy: 0.330078 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 029 | Total loss: 7.493 | Reg loss: 0.025 | Tree loss: 7.493 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 029 | Total loss: 7.492 | Reg loss: 0.026 | Tree loss: 7.492 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 029 | Total loss: 7.470 | Reg loss: 0.026 | Tree loss: 7.470 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 029 | Total loss: 7.413 | Reg loss: 0.026 | Tree loss: 7.413 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 029 | Total loss: 7.399 | Reg loss: 0.026 | Tree loss: 7.399 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 029 | Total loss: 7.379 | Reg loss: 0.027 | Tree loss: 7.379 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 029 | Total loss: 7.374 | Reg loss: 0.027 | Tree loss: 7.374 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 029 | Total loss: 7.336 | Reg loss: 0.027 | Tree loss: 7.336 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 029 | Total loss: 7.249 | Reg loss: 0.028 | Tree loss: 7.249 | Accuracy: 0.324675 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 11 | Batch: 000 / 029 | Total loss: 7.712 | Reg loss: 0.023 | Tree loss: 7.712 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 029 | Total loss: 7.701 | Reg loss: 0.023 | Tree loss: 7.701 | Accuracy: 0.267578 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 029 | Total loss: 7.656 | Reg loss: 0.023 | Tree loss: 7.656 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 029 | Total loss: 7.683 | Reg loss: 0.023 | Tree loss: 7.683 | Accuracy: 0.273438 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batch: 004 / 029 | Total loss: 7.644 | Reg loss: 0.023 | Tree loss: 7.644 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 029 | Total loss: 7.589 | Reg loss: 0.023 | Tree loss: 7.589 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 029 | Total loss: 7.598 | Reg loss: 0.023 | Tree loss: 7.598 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 029 | Total loss: 7.530 | Reg loss: 0.023 | Tree loss: 7.530 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 029 | Total loss: 7.551 | Reg loss: 0.023 | Tree loss: 7.551 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 029 | Total loss: 7.458 | Reg loss: 0.024 | Tree loss: 7.458 | Accuracy: 0.335938 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 029 | Total loss: 7.536 | Reg loss: 0.024 | Tree loss: 7.536 | Accuracy: 0.253906 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 029 | Total loss: 7.469 | Reg loss: 0.024 | Tree loss: 7.469 | Accuracy: 0.322266 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 029 | Total loss: 7.455 | Reg loss: 0.024 | Tree loss: 7.455 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 029 | Total loss: 7.433 | Reg loss: 0.025 | Tree loss: 7.433 | Accuracy: 0.308594 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 029 | Total loss: 7.433 | Reg loss: 0.025 | Tree loss: 7.433 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 029 | Total loss: 7.328 | Reg loss: 0.025 | Tree loss: 7.328 | Accuracy: 0.328125 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 029 | Total loss: 7.353 | Reg loss: 0.025 | Tree loss: 7.353 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 029 | Total loss: 7.342 | Reg loss: 0.026 | Tree loss: 7.342 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 029 | Total loss: 7.384 | Reg loss: 0.026 | Tree loss: 7.384 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 029 | Total loss: 7.358 | Reg loss: 0.026 | Tree loss: 7.358 | Accuracy: 0.253906 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 029 | Total loss: 7.272 | Reg loss: 0.027 | Tree loss: 7.272 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 029 | Total loss: 7.270 | Reg loss: 0.027 | Tree loss: 7.270 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 029 | Total loss: 7.226 | Reg loss: 0.027 | Tree loss: 7.226 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 029 | Total loss: 7.214 | Reg loss: 0.027 | Tree loss: 7.214 | Accuracy: 0.308594 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 029 | Total loss: 7.158 | Reg loss: 0.028 | Tree loss: 7.158 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 029 | Total loss: 7.208 | Reg loss: 0.028 | Tree loss: 7.208 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 029 | Total loss: 7.113 | Reg loss: 0.028 | Tree loss: 7.113 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 029 | Total loss: 7.105 | Reg loss: 0.029 | Tree loss: 7.105 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 029 | Total loss: 6.976 | Reg loss: 0.029 | Tree loss: 6.976 | Accuracy: 0.324675 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 12 | Batch: 000 / 029 | Total loss: 7.541 | Reg loss: 0.024 | Tree loss: 7.541 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 029 | Total loss: 7.491 | Reg loss: 0.024 | Tree loss: 7.491 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 029 | Total loss: 7.477 | Reg loss: 0.024 | Tree loss: 7.477 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 029 | Total loss: 7.438 | Reg loss: 0.024 | Tree loss: 7.438 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 029 | Total loss: 7.395 | Reg loss: 0.024 | Tree loss: 7.395 | Accuracy: 0.330078 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 029 | Total loss: 7.417 | Reg loss: 0.024 | Tree loss: 7.417 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 029 | Total loss: 7.387 | Reg loss: 0.025 | Tree loss: 7.387 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 029 | Total loss: 7.368 | Reg loss: 0.025 | Tree loss: 7.368 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 029 | Total loss: 7.367 | Reg loss: 0.025 | Tree loss: 7.367 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 029 | Total loss: 7.296 | Reg loss: 0.025 | Tree loss: 7.296 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 029 | Total loss: 7.286 | Reg loss: 0.025 | Tree loss: 7.286 | Accuracy: 0.316406 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 029 | Total loss: 7.296 | Reg loss: 0.025 | Tree loss: 7.296 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 029 | Total loss: 7.293 | Reg loss: 0.026 | Tree loss: 7.293 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 029 | Total loss: 7.227 | Reg loss: 0.026 | Tree loss: 7.227 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 029 | Total loss: 7.264 | Reg loss: 0.026 | Tree loss: 7.264 | Accuracy: 0.267578 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 029 | Total loss: 7.217 | Reg loss: 0.026 | Tree loss: 7.217 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 029 | Total loss: 7.143 | Reg loss: 0.027 | Tree loss: 7.143 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 029 | Total loss: 7.142 | Reg loss: 0.027 | Tree loss: 7.142 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 029 | Total loss: 7.128 | Reg loss: 0.027 | Tree loss: 7.128 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 029 | Total loss: 7.164 | Reg loss: 0.028 | Tree loss: 7.164 | Accuracy: 0.253906 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 029 | Total loss: 7.131 | Reg loss: 0.028 | Tree loss: 7.131 | Accuracy: 0.250000 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 029 | Total loss: 7.055 | Reg loss: 0.028 | Tree loss: 7.055 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 029 | Total loss: 7.013 | Reg loss: 0.028 | Tree loss: 7.013 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 029 | Total loss: 6.987 | Reg loss: 0.029 | Tree loss: 6.987 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 029 | Total loss: 7.006 | Reg loss: 0.029 | Tree loss: 7.006 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 029 | Total loss: 6.941 | Reg loss: 0.029 | Tree loss: 6.941 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 029 | Total loss: 6.948 | Reg loss: 0.030 | Tree loss: 6.948 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 029 | Total loss: 6.958 | Reg loss: 0.030 | Tree loss: 6.958 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 029 | Total loss: 6.985 | Reg loss: 0.030 | Tree loss: 6.985 | Accuracy: 0.253247 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 13 | Batch: 000 / 029 | Total loss: 7.335 | Reg loss: 0.025 | Tree loss: 7.335 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 029 | Total loss: 7.311 | Reg loss: 0.026 | Tree loss: 7.311 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 029 | Total loss: 7.311 | Reg loss: 0.026 | Tree loss: 7.311 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 029 | Total loss: 7.288 | Reg loss: 0.026 | Tree loss: 7.288 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 029 | Total loss: 7.258 | Reg loss: 0.026 | Tree loss: 7.258 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 029 | Total loss: 7.254 | Reg loss: 0.026 | Tree loss: 7.254 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 029 | Total loss: 7.183 | Reg loss: 0.026 | Tree loss: 7.183 | Accuracy: 0.316406 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 029 | Total loss: 7.186 | Reg loss: 0.026 | Tree loss: 7.186 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 029 | Total loss: 7.140 | Reg loss: 0.026 | Tree loss: 7.140 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 029 | Total loss: 7.166 | Reg loss: 0.027 | Tree loss: 7.166 | Accuracy: 0.275391 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 010 / 029 | Total loss: 7.113 | Reg loss: 0.027 | Tree loss: 7.113 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 029 | Total loss: 7.048 | Reg loss: 0.027 | Tree loss: 7.048 | Accuracy: 0.316406 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 029 | Total loss: 7.005 | Reg loss: 0.027 | Tree loss: 7.005 | Accuracy: 0.357422 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 029 | Total loss: 7.051 | Reg loss: 0.027 | Tree loss: 7.051 | Accuracy: 0.316406 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 029 | Total loss: 7.042 | Reg loss: 0.028 | Tree loss: 7.042 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 029 | Total loss: 7.039 | Reg loss: 0.028 | Tree loss: 7.039 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 029 | Total loss: 6.965 | Reg loss: 0.028 | Tree loss: 6.965 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 029 | Total loss: 7.004 | Reg loss: 0.028 | Tree loss: 7.004 | Accuracy: 0.257812 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 029 | Total loss: 6.951 | Reg loss: 0.029 | Tree loss: 6.951 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 029 | Total loss: 6.942 | Reg loss: 0.029 | Tree loss: 6.942 | Accuracy: 0.269531 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 029 | Total loss: 6.905 | Reg loss: 0.029 | Tree loss: 6.905 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 029 | Total loss: 6.826 | Reg loss: 0.030 | Tree loss: 6.826 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 029 | Total loss: 6.858 | Reg loss: 0.030 | Tree loss: 6.858 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 029 | Total loss: 6.819 | Reg loss: 0.030 | Tree loss: 6.819 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 029 | Total loss: 6.801 | Reg loss: 0.030 | Tree loss: 6.801 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 029 | Total loss: 6.779 | Reg loss: 0.031 | Tree loss: 6.779 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 029 | Total loss: 6.815 | Reg loss: 0.031 | Tree loss: 6.815 | Accuracy: 0.261719 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 029 | Total loss: 6.772 | Reg loss: 0.031 | Tree loss: 6.772 | Accuracy: 0.263672 | 0.065 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 029 | Total loss: 6.680 | Reg loss: 0.032 | Tree loss: 6.680 | Accuracy: 0.292208 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 14 | Batch: 000 / 029 | Total loss: 7.135 | Reg loss: 0.027 | Tree loss: 7.135 | Accuracy: 0.308594 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 029 | Total loss: 7.140 | Reg loss: 0.027 | Tree loss: 7.140 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 029 | Total loss: 7.107 | Reg loss: 0.027 | Tree loss: 7.107 | Accuracy: 0.306641 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 029 | Total loss: 7.091 | Reg loss: 0.027 | Tree loss: 7.091 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 029 | Total loss: 7.065 | Reg loss: 0.027 | Tree loss: 7.065 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 029 | Total loss: 7.035 | Reg loss: 0.027 | Tree loss: 7.035 | Accuracy: 0.332031 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 029 | Total loss: 7.016 | Reg loss: 0.027 | Tree loss: 7.016 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 029 | Total loss: 7.048 | Reg loss: 0.028 | Tree loss: 7.048 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 029 | Total loss: 6.966 | Reg loss: 0.028 | Tree loss: 6.966 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 029 | Total loss: 6.967 | Reg loss: 0.028 | Tree loss: 6.967 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 029 | Total loss: 6.978 | Reg loss: 0.028 | Tree loss: 6.978 | Accuracy: 0.259766 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 029 | Total loss: 6.915 | Reg loss: 0.028 | Tree loss: 6.915 | Accuracy: 0.304688 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 029 | Total loss: 6.895 | Reg loss: 0.028 | Tree loss: 6.895 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 029 | Total loss: 6.839 | Reg loss: 0.029 | Tree loss: 6.839 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 029 | Total loss: 6.859 | Reg loss: 0.029 | Tree loss: 6.859 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 029 | Total loss: 6.853 | Reg loss: 0.029 | Tree loss: 6.853 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 029 | Total loss: 6.769 | Reg loss: 0.029 | Tree loss: 6.769 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 029 | Total loss: 6.791 | Reg loss: 0.030 | Tree loss: 6.791 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 029 | Total loss: 6.740 | Reg loss: 0.030 | Tree loss: 6.740 | Accuracy: 0.277344 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 029 | Total loss: 6.756 | Reg loss: 0.030 | Tree loss: 6.756 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 029 | Total loss: 6.737 | Reg loss: 0.030 | Tree loss: 6.737 | Accuracy: 0.263672 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 029 | Total loss: 6.650 | Reg loss: 0.031 | Tree loss: 6.650 | Accuracy: 0.330078 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 029 | Total loss: 6.601 | Reg loss: 0.031 | Tree loss: 6.601 | Accuracy: 0.322266 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 029 | Total loss: 6.672 | Reg loss: 0.031 | Tree loss: 6.672 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 029 | Total loss: 6.599 | Reg loss: 0.032 | Tree loss: 6.599 | Accuracy: 0.289062 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 029 | Total loss: 6.593 | Reg loss: 0.032 | Tree loss: 6.593 | Accuracy: 0.300781 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 029 | Total loss: 6.571 | Reg loss: 0.032 | Tree loss: 6.571 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 029 | Total loss: 6.540 | Reg loss: 0.032 | Tree loss: 6.540 | Accuracy: 0.294922 | 0.065 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 029 | Total loss: 6.489 | Reg loss: 0.033 | Tree loss: 6.489 | Accuracy: 0.324675 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 15 | Batch: 000 / 029 | Total loss: 6.940 | Reg loss: 0.028 | Tree loss: 6.940 | Accuracy: 0.318359 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 029 | Total loss: 6.961 | Reg loss: 0.028 | Tree loss: 6.961 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 029 | Total loss: 6.906 | Reg loss: 0.028 | Tree loss: 6.906 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 029 | Total loss: 6.938 | Reg loss: 0.028 | Tree loss: 6.938 | Accuracy: 0.265625 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 029 | Total loss: 6.878 | Reg loss: 0.028 | Tree loss: 6.878 | Accuracy: 0.328125 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 029 | Total loss: 6.872 | Reg loss: 0.029 | Tree loss: 6.872 | Accuracy: 0.298828 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 029 | Total loss: 6.857 | Reg loss: 0.029 | Tree loss: 6.857 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 029 | Total loss: 6.812 | Reg loss: 0.029 | Tree loss: 6.812 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 029 | Total loss: 6.798 | Reg loss: 0.029 | Tree loss: 6.798 | Accuracy: 0.291016 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 029 | Total loss: 6.782 | Reg loss: 0.029 | Tree loss: 6.782 | Accuracy: 0.296875 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 029 | Total loss: 6.706 | Reg loss: 0.029 | Tree loss: 6.706 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 029 | Total loss: 6.739 | Reg loss: 0.030 | Tree loss: 6.739 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 029 | Total loss: 6.745 | Reg loss: 0.030 | Tree loss: 6.745 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 029 | Total loss: 6.696 | Reg loss: 0.030 | Tree loss: 6.696 | Accuracy: 0.275391 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 029 | Total loss: 6.637 | Reg loss: 0.030 | Tree loss: 6.637 | Accuracy: 0.302734 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 029 | Total loss: 6.680 | Reg loss: 0.030 | Tree loss: 6.680 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 029 | Total loss: 6.607 | Reg loss: 0.031 | Tree loss: 6.607 | Accuracy: 0.289062 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 017 / 029 | Total loss: 6.616 | Reg loss: 0.031 | Tree loss: 6.616 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 029 | Total loss: 6.590 | Reg loss: 0.031 | Tree loss: 6.590 | Accuracy: 0.263672 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 029 | Total loss: 6.518 | Reg loss: 0.031 | Tree loss: 6.518 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 029 | Total loss: 6.546 | Reg loss: 0.032 | Tree loss: 6.546 | Accuracy: 0.285156 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 029 | Total loss: 6.489 | Reg loss: 0.032 | Tree loss: 6.489 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 029 | Total loss: 6.439 | Reg loss: 0.032 | Tree loss: 6.439 | Accuracy: 0.312500 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 029 | Total loss: 6.458 | Reg loss: 0.032 | Tree loss: 6.458 | Accuracy: 0.281250 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 029 | Total loss: 6.446 | Reg loss: 0.033 | Tree loss: 6.446 | Accuracy: 0.273438 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 029 | Total loss: 6.405 | Reg loss: 0.033 | Tree loss: 6.405 | Accuracy: 0.271484 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 029 | Total loss: 6.358 | Reg loss: 0.033 | Tree loss: 6.358 | Accuracy: 0.320312 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 029 | Total loss: 6.408 | Reg loss: 0.033 | Tree loss: 6.408 | Accuracy: 0.261719 | 0.065 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 029 | Total loss: 6.170 | Reg loss: 0.034 | Tree loss: 6.170 | Accuracy: 0.363636 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 16 | Batch: 000 / 029 | Total loss: 6.843 | Reg loss: 0.029 | Tree loss: 6.843 | Accuracy: 0.279297 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 029 | Total loss: 6.799 | Reg loss: 0.029 | Tree loss: 6.799 | Accuracy: 0.287109 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 029 | Total loss: 6.695 | Reg loss: 0.029 | Tree loss: 6.695 | Accuracy: 0.328125 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 029 | Total loss: 6.764 | Reg loss: 0.030 | Tree loss: 6.764 | Accuracy: 0.277344 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 029 | Total loss: 6.707 | Reg loss: 0.030 | Tree loss: 6.707 | Accuracy: 0.292969 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 029 | Total loss: 6.665 | Reg loss: 0.030 | Tree loss: 6.665 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 029 | Total loss: 6.700 | Reg loss: 0.030 | Tree loss: 6.700 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 029 | Total loss: 6.636 | Reg loss: 0.030 | Tree loss: 6.636 | Accuracy: 0.314453 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 029 | Total loss: 6.605 | Reg loss: 0.030 | Tree loss: 6.605 | Accuracy: 0.318359 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 029 | Total loss: 6.555 | Reg loss: 0.030 | Tree loss: 6.555 | Accuracy: 0.310547 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 029 | Total loss: 6.622 | Reg loss: 0.030 | Tree loss: 6.622 | Accuracy: 0.283203 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 029 | Total loss: 6.575 | Reg loss: 0.031 | Tree loss: 6.575 | Accuracy: 0.267578 | 0.065 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 029 | Total loss: 6.513 | Reg loss: 0.031 | Tree loss: 6.513 | Accuracy: 0.277344 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 029 | Total loss: 6.554 | Reg loss: 0.031 | Tree loss: 6.554 | Accuracy: 0.255859 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 029 | Total loss: 6.438 | Reg loss: 0.031 | Tree loss: 6.438 | Accuracy: 0.304688 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 029 | Total loss: 6.455 | Reg loss: 0.031 | Tree loss: 6.455 | Accuracy: 0.294922 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 029 | Total loss: 6.483 | Reg loss: 0.032 | Tree loss: 6.483 | Accuracy: 0.253906 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 029 | Total loss: 6.408 | Reg loss: 0.032 | Tree loss: 6.408 | Accuracy: 0.291016 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 029 | Total loss: 6.403 | Reg loss: 0.032 | Tree loss: 6.403 | Accuracy: 0.279297 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 029 | Total loss: 6.333 | Reg loss: 0.032 | Tree loss: 6.333 | Accuracy: 0.314453 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 029 | Total loss: 6.305 | Reg loss: 0.033 | Tree loss: 6.305 | Accuracy: 0.308594 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 029 | Total loss: 6.355 | Reg loss: 0.033 | Tree loss: 6.355 | Accuracy: 0.277344 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 029 | Total loss: 6.304 | Reg loss: 0.033 | Tree loss: 6.304 | Accuracy: 0.285156 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 029 | Total loss: 6.253 | Reg loss: 0.033 | Tree loss: 6.253 | Accuracy: 0.291016 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 029 | Total loss: 6.196 | Reg loss: 0.034 | Tree loss: 6.196 | Accuracy: 0.312500 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 029 | Total loss: 6.224 | Reg loss: 0.034 | Tree loss: 6.224 | Accuracy: 0.296875 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 029 | Total loss: 6.175 | Reg loss: 0.034 | Tree loss: 6.175 | Accuracy: 0.302734 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 029 | Total loss: 6.173 | Reg loss: 0.034 | Tree loss: 6.173 | Accuracy: 0.304688 | 0.066 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 029 | Total loss: 6.213 | Reg loss: 0.035 | Tree loss: 6.213 | Accuracy: 0.253247 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 17 | Batch: 000 / 029 | Total loss: 6.647 | Reg loss: 0.031 | Tree loss: 6.647 | Accuracy: 0.298828 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 029 | Total loss: 6.619 | Reg loss: 0.031 | Tree loss: 6.619 | Accuracy: 0.275391 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 029 | Total loss: 6.616 | Reg loss: 0.031 | Tree loss: 6.616 | Accuracy: 0.287109 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 029 | Total loss: 6.560 | Reg loss: 0.031 | Tree loss: 6.560 | Accuracy: 0.316406 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 029 | Total loss: 6.517 | Reg loss: 0.031 | Tree loss: 6.517 | Accuracy: 0.312500 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 029 | Total loss: 6.488 | Reg loss: 0.031 | Tree loss: 6.488 | Accuracy: 0.316406 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 029 | Total loss: 6.485 | Reg loss: 0.031 | Tree loss: 6.485 | Accuracy: 0.294922 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 029 | Total loss: 6.420 | Reg loss: 0.031 | Tree loss: 6.420 | Accuracy: 0.333984 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 029 | Total loss: 6.455 | Reg loss: 0.031 | Tree loss: 6.455 | Accuracy: 0.283203 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 029 | Total loss: 6.396 | Reg loss: 0.031 | Tree loss: 6.396 | Accuracy: 0.292969 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 029 | Total loss: 6.406 | Reg loss: 0.031 | Tree loss: 6.406 | Accuracy: 0.281250 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 029 | Total loss: 6.344 | Reg loss: 0.032 | Tree loss: 6.344 | Accuracy: 0.304688 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 029 | Total loss: 6.374 | Reg loss: 0.032 | Tree loss: 6.374 | Accuracy: 0.289062 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 029 | Total loss: 6.324 | Reg loss: 0.032 | Tree loss: 6.324 | Accuracy: 0.287109 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 029 | Total loss: 6.309 | Reg loss: 0.032 | Tree loss: 6.309 | Accuracy: 0.275391 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 029 | Total loss: 6.307 | Reg loss: 0.032 | Tree loss: 6.307 | Accuracy: 0.273438 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 029 | Total loss: 6.271 | Reg loss: 0.033 | Tree loss: 6.271 | Accuracy: 0.267578 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 029 | Total loss: 6.208 | Reg loss: 0.033 | Tree loss: 6.208 | Accuracy: 0.294922 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 029 | Total loss: 6.238 | Reg loss: 0.033 | Tree loss: 6.238 | Accuracy: 0.292969 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 029 | Total loss: 6.235 | Reg loss: 0.033 | Tree loss: 6.235 | Accuracy: 0.273438 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 029 | Total loss: 6.164 | Reg loss: 0.034 | Tree loss: 6.164 | Accuracy: 0.291016 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 029 | Total loss: 6.114 | Reg loss: 0.034 | Tree loss: 6.114 | Accuracy: 0.308594 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 029 | Total loss: 6.081 | Reg loss: 0.034 | Tree loss: 6.081 | Accuracy: 0.308594 | 0.066 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 023 / 029 | Total loss: 6.101 | Reg loss: 0.034 | Tree loss: 6.101 | Accuracy: 0.292969 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 029 | Total loss: 6.082 | Reg loss: 0.035 | Tree loss: 6.082 | Accuracy: 0.277344 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 029 | Total loss: 6.010 | Reg loss: 0.035 | Tree loss: 6.010 | Accuracy: 0.312500 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 029 | Total loss: 6.047 | Reg loss: 0.035 | Tree loss: 6.047 | Accuracy: 0.281250 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 029 | Total loss: 6.012 | Reg loss: 0.035 | Tree loss: 6.012 | Accuracy: 0.285156 | 0.066 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 029 | Total loss: 6.070 | Reg loss: 0.035 | Tree loss: 6.070 | Accuracy: 0.246753 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 18 | Batch: 000 / 029 | Total loss: 6.433 | Reg loss: 0.031 | Tree loss: 6.433 | Accuracy: 0.316406 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 029 | Total loss: 6.457 | Reg loss: 0.032 | Tree loss: 6.457 | Accuracy: 0.300781 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 029 | Total loss: 6.443 | Reg loss: 0.032 | Tree loss: 6.443 | Accuracy: 0.281250 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 029 | Total loss: 6.408 | Reg loss: 0.032 | Tree loss: 6.408 | Accuracy: 0.281250 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 029 | Total loss: 6.369 | Reg loss: 0.032 | Tree loss: 6.369 | Accuracy: 0.300781 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 029 | Total loss: 6.364 | Reg loss: 0.032 | Tree loss: 6.364 | Accuracy: 0.281250 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 029 | Total loss: 6.282 | Reg loss: 0.032 | Tree loss: 6.282 | Accuracy: 0.285156 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 029 | Total loss: 6.271 | Reg loss: 0.032 | Tree loss: 6.271 | Accuracy: 0.316406 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 029 | Total loss: 6.328 | Reg loss: 0.032 | Tree loss: 6.328 | Accuracy: 0.277344 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 029 | Total loss: 6.278 | Reg loss: 0.032 | Tree loss: 6.278 | Accuracy: 0.283203 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 029 | Total loss: 6.230 | Reg loss: 0.032 | Tree loss: 6.230 | Accuracy: 0.298828 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 029 | Total loss: 6.195 | Reg loss: 0.033 | Tree loss: 6.195 | Accuracy: 0.292969 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 029 | Total loss: 6.154 | Reg loss: 0.033 | Tree loss: 6.154 | Accuracy: 0.328125 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 029 | Total loss: 6.176 | Reg loss: 0.033 | Tree loss: 6.176 | Accuracy: 0.251953 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 029 | Total loss: 6.087 | Reg loss: 0.033 | Tree loss: 6.087 | Accuracy: 0.314453 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 029 | Total loss: 6.126 | Reg loss: 0.033 | Tree loss: 6.126 | Accuracy: 0.269531 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 029 | Total loss: 6.062 | Reg loss: 0.034 | Tree loss: 6.062 | Accuracy: 0.308594 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 029 | Total loss: 6.083 | Reg loss: 0.034 | Tree loss: 6.083 | Accuracy: 0.269531 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 029 | Total loss: 6.019 | Reg loss: 0.034 | Tree loss: 6.019 | Accuracy: 0.298828 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 029 | Total loss: 5.998 | Reg loss: 0.034 | Tree loss: 5.998 | Accuracy: 0.324219 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 029 | Total loss: 5.983 | Reg loss: 0.034 | Tree loss: 5.983 | Accuracy: 0.294922 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 029 | Total loss: 5.931 | Reg loss: 0.035 | Tree loss: 5.931 | Accuracy: 0.312500 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 029 | Total loss: 5.907 | Reg loss: 0.035 | Tree loss: 5.907 | Accuracy: 0.302734 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 029 | Total loss: 5.936 | Reg loss: 0.035 | Tree loss: 5.936 | Accuracy: 0.281250 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 029 | Total loss: 5.866 | Reg loss: 0.035 | Tree loss: 5.866 | Accuracy: 0.300781 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 029 | Total loss: 5.916 | Reg loss: 0.035 | Tree loss: 5.916 | Accuracy: 0.269531 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 029 | Total loss: 5.933 | Reg loss: 0.036 | Tree loss: 5.933 | Accuracy: 0.267578 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 029 | Total loss: 5.858 | Reg loss: 0.036 | Tree loss: 5.858 | Accuracy: 0.275391 | 0.066 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 029 | Total loss: 5.799 | Reg loss: 0.036 | Tree loss: 5.799 | Accuracy: 0.324675 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 19 | Batch: 000 / 029 | Total loss: 6.297 | Reg loss: 0.032 | Tree loss: 6.297 | Accuracy: 0.289062 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 029 | Total loss: 6.315 | Reg loss: 0.032 | Tree loss: 6.315 | Accuracy: 0.253906 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 029 | Total loss: 6.294 | Reg loss: 0.032 | Tree loss: 6.294 | Accuracy: 0.287109 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 029 | Total loss: 6.290 | Reg loss: 0.032 | Tree loss: 6.290 | Accuracy: 0.251953 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 029 | Total loss: 6.194 | Reg loss: 0.033 | Tree loss: 6.194 | Accuracy: 0.302734 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 029 | Total loss: 6.212 | Reg loss: 0.033 | Tree loss: 6.212 | Accuracy: 0.289062 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 029 | Total loss: 6.161 | Reg loss: 0.033 | Tree loss: 6.161 | Accuracy: 0.310547 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 029 | Total loss: 6.119 | Reg loss: 0.033 | Tree loss: 6.119 | Accuracy: 0.294922 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 029 | Total loss: 6.083 | Reg loss: 0.033 | Tree loss: 6.083 | Accuracy: 0.312500 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 029 | Total loss: 6.044 | Reg loss: 0.033 | Tree loss: 6.044 | Accuracy: 0.300781 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 029 | Total loss: 6.091 | Reg loss: 0.033 | Tree loss: 6.091 | Accuracy: 0.265625 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 029 | Total loss: 6.006 | Reg loss: 0.033 | Tree loss: 6.006 | Accuracy: 0.302734 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 029 | Total loss: 6.012 | Reg loss: 0.034 | Tree loss: 6.012 | Accuracy: 0.294922 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 029 | Total loss: 6.019 | Reg loss: 0.034 | Tree loss: 6.019 | Accuracy: 0.289062 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 029 | Total loss: 5.940 | Reg loss: 0.034 | Tree loss: 5.940 | Accuracy: 0.291016 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 029 | Total loss: 5.947 | Reg loss: 0.034 | Tree loss: 5.947 | Accuracy: 0.273438 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 029 | Total loss: 5.900 | Reg loss: 0.034 | Tree loss: 5.900 | Accuracy: 0.310547 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 029 | Total loss: 5.885 | Reg loss: 0.034 | Tree loss: 5.885 | Accuracy: 0.275391 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 029 | Total loss: 5.816 | Reg loss: 0.035 | Tree loss: 5.816 | Accuracy: 0.322266 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 029 | Total loss: 5.867 | Reg loss: 0.035 | Tree loss: 5.867 | Accuracy: 0.259766 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 029 | Total loss: 5.793 | Reg loss: 0.035 | Tree loss: 5.793 | Accuracy: 0.304688 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 029 | Total loss: 5.763 | Reg loss: 0.035 | Tree loss: 5.763 | Accuracy: 0.318359 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 029 | Total loss: 5.800 | Reg loss: 0.036 | Tree loss: 5.800 | Accuracy: 0.289062 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 029 | Total loss: 5.775 | Reg loss: 0.036 | Tree loss: 5.775 | Accuracy: 0.294922 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 029 | Total loss: 5.730 | Reg loss: 0.036 | Tree loss: 5.730 | Accuracy: 0.312500 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 029 | Total loss: 5.696 | Reg loss: 0.036 | Tree loss: 5.696 | Accuracy: 0.300781 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 029 | Total loss: 5.666 | Reg loss: 0.036 | Tree loss: 5.666 | Accuracy: 0.302734 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 029 | Total loss: 5.656 | Reg loss: 0.037 | Tree loss: 5.656 | Accuracy: 0.291016 | 0.066 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 029 | Total loss: 5.605 | Reg loss: 0.037 | Tree loss: 5.605 | Accuracy: 0.305195 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 000 / 029 | Total loss: 6.158 | Reg loss: 0.033 | Tree loss: 6.158 | Accuracy: 0.289062 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 029 | Total loss: 6.072 | Reg loss: 0.033 | Tree loss: 6.072 | Accuracy: 0.343750 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 029 | Total loss: 6.082 | Reg loss: 0.033 | Tree loss: 6.082 | Accuracy: 0.316406 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 029 | Total loss: 6.076 | Reg loss: 0.033 | Tree loss: 6.076 | Accuracy: 0.281250 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 029 | Total loss: 6.013 | Reg loss: 0.033 | Tree loss: 6.013 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 029 | Total loss: 6.000 | Reg loss: 0.033 | Tree loss: 6.000 | Accuracy: 0.318359 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 029 | Total loss: 5.987 | Reg loss: 0.033 | Tree loss: 5.987 | Accuracy: 0.273438 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 029 | Total loss: 5.995 | Reg loss: 0.034 | Tree loss: 5.995 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 029 | Total loss: 5.890 | Reg loss: 0.034 | Tree loss: 5.890 | Accuracy: 0.298828 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 029 | Total loss: 5.884 | Reg loss: 0.034 | Tree loss: 5.884 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 029 | Total loss: 5.944 | Reg loss: 0.034 | Tree loss: 5.944 | Accuracy: 0.253906 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 029 | Total loss: 5.866 | Reg loss: 0.034 | Tree loss: 5.866 | Accuracy: 0.306641 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 029 | Total loss: 5.800 | Reg loss: 0.034 | Tree loss: 5.800 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 029 | Total loss: 5.787 | Reg loss: 0.034 | Tree loss: 5.787 | Accuracy: 0.328125 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 029 | Total loss: 5.805 | Reg loss: 0.035 | Tree loss: 5.805 | Accuracy: 0.265625 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 029 | Total loss: 5.812 | Reg loss: 0.035 | Tree loss: 5.812 | Accuracy: 0.265625 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 029 | Total loss: 5.760 | Reg loss: 0.035 | Tree loss: 5.760 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 029 | Total loss: 5.723 | Reg loss: 0.035 | Tree loss: 5.723 | Accuracy: 0.273438 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 029 | Total loss: 5.715 | Reg loss: 0.035 | Tree loss: 5.715 | Accuracy: 0.298828 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 029 | Total loss: 5.650 | Reg loss: 0.036 | Tree loss: 5.650 | Accuracy: 0.304688 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 029 | Total loss: 5.685 | Reg loss: 0.036 | Tree loss: 5.685 | Accuracy: 0.283203 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 029 | Total loss: 5.674 | Reg loss: 0.036 | Tree loss: 5.674 | Accuracy: 0.263672 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 029 | Total loss: 5.653 | Reg loss: 0.036 | Tree loss: 5.653 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 029 | Total loss: 5.555 | Reg loss: 0.036 | Tree loss: 5.555 | Accuracy: 0.318359 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 029 | Total loss: 5.565 | Reg loss: 0.037 | Tree loss: 5.565 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 029 | Total loss: 5.550 | Reg loss: 0.037 | Tree loss: 5.550 | Accuracy: 0.296875 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 029 | Total loss: 5.555 | Reg loss: 0.037 | Tree loss: 5.555 | Accuracy: 0.281250 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 029 | Total loss: 5.501 | Reg loss: 0.037 | Tree loss: 5.501 | Accuracy: 0.287109 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 029 | Total loss: 5.465 | Reg loss: 0.037 | Tree loss: 5.465 | Accuracy: 0.272727 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 21 | Batch: 000 / 029 | Total loss: 5.966 | Reg loss: 0.034 | Tree loss: 5.966 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 029 | Total loss: 5.920 | Reg loss: 0.034 | Tree loss: 5.920 | Accuracy: 0.298828 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 029 | Total loss: 5.848 | Reg loss: 0.034 | Tree loss: 5.848 | Accuracy: 0.357422 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 029 | Total loss: 5.852 | Reg loss: 0.034 | Tree loss: 5.852 | Accuracy: 0.318359 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 029 | Total loss: 5.889 | Reg loss: 0.034 | Tree loss: 5.889 | Accuracy: 0.285156 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 029 | Total loss: 5.891 | Reg loss: 0.034 | Tree loss: 5.891 | Accuracy: 0.263672 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 029 | Total loss: 5.808 | Reg loss: 0.034 | Tree loss: 5.808 | Accuracy: 0.283203 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 029 | Total loss: 5.827 | Reg loss: 0.034 | Tree loss: 5.827 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 029 | Total loss: 5.803 | Reg loss: 0.034 | Tree loss: 5.803 | Accuracy: 0.306641 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 029 | Total loss: 5.775 | Reg loss: 0.035 | Tree loss: 5.775 | Accuracy: 0.298828 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 029 | Total loss: 5.737 | Reg loss: 0.035 | Tree loss: 5.737 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 029 | Total loss: 5.683 | Reg loss: 0.035 | Tree loss: 5.683 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 029 | Total loss: 5.685 | Reg loss: 0.035 | Tree loss: 5.685 | Accuracy: 0.300781 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 029 | Total loss: 5.629 | Reg loss: 0.035 | Tree loss: 5.629 | Accuracy: 0.292969 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 029 | Total loss: 5.635 | Reg loss: 0.035 | Tree loss: 5.635 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 029 | Total loss: 5.642 | Reg loss: 0.036 | Tree loss: 5.642 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 029 | Total loss: 5.559 | Reg loss: 0.036 | Tree loss: 5.559 | Accuracy: 0.296875 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 029 | Total loss: 5.554 | Reg loss: 0.036 | Tree loss: 5.554 | Accuracy: 0.292969 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 029 | Total loss: 5.609 | Reg loss: 0.036 | Tree loss: 5.609 | Accuracy: 0.246094 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 029 | Total loss: 5.524 | Reg loss: 0.036 | Tree loss: 5.524 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 029 | Total loss: 5.496 | Reg loss: 0.036 | Tree loss: 5.496 | Accuracy: 0.296875 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 029 | Total loss: 5.510 | Reg loss: 0.037 | Tree loss: 5.510 | Accuracy: 0.283203 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 029 | Total loss: 5.455 | Reg loss: 0.037 | Tree loss: 5.455 | Accuracy: 0.287109 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 029 | Total loss: 5.439 | Reg loss: 0.037 | Tree loss: 5.439 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 029 | Total loss: 5.425 | Reg loss: 0.037 | Tree loss: 5.425 | Accuracy: 0.261719 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 029 | Total loss: 5.354 | Reg loss: 0.037 | Tree loss: 5.354 | Accuracy: 0.300781 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 029 | Total loss: 5.373 | Reg loss: 0.038 | Tree loss: 5.373 | Accuracy: 0.292969 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 029 | Total loss: 5.327 | Reg loss: 0.038 | Tree loss: 5.327 | Accuracy: 0.275391 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 029 | Total loss: 5.255 | Reg loss: 0.038 | Tree loss: 5.255 | Accuracy: 0.344156 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 22 | Batch: 000 / 029 | Total loss: 5.816 | Reg loss: 0.035 | Tree loss: 5.816 | Accuracy: 0.314453 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 029 | Total loss: 5.814 | Reg loss: 0.035 | Tree loss: 5.814 | Accuracy: 0.275391 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 029 | Total loss: 5.757 | Reg loss: 0.035 | Tree loss: 5.757 | Accuracy: 0.312500 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 029 | Total loss: 5.778 | Reg loss: 0.035 | Tree loss: 5.778 | Accuracy: 0.287109 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 029 | Total loss: 5.710 | Reg loss: 0.035 | Tree loss: 5.710 | Accuracy: 0.312500 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 029 | Total loss: 5.719 | Reg loss: 0.035 | Tree loss: 5.719 | Accuracy: 0.289062 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batch: 006 / 029 | Total loss: 5.620 | Reg loss: 0.035 | Tree loss: 5.620 | Accuracy: 0.304688 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 029 | Total loss: 5.629 | Reg loss: 0.035 | Tree loss: 5.629 | Accuracy: 0.283203 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 029 | Total loss: 5.597 | Reg loss: 0.035 | Tree loss: 5.597 | Accuracy: 0.289062 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 029 | Total loss: 5.551 | Reg loss: 0.035 | Tree loss: 5.551 | Accuracy: 0.298828 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 029 | Total loss: 5.572 | Reg loss: 0.035 | Tree loss: 5.572 | Accuracy: 0.294922 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 029 | Total loss: 5.559 | Reg loss: 0.036 | Tree loss: 5.559 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 029 | Total loss: 5.534 | Reg loss: 0.036 | Tree loss: 5.534 | Accuracy: 0.279297 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 029 | Total loss: 5.516 | Reg loss: 0.036 | Tree loss: 5.516 | Accuracy: 0.275391 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 029 | Total loss: 5.458 | Reg loss: 0.036 | Tree loss: 5.458 | Accuracy: 0.275391 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 029 | Total loss: 5.410 | Reg loss: 0.036 | Tree loss: 5.410 | Accuracy: 0.320312 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 029 | Total loss: 5.432 | Reg loss: 0.036 | Tree loss: 5.432 | Accuracy: 0.304688 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 029 | Total loss: 5.479 | Reg loss: 0.037 | Tree loss: 5.479 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 029 | Total loss: 5.372 | Reg loss: 0.037 | Tree loss: 5.372 | Accuracy: 0.281250 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 029 | Total loss: 5.373 | Reg loss: 0.037 | Tree loss: 5.373 | Accuracy: 0.275391 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 029 | Total loss: 5.328 | Reg loss: 0.037 | Tree loss: 5.328 | Accuracy: 0.281250 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 029 | Total loss: 5.255 | Reg loss: 0.037 | Tree loss: 5.255 | Accuracy: 0.322266 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 029 | Total loss: 5.284 | Reg loss: 0.037 | Tree loss: 5.284 | Accuracy: 0.289062 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 029 | Total loss: 5.210 | Reg loss: 0.038 | Tree loss: 5.210 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 029 | Total loss: 5.210 | Reg loss: 0.038 | Tree loss: 5.210 | Accuracy: 0.324219 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 029 | Total loss: 5.222 | Reg loss: 0.038 | Tree loss: 5.222 | Accuracy: 0.259766 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 029 | Total loss: 5.236 | Reg loss: 0.038 | Tree loss: 5.236 | Accuracy: 0.279297 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 029 | Total loss: 5.110 | Reg loss: 0.038 | Tree loss: 5.110 | Accuracy: 0.300781 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 029 | Total loss: 5.121 | Reg loss: 0.039 | Tree loss: 5.121 | Accuracy: 0.318182 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 23 | Batch: 000 / 029 | Total loss: 5.712 | Reg loss: 0.035 | Tree loss: 5.712 | Accuracy: 0.261719 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 029 | Total loss: 5.673 | Reg loss: 0.035 | Tree loss: 5.673 | Accuracy: 0.279297 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 029 | Total loss: 5.617 | Reg loss: 0.035 | Tree loss: 5.617 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 029 | Total loss: 5.584 | Reg loss: 0.035 | Tree loss: 5.584 | Accuracy: 0.289062 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 029 | Total loss: 5.562 | Reg loss: 0.035 | Tree loss: 5.562 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 029 | Total loss: 5.472 | Reg loss: 0.036 | Tree loss: 5.472 | Accuracy: 0.314453 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 029 | Total loss: 5.486 | Reg loss: 0.036 | Tree loss: 5.486 | Accuracy: 0.292969 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 029 | Total loss: 5.536 | Reg loss: 0.036 | Tree loss: 5.536 | Accuracy: 0.232422 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 029 | Total loss: 5.446 | Reg loss: 0.036 | Tree loss: 5.446 | Accuracy: 0.279297 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 029 | Total loss: 5.406 | Reg loss: 0.036 | Tree loss: 5.406 | Accuracy: 0.320312 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 029 | Total loss: 5.392 | Reg loss: 0.036 | Tree loss: 5.392 | Accuracy: 0.308594 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 029 | Total loss: 5.335 | Reg loss: 0.036 | Tree loss: 5.335 | Accuracy: 0.302734 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 029 | Total loss: 5.385 | Reg loss: 0.036 | Tree loss: 5.385 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 029 | Total loss: 5.279 | Reg loss: 0.036 | Tree loss: 5.279 | Accuracy: 0.314453 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 029 | Total loss: 5.275 | Reg loss: 0.037 | Tree loss: 5.275 | Accuracy: 0.277344 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 029 | Total loss: 5.257 | Reg loss: 0.037 | Tree loss: 5.257 | Accuracy: 0.312500 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 029 | Total loss: 5.255 | Reg loss: 0.037 | Tree loss: 5.255 | Accuracy: 0.318359 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 029 | Total loss: 5.241 | Reg loss: 0.037 | Tree loss: 5.241 | Accuracy: 0.308594 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 029 | Total loss: 5.206 | Reg loss: 0.037 | Tree loss: 5.206 | Accuracy: 0.314453 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 029 | Total loss: 5.168 | Reg loss: 0.037 | Tree loss: 5.168 | Accuracy: 0.298828 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 029 | Total loss: 5.189 | Reg loss: 0.038 | Tree loss: 5.189 | Accuracy: 0.250000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 029 | Total loss: 5.098 | Reg loss: 0.038 | Tree loss: 5.098 | Accuracy: 0.318359 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 029 | Total loss: 5.064 | Reg loss: 0.038 | Tree loss: 5.064 | Accuracy: 0.287109 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 029 | Total loss: 5.096 | Reg loss: 0.038 | Tree loss: 5.096 | Accuracy: 0.296875 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 029 | Total loss: 5.086 | Reg loss: 0.038 | Tree loss: 5.086 | Accuracy: 0.261719 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 029 | Total loss: 5.053 | Reg loss: 0.039 | Tree loss: 5.053 | Accuracy: 0.281250 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 029 | Total loss: 4.997 | Reg loss: 0.039 | Tree loss: 4.997 | Accuracy: 0.308594 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 029 | Total loss: 4.979 | Reg loss: 0.039 | Tree loss: 4.979 | Accuracy: 0.324219 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 029 | Total loss: 4.998 | Reg loss: 0.039 | Tree loss: 4.998 | Accuracy: 0.240260 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 24 | Batch: 000 / 029 | Total loss: 5.563 | Reg loss: 0.036 | Tree loss: 5.563 | Accuracy: 0.251953 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 029 | Total loss: 5.434 | Reg loss: 0.036 | Tree loss: 5.434 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 029 | Total loss: 5.408 | Reg loss: 0.036 | Tree loss: 5.408 | Accuracy: 0.320312 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 029 | Total loss: 5.328 | Reg loss: 0.036 | Tree loss: 5.328 | Accuracy: 0.345703 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 029 | Total loss: 5.442 | Reg loss: 0.036 | Tree loss: 5.442 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 029 | Total loss: 5.339 | Reg loss: 0.036 | Tree loss: 5.339 | Accuracy: 0.304688 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 029 | Total loss: 5.345 | Reg loss: 0.036 | Tree loss: 5.345 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 029 | Total loss: 5.264 | Reg loss: 0.036 | Tree loss: 5.264 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 029 | Total loss: 5.306 | Reg loss: 0.036 | Tree loss: 5.306 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 029 | Total loss: 5.257 | Reg loss: 0.037 | Tree loss: 5.257 | Accuracy: 0.257812 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 029 | Total loss: 5.206 | Reg loss: 0.037 | Tree loss: 5.206 | Accuracy: 0.316406 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 029 | Total loss: 5.270 | Reg loss: 0.037 | Tree loss: 5.270 | Accuracy: 0.238281 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 029 | Total loss: 5.187 | Reg loss: 0.037 | Tree loss: 5.187 | Accuracy: 0.279297 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Batch: 013 / 029 | Total loss: 5.122 | Reg loss: 0.037 | Tree loss: 5.122 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 029 | Total loss: 5.132 | Reg loss: 0.037 | Tree loss: 5.132 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 029 | Total loss: 5.124 | Reg loss: 0.037 | Tree loss: 5.124 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 029 | Total loss: 5.077 | Reg loss: 0.037 | Tree loss: 5.077 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 029 | Total loss: 5.069 | Reg loss: 0.038 | Tree loss: 5.069 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 029 | Total loss: 5.046 | Reg loss: 0.038 | Tree loss: 5.046 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 029 | Total loss: 5.008 | Reg loss: 0.038 | Tree loss: 5.008 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 029 | Total loss: 4.983 | Reg loss: 0.038 | Tree loss: 4.983 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 029 | Total loss: 4.960 | Reg loss: 0.038 | Tree loss: 4.960 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 029 | Total loss: 4.946 | Reg loss: 0.039 | Tree loss: 4.946 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 029 | Total loss: 4.876 | Reg loss: 0.039 | Tree loss: 4.876 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 029 | Total loss: 4.853 | Reg loss: 0.039 | Tree loss: 4.853 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 029 | Total loss: 4.861 | Reg loss: 0.039 | Tree loss: 4.861 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 029 | Total loss: 4.830 | Reg loss: 0.039 | Tree loss: 4.830 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 029 | Total loss: 4.832 | Reg loss: 0.039 | Tree loss: 4.832 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 029 | Total loss: 4.859 | Reg loss: 0.040 | Tree loss: 4.859 | Accuracy: 0.266234 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 25 | Batch: 000 / 029 | Total loss: 5.320 | Reg loss: 0.036 | Tree loss: 5.320 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 029 | Total loss: 5.257 | Reg loss: 0.036 | Tree loss: 5.257 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 029 | Total loss: 5.297 | Reg loss: 0.037 | Tree loss: 5.297 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 029 | Total loss: 5.224 | Reg loss: 0.037 | Tree loss: 5.224 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 029 | Total loss: 5.194 | Reg loss: 0.037 | Tree loss: 5.194 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 029 | Total loss: 5.252 | Reg loss: 0.037 | Tree loss: 5.252 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 029 | Total loss: 5.193 | Reg loss: 0.037 | Tree loss: 5.193 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 029 | Total loss: 5.119 | Reg loss: 0.037 | Tree loss: 5.119 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 029 | Total loss: 5.131 | Reg loss: 0.037 | Tree loss: 5.131 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 029 | Total loss: 5.077 | Reg loss: 0.037 | Tree loss: 5.077 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 029 | Total loss: 5.108 | Reg loss: 0.037 | Tree loss: 5.108 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 029 | Total loss: 5.052 | Reg loss: 0.037 | Tree loss: 5.052 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 029 | Total loss: 4.994 | Reg loss: 0.037 | Tree loss: 4.994 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 029 | Total loss: 4.974 | Reg loss: 0.038 | Tree loss: 4.974 | Accuracy: 0.304688 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 029 | Total loss: 4.966 | Reg loss: 0.038 | Tree loss: 4.966 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 029 | Total loss: 4.939 | Reg loss: 0.038 | Tree loss: 4.939 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 029 | Total loss: 4.881 | Reg loss: 0.038 | Tree loss: 4.881 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 029 | Total loss: 4.862 | Reg loss: 0.038 | Tree loss: 4.862 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 029 | Total loss: 4.831 | Reg loss: 0.038 | Tree loss: 4.831 | Accuracy: 0.312500 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 029 | Total loss: 4.821 | Reg loss: 0.038 | Tree loss: 4.821 | Accuracy: 0.324219 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 029 | Total loss: 4.783 | Reg loss: 0.039 | Tree loss: 4.783 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 029 | Total loss: 4.802 | Reg loss: 0.039 | Tree loss: 4.802 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 029 | Total loss: 4.782 | Reg loss: 0.039 | Tree loss: 4.782 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 029 | Total loss: 4.706 | Reg loss: 0.039 | Tree loss: 4.706 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 029 | Total loss: 4.740 | Reg loss: 0.039 | Tree loss: 4.740 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 029 | Total loss: 4.700 | Reg loss: 0.039 | Tree loss: 4.700 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 029 | Total loss: 4.657 | Reg loss: 0.040 | Tree loss: 4.657 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 029 | Total loss: 4.644 | Reg loss: 0.040 | Tree loss: 4.644 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 029 | Total loss: 4.641 | Reg loss: 0.040 | Tree loss: 4.641 | Accuracy: 0.292208 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 26 | Batch: 000 / 029 | Total loss: 5.098 | Reg loss: 0.037 | Tree loss: 5.098 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 029 | Total loss: 5.147 | Reg loss: 0.037 | Tree loss: 5.147 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 029 | Total loss: 5.105 | Reg loss: 0.037 | Tree loss: 5.105 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 029 | Total loss: 5.106 | Reg loss: 0.037 | Tree loss: 5.106 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 029 | Total loss: 5.058 | Reg loss: 0.037 | Tree loss: 5.058 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 029 | Total loss: 4.958 | Reg loss: 0.037 | Tree loss: 4.958 | Accuracy: 0.328125 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 029 | Total loss: 5.011 | Reg loss: 0.037 | Tree loss: 5.011 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 029 | Total loss: 4.981 | Reg loss: 0.037 | Tree loss: 4.981 | Accuracy: 0.316406 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 029 | Total loss: 4.854 | Reg loss: 0.037 | Tree loss: 4.854 | Accuracy: 0.324219 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 029 | Total loss: 4.910 | Reg loss: 0.037 | Tree loss: 4.910 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 029 | Total loss: 4.947 | Reg loss: 0.038 | Tree loss: 4.947 | Accuracy: 0.257812 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 029 | Total loss: 4.857 | Reg loss: 0.038 | Tree loss: 4.857 | Accuracy: 0.304688 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 029 | Total loss: 4.819 | Reg loss: 0.038 | Tree loss: 4.819 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 029 | Total loss: 4.784 | Reg loss: 0.038 | Tree loss: 4.784 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 029 | Total loss: 4.827 | Reg loss: 0.038 | Tree loss: 4.827 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 029 | Total loss: 4.756 | Reg loss: 0.038 | Tree loss: 4.756 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 029 | Total loss: 4.752 | Reg loss: 0.038 | Tree loss: 4.752 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 029 | Total loss: 4.692 | Reg loss: 0.038 | Tree loss: 4.692 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 029 | Total loss: 4.643 | Reg loss: 0.039 | Tree loss: 4.643 | Accuracy: 0.330078 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 029 | Total loss: 4.662 | Reg loss: 0.039 | Tree loss: 4.662 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 029 | Total loss: 4.671 | Reg loss: 0.039 | Tree loss: 4.671 | Accuracy: 0.271484 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 021 / 029 | Total loss: 4.601 | Reg loss: 0.039 | Tree loss: 4.601 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 029 | Total loss: 4.621 | Reg loss: 0.039 | Tree loss: 4.621 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 029 | Total loss: 4.629 | Reg loss: 0.039 | Tree loss: 4.629 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 029 | Total loss: 4.548 | Reg loss: 0.040 | Tree loss: 4.548 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 029 | Total loss: 4.512 | Reg loss: 0.040 | Tree loss: 4.512 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 029 | Total loss: 4.484 | Reg loss: 0.040 | Tree loss: 4.484 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 029 | Total loss: 4.564 | Reg loss: 0.040 | Tree loss: 4.564 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 029 | Total loss: 4.455 | Reg loss: 0.040 | Tree loss: 4.455 | Accuracy: 0.285714 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 27 | Batch: 000 / 029 | Total loss: 5.014 | Reg loss: 0.037 | Tree loss: 5.014 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 029 | Total loss: 4.926 | Reg loss: 0.037 | Tree loss: 4.926 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 029 | Total loss: 4.922 | Reg loss: 0.037 | Tree loss: 4.922 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 029 | Total loss: 4.880 | Reg loss: 0.037 | Tree loss: 4.880 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 029 | Total loss: 4.905 | Reg loss: 0.037 | Tree loss: 4.905 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 029 | Total loss: 4.824 | Reg loss: 0.037 | Tree loss: 4.824 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 029 | Total loss: 4.843 | Reg loss: 0.038 | Tree loss: 4.843 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 029 | Total loss: 4.795 | Reg loss: 0.038 | Tree loss: 4.795 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 029 | Total loss: 4.741 | Reg loss: 0.038 | Tree loss: 4.741 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 029 | Total loss: 4.742 | Reg loss: 0.038 | Tree loss: 4.742 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 029 | Total loss: 4.730 | Reg loss: 0.038 | Tree loss: 4.730 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 029 | Total loss: 4.680 | Reg loss: 0.038 | Tree loss: 4.680 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 029 | Total loss: 4.697 | Reg loss: 0.038 | Tree loss: 4.697 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 029 | Total loss: 4.702 | Reg loss: 0.038 | Tree loss: 4.702 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 029 | Total loss: 4.620 | Reg loss: 0.038 | Tree loss: 4.620 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 029 | Total loss: 4.532 | Reg loss: 0.038 | Tree loss: 4.532 | Accuracy: 0.320312 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 029 | Total loss: 4.624 | Reg loss: 0.039 | Tree loss: 4.624 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 029 | Total loss: 4.605 | Reg loss: 0.039 | Tree loss: 4.605 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 029 | Total loss: 4.566 | Reg loss: 0.039 | Tree loss: 4.566 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 029 | Total loss: 4.507 | Reg loss: 0.039 | Tree loss: 4.507 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 029 | Total loss: 4.454 | Reg loss: 0.039 | Tree loss: 4.454 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 029 | Total loss: 4.409 | Reg loss: 0.039 | Tree loss: 4.409 | Accuracy: 0.318359 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 029 | Total loss: 4.403 | Reg loss: 0.039 | Tree loss: 4.403 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 029 | Total loss: 4.394 | Reg loss: 0.040 | Tree loss: 4.394 | Accuracy: 0.322266 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 029 | Total loss: 4.430 | Reg loss: 0.040 | Tree loss: 4.430 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 029 | Total loss: 4.329 | Reg loss: 0.040 | Tree loss: 4.329 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 029 | Total loss: 4.376 | Reg loss: 0.040 | Tree loss: 4.376 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 029 | Total loss: 4.315 | Reg loss: 0.040 | Tree loss: 4.315 | Accuracy: 0.312500 | 0.068 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 029 | Total loss: 4.306 | Reg loss: 0.040 | Tree loss: 4.306 | Accuracy: 0.292208 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 28 | Batch: 000 / 029 | Total loss: 4.769 | Reg loss: 0.038 | Tree loss: 4.769 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 029 | Total loss: 4.793 | Reg loss: 0.038 | Tree loss: 4.793 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 029 | Total loss: 4.785 | Reg loss: 0.038 | Tree loss: 4.785 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 029 | Total loss: 4.766 | Reg loss: 0.038 | Tree loss: 4.766 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 029 | Total loss: 4.728 | Reg loss: 0.038 | Tree loss: 4.728 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 029 | Total loss: 4.672 | Reg loss: 0.038 | Tree loss: 4.672 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 029 | Total loss: 4.669 | Reg loss: 0.038 | Tree loss: 4.669 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 029 | Total loss: 4.588 | Reg loss: 0.038 | Tree loss: 4.588 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 029 | Total loss: 4.572 | Reg loss: 0.038 | Tree loss: 4.572 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 029 | Total loss: 4.594 | Reg loss: 0.038 | Tree loss: 4.594 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 029 | Total loss: 4.553 | Reg loss: 0.038 | Tree loss: 4.553 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 029 | Total loss: 4.512 | Reg loss: 0.038 | Tree loss: 4.512 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 029 | Total loss: 4.548 | Reg loss: 0.038 | Tree loss: 4.548 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 029 | Total loss: 4.503 | Reg loss: 0.038 | Tree loss: 4.503 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 029 | Total loss: 4.457 | Reg loss: 0.039 | Tree loss: 4.457 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 029 | Total loss: 4.458 | Reg loss: 0.039 | Tree loss: 4.458 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 029 | Total loss: 4.406 | Reg loss: 0.039 | Tree loss: 4.406 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 029 | Total loss: 4.410 | Reg loss: 0.039 | Tree loss: 4.410 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 029 | Total loss: 4.377 | Reg loss: 0.039 | Tree loss: 4.377 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 029 | Total loss: 4.273 | Reg loss: 0.039 | Tree loss: 4.273 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 029 | Total loss: 4.329 | Reg loss: 0.039 | Tree loss: 4.329 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 029 | Total loss: 4.286 | Reg loss: 0.039 | Tree loss: 4.286 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 029 | Total loss: 4.289 | Reg loss: 0.040 | Tree loss: 4.289 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 029 | Total loss: 4.222 | Reg loss: 0.040 | Tree loss: 4.222 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 029 | Total loss: 4.236 | Reg loss: 0.040 | Tree loss: 4.236 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 029 | Total loss: 4.249 | Reg loss: 0.040 | Tree loss: 4.249 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 029 | Total loss: 4.169 | Reg loss: 0.040 | Tree loss: 4.169 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 029 | Total loss: 4.267 | Reg loss: 0.040 | Tree loss: 4.267 | Accuracy: 0.250000 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 028 / 029 | Total loss: 4.104 | Reg loss: 0.040 | Tree loss: 4.104 | Accuracy: 0.311688 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 29 | Batch: 000 / 029 | Total loss: 4.678 | Reg loss: 0.038 | Tree loss: 4.678 | Accuracy: 0.248047 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 029 | Total loss: 4.560 | Reg loss: 0.038 | Tree loss: 4.560 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 029 | Total loss: 4.582 | Reg loss: 0.038 | Tree loss: 4.582 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 029 | Total loss: 4.553 | Reg loss: 0.038 | Tree loss: 4.553 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 029 | Total loss: 4.582 | Reg loss: 0.038 | Tree loss: 4.582 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 029 | Total loss: 4.559 | Reg loss: 0.038 | Tree loss: 4.559 | Accuracy: 0.242188 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 029 | Total loss: 4.503 | Reg loss: 0.038 | Tree loss: 4.503 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 029 | Total loss: 4.482 | Reg loss: 0.038 | Tree loss: 4.482 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 029 | Total loss: 4.466 | Reg loss: 0.038 | Tree loss: 4.466 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 029 | Total loss: 4.399 | Reg loss: 0.038 | Tree loss: 4.399 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 029 | Total loss: 4.377 | Reg loss: 0.038 | Tree loss: 4.377 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 029 | Total loss: 4.397 | Reg loss: 0.038 | Tree loss: 4.397 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 029 | Total loss: 4.338 | Reg loss: 0.038 | Tree loss: 4.338 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 029 | Total loss: 4.354 | Reg loss: 0.039 | Tree loss: 4.354 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 029 | Total loss: 4.268 | Reg loss: 0.039 | Tree loss: 4.268 | Accuracy: 0.335938 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 029 | Total loss: 4.280 | Reg loss: 0.039 | Tree loss: 4.280 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 029 | Total loss: 4.240 | Reg loss: 0.039 | Tree loss: 4.240 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 029 | Total loss: 4.246 | Reg loss: 0.039 | Tree loss: 4.246 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 029 | Total loss: 4.245 | Reg loss: 0.039 | Tree loss: 4.245 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 029 | Total loss: 4.185 | Reg loss: 0.039 | Tree loss: 4.185 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 029 | Total loss: 4.147 | Reg loss: 0.039 | Tree loss: 4.147 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 029 | Total loss: 4.187 | Reg loss: 0.040 | Tree loss: 4.187 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 029 | Total loss: 4.130 | Reg loss: 0.040 | Tree loss: 4.130 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 029 | Total loss: 4.164 | Reg loss: 0.040 | Tree loss: 4.164 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 029 | Total loss: 4.072 | Reg loss: 0.040 | Tree loss: 4.072 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 029 | Total loss: 4.038 | Reg loss: 0.040 | Tree loss: 4.038 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 029 | Total loss: 4.008 | Reg loss: 0.040 | Tree loss: 4.008 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 029 | Total loss: 4.070 | Reg loss: 0.040 | Tree loss: 4.070 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 029 | Total loss: 3.979 | Reg loss: 0.040 | Tree loss: 3.979 | Accuracy: 0.266234 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 30 | Batch: 000 / 029 | Total loss: 4.484 | Reg loss: 0.038 | Tree loss: 4.484 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 029 | Total loss: 4.427 | Reg loss: 0.038 | Tree loss: 4.427 | Accuracy: 0.343750 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 029 | Total loss: 4.422 | Reg loss: 0.038 | Tree loss: 4.422 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 029 | Total loss: 4.461 | Reg loss: 0.038 | Tree loss: 4.461 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 029 | Total loss: 4.393 | Reg loss: 0.038 | Tree loss: 4.393 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 029 | Total loss: 4.365 | Reg loss: 0.038 | Tree loss: 4.365 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 029 | Total loss: 4.338 | Reg loss: 0.038 | Tree loss: 4.338 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 029 | Total loss: 4.265 | Reg loss: 0.038 | Tree loss: 4.265 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 029 | Total loss: 4.286 | Reg loss: 0.038 | Tree loss: 4.286 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 029 | Total loss: 4.294 | Reg loss: 0.038 | Tree loss: 4.294 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 029 | Total loss: 4.306 | Reg loss: 0.038 | Tree loss: 4.306 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 029 | Total loss: 4.190 | Reg loss: 0.038 | Tree loss: 4.190 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 029 | Total loss: 4.182 | Reg loss: 0.039 | Tree loss: 4.182 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 029 | Total loss: 4.167 | Reg loss: 0.039 | Tree loss: 4.167 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 029 | Total loss: 4.195 | Reg loss: 0.039 | Tree loss: 4.195 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 029 | Total loss: 4.122 | Reg loss: 0.039 | Tree loss: 4.122 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 029 | Total loss: 4.073 | Reg loss: 0.039 | Tree loss: 4.073 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 029 | Total loss: 4.073 | Reg loss: 0.039 | Tree loss: 4.073 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 029 | Total loss: 4.096 | Reg loss: 0.039 | Tree loss: 4.096 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 029 | Total loss: 3.996 | Reg loss: 0.039 | Tree loss: 3.996 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 029 | Total loss: 4.065 | Reg loss: 0.039 | Tree loss: 4.065 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 029 | Total loss: 3.998 | Reg loss: 0.040 | Tree loss: 3.998 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 029 | Total loss: 3.987 | Reg loss: 0.040 | Tree loss: 3.987 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 029 | Total loss: 3.973 | Reg loss: 0.040 | Tree loss: 3.973 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 029 | Total loss: 3.964 | Reg loss: 0.040 | Tree loss: 3.964 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 029 | Total loss: 3.934 | Reg loss: 0.040 | Tree loss: 3.934 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 029 | Total loss: 3.979 | Reg loss: 0.040 | Tree loss: 3.979 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 029 | Total loss: 3.853 | Reg loss: 0.040 | Tree loss: 3.853 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 029 | Total loss: 3.846 | Reg loss: 0.040 | Tree loss: 3.846 | Accuracy: 0.292208 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 31 | Batch: 000 / 029 | Total loss: 4.337 | Reg loss: 0.038 | Tree loss: 4.337 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 029 | Total loss: 4.291 | Reg loss: 0.038 | Tree loss: 4.291 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 029 | Total loss: 4.258 | Reg loss: 0.038 | Tree loss: 4.258 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 029 | Total loss: 4.298 | Reg loss: 0.038 | Tree loss: 4.298 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 029 | Total loss: 4.240 | Reg loss: 0.038 | Tree loss: 4.240 | Accuracy: 0.306641 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 005 / 029 | Total loss: 4.205 | Reg loss: 0.038 | Tree loss: 4.205 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 029 | Total loss: 4.278 | Reg loss: 0.038 | Tree loss: 4.278 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 029 | Total loss: 4.204 | Reg loss: 0.038 | Tree loss: 4.204 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 029 | Total loss: 4.137 | Reg loss: 0.038 | Tree loss: 4.137 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 029 | Total loss: 4.170 | Reg loss: 0.038 | Tree loss: 4.170 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 029 | Total loss: 4.076 | Reg loss: 0.038 | Tree loss: 4.076 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 029 | Total loss: 4.068 | Reg loss: 0.038 | Tree loss: 4.068 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 029 | Total loss: 4.065 | Reg loss: 0.038 | Tree loss: 4.065 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 029 | Total loss: 3.982 | Reg loss: 0.039 | Tree loss: 3.982 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 029 | Total loss: 4.061 | Reg loss: 0.039 | Tree loss: 4.061 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 029 | Total loss: 3.976 | Reg loss: 0.039 | Tree loss: 3.976 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 029 | Total loss: 3.980 | Reg loss: 0.039 | Tree loss: 3.980 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 029 | Total loss: 3.929 | Reg loss: 0.039 | Tree loss: 3.929 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 029 | Total loss: 3.933 | Reg loss: 0.039 | Tree loss: 3.933 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 029 | Total loss: 3.912 | Reg loss: 0.039 | Tree loss: 3.912 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 029 | Total loss: 3.898 | Reg loss: 0.039 | Tree loss: 3.898 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 029 | Total loss: 3.920 | Reg loss: 0.039 | Tree loss: 3.920 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 029 | Total loss: 3.786 | Reg loss: 0.040 | Tree loss: 3.786 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 029 | Total loss: 3.818 | Reg loss: 0.040 | Tree loss: 3.818 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 029 | Total loss: 3.745 | Reg loss: 0.040 | Tree loss: 3.745 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 029 | Total loss: 3.779 | Reg loss: 0.040 | Tree loss: 3.779 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 029 | Total loss: 3.741 | Reg loss: 0.040 | Tree loss: 3.741 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 029 | Total loss: 3.755 | Reg loss: 0.040 | Tree loss: 3.755 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 028 / 029 | Total loss: 3.721 | Reg loss: 0.040 | Tree loss: 3.721 | Accuracy: 0.279221 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 32 | Batch: 000 / 029 | Total loss: 4.185 | Reg loss: 0.038 | Tree loss: 4.185 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 029 | Total loss: 4.170 | Reg loss: 0.038 | Tree loss: 4.170 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 029 | Total loss: 4.202 | Reg loss: 0.038 | Tree loss: 4.202 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 029 | Total loss: 4.083 | Reg loss: 0.038 | Tree loss: 4.083 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 029 | Total loss: 4.094 | Reg loss: 0.038 | Tree loss: 4.094 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 029 | Total loss: 4.085 | Reg loss: 0.038 | Tree loss: 4.085 | Accuracy: 0.238281 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 029 | Total loss: 4.156 | Reg loss: 0.038 | Tree loss: 4.156 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 029 | Total loss: 4.052 | Reg loss: 0.038 | Tree loss: 4.052 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 029 | Total loss: 4.025 | Reg loss: 0.038 | Tree loss: 4.025 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 029 | Total loss: 4.026 | Reg loss: 0.038 | Tree loss: 4.026 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 029 | Total loss: 3.998 | Reg loss: 0.038 | Tree loss: 3.998 | Accuracy: 0.246094 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 029 | Total loss: 3.930 | Reg loss: 0.038 | Tree loss: 3.930 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 029 | Total loss: 3.931 | Reg loss: 0.038 | Tree loss: 3.931 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 029 | Total loss: 3.865 | Reg loss: 0.038 | Tree loss: 3.865 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 029 | Total loss: 3.861 | Reg loss: 0.038 | Tree loss: 3.861 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 029 | Total loss: 3.857 | Reg loss: 0.039 | Tree loss: 3.857 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 029 | Total loss: 3.781 | Reg loss: 0.039 | Tree loss: 3.781 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 029 | Total loss: 3.775 | Reg loss: 0.039 | Tree loss: 3.775 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 029 | Total loss: 3.793 | Reg loss: 0.039 | Tree loss: 3.793 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 029 | Total loss: 3.702 | Reg loss: 0.039 | Tree loss: 3.702 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 029 | Total loss: 3.725 | Reg loss: 0.039 | Tree loss: 3.725 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 029 | Total loss: 3.671 | Reg loss: 0.039 | Tree loss: 3.671 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 029 | Total loss: 3.735 | Reg loss: 0.039 | Tree loss: 3.735 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 029 | Total loss: 3.658 | Reg loss: 0.039 | Tree loss: 3.658 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 029 | Total loss: 3.677 | Reg loss: 0.040 | Tree loss: 3.677 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 029 | Total loss: 3.674 | Reg loss: 0.040 | Tree loss: 3.674 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 029 | Total loss: 3.627 | Reg loss: 0.040 | Tree loss: 3.627 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 029 | Total loss: 3.601 | Reg loss: 0.040 | Tree loss: 3.601 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 029 | Total loss: 3.554 | Reg loss: 0.040 | Tree loss: 3.554 | Accuracy: 0.324675 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 33 | Batch: 000 / 029 | Total loss: 4.059 | Reg loss: 0.038 | Tree loss: 4.059 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 029 | Total loss: 4.022 | Reg loss: 0.038 | Tree loss: 4.022 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 029 | Total loss: 4.007 | Reg loss: 0.038 | Tree loss: 4.007 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 029 | Total loss: 4.066 | Reg loss: 0.038 | Tree loss: 4.066 | Accuracy: 0.238281 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 029 | Total loss: 3.953 | Reg loss: 0.038 | Tree loss: 3.953 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 029 | Total loss: 3.905 | Reg loss: 0.038 | Tree loss: 3.905 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 029 | Total loss: 3.874 | Reg loss: 0.038 | Tree loss: 3.874 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 029 | Total loss: 3.866 | Reg loss: 0.038 | Tree loss: 3.866 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 029 | Total loss: 3.905 | Reg loss: 0.038 | Tree loss: 3.905 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 029 | Total loss: 3.830 | Reg loss: 0.038 | Tree loss: 3.830 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 029 | Total loss: 3.873 | Reg loss: 0.038 | Tree loss: 3.873 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 029 | Total loss: 3.805 | Reg loss: 0.038 | Tree loss: 3.805 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 029 | Total loss: 3.803 | Reg loss: 0.038 | Tree loss: 3.803 | Accuracy: 0.246094 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 013 / 029 | Total loss: 3.791 | Reg loss: 0.038 | Tree loss: 3.791 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 029 | Total loss: 3.710 | Reg loss: 0.038 | Tree loss: 3.710 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 029 | Total loss: 3.770 | Reg loss: 0.038 | Tree loss: 3.770 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 029 | Total loss: 3.694 | Reg loss: 0.038 | Tree loss: 3.694 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 029 | Total loss: 3.683 | Reg loss: 0.039 | Tree loss: 3.683 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 029 | Total loss: 3.681 | Reg loss: 0.039 | Tree loss: 3.681 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 029 | Total loss: 3.552 | Reg loss: 0.039 | Tree loss: 3.552 | Accuracy: 0.326172 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 029 | Total loss: 3.598 | Reg loss: 0.039 | Tree loss: 3.598 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 029 | Total loss: 3.591 | Reg loss: 0.039 | Tree loss: 3.591 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 029 | Total loss: 3.517 | Reg loss: 0.039 | Tree loss: 3.517 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 029 | Total loss: 3.565 | Reg loss: 0.039 | Tree loss: 3.565 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 029 | Total loss: 3.528 | Reg loss: 0.039 | Tree loss: 3.528 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 029 | Total loss: 3.536 | Reg loss: 0.039 | Tree loss: 3.536 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 029 | Total loss: 3.532 | Reg loss: 0.039 | Tree loss: 3.532 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 029 | Total loss: 3.461 | Reg loss: 0.040 | Tree loss: 3.461 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 029 | Total loss: 3.477 | Reg loss: 0.040 | Tree loss: 3.477 | Accuracy: 0.259740 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 34 | Batch: 000 / 029 | Total loss: 3.910 | Reg loss: 0.037 | Tree loss: 3.910 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 029 | Total loss: 3.946 | Reg loss: 0.037 | Tree loss: 3.946 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 029 | Total loss: 3.886 | Reg loss: 0.037 | Tree loss: 3.886 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 029 | Total loss: 3.851 | Reg loss: 0.037 | Tree loss: 3.851 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 029 | Total loss: 3.853 | Reg loss: 0.037 | Tree loss: 3.853 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 029 | Total loss: 3.759 | Reg loss: 0.037 | Tree loss: 3.759 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 029 | Total loss: 3.758 | Reg loss: 0.037 | Tree loss: 3.758 | Accuracy: 0.318359 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 029 | Total loss: 3.829 | Reg loss: 0.037 | Tree loss: 3.829 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 029 | Total loss: 3.830 | Reg loss: 0.038 | Tree loss: 3.830 | Accuracy: 0.251953 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 029 | Total loss: 3.696 | Reg loss: 0.038 | Tree loss: 3.696 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 029 | Total loss: 3.687 | Reg loss: 0.038 | Tree loss: 3.687 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 029 | Total loss: 3.687 | Reg loss: 0.038 | Tree loss: 3.687 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 029 | Total loss: 3.687 | Reg loss: 0.038 | Tree loss: 3.687 | Accuracy: 0.251953 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 029 | Total loss: 3.621 | Reg loss: 0.038 | Tree loss: 3.621 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 029 | Total loss: 3.661 | Reg loss: 0.038 | Tree loss: 3.661 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 029 | Total loss: 3.598 | Reg loss: 0.038 | Tree loss: 3.598 | Accuracy: 0.257812 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 029 | Total loss: 3.472 | Reg loss: 0.038 | Tree loss: 3.472 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 029 | Total loss: 3.499 | Reg loss: 0.038 | Tree loss: 3.499 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 029 | Total loss: 3.533 | Reg loss: 0.038 | Tree loss: 3.533 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 029 | Total loss: 3.521 | Reg loss: 0.038 | Tree loss: 3.521 | Accuracy: 0.250000 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 029 | Total loss: 3.451 | Reg loss: 0.039 | Tree loss: 3.451 | Accuracy: 0.326172 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 029 | Total loss: 3.468 | Reg loss: 0.039 | Tree loss: 3.468 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 029 | Total loss: 3.421 | Reg loss: 0.039 | Tree loss: 3.421 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 029 | Total loss: 3.460 | Reg loss: 0.039 | Tree loss: 3.460 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 029 | Total loss: 3.407 | Reg loss: 0.039 | Tree loss: 3.407 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 029 | Total loss: 3.321 | Reg loss: 0.039 | Tree loss: 3.321 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 029 | Total loss: 3.386 | Reg loss: 0.039 | Tree loss: 3.386 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 029 | Total loss: 3.325 | Reg loss: 0.039 | Tree loss: 3.325 | Accuracy: 0.246094 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 029 | Total loss: 3.311 | Reg loss: 0.039 | Tree loss: 3.311 | Accuracy: 0.285714 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 35 | Batch: 000 / 029 | Total loss: 3.860 | Reg loss: 0.037 | Tree loss: 3.860 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 029 | Total loss: 3.774 | Reg loss: 0.037 | Tree loss: 3.774 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 029 | Total loss: 3.778 | Reg loss: 0.037 | Tree loss: 3.778 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 029 | Total loss: 3.719 | Reg loss: 0.037 | Tree loss: 3.719 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 029 | Total loss: 3.743 | Reg loss: 0.037 | Tree loss: 3.743 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 029 | Total loss: 3.712 | Reg loss: 0.037 | Tree loss: 3.712 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 029 | Total loss: 3.613 | Reg loss: 0.037 | Tree loss: 3.613 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 029 | Total loss: 3.656 | Reg loss: 0.037 | Tree loss: 3.656 | Accuracy: 0.318359 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 029 | Total loss: 3.671 | Reg loss: 0.037 | Tree loss: 3.671 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 029 | Total loss: 3.619 | Reg loss: 0.037 | Tree loss: 3.619 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 029 | Total loss: 3.575 | Reg loss: 0.037 | Tree loss: 3.575 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 029 | Total loss: 3.599 | Reg loss: 0.037 | Tree loss: 3.599 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 029 | Total loss: 3.491 | Reg loss: 0.037 | Tree loss: 3.491 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 029 | Total loss: 3.489 | Reg loss: 0.037 | Tree loss: 3.489 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 029 | Total loss: 3.511 | Reg loss: 0.038 | Tree loss: 3.511 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 029 | Total loss: 3.453 | Reg loss: 0.038 | Tree loss: 3.453 | Accuracy: 0.312500 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 029 | Total loss: 3.483 | Reg loss: 0.038 | Tree loss: 3.483 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 029 | Total loss: 3.374 | Reg loss: 0.038 | Tree loss: 3.374 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 029 | Total loss: 3.338 | Reg loss: 0.038 | Tree loss: 3.338 | Accuracy: 0.314453 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 019 / 029 | Total loss: 3.397 | Reg loss: 0.038 | Tree loss: 3.397 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 029 | Total loss: 3.335 | Reg loss: 0.038 | Tree loss: 3.335 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 029 | Total loss: 3.340 | Reg loss: 0.038 | Tree loss: 3.340 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 029 | Total loss: 3.278 | Reg loss: 0.038 | Tree loss: 3.278 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 029 | Total loss: 3.320 | Reg loss: 0.039 | Tree loss: 3.320 | Accuracy: 0.232422 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 029 | Total loss: 3.238 | Reg loss: 0.039 | Tree loss: 3.238 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 029 | Total loss: 3.271 | Reg loss: 0.039 | Tree loss: 3.271 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 029 | Total loss: 3.250 | Reg loss: 0.039 | Tree loss: 3.250 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 029 | Total loss: 3.146 | Reg loss: 0.039 | Tree loss: 3.146 | Accuracy: 0.328125 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 029 | Total loss: 3.182 | Reg loss: 0.039 | Tree loss: 3.182 | Accuracy: 0.298701 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 36 | Batch: 000 / 029 | Total loss: 3.713 | Reg loss: 0.037 | Tree loss: 3.713 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 029 | Total loss: 3.675 | Reg loss: 0.037 | Tree loss: 3.675 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 029 | Total loss: 3.665 | Reg loss: 0.037 | Tree loss: 3.665 | Accuracy: 0.328125 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 029 | Total loss: 3.667 | Reg loss: 0.037 | Tree loss: 3.667 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 029 | Total loss: 3.528 | Reg loss: 0.037 | Tree loss: 3.528 | Accuracy: 0.318359 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 029 | Total loss: 3.612 | Reg loss: 0.037 | Tree loss: 3.612 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 029 | Total loss: 3.482 | Reg loss: 0.037 | Tree loss: 3.482 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 029 | Total loss: 3.550 | Reg loss: 0.037 | Tree loss: 3.550 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 029 | Total loss: 3.446 | Reg loss: 0.037 | Tree loss: 3.446 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 029 | Total loss: 3.453 | Reg loss: 0.037 | Tree loss: 3.453 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 029 | Total loss: 3.466 | Reg loss: 0.037 | Tree loss: 3.466 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 029 | Total loss: 3.434 | Reg loss: 0.037 | Tree loss: 3.434 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 029 | Total loss: 3.395 | Reg loss: 0.037 | Tree loss: 3.395 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 029 | Total loss: 3.373 | Reg loss: 0.037 | Tree loss: 3.373 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 029 | Total loss: 3.406 | Reg loss: 0.037 | Tree loss: 3.406 | Accuracy: 0.226562 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 029 | Total loss: 3.324 | Reg loss: 0.037 | Tree loss: 3.324 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 029 | Total loss: 3.300 | Reg loss: 0.038 | Tree loss: 3.300 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 029 | Total loss: 3.316 | Reg loss: 0.038 | Tree loss: 3.316 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 029 | Total loss: 3.303 | Reg loss: 0.038 | Tree loss: 3.303 | Accuracy: 0.257812 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 029 | Total loss: 3.292 | Reg loss: 0.038 | Tree loss: 3.292 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 029 | Total loss: 3.209 | Reg loss: 0.038 | Tree loss: 3.209 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 029 | Total loss: 3.208 | Reg loss: 0.038 | Tree loss: 3.208 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 029 | Total loss: 3.158 | Reg loss: 0.038 | Tree loss: 3.158 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 029 | Total loss: 3.180 | Reg loss: 0.039 | Tree loss: 3.180 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 029 | Total loss: 3.125 | Reg loss: 0.039 | Tree loss: 3.125 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 029 | Total loss: 3.141 | Reg loss: 0.039 | Tree loss: 3.141 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 029 | Total loss: 3.126 | Reg loss: 0.039 | Tree loss: 3.126 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 029 | Total loss: 3.027 | Reg loss: 0.039 | Tree loss: 3.027 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 029 | Total loss: 3.206 | Reg loss: 0.039 | Tree loss: 3.206 | Accuracy: 0.253247 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 37 | Batch: 000 / 029 | Total loss: 3.582 | Reg loss: 0.037 | Tree loss: 3.582 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 029 | Total loss: 3.625 | Reg loss: 0.037 | Tree loss: 3.625 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 029 | Total loss: 3.571 | Reg loss: 0.037 | Tree loss: 3.571 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 029 | Total loss: 3.523 | Reg loss: 0.037 | Tree loss: 3.523 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 029 | Total loss: 3.463 | Reg loss: 0.037 | Tree loss: 3.463 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 029 | Total loss: 3.524 | Reg loss: 0.037 | Tree loss: 3.524 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 029 | Total loss: 3.389 | Reg loss: 0.037 | Tree loss: 3.389 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 029 | Total loss: 3.411 | Reg loss: 0.037 | Tree loss: 3.411 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 029 | Total loss: 3.392 | Reg loss: 0.037 | Tree loss: 3.392 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 029 | Total loss: 3.357 | Reg loss: 0.037 | Tree loss: 3.357 | Accuracy: 0.322266 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 029 | Total loss: 3.334 | Reg loss: 0.037 | Tree loss: 3.334 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 029 | Total loss: 3.306 | Reg loss: 0.037 | Tree loss: 3.306 | Accuracy: 0.244141 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 029 | Total loss: 3.219 | Reg loss: 0.037 | Tree loss: 3.219 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 029 | Total loss: 3.278 | Reg loss: 0.038 | Tree loss: 3.278 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 029 | Total loss: 3.197 | Reg loss: 0.038 | Tree loss: 3.197 | Accuracy: 0.322266 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 029 | Total loss: 3.235 | Reg loss: 0.038 | Tree loss: 3.235 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 029 | Total loss: 3.177 | Reg loss: 0.038 | Tree loss: 3.177 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 029 | Total loss: 3.160 | Reg loss: 0.038 | Tree loss: 3.160 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 029 | Total loss: 3.129 | Reg loss: 0.038 | Tree loss: 3.129 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 029 | Total loss: 3.154 | Reg loss: 0.038 | Tree loss: 3.154 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 029 | Total loss: 3.101 | Reg loss: 0.039 | Tree loss: 3.101 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 029 | Total loss: 3.087 | Reg loss: 0.039 | Tree loss: 3.087 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 029 | Total loss: 3.060 | Reg loss: 0.039 | Tree loss: 3.060 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 029 | Total loss: 3.023 | Reg loss: 0.039 | Tree loss: 3.023 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 029 | Total loss: 3.002 | Reg loss: 0.039 | Tree loss: 3.002 | Accuracy: 0.283203 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 025 / 029 | Total loss: 2.992 | Reg loss: 0.039 | Tree loss: 2.992 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 029 | Total loss: 2.992 | Reg loss: 0.039 | Tree loss: 2.992 | Accuracy: 0.251953 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 029 | Total loss: 2.997 | Reg loss: 0.040 | Tree loss: 2.997 | Accuracy: 0.251953 | 0.068 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 029 | Total loss: 2.969 | Reg loss: 0.040 | Tree loss: 2.969 | Accuracy: 0.233766 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 38 | Batch: 000 / 029 | Total loss: 3.491 | Reg loss: 0.037 | Tree loss: 3.491 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 029 | Total loss: 3.488 | Reg loss: 0.037 | Tree loss: 3.488 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 029 | Total loss: 3.460 | Reg loss: 0.037 | Tree loss: 3.460 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 029 | Total loss: 3.422 | Reg loss: 0.037 | Tree loss: 3.422 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 029 | Total loss: 3.365 | Reg loss: 0.037 | Tree loss: 3.365 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 029 | Total loss: 3.344 | Reg loss: 0.037 | Tree loss: 3.344 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 029 | Total loss: 3.374 | Reg loss: 0.037 | Tree loss: 3.374 | Accuracy: 0.246094 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 029 | Total loss: 3.248 | Reg loss: 0.037 | Tree loss: 3.248 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 029 | Total loss: 3.300 | Reg loss: 0.037 | Tree loss: 3.300 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 029 | Total loss: 3.251 | Reg loss: 0.038 | Tree loss: 3.251 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 029 | Total loss: 3.209 | Reg loss: 0.038 | Tree loss: 3.209 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 029 | Total loss: 3.142 | Reg loss: 0.038 | Tree loss: 3.142 | Accuracy: 0.312500 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 029 | Total loss: 3.263 | Reg loss: 0.038 | Tree loss: 3.263 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 029 | Total loss: 3.172 | Reg loss: 0.038 | Tree loss: 3.172 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 029 | Total loss: 3.129 | Reg loss: 0.038 | Tree loss: 3.129 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 029 | Total loss: 3.067 | Reg loss: 0.038 | Tree loss: 3.067 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 029 | Total loss: 3.007 | Reg loss: 0.038 | Tree loss: 3.007 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 029 | Total loss: 3.061 | Reg loss: 0.038 | Tree loss: 3.061 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 029 | Total loss: 3.046 | Reg loss: 0.039 | Tree loss: 3.046 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 029 | Total loss: 2.988 | Reg loss: 0.039 | Tree loss: 2.988 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 029 | Total loss: 3.013 | Reg loss: 0.039 | Tree loss: 3.013 | Accuracy: 0.246094 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 029 | Total loss: 2.986 | Reg loss: 0.039 | Tree loss: 2.986 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 029 | Total loss: 2.916 | Reg loss: 0.039 | Tree loss: 2.916 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 029 | Total loss: 2.945 | Reg loss: 0.039 | Tree loss: 2.945 | Accuracy: 0.232422 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 029 | Total loss: 2.916 | Reg loss: 0.039 | Tree loss: 2.916 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 029 | Total loss: 2.885 | Reg loss: 0.040 | Tree loss: 2.885 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 029 | Total loss: 2.915 | Reg loss: 0.040 | Tree loss: 2.915 | Accuracy: 0.257812 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 029 | Total loss: 2.866 | Reg loss: 0.040 | Tree loss: 2.866 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 029 | Total loss: 2.882 | Reg loss: 0.040 | Tree loss: 2.882 | Accuracy: 0.253247 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 39 | Batch: 000 / 029 | Total loss: 3.284 | Reg loss: 0.037 | Tree loss: 3.284 | Accuracy: 0.312500 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 029 | Total loss: 3.349 | Reg loss: 0.037 | Tree loss: 3.349 | Accuracy: 0.246094 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 029 | Total loss: 3.312 | Reg loss: 0.037 | Tree loss: 3.312 | Accuracy: 0.238281 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 029 | Total loss: 3.342 | Reg loss: 0.037 | Tree loss: 3.342 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 029 | Total loss: 3.225 | Reg loss: 0.038 | Tree loss: 3.225 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 029 | Total loss: 3.233 | Reg loss: 0.038 | Tree loss: 3.233 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 029 | Total loss: 3.267 | Reg loss: 0.038 | Tree loss: 3.267 | Accuracy: 0.251953 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 029 | Total loss: 3.147 | Reg loss: 0.038 | Tree loss: 3.147 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 029 | Total loss: 3.153 | Reg loss: 0.038 | Tree loss: 3.153 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 029 | Total loss: 3.101 | Reg loss: 0.038 | Tree loss: 3.101 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 029 | Total loss: 3.127 | Reg loss: 0.038 | Tree loss: 3.127 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 029 | Total loss: 3.138 | Reg loss: 0.038 | Tree loss: 3.138 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 029 | Total loss: 3.074 | Reg loss: 0.038 | Tree loss: 3.074 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 029 | Total loss: 3.051 | Reg loss: 0.038 | Tree loss: 3.051 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 029 | Total loss: 3.020 | Reg loss: 0.038 | Tree loss: 3.020 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 029 | Total loss: 3.077 | Reg loss: 0.038 | Tree loss: 3.077 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 029 | Total loss: 3.038 | Reg loss: 0.039 | Tree loss: 3.038 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 029 | Total loss: 2.946 | Reg loss: 0.039 | Tree loss: 2.946 | Accuracy: 0.337891 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 029 | Total loss: 2.949 | Reg loss: 0.039 | Tree loss: 2.949 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 029 | Total loss: 2.958 | Reg loss: 0.039 | Tree loss: 2.958 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 029 | Total loss: 2.874 | Reg loss: 0.039 | Tree loss: 2.874 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 029 | Total loss: 2.857 | Reg loss: 0.039 | Tree loss: 2.857 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 029 | Total loss: 2.918 | Reg loss: 0.039 | Tree loss: 2.918 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 029 | Total loss: 2.824 | Reg loss: 0.040 | Tree loss: 2.824 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 029 | Total loss: 2.829 | Reg loss: 0.040 | Tree loss: 2.829 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 029 | Total loss: 2.851 | Reg loss: 0.040 | Tree loss: 2.851 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 029 | Total loss: 2.772 | Reg loss: 0.040 | Tree loss: 2.772 | Accuracy: 0.251953 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 029 | Total loss: 2.836 | Reg loss: 0.040 | Tree loss: 2.836 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 029 | Total loss: 2.774 | Reg loss: 0.040 | Tree loss: 2.774 | Accuracy: 0.331169 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 40 | Batch: 000 / 029 | Total loss: 3.270 | Reg loss: 0.038 | Tree loss: 3.270 | Accuracy: 0.314453 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 001 / 029 | Total loss: 3.224 | Reg loss: 0.038 | Tree loss: 3.224 | Accuracy: 0.320312 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 029 | Total loss: 3.248 | Reg loss: 0.038 | Tree loss: 3.248 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 029 | Total loss: 3.189 | Reg loss: 0.038 | Tree loss: 3.189 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 029 | Total loss: 3.177 | Reg loss: 0.038 | Tree loss: 3.177 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 029 | Total loss: 3.226 | Reg loss: 0.038 | Tree loss: 3.226 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 029 | Total loss: 3.112 | Reg loss: 0.038 | Tree loss: 3.112 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 029 | Total loss: 3.049 | Reg loss: 0.038 | Tree loss: 3.049 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 029 | Total loss: 3.080 | Reg loss: 0.038 | Tree loss: 3.080 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 029 | Total loss: 3.041 | Reg loss: 0.038 | Tree loss: 3.041 | Accuracy: 0.257812 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 029 | Total loss: 3.079 | Reg loss: 0.038 | Tree loss: 3.079 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 029 | Total loss: 2.993 | Reg loss: 0.038 | Tree loss: 2.993 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 029 | Total loss: 2.963 | Reg loss: 0.038 | Tree loss: 2.963 | Accuracy: 0.322266 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 029 | Total loss: 2.988 | Reg loss: 0.039 | Tree loss: 2.988 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 029 | Total loss: 2.941 | Reg loss: 0.039 | Tree loss: 2.941 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 029 | Total loss: 2.941 | Reg loss: 0.039 | Tree loss: 2.941 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 029 | Total loss: 2.919 | Reg loss: 0.039 | Tree loss: 2.919 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 029 | Total loss: 2.891 | Reg loss: 0.039 | Tree loss: 2.891 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 029 | Total loss: 2.866 | Reg loss: 0.039 | Tree loss: 2.866 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 029 | Total loss: 2.866 | Reg loss: 0.039 | Tree loss: 2.866 | Accuracy: 0.244141 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 029 | Total loss: 2.781 | Reg loss: 0.039 | Tree loss: 2.781 | Accuracy: 0.318359 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 029 | Total loss: 2.815 | Reg loss: 0.040 | Tree loss: 2.815 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 029 | Total loss: 2.806 | Reg loss: 0.040 | Tree loss: 2.806 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 029 | Total loss: 2.757 | Reg loss: 0.040 | Tree loss: 2.757 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 029 | Total loss: 2.716 | Reg loss: 0.040 | Tree loss: 2.716 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 029 | Total loss: 2.641 | Reg loss: 0.040 | Tree loss: 2.641 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 029 | Total loss: 2.744 | Reg loss: 0.040 | Tree loss: 2.744 | Accuracy: 0.236328 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 029 | Total loss: 2.718 | Reg loss: 0.040 | Tree loss: 2.718 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 029 | Total loss: 2.703 | Reg loss: 0.040 | Tree loss: 2.703 | Accuracy: 0.324675 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 41 | Batch: 000 / 029 | Total loss: 3.213 | Reg loss: 0.038 | Tree loss: 3.213 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 029 | Total loss: 3.201 | Reg loss: 0.038 | Tree loss: 3.201 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 029 | Total loss: 3.176 | Reg loss: 0.038 | Tree loss: 3.176 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 029 | Total loss: 3.109 | Reg loss: 0.038 | Tree loss: 3.109 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 029 | Total loss: 3.079 | Reg loss: 0.038 | Tree loss: 3.079 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 029 | Total loss: 3.082 | Reg loss: 0.038 | Tree loss: 3.082 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 029 | Total loss: 3.034 | Reg loss: 0.038 | Tree loss: 3.034 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 029 | Total loss: 2.980 | Reg loss: 0.038 | Tree loss: 2.980 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 029 | Total loss: 2.945 | Reg loss: 0.038 | Tree loss: 2.945 | Accuracy: 0.326172 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 029 | Total loss: 2.985 | Reg loss: 0.038 | Tree loss: 2.985 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 029 | Total loss: 3.008 | Reg loss: 0.038 | Tree loss: 3.008 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 029 | Total loss: 2.904 | Reg loss: 0.039 | Tree loss: 2.904 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 029 | Total loss: 2.922 | Reg loss: 0.039 | Tree loss: 2.922 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 029 | Total loss: 2.827 | Reg loss: 0.039 | Tree loss: 2.827 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 029 | Total loss: 2.926 | Reg loss: 0.039 | Tree loss: 2.926 | Accuracy: 0.257812 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 029 | Total loss: 2.844 | Reg loss: 0.039 | Tree loss: 2.844 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 029 | Total loss: 2.804 | Reg loss: 0.039 | Tree loss: 2.804 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 029 | Total loss: 2.781 | Reg loss: 0.039 | Tree loss: 2.781 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 029 | Total loss: 2.748 | Reg loss: 0.039 | Tree loss: 2.748 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 029 | Total loss: 2.790 | Reg loss: 0.039 | Tree loss: 2.790 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 029 | Total loss: 2.732 | Reg loss: 0.040 | Tree loss: 2.732 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 029 | Total loss: 2.726 | Reg loss: 0.040 | Tree loss: 2.726 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 029 | Total loss: 2.730 | Reg loss: 0.040 | Tree loss: 2.730 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 029 | Total loss: 2.677 | Reg loss: 0.040 | Tree loss: 2.677 | Accuracy: 0.246094 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 029 | Total loss: 2.666 | Reg loss: 0.040 | Tree loss: 2.666 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 029 | Total loss: 2.667 | Reg loss: 0.040 | Tree loss: 2.667 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 029 | Total loss: 2.640 | Reg loss: 0.040 | Tree loss: 2.640 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 029 | Total loss: 2.584 | Reg loss: 0.040 | Tree loss: 2.584 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 029 | Total loss: 2.526 | Reg loss: 0.041 | Tree loss: 2.526 | Accuracy: 0.292208 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 42 | Batch: 000 / 029 | Total loss: 3.133 | Reg loss: 0.038 | Tree loss: 3.133 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 029 | Total loss: 3.129 | Reg loss: 0.038 | Tree loss: 3.129 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 029 | Total loss: 3.118 | Reg loss: 0.038 | Tree loss: 3.118 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 029 | Total loss: 3.064 | Reg loss: 0.038 | Tree loss: 3.064 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 029 | Total loss: 3.112 | Reg loss: 0.038 | Tree loss: 3.112 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 029 | Total loss: 3.006 | Reg loss: 0.038 | Tree loss: 3.006 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 029 | Total loss: 2.945 | Reg loss: 0.038 | Tree loss: 2.945 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 029 | Total loss: 3.010 | Reg loss: 0.038 | Tree loss: 3.010 | Accuracy: 0.242188 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 029 | Total loss: 2.917 | Reg loss: 0.038 | Tree loss: 2.917 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 029 | Total loss: 2.836 | Reg loss: 0.039 | Tree loss: 2.836 | Accuracy: 0.298828 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Batch: 010 / 029 | Total loss: 2.832 | Reg loss: 0.039 | Tree loss: 2.832 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 029 | Total loss: 2.822 | Reg loss: 0.039 | Tree loss: 2.822 | Accuracy: 0.259766 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 029 | Total loss: 2.782 | Reg loss: 0.039 | Tree loss: 2.782 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 029 | Total loss: 2.811 | Reg loss: 0.039 | Tree loss: 2.811 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 029 | Total loss: 2.759 | Reg loss: 0.039 | Tree loss: 2.759 | Accuracy: 0.304688 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 029 | Total loss: 2.781 | Reg loss: 0.039 | Tree loss: 2.781 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 029 | Total loss: 2.756 | Reg loss: 0.039 | Tree loss: 2.756 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 029 | Total loss: 2.701 | Reg loss: 0.039 | Tree loss: 2.701 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 029 | Total loss: 2.730 | Reg loss: 0.039 | Tree loss: 2.730 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 029 | Total loss: 2.678 | Reg loss: 0.040 | Tree loss: 2.678 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 029 | Total loss: 2.642 | Reg loss: 0.040 | Tree loss: 2.642 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 029 | Total loss: 2.676 | Reg loss: 0.040 | Tree loss: 2.676 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 029 | Total loss: 2.581 | Reg loss: 0.040 | Tree loss: 2.581 | Accuracy: 0.240234 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 029 | Total loss: 2.598 | Reg loss: 0.040 | Tree loss: 2.598 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 029 | Total loss: 2.563 | Reg loss: 0.040 | Tree loss: 2.563 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 029 | Total loss: 2.548 | Reg loss: 0.040 | Tree loss: 2.548 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 029 | Total loss: 2.558 | Reg loss: 0.040 | Tree loss: 2.558 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 029 | Total loss: 2.547 | Reg loss: 0.041 | Tree loss: 2.547 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 029 | Total loss: 2.644 | Reg loss: 0.041 | Tree loss: 2.644 | Accuracy: 0.246753 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 43 | Batch: 000 / 029 | Total loss: 3.038 | Reg loss: 0.038 | Tree loss: 3.038 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 029 | Total loss: 2.993 | Reg loss: 0.038 | Tree loss: 2.993 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 029 | Total loss: 3.034 | Reg loss: 0.038 | Tree loss: 3.034 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 029 | Total loss: 2.975 | Reg loss: 0.038 | Tree loss: 2.975 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 029 | Total loss: 2.927 | Reg loss: 0.038 | Tree loss: 2.927 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 029 | Total loss: 2.952 | Reg loss: 0.038 | Tree loss: 2.952 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 029 | Total loss: 2.961 | Reg loss: 0.038 | Tree loss: 2.961 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 029 | Total loss: 2.880 | Reg loss: 0.039 | Tree loss: 2.880 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 029 | Total loss: 2.847 | Reg loss: 0.039 | Tree loss: 2.847 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 029 | Total loss: 2.866 | Reg loss: 0.039 | Tree loss: 2.866 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 029 | Total loss: 2.819 | Reg loss: 0.039 | Tree loss: 2.819 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 029 | Total loss: 2.744 | Reg loss: 0.039 | Tree loss: 2.744 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 029 | Total loss: 2.703 | Reg loss: 0.039 | Tree loss: 2.703 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 029 | Total loss: 2.739 | Reg loss: 0.039 | Tree loss: 2.739 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 029 | Total loss: 2.740 | Reg loss: 0.039 | Tree loss: 2.740 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 029 | Total loss: 2.739 | Reg loss: 0.039 | Tree loss: 2.739 | Accuracy: 0.248047 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 029 | Total loss: 2.697 | Reg loss: 0.039 | Tree loss: 2.697 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 029 | Total loss: 2.630 | Reg loss: 0.039 | Tree loss: 2.630 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 029 | Total loss: 2.657 | Reg loss: 0.040 | Tree loss: 2.657 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 029 | Total loss: 2.678 | Reg loss: 0.040 | Tree loss: 2.678 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 029 | Total loss: 2.617 | Reg loss: 0.040 | Tree loss: 2.617 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 029 | Total loss: 2.571 | Reg loss: 0.040 | Tree loss: 2.571 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 029 | Total loss: 2.561 | Reg loss: 0.040 | Tree loss: 2.561 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 029 | Total loss: 2.497 | Reg loss: 0.040 | Tree loss: 2.497 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 029 | Total loss: 2.498 | Reg loss: 0.040 | Tree loss: 2.498 | Accuracy: 0.250000 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 029 | Total loss: 2.511 | Reg loss: 0.040 | Tree loss: 2.511 | Accuracy: 0.232422 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 029 | Total loss: 2.509 | Reg loss: 0.040 | Tree loss: 2.509 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 029 | Total loss: 2.423 | Reg loss: 0.041 | Tree loss: 2.423 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 029 | Total loss: 2.427 | Reg loss: 0.041 | Tree loss: 2.427 | Accuracy: 0.259740 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 44 | Batch: 000 / 029 | Total loss: 2.971 | Reg loss: 0.038 | Tree loss: 2.971 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 029 | Total loss: 2.989 | Reg loss: 0.038 | Tree loss: 2.989 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 029 | Total loss: 2.915 | Reg loss: 0.038 | Tree loss: 2.915 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 029 | Total loss: 2.911 | Reg loss: 0.038 | Tree loss: 2.911 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 029 | Total loss: 2.850 | Reg loss: 0.038 | Tree loss: 2.850 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 029 | Total loss: 2.858 | Reg loss: 0.039 | Tree loss: 2.858 | Accuracy: 0.312500 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 029 | Total loss: 2.801 | Reg loss: 0.039 | Tree loss: 2.801 | Accuracy: 0.283203 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 029 | Total loss: 2.849 | Reg loss: 0.039 | Tree loss: 2.849 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 029 | Total loss: 2.784 | Reg loss: 0.039 | Tree loss: 2.784 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 029 | Total loss: 2.756 | Reg loss: 0.039 | Tree loss: 2.756 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 029 | Total loss: 2.694 | Reg loss: 0.039 | Tree loss: 2.694 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 029 | Total loss: 2.679 | Reg loss: 0.039 | Tree loss: 2.679 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 029 | Total loss: 2.696 | Reg loss: 0.039 | Tree loss: 2.696 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 029 | Total loss: 2.682 | Reg loss: 0.039 | Tree loss: 2.682 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 029 | Total loss: 2.746 | Reg loss: 0.039 | Tree loss: 2.746 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 029 | Total loss: 2.621 | Reg loss: 0.039 | Tree loss: 2.621 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 029 | Total loss: 2.627 | Reg loss: 0.039 | Tree loss: 2.627 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 029 | Total loss: 2.560 | Reg loss: 0.040 | Tree loss: 2.560 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 029 | Total loss: 2.587 | Reg loss: 0.040 | Tree loss: 2.587 | Accuracy: 0.289062 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 019 / 029 | Total loss: 2.516 | Reg loss: 0.040 | Tree loss: 2.516 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 029 | Total loss: 2.475 | Reg loss: 0.040 | Tree loss: 2.475 | Accuracy: 0.306641 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 029 | Total loss: 2.570 | Reg loss: 0.040 | Tree loss: 2.570 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 029 | Total loss: 2.485 | Reg loss: 0.040 | Tree loss: 2.485 | Accuracy: 0.287109 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 029 | Total loss: 2.523 | Reg loss: 0.040 | Tree loss: 2.523 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 029 | Total loss: 2.490 | Reg loss: 0.040 | Tree loss: 2.490 | Accuracy: 0.273438 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 029 | Total loss: 2.512 | Reg loss: 0.040 | Tree loss: 2.512 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 029 | Total loss: 2.457 | Reg loss: 0.040 | Tree loss: 2.457 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 029 | Total loss: 2.460 | Reg loss: 0.041 | Tree loss: 2.460 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 029 | Total loss: 2.488 | Reg loss: 0.041 | Tree loss: 2.488 | Accuracy: 0.344156 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 45 | Batch: 000 / 029 | Total loss: 2.977 | Reg loss: 0.039 | Tree loss: 2.977 | Accuracy: 0.230469 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 029 | Total loss: 2.853 | Reg loss: 0.039 | Tree loss: 2.853 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 029 | Total loss: 2.809 | Reg loss: 0.039 | Tree loss: 2.809 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 029 | Total loss: 2.845 | Reg loss: 0.039 | Tree loss: 2.845 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 029 | Total loss: 2.825 | Reg loss: 0.039 | Tree loss: 2.825 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 029 | Total loss: 2.791 | Reg loss: 0.039 | Tree loss: 2.791 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 029 | Total loss: 2.798 | Reg loss: 0.039 | Tree loss: 2.798 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 029 | Total loss: 2.781 | Reg loss: 0.039 | Tree loss: 2.781 | Accuracy: 0.308594 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 029 | Total loss: 2.776 | Reg loss: 0.039 | Tree loss: 2.776 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 029 | Total loss: 2.718 | Reg loss: 0.039 | Tree loss: 2.718 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 029 | Total loss: 2.711 | Reg loss: 0.039 | Tree loss: 2.711 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 029 | Total loss: 2.714 | Reg loss: 0.039 | Tree loss: 2.714 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 029 | Total loss: 2.586 | Reg loss: 0.039 | Tree loss: 2.586 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 029 | Total loss: 2.662 | Reg loss: 0.039 | Tree loss: 2.662 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 029 | Total loss: 2.582 | Reg loss: 0.039 | Tree loss: 2.582 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 029 | Total loss: 2.564 | Reg loss: 0.039 | Tree loss: 2.564 | Accuracy: 0.240234 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 029 | Total loss: 2.538 | Reg loss: 0.039 | Tree loss: 2.538 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 029 | Total loss: 2.520 | Reg loss: 0.040 | Tree loss: 2.520 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 029 | Total loss: 2.478 | Reg loss: 0.040 | Tree loss: 2.478 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 029 | Total loss: 2.512 | Reg loss: 0.040 | Tree loss: 2.512 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 029 | Total loss: 2.532 | Reg loss: 0.040 | Tree loss: 2.532 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 029 | Total loss: 2.476 | Reg loss: 0.040 | Tree loss: 2.476 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 029 | Total loss: 2.480 | Reg loss: 0.040 | Tree loss: 2.480 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 029 | Total loss: 2.438 | Reg loss: 0.040 | Tree loss: 2.438 | Accuracy: 0.314453 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 029 | Total loss: 2.413 | Reg loss: 0.040 | Tree loss: 2.413 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 029 | Total loss: 2.397 | Reg loss: 0.040 | Tree loss: 2.397 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 029 | Total loss: 2.432 | Reg loss: 0.041 | Tree loss: 2.432 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 029 | Total loss: 2.362 | Reg loss: 0.041 | Tree loss: 2.362 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 029 | Total loss: 2.216 | Reg loss: 0.041 | Tree loss: 2.216 | Accuracy: 0.331169 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 46 | Batch: 000 / 029 | Total loss: 2.885 | Reg loss: 0.039 | Tree loss: 2.885 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 029 | Total loss: 2.836 | Reg loss: 0.039 | Tree loss: 2.836 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 029 | Total loss: 2.799 | Reg loss: 0.039 | Tree loss: 2.799 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 029 | Total loss: 2.725 | Reg loss: 0.039 | Tree loss: 2.725 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 029 | Total loss: 2.723 | Reg loss: 0.039 | Tree loss: 2.723 | Accuracy: 0.306641 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 029 | Total loss: 2.788 | Reg loss: 0.039 | Tree loss: 2.788 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 029 | Total loss: 2.772 | Reg loss: 0.039 | Tree loss: 2.772 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 029 | Total loss: 2.703 | Reg loss: 0.039 | Tree loss: 2.703 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 029 | Total loss: 2.725 | Reg loss: 0.039 | Tree loss: 2.725 | Accuracy: 0.236328 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 029 | Total loss: 2.677 | Reg loss: 0.039 | Tree loss: 2.677 | Accuracy: 0.279297 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 029 | Total loss: 2.668 | Reg loss: 0.039 | Tree loss: 2.668 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 029 | Total loss: 2.604 | Reg loss: 0.039 | Tree loss: 2.604 | Accuracy: 0.298828 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 029 | Total loss: 2.599 | Reg loss: 0.039 | Tree loss: 2.599 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 029 | Total loss: 2.538 | Reg loss: 0.039 | Tree loss: 2.538 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 029 | Total loss: 2.622 | Reg loss: 0.039 | Tree loss: 2.622 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 029 | Total loss: 2.584 | Reg loss: 0.039 | Tree loss: 2.584 | Accuracy: 0.281250 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 029 | Total loss: 2.561 | Reg loss: 0.039 | Tree loss: 2.561 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 029 | Total loss: 2.462 | Reg loss: 0.040 | Tree loss: 2.462 | Accuracy: 0.300781 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 029 | Total loss: 2.458 | Reg loss: 0.040 | Tree loss: 2.458 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 029 | Total loss: 2.505 | Reg loss: 0.040 | Tree loss: 2.505 | Accuracy: 0.263672 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 029 | Total loss: 2.394 | Reg loss: 0.040 | Tree loss: 2.394 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 029 | Total loss: 2.377 | Reg loss: 0.040 | Tree loss: 2.377 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 029 | Total loss: 2.385 | Reg loss: 0.040 | Tree loss: 2.385 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 029 | Total loss: 2.405 | Reg loss: 0.040 | Tree loss: 2.405 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 029 | Total loss: 2.324 | Reg loss: 0.040 | Tree loss: 2.324 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 029 | Total loss: 2.384 | Reg loss: 0.040 | Tree loss: 2.384 | Accuracy: 0.294922 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 029 | Total loss: 2.319 | Reg loss: 0.041 | Tree loss: 2.319 | Accuracy: 0.265625 | 0.068 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 029 | Total loss: 2.323 | Reg loss: 0.041 | Tree loss: 2.323 | Accuracy: 0.322266 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 028 / 029 | Total loss: 2.317 | Reg loss: 0.041 | Tree loss: 2.317 | Accuracy: 0.298701 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 47 | Batch: 000 / 029 | Total loss: 2.826 | Reg loss: 0.039 | Tree loss: 2.826 | Accuracy: 0.269531 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 029 | Total loss: 2.728 | Reg loss: 0.039 | Tree loss: 2.728 | Accuracy: 0.318359 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 029 | Total loss: 2.739 | Reg loss: 0.039 | Tree loss: 2.739 | Accuracy: 0.271484 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 029 | Total loss: 2.699 | Reg loss: 0.039 | Tree loss: 2.699 | Accuracy: 0.291016 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 029 | Total loss: 2.698 | Reg loss: 0.039 | Tree loss: 2.698 | Accuracy: 0.310547 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 029 | Total loss: 2.680 | Reg loss: 0.039 | Tree loss: 2.680 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 029 | Total loss: 2.659 | Reg loss: 0.039 | Tree loss: 2.659 | Accuracy: 0.277344 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 029 | Total loss: 2.609 | Reg loss: 0.039 | Tree loss: 2.609 | Accuracy: 0.306641 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 029 | Total loss: 2.688 | Reg loss: 0.039 | Tree loss: 2.688 | Accuracy: 0.292969 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 029 | Total loss: 2.692 | Reg loss: 0.039 | Tree loss: 2.692 | Accuracy: 0.261719 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 029 | Total loss: 2.535 | Reg loss: 0.039 | Tree loss: 2.535 | Accuracy: 0.296875 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 029 | Total loss: 2.593 | Reg loss: 0.039 | Tree loss: 2.593 | Accuracy: 0.330078 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 029 | Total loss: 2.550 | Reg loss: 0.039 | Tree loss: 2.550 | Accuracy: 0.275391 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 029 | Total loss: 2.512 | Reg loss: 0.039 | Tree loss: 2.512 | Accuracy: 0.306641 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 029 | Total loss: 2.479 | Reg loss: 0.039 | Tree loss: 2.479 | Accuracy: 0.302734 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 029 | Total loss: 2.495 | Reg loss: 0.039 | Tree loss: 2.495 | Accuracy: 0.253906 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 029 | Total loss: 2.463 | Reg loss: 0.039 | Tree loss: 2.463 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 029 | Total loss: 2.455 | Reg loss: 0.040 | Tree loss: 2.455 | Accuracy: 0.285156 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 029 | Total loss: 2.420 | Reg loss: 0.040 | Tree loss: 2.420 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 029 | Total loss: 2.401 | Reg loss: 0.040 | Tree loss: 2.401 | Accuracy: 0.328125 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 029 | Total loss: 2.393 | Reg loss: 0.040 | Tree loss: 2.393 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 029 | Total loss: 2.333 | Reg loss: 0.040 | Tree loss: 2.333 | Accuracy: 0.267578 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 029 | Total loss: 2.341 | Reg loss: 0.040 | Tree loss: 2.341 | Accuracy: 0.289062 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 029 | Total loss: 2.415 | Reg loss: 0.040 | Tree loss: 2.415 | Accuracy: 0.248047 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 029 | Total loss: 2.400 | Reg loss: 0.040 | Tree loss: 2.400 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 029 | Total loss: 2.391 | Reg loss: 0.040 | Tree loss: 2.391 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 029 | Total loss: 2.300 | Reg loss: 0.040 | Tree loss: 2.300 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 029 | Total loss: 2.329 | Reg loss: 0.041 | Tree loss: 2.329 | Accuracy: 0.255859 | 0.068 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 029 | Total loss: 2.306 | Reg loss: 0.041 | Tree loss: 2.306 | Accuracy: 0.337662 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 48 | Batch: 000 / 029 | Total loss: 2.736 | Reg loss: 0.039 | Tree loss: 2.736 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 029 | Total loss: 2.736 | Reg loss: 0.039 | Tree loss: 2.736 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 029 | Total loss: 2.718 | Reg loss: 0.039 | Tree loss: 2.718 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 029 | Total loss: 2.683 | Reg loss: 0.039 | Tree loss: 2.683 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 029 | Total loss: 2.631 | Reg loss: 0.039 | Tree loss: 2.631 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 029 | Total loss: 2.585 | Reg loss: 0.039 | Tree loss: 2.585 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 029 | Total loss: 2.688 | Reg loss: 0.039 | Tree loss: 2.688 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 029 | Total loss: 2.574 | Reg loss: 0.039 | Tree loss: 2.574 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 029 | Total loss: 2.560 | Reg loss: 0.039 | Tree loss: 2.560 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 029 | Total loss: 2.567 | Reg loss: 0.039 | Tree loss: 2.567 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 029 | Total loss: 2.591 | Reg loss: 0.039 | Tree loss: 2.591 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 029 | Total loss: 2.491 | Reg loss: 0.039 | Tree loss: 2.491 | Accuracy: 0.330078 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 029 | Total loss: 2.461 | Reg loss: 0.039 | Tree loss: 2.461 | Accuracy: 0.230469 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 029 | Total loss: 2.485 | Reg loss: 0.039 | Tree loss: 2.485 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 029 | Total loss: 2.470 | Reg loss: 0.039 | Tree loss: 2.470 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 029 | Total loss: 2.491 | Reg loss: 0.039 | Tree loss: 2.491 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 029 | Total loss: 2.510 | Reg loss: 0.039 | Tree loss: 2.510 | Accuracy: 0.248047 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 029 | Total loss: 2.400 | Reg loss: 0.040 | Tree loss: 2.400 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 029 | Total loss: 2.387 | Reg loss: 0.040 | Tree loss: 2.387 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 029 | Total loss: 2.332 | Reg loss: 0.040 | Tree loss: 2.332 | Accuracy: 0.343750 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 029 | Total loss: 2.363 | Reg loss: 0.040 | Tree loss: 2.363 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 029 | Total loss: 2.357 | Reg loss: 0.040 | Tree loss: 2.357 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 029 | Total loss: 2.356 | Reg loss: 0.040 | Tree loss: 2.356 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 029 | Total loss: 2.324 | Reg loss: 0.040 | Tree loss: 2.324 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 029 | Total loss: 2.349 | Reg loss: 0.040 | Tree loss: 2.349 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 029 | Total loss: 2.326 | Reg loss: 0.040 | Tree loss: 2.326 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 029 | Total loss: 2.242 | Reg loss: 0.040 | Tree loss: 2.242 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 029 | Total loss: 2.275 | Reg loss: 0.040 | Tree loss: 2.275 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 029 | Total loss: 2.199 | Reg loss: 0.041 | Tree loss: 2.199 | Accuracy: 0.272727 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 49 | Batch: 000 / 029 | Total loss: 2.703 | Reg loss: 0.039 | Tree loss: 2.703 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 029 | Total loss: 2.690 | Reg loss: 0.039 | Tree loss: 2.690 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 029 | Total loss: 2.642 | Reg loss: 0.039 | Tree loss: 2.642 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 029 | Total loss: 2.642 | Reg loss: 0.039 | Tree loss: 2.642 | Accuracy: 0.310547 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 004 / 029 | Total loss: 2.581 | Reg loss: 0.039 | Tree loss: 2.581 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 029 | Total loss: 2.575 | Reg loss: 0.039 | Tree loss: 2.575 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 029 | Total loss: 2.596 | Reg loss: 0.039 | Tree loss: 2.596 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 029 | Total loss: 2.556 | Reg loss: 0.039 | Tree loss: 2.556 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 029 | Total loss: 2.536 | Reg loss: 0.039 | Tree loss: 2.536 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 029 | Total loss: 2.548 | Reg loss: 0.039 | Tree loss: 2.548 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 029 | Total loss: 2.530 | Reg loss: 0.039 | Tree loss: 2.530 | Accuracy: 0.246094 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 029 | Total loss: 2.514 | Reg loss: 0.039 | Tree loss: 2.514 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 029 | Total loss: 2.443 | Reg loss: 0.039 | Tree loss: 2.443 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 029 | Total loss: 2.421 | Reg loss: 0.039 | Tree loss: 2.421 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 029 | Total loss: 2.400 | Reg loss: 0.039 | Tree loss: 2.400 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 029 | Total loss: 2.413 | Reg loss: 0.039 | Tree loss: 2.413 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 029 | Total loss: 2.400 | Reg loss: 0.039 | Tree loss: 2.400 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 029 | Total loss: 2.380 | Reg loss: 0.039 | Tree loss: 2.380 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 029 | Total loss: 2.381 | Reg loss: 0.040 | Tree loss: 2.381 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 029 | Total loss: 2.373 | Reg loss: 0.040 | Tree loss: 2.373 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 029 | Total loss: 2.337 | Reg loss: 0.040 | Tree loss: 2.337 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 029 | Total loss: 2.317 | Reg loss: 0.040 | Tree loss: 2.317 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 029 | Total loss: 2.320 | Reg loss: 0.040 | Tree loss: 2.320 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 029 | Total loss: 2.301 | Reg loss: 0.040 | Tree loss: 2.301 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 029 | Total loss: 2.237 | Reg loss: 0.040 | Tree loss: 2.237 | Accuracy: 0.244141 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 029 | Total loss: 2.222 | Reg loss: 0.040 | Tree loss: 2.222 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 029 | Total loss: 2.271 | Reg loss: 0.040 | Tree loss: 2.271 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 029 | Total loss: 2.302 | Reg loss: 0.040 | Tree loss: 2.302 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 029 | Total loss: 2.160 | Reg loss: 0.040 | Tree loss: 2.160 | Accuracy: 0.292208 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 50 | Batch: 000 / 029 | Total loss: 2.694 | Reg loss: 0.038 | Tree loss: 2.694 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 029 | Total loss: 2.608 | Reg loss: 0.038 | Tree loss: 2.608 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 029 | Total loss: 2.660 | Reg loss: 0.038 | Tree loss: 2.660 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 029 | Total loss: 2.576 | Reg loss: 0.038 | Tree loss: 2.576 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 029 | Total loss: 2.701 | Reg loss: 0.039 | Tree loss: 2.701 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 029 | Total loss: 2.555 | Reg loss: 0.039 | Tree loss: 2.555 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 029 | Total loss: 2.565 | Reg loss: 0.039 | Tree loss: 2.565 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 029 | Total loss: 2.450 | Reg loss: 0.039 | Tree loss: 2.450 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 029 | Total loss: 2.453 | Reg loss: 0.039 | Tree loss: 2.453 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 029 | Total loss: 2.488 | Reg loss: 0.039 | Tree loss: 2.488 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 029 | Total loss: 2.445 | Reg loss: 0.039 | Tree loss: 2.445 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 029 | Total loss: 2.490 | Reg loss: 0.039 | Tree loss: 2.490 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 029 | Total loss: 2.406 | Reg loss: 0.039 | Tree loss: 2.406 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 029 | Total loss: 2.420 | Reg loss: 0.039 | Tree loss: 2.420 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 029 | Total loss: 2.380 | Reg loss: 0.039 | Tree loss: 2.380 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 029 | Total loss: 2.362 | Reg loss: 0.039 | Tree loss: 2.362 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 029 | Total loss: 2.405 | Reg loss: 0.039 | Tree loss: 2.405 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 029 | Total loss: 2.314 | Reg loss: 0.039 | Tree loss: 2.314 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 029 | Total loss: 2.352 | Reg loss: 0.039 | Tree loss: 2.352 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 029 | Total loss: 2.291 | Reg loss: 0.040 | Tree loss: 2.291 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 029 | Total loss: 2.269 | Reg loss: 0.040 | Tree loss: 2.269 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 029 | Total loss: 2.266 | Reg loss: 0.040 | Tree loss: 2.266 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 029 | Total loss: 2.211 | Reg loss: 0.040 | Tree loss: 2.211 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 029 | Total loss: 2.223 | Reg loss: 0.040 | Tree loss: 2.223 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 029 | Total loss: 2.323 | Reg loss: 0.040 | Tree loss: 2.323 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 029 | Total loss: 2.272 | Reg loss: 0.040 | Tree loss: 2.272 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 029 | Total loss: 2.204 | Reg loss: 0.040 | Tree loss: 2.204 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 029 | Total loss: 2.221 | Reg loss: 0.040 | Tree loss: 2.221 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 029 | Total loss: 2.171 | Reg loss: 0.040 | Tree loss: 2.171 | Accuracy: 0.253247 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 51 | Batch: 000 / 029 | Total loss: 2.669 | Reg loss: 0.038 | Tree loss: 2.669 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 029 | Total loss: 2.645 | Reg loss: 0.038 | Tree loss: 2.645 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 029 | Total loss: 2.588 | Reg loss: 0.038 | Tree loss: 2.588 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 029 | Total loss: 2.654 | Reg loss: 0.038 | Tree loss: 2.654 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 029 | Total loss: 2.495 | Reg loss: 0.038 | Tree loss: 2.495 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 029 | Total loss: 2.516 | Reg loss: 0.038 | Tree loss: 2.516 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 029 | Total loss: 2.508 | Reg loss: 0.038 | Tree loss: 2.508 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 029 | Total loss: 2.516 | Reg loss: 0.039 | Tree loss: 2.516 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 029 | Total loss: 2.472 | Reg loss: 0.039 | Tree loss: 2.472 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 029 | Total loss: 2.415 | Reg loss: 0.039 | Tree loss: 2.415 | Accuracy: 0.277344 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 010 / 029 | Total loss: 2.366 | Reg loss: 0.039 | Tree loss: 2.366 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 029 | Total loss: 2.424 | Reg loss: 0.039 | Tree loss: 2.424 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 029 | Total loss: 2.386 | Reg loss: 0.039 | Tree loss: 2.386 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 029 | Total loss: 2.444 | Reg loss: 0.039 | Tree loss: 2.444 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 029 | Total loss: 2.329 | Reg loss: 0.039 | Tree loss: 2.329 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 029 | Total loss: 2.340 | Reg loss: 0.039 | Tree loss: 2.340 | Accuracy: 0.248047 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 029 | Total loss: 2.330 | Reg loss: 0.039 | Tree loss: 2.330 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 029 | Total loss: 2.329 | Reg loss: 0.039 | Tree loss: 2.329 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 029 | Total loss: 2.275 | Reg loss: 0.039 | Tree loss: 2.275 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 029 | Total loss: 2.289 | Reg loss: 0.039 | Tree loss: 2.289 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 029 | Total loss: 2.261 | Reg loss: 0.040 | Tree loss: 2.261 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 029 | Total loss: 2.250 | Reg loss: 0.040 | Tree loss: 2.250 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 029 | Total loss: 2.236 | Reg loss: 0.040 | Tree loss: 2.236 | Accuracy: 0.337891 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 029 | Total loss: 2.183 | Reg loss: 0.040 | Tree loss: 2.183 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 029 | Total loss: 2.201 | Reg loss: 0.040 | Tree loss: 2.201 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 029 | Total loss: 2.200 | Reg loss: 0.040 | Tree loss: 2.200 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 029 | Total loss: 2.168 | Reg loss: 0.040 | Tree loss: 2.168 | Accuracy: 0.333984 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 029 | Total loss: 2.186 | Reg loss: 0.040 | Tree loss: 2.186 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 029 | Total loss: 2.297 | Reg loss: 0.040 | Tree loss: 2.297 | Accuracy: 0.285714 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 52 | Batch: 000 / 029 | Total loss: 2.533 | Reg loss: 0.038 | Tree loss: 2.533 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 029 | Total loss: 2.656 | Reg loss: 0.038 | Tree loss: 2.656 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 029 | Total loss: 2.488 | Reg loss: 0.038 | Tree loss: 2.488 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 029 | Total loss: 2.500 | Reg loss: 0.038 | Tree loss: 2.500 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 029 | Total loss: 2.510 | Reg loss: 0.038 | Tree loss: 2.510 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 029 | Total loss: 2.514 | Reg loss: 0.038 | Tree loss: 2.514 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 029 | Total loss: 2.480 | Reg loss: 0.038 | Tree loss: 2.480 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 029 | Total loss: 2.474 | Reg loss: 0.038 | Tree loss: 2.474 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 029 | Total loss: 2.413 | Reg loss: 0.039 | Tree loss: 2.413 | Accuracy: 0.335938 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 029 | Total loss: 2.401 | Reg loss: 0.039 | Tree loss: 2.401 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 029 | Total loss: 2.391 | Reg loss: 0.039 | Tree loss: 2.391 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 029 | Total loss: 2.354 | Reg loss: 0.039 | Tree loss: 2.354 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 029 | Total loss: 2.416 | Reg loss: 0.039 | Tree loss: 2.416 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 029 | Total loss: 2.350 | Reg loss: 0.039 | Tree loss: 2.350 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 029 | Total loss: 2.295 | Reg loss: 0.039 | Tree loss: 2.295 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 029 | Total loss: 2.295 | Reg loss: 0.039 | Tree loss: 2.295 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 029 | Total loss: 2.327 | Reg loss: 0.039 | Tree loss: 2.327 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 029 | Total loss: 2.300 | Reg loss: 0.039 | Tree loss: 2.300 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 029 | Total loss: 2.327 | Reg loss: 0.039 | Tree loss: 2.327 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 029 | Total loss: 2.216 | Reg loss: 0.039 | Tree loss: 2.216 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 029 | Total loss: 2.249 | Reg loss: 0.040 | Tree loss: 2.249 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 029 | Total loss: 2.195 | Reg loss: 0.040 | Tree loss: 2.195 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 029 | Total loss: 2.231 | Reg loss: 0.040 | Tree loss: 2.231 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 029 | Total loss: 2.170 | Reg loss: 0.040 | Tree loss: 2.170 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 029 | Total loss: 2.223 | Reg loss: 0.040 | Tree loss: 2.223 | Accuracy: 0.236328 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 029 | Total loss: 2.270 | Reg loss: 0.040 | Tree loss: 2.270 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 029 | Total loss: 2.123 | Reg loss: 0.040 | Tree loss: 2.123 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 029 | Total loss: 2.152 | Reg loss: 0.040 | Tree loss: 2.152 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 029 | Total loss: 2.254 | Reg loss: 0.040 | Tree loss: 2.254 | Accuracy: 0.240260 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 53 | Batch: 000 / 029 | Total loss: 2.531 | Reg loss: 0.038 | Tree loss: 2.531 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 029 | Total loss: 2.539 | Reg loss: 0.038 | Tree loss: 2.539 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 029 | Total loss: 2.453 | Reg loss: 0.038 | Tree loss: 2.453 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 029 | Total loss: 2.489 | Reg loss: 0.038 | Tree loss: 2.489 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 029 | Total loss: 2.507 | Reg loss: 0.038 | Tree loss: 2.507 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 029 | Total loss: 2.510 | Reg loss: 0.038 | Tree loss: 2.510 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 029 | Total loss: 2.394 | Reg loss: 0.038 | Tree loss: 2.394 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 029 | Total loss: 2.445 | Reg loss: 0.038 | Tree loss: 2.445 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 029 | Total loss: 2.380 | Reg loss: 0.038 | Tree loss: 2.380 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 029 | Total loss: 2.300 | Reg loss: 0.039 | Tree loss: 2.300 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 029 | Total loss: 2.373 | Reg loss: 0.039 | Tree loss: 2.373 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 029 | Total loss: 2.368 | Reg loss: 0.039 | Tree loss: 2.368 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 029 | Total loss: 2.387 | Reg loss: 0.039 | Tree loss: 2.387 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 029 | Total loss: 2.314 | Reg loss: 0.039 | Tree loss: 2.314 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 029 | Total loss: 2.285 | Reg loss: 0.039 | Tree loss: 2.285 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 029 | Total loss: 2.313 | Reg loss: 0.039 | Tree loss: 2.313 | Accuracy: 0.279297 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 016 / 029 | Total loss: 2.278 | Reg loss: 0.039 | Tree loss: 2.278 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 029 | Total loss: 2.253 | Reg loss: 0.039 | Tree loss: 2.253 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 029 | Total loss: 2.193 | Reg loss: 0.039 | Tree loss: 2.193 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 029 | Total loss: 2.191 | Reg loss: 0.039 | Tree loss: 2.191 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 029 | Total loss: 2.241 | Reg loss: 0.039 | Tree loss: 2.241 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 029 | Total loss: 2.190 | Reg loss: 0.040 | Tree loss: 2.190 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 029 | Total loss: 2.219 | Reg loss: 0.040 | Tree loss: 2.219 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 029 | Total loss: 2.168 | Reg loss: 0.040 | Tree loss: 2.168 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 029 | Total loss: 2.234 | Reg loss: 0.040 | Tree loss: 2.234 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 029 | Total loss: 2.191 | Reg loss: 0.040 | Tree loss: 2.191 | Accuracy: 0.238281 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 029 | Total loss: 2.185 | Reg loss: 0.040 | Tree loss: 2.185 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 029 | Total loss: 2.160 | Reg loss: 0.040 | Tree loss: 2.160 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 029 | Total loss: 2.146 | Reg loss: 0.040 | Tree loss: 2.146 | Accuracy: 0.272727 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 54 | Batch: 000 / 029 | Total loss: 2.549 | Reg loss: 0.038 | Tree loss: 2.549 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 029 | Total loss: 2.470 | Reg loss: 0.038 | Tree loss: 2.470 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 029 | Total loss: 2.506 | Reg loss: 0.038 | Tree loss: 2.506 | Accuracy: 0.246094 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 029 | Total loss: 2.475 | Reg loss: 0.038 | Tree loss: 2.475 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 029 | Total loss: 2.457 | Reg loss: 0.038 | Tree loss: 2.457 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 029 | Total loss: 2.451 | Reg loss: 0.038 | Tree loss: 2.451 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 029 | Total loss: 2.380 | Reg loss: 0.038 | Tree loss: 2.380 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 029 | Total loss: 2.467 | Reg loss: 0.038 | Tree loss: 2.467 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 029 | Total loss: 2.365 | Reg loss: 0.038 | Tree loss: 2.365 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 029 | Total loss: 2.313 | Reg loss: 0.038 | Tree loss: 2.313 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 029 | Total loss: 2.360 | Reg loss: 0.038 | Tree loss: 2.360 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 029 | Total loss: 2.369 | Reg loss: 0.039 | Tree loss: 2.369 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 029 | Total loss: 2.241 | Reg loss: 0.039 | Tree loss: 2.241 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 029 | Total loss: 2.280 | Reg loss: 0.039 | Tree loss: 2.280 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 029 | Total loss: 2.289 | Reg loss: 0.039 | Tree loss: 2.289 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 029 | Total loss: 2.309 | Reg loss: 0.039 | Tree loss: 2.309 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 029 | Total loss: 2.228 | Reg loss: 0.039 | Tree loss: 2.228 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 029 | Total loss: 2.267 | Reg loss: 0.039 | Tree loss: 2.267 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 029 | Total loss: 2.172 | Reg loss: 0.039 | Tree loss: 2.172 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 029 | Total loss: 2.195 | Reg loss: 0.039 | Tree loss: 2.195 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 029 | Total loss: 2.260 | Reg loss: 0.039 | Tree loss: 2.260 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 029 | Total loss: 2.166 | Reg loss: 0.039 | Tree loss: 2.166 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 029 | Total loss: 2.190 | Reg loss: 0.039 | Tree loss: 2.190 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 029 | Total loss: 2.152 | Reg loss: 0.040 | Tree loss: 2.152 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 029 | Total loss: 2.120 | Reg loss: 0.040 | Tree loss: 2.120 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 029 | Total loss: 2.104 | Reg loss: 0.040 | Tree loss: 2.104 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 029 | Total loss: 2.148 | Reg loss: 0.040 | Tree loss: 2.148 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 029 | Total loss: 2.142 | Reg loss: 0.040 | Tree loss: 2.142 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 029 | Total loss: 2.037 | Reg loss: 0.040 | Tree loss: 2.037 | Accuracy: 0.311688 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 55 | Batch: 000 / 029 | Total loss: 2.462 | Reg loss: 0.038 | Tree loss: 2.462 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 029 | Total loss: 2.535 | Reg loss: 0.038 | Tree loss: 2.535 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 029 | Total loss: 2.486 | Reg loss: 0.038 | Tree loss: 2.486 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 029 | Total loss: 2.445 | Reg loss: 0.038 | Tree loss: 2.445 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 029 | Total loss: 2.429 | Reg loss: 0.038 | Tree loss: 2.429 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 029 | Total loss: 2.388 | Reg loss: 0.038 | Tree loss: 2.388 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 029 | Total loss: 2.525 | Reg loss: 0.038 | Tree loss: 2.525 | Accuracy: 0.250000 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 029 | Total loss: 2.352 | Reg loss: 0.038 | Tree loss: 2.352 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 029 | Total loss: 2.372 | Reg loss: 0.038 | Tree loss: 2.372 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 029 | Total loss: 2.256 | Reg loss: 0.038 | Tree loss: 2.256 | Accuracy: 0.320312 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 029 | Total loss: 2.333 | Reg loss: 0.038 | Tree loss: 2.333 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 029 | Total loss: 2.260 | Reg loss: 0.038 | Tree loss: 2.260 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 029 | Total loss: 2.349 | Reg loss: 0.039 | Tree loss: 2.349 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 029 | Total loss: 2.219 | Reg loss: 0.039 | Tree loss: 2.219 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 029 | Total loss: 2.272 | Reg loss: 0.039 | Tree loss: 2.272 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 029 | Total loss: 2.250 | Reg loss: 0.039 | Tree loss: 2.250 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 029 | Total loss: 2.183 | Reg loss: 0.039 | Tree loss: 2.183 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 029 | Total loss: 2.192 | Reg loss: 0.039 | Tree loss: 2.192 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 029 | Total loss: 2.158 | Reg loss: 0.039 | Tree loss: 2.158 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 029 | Total loss: 2.265 | Reg loss: 0.039 | Tree loss: 2.265 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 029 | Total loss: 2.163 | Reg loss: 0.039 | Tree loss: 2.163 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 029 | Total loss: 2.191 | Reg loss: 0.039 | Tree loss: 2.191 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 029 | Total loss: 2.130 | Reg loss: 0.039 | Tree loss: 2.130 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 029 | Total loss: 2.178 | Reg loss: 0.039 | Tree loss: 2.178 | Accuracy: 0.287109 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 024 / 029 | Total loss: 2.049 | Reg loss: 0.040 | Tree loss: 2.049 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 029 | Total loss: 2.128 | Reg loss: 0.040 | Tree loss: 2.128 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 029 | Total loss: 2.114 | Reg loss: 0.040 | Tree loss: 2.114 | Accuracy: 0.318359 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 029 | Total loss: 2.118 | Reg loss: 0.040 | Tree loss: 2.118 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 029 | Total loss: 2.039 | Reg loss: 0.040 | Tree loss: 2.039 | Accuracy: 0.279221 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 56 | Batch: 000 / 029 | Total loss: 2.436 | Reg loss: 0.038 | Tree loss: 2.436 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 029 | Total loss: 2.459 | Reg loss: 0.038 | Tree loss: 2.459 | Accuracy: 0.259766 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 029 | Total loss: 2.420 | Reg loss: 0.038 | Tree loss: 2.420 | Accuracy: 0.320312 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 029 | Total loss: 2.339 | Reg loss: 0.038 | Tree loss: 2.339 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 029 | Total loss: 2.406 | Reg loss: 0.038 | Tree loss: 2.406 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 029 | Total loss: 2.381 | Reg loss: 0.038 | Tree loss: 2.381 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 029 | Total loss: 2.431 | Reg loss: 0.038 | Tree loss: 2.431 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 029 | Total loss: 2.373 | Reg loss: 0.038 | Tree loss: 2.373 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 029 | Total loss: 2.320 | Reg loss: 0.038 | Tree loss: 2.320 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 029 | Total loss: 2.342 | Reg loss: 0.038 | Tree loss: 2.342 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 029 | Total loss: 2.316 | Reg loss: 0.038 | Tree loss: 2.316 | Accuracy: 0.240234 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 029 | Total loss: 2.312 | Reg loss: 0.038 | Tree loss: 2.312 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 029 | Total loss: 2.303 | Reg loss: 0.038 | Tree loss: 2.303 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 029 | Total loss: 2.240 | Reg loss: 0.039 | Tree loss: 2.240 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 029 | Total loss: 2.240 | Reg loss: 0.039 | Tree loss: 2.240 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 029 | Total loss: 2.224 | Reg loss: 0.039 | Tree loss: 2.224 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 029 | Total loss: 2.227 | Reg loss: 0.039 | Tree loss: 2.227 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 029 | Total loss: 2.165 | Reg loss: 0.039 | Tree loss: 2.165 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 029 | Total loss: 2.184 | Reg loss: 0.039 | Tree loss: 2.184 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 029 | Total loss: 2.137 | Reg loss: 0.039 | Tree loss: 2.137 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 029 | Total loss: 2.202 | Reg loss: 0.039 | Tree loss: 2.202 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 029 | Total loss: 2.142 | Reg loss: 0.039 | Tree loss: 2.142 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 029 | Total loss: 2.092 | Reg loss: 0.039 | Tree loss: 2.092 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 029 | Total loss: 2.161 | Reg loss: 0.039 | Tree loss: 2.161 | Accuracy: 0.244141 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 029 | Total loss: 2.078 | Reg loss: 0.039 | Tree loss: 2.078 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 029 | Total loss: 2.080 | Reg loss: 0.040 | Tree loss: 2.080 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 029 | Total loss: 2.119 | Reg loss: 0.040 | Tree loss: 2.119 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 029 | Total loss: 2.026 | Reg loss: 0.040 | Tree loss: 2.026 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 029 | Total loss: 2.143 | Reg loss: 0.040 | Tree loss: 2.143 | Accuracy: 0.285714 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 57 | Batch: 000 / 029 | Total loss: 2.404 | Reg loss: 0.038 | Tree loss: 2.404 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 029 | Total loss: 2.389 | Reg loss: 0.038 | Tree loss: 2.389 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 029 | Total loss: 2.445 | Reg loss: 0.038 | Tree loss: 2.445 | Accuracy: 0.283203 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 029 | Total loss: 2.438 | Reg loss: 0.038 | Tree loss: 2.438 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 029 | Total loss: 2.359 | Reg loss: 0.038 | Tree loss: 2.359 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 029 | Total loss: 2.436 | Reg loss: 0.038 | Tree loss: 2.436 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 029 | Total loss: 2.285 | Reg loss: 0.038 | Tree loss: 2.285 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 029 | Total loss: 2.313 | Reg loss: 0.038 | Tree loss: 2.313 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 029 | Total loss: 2.325 | Reg loss: 0.038 | Tree loss: 2.325 | Accuracy: 0.263672 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 029 | Total loss: 2.281 | Reg loss: 0.038 | Tree loss: 2.281 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 029 | Total loss: 2.220 | Reg loss: 0.038 | Tree loss: 2.220 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 029 | Total loss: 2.300 | Reg loss: 0.038 | Tree loss: 2.300 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 029 | Total loss: 2.251 | Reg loss: 0.038 | Tree loss: 2.251 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 029 | Total loss: 2.188 | Reg loss: 0.038 | Tree loss: 2.188 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 029 | Total loss: 2.242 | Reg loss: 0.039 | Tree loss: 2.242 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 029 | Total loss: 2.211 | Reg loss: 0.039 | Tree loss: 2.211 | Accuracy: 0.263672 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 029 | Total loss: 2.198 | Reg loss: 0.039 | Tree loss: 2.198 | Accuracy: 0.318359 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 029 | Total loss: 2.151 | Reg loss: 0.039 | Tree loss: 2.151 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 029 | Total loss: 2.177 | Reg loss: 0.039 | Tree loss: 2.177 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 029 | Total loss: 2.182 | Reg loss: 0.039 | Tree loss: 2.182 | Accuracy: 0.259766 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 029 | Total loss: 2.196 | Reg loss: 0.039 | Tree loss: 2.196 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 029 | Total loss: 2.131 | Reg loss: 0.039 | Tree loss: 2.131 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 029 | Total loss: 2.172 | Reg loss: 0.039 | Tree loss: 2.172 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 029 | Total loss: 2.100 | Reg loss: 0.039 | Tree loss: 2.100 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 029 | Total loss: 2.098 | Reg loss: 0.039 | Tree loss: 2.098 | Accuracy: 0.263672 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 029 | Total loss: 2.039 | Reg loss: 0.039 | Tree loss: 2.039 | Accuracy: 0.335938 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 029 | Total loss: 2.073 | Reg loss: 0.040 | Tree loss: 2.073 | Accuracy: 0.251953 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 029 | Total loss: 2.035 | Reg loss: 0.040 | Tree loss: 2.035 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 029 | Total loss: 2.075 | Reg loss: 0.040 | Tree loss: 2.075 | Accuracy: 0.324675 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 58 | Batch: 000 / 029 | Total loss: 2.446 | Reg loss: 0.038 | Tree loss: 2.446 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 029 | Total loss: 2.414 | Reg loss: 0.038 | Tree loss: 2.414 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 029 | Total loss: 2.403 | Reg loss: 0.038 | Tree loss: 2.403 | Accuracy: 0.269531 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 003 / 029 | Total loss: 2.462 | Reg loss: 0.038 | Tree loss: 2.462 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 029 | Total loss: 2.421 | Reg loss: 0.038 | Tree loss: 2.421 | Accuracy: 0.253906 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 029 | Total loss: 2.349 | Reg loss: 0.038 | Tree loss: 2.349 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 029 | Total loss: 2.381 | Reg loss: 0.038 | Tree loss: 2.381 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 029 | Total loss: 2.245 | Reg loss: 0.038 | Tree loss: 2.245 | Accuracy: 0.318359 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 029 | Total loss: 2.234 | Reg loss: 0.038 | Tree loss: 2.234 | Accuracy: 0.316406 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 029 | Total loss: 2.215 | Reg loss: 0.038 | Tree loss: 2.215 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 029 | Total loss: 2.209 | Reg loss: 0.038 | Tree loss: 2.209 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 029 | Total loss: 2.273 | Reg loss: 0.038 | Tree loss: 2.273 | Accuracy: 0.246094 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 029 | Total loss: 2.269 | Reg loss: 0.038 | Tree loss: 2.269 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 029 | Total loss: 2.210 | Reg loss: 0.038 | Tree loss: 2.210 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 029 | Total loss: 2.246 | Reg loss: 0.038 | Tree loss: 2.246 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 029 | Total loss: 2.159 | Reg loss: 0.038 | Tree loss: 2.159 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 029 | Total loss: 2.223 | Reg loss: 0.039 | Tree loss: 2.223 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 029 | Total loss: 2.141 | Reg loss: 0.039 | Tree loss: 2.141 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 029 | Total loss: 2.099 | Reg loss: 0.039 | Tree loss: 2.099 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 029 | Total loss: 2.125 | Reg loss: 0.039 | Tree loss: 2.125 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 029 | Total loss: 2.101 | Reg loss: 0.039 | Tree loss: 2.101 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 029 | Total loss: 2.109 | Reg loss: 0.039 | Tree loss: 2.109 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 029 | Total loss: 2.092 | Reg loss: 0.039 | Tree loss: 2.092 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 029 | Total loss: 2.048 | Reg loss: 0.039 | Tree loss: 2.048 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 029 | Total loss: 2.041 | Reg loss: 0.039 | Tree loss: 2.041 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 029 | Total loss: 2.088 | Reg loss: 0.039 | Tree loss: 2.088 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 029 | Total loss: 2.141 | Reg loss: 0.039 | Tree loss: 2.141 | Accuracy: 0.244141 | 0.069 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 029 | Total loss: 1.998 | Reg loss: 0.039 | Tree loss: 1.998 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 029 | Total loss: 2.039 | Reg loss: 0.039 | Tree loss: 2.039 | Accuracy: 0.279221 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 59 | Batch: 000 / 029 | Total loss: 2.367 | Reg loss: 0.038 | Tree loss: 2.367 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 029 | Total loss: 2.405 | Reg loss: 0.038 | Tree loss: 2.405 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 029 | Total loss: 2.375 | Reg loss: 0.038 | Tree loss: 2.375 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 029 | Total loss: 2.343 | Reg loss: 0.038 | Tree loss: 2.343 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 029 | Total loss: 2.348 | Reg loss: 0.038 | Tree loss: 2.348 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 029 | Total loss: 2.347 | Reg loss: 0.038 | Tree loss: 2.347 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 029 | Total loss: 2.344 | Reg loss: 0.038 | Tree loss: 2.344 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 029 | Total loss: 2.270 | Reg loss: 0.038 | Tree loss: 2.270 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 029 | Total loss: 2.285 | Reg loss: 0.038 | Tree loss: 2.285 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 029 | Total loss: 2.238 | Reg loss: 0.038 | Tree loss: 2.238 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 029 | Total loss: 2.181 | Reg loss: 0.038 | Tree loss: 2.181 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 029 | Total loss: 2.229 | Reg loss: 0.038 | Tree loss: 2.229 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 029 | Total loss: 2.217 | Reg loss: 0.038 | Tree loss: 2.217 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 029 | Total loss: 2.182 | Reg loss: 0.038 | Tree loss: 2.182 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 029 | Total loss: 2.219 | Reg loss: 0.038 | Tree loss: 2.219 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 029 | Total loss: 2.206 | Reg loss: 0.038 | Tree loss: 2.206 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 029 | Total loss: 2.209 | Reg loss: 0.038 | Tree loss: 2.209 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 029 | Total loss: 2.188 | Reg loss: 0.039 | Tree loss: 2.188 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 029 | Total loss: 2.138 | Reg loss: 0.039 | Tree loss: 2.138 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 029 | Total loss: 2.068 | Reg loss: 0.039 | Tree loss: 2.068 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 029 | Total loss: 2.127 | Reg loss: 0.039 | Tree loss: 2.127 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 029 | Total loss: 2.083 | Reg loss: 0.039 | Tree loss: 2.083 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 029 | Total loss: 2.094 | Reg loss: 0.039 | Tree loss: 2.094 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 029 | Total loss: 2.040 | Reg loss: 0.039 | Tree loss: 2.040 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 029 | Total loss: 2.055 | Reg loss: 0.039 | Tree loss: 2.055 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 029 | Total loss: 2.013 | Reg loss: 0.039 | Tree loss: 2.013 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 029 | Total loss: 2.031 | Reg loss: 0.039 | Tree loss: 2.031 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 029 | Total loss: 2.063 | Reg loss: 0.039 | Tree loss: 2.063 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 029 | Total loss: 1.991 | Reg loss: 0.039 | Tree loss: 1.991 | Accuracy: 0.246753 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 60 | Batch: 000 / 029 | Total loss: 2.414 | Reg loss: 0.038 | Tree loss: 2.414 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 029 | Total loss: 2.396 | Reg loss: 0.038 | Tree loss: 2.396 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 029 | Total loss: 2.362 | Reg loss: 0.038 | Tree loss: 2.362 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 029 | Total loss: 2.400 | Reg loss: 0.038 | Tree loss: 2.400 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 029 | Total loss: 2.364 | Reg loss: 0.038 | Tree loss: 2.364 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 029 | Total loss: 2.306 | Reg loss: 0.038 | Tree loss: 2.306 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 029 | Total loss: 2.262 | Reg loss: 0.038 | Tree loss: 2.262 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 029 | Total loss: 2.203 | Reg loss: 0.038 | Tree loss: 2.203 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 029 | Total loss: 2.268 | Reg loss: 0.038 | Tree loss: 2.268 | Accuracy: 0.318359 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 009 / 029 | Total loss: 2.235 | Reg loss: 0.038 | Tree loss: 2.235 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 029 | Total loss: 2.153 | Reg loss: 0.038 | Tree loss: 2.153 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 029 | Total loss: 2.250 | Reg loss: 0.038 | Tree loss: 2.250 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 029 | Total loss: 2.259 | Reg loss: 0.038 | Tree loss: 2.259 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 029 | Total loss: 2.180 | Reg loss: 0.038 | Tree loss: 2.180 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 029 | Total loss: 2.112 | Reg loss: 0.038 | Tree loss: 2.112 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 029 | Total loss: 2.193 | Reg loss: 0.038 | Tree loss: 2.193 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 029 | Total loss: 2.164 | Reg loss: 0.038 | Tree loss: 2.164 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 029 | Total loss: 2.150 | Reg loss: 0.038 | Tree loss: 2.150 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 029 | Total loss: 2.076 | Reg loss: 0.039 | Tree loss: 2.076 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 029 | Total loss: 2.056 | Reg loss: 0.039 | Tree loss: 2.056 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 029 | Total loss: 2.056 | Reg loss: 0.039 | Tree loss: 2.056 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 029 | Total loss: 2.076 | Reg loss: 0.039 | Tree loss: 2.076 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 029 | Total loss: 2.051 | Reg loss: 0.039 | Tree loss: 2.051 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 029 | Total loss: 2.069 | Reg loss: 0.039 | Tree loss: 2.069 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 029 | Total loss: 2.085 | Reg loss: 0.039 | Tree loss: 2.085 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 029 | Total loss: 2.037 | Reg loss: 0.039 | Tree loss: 2.037 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 029 | Total loss: 2.025 | Reg loss: 0.039 | Tree loss: 2.025 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 029 | Total loss: 2.041 | Reg loss: 0.039 | Tree loss: 2.041 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 029 | Total loss: 1.928 | Reg loss: 0.039 | Tree loss: 1.928 | Accuracy: 0.298701 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 61 | Batch: 000 / 029 | Total loss: 2.382 | Reg loss: 0.038 | Tree loss: 2.382 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 029 | Total loss: 2.314 | Reg loss: 0.038 | Tree loss: 2.314 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 029 | Total loss: 2.317 | Reg loss: 0.038 | Tree loss: 2.317 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 029 | Total loss: 2.334 | Reg loss: 0.038 | Tree loss: 2.334 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 029 | Total loss: 2.278 | Reg loss: 0.038 | Tree loss: 2.278 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 029 | Total loss: 2.338 | Reg loss: 0.038 | Tree loss: 2.338 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 029 | Total loss: 2.265 | Reg loss: 0.038 | Tree loss: 2.265 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 029 | Total loss: 2.243 | Reg loss: 0.038 | Tree loss: 2.243 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 029 | Total loss: 2.272 | Reg loss: 0.038 | Tree loss: 2.272 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 029 | Total loss: 2.216 | Reg loss: 0.038 | Tree loss: 2.216 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 029 | Total loss: 2.263 | Reg loss: 0.038 | Tree loss: 2.263 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 029 | Total loss: 2.223 | Reg loss: 0.038 | Tree loss: 2.223 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 029 | Total loss: 2.154 | Reg loss: 0.038 | Tree loss: 2.154 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 029 | Total loss: 2.228 | Reg loss: 0.038 | Tree loss: 2.228 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 029 | Total loss: 2.149 | Reg loss: 0.038 | Tree loss: 2.149 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 029 | Total loss: 2.129 | Reg loss: 0.038 | Tree loss: 2.129 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 029 | Total loss: 2.152 | Reg loss: 0.038 | Tree loss: 2.152 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 029 | Total loss: 2.112 | Reg loss: 0.038 | Tree loss: 2.112 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 029 | Total loss: 2.173 | Reg loss: 0.038 | Tree loss: 2.173 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 029 | Total loss: 2.044 | Reg loss: 0.038 | Tree loss: 2.044 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 029 | Total loss: 2.070 | Reg loss: 0.039 | Tree loss: 2.070 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 029 | Total loss: 2.078 | Reg loss: 0.039 | Tree loss: 2.078 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 029 | Total loss: 1.989 | Reg loss: 0.039 | Tree loss: 1.989 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 029 | Total loss: 2.000 | Reg loss: 0.039 | Tree loss: 2.000 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 029 | Total loss: 2.071 | Reg loss: 0.039 | Tree loss: 2.071 | Accuracy: 0.240234 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 029 | Total loss: 1.980 | Reg loss: 0.039 | Tree loss: 1.980 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 029 | Total loss: 2.009 | Reg loss: 0.039 | Tree loss: 2.009 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 029 | Total loss: 2.046 | Reg loss: 0.039 | Tree loss: 2.046 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 029 | Total loss: 1.913 | Reg loss: 0.039 | Tree loss: 1.913 | Accuracy: 0.311688 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 62 | Batch: 000 / 029 | Total loss: 2.355 | Reg loss: 0.037 | Tree loss: 2.355 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 029 | Total loss: 2.360 | Reg loss: 0.037 | Tree loss: 2.360 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 029 | Total loss: 2.313 | Reg loss: 0.037 | Tree loss: 2.313 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 029 | Total loss: 2.364 | Reg loss: 0.037 | Tree loss: 2.364 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 029 | Total loss: 2.316 | Reg loss: 0.037 | Tree loss: 2.316 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 029 | Total loss: 2.271 | Reg loss: 0.037 | Tree loss: 2.271 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 029 | Total loss: 2.223 | Reg loss: 0.037 | Tree loss: 2.223 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 029 | Total loss: 2.242 | Reg loss: 0.038 | Tree loss: 2.242 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 029 | Total loss: 2.201 | Reg loss: 0.038 | Tree loss: 2.201 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 029 | Total loss: 2.191 | Reg loss: 0.038 | Tree loss: 2.191 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 029 | Total loss: 2.164 | Reg loss: 0.038 | Tree loss: 2.164 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 029 | Total loss: 2.187 | Reg loss: 0.038 | Tree loss: 2.187 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 029 | Total loss: 2.117 | Reg loss: 0.038 | Tree loss: 2.117 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 029 | Total loss: 2.099 | Reg loss: 0.038 | Tree loss: 2.099 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 029 | Total loss: 2.175 | Reg loss: 0.038 | Tree loss: 2.175 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 029 | Total loss: 2.199 | Reg loss: 0.038 | Tree loss: 2.199 | Accuracy: 0.283203 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 016 / 029 | Total loss: 2.075 | Reg loss: 0.038 | Tree loss: 2.075 | Accuracy: 0.246094 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 029 | Total loss: 2.143 | Reg loss: 0.038 | Tree loss: 2.143 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 029 | Total loss: 2.061 | Reg loss: 0.038 | Tree loss: 2.061 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 029 | Total loss: 2.037 | Reg loss: 0.038 | Tree loss: 2.037 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 029 | Total loss: 2.058 | Reg loss: 0.038 | Tree loss: 2.058 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 029 | Total loss: 2.098 | Reg loss: 0.039 | Tree loss: 2.098 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 029 | Total loss: 2.120 | Reg loss: 0.039 | Tree loss: 2.120 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 029 | Total loss: 2.020 | Reg loss: 0.039 | Tree loss: 2.020 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 029 | Total loss: 2.036 | Reg loss: 0.039 | Tree loss: 2.036 | Accuracy: 0.333984 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 029 | Total loss: 1.980 | Reg loss: 0.039 | Tree loss: 1.980 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 029 | Total loss: 2.024 | Reg loss: 0.039 | Tree loss: 2.024 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 029 | Total loss: 2.019 | Reg loss: 0.039 | Tree loss: 2.019 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 029 | Total loss: 1.974 | Reg loss: 0.039 | Tree loss: 1.974 | Accuracy: 0.376623 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 63 | Batch: 000 / 029 | Total loss: 2.417 | Reg loss: 0.037 | Tree loss: 2.417 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 029 | Total loss: 2.339 | Reg loss: 0.037 | Tree loss: 2.339 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 029 | Total loss: 2.325 | Reg loss: 0.037 | Tree loss: 2.325 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 029 | Total loss: 2.315 | Reg loss: 0.037 | Tree loss: 2.315 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 029 | Total loss: 2.268 | Reg loss: 0.037 | Tree loss: 2.268 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 029 | Total loss: 2.272 | Reg loss: 0.037 | Tree loss: 2.272 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 029 | Total loss: 2.219 | Reg loss: 0.037 | Tree loss: 2.219 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 029 | Total loss: 2.212 | Reg loss: 0.037 | Tree loss: 2.212 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 029 | Total loss: 2.217 | Reg loss: 0.037 | Tree loss: 2.217 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 029 | Total loss: 2.175 | Reg loss: 0.038 | Tree loss: 2.175 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 029 | Total loss: 2.144 | Reg loss: 0.038 | Tree loss: 2.144 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 029 | Total loss: 2.157 | Reg loss: 0.038 | Tree loss: 2.157 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 029 | Total loss: 2.250 | Reg loss: 0.038 | Tree loss: 2.250 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 029 | Total loss: 2.131 | Reg loss: 0.038 | Tree loss: 2.131 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 029 | Total loss: 2.156 | Reg loss: 0.038 | Tree loss: 2.156 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 029 | Total loss: 2.010 | Reg loss: 0.038 | Tree loss: 2.010 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 029 | Total loss: 2.107 | Reg loss: 0.038 | Tree loss: 2.107 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 029 | Total loss: 2.055 | Reg loss: 0.038 | Tree loss: 2.055 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 029 | Total loss: 2.013 | Reg loss: 0.038 | Tree loss: 2.013 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 029 | Total loss: 2.060 | Reg loss: 0.038 | Tree loss: 2.060 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 029 | Total loss: 2.025 | Reg loss: 0.038 | Tree loss: 2.025 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 029 | Total loss: 2.132 | Reg loss: 0.038 | Tree loss: 2.132 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 029 | Total loss: 2.047 | Reg loss: 0.039 | Tree loss: 2.047 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 029 | Total loss: 2.066 | Reg loss: 0.039 | Tree loss: 2.066 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 029 | Total loss: 2.045 | Reg loss: 0.039 | Tree loss: 2.045 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 029 | Total loss: 1.983 | Reg loss: 0.039 | Tree loss: 1.983 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 029 | Total loss: 2.043 | Reg loss: 0.039 | Tree loss: 2.043 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 029 | Total loss: 1.910 | Reg loss: 0.039 | Tree loss: 1.910 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 029 | Total loss: 2.001 | Reg loss: 0.039 | Tree loss: 2.001 | Accuracy: 0.318182 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 64 | Batch: 000 / 029 | Total loss: 2.314 | Reg loss: 0.037 | Tree loss: 2.314 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 029 | Total loss: 2.355 | Reg loss: 0.037 | Tree loss: 2.355 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 029 | Total loss: 2.359 | Reg loss: 0.037 | Tree loss: 2.359 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 029 | Total loss: 2.327 | Reg loss: 0.037 | Tree loss: 2.327 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 029 | Total loss: 2.233 | Reg loss: 0.037 | Tree loss: 2.233 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 029 | Total loss: 2.239 | Reg loss: 0.037 | Tree loss: 2.239 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 029 | Total loss: 2.229 | Reg loss: 0.037 | Tree loss: 2.229 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 029 | Total loss: 2.263 | Reg loss: 0.037 | Tree loss: 2.263 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 029 | Total loss: 2.213 | Reg loss: 0.037 | Tree loss: 2.213 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 029 | Total loss: 2.175 | Reg loss: 0.037 | Tree loss: 2.175 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 029 | Total loss: 2.207 | Reg loss: 0.037 | Tree loss: 2.207 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 029 | Total loss: 2.144 | Reg loss: 0.038 | Tree loss: 2.144 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 029 | Total loss: 2.139 | Reg loss: 0.038 | Tree loss: 2.139 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 029 | Total loss: 2.144 | Reg loss: 0.038 | Tree loss: 2.144 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 029 | Total loss: 2.058 | Reg loss: 0.038 | Tree loss: 2.058 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 029 | Total loss: 2.146 | Reg loss: 0.038 | Tree loss: 2.146 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 029 | Total loss: 2.096 | Reg loss: 0.038 | Tree loss: 2.096 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 029 | Total loss: 2.071 | Reg loss: 0.038 | Tree loss: 2.071 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 029 | Total loss: 2.010 | Reg loss: 0.038 | Tree loss: 2.010 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 029 | Total loss: 2.039 | Reg loss: 0.038 | Tree loss: 2.039 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 029 | Total loss: 1.997 | Reg loss: 0.038 | Tree loss: 1.997 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 029 | Total loss: 2.022 | Reg loss: 0.038 | Tree loss: 2.022 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 029 | Total loss: 2.032 | Reg loss: 0.038 | Tree loss: 2.032 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 029 | Total loss: 2.062 | Reg loss: 0.038 | Tree loss: 2.062 | Accuracy: 0.259766 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Batch: 024 / 029 | Total loss: 2.003 | Reg loss: 0.039 | Tree loss: 2.003 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 029 | Total loss: 1.973 | Reg loss: 0.039 | Tree loss: 1.973 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 029 | Total loss: 1.975 | Reg loss: 0.039 | Tree loss: 1.975 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 029 | Total loss: 1.965 | Reg loss: 0.039 | Tree loss: 1.965 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 029 | Total loss: 1.872 | Reg loss: 0.039 | Tree loss: 1.872 | Accuracy: 0.370130 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 65 | Batch: 000 / 029 | Total loss: 2.308 | Reg loss: 0.037 | Tree loss: 2.308 | Accuracy: 0.332031 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 029 | Total loss: 2.289 | Reg loss: 0.037 | Tree loss: 2.289 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 029 | Total loss: 2.308 | Reg loss: 0.037 | Tree loss: 2.308 | Accuracy: 0.246094 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 029 | Total loss: 2.233 | Reg loss: 0.037 | Tree loss: 2.233 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 029 | Total loss: 2.236 | Reg loss: 0.037 | Tree loss: 2.236 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 029 | Total loss: 2.211 | Reg loss: 0.037 | Tree loss: 2.211 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 029 | Total loss: 2.250 | Reg loss: 0.037 | Tree loss: 2.250 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 029 | Total loss: 2.163 | Reg loss: 0.037 | Tree loss: 2.163 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 029 | Total loss: 2.214 | Reg loss: 0.037 | Tree loss: 2.214 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 029 | Total loss: 2.222 | Reg loss: 0.037 | Tree loss: 2.222 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 029 | Total loss: 2.262 | Reg loss: 0.037 | Tree loss: 2.262 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 029 | Total loss: 2.148 | Reg loss: 0.037 | Tree loss: 2.148 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 029 | Total loss: 2.155 | Reg loss: 0.038 | Tree loss: 2.155 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 029 | Total loss: 2.190 | Reg loss: 0.038 | Tree loss: 2.190 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 029 | Total loss: 2.156 | Reg loss: 0.038 | Tree loss: 2.156 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 029 | Total loss: 2.090 | Reg loss: 0.038 | Tree loss: 2.090 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 029 | Total loss: 1.996 | Reg loss: 0.038 | Tree loss: 1.996 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 029 | Total loss: 2.071 | Reg loss: 0.038 | Tree loss: 2.071 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 029 | Total loss: 2.061 | Reg loss: 0.038 | Tree loss: 2.061 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 029 | Total loss: 1.960 | Reg loss: 0.038 | Tree loss: 1.960 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 029 | Total loss: 2.061 | Reg loss: 0.038 | Tree loss: 2.061 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 029 | Total loss: 1.997 | Reg loss: 0.038 | Tree loss: 1.997 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 029 | Total loss: 2.028 | Reg loss: 0.038 | Tree loss: 2.028 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 029 | Total loss: 1.921 | Reg loss: 0.038 | Tree loss: 1.921 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 029 | Total loss: 2.003 | Reg loss: 0.038 | Tree loss: 2.003 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 029 | Total loss: 1.950 | Reg loss: 0.039 | Tree loss: 1.950 | Accuracy: 0.259766 | 0.07 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 029 | Total loss: 2.010 | Reg loss: 0.039 | Tree loss: 2.010 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 029 | Total loss: 1.974 | Reg loss: 0.039 | Tree loss: 1.974 | Accuracy: 0.337891 | 0.07 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 029 | Total loss: 1.921 | Reg loss: 0.039 | Tree loss: 1.921 | Accuracy: 0.240260 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 66 | Batch: 000 / 029 | Total loss: 2.324 | Reg loss: 0.037 | Tree loss: 2.324 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 029 | Total loss: 2.274 | Reg loss: 0.037 | Tree loss: 2.274 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 029 | Total loss: 2.347 | Reg loss: 0.037 | Tree loss: 2.347 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 029 | Total loss: 2.263 | Reg loss: 0.037 | Tree loss: 2.263 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 029 | Total loss: 2.263 | Reg loss: 0.037 | Tree loss: 2.263 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 029 | Total loss: 2.225 | Reg loss: 0.037 | Tree loss: 2.225 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 029 | Total loss: 2.211 | Reg loss: 0.037 | Tree loss: 2.211 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 029 | Total loss: 2.214 | Reg loss: 0.037 | Tree loss: 2.214 | Accuracy: 0.255859 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 029 | Total loss: 2.218 | Reg loss: 0.037 | Tree loss: 2.218 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 029 | Total loss: 2.143 | Reg loss: 0.037 | Tree loss: 2.143 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 029 | Total loss: 2.196 | Reg loss: 0.037 | Tree loss: 2.196 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 029 | Total loss: 2.117 | Reg loss: 0.037 | Tree loss: 2.117 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 029 | Total loss: 2.161 | Reg loss: 0.037 | Tree loss: 2.161 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 029 | Total loss: 2.024 | Reg loss: 0.038 | Tree loss: 2.024 | Accuracy: 0.347656 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 029 | Total loss: 2.085 | Reg loss: 0.038 | Tree loss: 2.085 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 029 | Total loss: 2.045 | Reg loss: 0.038 | Tree loss: 2.045 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 029 | Total loss: 2.072 | Reg loss: 0.038 | Tree loss: 2.072 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 029 | Total loss: 2.065 | Reg loss: 0.038 | Tree loss: 2.065 | Accuracy: 0.259766 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 029 | Total loss: 2.089 | Reg loss: 0.038 | Tree loss: 2.089 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 029 | Total loss: 2.028 | Reg loss: 0.038 | Tree loss: 2.028 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 029 | Total loss: 2.030 | Reg loss: 0.038 | Tree loss: 2.030 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 029 | Total loss: 1.997 | Reg loss: 0.038 | Tree loss: 1.997 | Accuracy: 0.324219 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 029 | Total loss: 1.977 | Reg loss: 0.038 | Tree loss: 1.977 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 029 | Total loss: 1.991 | Reg loss: 0.038 | Tree loss: 1.991 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 029 | Total loss: 1.912 | Reg loss: 0.038 | Tree loss: 1.912 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 029 | Total loss: 1.959 | Reg loss: 0.038 | Tree loss: 1.959 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 029 | Total loss: 1.935 | Reg loss: 0.039 | Tree loss: 1.935 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 029 | Total loss: 1.973 | Reg loss: 0.039 | Tree loss: 1.973 | Accuracy: 0.320312 | 0.07 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 029 | Total loss: 2.024 | Reg loss: 0.039 | Tree loss: 2.024 | Accuracy: 0.240260 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 67 | Batch: 000 / 029 | Total loss: 2.334 | Reg loss: 0.037 | Tree loss: 2.334 | Accuracy: 0.281250 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 001 / 029 | Total loss: 2.277 | Reg loss: 0.037 | Tree loss: 2.277 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 029 | Total loss: 2.273 | Reg loss: 0.037 | Tree loss: 2.273 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 029 | Total loss: 2.274 | Reg loss: 0.037 | Tree loss: 2.274 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 029 | Total loss: 2.231 | Reg loss: 0.037 | Tree loss: 2.231 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 029 | Total loss: 2.248 | Reg loss: 0.037 | Tree loss: 2.248 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 029 | Total loss: 2.153 | Reg loss: 0.037 | Tree loss: 2.153 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 029 | Total loss: 2.191 | Reg loss: 0.037 | Tree loss: 2.191 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 029 | Total loss: 2.145 | Reg loss: 0.037 | Tree loss: 2.145 | Accuracy: 0.328125 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 029 | Total loss: 2.117 | Reg loss: 0.037 | Tree loss: 2.117 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 029 | Total loss: 2.177 | Reg loss: 0.037 | Tree loss: 2.177 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 029 | Total loss: 2.121 | Reg loss: 0.037 | Tree loss: 2.121 | Accuracy: 0.253906 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 029 | Total loss: 2.065 | Reg loss: 0.037 | Tree loss: 2.065 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 029 | Total loss: 2.065 | Reg loss: 0.037 | Tree loss: 2.065 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 029 | Total loss: 2.049 | Reg loss: 0.037 | Tree loss: 2.049 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 029 | Total loss: 2.099 | Reg loss: 0.038 | Tree loss: 2.099 | Accuracy: 0.240234 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 029 | Total loss: 2.046 | Reg loss: 0.038 | Tree loss: 2.046 | Accuracy: 0.320312 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 029 | Total loss: 2.068 | Reg loss: 0.038 | Tree loss: 2.068 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 029 | Total loss: 2.117 | Reg loss: 0.038 | Tree loss: 2.117 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 029 | Total loss: 1.983 | Reg loss: 0.038 | Tree loss: 1.983 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 029 | Total loss: 2.046 | Reg loss: 0.038 | Tree loss: 2.046 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 029 | Total loss: 2.011 | Reg loss: 0.038 | Tree loss: 2.011 | Accuracy: 0.238281 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 029 | Total loss: 1.956 | Reg loss: 0.038 | Tree loss: 1.956 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 029 | Total loss: 2.013 | Reg loss: 0.038 | Tree loss: 2.013 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 029 | Total loss: 1.920 | Reg loss: 0.038 | Tree loss: 1.920 | Accuracy: 0.330078 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 029 | Total loss: 1.981 | Reg loss: 0.038 | Tree loss: 1.981 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 029 | Total loss: 1.972 | Reg loss: 0.038 | Tree loss: 1.972 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 029 | Total loss: 1.956 | Reg loss: 0.038 | Tree loss: 1.956 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 029 | Total loss: 1.934 | Reg loss: 0.039 | Tree loss: 1.934 | Accuracy: 0.240260 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 68 | Batch: 000 / 029 | Total loss: 2.264 | Reg loss: 0.037 | Tree loss: 2.264 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 029 | Total loss: 2.339 | Reg loss: 0.037 | Tree loss: 2.339 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 029 | Total loss: 2.201 | Reg loss: 0.037 | Tree loss: 2.201 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 029 | Total loss: 2.263 | Reg loss: 0.037 | Tree loss: 2.263 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 029 | Total loss: 2.207 | Reg loss: 0.037 | Tree loss: 2.207 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 029 | Total loss: 2.135 | Reg loss: 0.037 | Tree loss: 2.135 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 029 | Total loss: 2.169 | Reg loss: 0.037 | Tree loss: 2.169 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 029 | Total loss: 2.228 | Reg loss: 0.037 | Tree loss: 2.228 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 029 | Total loss: 2.242 | Reg loss: 0.037 | Tree loss: 2.242 | Accuracy: 0.253906 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 029 | Total loss: 2.104 | Reg loss: 0.037 | Tree loss: 2.104 | Accuracy: 0.326172 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 029 | Total loss: 2.177 | Reg loss: 0.037 | Tree loss: 2.177 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 029 | Total loss: 2.093 | Reg loss: 0.037 | Tree loss: 2.093 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 029 | Total loss: 2.159 | Reg loss: 0.037 | Tree loss: 2.159 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 029 | Total loss: 1.996 | Reg loss: 0.037 | Tree loss: 1.996 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 029 | Total loss: 2.043 | Reg loss: 0.037 | Tree loss: 2.043 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 029 | Total loss: 2.051 | Reg loss: 0.037 | Tree loss: 2.051 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 029 | Total loss: 2.064 | Reg loss: 0.038 | Tree loss: 2.064 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 029 | Total loss: 2.067 | Reg loss: 0.038 | Tree loss: 2.067 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 029 | Total loss: 1.973 | Reg loss: 0.038 | Tree loss: 1.973 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 029 | Total loss: 2.008 | Reg loss: 0.038 | Tree loss: 2.008 | Accuracy: 0.308594 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 029 | Total loss: 1.992 | Reg loss: 0.038 | Tree loss: 1.992 | Accuracy: 0.244141 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 029 | Total loss: 2.021 | Reg loss: 0.038 | Tree loss: 2.021 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 029 | Total loss: 1.952 | Reg loss: 0.038 | Tree loss: 1.952 | Accuracy: 0.332031 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 029 | Total loss: 1.992 | Reg loss: 0.038 | Tree loss: 1.992 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 029 | Total loss: 1.963 | Reg loss: 0.038 | Tree loss: 1.963 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 029 | Total loss: 1.978 | Reg loss: 0.038 | Tree loss: 1.978 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 029 | Total loss: 1.944 | Reg loss: 0.038 | Tree loss: 1.944 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 029 | Total loss: 1.986 | Reg loss: 0.038 | Tree loss: 1.986 | Accuracy: 0.283203 | 0.07 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 029 | Total loss: 2.025 | Reg loss: 0.038 | Tree loss: 2.025 | Accuracy: 0.292208 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 69 | Batch: 000 / 029 | Total loss: 2.315 | Reg loss: 0.037 | Tree loss: 2.315 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 029 | Total loss: 2.281 | Reg loss: 0.037 | Tree loss: 2.281 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 029 | Total loss: 2.297 | Reg loss: 0.037 | Tree loss: 2.297 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 029 | Total loss: 2.210 | Reg loss: 0.037 | Tree loss: 2.210 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 029 | Total loss: 2.256 | Reg loss: 0.037 | Tree loss: 2.256 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 029 | Total loss: 2.171 | Reg loss: 0.037 | Tree loss: 2.171 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 029 | Total loss: 2.123 | Reg loss: 0.037 | Tree loss: 2.123 | Accuracy: 0.328125 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 007 / 029 | Total loss: 2.204 | Reg loss: 0.037 | Tree loss: 2.204 | Accuracy: 0.330078 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 029 | Total loss: 2.173 | Reg loss: 0.037 | Tree loss: 2.173 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 029 | Total loss: 2.186 | Reg loss: 0.037 | Tree loss: 2.186 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 029 | Total loss: 2.156 | Reg loss: 0.037 | Tree loss: 2.156 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 029 | Total loss: 2.107 | Reg loss: 0.037 | Tree loss: 2.107 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 029 | Total loss: 2.091 | Reg loss: 0.037 | Tree loss: 2.091 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 029 | Total loss: 2.087 | Reg loss: 0.037 | Tree loss: 2.087 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 029 | Total loss: 2.024 | Reg loss: 0.037 | Tree loss: 2.024 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 029 | Total loss: 2.045 | Reg loss: 0.037 | Tree loss: 2.045 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 029 | Total loss: 2.032 | Reg loss: 0.037 | Tree loss: 2.032 | Accuracy: 0.330078 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 029 | Total loss: 1.970 | Reg loss: 0.038 | Tree loss: 1.970 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 029 | Total loss: 1.996 | Reg loss: 0.038 | Tree loss: 1.996 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 029 | Total loss: 2.040 | Reg loss: 0.038 | Tree loss: 2.040 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 029 | Total loss: 2.018 | Reg loss: 0.038 | Tree loss: 2.018 | Accuracy: 0.248047 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 029 | Total loss: 1.973 | Reg loss: 0.038 | Tree loss: 1.973 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 029 | Total loss: 1.968 | Reg loss: 0.038 | Tree loss: 1.968 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 029 | Total loss: 1.905 | Reg loss: 0.038 | Tree loss: 1.905 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 029 | Total loss: 1.957 | Reg loss: 0.038 | Tree loss: 1.957 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 029 | Total loss: 1.956 | Reg loss: 0.038 | Tree loss: 1.956 | Accuracy: 0.259766 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 029 | Total loss: 1.966 | Reg loss: 0.038 | Tree loss: 1.966 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 029 | Total loss: 1.892 | Reg loss: 0.038 | Tree loss: 1.892 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 029 | Total loss: 1.875 | Reg loss: 0.038 | Tree loss: 1.875 | Accuracy: 0.279221 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 70 | Batch: 000 / 029 | Total loss: 2.230 | Reg loss: 0.037 | Tree loss: 2.230 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 029 | Total loss: 2.171 | Reg loss: 0.037 | Tree loss: 2.171 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 029 | Total loss: 2.215 | Reg loss: 0.037 | Tree loss: 2.215 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 029 | Total loss: 2.191 | Reg loss: 0.037 | Tree loss: 2.191 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 029 | Total loss: 2.244 | Reg loss: 0.037 | Tree loss: 2.244 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 029 | Total loss: 2.181 | Reg loss: 0.037 | Tree loss: 2.181 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 029 | Total loss: 2.173 | Reg loss: 0.037 | Tree loss: 2.173 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 029 | Total loss: 2.186 | Reg loss: 0.037 | Tree loss: 2.186 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 029 | Total loss: 2.122 | Reg loss: 0.037 | Tree loss: 2.122 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 029 | Total loss: 2.035 | Reg loss: 0.037 | Tree loss: 2.035 | Accuracy: 0.318359 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 029 | Total loss: 2.118 | Reg loss: 0.037 | Tree loss: 2.118 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 029 | Total loss: 2.107 | Reg loss: 0.037 | Tree loss: 2.107 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 029 | Total loss: 2.118 | Reg loss: 0.037 | Tree loss: 2.118 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 029 | Total loss: 2.115 | Reg loss: 0.037 | Tree loss: 2.115 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 029 | Total loss: 2.030 | Reg loss: 0.037 | Tree loss: 2.030 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 029 | Total loss: 2.027 | Reg loss: 0.037 | Tree loss: 2.027 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 029 | Total loss: 2.101 | Reg loss: 0.037 | Tree loss: 2.101 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 029 | Total loss: 2.000 | Reg loss: 0.037 | Tree loss: 2.000 | Accuracy: 0.308594 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 029 | Total loss: 1.984 | Reg loss: 0.038 | Tree loss: 1.984 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 029 | Total loss: 2.002 | Reg loss: 0.038 | Tree loss: 2.002 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 029 | Total loss: 2.037 | Reg loss: 0.038 | Tree loss: 2.037 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 029 | Total loss: 2.007 | Reg loss: 0.038 | Tree loss: 2.007 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 029 | Total loss: 1.947 | Reg loss: 0.038 | Tree loss: 1.947 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 029 | Total loss: 1.933 | Reg loss: 0.038 | Tree loss: 1.933 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 029 | Total loss: 1.961 | Reg loss: 0.038 | Tree loss: 1.961 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 029 | Total loss: 1.932 | Reg loss: 0.038 | Tree loss: 1.932 | Accuracy: 0.246094 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 029 | Total loss: 1.992 | Reg loss: 0.038 | Tree loss: 1.992 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 029 | Total loss: 1.977 | Reg loss: 0.038 | Tree loss: 1.977 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 029 | Total loss: 2.011 | Reg loss: 0.038 | Tree loss: 2.011 | Accuracy: 0.279221 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 71 | Batch: 000 / 029 | Total loss: 2.297 | Reg loss: 0.037 | Tree loss: 2.297 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 029 | Total loss: 2.196 | Reg loss: 0.037 | Tree loss: 2.196 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 029 | Total loss: 2.179 | Reg loss: 0.037 | Tree loss: 2.179 | Accuracy: 0.337891 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 029 | Total loss: 2.213 | Reg loss: 0.037 | Tree loss: 2.213 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 029 | Total loss: 2.168 | Reg loss: 0.037 | Tree loss: 2.168 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 029 | Total loss: 2.185 | Reg loss: 0.037 | Tree loss: 2.185 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 029 | Total loss: 2.205 | Reg loss: 0.037 | Tree loss: 2.205 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 029 | Total loss: 2.150 | Reg loss: 0.037 | Tree loss: 2.150 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 029 | Total loss: 2.163 | Reg loss: 0.037 | Tree loss: 2.163 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 029 | Total loss: 2.147 | Reg loss: 0.037 | Tree loss: 2.147 | Accuracy: 0.246094 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 029 | Total loss: 2.113 | Reg loss: 0.037 | Tree loss: 2.113 | Accuracy: 0.257812 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 029 | Total loss: 2.097 | Reg loss: 0.037 | Tree loss: 2.097 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 029 | Total loss: 2.029 | Reg loss: 0.037 | Tree loss: 2.029 | Accuracy: 0.324219 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 029 | Total loss: 2.049 | Reg loss: 0.037 | Tree loss: 2.049 | Accuracy: 0.259766 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 014 / 029 | Total loss: 2.043 | Reg loss: 0.037 | Tree loss: 2.043 | Accuracy: 0.308594 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 029 | Total loss: 2.041 | Reg loss: 0.037 | Tree loss: 2.041 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 029 | Total loss: 2.050 | Reg loss: 0.037 | Tree loss: 2.050 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 029 | Total loss: 2.050 | Reg loss: 0.037 | Tree loss: 2.050 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 029 | Total loss: 1.983 | Reg loss: 0.037 | Tree loss: 1.983 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 029 | Total loss: 1.971 | Reg loss: 0.038 | Tree loss: 1.971 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 029 | Total loss: 2.008 | Reg loss: 0.038 | Tree loss: 2.008 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 029 | Total loss: 1.967 | Reg loss: 0.038 | Tree loss: 1.967 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 029 | Total loss: 1.925 | Reg loss: 0.038 | Tree loss: 1.925 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 029 | Total loss: 1.915 | Reg loss: 0.038 | Tree loss: 1.915 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 029 | Total loss: 1.887 | Reg loss: 0.038 | Tree loss: 1.887 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 029 | Total loss: 1.941 | Reg loss: 0.038 | Tree loss: 1.941 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 029 | Total loss: 1.962 | Reg loss: 0.038 | Tree loss: 1.962 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 029 | Total loss: 2.010 | Reg loss: 0.038 | Tree loss: 2.010 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 029 | Total loss: 1.924 | Reg loss: 0.038 | Tree loss: 1.924 | Accuracy: 0.253247 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 72 | Batch: 000 / 029 | Total loss: 2.203 | Reg loss: 0.037 | Tree loss: 2.203 | Accuracy: 0.320312 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 029 | Total loss: 2.206 | Reg loss: 0.037 | Tree loss: 2.206 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 029 | Total loss: 2.293 | Reg loss: 0.037 | Tree loss: 2.293 | Accuracy: 0.259766 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 029 | Total loss: 2.194 | Reg loss: 0.037 | Tree loss: 2.194 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 029 | Total loss: 2.246 | Reg loss: 0.037 | Tree loss: 2.246 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 029 | Total loss: 2.161 | Reg loss: 0.037 | Tree loss: 2.161 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 029 | Total loss: 2.152 | Reg loss: 0.037 | Tree loss: 2.152 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 029 | Total loss: 2.096 | Reg loss: 0.037 | Tree loss: 2.096 | Accuracy: 0.316406 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 029 | Total loss: 2.117 | Reg loss: 0.037 | Tree loss: 2.117 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 029 | Total loss: 2.110 | Reg loss: 0.037 | Tree loss: 2.110 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 029 | Total loss: 2.111 | Reg loss: 0.037 | Tree loss: 2.111 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 029 | Total loss: 2.099 | Reg loss: 0.037 | Tree loss: 2.099 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 029 | Total loss: 2.045 | Reg loss: 0.037 | Tree loss: 2.045 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 029 | Total loss: 2.037 | Reg loss: 0.037 | Tree loss: 2.037 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 029 | Total loss: 2.004 | Reg loss: 0.037 | Tree loss: 2.004 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 029 | Total loss: 2.073 | Reg loss: 0.037 | Tree loss: 2.073 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 029 | Total loss: 1.944 | Reg loss: 0.037 | Tree loss: 1.944 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 029 | Total loss: 2.058 | Reg loss: 0.037 | Tree loss: 2.058 | Accuracy: 0.283203 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 029 | Total loss: 2.046 | Reg loss: 0.037 | Tree loss: 2.046 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 029 | Total loss: 1.948 | Reg loss: 0.037 | Tree loss: 1.948 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 029 | Total loss: 1.971 | Reg loss: 0.038 | Tree loss: 1.971 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 029 | Total loss: 1.917 | Reg loss: 0.038 | Tree loss: 1.917 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 029 | Total loss: 2.009 | Reg loss: 0.038 | Tree loss: 2.009 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 029 | Total loss: 1.927 | Reg loss: 0.038 | Tree loss: 1.927 | Accuracy: 0.328125 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 029 | Total loss: 1.987 | Reg loss: 0.038 | Tree loss: 1.987 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 029 | Total loss: 1.936 | Reg loss: 0.038 | Tree loss: 1.936 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 029 | Total loss: 1.877 | Reg loss: 0.038 | Tree loss: 1.877 | Accuracy: 0.263672 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 029 | Total loss: 1.937 | Reg loss: 0.038 | Tree loss: 1.937 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 029 | Total loss: 2.058 | Reg loss: 0.038 | Tree loss: 2.058 | Accuracy: 0.233766 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 73 | Batch: 000 / 029 | Total loss: 2.182 | Reg loss: 0.036 | Tree loss: 2.182 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 029 | Total loss: 2.190 | Reg loss: 0.036 | Tree loss: 2.190 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 029 | Total loss: 2.303 | Reg loss: 0.036 | Tree loss: 2.303 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 029 | Total loss: 2.225 | Reg loss: 0.036 | Tree loss: 2.225 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 029 | Total loss: 2.248 | Reg loss: 0.036 | Tree loss: 2.248 | Accuracy: 0.283203 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 029 | Total loss: 2.158 | Reg loss: 0.037 | Tree loss: 2.158 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 029 | Total loss: 2.096 | Reg loss: 0.037 | Tree loss: 2.096 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 029 | Total loss: 2.157 | Reg loss: 0.037 | Tree loss: 2.157 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 029 | Total loss: 2.110 | Reg loss: 0.037 | Tree loss: 2.110 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 029 | Total loss: 2.171 | Reg loss: 0.037 | Tree loss: 2.171 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 029 | Total loss: 2.126 | Reg loss: 0.037 | Tree loss: 2.126 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 029 | Total loss: 2.038 | Reg loss: 0.037 | Tree loss: 2.038 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 029 | Total loss: 2.065 | Reg loss: 0.037 | Tree loss: 2.065 | Accuracy: 0.257812 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 029 | Total loss: 2.075 | Reg loss: 0.037 | Tree loss: 2.075 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 029 | Total loss: 2.028 | Reg loss: 0.037 | Tree loss: 2.028 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 029 | Total loss: 1.991 | Reg loss: 0.037 | Tree loss: 1.991 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 029 | Total loss: 1.942 | Reg loss: 0.037 | Tree loss: 1.942 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 029 | Total loss: 2.004 | Reg loss: 0.037 | Tree loss: 2.004 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 029 | Total loss: 1.967 | Reg loss: 0.037 | Tree loss: 1.967 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 029 | Total loss: 1.985 | Reg loss: 0.037 | Tree loss: 1.985 | Accuracy: 0.257812 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 029 | Total loss: 1.945 | Reg loss: 0.037 | Tree loss: 1.945 | Accuracy: 0.314453 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 021 / 029 | Total loss: 1.954 | Reg loss: 0.038 | Tree loss: 1.954 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 029 | Total loss: 1.944 | Reg loss: 0.038 | Tree loss: 1.944 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 029 | Total loss: 1.920 | Reg loss: 0.038 | Tree loss: 1.920 | Accuracy: 0.263672 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 029 | Total loss: 1.949 | Reg loss: 0.038 | Tree loss: 1.949 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 029 | Total loss: 1.906 | Reg loss: 0.038 | Tree loss: 1.906 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 029 | Total loss: 1.951 | Reg loss: 0.038 | Tree loss: 1.951 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 029 | Total loss: 1.941 | Reg loss: 0.038 | Tree loss: 1.941 | Accuracy: 0.308594 | 0.07 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 029 | Total loss: 1.835 | Reg loss: 0.038 | Tree loss: 1.835 | Accuracy: 0.363636 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 74 | Batch: 000 / 029 | Total loss: 2.220 | Reg loss: 0.036 | Tree loss: 2.220 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 029 | Total loss: 2.195 | Reg loss: 0.036 | Tree loss: 2.195 | Accuracy: 0.328125 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 029 | Total loss: 2.155 | Reg loss: 0.036 | Tree loss: 2.155 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 029 | Total loss: 2.133 | Reg loss: 0.036 | Tree loss: 2.133 | Accuracy: 0.314453 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 029 | Total loss: 2.253 | Reg loss: 0.036 | Tree loss: 2.253 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 029 | Total loss: 2.164 | Reg loss: 0.036 | Tree loss: 2.164 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 029 | Total loss: 2.133 | Reg loss: 0.036 | Tree loss: 2.133 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 029 | Total loss: 2.086 | Reg loss: 0.036 | Tree loss: 2.086 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 029 | Total loss: 2.059 | Reg loss: 0.037 | Tree loss: 2.059 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 029 | Total loss: 2.179 | Reg loss: 0.037 | Tree loss: 2.179 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 029 | Total loss: 2.142 | Reg loss: 0.037 | Tree loss: 2.142 | Accuracy: 0.253906 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 029 | Total loss: 2.005 | Reg loss: 0.037 | Tree loss: 2.005 | Accuracy: 0.322266 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 029 | Total loss: 2.076 | Reg loss: 0.037 | Tree loss: 2.076 | Accuracy: 0.263672 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 029 | Total loss: 2.092 | Reg loss: 0.037 | Tree loss: 2.092 | Accuracy: 0.246094 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 029 | Total loss: 2.019 | Reg loss: 0.037 | Tree loss: 2.019 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 029 | Total loss: 2.033 | Reg loss: 0.037 | Tree loss: 2.033 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 029 | Total loss: 1.999 | Reg loss: 0.037 | Tree loss: 1.999 | Accuracy: 0.328125 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 029 | Total loss: 1.947 | Reg loss: 0.037 | Tree loss: 1.947 | Accuracy: 0.283203 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 029 | Total loss: 2.011 | Reg loss: 0.037 | Tree loss: 2.011 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 029 | Total loss: 1.996 | Reg loss: 0.037 | Tree loss: 1.996 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 029 | Total loss: 1.950 | Reg loss: 0.037 | Tree loss: 1.950 | Accuracy: 0.312500 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 029 | Total loss: 1.950 | Reg loss: 0.037 | Tree loss: 1.950 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 029 | Total loss: 1.960 | Reg loss: 0.038 | Tree loss: 1.960 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 029 | Total loss: 1.972 | Reg loss: 0.038 | Tree loss: 1.972 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 029 | Total loss: 1.900 | Reg loss: 0.038 | Tree loss: 1.900 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 029 | Total loss: 1.869 | Reg loss: 0.038 | Tree loss: 1.869 | Accuracy: 0.308594 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 029 | Total loss: 1.925 | Reg loss: 0.038 | Tree loss: 1.925 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 029 | Total loss: 1.979 | Reg loss: 0.038 | Tree loss: 1.979 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 029 | Total loss: 1.864 | Reg loss: 0.038 | Tree loss: 1.864 | Accuracy: 0.207792 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 75 | Batch: 000 / 029 | Total loss: 2.195 | Reg loss: 0.036 | Tree loss: 2.195 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 029 | Total loss: 2.205 | Reg loss: 0.036 | Tree loss: 2.205 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 029 | Total loss: 2.167 | Reg loss: 0.036 | Tree loss: 2.167 | Accuracy: 0.255859 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 029 | Total loss: 2.190 | Reg loss: 0.036 | Tree loss: 2.190 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 029 | Total loss: 2.211 | Reg loss: 0.036 | Tree loss: 2.211 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 029 | Total loss: 2.195 | Reg loss: 0.036 | Tree loss: 2.195 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 029 | Total loss: 2.165 | Reg loss: 0.036 | Tree loss: 2.165 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 029 | Total loss: 2.156 | Reg loss: 0.036 | Tree loss: 2.156 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 029 | Total loss: 2.096 | Reg loss: 0.036 | Tree loss: 2.096 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 029 | Total loss: 2.108 | Reg loss: 0.037 | Tree loss: 2.108 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 029 | Total loss: 2.135 | Reg loss: 0.037 | Tree loss: 2.135 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 029 | Total loss: 2.014 | Reg loss: 0.037 | Tree loss: 2.014 | Accuracy: 0.318359 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 029 | Total loss: 2.089 | Reg loss: 0.037 | Tree loss: 2.089 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 029 | Total loss: 2.011 | Reg loss: 0.037 | Tree loss: 2.011 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 029 | Total loss: 2.003 | Reg loss: 0.037 | Tree loss: 2.003 | Accuracy: 0.253906 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 029 | Total loss: 1.954 | Reg loss: 0.037 | Tree loss: 1.954 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 029 | Total loss: 1.972 | Reg loss: 0.037 | Tree loss: 1.972 | Accuracy: 0.322266 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 029 | Total loss: 2.066 | Reg loss: 0.037 | Tree loss: 2.066 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 029 | Total loss: 1.929 | Reg loss: 0.037 | Tree loss: 1.929 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 029 | Total loss: 1.913 | Reg loss: 0.037 | Tree loss: 1.913 | Accuracy: 0.322266 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 029 | Total loss: 1.904 | Reg loss: 0.037 | Tree loss: 1.904 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 029 | Total loss: 1.909 | Reg loss: 0.037 | Tree loss: 1.909 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 029 | Total loss: 2.008 | Reg loss: 0.037 | Tree loss: 2.008 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 029 | Total loss: 1.942 | Reg loss: 0.038 | Tree loss: 1.942 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 029 | Total loss: 1.974 | Reg loss: 0.038 | Tree loss: 1.974 | Accuracy: 0.275391 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 029 | Total loss: 1.901 | Reg loss: 0.038 | Tree loss: 1.901 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 029 | Total loss: 1.933 | Reg loss: 0.038 | Tree loss: 1.933 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 029 | Total loss: 1.903 | Reg loss: 0.038 | Tree loss: 1.903 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 029 | Total loss: 1.844 | Reg loss: 0.038 | Tree loss: 1.844 | Accuracy: 0.292208 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 000 / 029 | Total loss: 2.234 | Reg loss: 0.036 | Tree loss: 2.234 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 029 | Total loss: 2.229 | Reg loss: 0.036 | Tree loss: 2.229 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 029 | Total loss: 2.255 | Reg loss: 0.036 | Tree loss: 2.255 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 029 | Total loss: 2.186 | Reg loss: 0.036 | Tree loss: 2.186 | Accuracy: 0.291016 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 029 | Total loss: 2.161 | Reg loss: 0.036 | Tree loss: 2.161 | Accuracy: 0.277344 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 029 | Total loss: 2.160 | Reg loss: 0.036 | Tree loss: 2.160 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 029 | Total loss: 2.195 | Reg loss: 0.036 | Tree loss: 2.195 | Accuracy: 0.263672 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 029 | Total loss: 2.065 | Reg loss: 0.036 | Tree loss: 2.065 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 029 | Total loss: 2.075 | Reg loss: 0.036 | Tree loss: 2.075 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 029 | Total loss: 2.024 | Reg loss: 0.036 | Tree loss: 2.024 | Accuracy: 0.328125 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 029 | Total loss: 1.988 | Reg loss: 0.037 | Tree loss: 1.988 | Accuracy: 0.308594 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 029 | Total loss: 2.076 | Reg loss: 0.037 | Tree loss: 2.076 | Accuracy: 0.257812 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 029 | Total loss: 2.009 | Reg loss: 0.037 | Tree loss: 2.009 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 029 | Total loss: 1.997 | Reg loss: 0.037 | Tree loss: 1.997 | Accuracy: 0.251953 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 029 | Total loss: 2.030 | Reg loss: 0.037 | Tree loss: 2.030 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 029 | Total loss: 2.023 | Reg loss: 0.037 | Tree loss: 2.023 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 029 | Total loss: 1.967 | Reg loss: 0.037 | Tree loss: 1.967 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 029 | Total loss: 1.943 | Reg loss: 0.037 | Tree loss: 1.943 | Accuracy: 0.318359 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 029 | Total loss: 2.033 | Reg loss: 0.037 | Tree loss: 2.033 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 029 | Total loss: 1.967 | Reg loss: 0.037 | Tree loss: 1.967 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 029 | Total loss: 2.050 | Reg loss: 0.037 | Tree loss: 2.050 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 029 | Total loss: 1.966 | Reg loss: 0.037 | Tree loss: 1.966 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 029 | Total loss: 1.894 | Reg loss: 0.037 | Tree loss: 1.894 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 029 | Total loss: 1.873 | Reg loss: 0.037 | Tree loss: 1.873 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 029 | Total loss: 1.925 | Reg loss: 0.038 | Tree loss: 1.925 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 029 | Total loss: 1.908 | Reg loss: 0.038 | Tree loss: 1.908 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 029 | Total loss: 1.948 | Reg loss: 0.038 | Tree loss: 1.948 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 029 | Total loss: 1.888 | Reg loss: 0.038 | Tree loss: 1.888 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 029 | Total loss: 1.890 | Reg loss: 0.038 | Tree loss: 1.890 | Accuracy: 0.292208 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 77 | Batch: 000 / 029 | Total loss: 2.180 | Reg loss: 0.036 | Tree loss: 2.180 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 029 | Total loss: 2.207 | Reg loss: 0.036 | Tree loss: 2.207 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 029 | Total loss: 2.236 | Reg loss: 0.036 | Tree loss: 2.236 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 029 | Total loss: 2.231 | Reg loss: 0.036 | Tree loss: 2.231 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 029 | Total loss: 2.158 | Reg loss: 0.036 | Tree loss: 2.158 | Accuracy: 0.265625 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 029 | Total loss: 2.172 | Reg loss: 0.036 | Tree loss: 2.172 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 029 | Total loss: 2.116 | Reg loss: 0.036 | Tree loss: 2.116 | Accuracy: 0.283203 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 029 | Total loss: 2.146 | Reg loss: 0.036 | Tree loss: 2.146 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 029 | Total loss: 2.089 | Reg loss: 0.036 | Tree loss: 2.089 | Accuracy: 0.302734 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 029 | Total loss: 2.057 | Reg loss: 0.036 | Tree loss: 2.057 | Accuracy: 0.292969 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 029 | Total loss: 2.091 | Reg loss: 0.036 | Tree loss: 2.091 | Accuracy: 0.304688 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 029 | Total loss: 2.018 | Reg loss: 0.036 | Tree loss: 2.018 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 029 | Total loss: 1.985 | Reg loss: 0.037 | Tree loss: 1.985 | Accuracy: 0.320312 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 029 | Total loss: 2.067 | Reg loss: 0.037 | Tree loss: 2.067 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 029 | Total loss: 2.046 | Reg loss: 0.037 | Tree loss: 2.046 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 029 | Total loss: 1.975 | Reg loss: 0.037 | Tree loss: 1.975 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 029 | Total loss: 1.966 | Reg loss: 0.037 | Tree loss: 1.966 | Accuracy: 0.330078 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 029 | Total loss: 1.990 | Reg loss: 0.037 | Tree loss: 1.990 | Accuracy: 0.328125 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 029 | Total loss: 2.001 | Reg loss: 0.037 | Tree loss: 2.001 | Accuracy: 0.273438 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 029 | Total loss: 1.980 | Reg loss: 0.037 | Tree loss: 1.980 | Accuracy: 0.279297 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 029 | Total loss: 1.891 | Reg loss: 0.037 | Tree loss: 1.891 | Accuracy: 0.310547 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 029 | Total loss: 1.948 | Reg loss: 0.037 | Tree loss: 1.948 | Accuracy: 0.298828 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 029 | Total loss: 1.941 | Reg loss: 0.037 | Tree loss: 1.941 | Accuracy: 0.261719 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 029 | Total loss: 2.006 | Reg loss: 0.037 | Tree loss: 2.006 | Accuracy: 0.218750 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 029 | Total loss: 1.870 | Reg loss: 0.037 | Tree loss: 1.870 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 029 | Total loss: 1.918 | Reg loss: 0.038 | Tree loss: 1.918 | Accuracy: 0.267578 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 029 | Total loss: 1.810 | Reg loss: 0.038 | Tree loss: 1.810 | Accuracy: 0.330078 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 029 | Total loss: 1.856 | Reg loss: 0.038 | Tree loss: 1.856 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 029 | Total loss: 1.879 | Reg loss: 0.038 | Tree loss: 1.879 | Accuracy: 0.285714 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 78 | Batch: 000 / 029 | Total loss: 2.241 | Reg loss: 0.036 | Tree loss: 2.241 | Accuracy: 0.255859 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 029 | Total loss: 2.240 | Reg loss: 0.036 | Tree loss: 2.240 | Accuracy: 0.271484 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 029 | Total loss: 2.131 | Reg loss: 0.036 | Tree loss: 2.131 | Accuracy: 0.306641 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 029 | Total loss: 2.236 | Reg loss: 0.036 | Tree loss: 2.236 | Accuracy: 0.269531 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 029 | Total loss: 2.126 | Reg loss: 0.036 | Tree loss: 2.126 | Accuracy: 0.300781 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 029 | Total loss: 2.145 | Reg loss: 0.036 | Tree loss: 2.145 | Accuracy: 0.248047 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 029 | Total loss: 2.103 | Reg loss: 0.036 | Tree loss: 2.103 | Accuracy: 0.291016 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 007 / 029 | Total loss: 2.089 | Reg loss: 0.036 | Tree loss: 2.089 | Accuracy: 0.296875 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 029 | Total loss: 2.128 | Reg loss: 0.036 | Tree loss: 2.128 | Accuracy: 0.316406 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 029 | Total loss: 2.088 | Reg loss: 0.036 | Tree loss: 2.088 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 029 | Total loss: 2.108 | Reg loss: 0.036 | Tree loss: 2.108 | Accuracy: 0.259766 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 029 | Total loss: 2.018 | Reg loss: 0.036 | Tree loss: 2.018 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 029 | Total loss: 2.024 | Reg loss: 0.036 | Tree loss: 2.024 | Accuracy: 0.289062 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 029 | Total loss: 2.080 | Reg loss: 0.037 | Tree loss: 2.080 | Accuracy: 0.294922 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 029 | Total loss: 1.927 | Reg loss: 0.037 | Tree loss: 1.927 | Accuracy: 0.281250 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 029 | Total loss: 1.995 | Reg loss: 0.037 | Tree loss: 1.995 | Accuracy: 0.322266 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 029 | Total loss: 1.970 | Reg loss: 0.037 | Tree loss: 1.970 | Accuracy: 0.285156 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 029 | Total loss: 1.959 | Reg loss: 0.037 | Tree loss: 1.959 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 029 | Total loss: 1.964 | Reg loss: 0.037 | Tree loss: 1.964 | Accuracy: 0.287109 | 0.07 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 029 | Total loss: 1.958 | Reg loss: 0.037 | Tree loss: 1.958 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 029 | Total loss: 1.928 | Reg loss: 0.037 | Tree loss: 1.928 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 029 | Total loss: 1.867 | Reg loss: 0.037 | Tree loss: 1.867 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 029 | Total loss: 1.875 | Reg loss: 0.037 | Tree loss: 1.875 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 029 | Total loss: 1.867 | Reg loss: 0.037 | Tree loss: 1.867 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 029 | Total loss: 2.003 | Reg loss: 0.037 | Tree loss: 2.003 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 029 | Total loss: 1.830 | Reg loss: 0.037 | Tree loss: 1.830 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 029 | Total loss: 1.943 | Reg loss: 0.037 | Tree loss: 1.943 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 029 | Total loss: 1.934 | Reg loss: 0.038 | Tree loss: 1.934 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 029 | Total loss: 1.964 | Reg loss: 0.038 | Tree loss: 1.964 | Accuracy: 0.220779 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 79 | Batch: 000 / 029 | Total loss: 2.293 | Reg loss: 0.036 | Tree loss: 2.293 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 029 | Total loss: 2.177 | Reg loss: 0.036 | Tree loss: 2.177 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 029 | Total loss: 2.167 | Reg loss: 0.036 | Tree loss: 2.167 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 029 | Total loss: 2.236 | Reg loss: 0.036 | Tree loss: 2.236 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 029 | Total loss: 2.160 | Reg loss: 0.036 | Tree loss: 2.160 | Accuracy: 0.248047 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 029 | Total loss: 2.097 | Reg loss: 0.036 | Tree loss: 2.097 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 029 | Total loss: 2.148 | Reg loss: 0.036 | Tree loss: 2.148 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 029 | Total loss: 2.016 | Reg loss: 0.036 | Tree loss: 2.016 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 029 | Total loss: 2.118 | Reg loss: 0.036 | Tree loss: 2.118 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 029 | Total loss: 2.027 | Reg loss: 0.036 | Tree loss: 2.027 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 029 | Total loss: 2.079 | Reg loss: 0.036 | Tree loss: 2.079 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 029 | Total loss: 2.010 | Reg loss: 0.036 | Tree loss: 2.010 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 029 | Total loss: 2.043 | Reg loss: 0.036 | Tree loss: 2.043 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 029 | Total loss: 2.052 | Reg loss: 0.036 | Tree loss: 2.052 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 029 | Total loss: 2.005 | Reg loss: 0.037 | Tree loss: 2.005 | Accuracy: 0.332031 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 029 | Total loss: 1.988 | Reg loss: 0.037 | Tree loss: 1.988 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 029 | Total loss: 1.934 | Reg loss: 0.037 | Tree loss: 1.934 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 029 | Total loss: 2.038 | Reg loss: 0.037 | Tree loss: 2.038 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 029 | Total loss: 1.995 | Reg loss: 0.037 | Tree loss: 1.995 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 029 | Total loss: 1.906 | Reg loss: 0.037 | Tree loss: 1.906 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 029 | Total loss: 1.940 | Reg loss: 0.037 | Tree loss: 1.940 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 029 | Total loss: 1.898 | Reg loss: 0.037 | Tree loss: 1.898 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 029 | Total loss: 1.956 | Reg loss: 0.037 | Tree loss: 1.956 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 029 | Total loss: 1.890 | Reg loss: 0.037 | Tree loss: 1.890 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 029 | Total loss: 1.857 | Reg loss: 0.037 | Tree loss: 1.857 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 029 | Total loss: 1.883 | Reg loss: 0.037 | Tree loss: 1.883 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 029 | Total loss: 1.858 | Reg loss: 0.037 | Tree loss: 1.858 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 029 | Total loss: 1.908 | Reg loss: 0.037 | Tree loss: 1.908 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 029 | Total loss: 1.854 | Reg loss: 0.038 | Tree loss: 1.854 | Accuracy: 0.285714 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 80 | Batch: 000 / 029 | Total loss: 2.193 | Reg loss: 0.036 | Tree loss: 2.193 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 029 | Total loss: 2.203 | Reg loss: 0.036 | Tree loss: 2.203 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 029 | Total loss: 2.266 | Reg loss: 0.036 | Tree loss: 2.266 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 029 | Total loss: 2.105 | Reg loss: 0.036 | Tree loss: 2.105 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 029 | Total loss: 2.104 | Reg loss: 0.036 | Tree loss: 2.104 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 029 | Total loss: 2.086 | Reg loss: 0.036 | Tree loss: 2.086 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 029 | Total loss: 2.104 | Reg loss: 0.036 | Tree loss: 2.104 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 029 | Total loss: 2.130 | Reg loss: 0.036 | Tree loss: 2.130 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 029 | Total loss: 2.070 | Reg loss: 0.036 | Tree loss: 2.070 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 029 | Total loss: 2.132 | Reg loss: 0.036 | Tree loss: 2.132 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 029 | Total loss: 2.008 | Reg loss: 0.036 | Tree loss: 2.008 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 029 | Total loss: 2.129 | Reg loss: 0.036 | Tree loss: 2.129 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 029 | Total loss: 2.014 | Reg loss: 0.036 | Tree loss: 2.014 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 029 | Total loss: 1.930 | Reg loss: 0.036 | Tree loss: 1.930 | Accuracy: 0.318359 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 014 / 029 | Total loss: 2.007 | Reg loss: 0.037 | Tree loss: 2.007 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 029 | Total loss: 1.980 | Reg loss: 0.037 | Tree loss: 1.980 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 029 | Total loss: 1.985 | Reg loss: 0.037 | Tree loss: 1.985 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 029 | Total loss: 2.043 | Reg loss: 0.037 | Tree loss: 2.043 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 029 | Total loss: 1.883 | Reg loss: 0.037 | Tree loss: 1.883 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 029 | Total loss: 1.971 | Reg loss: 0.037 | Tree loss: 1.971 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 029 | Total loss: 1.914 | Reg loss: 0.037 | Tree loss: 1.914 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 029 | Total loss: 1.958 | Reg loss: 0.037 | Tree loss: 1.958 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 029 | Total loss: 1.875 | Reg loss: 0.037 | Tree loss: 1.875 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 029 | Total loss: 1.928 | Reg loss: 0.037 | Tree loss: 1.928 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 029 | Total loss: 1.862 | Reg loss: 0.037 | Tree loss: 1.862 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 029 | Total loss: 1.875 | Reg loss: 0.037 | Tree loss: 1.875 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 029 | Total loss: 1.868 | Reg loss: 0.037 | Tree loss: 1.868 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 029 | Total loss: 1.875 | Reg loss: 0.037 | Tree loss: 1.875 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 80 | Batch: 028 / 029 | Total loss: 1.950 | Reg loss: 0.037 | Tree loss: 1.950 | Accuracy: 0.272727 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 81 | Batch: 000 / 029 | Total loss: 2.259 | Reg loss: 0.036 | Tree loss: 2.259 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 029 | Total loss: 2.227 | Reg loss: 0.036 | Tree loss: 2.227 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 029 | Total loss: 2.179 | Reg loss: 0.036 | Tree loss: 2.179 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 029 | Total loss: 2.155 | Reg loss: 0.036 | Tree loss: 2.155 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 029 | Total loss: 2.122 | Reg loss: 0.036 | Tree loss: 2.122 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 029 | Total loss: 2.092 | Reg loss: 0.036 | Tree loss: 2.092 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 029 | Total loss: 2.117 | Reg loss: 0.036 | Tree loss: 2.117 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 029 | Total loss: 2.079 | Reg loss: 0.036 | Tree loss: 2.079 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 029 | Total loss: 2.101 | Reg loss: 0.036 | Tree loss: 2.101 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 029 | Total loss: 2.053 | Reg loss: 0.036 | Tree loss: 2.053 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 029 | Total loss: 2.023 | Reg loss: 0.036 | Tree loss: 2.023 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 029 | Total loss: 2.011 | Reg loss: 0.036 | Tree loss: 2.011 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 029 | Total loss: 2.027 | Reg loss: 0.036 | Tree loss: 2.027 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 029 | Total loss: 1.965 | Reg loss: 0.036 | Tree loss: 1.965 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 029 | Total loss: 2.039 | Reg loss: 0.036 | Tree loss: 2.039 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 029 | Total loss: 2.015 | Reg loss: 0.036 | Tree loss: 2.015 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 029 | Total loss: 2.032 | Reg loss: 0.037 | Tree loss: 2.032 | Accuracy: 0.246094 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 029 | Total loss: 1.981 | Reg loss: 0.037 | Tree loss: 1.981 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 029 | Total loss: 1.911 | Reg loss: 0.037 | Tree loss: 1.911 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 029 | Total loss: 1.880 | Reg loss: 0.037 | Tree loss: 1.880 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 029 | Total loss: 1.868 | Reg loss: 0.037 | Tree loss: 1.868 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 029 | Total loss: 1.944 | Reg loss: 0.037 | Tree loss: 1.944 | Accuracy: 0.248047 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 029 | Total loss: 1.862 | Reg loss: 0.037 | Tree loss: 1.862 | Accuracy: 0.330078 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 029 | Total loss: 1.955 | Reg loss: 0.037 | Tree loss: 1.955 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 029 | Total loss: 1.983 | Reg loss: 0.037 | Tree loss: 1.983 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 029 | Total loss: 1.884 | Reg loss: 0.037 | Tree loss: 1.884 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 029 | Total loss: 1.786 | Reg loss: 0.037 | Tree loss: 1.786 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 029 | Total loss: 1.896 | Reg loss: 0.037 | Tree loss: 1.896 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 029 | Total loss: 1.767 | Reg loss: 0.037 | Tree loss: 1.767 | Accuracy: 0.363636 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 82 | Batch: 000 / 029 | Total loss: 2.246 | Reg loss: 0.036 | Tree loss: 2.246 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 029 | Total loss: 2.163 | Reg loss: 0.036 | Tree loss: 2.163 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 029 | Total loss: 2.184 | Reg loss: 0.036 | Tree loss: 2.184 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 029 | Total loss: 2.199 | Reg loss: 0.036 | Tree loss: 2.199 | Accuracy: 0.250000 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 029 | Total loss: 2.180 | Reg loss: 0.036 | Tree loss: 2.180 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 029 | Total loss: 2.230 | Reg loss: 0.036 | Tree loss: 2.230 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 029 | Total loss: 2.111 | Reg loss: 0.036 | Tree loss: 2.111 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 029 | Total loss: 2.062 | Reg loss: 0.036 | Tree loss: 2.062 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 029 | Total loss: 2.076 | Reg loss: 0.036 | Tree loss: 2.076 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 029 | Total loss: 2.053 | Reg loss: 0.036 | Tree loss: 2.053 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 029 | Total loss: 2.011 | Reg loss: 0.036 | Tree loss: 2.011 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 029 | Total loss: 2.041 | Reg loss: 0.036 | Tree loss: 2.041 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 029 | Total loss: 2.070 | Reg loss: 0.036 | Tree loss: 2.070 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 029 | Total loss: 2.013 | Reg loss: 0.036 | Tree loss: 2.013 | Accuracy: 0.326172 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 029 | Total loss: 1.947 | Reg loss: 0.036 | Tree loss: 1.947 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 029 | Total loss: 1.976 | Reg loss: 0.036 | Tree loss: 1.976 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 029 | Total loss: 1.974 | Reg loss: 0.037 | Tree loss: 1.974 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 029 | Total loss: 1.953 | Reg loss: 0.037 | Tree loss: 1.953 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 029 | Total loss: 1.957 | Reg loss: 0.037 | Tree loss: 1.957 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 029 | Total loss: 1.912 | Reg loss: 0.037 | Tree loss: 1.912 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 029 | Total loss: 1.922 | Reg loss: 0.037 | Tree loss: 1.922 | Accuracy: 0.287109 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 021 / 029 | Total loss: 1.901 | Reg loss: 0.037 | Tree loss: 1.901 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 029 | Total loss: 1.861 | Reg loss: 0.037 | Tree loss: 1.861 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 029 | Total loss: 1.932 | Reg loss: 0.037 | Tree loss: 1.932 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 029 | Total loss: 1.813 | Reg loss: 0.037 | Tree loss: 1.813 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 029 | Total loss: 1.832 | Reg loss: 0.037 | Tree loss: 1.832 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 029 | Total loss: 1.826 | Reg loss: 0.037 | Tree loss: 1.826 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 029 | Total loss: 1.892 | Reg loss: 0.037 | Tree loss: 1.892 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 029 | Total loss: 1.757 | Reg loss: 0.037 | Tree loss: 1.757 | Accuracy: 0.240260 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 83 | Batch: 000 / 029 | Total loss: 2.249 | Reg loss: 0.036 | Tree loss: 2.249 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 029 | Total loss: 2.120 | Reg loss: 0.036 | Tree loss: 2.120 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 029 | Total loss: 2.182 | Reg loss: 0.036 | Tree loss: 2.182 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 029 | Total loss: 2.149 | Reg loss: 0.036 | Tree loss: 2.149 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 029 | Total loss: 2.166 | Reg loss: 0.036 | Tree loss: 2.166 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 029 | Total loss: 2.068 | Reg loss: 0.036 | Tree loss: 2.068 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 029 | Total loss: 2.107 | Reg loss: 0.036 | Tree loss: 2.107 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 029 | Total loss: 2.075 | Reg loss: 0.036 | Tree loss: 2.075 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 029 | Total loss: 2.114 | Reg loss: 0.036 | Tree loss: 2.114 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 029 | Total loss: 2.063 | Reg loss: 0.036 | Tree loss: 2.063 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 029 | Total loss: 2.112 | Reg loss: 0.036 | Tree loss: 2.112 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 029 | Total loss: 2.004 | Reg loss: 0.036 | Tree loss: 2.004 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 029 | Total loss: 1.994 | Reg loss: 0.036 | Tree loss: 1.994 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 029 | Total loss: 2.014 | Reg loss: 0.036 | Tree loss: 2.014 | Accuracy: 0.240234 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 029 | Total loss: 1.977 | Reg loss: 0.036 | Tree loss: 1.977 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 029 | Total loss: 1.931 | Reg loss: 0.036 | Tree loss: 1.931 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 029 | Total loss: 1.987 | Reg loss: 0.036 | Tree loss: 1.987 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 029 | Total loss: 1.964 | Reg loss: 0.037 | Tree loss: 1.964 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 029 | Total loss: 1.917 | Reg loss: 0.037 | Tree loss: 1.917 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 029 | Total loss: 1.885 | Reg loss: 0.037 | Tree loss: 1.885 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 029 | Total loss: 1.881 | Reg loss: 0.037 | Tree loss: 1.881 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 029 | Total loss: 1.918 | Reg loss: 0.037 | Tree loss: 1.918 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 029 | Total loss: 1.870 | Reg loss: 0.037 | Tree loss: 1.870 | Accuracy: 0.343750 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 029 | Total loss: 1.895 | Reg loss: 0.037 | Tree loss: 1.895 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 029 | Total loss: 1.873 | Reg loss: 0.037 | Tree loss: 1.873 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 029 | Total loss: 1.904 | Reg loss: 0.037 | Tree loss: 1.904 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 029 | Total loss: 1.889 | Reg loss: 0.037 | Tree loss: 1.889 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 029 | Total loss: 1.881 | Reg loss: 0.037 | Tree loss: 1.881 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 029 | Total loss: 1.867 | Reg loss: 0.037 | Tree loss: 1.867 | Accuracy: 0.331169 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 84 | Batch: 000 / 029 | Total loss: 2.211 | Reg loss: 0.036 | Tree loss: 2.211 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 029 | Total loss: 2.189 | Reg loss: 0.036 | Tree loss: 2.189 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 029 | Total loss: 2.208 | Reg loss: 0.036 | Tree loss: 2.208 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 029 | Total loss: 2.161 | Reg loss: 0.036 | Tree loss: 2.161 | Accuracy: 0.240234 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 029 | Total loss: 2.147 | Reg loss: 0.036 | Tree loss: 2.147 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 029 | Total loss: 2.061 | Reg loss: 0.036 | Tree loss: 2.061 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 029 | Total loss: 2.129 | Reg loss: 0.036 | Tree loss: 2.129 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 029 | Total loss: 2.147 | Reg loss: 0.036 | Tree loss: 2.147 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 029 | Total loss: 1.984 | Reg loss: 0.036 | Tree loss: 1.984 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 029 | Total loss: 2.072 | Reg loss: 0.036 | Tree loss: 2.072 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 029 | Total loss: 2.066 | Reg loss: 0.036 | Tree loss: 2.066 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 029 | Total loss: 1.953 | Reg loss: 0.036 | Tree loss: 1.953 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 029 | Total loss: 2.019 | Reg loss: 0.036 | Tree loss: 2.019 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 029 | Total loss: 2.035 | Reg loss: 0.036 | Tree loss: 2.035 | Accuracy: 0.347656 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 029 | Total loss: 1.905 | Reg loss: 0.036 | Tree loss: 1.905 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 029 | Total loss: 1.990 | Reg loss: 0.036 | Tree loss: 1.990 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 029 | Total loss: 1.960 | Reg loss: 0.036 | Tree loss: 1.960 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 029 | Total loss: 1.903 | Reg loss: 0.036 | Tree loss: 1.903 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 029 | Total loss: 1.919 | Reg loss: 0.037 | Tree loss: 1.919 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 029 | Total loss: 1.897 | Reg loss: 0.037 | Tree loss: 1.897 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 029 | Total loss: 1.883 | Reg loss: 0.037 | Tree loss: 1.883 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 029 | Total loss: 1.819 | Reg loss: 0.037 | Tree loss: 1.819 | Accuracy: 0.337891 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 029 | Total loss: 1.912 | Reg loss: 0.037 | Tree loss: 1.912 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 029 | Total loss: 1.880 | Reg loss: 0.037 | Tree loss: 1.880 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 029 | Total loss: 1.920 | Reg loss: 0.037 | Tree loss: 1.920 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 029 | Total loss: 1.906 | Reg loss: 0.037 | Tree loss: 1.906 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 029 | Total loss: 1.921 | Reg loss: 0.037 | Tree loss: 1.921 | Accuracy: 0.275391 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 027 / 029 | Total loss: 1.881 | Reg loss: 0.037 | Tree loss: 1.881 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 029 | Total loss: 1.907 | Reg loss: 0.037 | Tree loss: 1.907 | Accuracy: 0.337662 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 85 | Batch: 000 / 029 | Total loss: 2.211 | Reg loss: 0.036 | Tree loss: 2.211 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 029 | Total loss: 2.144 | Reg loss: 0.036 | Tree loss: 2.144 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 029 | Total loss: 2.231 | Reg loss: 0.036 | Tree loss: 2.231 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 029 | Total loss: 2.141 | Reg loss: 0.036 | Tree loss: 2.141 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 029 | Total loss: 2.095 | Reg loss: 0.036 | Tree loss: 2.095 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 029 | Total loss: 2.090 | Reg loss: 0.036 | Tree loss: 2.090 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 029 | Total loss: 2.141 | Reg loss: 0.036 | Tree loss: 2.141 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 029 | Total loss: 2.109 | Reg loss: 0.036 | Tree loss: 2.109 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 029 | Total loss: 2.041 | Reg loss: 0.036 | Tree loss: 2.041 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 029 | Total loss: 2.100 | Reg loss: 0.036 | Tree loss: 2.100 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 029 | Total loss: 1.978 | Reg loss: 0.036 | Tree loss: 1.978 | Accuracy: 0.330078 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 029 | Total loss: 1.940 | Reg loss: 0.036 | Tree loss: 1.940 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 029 | Total loss: 2.009 | Reg loss: 0.036 | Tree loss: 2.009 | Accuracy: 0.248047 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 029 | Total loss: 1.965 | Reg loss: 0.036 | Tree loss: 1.965 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 029 | Total loss: 1.990 | Reg loss: 0.036 | Tree loss: 1.990 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 029 | Total loss: 2.016 | Reg loss: 0.036 | Tree loss: 2.016 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 029 | Total loss: 1.913 | Reg loss: 0.036 | Tree loss: 1.913 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 029 | Total loss: 1.947 | Reg loss: 0.036 | Tree loss: 1.947 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 029 | Total loss: 1.951 | Reg loss: 0.037 | Tree loss: 1.951 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 029 | Total loss: 1.852 | Reg loss: 0.037 | Tree loss: 1.852 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 029 | Total loss: 1.970 | Reg loss: 0.037 | Tree loss: 1.970 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 029 | Total loss: 1.909 | Reg loss: 0.037 | Tree loss: 1.909 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 029 | Total loss: 1.874 | Reg loss: 0.037 | Tree loss: 1.874 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 029 | Total loss: 1.873 | Reg loss: 0.037 | Tree loss: 1.873 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 029 | Total loss: 1.898 | Reg loss: 0.037 | Tree loss: 1.898 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 029 | Total loss: 1.864 | Reg loss: 0.037 | Tree loss: 1.864 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 029 | Total loss: 1.927 | Reg loss: 0.037 | Tree loss: 1.927 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 029 | Total loss: 1.834 | Reg loss: 0.037 | Tree loss: 1.834 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 029 | Total loss: 1.791 | Reg loss: 0.037 | Tree loss: 1.791 | Accuracy: 0.292208 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 86 | Batch: 000 / 029 | Total loss: 2.183 | Reg loss: 0.036 | Tree loss: 2.183 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 029 | Total loss: 2.177 | Reg loss: 0.036 | Tree loss: 2.177 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 029 | Total loss: 2.101 | Reg loss: 0.036 | Tree loss: 2.101 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 029 | Total loss: 2.145 | Reg loss: 0.036 | Tree loss: 2.145 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 029 | Total loss: 2.039 | Reg loss: 0.036 | Tree loss: 2.039 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 029 | Total loss: 2.041 | Reg loss: 0.036 | Tree loss: 2.041 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 029 | Total loss: 2.121 | Reg loss: 0.036 | Tree loss: 2.121 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 029 | Total loss: 2.174 | Reg loss: 0.036 | Tree loss: 2.174 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 029 | Total loss: 2.049 | Reg loss: 0.036 | Tree loss: 2.049 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 029 | Total loss: 2.004 | Reg loss: 0.036 | Tree loss: 2.004 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 029 | Total loss: 2.018 | Reg loss: 0.036 | Tree loss: 2.018 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 029 | Total loss: 2.058 | Reg loss: 0.036 | Tree loss: 2.058 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 029 | Total loss: 1.981 | Reg loss: 0.036 | Tree loss: 1.981 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 029 | Total loss: 1.962 | Reg loss: 0.036 | Tree loss: 1.962 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 029 | Total loss: 2.007 | Reg loss: 0.036 | Tree loss: 2.007 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 029 | Total loss: 2.022 | Reg loss: 0.036 | Tree loss: 2.022 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 029 | Total loss: 1.929 | Reg loss: 0.036 | Tree loss: 1.929 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 029 | Total loss: 1.910 | Reg loss: 0.036 | Tree loss: 1.910 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 029 | Total loss: 1.959 | Reg loss: 0.036 | Tree loss: 1.959 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 029 | Total loss: 1.965 | Reg loss: 0.036 | Tree loss: 1.965 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 029 | Total loss: 1.860 | Reg loss: 0.037 | Tree loss: 1.860 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 029 | Total loss: 1.909 | Reg loss: 0.037 | Tree loss: 1.909 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 029 | Total loss: 1.918 | Reg loss: 0.037 | Tree loss: 1.918 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 029 | Total loss: 1.951 | Reg loss: 0.037 | Tree loss: 1.951 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 029 | Total loss: 1.905 | Reg loss: 0.037 | Tree loss: 1.905 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 029 | Total loss: 1.822 | Reg loss: 0.037 | Tree loss: 1.822 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 029 | Total loss: 1.877 | Reg loss: 0.037 | Tree loss: 1.877 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 029 | Total loss: 1.815 | Reg loss: 0.037 | Tree loss: 1.815 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 029 | Total loss: 1.796 | Reg loss: 0.037 | Tree loss: 1.796 | Accuracy: 0.240260 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 87 | Batch: 000 / 029 | Total loss: 2.177 | Reg loss: 0.036 | Tree loss: 2.177 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 029 | Total loss: 2.115 | Reg loss: 0.035 | Tree loss: 2.115 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 029 | Total loss: 2.174 | Reg loss: 0.035 | Tree loss: 2.174 | Accuracy: 0.314453 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 003 / 029 | Total loss: 2.112 | Reg loss: 0.035 | Tree loss: 2.112 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 029 | Total loss: 2.191 | Reg loss: 0.035 | Tree loss: 2.191 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 029 | Total loss: 2.093 | Reg loss: 0.036 | Tree loss: 2.093 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 029 | Total loss: 2.161 | Reg loss: 0.036 | Tree loss: 2.161 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 029 | Total loss: 2.027 | Reg loss: 0.036 | Tree loss: 2.027 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 029 | Total loss: 1.975 | Reg loss: 0.036 | Tree loss: 1.975 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 029 | Total loss: 2.075 | Reg loss: 0.036 | Tree loss: 2.075 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 029 | Total loss: 2.020 | Reg loss: 0.036 | Tree loss: 2.020 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 029 | Total loss: 2.024 | Reg loss: 0.036 | Tree loss: 2.024 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 029 | Total loss: 1.992 | Reg loss: 0.036 | Tree loss: 1.992 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 029 | Total loss: 1.917 | Reg loss: 0.036 | Tree loss: 1.917 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 029 | Total loss: 2.083 | Reg loss: 0.036 | Tree loss: 2.083 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 029 | Total loss: 1.930 | Reg loss: 0.036 | Tree loss: 1.930 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 029 | Total loss: 1.940 | Reg loss: 0.036 | Tree loss: 1.940 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 029 | Total loss: 2.000 | Reg loss: 0.036 | Tree loss: 2.000 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 029 | Total loss: 1.904 | Reg loss: 0.036 | Tree loss: 1.904 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 029 | Total loss: 1.950 | Reg loss: 0.036 | Tree loss: 1.950 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 029 | Total loss: 1.868 | Reg loss: 0.036 | Tree loss: 1.868 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 029 | Total loss: 1.924 | Reg loss: 0.037 | Tree loss: 1.924 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 029 | Total loss: 1.808 | Reg loss: 0.037 | Tree loss: 1.808 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 029 | Total loss: 1.912 | Reg loss: 0.037 | Tree loss: 1.912 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 029 | Total loss: 1.827 | Reg loss: 0.037 | Tree loss: 1.827 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 029 | Total loss: 1.876 | Reg loss: 0.037 | Tree loss: 1.876 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 029 | Total loss: 1.865 | Reg loss: 0.037 | Tree loss: 1.865 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 029 | Total loss: 1.856 | Reg loss: 0.037 | Tree loss: 1.856 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 029 | Total loss: 1.849 | Reg loss: 0.037 | Tree loss: 1.849 | Accuracy: 0.285714 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 88 | Batch: 000 / 029 | Total loss: 2.141 | Reg loss: 0.035 | Tree loss: 2.141 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 029 | Total loss: 2.155 | Reg loss: 0.035 | Tree loss: 2.155 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 029 | Total loss: 2.197 | Reg loss: 0.035 | Tree loss: 2.197 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 029 | Total loss: 2.138 | Reg loss: 0.035 | Tree loss: 2.138 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 029 | Total loss: 2.133 | Reg loss: 0.035 | Tree loss: 2.133 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 029 | Total loss: 2.109 | Reg loss: 0.035 | Tree loss: 2.109 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 029 | Total loss: 2.125 | Reg loss: 0.035 | Tree loss: 2.125 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 029 | Total loss: 2.030 | Reg loss: 0.036 | Tree loss: 2.030 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 029 | Total loss: 2.072 | Reg loss: 0.036 | Tree loss: 2.072 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 029 | Total loss: 2.013 | Reg loss: 0.036 | Tree loss: 2.013 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 029 | Total loss: 1.993 | Reg loss: 0.036 | Tree loss: 1.993 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 029 | Total loss: 1.984 | Reg loss: 0.036 | Tree loss: 1.984 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 029 | Total loss: 2.057 | Reg loss: 0.036 | Tree loss: 2.057 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 029 | Total loss: 1.995 | Reg loss: 0.036 | Tree loss: 1.995 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 029 | Total loss: 1.969 | Reg loss: 0.036 | Tree loss: 1.969 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 029 | Total loss: 1.901 | Reg loss: 0.036 | Tree loss: 1.901 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 029 | Total loss: 1.984 | Reg loss: 0.036 | Tree loss: 1.984 | Accuracy: 0.248047 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 029 | Total loss: 1.856 | Reg loss: 0.036 | Tree loss: 1.856 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 029 | Total loss: 1.918 | Reg loss: 0.036 | Tree loss: 1.918 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 029 | Total loss: 1.927 | Reg loss: 0.036 | Tree loss: 1.927 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 029 | Total loss: 1.938 | Reg loss: 0.036 | Tree loss: 1.938 | Accuracy: 0.250000 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 029 | Total loss: 1.920 | Reg loss: 0.037 | Tree loss: 1.920 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 029 | Total loss: 1.974 | Reg loss: 0.037 | Tree loss: 1.974 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 029 | Total loss: 1.812 | Reg loss: 0.037 | Tree loss: 1.812 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 029 | Total loss: 1.838 | Reg loss: 0.037 | Tree loss: 1.838 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 029 | Total loss: 1.879 | Reg loss: 0.037 | Tree loss: 1.879 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 029 | Total loss: 1.885 | Reg loss: 0.037 | Tree loss: 1.885 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 029 | Total loss: 1.774 | Reg loss: 0.037 | Tree loss: 1.774 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 029 | Total loss: 1.869 | Reg loss: 0.037 | Tree loss: 1.869 | Accuracy: 0.246753 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 89 | Batch: 000 / 029 | Total loss: 2.186 | Reg loss: 0.035 | Tree loss: 2.186 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 029 | Total loss: 2.138 | Reg loss: 0.035 | Tree loss: 2.138 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 029 | Total loss: 2.166 | Reg loss: 0.035 | Tree loss: 2.166 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 029 | Total loss: 2.078 | Reg loss: 0.035 | Tree loss: 2.078 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 029 | Total loss: 2.062 | Reg loss: 0.035 | Tree loss: 2.062 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 029 | Total loss: 2.074 | Reg loss: 0.035 | Tree loss: 2.074 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 029 | Total loss: 2.058 | Reg loss: 0.035 | Tree loss: 2.058 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 029 | Total loss: 2.035 | Reg loss: 0.035 | Tree loss: 2.035 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 029 | Total loss: 2.055 | Reg loss: 0.036 | Tree loss: 2.055 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 029 | Total loss: 2.027 | Reg loss: 0.036 | Tree loss: 2.027 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 029 | Total loss: 1.955 | Reg loss: 0.036 | Tree loss: 1.955 | Accuracy: 0.300781 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 011 / 029 | Total loss: 1.959 | Reg loss: 0.036 | Tree loss: 1.959 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 029 | Total loss: 1.968 | Reg loss: 0.036 | Tree loss: 1.968 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 029 | Total loss: 1.996 | Reg loss: 0.036 | Tree loss: 1.996 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 029 | Total loss: 1.982 | Reg loss: 0.036 | Tree loss: 1.982 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 029 | Total loss: 1.949 | Reg loss: 0.036 | Tree loss: 1.949 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 029 | Total loss: 1.917 | Reg loss: 0.036 | Tree loss: 1.917 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 029 | Total loss: 1.956 | Reg loss: 0.036 | Tree loss: 1.956 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 029 | Total loss: 1.957 | Reg loss: 0.036 | Tree loss: 1.957 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 029 | Total loss: 1.928 | Reg loss: 0.036 | Tree loss: 1.928 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 029 | Total loss: 1.952 | Reg loss: 0.036 | Tree loss: 1.952 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 029 | Total loss: 1.897 | Reg loss: 0.036 | Tree loss: 1.897 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 029 | Total loss: 1.913 | Reg loss: 0.037 | Tree loss: 1.913 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 029 | Total loss: 1.957 | Reg loss: 0.037 | Tree loss: 1.957 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 029 | Total loss: 1.911 | Reg loss: 0.037 | Tree loss: 1.911 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 029 | Total loss: 1.903 | Reg loss: 0.037 | Tree loss: 1.903 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 029 | Total loss: 1.841 | Reg loss: 0.037 | Tree loss: 1.841 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 029 | Total loss: 1.803 | Reg loss: 0.037 | Tree loss: 1.803 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 029 | Total loss: 1.845 | Reg loss: 0.037 | Tree loss: 1.845 | Accuracy: 0.259740 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 90 | Batch: 000 / 029 | Total loss: 2.168 | Reg loss: 0.035 | Tree loss: 2.168 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 029 | Total loss: 2.114 | Reg loss: 0.035 | Tree loss: 2.114 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 029 | Total loss: 2.165 | Reg loss: 0.035 | Tree loss: 2.165 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 029 | Total loss: 2.092 | Reg loss: 0.035 | Tree loss: 2.092 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 029 | Total loss: 2.050 | Reg loss: 0.035 | Tree loss: 2.050 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 029 | Total loss: 2.045 | Reg loss: 0.035 | Tree loss: 2.045 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 029 | Total loss: 2.069 | Reg loss: 0.035 | Tree loss: 2.069 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 029 | Total loss: 2.029 | Reg loss: 0.035 | Tree loss: 2.029 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 029 | Total loss: 2.115 | Reg loss: 0.035 | Tree loss: 2.115 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 029 | Total loss: 2.023 | Reg loss: 0.036 | Tree loss: 2.023 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 029 | Total loss: 2.027 | Reg loss: 0.036 | Tree loss: 2.027 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 029 | Total loss: 1.998 | Reg loss: 0.036 | Tree loss: 1.998 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 029 | Total loss: 1.956 | Reg loss: 0.036 | Tree loss: 1.956 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 029 | Total loss: 2.005 | Reg loss: 0.036 | Tree loss: 2.005 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 029 | Total loss: 2.029 | Reg loss: 0.036 | Tree loss: 2.029 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 029 | Total loss: 1.927 | Reg loss: 0.036 | Tree loss: 1.927 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 029 | Total loss: 1.943 | Reg loss: 0.036 | Tree loss: 1.943 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 029 | Total loss: 1.940 | Reg loss: 0.036 | Tree loss: 1.940 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 029 | Total loss: 1.927 | Reg loss: 0.036 | Tree loss: 1.927 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 029 | Total loss: 1.952 | Reg loss: 0.036 | Tree loss: 1.952 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 029 | Total loss: 1.881 | Reg loss: 0.036 | Tree loss: 1.881 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 029 | Total loss: 1.893 | Reg loss: 0.036 | Tree loss: 1.893 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 029 | Total loss: 1.933 | Reg loss: 0.036 | Tree loss: 1.933 | Accuracy: 0.318359 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 029 | Total loss: 1.879 | Reg loss: 0.037 | Tree loss: 1.879 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 029 | Total loss: 1.887 | Reg loss: 0.037 | Tree loss: 1.887 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 029 | Total loss: 1.892 | Reg loss: 0.037 | Tree loss: 1.892 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 029 | Total loss: 1.804 | Reg loss: 0.037 | Tree loss: 1.804 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 029 | Total loss: 1.812 | Reg loss: 0.037 | Tree loss: 1.812 | Accuracy: 0.333984 | 0.069 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 029 | Total loss: 1.835 | Reg loss: 0.037 | Tree loss: 1.835 | Accuracy: 0.337662 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 91 | Batch: 000 / 029 | Total loss: 2.216 | Reg loss: 0.035 | Tree loss: 2.216 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 029 | Total loss: 2.134 | Reg loss: 0.035 | Tree loss: 2.134 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 029 | Total loss: 2.136 | Reg loss: 0.035 | Tree loss: 2.136 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 029 | Total loss: 2.144 | Reg loss: 0.035 | Tree loss: 2.144 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 029 | Total loss: 2.115 | Reg loss: 0.035 | Tree loss: 2.115 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 029 | Total loss: 2.089 | Reg loss: 0.035 | Tree loss: 2.089 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 029 | Total loss: 2.089 | Reg loss: 0.035 | Tree loss: 2.089 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 029 | Total loss: 2.012 | Reg loss: 0.035 | Tree loss: 2.012 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 029 | Total loss: 2.101 | Reg loss: 0.035 | Tree loss: 2.101 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 029 | Total loss: 1.997 | Reg loss: 0.035 | Tree loss: 1.997 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 029 | Total loss: 1.978 | Reg loss: 0.036 | Tree loss: 1.978 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 029 | Total loss: 2.024 | Reg loss: 0.036 | Tree loss: 2.024 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 029 | Total loss: 1.953 | Reg loss: 0.036 | Tree loss: 1.953 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 029 | Total loss: 1.950 | Reg loss: 0.036 | Tree loss: 1.950 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 029 | Total loss: 1.918 | Reg loss: 0.036 | Tree loss: 1.918 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 029 | Total loss: 1.908 | Reg loss: 0.036 | Tree loss: 1.908 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 029 | Total loss: 1.872 | Reg loss: 0.036 | Tree loss: 1.872 | Accuracy: 0.335938 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 029 | Total loss: 1.900 | Reg loss: 0.036 | Tree loss: 1.900 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 029 | Total loss: 1.928 | Reg loss: 0.036 | Tree loss: 1.928 | Accuracy: 0.275391 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 019 / 029 | Total loss: 1.930 | Reg loss: 0.036 | Tree loss: 1.930 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 029 | Total loss: 1.907 | Reg loss: 0.036 | Tree loss: 1.907 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 029 | Total loss: 1.961 | Reg loss: 0.036 | Tree loss: 1.961 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 029 | Total loss: 1.885 | Reg loss: 0.036 | Tree loss: 1.885 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 029 | Total loss: 1.854 | Reg loss: 0.036 | Tree loss: 1.854 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 029 | Total loss: 1.905 | Reg loss: 0.037 | Tree loss: 1.905 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 029 | Total loss: 1.935 | Reg loss: 0.037 | Tree loss: 1.935 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 029 | Total loss: 1.787 | Reg loss: 0.037 | Tree loss: 1.787 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 029 | Total loss: 1.859 | Reg loss: 0.037 | Tree loss: 1.859 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 029 | Total loss: 1.860 | Reg loss: 0.037 | Tree loss: 1.860 | Accuracy: 0.337662 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 92 | Batch: 000 / 029 | Total loss: 2.163 | Reg loss: 0.035 | Tree loss: 2.163 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 029 | Total loss: 2.181 | Reg loss: 0.035 | Tree loss: 2.181 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 029 | Total loss: 2.168 | Reg loss: 0.035 | Tree loss: 2.168 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 029 | Total loss: 2.167 | Reg loss: 0.035 | Tree loss: 2.167 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 029 | Total loss: 2.079 | Reg loss: 0.035 | Tree loss: 2.079 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 029 | Total loss: 2.052 | Reg loss: 0.035 | Tree loss: 2.052 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 029 | Total loss: 2.042 | Reg loss: 0.035 | Tree loss: 2.042 | Accuracy: 0.333984 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 029 | Total loss: 2.088 | Reg loss: 0.035 | Tree loss: 2.088 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 029 | Total loss: 2.040 | Reg loss: 0.035 | Tree loss: 2.040 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 029 | Total loss: 2.076 | Reg loss: 0.035 | Tree loss: 2.076 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 029 | Total loss: 2.022 | Reg loss: 0.035 | Tree loss: 2.022 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 029 | Total loss: 1.929 | Reg loss: 0.036 | Tree loss: 1.929 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 029 | Total loss: 1.891 | Reg loss: 0.036 | Tree loss: 1.891 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 029 | Total loss: 2.006 | Reg loss: 0.036 | Tree loss: 2.006 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 029 | Total loss: 1.991 | Reg loss: 0.036 | Tree loss: 1.991 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 029 | Total loss: 1.974 | Reg loss: 0.036 | Tree loss: 1.974 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 029 | Total loss: 2.059 | Reg loss: 0.036 | Tree loss: 2.059 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 029 | Total loss: 1.905 | Reg loss: 0.036 | Tree loss: 1.905 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 029 | Total loss: 1.951 | Reg loss: 0.036 | Tree loss: 1.951 | Accuracy: 0.244141 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 029 | Total loss: 1.861 | Reg loss: 0.036 | Tree loss: 1.861 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 029 | Total loss: 1.842 | Reg loss: 0.036 | Tree loss: 1.842 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 029 | Total loss: 1.814 | Reg loss: 0.036 | Tree loss: 1.814 | Accuracy: 0.335938 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 029 | Total loss: 1.940 | Reg loss: 0.036 | Tree loss: 1.940 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 029 | Total loss: 1.845 | Reg loss: 0.036 | Tree loss: 1.845 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 029 | Total loss: 1.820 | Reg loss: 0.036 | Tree loss: 1.820 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 029 | Total loss: 1.827 | Reg loss: 0.037 | Tree loss: 1.827 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 029 | Total loss: 1.850 | Reg loss: 0.037 | Tree loss: 1.850 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 029 | Total loss: 1.851 | Reg loss: 0.037 | Tree loss: 1.851 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 029 | Total loss: 1.769 | Reg loss: 0.037 | Tree loss: 1.769 | Accuracy: 0.305195 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 93 | Batch: 000 / 029 | Total loss: 2.136 | Reg loss: 0.035 | Tree loss: 2.136 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 029 | Total loss: 2.174 | Reg loss: 0.035 | Tree loss: 2.174 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 029 | Total loss: 2.189 | Reg loss: 0.035 | Tree loss: 2.189 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 029 | Total loss: 2.129 | Reg loss: 0.035 | Tree loss: 2.129 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 029 | Total loss: 2.153 | Reg loss: 0.035 | Tree loss: 2.153 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 029 | Total loss: 2.119 | Reg loss: 0.035 | Tree loss: 2.119 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 029 | Total loss: 2.050 | Reg loss: 0.035 | Tree loss: 2.050 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 029 | Total loss: 2.088 | Reg loss: 0.035 | Tree loss: 2.088 | Accuracy: 0.330078 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 029 | Total loss: 2.048 | Reg loss: 0.035 | Tree loss: 2.048 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 029 | Total loss: 2.016 | Reg loss: 0.035 | Tree loss: 2.016 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 029 | Total loss: 2.005 | Reg loss: 0.035 | Tree loss: 2.005 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 029 | Total loss: 1.957 | Reg loss: 0.036 | Tree loss: 1.957 | Accuracy: 0.322266 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 029 | Total loss: 1.943 | Reg loss: 0.036 | Tree loss: 1.943 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 029 | Total loss: 1.943 | Reg loss: 0.036 | Tree loss: 1.943 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 029 | Total loss: 1.942 | Reg loss: 0.036 | Tree loss: 1.942 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 029 | Total loss: 1.930 | Reg loss: 0.036 | Tree loss: 1.930 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 029 | Total loss: 1.911 | Reg loss: 0.036 | Tree loss: 1.911 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 029 | Total loss: 1.900 | Reg loss: 0.036 | Tree loss: 1.900 | Accuracy: 0.341797 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 029 | Total loss: 1.860 | Reg loss: 0.036 | Tree loss: 1.860 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 029 | Total loss: 1.874 | Reg loss: 0.036 | Tree loss: 1.874 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 029 | Total loss: 1.963 | Reg loss: 0.036 | Tree loss: 1.963 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 029 | Total loss: 1.894 | Reg loss: 0.036 | Tree loss: 1.894 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 029 | Total loss: 1.861 | Reg loss: 0.036 | Tree loss: 1.861 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 029 | Total loss: 1.847 | Reg loss: 0.036 | Tree loss: 1.847 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 029 | Total loss: 1.812 | Reg loss: 0.036 | Tree loss: 1.812 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 029 | Total loss: 1.884 | Reg loss: 0.037 | Tree loss: 1.884 | Accuracy: 0.289062 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 026 / 029 | Total loss: 1.811 | Reg loss: 0.037 | Tree loss: 1.811 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 029 | Total loss: 1.893 | Reg loss: 0.037 | Tree loss: 1.893 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 029 | Total loss: 1.895 | Reg loss: 0.037 | Tree loss: 1.895 | Accuracy: 0.253247 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 94 | Batch: 000 / 029 | Total loss: 2.163 | Reg loss: 0.035 | Tree loss: 2.163 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 029 | Total loss: 2.150 | Reg loss: 0.035 | Tree loss: 2.150 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 029 | Total loss: 2.127 | Reg loss: 0.035 | Tree loss: 2.127 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 029 | Total loss: 2.119 | Reg loss: 0.035 | Tree loss: 2.119 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 029 | Total loss: 2.080 | Reg loss: 0.035 | Tree loss: 2.080 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 029 | Total loss: 2.066 | Reg loss: 0.035 | Tree loss: 2.066 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 029 | Total loss: 2.055 | Reg loss: 0.035 | Tree loss: 2.055 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 029 | Total loss: 2.028 | Reg loss: 0.035 | Tree loss: 2.028 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 029 | Total loss: 2.037 | Reg loss: 0.035 | Tree loss: 2.037 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 029 | Total loss: 2.030 | Reg loss: 0.035 | Tree loss: 2.030 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 029 | Total loss: 1.996 | Reg loss: 0.035 | Tree loss: 1.996 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 029 | Total loss: 2.017 | Reg loss: 0.035 | Tree loss: 2.017 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 029 | Total loss: 1.979 | Reg loss: 0.036 | Tree loss: 1.979 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 029 | Total loss: 1.966 | Reg loss: 0.036 | Tree loss: 1.966 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 029 | Total loss: 1.960 | Reg loss: 0.036 | Tree loss: 1.960 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 029 | Total loss: 1.922 | Reg loss: 0.036 | Tree loss: 1.922 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 029 | Total loss: 1.946 | Reg loss: 0.036 | Tree loss: 1.946 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 029 | Total loss: 1.912 | Reg loss: 0.036 | Tree loss: 1.912 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 029 | Total loss: 1.855 | Reg loss: 0.036 | Tree loss: 1.855 | Accuracy: 0.324219 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 029 | Total loss: 1.900 | Reg loss: 0.036 | Tree loss: 1.900 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 029 | Total loss: 1.868 | Reg loss: 0.036 | Tree loss: 1.868 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 029 | Total loss: 1.858 | Reg loss: 0.036 | Tree loss: 1.858 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 029 | Total loss: 1.858 | Reg loss: 0.036 | Tree loss: 1.858 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 029 | Total loss: 1.926 | Reg loss: 0.036 | Tree loss: 1.926 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 029 | Total loss: 1.870 | Reg loss: 0.036 | Tree loss: 1.870 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 029 | Total loss: 1.790 | Reg loss: 0.036 | Tree loss: 1.790 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 029 | Total loss: 1.861 | Reg loss: 0.037 | Tree loss: 1.861 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 029 | Total loss: 1.877 | Reg loss: 0.037 | Tree loss: 1.877 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 029 | Total loss: 1.996 | Reg loss: 0.037 | Tree loss: 1.996 | Accuracy: 0.305195 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 95 | Batch: 000 / 029 | Total loss: 2.147 | Reg loss: 0.035 | Tree loss: 2.147 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 029 | Total loss: 2.165 | Reg loss: 0.035 | Tree loss: 2.165 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 029 | Total loss: 2.193 | Reg loss: 0.035 | Tree loss: 2.193 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 029 | Total loss: 2.131 | Reg loss: 0.035 | Tree loss: 2.131 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 029 | Total loss: 2.109 | Reg loss: 0.035 | Tree loss: 2.109 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 029 | Total loss: 2.047 | Reg loss: 0.035 | Tree loss: 2.047 | Accuracy: 0.326172 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 029 | Total loss: 2.049 | Reg loss: 0.035 | Tree loss: 2.049 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 029 | Total loss: 1.997 | Reg loss: 0.035 | Tree loss: 1.997 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 029 | Total loss: 2.095 | Reg loss: 0.035 | Tree loss: 2.095 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 029 | Total loss: 1.932 | Reg loss: 0.035 | Tree loss: 1.932 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 029 | Total loss: 2.001 | Reg loss: 0.035 | Tree loss: 2.001 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 029 | Total loss: 1.947 | Reg loss: 0.035 | Tree loss: 1.947 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 029 | Total loss: 2.024 | Reg loss: 0.035 | Tree loss: 2.024 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 029 | Total loss: 1.951 | Reg loss: 0.036 | Tree loss: 1.951 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 029 | Total loss: 1.884 | Reg loss: 0.036 | Tree loss: 1.884 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 029 | Total loss: 1.921 | Reg loss: 0.036 | Tree loss: 1.921 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 029 | Total loss: 1.859 | Reg loss: 0.036 | Tree loss: 1.859 | Accuracy: 0.328125 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 029 | Total loss: 1.973 | Reg loss: 0.036 | Tree loss: 1.973 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 029 | Total loss: 1.934 | Reg loss: 0.036 | Tree loss: 1.934 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 029 | Total loss: 1.860 | Reg loss: 0.036 | Tree loss: 1.860 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 029 | Total loss: 1.907 | Reg loss: 0.036 | Tree loss: 1.907 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 029 | Total loss: 1.851 | Reg loss: 0.036 | Tree loss: 1.851 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 029 | Total loss: 1.847 | Reg loss: 0.036 | Tree loss: 1.847 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 029 | Total loss: 1.855 | Reg loss: 0.036 | Tree loss: 1.855 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 029 | Total loss: 1.912 | Reg loss: 0.036 | Tree loss: 1.912 | Accuracy: 0.251953 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 029 | Total loss: 1.830 | Reg loss: 0.036 | Tree loss: 1.830 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 029 | Total loss: 1.888 | Reg loss: 0.037 | Tree loss: 1.888 | Accuracy: 0.259766 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 029 | Total loss: 1.922 | Reg loss: 0.037 | Tree loss: 1.922 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 029 | Total loss: 1.738 | Reg loss: 0.037 | Tree loss: 1.738 | Accuracy: 0.350649 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 96 | Batch: 000 / 029 | Total loss: 2.138 | Reg loss: 0.035 | Tree loss: 2.138 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 029 | Total loss: 2.164 | Reg loss: 0.035 | Tree loss: 2.164 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 029 | Total loss: 2.079 | Reg loss: 0.035 | Tree loss: 2.079 | Accuracy: 0.294922 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Batch: 003 / 029 | Total loss: 2.133 | Reg loss: 0.035 | Tree loss: 2.133 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 029 | Total loss: 2.096 | Reg loss: 0.035 | Tree loss: 2.096 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 029 | Total loss: 2.115 | Reg loss: 0.035 | Tree loss: 2.115 | Accuracy: 0.257812 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 029 | Total loss: 2.028 | Reg loss: 0.035 | Tree loss: 2.028 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 029 | Total loss: 2.031 | Reg loss: 0.035 | Tree loss: 2.031 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 029 | Total loss: 2.025 | Reg loss: 0.035 | Tree loss: 2.025 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 029 | Total loss: 2.035 | Reg loss: 0.035 | Tree loss: 2.035 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 029 | Total loss: 2.009 | Reg loss: 0.035 | Tree loss: 2.009 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 029 | Total loss: 1.980 | Reg loss: 0.035 | Tree loss: 1.980 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 029 | Total loss: 1.965 | Reg loss: 0.035 | Tree loss: 1.965 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 029 | Total loss: 1.908 | Reg loss: 0.035 | Tree loss: 1.908 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 029 | Total loss: 1.964 | Reg loss: 0.036 | Tree loss: 1.964 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 029 | Total loss: 1.942 | Reg loss: 0.036 | Tree loss: 1.942 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 029 | Total loss: 1.905 | Reg loss: 0.036 | Tree loss: 1.905 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 029 | Total loss: 1.896 | Reg loss: 0.036 | Tree loss: 1.896 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 029 | Total loss: 1.929 | Reg loss: 0.036 | Tree loss: 1.929 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 029 | Total loss: 1.936 | Reg loss: 0.036 | Tree loss: 1.936 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 029 | Total loss: 1.872 | Reg loss: 0.036 | Tree loss: 1.872 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 029 | Total loss: 1.918 | Reg loss: 0.036 | Tree loss: 1.918 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 029 | Total loss: 1.783 | Reg loss: 0.036 | Tree loss: 1.783 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 029 | Total loss: 1.888 | Reg loss: 0.036 | Tree loss: 1.888 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 029 | Total loss: 1.818 | Reg loss: 0.036 | Tree loss: 1.818 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 029 | Total loss: 1.844 | Reg loss: 0.036 | Tree loss: 1.844 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 029 | Total loss: 1.903 | Reg loss: 0.036 | Tree loss: 1.903 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 029 | Total loss: 1.845 | Reg loss: 0.036 | Tree loss: 1.845 | Accuracy: 0.261719 | 0.069 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 029 | Total loss: 1.852 | Reg loss: 0.037 | Tree loss: 1.852 | Accuracy: 0.285714 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 97 | Batch: 000 / 029 | Total loss: 2.203 | Reg loss: 0.035 | Tree loss: 2.203 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 029 | Total loss: 2.155 | Reg loss: 0.035 | Tree loss: 2.155 | Accuracy: 0.246094 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 029 | Total loss: 2.089 | Reg loss: 0.035 | Tree loss: 2.089 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 029 | Total loss: 2.105 | Reg loss: 0.035 | Tree loss: 2.105 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 029 | Total loss: 2.152 | Reg loss: 0.035 | Tree loss: 2.152 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 029 | Total loss: 2.051 | Reg loss: 0.035 | Tree loss: 2.051 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 029 | Total loss: 1.976 | Reg loss: 0.035 | Tree loss: 1.976 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 029 | Total loss: 2.037 | Reg loss: 0.035 | Tree loss: 2.037 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 029 | Total loss: 1.994 | Reg loss: 0.035 | Tree loss: 1.994 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 029 | Total loss: 2.034 | Reg loss: 0.035 | Tree loss: 2.034 | Accuracy: 0.244141 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 029 | Total loss: 2.028 | Reg loss: 0.035 | Tree loss: 2.028 | Accuracy: 0.253906 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 029 | Total loss: 1.997 | Reg loss: 0.035 | Tree loss: 1.997 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 029 | Total loss: 1.924 | Reg loss: 0.035 | Tree loss: 1.924 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 029 | Total loss: 1.935 | Reg loss: 0.035 | Tree loss: 1.935 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 029 | Total loss: 1.985 | Reg loss: 0.036 | Tree loss: 1.985 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 029 | Total loss: 1.934 | Reg loss: 0.036 | Tree loss: 1.934 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 029 | Total loss: 1.909 | Reg loss: 0.036 | Tree loss: 1.909 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 029 | Total loss: 1.883 | Reg loss: 0.036 | Tree loss: 1.883 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 029 | Total loss: 1.861 | Reg loss: 0.036 | Tree loss: 1.861 | Accuracy: 0.330078 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 029 | Total loss: 1.889 | Reg loss: 0.036 | Tree loss: 1.889 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 029 | Total loss: 1.971 | Reg loss: 0.036 | Tree loss: 1.971 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 029 | Total loss: 1.897 | Reg loss: 0.036 | Tree loss: 1.897 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 029 | Total loss: 1.855 | Reg loss: 0.036 | Tree loss: 1.855 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 029 | Total loss: 1.874 | Reg loss: 0.036 | Tree loss: 1.874 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 029 | Total loss: 1.864 | Reg loss: 0.036 | Tree loss: 1.864 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 029 | Total loss: 1.786 | Reg loss: 0.036 | Tree loss: 1.786 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 029 | Total loss: 1.872 | Reg loss: 0.036 | Tree loss: 1.872 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 029 | Total loss: 1.804 | Reg loss: 0.036 | Tree loss: 1.804 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 029 | Total loss: 1.926 | Reg loss: 0.037 | Tree loss: 1.926 | Accuracy: 0.305195 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 98 | Batch: 000 / 029 | Total loss: 2.136 | Reg loss: 0.035 | Tree loss: 2.136 | Accuracy: 0.273438 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 029 | Total loss: 2.156 | Reg loss: 0.035 | Tree loss: 2.156 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 029 | Total loss: 2.020 | Reg loss: 0.035 | Tree loss: 2.020 | Accuracy: 0.316406 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 029 | Total loss: 2.123 | Reg loss: 0.035 | Tree loss: 2.123 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 029 | Total loss: 2.115 | Reg loss: 0.035 | Tree loss: 2.115 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 029 | Total loss: 2.050 | Reg loss: 0.035 | Tree loss: 2.050 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 029 | Total loss: 2.078 | Reg loss: 0.035 | Tree loss: 2.078 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 029 | Total loss: 2.052 | Reg loss: 0.035 | Tree loss: 2.052 | Accuracy: 0.283203 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 029 | Total loss: 2.006 | Reg loss: 0.035 | Tree loss: 2.006 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 029 | Total loss: 1.983 | Reg loss: 0.035 | Tree loss: 1.983 | Accuracy: 0.275391 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Batch: 010 / 029 | Total loss: 2.010 | Reg loss: 0.035 | Tree loss: 2.010 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 029 | Total loss: 2.027 | Reg loss: 0.035 | Tree loss: 2.027 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 029 | Total loss: 2.059 | Reg loss: 0.035 | Tree loss: 2.059 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 029 | Total loss: 1.990 | Reg loss: 0.035 | Tree loss: 1.990 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 029 | Total loss: 1.944 | Reg loss: 0.035 | Tree loss: 1.944 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 029 | Total loss: 1.937 | Reg loss: 0.036 | Tree loss: 1.937 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 029 | Total loss: 1.861 | Reg loss: 0.036 | Tree loss: 1.861 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 029 | Total loss: 1.926 | Reg loss: 0.036 | Tree loss: 1.926 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 029 | Total loss: 1.933 | Reg loss: 0.036 | Tree loss: 1.933 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 029 | Total loss: 1.870 | Reg loss: 0.036 | Tree loss: 1.870 | Accuracy: 0.277344 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 029 | Total loss: 1.877 | Reg loss: 0.036 | Tree loss: 1.877 | Accuracy: 0.292969 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 029 | Total loss: 1.823 | Reg loss: 0.036 | Tree loss: 1.823 | Accuracy: 0.320312 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 029 | Total loss: 1.845 | Reg loss: 0.036 | Tree loss: 1.845 | Accuracy: 0.267578 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 029 | Total loss: 1.884 | Reg loss: 0.036 | Tree loss: 1.884 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 029 | Total loss: 1.835 | Reg loss: 0.036 | Tree loss: 1.835 | Accuracy: 0.304688 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 029 | Total loss: 1.856 | Reg loss: 0.036 | Tree loss: 1.856 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 029 | Total loss: 1.811 | Reg loss: 0.036 | Tree loss: 1.811 | Accuracy: 0.300781 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 029 | Total loss: 1.807 | Reg loss: 0.036 | Tree loss: 1.807 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 029 | Total loss: 1.873 | Reg loss: 0.036 | Tree loss: 1.873 | Accuracy: 0.279221 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 99 | Batch: 000 / 029 | Total loss: 2.126 | Reg loss: 0.035 | Tree loss: 2.126 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 029 | Total loss: 2.180 | Reg loss: 0.035 | Tree loss: 2.180 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 029 | Total loss: 2.149 | Reg loss: 0.035 | Tree loss: 2.149 | Accuracy: 0.263672 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 029 | Total loss: 2.064 | Reg loss: 0.035 | Tree loss: 2.064 | Accuracy: 0.308594 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 029 | Total loss: 2.011 | Reg loss: 0.035 | Tree loss: 2.011 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 029 | Total loss: 2.088 | Reg loss: 0.035 | Tree loss: 2.088 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 029 | Total loss: 2.038 | Reg loss: 0.035 | Tree loss: 2.038 | Accuracy: 0.291016 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 029 | Total loss: 2.081 | Reg loss: 0.035 | Tree loss: 2.081 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 029 | Total loss: 2.126 | Reg loss: 0.035 | Tree loss: 2.126 | Accuracy: 0.265625 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 029 | Total loss: 1.989 | Reg loss: 0.035 | Tree loss: 1.989 | Accuracy: 0.306641 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 029 | Total loss: 2.003 | Reg loss: 0.035 | Tree loss: 2.003 | Accuracy: 0.314453 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 029 | Total loss: 1.963 | Reg loss: 0.035 | Tree loss: 1.963 | Accuracy: 0.287109 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 029 | Total loss: 1.957 | Reg loss: 0.035 | Tree loss: 1.957 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 029 | Total loss: 2.001 | Reg loss: 0.035 | Tree loss: 2.001 | Accuracy: 0.281250 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 029 | Total loss: 1.937 | Reg loss: 0.035 | Tree loss: 1.937 | Accuracy: 0.269531 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 029 | Total loss: 1.894 | Reg loss: 0.036 | Tree loss: 1.894 | Accuracy: 0.296875 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 029 | Total loss: 1.881 | Reg loss: 0.036 | Tree loss: 1.881 | Accuracy: 0.298828 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 029 | Total loss: 1.909 | Reg loss: 0.036 | Tree loss: 1.909 | Accuracy: 0.255859 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 029 | Total loss: 1.888 | Reg loss: 0.036 | Tree loss: 1.888 | Accuracy: 0.279297 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 029 | Total loss: 1.835 | Reg loss: 0.036 | Tree loss: 1.835 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 029 | Total loss: 1.918 | Reg loss: 0.036 | Tree loss: 1.918 | Accuracy: 0.294922 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 029 | Total loss: 1.826 | Reg loss: 0.036 | Tree loss: 1.826 | Accuracy: 0.312500 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 029 | Total loss: 1.913 | Reg loss: 0.036 | Tree loss: 1.913 | Accuracy: 0.302734 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 029 | Total loss: 1.855 | Reg loss: 0.036 | Tree loss: 1.855 | Accuracy: 0.285156 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 029 | Total loss: 1.881 | Reg loss: 0.036 | Tree loss: 1.881 | Accuracy: 0.275391 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 029 | Total loss: 1.812 | Reg loss: 0.036 | Tree loss: 1.812 | Accuracy: 0.271484 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 029 | Total loss: 1.858 | Reg loss: 0.036 | Tree loss: 1.858 | Accuracy: 0.310547 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 029 | Total loss: 1.770 | Reg loss: 0.036 | Tree loss: 1.770 | Accuracy: 0.289062 | 0.069 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 029 | Total loss: 1.871 | Reg loss: 0.036 | Tree loss: 1.871 | Accuracy: 0.259740 | 0.069 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f5b4cfda724d869392e8c182bbe351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea345281f9f44664a9a256a23d789e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e24e5c023e44e539c2c78d3c5c1a77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36b9b42ff2242c8a0007c8f625a0836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 5.185185185185185\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7804\n",
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "1659\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "2261\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "345\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "2098\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "56\n",
      "============== Pattern 23 ==============\n",
      "267\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "Average comprehensibility: 26.444444444444443\n",
      "std comprehensibility: 4.755763235340599\n",
      "var comprehensibility: 22.61728395061729\n",
      "minimum comprehensibility: 16\n",
      "maximum comprehensibility: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
