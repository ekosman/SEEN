{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 128\n",
    "tree_depth = 12\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.222156524658203 | KNN Loss: 6.2308759689331055 | BCE Loss: 1.9912800788879395\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.250527381896973 | KNN Loss: 6.230742454528809 | BCE Loss: 2.019784927368164\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.177160263061523 | KNN Loss: 6.230837821960449 | BCE Loss: 1.9463227987289429\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.158607482910156 | KNN Loss: 6.2306413650512695 | BCE Loss: 1.9279663562774658\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.189087867736816 | KNN Loss: 6.230562686920166 | BCE Loss: 1.9585249423980713\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.206348419189453 | KNN Loss: 6.230489253997803 | BCE Loss: 1.97585928440094\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.168549537658691 | KNN Loss: 6.230339050292969 | BCE Loss: 1.9382104873657227\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.182073593139648 | KNN Loss: 6.230133056640625 | BCE Loss: 1.9519405364990234\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.113561630249023 | KNN Loss: 6.229896545410156 | BCE Loss: 1.8836650848388672\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.165735244750977 | KNN Loss: 6.229830741882324 | BCE Loss: 1.9359041452407837\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.111108779907227 | KNN Loss: 6.230179786682129 | BCE Loss: 1.8809287548065186\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.150984764099121 | KNN Loss: 6.229585647583008 | BCE Loss: 1.9213988780975342\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.143277168273926 | KNN Loss: 6.229593276977539 | BCE Loss: 1.9136838912963867\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.043752670288086 | KNN Loss: 6.229527473449707 | BCE Loss: 1.8142248392105103\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.102785110473633 | KNN Loss: 6.229386329650879 | BCE Loss: 1.8733985424041748\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.092802047729492 | KNN Loss: 6.229145526885986 | BCE Loss: 1.8636560440063477\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.104915618896484 | KNN Loss: 6.22912073135376 | BCE Loss: 1.8757953643798828\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.06900405883789 | KNN Loss: 6.228950500488281 | BCE Loss: 1.8400530815124512\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.107356071472168 | KNN Loss: 6.22875452041626 | BCE Loss: 1.8786015510559082\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.063179016113281 | KNN Loss: 6.228764057159424 | BCE Loss: 1.8344154357910156\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.032271385192871 | KNN Loss: 6.228270053863525 | BCE Loss: 1.8040012121200562\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.038585662841797 | KNN Loss: 6.228368759155273 | BCE Loss: 1.8102171421051025\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.025588989257812 | KNN Loss: 6.228265285491943 | BCE Loss: 1.7973237037658691\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.040607452392578 | KNN Loss: 6.227586269378662 | BCE Loss: 1.8130210638046265\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 8.035112380981445 | KNN Loss: 6.228187084197998 | BCE Loss: 1.8069257736206055\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.9913434982299805 | KNN Loss: 6.227604866027832 | BCE Loss: 1.7637388706207275\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.995816230773926 | KNN Loss: 6.227741241455078 | BCE Loss: 1.7680752277374268\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.958620071411133 | KNN Loss: 6.227123260498047 | BCE Loss: 1.7314966917037964\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.979862213134766 | KNN Loss: 6.226604461669922 | BCE Loss: 1.7532575130462646\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.977690696716309 | KNN Loss: 6.2265448570251465 | BCE Loss: 1.751145601272583\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.981669902801514 | KNN Loss: 6.226545810699463 | BCE Loss: 1.7551239728927612\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.936943531036377 | KNN Loss: 6.226119518280029 | BCE Loss: 1.7108241319656372\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.944642543792725 | KNN Loss: 6.2254252433776855 | BCE Loss: 1.7192171812057495\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.902640342712402 | KNN Loss: 6.225793838500977 | BCE Loss: 1.6768465042114258\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.863619804382324 | KNN Loss: 6.224541187286377 | BCE Loss: 1.6390788555145264\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.879015922546387 | KNN Loss: 6.22449254989624 | BCE Loss: 1.6545231342315674\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.859114646911621 | KNN Loss: 6.2239603996276855 | BCE Loss: 1.6351544857025146\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.855412483215332 | KNN Loss: 6.223293304443359 | BCE Loss: 1.6321191787719727\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.874879837036133 | KNN Loss: 6.222224235534668 | BCE Loss: 1.6526557207107544\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.822748184204102 | KNN Loss: 6.22284460067749 | BCE Loss: 1.5999033451080322\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.876542091369629 | KNN Loss: 6.221104621887207 | BCE Loss: 1.6554373502731323\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.790888786315918 | KNN Loss: 6.221106052398682 | BCE Loss: 1.5697827339172363\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.74785041809082 | KNN Loss: 6.220006942749023 | BCE Loss: 1.5278432369232178\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.728416442871094 | KNN Loss: 6.220203876495361 | BCE Loss: 1.5082125663757324\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.754940032958984 | KNN Loss: 6.2184834480285645 | BCE Loss: 1.536456823348999\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.720585823059082 | KNN Loss: 6.215882301330566 | BCE Loss: 1.5047035217285156\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.679465293884277 | KNN Loss: 6.216121673583984 | BCE Loss: 1.463343858718872\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.665611267089844 | KNN Loss: 6.215128421783447 | BCE Loss: 1.4504828453063965\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.713409900665283 | KNN Loss: 6.2145562171936035 | BCE Loss: 1.4988536834716797\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.631621837615967 | KNN Loss: 6.212111473083496 | BCE Loss: 1.4195103645324707\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.617395401000977 | KNN Loss: 6.209238052368164 | BCE Loss: 1.4081571102142334\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.587442874908447 | KNN Loss: 6.205624580383301 | BCE Loss: 1.381818413734436\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.52584981918335 | KNN Loss: 6.205966949462891 | BCE Loss: 1.319882869720459\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.5151848793029785 | KNN Loss: 6.202861785888672 | BCE Loss: 1.3123230934143066\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.510234832763672 | KNN Loss: 6.199321269989014 | BCE Loss: 1.3109135627746582\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.4568328857421875 | KNN Loss: 6.194806098937988 | BCE Loss: 1.2620265483856201\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.457932949066162 | KNN Loss: 6.193556308746338 | BCE Loss: 1.2643766403198242\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.449611663818359 | KNN Loss: 6.186864852905273 | BCE Loss: 1.2627466917037964\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.44578742980957 | KNN Loss: 6.180701732635498 | BCE Loss: 1.2650859355926514\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.395572662353516 | KNN Loss: 6.176313877105713 | BCE Loss: 1.2192585468292236\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.339052677154541 | KNN Loss: 6.172194480895996 | BCE Loss: 1.166858196258545\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.353067874908447 | KNN Loss: 6.162117958068848 | BCE Loss: 1.1909499168395996\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.3522539138793945 | KNN Loss: 6.156938552856445 | BCE Loss: 1.1953151226043701\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.3020429611206055 | KNN Loss: 6.152308940887451 | BCE Loss: 1.1497340202331543\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.301517963409424 | KNN Loss: 6.143702983856201 | BCE Loss: 1.157814860343933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.244460582733154 | KNN Loss: 6.120423793792725 | BCE Loss: 1.1240366697311401\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.239505767822266 | KNN Loss: 6.110269069671631 | BCE Loss: 1.1292364597320557\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.203968048095703 | KNN Loss: 6.089426040649414 | BCE Loss: 1.11454176902771\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 7.219274044036865 | KNN Loss: 6.091695785522461 | BCE Loss: 1.1275783777236938\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 7.180758476257324 | KNN Loss: 6.0650410652160645 | BCE Loss: 1.1157176494598389\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 7.187288284301758 | KNN Loss: 6.0549516677856445 | BCE Loss: 1.1323363780975342\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 7.148846626281738 | KNN Loss: 6.03350305557251 | BCE Loss: 1.115343689918518\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 7.079246997833252 | KNN Loss: 5.996521472930908 | BCE Loss: 1.0827255249023438\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 7.0696492195129395 | KNN Loss: 5.976954460144043 | BCE Loss: 1.0926947593688965\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 7.035922050476074 | KNN Loss: 5.957106113433838 | BCE Loss: 1.0788160562515259\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 7.037320137023926 | KNN Loss: 5.924886226654053 | BCE Loss: 1.112433910369873\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 7.009902000427246 | KNN Loss: 5.907741546630859 | BCE Loss: 1.1021604537963867\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 6.939625263214111 | KNN Loss: 5.847634792327881 | BCE Loss: 1.09199059009552\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 6.872337818145752 | KNN Loss: 5.807824611663818 | BCE Loss: 1.0645132064819336\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 6.879428386688232 | KNN Loss: 5.771683216094971 | BCE Loss: 1.1077451705932617\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 6.832912445068359 | KNN Loss: 5.733043193817139 | BCE Loss: 1.0998690128326416\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 6.779215335845947 | KNN Loss: 5.7007737159729 | BCE Loss: 1.0784415006637573\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 6.703583717346191 | KNN Loss: 5.639688014984131 | BCE Loss: 1.0638957023620605\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 6.691395282745361 | KNN Loss: 5.605442523956299 | BCE Loss: 1.085952639579773\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 6.683744430541992 | KNN Loss: 5.5755696296691895 | BCE Loss: 1.1081748008728027\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 6.566920280456543 | KNN Loss: 5.502763271331787 | BCE Loss: 1.064157247543335\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 6.553193092346191 | KNN Loss: 5.485095024108887 | BCE Loss: 1.0680980682373047\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 6.519294261932373 | KNN Loss: 5.436758041381836 | BCE Loss: 1.082536220550537\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 6.471896648406982 | KNN Loss: 5.39912223815918 | BCE Loss: 1.0727744102478027\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 6.417093276977539 | KNN Loss: 5.3514556884765625 | BCE Loss: 1.0656375885009766\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 6.389519691467285 | KNN Loss: 5.32072114944458 | BCE Loss: 1.068798542022705\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 6.371343612670898 | KNN Loss: 5.2928290367126465 | BCE Loss: 1.078514575958252\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 6.345501899719238 | KNN Loss: 5.2773237228393555 | BCE Loss: 1.0681781768798828\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 6.317817211151123 | KNN Loss: 5.247865676879883 | BCE Loss: 1.0699515342712402\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 6.321103096008301 | KNN Loss: 5.214831352233887 | BCE Loss: 1.106271743774414\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 6.275078773498535 | KNN Loss: 5.199112892150879 | BCE Loss: 1.0759658813476562\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 6.237552165985107 | KNN Loss: 5.179501056671143 | BCE Loss: 1.0580511093139648\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 6.231285572052002 | KNN Loss: 5.174777984619141 | BCE Loss: 1.0565074682235718\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 6.223658084869385 | KNN Loss: 5.163697242736816 | BCE Loss: 1.059960961341858\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 6.219439506530762 | KNN Loss: 5.150609016418457 | BCE Loss: 1.0688304901123047\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 6.2272467613220215 | KNN Loss: 5.1451416015625 | BCE Loss: 1.0821051597595215\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 6.231985092163086 | KNN Loss: 5.159176349639893 | BCE Loss: 1.0728087425231934\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 6.211433410644531 | KNN Loss: 5.126737117767334 | BCE Loss: 1.0846964120864868\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.204524517059326 | KNN Loss: 5.119349479675293 | BCE Loss: 1.0851750373840332\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.201265335083008 | KNN Loss: 5.125926494598389 | BCE Loss: 1.0753390789031982\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.185260772705078 | KNN Loss: 5.123439788818359 | BCE Loss: 1.0618208646774292\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.152672290802002 | KNN Loss: 5.088560104370117 | BCE Loss: 1.0641123056411743\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.173138618469238 | KNN Loss: 5.120807647705078 | BCE Loss: 1.0523308515548706\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.156583786010742 | KNN Loss: 5.0984954833984375 | BCE Loss: 1.0580885410308838\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.132955551147461 | KNN Loss: 5.094236373901367 | BCE Loss: 1.0387189388275146\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.164591312408447 | KNN Loss: 5.09522819519043 | BCE Loss: 1.0693631172180176\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.146541118621826 | KNN Loss: 5.110107898712158 | BCE Loss: 1.036433219909668\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.179261207580566 | KNN Loss: 5.099499702453613 | BCE Loss: 1.0797617435455322\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.1417436599731445 | KNN Loss: 5.086058139801025 | BCE Loss: 1.0556855201721191\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.155769348144531 | KNN Loss: 5.098453044891357 | BCE Loss: 1.057316541671753\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.156279563903809 | KNN Loss: 5.109646797180176 | BCE Loss: 1.0466326475143433\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.156073570251465 | KNN Loss: 5.097309112548828 | BCE Loss: 1.0587645769119263\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.109541416168213 | KNN Loss: 5.085092544555664 | BCE Loss: 1.0244487524032593\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.197894096374512 | KNN Loss: 5.122513294219971 | BCE Loss: 1.075380802154541\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.14244270324707 | KNN Loss: 5.070356369018555 | BCE Loss: 1.0720863342285156\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.117318153381348 | KNN Loss: 5.071948528289795 | BCE Loss: 1.0453697443008423\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.1605329513549805 | KNN Loss: 5.087148666381836 | BCE Loss: 1.0733840465545654\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.140049934387207 | KNN Loss: 5.069261074066162 | BCE Loss: 1.070789098739624\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.164389610290527 | KNN Loss: 5.093790054321289 | BCE Loss: 1.0705997943878174\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.173831939697266 | KNN Loss: 5.095073699951172 | BCE Loss: 1.0787584781646729\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.175380706787109 | KNN Loss: 5.089723110198975 | BCE Loss: 1.0856573581695557\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.167777061462402 | KNN Loss: 5.081339359283447 | BCE Loss: 1.0864375829696655\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.148828506469727 | KNN Loss: 5.081071376800537 | BCE Loss: 1.0677573680877686\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.123378753662109 | KNN Loss: 5.074198246002197 | BCE Loss: 1.049180507659912\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.131124496459961 | KNN Loss: 5.086754322052002 | BCE Loss: 1.044370174407959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.100081443786621 | KNN Loss: 5.0664381980896 | BCE Loss: 1.033643126487732\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.133056640625 | KNN Loss: 5.060835361480713 | BCE Loss: 1.0722215175628662\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.144747734069824 | KNN Loss: 5.062102794647217 | BCE Loss: 1.0826447010040283\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.1050519943237305 | KNN Loss: 5.070614814758301 | BCE Loss: 1.0344369411468506\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.1240739822387695 | KNN Loss: 5.068467617034912 | BCE Loss: 1.0556062459945679\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.120849609375 | KNN Loss: 5.079685688018799 | BCE Loss: 1.0411639213562012\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.108098983764648 | KNN Loss: 5.069944858551025 | BCE Loss: 1.038154125213623\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.146769046783447 | KNN Loss: 5.057798862457275 | BCE Loss: 1.0889701843261719\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.160135746002197 | KNN Loss: 5.102927207946777 | BCE Loss: 1.05720853805542\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.118646621704102 | KNN Loss: 5.074175834655762 | BCE Loss: 1.0444707870483398\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.1466898918151855 | KNN Loss: 5.088726043701172 | BCE Loss: 1.0579639673233032\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.153831481933594 | KNN Loss: 5.074536323547363 | BCE Loss: 1.07929527759552\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.127786636352539 | KNN Loss: 5.071228981018066 | BCE Loss: 1.0565576553344727\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.138223171234131 | KNN Loss: 5.074668884277344 | BCE Loss: 1.063554286956787\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.101329803466797 | KNN Loss: 5.066654205322266 | BCE Loss: 1.0346753597259521\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.126607418060303 | KNN Loss: 5.055680751800537 | BCE Loss: 1.0709267854690552\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.086738109588623 | KNN Loss: 5.05519437789917 | BCE Loss: 1.0315437316894531\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.117583274841309 | KNN Loss: 5.060863494873047 | BCE Loss: 1.0567197799682617\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.132366180419922 | KNN Loss: 5.072505950927734 | BCE Loss: 1.0598602294921875\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.086207389831543 | KNN Loss: 5.057949542999268 | BCE Loss: 1.0282576084136963\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.172794342041016 | KNN Loss: 5.090055465698242 | BCE Loss: 1.0827387571334839\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.134800910949707 | KNN Loss: 5.069184303283691 | BCE Loss: 1.0656167268753052\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.101778984069824 | KNN Loss: 5.07102108001709 | BCE Loss: 1.0307577848434448\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.090425491333008 | KNN Loss: 5.060291767120361 | BCE Loss: 1.030133843421936\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.098077774047852 | KNN Loss: 5.053999423980713 | BCE Loss: 1.0440783500671387\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.155552864074707 | KNN Loss: 5.057973384857178 | BCE Loss: 1.0975794792175293\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.141912460327148 | KNN Loss: 5.079793930053711 | BCE Loss: 1.0621185302734375\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.086635589599609 | KNN Loss: 5.047924995422363 | BCE Loss: 1.038710594177246\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.157837867736816 | KNN Loss: 5.090007305145264 | BCE Loss: 1.0678303241729736\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.063274383544922 | KNN Loss: 5.048275947570801 | BCE Loss: 1.0149986743927002\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.10709810256958 | KNN Loss: 5.062215805053711 | BCE Loss: 1.0448824167251587\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.105630874633789 | KNN Loss: 5.056076526641846 | BCE Loss: 1.0495545864105225\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.117391586303711 | KNN Loss: 5.07036018371582 | BCE Loss: 1.0470316410064697\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.1229939460754395 | KNN Loss: 5.065062046051025 | BCE Loss: 1.057931900024414\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.116446018218994 | KNN Loss: 5.083329677581787 | BCE Loss: 1.033116340637207\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.101985931396484 | KNN Loss: 5.056286334991455 | BCE Loss: 1.0456998348236084\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.095981597900391 | KNN Loss: 5.053195476531982 | BCE Loss: 1.042785882949829\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.169529438018799 | KNN Loss: 5.091003894805908 | BCE Loss: 1.0785255432128906\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.115185260772705 | KNN Loss: 5.053616046905518 | BCE Loss: 1.061569094657898\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.130918979644775 | KNN Loss: 5.052161693572998 | BCE Loss: 1.0787572860717773\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.1380085945129395 | KNN Loss: 5.064599990844727 | BCE Loss: 1.073408603668213\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.106878757476807 | KNN Loss: 5.060450553894043 | BCE Loss: 1.0464282035827637\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.148004531860352 | KNN Loss: 5.117039680480957 | BCE Loss: 1.0309646129608154\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.13684606552124 | KNN Loss: 5.064258098602295 | BCE Loss: 1.0725879669189453\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.093111991882324 | KNN Loss: 5.053580284118652 | BCE Loss: 1.0395314693450928\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.110733985900879 | KNN Loss: 5.053695201873779 | BCE Loss: 1.0570387840270996\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.101775169372559 | KNN Loss: 5.059603214263916 | BCE Loss: 1.0421719551086426\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.114994049072266 | KNN Loss: 5.064986705780029 | BCE Loss: 1.0500075817108154\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.094230651855469 | KNN Loss: 5.046777725219727 | BCE Loss: 1.0474529266357422\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.111357688903809 | KNN Loss: 5.0461344718933105 | BCE Loss: 1.065222978591919\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.096125602722168 | KNN Loss: 5.054396152496338 | BCE Loss: 1.04172945022583\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.111725807189941 | KNN Loss: 5.0485029220581055 | BCE Loss: 1.063222885131836\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.1251420974731445 | KNN Loss: 5.052218437194824 | BCE Loss: 1.0729238986968994\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.094308853149414 | KNN Loss: 5.0431904792785645 | BCE Loss: 1.0511186122894287\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.128199577331543 | KNN Loss: 5.065257549285889 | BCE Loss: 1.0629420280456543\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.071281433105469 | KNN Loss: 5.037728786468506 | BCE Loss: 1.0335524082183838\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.1096296310424805 | KNN Loss: 5.055912494659424 | BCE Loss: 1.0537168979644775\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.154926300048828 | KNN Loss: 5.058506011962891 | BCE Loss: 1.0964205265045166\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.107252597808838 | KNN Loss: 5.053273677825928 | BCE Loss: 1.0539789199829102\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.100597858428955 | KNN Loss: 5.046611309051514 | BCE Loss: 1.0539865493774414\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.088644027709961 | KNN Loss: 5.056055545806885 | BCE Loss: 1.0325884819030762\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.08108377456665 | KNN Loss: 5.048933506011963 | BCE Loss: 1.0321502685546875\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.105823993682861 | KNN Loss: 5.0430731773376465 | BCE Loss: 1.0627508163452148\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.103931427001953 | KNN Loss: 5.061117172241211 | BCE Loss: 1.042814016342163\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.116845607757568 | KNN Loss: 5.0611348152160645 | BCE Loss: 1.0557109117507935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.132634162902832 | KNN Loss: 5.058141708374023 | BCE Loss: 1.0744925737380981\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.13221549987793 | KNN Loss: 5.069360256195068 | BCE Loss: 1.0628553628921509\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.0787672996521 | KNN Loss: 5.034670352935791 | BCE Loss: 1.0440969467163086\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.0907511711120605 | KNN Loss: 5.040624618530273 | BCE Loss: 1.050126552581787\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.109333515167236 | KNN Loss: 5.041493892669678 | BCE Loss: 1.067839503288269\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.110171318054199 | KNN Loss: 5.057861804962158 | BCE Loss: 1.052309274673462\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.108434677124023 | KNN Loss: 5.054862976074219 | BCE Loss: 1.0535714626312256\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.096025466918945 | KNN Loss: 5.044345855712891 | BCE Loss: 1.0516793727874756\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.097407817840576 | KNN Loss: 5.050382614135742 | BCE Loss: 1.0470250844955444\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.119106292724609 | KNN Loss: 5.065855503082275 | BCE Loss: 1.053250789642334\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.110136032104492 | KNN Loss: 5.047927379608154 | BCE Loss: 1.0622084140777588\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.112285614013672 | KNN Loss: 5.043873310089111 | BCE Loss: 1.0684123039245605\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.102028846740723 | KNN Loss: 5.0513458251953125 | BCE Loss: 1.050682783126831\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.097692012786865 | KNN Loss: 5.041058540344238 | BCE Loss: 1.056633472442627\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.134182453155518 | KNN Loss: 5.07547664642334 | BCE Loss: 1.0587058067321777\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.1052446365356445 | KNN Loss: 5.049496650695801 | BCE Loss: 1.0557482242584229\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.093323230743408 | KNN Loss: 5.044034481048584 | BCE Loss: 1.0492887496948242\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.140058994293213 | KNN Loss: 5.063037395477295 | BCE Loss: 1.077021598815918\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.094916343688965 | KNN Loss: 5.0501580238342285 | BCE Loss: 1.0447582006454468\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.09506893157959 | KNN Loss: 5.05124568939209 | BCE Loss: 1.0438231229782104\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.0862507820129395 | KNN Loss: 5.042994499206543 | BCE Loss: 1.0432562828063965\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.093772888183594 | KNN Loss: 5.045283317565918 | BCE Loss: 1.0484898090362549\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.07814884185791 | KNN Loss: 5.046558856964111 | BCE Loss: 1.031590223312378\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.1140217781066895 | KNN Loss: 5.050934314727783 | BCE Loss: 1.0630875825881958\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.09604549407959 | KNN Loss: 5.047150135040283 | BCE Loss: 1.0488955974578857\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.099989414215088 | KNN Loss: 5.039551258087158 | BCE Loss: 1.0604381561279297\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.099381446838379 | KNN Loss: 5.046323299407959 | BCE Loss: 1.053058385848999\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.100886344909668 | KNN Loss: 5.044552326202393 | BCE Loss: 1.0563340187072754\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.093168258666992 | KNN Loss: 5.046547889709473 | BCE Loss: 1.04662024974823\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.078093528747559 | KNN Loss: 5.037757396697998 | BCE Loss: 1.040336012840271\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.072271347045898 | KNN Loss: 5.026671886444092 | BCE Loss: 1.0455992221832275\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.105755805969238 | KNN Loss: 5.064247131347656 | BCE Loss: 1.041508436203003\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.084782123565674 | KNN Loss: 5.0345988273620605 | BCE Loss: 1.0501834154129028\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.096858978271484 | KNN Loss: 5.034656047821045 | BCE Loss: 1.0622026920318604\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.078639030456543 | KNN Loss: 5.04664945602417 | BCE Loss: 1.031989336013794\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.073495864868164 | KNN Loss: 5.0428338050842285 | BCE Loss: 1.030661940574646\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.102349758148193 | KNN Loss: 5.064105987548828 | BCE Loss: 1.0382437705993652\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.083105087280273 | KNN Loss: 5.039073944091797 | BCE Loss: 1.0440313816070557\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.0744781494140625 | KNN Loss: 5.026317119598389 | BCE Loss: 1.0481609106063843\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.089236736297607 | KNN Loss: 5.041752338409424 | BCE Loss: 1.047484278678894\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.118982791900635 | KNN Loss: 5.048271179199219 | BCE Loss: 1.0707117319107056\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.076078414916992 | KNN Loss: 5.046886444091797 | BCE Loss: 1.0291919708251953\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.095149517059326 | KNN Loss: 5.05197811126709 | BCE Loss: 1.0431715250015259\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.066223621368408 | KNN Loss: 5.031082630157471 | BCE Loss: 1.035140872001648\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.100039482116699 | KNN Loss: 5.049973964691162 | BCE Loss: 1.0500657558441162\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.061740398406982 | KNN Loss: 5.031469345092773 | BCE Loss: 1.030271053314209\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.098224639892578 | KNN Loss: 5.049665927886963 | BCE Loss: 1.0485587120056152\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.069551467895508 | KNN Loss: 5.047937870025635 | BCE Loss: 1.021613359451294\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.09912109375 | KNN Loss: 5.039997100830078 | BCE Loss: 1.059124231338501\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.112075328826904 | KNN Loss: 5.04313325881958 | BCE Loss: 1.0689420700073242\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.095230579376221 | KNN Loss: 5.046291828155518 | BCE Loss: 1.0489388704299927\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.085330963134766 | KNN Loss: 5.036726951599121 | BCE Loss: 1.0486042499542236\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.056008338928223 | KNN Loss: 5.044794082641602 | BCE Loss: 1.0112144947052002\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.12516975402832 | KNN Loss: 5.0451884269714355 | BCE Loss: 1.0799815654754639\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.117398262023926 | KNN Loss: 5.044273853302002 | BCE Loss: 1.073124647140503\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.054905891418457 | KNN Loss: 5.035014629364014 | BCE Loss: 1.0198910236358643\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.097087383270264 | KNN Loss: 5.039492130279541 | BCE Loss: 1.0575952529907227\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.090221405029297 | KNN Loss: 5.040161609649658 | BCE Loss: 1.0500597953796387\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.094805717468262 | KNN Loss: 5.048933029174805 | BCE Loss: 1.045872688293457\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.0980939865112305 | KNN Loss: 5.048391819000244 | BCE Loss: 1.0497019290924072\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.095430850982666 | KNN Loss: 5.071300506591797 | BCE Loss: 1.0241302251815796\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.057643890380859 | KNN Loss: 5.030473232269287 | BCE Loss: 1.0271708965301514\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.083040237426758 | KNN Loss: 5.052365303039551 | BCE Loss: 1.030674695968628\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.074268341064453 | KNN Loss: 5.03131103515625 | BCE Loss: 1.0429575443267822\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.088566780090332 | KNN Loss: 5.040413856506348 | BCE Loss: 1.0481531620025635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.101199626922607 | KNN Loss: 5.0482258796691895 | BCE Loss: 1.0529736280441284\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.084390163421631 | KNN Loss: 5.035100936889648 | BCE Loss: 1.0492892265319824\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.100293159484863 | KNN Loss: 5.055597305297852 | BCE Loss: 1.0446959733963013\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.0953521728515625 | KNN Loss: 5.036495208740234 | BCE Loss: 1.0588572025299072\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.095643997192383 | KNN Loss: 5.040556907653809 | BCE Loss: 1.0550869703292847\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.09891414642334 | KNN Loss: 5.032977104187012 | BCE Loss: 1.0659370422363281\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.089578628540039 | KNN Loss: 5.052615642547607 | BCE Loss: 1.036962866783142\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.122159004211426 | KNN Loss: 5.065701961517334 | BCE Loss: 1.0564571619033813\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.08574914932251 | KNN Loss: 5.03969669342041 | BCE Loss: 1.0460525751113892\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.0711541175842285 | KNN Loss: 5.026778697967529 | BCE Loss: 1.0443754196166992\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.145367622375488 | KNN Loss: 5.048227310180664 | BCE Loss: 1.0971400737762451\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.117379188537598 | KNN Loss: 5.044590950012207 | BCE Loss: 1.0727884769439697\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.101287364959717 | KNN Loss: 5.0571160316467285 | BCE Loss: 1.0441712141036987\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.114198207855225 | KNN Loss: 5.051184177398682 | BCE Loss: 1.063014030456543\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.0882415771484375 | KNN Loss: 5.051044464111328 | BCE Loss: 1.0371973514556885\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.062801837921143 | KNN Loss: 5.045388221740723 | BCE Loss: 1.01741361618042\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.127843856811523 | KNN Loss: 5.095849514007568 | BCE Loss: 1.031994342803955\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.097381591796875 | KNN Loss: 5.052300930023193 | BCE Loss: 1.0450806617736816\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.096011161804199 | KNN Loss: 5.042068004608154 | BCE Loss: 1.0539432764053345\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.09597110748291 | KNN Loss: 5.050917625427246 | BCE Loss: 1.045053243637085\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.098670482635498 | KNN Loss: 5.03415584564209 | BCE Loss: 1.0645146369934082\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.094308376312256 | KNN Loss: 5.034729480743408 | BCE Loss: 1.0595790147781372\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.078286647796631 | KNN Loss: 5.0353169441223145 | BCE Loss: 1.0429697036743164\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.0711846351623535 | KNN Loss: 5.040992736816406 | BCE Loss: 1.0301920175552368\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.073953628540039 | KNN Loss: 5.044440746307373 | BCE Loss: 1.029512882232666\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.069669246673584 | KNN Loss: 5.0348944664001465 | BCE Loss: 1.034774899482727\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.088567733764648 | KNN Loss: 5.057229042053223 | BCE Loss: 1.0313385725021362\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.089696884155273 | KNN Loss: 5.024102687835693 | BCE Loss: 1.0655940771102905\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.068080425262451 | KNN Loss: 5.019634246826172 | BCE Loss: 1.0484462976455688\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.056609630584717 | KNN Loss: 5.0275797843933105 | BCE Loss: 1.0290297269821167\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.132547378540039 | KNN Loss: 5.090861797332764 | BCE Loss: 1.0416858196258545\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.101330757141113 | KNN Loss: 5.034441947937012 | BCE Loss: 1.0668885707855225\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.113567352294922 | KNN Loss: 5.037242889404297 | BCE Loss: 1.076324462890625\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.069377899169922 | KNN Loss: 5.025552272796631 | BCE Loss: 1.043825387954712\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.113042831420898 | KNN Loss: 5.062855243682861 | BCE Loss: 1.050187587738037\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.0740742683410645 | KNN Loss: 5.026474952697754 | BCE Loss: 1.0475994348526\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.102984428405762 | KNN Loss: 5.055197238922119 | BCE Loss: 1.047787070274353\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.078238487243652 | KNN Loss: 5.036474704742432 | BCE Loss: 1.0417640209197998\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.122949123382568 | KNN Loss: 5.040131568908691 | BCE Loss: 1.082817554473877\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.068729400634766 | KNN Loss: 5.027589797973633 | BCE Loss: 1.0411396026611328\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.076165199279785 | KNN Loss: 5.043996810913086 | BCE Loss: 1.0321683883666992\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.112997055053711 | KNN Loss: 5.068767070770264 | BCE Loss: 1.0442297458648682\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.114509105682373 | KNN Loss: 5.0731987953186035 | BCE Loss: 1.0413103103637695\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.125908374786377 | KNN Loss: 5.044317245483398 | BCE Loss: 1.0815911293029785\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.051645278930664 | KNN Loss: 5.045390605926514 | BCE Loss: 1.0062544345855713\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.1040496826171875 | KNN Loss: 5.016946792602539 | BCE Loss: 1.0871031284332275\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.094776630401611 | KNN Loss: 5.045742034912109 | BCE Loss: 1.049034595489502\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.098223686218262 | KNN Loss: 5.039144039154053 | BCE Loss: 1.0590794086456299\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.073179244995117 | KNN Loss: 5.029776096343994 | BCE Loss: 1.043402910232544\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.075431823730469 | KNN Loss: 5.035179615020752 | BCE Loss: 1.0402522087097168\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.094393730163574 | KNN Loss: 5.017073631286621 | BCE Loss: 1.0773203372955322\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.068662643432617 | KNN Loss: 5.021335601806641 | BCE Loss: 1.0473272800445557\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.107752323150635 | KNN Loss: 5.023033142089844 | BCE Loss: 1.084719181060791\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.098584175109863 | KNN Loss: 5.040744781494141 | BCE Loss: 1.0578396320343018\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.019190311431885 | KNN Loss: 5.019677639007568 | BCE Loss: 0.9995126724243164\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.075705528259277 | KNN Loss: 5.024218559265137 | BCE Loss: 1.0514870882034302\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.070060729980469 | KNN Loss: 5.026257038116455 | BCE Loss: 1.0438036918640137\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.124137878417969 | KNN Loss: 5.05866003036499 | BCE Loss: 1.0654780864715576\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.078203201293945 | KNN Loss: 5.024146556854248 | BCE Loss: 1.0540565252304077\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.070967674255371 | KNN Loss: 5.023176193237305 | BCE Loss: 1.0477912425994873\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.12504768371582 | KNN Loss: 5.07684326171875 | BCE Loss: 1.0482046604156494\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.042124271392822 | KNN Loss: 5.020451545715332 | BCE Loss: 1.0216727256774902\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.101222038269043 | KNN Loss: 5.053773403167725 | BCE Loss: 1.0474486351013184\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.075145721435547 | KNN Loss: 5.020610332489014 | BCE Loss: 1.0545356273651123\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.137348651885986 | KNN Loss: 5.051908493041992 | BCE Loss: 1.0854400396347046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.062124252319336 | KNN Loss: 5.033090114593506 | BCE Loss: 1.029033899307251\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.083619117736816 | KNN Loss: 5.032845973968506 | BCE Loss: 1.0507731437683105\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.0724873542785645 | KNN Loss: 5.0229926109313965 | BCE Loss: 1.0494946241378784\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.0718302726745605 | KNN Loss: 5.022273063659668 | BCE Loss: 1.049557089805603\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.0956268310546875 | KNN Loss: 5.032552242279053 | BCE Loss: 1.0630748271942139\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.079458236694336 | KNN Loss: 5.040228843688965 | BCE Loss: 1.039229154586792\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.079995632171631 | KNN Loss: 5.024451732635498 | BCE Loss: 1.0555437803268433\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.05195951461792 | KNN Loss: 5.022393226623535 | BCE Loss: 1.0295661687850952\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.096658706665039 | KNN Loss: 5.041628360748291 | BCE Loss: 1.055030107498169\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.056857585906982 | KNN Loss: 5.02848482131958 | BCE Loss: 1.0283726453781128\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.0885419845581055 | KNN Loss: 5.064759731292725 | BCE Loss: 1.02378249168396\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.132514953613281 | KNN Loss: 5.061053276062012 | BCE Loss: 1.0714619159698486\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.068793296813965 | KNN Loss: 5.023976802825928 | BCE Loss: 1.044816493988037\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.0868330001831055 | KNN Loss: 5.03617525100708 | BCE Loss: 1.0506575107574463\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.097047328948975 | KNN Loss: 5.047871112823486 | BCE Loss: 1.0491760969161987\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.064252853393555 | KNN Loss: 5.016582489013672 | BCE Loss: 1.047670602798462\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.085941791534424 | KNN Loss: 5.044139862060547 | BCE Loss: 1.0418020486831665\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.071113586425781 | KNN Loss: 5.037010192871094 | BCE Loss: 1.0341036319732666\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.067739963531494 | KNN Loss: 5.0167131423950195 | BCE Loss: 1.051026701927185\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.0995354652404785 | KNN Loss: 5.053773880004883 | BCE Loss: 1.0457617044448853\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.085296630859375 | KNN Loss: 5.0198211669921875 | BCE Loss: 1.0654754638671875\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.106678009033203 | KNN Loss: 5.03983736038208 | BCE Loss: 1.0668407678604126\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.079858303070068 | KNN Loss: 5.035975933074951 | BCE Loss: 1.0438823699951172\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.132279872894287 | KNN Loss: 5.069438457489014 | BCE Loss: 1.062841534614563\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.103504180908203 | KNN Loss: 5.058263301849365 | BCE Loss: 1.0452407598495483\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.01815128326416 | KNN Loss: 5.018922805786133 | BCE Loss: 0.9992283582687378\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.089385509490967 | KNN Loss: 5.037791728973389 | BCE Loss: 1.0515937805175781\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.114139080047607 | KNN Loss: 5.048503875732422 | BCE Loss: 1.0656352043151855\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.058505535125732 | KNN Loss: 5.021552562713623 | BCE Loss: 1.0369528532028198\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.135515213012695 | KNN Loss: 5.055735111236572 | BCE Loss: 1.0797803401947021\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.048506736755371 | KNN Loss: 5.034907341003418 | BCE Loss: 1.0135992765426636\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.081485748291016 | KNN Loss: 5.024393558502197 | BCE Loss: 1.0570919513702393\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.070308685302734 | KNN Loss: 5.027946472167969 | BCE Loss: 1.0423619747161865\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.065403938293457 | KNN Loss: 5.041798114776611 | BCE Loss: 1.0236057043075562\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.074317932128906 | KNN Loss: 5.016560077667236 | BCE Loss: 1.05775785446167\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.083837985992432 | KNN Loss: 5.0286102294921875 | BCE Loss: 1.0552277565002441\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.065605640411377 | KNN Loss: 5.024085521697998 | BCE Loss: 1.0415202379226685\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.079481601715088 | KNN Loss: 5.037625789642334 | BCE Loss: 1.0418556928634644\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.041537284851074 | KNN Loss: 5.023120880126953 | BCE Loss: 1.018416166305542\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.044245719909668 | KNN Loss: 5.0269622802734375 | BCE Loss: 1.0172836780548096\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.117721080780029 | KNN Loss: 5.069403171539307 | BCE Loss: 1.048317790031433\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.130249500274658 | KNN Loss: 5.055388927459717 | BCE Loss: 1.074860692024231\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.076621055603027 | KNN Loss: 5.023016929626465 | BCE Loss: 1.0536041259765625\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.05989933013916 | KNN Loss: 5.018815994262695 | BCE Loss: 1.0410833358764648\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.170988082885742 | KNN Loss: 5.095579624176025 | BCE Loss: 1.0754084587097168\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.05689811706543 | KNN Loss: 5.030484676361084 | BCE Loss: 1.0264132022857666\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.049201965332031 | KNN Loss: 5.023974418640137 | BCE Loss: 1.0252277851104736\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.119330883026123 | KNN Loss: 5.0530009269714355 | BCE Loss: 1.066330075263977\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.085360527038574 | KNN Loss: 5.024349689483643 | BCE Loss: 1.061010718345642\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.087738037109375 | KNN Loss: 5.053310871124268 | BCE Loss: 1.0344271659851074\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.1267218589782715 | KNN Loss: 5.0455217361450195 | BCE Loss: 1.0812000036239624\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.080988883972168 | KNN Loss: 5.044834136962891 | BCE Loss: 1.0361545085906982\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.036384582519531 | KNN Loss: 5.011099338531494 | BCE Loss: 1.0252851247787476\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.072177410125732 | KNN Loss: 5.013107776641846 | BCE Loss: 1.0590696334838867\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.039819717407227 | KNN Loss: 5.028082847595215 | BCE Loss: 1.0117371082305908\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.059568405151367 | KNN Loss: 5.019883155822754 | BCE Loss: 1.0396854877471924\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.096394062042236 | KNN Loss: 5.041791915893555 | BCE Loss: 1.0546021461486816\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.08741569519043 | KNN Loss: 5.045047760009766 | BCE Loss: 1.0423681735992432\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.051836013793945 | KNN Loss: 5.019470691680908 | BCE Loss: 1.0323654413223267\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.118831157684326 | KNN Loss: 5.040130615234375 | BCE Loss: 1.0787006616592407\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.102665424346924 | KNN Loss: 5.021899223327637 | BCE Loss: 1.080766201019287\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.073209762573242 | KNN Loss: 5.042365074157715 | BCE Loss: 1.0308444499969482\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.052648544311523 | KNN Loss: 5.014325141906738 | BCE Loss: 1.0383232831954956\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.08884334564209 | KNN Loss: 5.046624183654785 | BCE Loss: 1.0422190427780151\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.090994834899902 | KNN Loss: 5.07051420211792 | BCE Loss: 1.0204803943634033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.075306415557861 | KNN Loss: 5.015254020690918 | BCE Loss: 1.0600523948669434\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.068122863769531 | KNN Loss: 5.027667045593262 | BCE Loss: 1.040455937385559\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.065857410430908 | KNN Loss: 5.02804708480835 | BCE Loss: 1.037810206413269\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.082126617431641 | KNN Loss: 5.04140043258667 | BCE Loss: 1.0407260656356812\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.1417083740234375 | KNN Loss: 5.075972557067871 | BCE Loss: 1.0657358169555664\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.149733543395996 | KNN Loss: 5.080692768096924 | BCE Loss: 1.0690405368804932\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.125180721282959 | KNN Loss: 5.075532913208008 | BCE Loss: 1.0496478080749512\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.063169479370117 | KNN Loss: 5.040223121643066 | BCE Loss: 1.0229465961456299\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.05277681350708 | KNN Loss: 5.026973247528076 | BCE Loss: 1.025803565979004\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.083810329437256 | KNN Loss: 5.039812088012695 | BCE Loss: 1.043998122215271\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.065194129943848 | KNN Loss: 5.023426532745361 | BCE Loss: 1.0417678356170654\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.047534942626953 | KNN Loss: 5.022459506988525 | BCE Loss: 1.0250756740570068\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.057822227478027 | KNN Loss: 5.020467281341553 | BCE Loss: 1.0373550653457642\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.057742595672607 | KNN Loss: 5.020166397094727 | BCE Loss: 1.0375760793685913\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.089306831359863 | KNN Loss: 5.029007434844971 | BCE Loss: 1.0602993965148926\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.094426155090332 | KNN Loss: 5.044300556182861 | BCE Loss: 1.0501257181167603\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.045548439025879 | KNN Loss: 5.027919292449951 | BCE Loss: 1.0176290273666382\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.060646057128906 | KNN Loss: 5.026167392730713 | BCE Loss: 1.0344784259796143\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.123244285583496 | KNN Loss: 5.047123432159424 | BCE Loss: 1.0761208534240723\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.067598342895508 | KNN Loss: 5.039810657501221 | BCE Loss: 1.0277875661849976\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.064966201782227 | KNN Loss: 5.028318405151367 | BCE Loss: 1.0366476774215698\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.107512474060059 | KNN Loss: 5.011009216308594 | BCE Loss: 1.0965030193328857\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.07830286026001 | KNN Loss: 5.046029567718506 | BCE Loss: 1.032273292541504\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.048120498657227 | KNN Loss: 5.0115251541137695 | BCE Loss: 1.036595106124878\n",
      "Epoch    69: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.079763412475586 | KNN Loss: 5.037729263305664 | BCE Loss: 1.0420341491699219\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.059950828552246 | KNN Loss: 5.040513515472412 | BCE Loss: 1.019437313079834\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.090043544769287 | KNN Loss: 5.042558193206787 | BCE Loss: 1.0474853515625\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.07916259765625 | KNN Loss: 5.058660507202148 | BCE Loss: 1.0205023288726807\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.064859390258789 | KNN Loss: 5.043519496917725 | BCE Loss: 1.0213398933410645\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.122672080993652 | KNN Loss: 5.063843727111816 | BCE Loss: 1.0588284730911255\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.061822414398193 | KNN Loss: 5.024115562438965 | BCE Loss: 1.0377068519592285\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.0511298179626465 | KNN Loss: 5.024281024932861 | BCE Loss: 1.0268487930297852\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.078171730041504 | KNN Loss: 5.029103755950928 | BCE Loss: 1.049067735671997\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.079367160797119 | KNN Loss: 5.020909309387207 | BCE Loss: 1.058457851409912\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.139323711395264 | KNN Loss: 5.096477031707764 | BCE Loss: 1.0428465604782104\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.069123268127441 | KNN Loss: 5.024937152862549 | BCE Loss: 1.0441861152648926\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.079982757568359 | KNN Loss: 5.039905548095703 | BCE Loss: 1.0400769710540771\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.061986923217773 | KNN Loss: 5.028438091278076 | BCE Loss: 1.0335485935211182\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.098615646362305 | KNN Loss: 5.043724060058594 | BCE Loss: 1.05489182472229\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.092886924743652 | KNN Loss: 5.067359447479248 | BCE Loss: 1.0255274772644043\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.039660453796387 | KNN Loss: 5.026872158050537 | BCE Loss: 1.01278817653656\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.046489715576172 | KNN Loss: 5.002188205718994 | BCE Loss: 1.0443012714385986\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.070465087890625 | KNN Loss: 5.014904022216797 | BCE Loss: 1.0555610656738281\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.070669651031494 | KNN Loss: 5.009761810302734 | BCE Loss: 1.0609078407287598\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.047032833099365 | KNN Loss: 5.0412116050720215 | BCE Loss: 1.0058212280273438\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.101867198944092 | KNN Loss: 5.038862228393555 | BCE Loss: 1.063004970550537\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.05797815322876 | KNN Loss: 5.016937255859375 | BCE Loss: 1.0410407781600952\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.057640075683594 | KNN Loss: 5.03853178024292 | BCE Loss: 1.0191080570220947\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.030567169189453 | KNN Loss: 5.019685745239258 | BCE Loss: 1.0108813047409058\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.045970916748047 | KNN Loss: 5.013609886169434 | BCE Loss: 1.0323612689971924\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.077124118804932 | KNN Loss: 5.03390645980835 | BCE Loss: 1.043217658996582\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.155910491943359 | KNN Loss: 5.103207111358643 | BCE Loss: 1.0527031421661377\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.111313819885254 | KNN Loss: 5.080527305603027 | BCE Loss: 1.0307862758636475\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.027165412902832 | KNN Loss: 5.012661457061768 | BCE Loss: 1.0145037174224854\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.035261154174805 | KNN Loss: 5.01368522644043 | BCE Loss: 1.021576166152954\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.062625885009766 | KNN Loss: 5.020895004272461 | BCE Loss: 1.0417308807373047\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.057100772857666 | KNN Loss: 5.025235652923584 | BCE Loss: 1.0318650007247925\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.107812881469727 | KNN Loss: 5.040043830871582 | BCE Loss: 1.0677692890167236\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.094153881072998 | KNN Loss: 5.0547685623168945 | BCE Loss: 1.0393853187561035\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.060789585113525 | KNN Loss: 5.020906448364258 | BCE Loss: 1.0398832559585571\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.073534965515137 | KNN Loss: 5.013311386108398 | BCE Loss: 1.0602234601974487\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.06617546081543 | KNN Loss: 5.0439558029174805 | BCE Loss: 1.0222195386886597\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.103524208068848 | KNN Loss: 5.027031898498535 | BCE Loss: 1.0764923095703125\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.0748209953308105 | KNN Loss: 5.037314414978027 | BCE Loss: 1.0375065803527832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.082497596740723 | KNN Loss: 5.045161724090576 | BCE Loss: 1.0373361110687256\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.061762809753418 | KNN Loss: 5.039983749389648 | BCE Loss: 1.0217788219451904\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.069907188415527 | KNN Loss: 5.0101399421691895 | BCE Loss: 1.0597673654556274\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.053818702697754 | KNN Loss: 5.036397933959961 | BCE Loss: 1.0174205303192139\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.0723395347595215 | KNN Loss: 5.036162853240967 | BCE Loss: 1.0361768007278442\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.066215991973877 | KNN Loss: 5.015169620513916 | BCE Loss: 1.0510462522506714\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.097414016723633 | KNN Loss: 5.041859149932861 | BCE Loss: 1.0555546283721924\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.119195461273193 | KNN Loss: 5.044164180755615 | BCE Loss: 1.0750312805175781\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.082691192626953 | KNN Loss: 5.024633884429932 | BCE Loss: 1.058057188987732\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.061549663543701 | KNN Loss: 5.019965171813965 | BCE Loss: 1.0415846109390259\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.072506904602051 | KNN Loss: 5.023571014404297 | BCE Loss: 1.048935890197754\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.047483444213867 | KNN Loss: 5.020767688751221 | BCE Loss: 1.0267155170440674\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.045346260070801 | KNN Loss: 5.016696453094482 | BCE Loss: 1.028649926185608\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.043562889099121 | KNN Loss: 5.007004737854004 | BCE Loss: 1.0365583896636963\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.041767120361328 | KNN Loss: 5.010265350341797 | BCE Loss: 1.0315020084381104\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.080437183380127 | KNN Loss: 5.021548271179199 | BCE Loss: 1.0588889122009277\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.064533233642578 | KNN Loss: 5.024174690246582 | BCE Loss: 1.0403586626052856\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.080389976501465 | KNN Loss: 5.035675525665283 | BCE Loss: 1.0447142124176025\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.048706531524658 | KNN Loss: 5.023906707763672 | BCE Loss: 1.0247998237609863\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.056289196014404 | KNN Loss: 5.040260314941406 | BCE Loss: 1.016028881072998\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.087050437927246 | KNN Loss: 5.056185722351074 | BCE Loss: 1.0308648347854614\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.064799785614014 | KNN Loss: 5.031036853790283 | BCE Loss: 1.033762812614441\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.077487945556641 | KNN Loss: 5.02149772644043 | BCE Loss: 1.0559903383255005\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.07560396194458 | KNN Loss: 5.01912784576416 | BCE Loss: 1.05647611618042\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.102292537689209 | KNN Loss: 5.0359086990356445 | BCE Loss: 1.066383719444275\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.046880722045898 | KNN Loss: 5.0225090980529785 | BCE Loss: 1.0243713855743408\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.015711784362793 | KNN Loss: 5.009296417236328 | BCE Loss: 1.0064151287078857\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.067106246948242 | KNN Loss: 5.0528435707092285 | BCE Loss: 1.0142629146575928\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.06697940826416 | KNN Loss: 5.0231781005859375 | BCE Loss: 1.0438010692596436\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.0384345054626465 | KNN Loss: 5.029351234436035 | BCE Loss: 1.0090833902359009\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.075850963592529 | KNN Loss: 5.018640518188477 | BCE Loss: 1.0572105646133423\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.060097694396973 | KNN Loss: 5.012279510498047 | BCE Loss: 1.0478183031082153\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.060608863830566 | KNN Loss: 5.0147600173950195 | BCE Loss: 1.045849084854126\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.154934883117676 | KNN Loss: 5.092067241668701 | BCE Loss: 1.0628674030303955\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.058332920074463 | KNN Loss: 5.008946418762207 | BCE Loss: 1.0493865013122559\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.041210174560547 | KNN Loss: 5.025421619415283 | BCE Loss: 1.0157883167266846\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.105925559997559 | KNN Loss: 5.057888507843018 | BCE Loss: 1.048037052154541\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.0526909828186035 | KNN Loss: 5.038071632385254 | BCE Loss: 1.0146194696426392\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.084189414978027 | KNN Loss: 5.034754753112793 | BCE Loss: 1.0494346618652344\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.0932817459106445 | KNN Loss: 5.045114994049072 | BCE Loss: 1.0481669902801514\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.093544960021973 | KNN Loss: 5.040905475616455 | BCE Loss: 1.0526394844055176\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.054732322692871 | KNN Loss: 5.0150346755981445 | BCE Loss: 1.0396978855133057\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.046015739440918 | KNN Loss: 5.0495429039001465 | BCE Loss: 0.9964728355407715\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.033912658691406 | KNN Loss: 5.008688449859619 | BCE Loss: 1.025224208831787\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.078364372253418 | KNN Loss: 5.054688930511475 | BCE Loss: 1.0236754417419434\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.078932285308838 | KNN Loss: 5.0283050537109375 | BCE Loss: 1.0506272315979004\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.073291301727295 | KNN Loss: 5.0410871505737305 | BCE Loss: 1.0322041511535645\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.118261337280273 | KNN Loss: 5.060666561126709 | BCE Loss: 1.0575947761535645\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.066934108734131 | KNN Loss: 5.026321887969971 | BCE Loss: 1.0406122207641602\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.04798698425293 | KNN Loss: 5.02006196975708 | BCE Loss: 1.0279247760772705\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.081277370452881 | KNN Loss: 5.035369396209717 | BCE Loss: 1.0459078550338745\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.041653156280518 | KNN Loss: 5.023745536804199 | BCE Loss: 1.0179076194763184\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.1227335929870605 | KNN Loss: 5.071718692779541 | BCE Loss: 1.05101478099823\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.075098991394043 | KNN Loss: 5.057747840881348 | BCE Loss: 1.0173513889312744\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.137940406799316 | KNN Loss: 5.0934953689575195 | BCE Loss: 1.044445276260376\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.081961631774902 | KNN Loss: 5.0261125564575195 | BCE Loss: 1.0558490753173828\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.091143608093262 | KNN Loss: 5.065598964691162 | BCE Loss: 1.0255446434020996\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.059986114501953 | KNN Loss: 5.045002460479736 | BCE Loss: 1.014983892440796\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.046565055847168 | KNN Loss: 5.023523807525635 | BCE Loss: 1.0230414867401123\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.063141822814941 | KNN Loss: 5.029829025268555 | BCE Loss: 1.0333125591278076\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.045413970947266 | KNN Loss: 5.019256114959717 | BCE Loss: 1.0261579751968384\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.056475639343262 | KNN Loss: 5.019286155700684 | BCE Loss: 1.0371896028518677\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.145849227905273 | KNN Loss: 5.101984024047852 | BCE Loss: 1.0438653230667114\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.071671485900879 | KNN Loss: 5.033303260803223 | BCE Loss: 1.0383681058883667\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.072868347167969 | KNN Loss: 5.010188579559326 | BCE Loss: 1.0626797676086426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.0418195724487305 | KNN Loss: 5.011046409606934 | BCE Loss: 1.0307729244232178\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.027312278747559 | KNN Loss: 5.017266750335693 | BCE Loss: 1.0100456476211548\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.09340763092041 | KNN Loss: 5.028271198272705 | BCE Loss: 1.065136432647705\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.071282863616943 | KNN Loss: 5.018850803375244 | BCE Loss: 1.0524321794509888\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.082579612731934 | KNN Loss: 5.024210453033447 | BCE Loss: 1.0583691596984863\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.052914619445801 | KNN Loss: 5.029180526733398 | BCE Loss: 1.0237343311309814\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.050444602966309 | KNN Loss: 5.01887321472168 | BCE Loss: 1.031571388244629\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.111946105957031 | KNN Loss: 5.057621479034424 | BCE Loss: 1.0543245077133179\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.062745094299316 | KNN Loss: 5.013552188873291 | BCE Loss: 1.0491926670074463\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.100107192993164 | KNN Loss: 5.030862331390381 | BCE Loss: 1.0692448616027832\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.070798397064209 | KNN Loss: 5.024946689605713 | BCE Loss: 1.045851707458496\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.045454502105713 | KNN Loss: 5.0166425704956055 | BCE Loss: 1.0288118124008179\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.049480438232422 | KNN Loss: 5.024855613708496 | BCE Loss: 1.0246248245239258\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.0413103103637695 | KNN Loss: 5.034002780914307 | BCE Loss: 1.007307767868042\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.049288272857666 | KNN Loss: 5.037630081176758 | BCE Loss: 1.0116583108901978\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.056454181671143 | KNN Loss: 5.02133321762085 | BCE Loss: 1.035120964050293\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.057923316955566 | KNN Loss: 5.008559703826904 | BCE Loss: 1.049363374710083\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.053840637207031 | KNN Loss: 5.035243988037109 | BCE Loss: 1.018596887588501\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.093620300292969 | KNN Loss: 5.026363372802734 | BCE Loss: 1.0672571659088135\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.051885604858398 | KNN Loss: 5.0127949714660645 | BCE Loss: 1.039090633392334\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.06926965713501 | KNN Loss: 5.021229267120361 | BCE Loss: 1.048040509223938\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.063127517700195 | KNN Loss: 5.014608383178711 | BCE Loss: 1.0485193729400635\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.037959098815918 | KNN Loss: 5.013280391693115 | BCE Loss: 1.0246789455413818\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.068136215209961 | KNN Loss: 5.034762382507324 | BCE Loss: 1.0333738327026367\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.074642181396484 | KNN Loss: 5.0261430740356445 | BCE Loss: 1.0484988689422607\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.1414265632629395 | KNN Loss: 5.096736431121826 | BCE Loss: 1.0446900129318237\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.072789192199707 | KNN Loss: 5.032286167144775 | BCE Loss: 1.040502905845642\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.05146598815918 | KNN Loss: 5.008667945861816 | BCE Loss: 1.0427982807159424\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.050403594970703 | KNN Loss: 5.008550643920898 | BCE Loss: 1.0418531894683838\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.058006286621094 | KNN Loss: 5.02733850479126 | BCE Loss: 1.0306676626205444\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.0577006340026855 | KNN Loss: 5.029204845428467 | BCE Loss: 1.0284957885742188\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.057987689971924 | KNN Loss: 5.016207695007324 | BCE Loss: 1.04177987575531\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 6.071152210235596 | KNN Loss: 5.040456771850586 | BCE Loss: 1.0306954383850098\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.057193279266357 | KNN Loss: 5.024629592895508 | BCE Loss: 1.0325638055801392\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.058657646179199 | KNN Loss: 5.033569812774658 | BCE Loss: 1.025087594985962\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.043359756469727 | KNN Loss: 5.033644199371338 | BCE Loss: 1.0097157955169678\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.07049036026001 | KNN Loss: 5.0305938720703125 | BCE Loss: 1.0398964881896973\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.096860885620117 | KNN Loss: 5.0337419509887695 | BCE Loss: 1.0631186962127686\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.073114395141602 | KNN Loss: 5.0202436447143555 | BCE Loss: 1.052870750427246\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.053868293762207 | KNN Loss: 5.0299601554870605 | BCE Loss: 1.0239081382751465\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.053167819976807 | KNN Loss: 5.02186918258667 | BCE Loss: 1.0312986373901367\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.074804306030273 | KNN Loss: 5.036598205566406 | BCE Loss: 1.0382063388824463\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.0926513671875 | KNN Loss: 5.038381576538086 | BCE Loss: 1.054269790649414\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.060202121734619 | KNN Loss: 5.017306804656982 | BCE Loss: 1.0428953170776367\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.144250869750977 | KNN Loss: 5.102585315704346 | BCE Loss: 1.04166579246521\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.054703235626221 | KNN Loss: 5.004356384277344 | BCE Loss: 1.050346851348877\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.027026176452637 | KNN Loss: 5.009918689727783 | BCE Loss: 1.0171077251434326\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.047008991241455 | KNN Loss: 5.036836624145508 | BCE Loss: 1.0101724863052368\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.0652337074279785 | KNN Loss: 5.013827323913574 | BCE Loss: 1.0514063835144043\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.083922386169434 | KNN Loss: 5.021049976348877 | BCE Loss: 1.062872290611267\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.068020820617676 | KNN Loss: 5.029807090759277 | BCE Loss: 1.0382137298583984\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.047634124755859 | KNN Loss: 5.0189208984375 | BCE Loss: 1.028713345527649\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.04449462890625 | KNN Loss: 5.033468246459961 | BCE Loss: 1.011026382446289\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.055375099182129 | KNN Loss: 5.008787155151367 | BCE Loss: 1.0465877056121826\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.041528701782227 | KNN Loss: 5.036886692047119 | BCE Loss: 1.004642128944397\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.088459014892578 | KNN Loss: 5.023309230804443 | BCE Loss: 1.0651496648788452\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.054872035980225 | KNN Loss: 5.020912170410156 | BCE Loss: 1.0339598655700684\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.078625679016113 | KNN Loss: 5.021389961242676 | BCE Loss: 1.057235836982727\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.08632755279541 | KNN Loss: 5.052975177764893 | BCE Loss: 1.0333524942398071\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.066274642944336 | KNN Loss: 5.022098064422607 | BCE Loss: 1.0441768169403076\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.068592071533203 | KNN Loss: 5.0276780128479 | BCE Loss: 1.0409140586853027\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.119691848754883 | KNN Loss: 5.0658111572265625 | BCE Loss: 1.0538804531097412\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.026992321014404 | KNN Loss: 5.023714065551758 | BCE Loss: 1.003278136253357\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.083715915679932 | KNN Loss: 5.047994136810303 | BCE Loss: 1.035721778869629\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.049604415893555 | KNN Loss: 5.025286674499512 | BCE Loss: 1.0243175029754639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.06715202331543 | KNN Loss: 5.02771520614624 | BCE Loss: 1.0394368171691895\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.055013179779053 | KNN Loss: 5.007699012756348 | BCE Loss: 1.0473142862319946\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.033638000488281 | KNN Loss: 5.0281291007995605 | BCE Loss: 1.0055087804794312\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.043074607849121 | KNN Loss: 5.022216320037842 | BCE Loss: 1.0208581686019897\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.109987735748291 | KNN Loss: 5.0679521560668945 | BCE Loss: 1.0420355796813965\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.096314907073975 | KNN Loss: 5.042057037353516 | BCE Loss: 1.0542577505111694\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.046238899230957 | KNN Loss: 5.041769504547119 | BCE Loss: 1.0044695138931274\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.0885114669799805 | KNN Loss: 5.041902542114258 | BCE Loss: 1.0466091632843018\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.052988052368164 | KNN Loss: 5.032042026519775 | BCE Loss: 1.0209459066390991\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.043406009674072 | KNN Loss: 5.014186382293701 | BCE Loss: 1.0292195081710815\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.031271457672119 | KNN Loss: 5.011892318725586 | BCE Loss: 1.0193790197372437\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.083861351013184 | KNN Loss: 5.03954553604126 | BCE Loss: 1.0443155765533447\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.046743392944336 | KNN Loss: 5.024338245391846 | BCE Loss: 1.0224052667617798\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.042802333831787 | KNN Loss: 5.0339837074279785 | BCE Loss: 1.008818507194519\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.123719215393066 | KNN Loss: 5.11043643951416 | BCE Loss: 1.0132830142974854\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.044715404510498 | KNN Loss: 5.010391712188721 | BCE Loss: 1.0343236923217773\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.066790580749512 | KNN Loss: 5.0373148918151855 | BCE Loss: 1.029475450515747\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.04075813293457 | KNN Loss: 5.009549140930176 | BCE Loss: 1.031208872795105\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.071622848510742 | KNN Loss: 5.031548500061035 | BCE Loss: 1.040074110031128\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.110580921173096 | KNN Loss: 5.068728923797607 | BCE Loss: 1.0418518781661987\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.060170650482178 | KNN Loss: 5.024957180023193 | BCE Loss: 1.0352134704589844\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.015009880065918 | KNN Loss: 5.015730857849121 | BCE Loss: 0.9992790222167969\n",
      "Epoch   101: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.041431903839111 | KNN Loss: 5.021917343139648 | BCE Loss: 1.0195146799087524\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.094470024108887 | KNN Loss: 5.050112724304199 | BCE Loss: 1.0443572998046875\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.058555603027344 | KNN Loss: 5.0328850746154785 | BCE Loss: 1.0256705284118652\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.0594305992126465 | KNN Loss: 5.006901741027832 | BCE Loss: 1.0525288581848145\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.065738201141357 | KNN Loss: 5.024632453918457 | BCE Loss: 1.0411057472229004\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.0260210037231445 | KNN Loss: 5.036008358001709 | BCE Loss: 0.9900124669075012\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.064268112182617 | KNN Loss: 5.0525078773498535 | BCE Loss: 1.0117599964141846\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.056177139282227 | KNN Loss: 5.027286052703857 | BCE Loss: 1.02889084815979\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.076854705810547 | KNN Loss: 5.036655426025391 | BCE Loss: 1.0401991605758667\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.031944274902344 | KNN Loss: 5.016958236694336 | BCE Loss: 1.014986276626587\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.038372039794922 | KNN Loss: 5.018889904022217 | BCE Loss: 1.019482135772705\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.019771099090576 | KNN Loss: 5.015130996704102 | BCE Loss: 1.004639983177185\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.07705545425415 | KNN Loss: 5.013611316680908 | BCE Loss: 1.0634440183639526\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.043811798095703 | KNN Loss: 5.017397403717041 | BCE Loss: 1.026414155960083\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.0537919998168945 | KNN Loss: 5.022085666656494 | BCE Loss: 1.0317060947418213\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.093090057373047 | KNN Loss: 5.036890506744385 | BCE Loss: 1.0561997890472412\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.063908576965332 | KNN Loss: 5.019616603851318 | BCE Loss: 1.0442922115325928\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.06324577331543 | KNN Loss: 5.026700496673584 | BCE Loss: 1.0365450382232666\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.047702789306641 | KNN Loss: 5.014149188995361 | BCE Loss: 1.0335536003112793\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.050976753234863 | KNN Loss: 5.014547824859619 | BCE Loss: 1.0364290475845337\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.072942733764648 | KNN Loss: 5.023248672485352 | BCE Loss: 1.0496941804885864\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.115839958190918 | KNN Loss: 5.06140661239624 | BCE Loss: 1.0544333457946777\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.043196678161621 | KNN Loss: 5.013869285583496 | BCE Loss: 1.029327392578125\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.040478706359863 | KNN Loss: 5.026088237762451 | BCE Loss: 1.014390230178833\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.0525970458984375 | KNN Loss: 5.005031585693359 | BCE Loss: 1.0475656986236572\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.062233924865723 | KNN Loss: 5.039463043212891 | BCE Loss: 1.0227710008621216\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.06867790222168 | KNN Loss: 5.039520740509033 | BCE Loss: 1.0291574001312256\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.127584457397461 | KNN Loss: 5.093080520629883 | BCE Loss: 1.0345039367675781\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.110454082489014 | KNN Loss: 5.0344648361206055 | BCE Loss: 1.0759892463684082\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.0464911460876465 | KNN Loss: 5.017696857452393 | BCE Loss: 1.0287941694259644\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.019376277923584 | KNN Loss: 5.028306484222412 | BCE Loss: 0.9910697937011719\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.077399730682373 | KNN Loss: 5.034470558166504 | BCE Loss: 1.0429290533065796\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.072966575622559 | KNN Loss: 5.037783622741699 | BCE Loss: 1.0351829528808594\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.080650329589844 | KNN Loss: 5.012454986572266 | BCE Loss: 1.0681953430175781\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.059475898742676 | KNN Loss: 5.0366902351379395 | BCE Loss: 1.0227857828140259\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.091175556182861 | KNN Loss: 5.02587890625 | BCE Loss: 1.0652967691421509\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.050473213195801 | KNN Loss: 5.021312236785889 | BCE Loss: 1.0291610956192017\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.037863731384277 | KNN Loss: 5.017063617706299 | BCE Loss: 1.0208001136779785\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.068077564239502 | KNN Loss: 5.039340972900391 | BCE Loss: 1.0287364721298218\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.055909156799316 | KNN Loss: 5.010129451751709 | BCE Loss: 1.0457799434661865\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.082211971282959 | KNN Loss: 5.027528285980225 | BCE Loss: 1.054683804512024\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.0838623046875 | KNN Loss: 5.019437313079834 | BCE Loss: 1.0644252300262451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.069351673126221 | KNN Loss: 5.03022575378418 | BCE Loss: 1.0391258001327515\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.065885543823242 | KNN Loss: 5.019021034240723 | BCE Loss: 1.0468647480010986\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.047036170959473 | KNN Loss: 5.007876873016357 | BCE Loss: 1.0391590595245361\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.0364556312561035 | KNN Loss: 5.002388000488281 | BCE Loss: 1.0340677499771118\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.079538345336914 | KNN Loss: 5.028631210327148 | BCE Loss: 1.0509068965911865\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.06098747253418 | KNN Loss: 5.022965908050537 | BCE Loss: 1.0380216836929321\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.075107097625732 | KNN Loss: 5.0250749588012695 | BCE Loss: 1.0500322580337524\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.062103748321533 | KNN Loss: 5.023305892944336 | BCE Loss: 1.0387977361679077\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.1007208824157715 | KNN Loss: 5.070864200592041 | BCE Loss: 1.02985680103302\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.070146560668945 | KNN Loss: 5.033990383148193 | BCE Loss: 1.0361559391021729\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.073614120483398 | KNN Loss: 5.025527000427246 | BCE Loss: 1.0480873584747314\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.025803565979004 | KNN Loss: 5.026324272155762 | BCE Loss: 0.9994790554046631\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.063303470611572 | KNN Loss: 5.038559436798096 | BCE Loss: 1.0247441530227661\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.04493522644043 | KNN Loss: 5.011288642883301 | BCE Loss: 1.033646583557129\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.035122871398926 | KNN Loss: 5.021542072296143 | BCE Loss: 1.0135809183120728\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.0680317878723145 | KNN Loss: 5.017817974090576 | BCE Loss: 1.0502139329910278\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.078553199768066 | KNN Loss: 5.028006076812744 | BCE Loss: 1.0505471229553223\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.066340446472168 | KNN Loss: 5.0177388191223145 | BCE Loss: 1.0486018657684326\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.058622360229492 | KNN Loss: 5.029536724090576 | BCE Loss: 1.0290857553482056\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.084807395935059 | KNN Loss: 5.043278694152832 | BCE Loss: 1.0415284633636475\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.063084125518799 | KNN Loss: 5.018835067749023 | BCE Loss: 1.0442490577697754\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.092556953430176 | KNN Loss: 5.057041645050049 | BCE Loss: 1.0355150699615479\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.050933837890625 | KNN Loss: 5.024901390075684 | BCE Loss: 1.0260326862335205\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.083601951599121 | KNN Loss: 5.045182228088379 | BCE Loss: 1.038419485092163\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.101899147033691 | KNN Loss: 5.048920154571533 | BCE Loss: 1.0529789924621582\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.035828590393066 | KNN Loss: 5.016424655914307 | BCE Loss: 1.0194041728973389\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.050083160400391 | KNN Loss: 5.015772342681885 | BCE Loss: 1.034311056137085\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.062425136566162 | KNN Loss: 5.01512336730957 | BCE Loss: 1.0473018884658813\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.062527656555176 | KNN Loss: 5.035033226013184 | BCE Loss: 1.0274944305419922\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.051647186279297 | KNN Loss: 5.0387797355651855 | BCE Loss: 1.0128675699234009\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.042453765869141 | KNN Loss: 5.033805847167969 | BCE Loss: 1.008648157119751\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.0460357666015625 | KNN Loss: 5.018033981323242 | BCE Loss: 1.0280019044876099\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.048027992248535 | KNN Loss: 5.033061504364014 | BCE Loss: 1.0149664878845215\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.084789752960205 | KNN Loss: 5.061577796936035 | BCE Loss: 1.02321195602417\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 6.042180061340332 | KNN Loss: 5.017546653747559 | BCE Loss: 1.024633526802063\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.055137634277344 | KNN Loss: 5.019383907318115 | BCE Loss: 1.0357539653778076\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.017473220825195 | KNN Loss: 4.99669885635376 | BCE Loss: 1.0207741260528564\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.052608013153076 | KNN Loss: 5.017566204071045 | BCE Loss: 1.0350419282913208\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.037019729614258 | KNN Loss: 5.0099639892578125 | BCE Loss: 1.0270555019378662\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.055364608764648 | KNN Loss: 5.021947860717773 | BCE Loss: 1.033416748046875\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.06004524230957 | KNN Loss: 5.032862663269043 | BCE Loss: 1.0271823406219482\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.035788536071777 | KNN Loss: 5.017385005950928 | BCE Loss: 1.01840341091156\n",
      "Epoch   115: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.063540458679199 | KNN Loss: 5.051299095153809 | BCE Loss: 1.0122413635253906\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 6.093265056610107 | KNN Loss: 5.019742965698242 | BCE Loss: 1.0735222101211548\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.122071743011475 | KNN Loss: 5.066069602966309 | BCE Loss: 1.056002140045166\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.0841779708862305 | KNN Loss: 5.028957843780518 | BCE Loss: 1.055220365524292\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.049600601196289 | KNN Loss: 5.025014877319336 | BCE Loss: 1.0245857238769531\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.051360607147217 | KNN Loss: 5.029759407043457 | BCE Loss: 1.0216010808944702\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.05147123336792 | KNN Loss: 5.0210347175598145 | BCE Loss: 1.0304365158081055\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.006008148193359 | KNN Loss: 5.014410495758057 | BCE Loss: 0.9915978908538818\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 6.046679973602295 | KNN Loss: 5.04822301864624 | BCE Loss: 0.9984569549560547\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.075001239776611 | KNN Loss: 5.041072845458984 | BCE Loss: 1.0339285135269165\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.049313068389893 | KNN Loss: 5.008894443511963 | BCE Loss: 1.0404186248779297\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 6.066237449645996 | KNN Loss: 5.014665126800537 | BCE Loss: 1.051572561264038\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.022615909576416 | KNN Loss: 5.006986618041992 | BCE Loss: 1.0156294107437134\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.063733100891113 | KNN Loss: 5.006574630737305 | BCE Loss: 1.0571587085723877\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.09699821472168 | KNN Loss: 5.051429748535156 | BCE Loss: 1.0455682277679443\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.049557685852051 | KNN Loss: 5.025216579437256 | BCE Loss: 1.024341106414795\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.0755720138549805 | KNN Loss: 5.016067028045654 | BCE Loss: 1.059504747390747\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.0629353523254395 | KNN Loss: 5.035181522369385 | BCE Loss: 1.0277539491653442\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.04282808303833 | KNN Loss: 5.0279974937438965 | BCE Loss: 1.014830470085144\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 6.073698997497559 | KNN Loss: 5.036328315734863 | BCE Loss: 1.0373706817626953\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.06611967086792 | KNN Loss: 5.020644664764404 | BCE Loss: 1.0454750061035156\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.057496070861816 | KNN Loss: 5.017548561096191 | BCE Loss: 1.039947748184204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.092315673828125 | KNN Loss: 5.0320563316345215 | BCE Loss: 1.060259461402893\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.067868232727051 | KNN Loss: 5.0366034507751465 | BCE Loss: 1.0312647819519043\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.031923770904541 | KNN Loss: 5.015937328338623 | BCE Loss: 1.0159865617752075\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.051070213317871 | KNN Loss: 5.031985759735107 | BCE Loss: 1.0190842151641846\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.126895904541016 | KNN Loss: 5.071123123168945 | BCE Loss: 1.0557730197906494\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.031171798706055 | KNN Loss: 5.012972354888916 | BCE Loss: 1.0181993246078491\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.123788356781006 | KNN Loss: 5.079809188842773 | BCE Loss: 1.0439791679382324\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.052117347717285 | KNN Loss: 5.0202717781066895 | BCE Loss: 1.0318453311920166\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.046165466308594 | KNN Loss: 5.01308012008667 | BCE Loss: 1.033085584640503\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.064914226531982 | KNN Loss: 5.026778697967529 | BCE Loss: 1.0381355285644531\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.051433563232422 | KNN Loss: 5.019700050354004 | BCE Loss: 1.031733751296997\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.069728851318359 | KNN Loss: 5.014376640319824 | BCE Loss: 1.0553524494171143\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.047889709472656 | KNN Loss: 5.015981674194336 | BCE Loss: 1.0319082736968994\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.058932304382324 | KNN Loss: 5.038532257080078 | BCE Loss: 1.020399808883667\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.066977500915527 | KNN Loss: 5.017789363861084 | BCE Loss: 1.0491881370544434\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.026392936706543 | KNN Loss: 5.0148773193359375 | BCE Loss: 1.0115156173706055\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.061856269836426 | KNN Loss: 5.028275966644287 | BCE Loss: 1.0335805416107178\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.088186264038086 | KNN Loss: 5.058068752288818 | BCE Loss: 1.0301176309585571\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.042140007019043 | KNN Loss: 5.020963191986084 | BCE Loss: 1.021176815032959\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.056017875671387 | KNN Loss: 5.014973163604736 | BCE Loss: 1.0410449504852295\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.061679840087891 | KNN Loss: 5.014857292175293 | BCE Loss: 1.0468223094940186\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.09248685836792 | KNN Loss: 5.0574798583984375 | BCE Loss: 1.0350068807601929\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.0407586097717285 | KNN Loss: 5.0251688957214355 | BCE Loss: 1.015589714050293\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.03761625289917 | KNN Loss: 5.032454967498779 | BCE Loss: 1.0051612854003906\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.034097194671631 | KNN Loss: 5.0205583572387695 | BCE Loss: 1.0135387182235718\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.055866241455078 | KNN Loss: 5.023617267608643 | BCE Loss: 1.0322487354278564\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.023387432098389 | KNN Loss: 5.005356311798096 | BCE Loss: 1.0180312395095825\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.004719257354736 | KNN Loss: 5.016183376312256 | BCE Loss: 0.98853600025177\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.034027576446533 | KNN Loss: 5.020740032196045 | BCE Loss: 1.0132874250411987\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.071969032287598 | KNN Loss: 5.036556720733643 | BCE Loss: 1.0354121923446655\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.0520853996276855 | KNN Loss: 5.036741733551025 | BCE Loss: 1.0153436660766602\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.071660995483398 | KNN Loss: 5.031857967376709 | BCE Loss: 1.0398027896881104\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.063138484954834 | KNN Loss: 5.015743732452393 | BCE Loss: 1.0473947525024414\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.0718231201171875 | KNN Loss: 5.023218154907227 | BCE Loss: 1.0486050844192505\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.01971435546875 | KNN Loss: 5.017346382141113 | BCE Loss: 1.0023677349090576\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.042995929718018 | KNN Loss: 5.01336669921875 | BCE Loss: 1.0296292304992676\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.101326942443848 | KNN Loss: 5.08096170425415 | BCE Loss: 1.0203653573989868\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.0394816398620605 | KNN Loss: 5.010264873504639 | BCE Loss: 1.0292166471481323\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.026565074920654 | KNN Loss: 5.003973484039307 | BCE Loss: 1.0225915908813477\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.057339668273926 | KNN Loss: 5.031917572021484 | BCE Loss: 1.0254218578338623\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.104935646057129 | KNN Loss: 5.055670261383057 | BCE Loss: 1.0492651462554932\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.04695987701416 | KNN Loss: 5.0258870124816895 | BCE Loss: 1.0210727453231812\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.0329132080078125 | KNN Loss: 5.012879848480225 | BCE Loss: 1.0200331211090088\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.046464920043945 | KNN Loss: 5.006795406341553 | BCE Loss: 1.0396695137023926\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.030122756958008 | KNN Loss: 5.010900020599365 | BCE Loss: 1.0192224979400635\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.066529750823975 | KNN Loss: 5.020989894866943 | BCE Loss: 1.0455398559570312\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.052101135253906 | KNN Loss: 5.011751174926758 | BCE Loss: 1.0403499603271484\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.041609764099121 | KNN Loss: 5.023590087890625 | BCE Loss: 1.0180199146270752\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.068913459777832 | KNN Loss: 5.020900249481201 | BCE Loss: 1.04801344871521\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.026864051818848 | KNN Loss: 5.019777774810791 | BCE Loss: 1.0070860385894775\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.089144706726074 | KNN Loss: 5.028065204620361 | BCE Loss: 1.0610792636871338\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.048035621643066 | KNN Loss: 5.028918743133545 | BCE Loss: 1.019116759300232\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.076123237609863 | KNN Loss: 5.027563095092773 | BCE Loss: 1.048560380935669\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.101241111755371 | KNN Loss: 5.034697532653809 | BCE Loss: 1.0665433406829834\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.043849468231201 | KNN Loss: 5.018086910247803 | BCE Loss: 1.0257625579833984\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.041787624359131 | KNN Loss: 5.02706241607666 | BCE Loss: 1.0147250890731812\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.052875518798828 | KNN Loss: 5.041841506958008 | BCE Loss: 1.0110340118408203\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.0490641593933105 | KNN Loss: 5.01426362991333 | BCE Loss: 1.0348005294799805\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.094569206237793 | KNN Loss: 5.025540828704834 | BCE Loss: 1.069028377532959\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.038771629333496 | KNN Loss: 5.005352020263672 | BCE Loss: 1.0334198474884033\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.083611488342285 | KNN Loss: 5.03521203994751 | BCE Loss: 1.0483993291854858\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.048828125 | KNN Loss: 5.046926021575928 | BCE Loss: 1.0019021034240723\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.049783706665039 | KNN Loss: 5.035634517669678 | BCE Loss: 1.0141489505767822\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.0538482666015625 | KNN Loss: 5.0135064125061035 | BCE Loss: 1.0403417348861694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.035946846008301 | KNN Loss: 5.021030902862549 | BCE Loss: 1.014915943145752\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.060025215148926 | KNN Loss: 5.039061546325684 | BCE Loss: 1.020963430404663\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.035989761352539 | KNN Loss: 5.024127006530762 | BCE Loss: 1.0118627548217773\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 6.082831859588623 | KNN Loss: 5.0340657234191895 | BCE Loss: 1.0487661361694336\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.04049015045166 | KNN Loss: 5.009990692138672 | BCE Loss: 1.0304994583129883\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.04350471496582 | KNN Loss: 5.023612022399902 | BCE Loss: 1.0198924541473389\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.034991264343262 | KNN Loss: 5.013096332550049 | BCE Loss: 1.021894931793213\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.052977085113525 | KNN Loss: 5.031552314758301 | BCE Loss: 1.021424651145935\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.092926979064941 | KNN Loss: 5.039261817932129 | BCE Loss: 1.0536651611328125\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.036405563354492 | KNN Loss: 5.024573802947998 | BCE Loss: 1.0118317604064941\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.017772197723389 | KNN Loss: 5.0169677734375 | BCE Loss: 1.0008044242858887\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.046291351318359 | KNN Loss: 5.035555839538574 | BCE Loss: 1.0107356309890747\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.045225143432617 | KNN Loss: 5.00899600982666 | BCE Loss: 1.0362290143966675\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.043249130249023 | KNN Loss: 5.0372748374938965 | BCE Loss: 1.0059744119644165\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.0043625831604 | KNN Loss: 5.013978481292725 | BCE Loss: 0.9903841018676758\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.081305503845215 | KNN Loss: 5.057278156280518 | BCE Loss: 1.0240275859832764\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.079794883728027 | KNN Loss: 5.069725513458252 | BCE Loss: 1.010069489479065\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.049529075622559 | KNN Loss: 5.026519298553467 | BCE Loss: 1.023010015487671\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.039115905761719 | KNN Loss: 5.017299175262451 | BCE Loss: 1.021816611289978\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.052304267883301 | KNN Loss: 5.026212692260742 | BCE Loss: 1.0260918140411377\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.05111026763916 | KNN Loss: 5.018442153930664 | BCE Loss: 1.0326679944992065\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.067641258239746 | KNN Loss: 5.024204254150391 | BCE Loss: 1.0434367656707764\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.021526336669922 | KNN Loss: 5.006985187530518 | BCE Loss: 1.0145409107208252\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.024710655212402 | KNN Loss: 5.022719383239746 | BCE Loss: 1.0019915103912354\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.09732723236084 | KNN Loss: 5.050380229949951 | BCE Loss: 1.0469467639923096\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.075850486755371 | KNN Loss: 5.038463115692139 | BCE Loss: 1.0373872518539429\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.059130668640137 | KNN Loss: 5.010036945343018 | BCE Loss: 1.0490936040878296\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.008788108825684 | KNN Loss: 5.020888328552246 | BCE Loss: 0.9879000186920166\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.0537543296813965 | KNN Loss: 5.02406120300293 | BCE Loss: 1.0296931266784668\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.037263870239258 | KNN Loss: 5.007675647735596 | BCE Loss: 1.029588222503662\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.062382221221924 | KNN Loss: 5.033294200897217 | BCE Loss: 1.0290881395339966\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.078326225280762 | KNN Loss: 5.054260730743408 | BCE Loss: 1.024065375328064\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.078436851501465 | KNN Loss: 5.058837890625 | BCE Loss: 1.0195987224578857\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.047113418579102 | KNN Loss: 5.017570972442627 | BCE Loss: 1.0295422077178955\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.044808387756348 | KNN Loss: 5.006694316864014 | BCE Loss: 1.038114309310913\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.070428848266602 | KNN Loss: 5.061070919036865 | BCE Loss: 1.0093580484390259\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.066167831420898 | KNN Loss: 5.0208048820495605 | BCE Loss: 1.0453630685806274\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.068519592285156 | KNN Loss: 5.03095817565918 | BCE Loss: 1.0375614166259766\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.048211574554443 | KNN Loss: 5.026302337646484 | BCE Loss: 1.0219093561172485\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.057289123535156 | KNN Loss: 5.027820110321045 | BCE Loss: 1.0294688940048218\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.008603572845459 | KNN Loss: 5.007400035858154 | BCE Loss: 1.0012034177780151\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.052145957946777 | KNN Loss: 5.012162208557129 | BCE Loss: 1.0399835109710693\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.057649612426758 | KNN Loss: 5.025618076324463 | BCE Loss: 1.032031774520874\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.061740875244141 | KNN Loss: 5.029353618621826 | BCE Loss: 1.0323870182037354\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.060351371765137 | KNN Loss: 5.024834632873535 | BCE Loss: 1.035516619682312\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.043326377868652 | KNN Loss: 5.009942531585693 | BCE Loss: 1.0333839654922485\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.06694221496582 | KNN Loss: 5.045036792755127 | BCE Loss: 1.0219054222106934\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.0833306312561035 | KNN Loss: 5.040355205535889 | BCE Loss: 1.0429755449295044\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.030433654785156 | KNN Loss: 5.021554470062256 | BCE Loss: 1.0088791847229004\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.063409805297852 | KNN Loss: 5.016238212585449 | BCE Loss: 1.0471714735031128\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.036881446838379 | KNN Loss: 5.0027079582214355 | BCE Loss: 1.034173607826233\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.0443644523620605 | KNN Loss: 5.014175891876221 | BCE Loss: 1.0301885604858398\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 6.050799369812012 | KNN Loss: 5.011543273925781 | BCE Loss: 1.0392560958862305\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.056377410888672 | KNN Loss: 5.022700786590576 | BCE Loss: 1.0336768627166748\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.060864448547363 | KNN Loss: 5.014367580413818 | BCE Loss: 1.046497106552124\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.065845489501953 | KNN Loss: 5.015379905700684 | BCE Loss: 1.0504655838012695\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.090488910675049 | KNN Loss: 5.043290138244629 | BCE Loss: 1.0471988916397095\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.057542324066162 | KNN Loss: 5.0586838722229 | BCE Loss: 0.9988583326339722\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.092859745025635 | KNN Loss: 5.038581848144531 | BCE Loss: 1.054278016090393\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.047110557556152 | KNN Loss: 5.039586067199707 | BCE Loss: 1.0075246095657349\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.045636177062988 | KNN Loss: 5.017159461975098 | BCE Loss: 1.0284764766693115\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.099681854248047 | KNN Loss: 5.079425811767578 | BCE Loss: 1.0202560424804688\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.052877426147461 | KNN Loss: 5.027833938598633 | BCE Loss: 1.0250437259674072\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.046413421630859 | KNN Loss: 5.00788688659668 | BCE Loss: 1.0385265350341797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.071063995361328 | KNN Loss: 5.017164707183838 | BCE Loss: 1.0538992881774902\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.044007301330566 | KNN Loss: 5.017685890197754 | BCE Loss: 1.026321291923523\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.026648044586182 | KNN Loss: 5.009760856628418 | BCE Loss: 1.0168873071670532\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.05915641784668 | KNN Loss: 5.034256935119629 | BCE Loss: 1.0248994827270508\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.070659637451172 | KNN Loss: 5.028651237487793 | BCE Loss: 1.0420082807540894\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.077164649963379 | KNN Loss: 5.040660381317139 | BCE Loss: 1.0365042686462402\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.098871231079102 | KNN Loss: 5.047292709350586 | BCE Loss: 1.0515785217285156\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.071250915527344 | KNN Loss: 5.045877456665039 | BCE Loss: 1.0253732204437256\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.0396728515625 | KNN Loss: 5.0230712890625 | BCE Loss: 1.016601324081421\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.122488975524902 | KNN Loss: 5.096797466278076 | BCE Loss: 1.0256917476654053\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.074448108673096 | KNN Loss: 5.033009052276611 | BCE Loss: 1.041439175605774\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 6.088911056518555 | KNN Loss: 5.043325901031494 | BCE Loss: 1.0455853939056396\n",
      "Epoch   142: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.063752174377441 | KNN Loss: 5.026954650878906 | BCE Loss: 1.0367975234985352\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.043820858001709 | KNN Loss: 5.0149664878845215 | BCE Loss: 1.0288543701171875\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.075605392456055 | KNN Loss: 5.039554119110107 | BCE Loss: 1.0360510349273682\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.0414533615112305 | KNN Loss: 5.0264716148376465 | BCE Loss: 1.014981985092163\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.0720319747924805 | KNN Loss: 5.0396623611450195 | BCE Loss: 1.0323697328567505\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.059831619262695 | KNN Loss: 5.062446117401123 | BCE Loss: 0.9973853826522827\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.053009986877441 | KNN Loss: 5.028928279876709 | BCE Loss: 1.0240817070007324\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.09467887878418 | KNN Loss: 5.033719539642334 | BCE Loss: 1.0609593391418457\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 6.049653053283691 | KNN Loss: 5.019481182098389 | BCE Loss: 1.0301721096038818\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.093269348144531 | KNN Loss: 5.035184860229492 | BCE Loss: 1.0580847263336182\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.066522121429443 | KNN Loss: 5.022366046905518 | BCE Loss: 1.0441559553146362\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.075629234313965 | KNN Loss: 5.047441482543945 | BCE Loss: 1.0281875133514404\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.0499725341796875 | KNN Loss: 5.002338886260986 | BCE Loss: 1.0476336479187012\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 6.068795680999756 | KNN Loss: 5.027033805847168 | BCE Loss: 1.041761875152588\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.034870147705078 | KNN Loss: 5.019649505615234 | BCE Loss: 1.0152207612991333\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.1244282722473145 | KNN Loss: 5.077546119689941 | BCE Loss: 1.0468822717666626\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.076513290405273 | KNN Loss: 5.017537593841553 | BCE Loss: 1.0589759349822998\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.025631904602051 | KNN Loss: 5.016629219055176 | BCE Loss: 1.0090028047561646\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.036860942840576 | KNN Loss: 5.002145767211914 | BCE Loss: 1.0347150564193726\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.08913516998291 | KNN Loss: 5.037174701690674 | BCE Loss: 1.0519604682922363\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.0338335037231445 | KNN Loss: 5.00596284866333 | BCE Loss: 1.0278708934783936\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.041591644287109 | KNN Loss: 5.011380195617676 | BCE Loss: 1.0302116870880127\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.071159362792969 | KNN Loss: 5.0443034172058105 | BCE Loss: 1.0268561840057373\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 6.060915946960449 | KNN Loss: 5.035029888153076 | BCE Loss: 1.0258859395980835\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.045655727386475 | KNN Loss: 5.030710697174072 | BCE Loss: 1.014945149421692\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.065608024597168 | KNN Loss: 5.009270668029785 | BCE Loss: 1.0563371181488037\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.102856159210205 | KNN Loss: 5.063802719116211 | BCE Loss: 1.0390534400939941\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.063421249389648 | KNN Loss: 5.034855842590332 | BCE Loss: 1.0285654067993164\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.056188583374023 | KNN Loss: 5.023233413696289 | BCE Loss: 1.0329554080963135\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.032740592956543 | KNN Loss: 5.005202770233154 | BCE Loss: 1.0275375843048096\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.087632179260254 | KNN Loss: 5.040126323699951 | BCE Loss: 1.0475058555603027\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 6.096177577972412 | KNN Loss: 5.0573039054870605 | BCE Loss: 1.0388736724853516\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.031359672546387 | KNN Loss: 5.032043933868408 | BCE Loss: 0.999315619468689\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.048610687255859 | KNN Loss: 5.019977569580078 | BCE Loss: 1.0286331176757812\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.034859657287598 | KNN Loss: 5.012002944946289 | BCE Loss: 1.0228567123413086\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.054843425750732 | KNN Loss: 5.006350517272949 | BCE Loss: 1.0484930276870728\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.024572372436523 | KNN Loss: 5.017942428588867 | BCE Loss: 1.0066297054290771\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.045954704284668 | KNN Loss: 5.013452053070068 | BCE Loss: 1.03250253200531\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.061477184295654 | KNN Loss: 5.032721042633057 | BCE Loss: 1.0287562608718872\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.023568153381348 | KNN Loss: 5.013810157775879 | BCE Loss: 1.0097577571868896\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.039643287658691 | KNN Loss: 5.020007610321045 | BCE Loss: 1.0196356773376465\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.080961227416992 | KNN Loss: 5.023228645324707 | BCE Loss: 1.0577325820922852\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.022570610046387 | KNN Loss: 5.00513219833374 | BCE Loss: 1.0174384117126465\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.05710506439209 | KNN Loss: 5.011843204498291 | BCE Loss: 1.0452616214752197\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.046594142913818 | KNN Loss: 5.010249614715576 | BCE Loss: 1.0363445281982422\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.018340110778809 | KNN Loss: 5.013510704040527 | BCE Loss: 1.0048294067382812\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.034628868103027 | KNN Loss: 5.012266635894775 | BCE Loss: 1.0223623514175415\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.034054756164551 | KNN Loss: 5.0111846923828125 | BCE Loss: 1.0228698253631592\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.065561294555664 | KNN Loss: 5.043857097625732 | BCE Loss: 1.0217043161392212\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.049558639526367 | KNN Loss: 5.003273963928223 | BCE Loss: 1.0462849140167236\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.0540690422058105 | KNN Loss: 5.013507843017578 | BCE Loss: 1.0405610799789429\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.05094051361084 | KNN Loss: 5.02432918548584 | BCE Loss: 1.0266114473342896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.098261833190918 | KNN Loss: 5.060215473175049 | BCE Loss: 1.0380464792251587\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.112943172454834 | KNN Loss: 5.0787672996521 | BCE Loss: 1.0341758728027344\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.051935195922852 | KNN Loss: 5.019477367401123 | BCE Loss: 1.0324578285217285\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.022839546203613 | KNN Loss: 5.011096477508545 | BCE Loss: 1.0117428302764893\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.067074775695801 | KNN Loss: 5.031554698944092 | BCE Loss: 1.0355199575424194\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.080998420715332 | KNN Loss: 5.021121501922607 | BCE Loss: 1.0598770380020142\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.050303936004639 | KNN Loss: 5.024550914764404 | BCE Loss: 1.025753140449524\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.054780006408691 | KNN Loss: 5.0077080726623535 | BCE Loss: 1.047072172164917\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.031289100646973 | KNN Loss: 5.021443843841553 | BCE Loss: 1.0098450183868408\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.083265781402588 | KNN Loss: 5.048282146453857 | BCE Loss: 1.0349836349487305\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.047014236450195 | KNN Loss: 5.01512336730957 | BCE Loss: 1.031891107559204\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 6.057584762573242 | KNN Loss: 5.02398681640625 | BCE Loss: 1.0335978269577026\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.034819602966309 | KNN Loss: 5.03226375579834 | BCE Loss: 1.0025558471679688\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.032873153686523 | KNN Loss: 5.0262298583984375 | BCE Loss: 1.006643533706665\n",
      "Epoch   153: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.042364597320557 | KNN Loss: 5.022100925445557 | BCE Loss: 1.0202637910842896\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.016629219055176 | KNN Loss: 5.013886451721191 | BCE Loss: 1.0027427673339844\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.048616886138916 | KNN Loss: 5.014288902282715 | BCE Loss: 1.0343279838562012\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.046124458312988 | KNN Loss: 5.013571739196777 | BCE Loss: 1.0325525999069214\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.06654167175293 | KNN Loss: 5.058938503265381 | BCE Loss: 1.007603406906128\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.05768346786499 | KNN Loss: 5.030916690826416 | BCE Loss: 1.0267667770385742\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.040598392486572 | KNN Loss: 5.014713764190674 | BCE Loss: 1.0258846282958984\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.048403739929199 | KNN Loss: 5.032088279724121 | BCE Loss: 1.0163154602050781\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.021783828735352 | KNN Loss: 5.007978439331055 | BCE Loss: 1.0138055086135864\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.043737888336182 | KNN Loss: 5.014361381530762 | BCE Loss: 1.0293763875961304\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.065080165863037 | KNN Loss: 5.043248653411865 | BCE Loss: 1.0218316316604614\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.043665885925293 | KNN Loss: 5.015359401702881 | BCE Loss: 1.0283067226409912\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.054716110229492 | KNN Loss: 5.020942687988281 | BCE Loss: 1.0337731838226318\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.026571750640869 | KNN Loss: 5.0074663162231445 | BCE Loss: 1.0191055536270142\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.020282745361328 | KNN Loss: 5.002207279205322 | BCE Loss: 1.0180754661560059\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.063884735107422 | KNN Loss: 5.034043312072754 | BCE Loss: 1.029841661453247\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.03547477722168 | KNN Loss: 5.029598712921143 | BCE Loss: 1.005876064300537\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.045255184173584 | KNN Loss: 5.032735824584961 | BCE Loss: 1.0125194787979126\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.072772979736328 | KNN Loss: 5.026686191558838 | BCE Loss: 1.0460870265960693\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.080480575561523 | KNN Loss: 5.028012275695801 | BCE Loss: 1.0524680614471436\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.042133808135986 | KNN Loss: 5.0233001708984375 | BCE Loss: 1.0188336372375488\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.074831008911133 | KNN Loss: 5.019225597381592 | BCE Loss: 1.0556052923202515\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.0561017990112305 | KNN Loss: 5.019580364227295 | BCE Loss: 1.0365211963653564\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.105834007263184 | KNN Loss: 5.0659332275390625 | BCE Loss: 1.039900779724121\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.093525409698486 | KNN Loss: 5.071891784667969 | BCE Loss: 1.0216336250305176\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.011388301849365 | KNN Loss: 5.013247966766357 | BCE Loss: 0.9981404542922974\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.057914733886719 | KNN Loss: 5.017960548400879 | BCE Loss: 1.039954423904419\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 6.058935642242432 | KNN Loss: 5.012521266937256 | BCE Loss: 1.0464144945144653\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.051990985870361 | KNN Loss: 5.027392864227295 | BCE Loss: 1.0245980024337769\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.070409774780273 | KNN Loss: 5.013326644897461 | BCE Loss: 1.0570831298828125\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.036884784698486 | KNN Loss: 5.017228126525879 | BCE Loss: 1.0196566581726074\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.08113956451416 | KNN Loss: 5.035027027130127 | BCE Loss: 1.046112298965454\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.104455471038818 | KNN Loss: 5.045382022857666 | BCE Loss: 1.0590733289718628\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.0945963859558105 | KNN Loss: 5.031250476837158 | BCE Loss: 1.0633459091186523\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.032113552093506 | KNN Loss: 5.040126800537109 | BCE Loss: 0.9919866323471069\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.118869781494141 | KNN Loss: 5.073996067047119 | BCE Loss: 1.0448737144470215\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.023728847503662 | KNN Loss: 5.001223564147949 | BCE Loss: 1.022505283355713\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.043183326721191 | KNN Loss: 5.019643306732178 | BCE Loss: 1.0235400199890137\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.024929523468018 | KNN Loss: 5.004706382751465 | BCE Loss: 1.0202231407165527\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.039374828338623 | KNN Loss: 5.006995677947998 | BCE Loss: 1.0323792695999146\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.10267972946167 | KNN Loss: 5.057308197021484 | BCE Loss: 1.0453715324401855\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.038102149963379 | KNN Loss: 5.025910377502441 | BCE Loss: 1.0121915340423584\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.094719409942627 | KNN Loss: 5.027432441711426 | BCE Loss: 1.0672868490219116\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.051229476928711 | KNN Loss: 5.019252777099609 | BCE Loss: 1.0319766998291016\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 6.034294605255127 | KNN Loss: 5.039406776428223 | BCE Loss: 0.99488765001297\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.022587776184082 | KNN Loss: 5.0247344970703125 | BCE Loss: 0.9978531002998352\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.026693344116211 | KNN Loss: 5.008097171783447 | BCE Loss: 1.0185962915420532\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.083608150482178 | KNN Loss: 5.039355754852295 | BCE Loss: 1.0442523956298828\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.03897762298584 | KNN Loss: 5.026547908782959 | BCE Loss: 1.01242995262146\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.04661750793457 | KNN Loss: 5.031566619873047 | BCE Loss: 1.0150507688522339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.029268264770508 | KNN Loss: 5.0188069343566895 | BCE Loss: 1.0104610919952393\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.009766578674316 | KNN Loss: 5.0188307762146 | BCE Loss: 0.9909359216690063\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.075444221496582 | KNN Loss: 5.044018268585205 | BCE Loss: 1.031425952911377\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.0405120849609375 | KNN Loss: 5.004584312438965 | BCE Loss: 1.0359277725219727\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.047030448913574 | KNN Loss: 5.023823261260986 | BCE Loss: 1.0232073068618774\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.016084671020508 | KNN Loss: 5.027547359466553 | BCE Loss: 0.9885373115539551\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.045001983642578 | KNN Loss: 5.013946533203125 | BCE Loss: 1.0310556888580322\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.081828594207764 | KNN Loss: 5.041665077209473 | BCE Loss: 1.040163516998291\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.026300430297852 | KNN Loss: 5.016890525817871 | BCE Loss: 1.009409785270691\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.050152778625488 | KNN Loss: 5.025533676147461 | BCE Loss: 1.0246188640594482\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.076198577880859 | KNN Loss: 5.046557426452637 | BCE Loss: 1.0296409130096436\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.036910057067871 | KNN Loss: 5.005340576171875 | BCE Loss: 1.031569480895996\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.04252815246582 | KNN Loss: 5.020249843597412 | BCE Loss: 1.0222781896591187\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.06856632232666 | KNN Loss: 5.010659694671631 | BCE Loss: 1.0579063892364502\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.058026313781738 | KNN Loss: 5.059463977813721 | BCE Loss: 0.9985624551773071\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.032722473144531 | KNN Loss: 5.003527641296387 | BCE Loss: 1.0291945934295654\n",
      "Epoch   164: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.058787822723389 | KNN Loss: 5.030272483825684 | BCE Loss: 1.0285154581069946\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 6.100976943969727 | KNN Loss: 5.064733028411865 | BCE Loss: 1.0362439155578613\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.059564113616943 | KNN Loss: 5.0110249519348145 | BCE Loss: 1.0485390424728394\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.057811737060547 | KNN Loss: 5.0118021965026855 | BCE Loss: 1.0460097789764404\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 6.060970306396484 | KNN Loss: 5.04191780090332 | BCE Loss: 1.019052267074585\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.041938304901123 | KNN Loss: 5.0065999031066895 | BCE Loss: 1.035338282585144\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.06095027923584 | KNN Loss: 5.027592658996582 | BCE Loss: 1.0333575010299683\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.070387840270996 | KNN Loss: 5.0170207023620605 | BCE Loss: 1.0533671379089355\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.036840915679932 | KNN Loss: 5.010451316833496 | BCE Loss: 1.0263895988464355\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.074580192565918 | KNN Loss: 5.034389972686768 | BCE Loss: 1.0401902198791504\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.105639457702637 | KNN Loss: 5.043900012969971 | BCE Loss: 1.0617393255233765\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.036204814910889 | KNN Loss: 5.011295795440674 | BCE Loss: 1.0249089002609253\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.099878311157227 | KNN Loss: 5.020932197570801 | BCE Loss: 1.0789459943771362\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.067817687988281 | KNN Loss: 5.033169269561768 | BCE Loss: 1.0346485376358032\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.095459938049316 | KNN Loss: 5.078006744384766 | BCE Loss: 1.0174533128738403\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.046041488647461 | KNN Loss: 5.017621994018555 | BCE Loss: 1.0284192562103271\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.007730007171631 | KNN Loss: 5.000749588012695 | BCE Loss: 1.0069804191589355\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.064830780029297 | KNN Loss: 5.02667760848999 | BCE Loss: 1.0381531715393066\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.0668721199035645 | KNN Loss: 5.031655311584473 | BCE Loss: 1.0352166891098022\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 6.034458637237549 | KNN Loss: 5.013331413269043 | BCE Loss: 1.0211272239685059\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.022238731384277 | KNN Loss: 5.007991790771484 | BCE Loss: 1.0142470598220825\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.057494163513184 | KNN Loss: 5.041895866394043 | BCE Loss: 1.0155980587005615\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.053518295288086 | KNN Loss: 5.023515224456787 | BCE Loss: 1.0300028324127197\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.083270072937012 | KNN Loss: 5.032896995544434 | BCE Loss: 1.0503730773925781\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.0982208251953125 | KNN Loss: 5.051697254180908 | BCE Loss: 1.0465234518051147\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.094274997711182 | KNN Loss: 5.021424293518066 | BCE Loss: 1.0728508234024048\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.092128276824951 | KNN Loss: 5.071325778961182 | BCE Loss: 1.0208024978637695\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.0427656173706055 | KNN Loss: 5.016364574432373 | BCE Loss: 1.0264008045196533\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.049120903015137 | KNN Loss: 5.013098239898682 | BCE Loss: 1.036022663116455\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.050130844116211 | KNN Loss: 5.029365539550781 | BCE Loss: 1.0207654237747192\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.072087287902832 | KNN Loss: 5.016839981079102 | BCE Loss: 1.0552473068237305\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.015823841094971 | KNN Loss: 5.016124248504639 | BCE Loss: 0.9996996521949768\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.032081127166748 | KNN Loss: 5.010485649108887 | BCE Loss: 1.0215954780578613\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.055706024169922 | KNN Loss: 5.0140180587768555 | BCE Loss: 1.0416878461837769\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.086294174194336 | KNN Loss: 5.051147937774658 | BCE Loss: 1.0351459980010986\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.0746002197265625 | KNN Loss: 5.051699638366699 | BCE Loss: 1.0229008197784424\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.0426483154296875 | KNN Loss: 5.017804145812988 | BCE Loss: 1.0248444080352783\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.0583696365356445 | KNN Loss: 5.031483173370361 | BCE Loss: 1.0268863439559937\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.0091552734375 | KNN Loss: 5.012851715087891 | BCE Loss: 0.9963034987449646\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 6.011672019958496 | KNN Loss: 4.998470783233643 | BCE Loss: 1.0132014751434326\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.068249702453613 | KNN Loss: 5.0209245681762695 | BCE Loss: 1.0473248958587646\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 6.064548969268799 | KNN Loss: 5.030790328979492 | BCE Loss: 1.0337586402893066\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.008571624755859 | KNN Loss: 5.017692565917969 | BCE Loss: 0.9908792972564697\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.085803985595703 | KNN Loss: 5.0529465675354 | BCE Loss: 1.0328575372695923\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.045483112335205 | KNN Loss: 5.012688636779785 | BCE Loss: 1.0327945947647095\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.102959632873535 | KNN Loss: 5.0496745109558105 | BCE Loss: 1.0532853603363037\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.05010986328125 | KNN Loss: 5.027431011199951 | BCE Loss: 1.0226786136627197\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.068166732788086 | KNN Loss: 5.024824619293213 | BCE Loss: 1.0433423519134521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 6.048678398132324 | KNN Loss: 5.02221155166626 | BCE Loss: 1.0264666080474854\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.078570365905762 | KNN Loss: 5.0205559730529785 | BCE Loss: 1.0580143928527832\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.0734429359436035 | KNN Loss: 5.018912315368652 | BCE Loss: 1.0545306205749512\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.033078670501709 | KNN Loss: 5.019141674041748 | BCE Loss: 1.0139371156692505\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.043820381164551 | KNN Loss: 4.998090744018555 | BCE Loss: 1.0457298755645752\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.021022319793701 | KNN Loss: 5.012973308563232 | BCE Loss: 1.0080488920211792\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.018740653991699 | KNN Loss: 5.00325870513916 | BCE Loss: 1.015481948852539\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.057538986206055 | KNN Loss: 5.027087211608887 | BCE Loss: 1.0304516553878784\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.048712730407715 | KNN Loss: 5.012569427490234 | BCE Loss: 1.0361430644989014\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.056114196777344 | KNN Loss: 5.019786834716797 | BCE Loss: 1.0363271236419678\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.04302978515625 | KNN Loss: 5.031282424926758 | BCE Loss: 1.0117473602294922\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.09548807144165 | KNN Loss: 5.02764892578125 | BCE Loss: 1.0678391456604004\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.068075180053711 | KNN Loss: 5.003849029541016 | BCE Loss: 1.0642261505126953\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 6.031912803649902 | KNN Loss: 5.01035213470459 | BCE Loss: 1.0215606689453125\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.109897613525391 | KNN Loss: 5.069085121154785 | BCE Loss: 1.0408124923706055\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.05230712890625 | KNN Loss: 5.035360336303711 | BCE Loss: 1.01694655418396\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.0589518547058105 | KNN Loss: 5.006524562835693 | BCE Loss: 1.0524272918701172\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 6.0883612632751465 | KNN Loss: 5.0109405517578125 | BCE Loss: 1.077420711517334\n",
      "Epoch   175: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.077544212341309 | KNN Loss: 5.031766414642334 | BCE Loss: 1.0457780361175537\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.070577621459961 | KNN Loss: 5.039456367492676 | BCE Loss: 1.0311214923858643\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.060250282287598 | KNN Loss: 5.040491104125977 | BCE Loss: 1.0197594165802002\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.0756683349609375 | KNN Loss: 5.0328545570373535 | BCE Loss: 1.042814016342163\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.048708915710449 | KNN Loss: 5.0237555503845215 | BCE Loss: 1.0249533653259277\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.022624492645264 | KNN Loss: 5.0138044357299805 | BCE Loss: 1.0088201761245728\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.093830108642578 | KNN Loss: 5.029605865478516 | BCE Loss: 1.0642240047454834\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.106117248535156 | KNN Loss: 5.037124156951904 | BCE Loss: 1.068993091583252\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.042921543121338 | KNN Loss: 5.0207648277282715 | BCE Loss: 1.022156834602356\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.035653114318848 | KNN Loss: 5.0266218185424805 | BCE Loss: 1.009031057357788\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.0549421310424805 | KNN Loss: 5.015839576721191 | BCE Loss: 1.0391027927398682\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.086295127868652 | KNN Loss: 5.0581464767456055 | BCE Loss: 1.0281486511230469\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.019199371337891 | KNN Loss: 5.0080084800720215 | BCE Loss: 1.01119065284729\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.052848815917969 | KNN Loss: 5.023457050323486 | BCE Loss: 1.029391884803772\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.047433376312256 | KNN Loss: 5.005850315093994 | BCE Loss: 1.0415830612182617\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.044529438018799 | KNN Loss: 5.022619724273682 | BCE Loss: 1.0219095945358276\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.086643218994141 | KNN Loss: 5.029721736907959 | BCE Loss: 1.0569216012954712\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 6.055099010467529 | KNN Loss: 5.013933181762695 | BCE Loss: 1.041165828704834\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 6.030397891998291 | KNN Loss: 5.002991199493408 | BCE Loss: 1.0274065732955933\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.024361610412598 | KNN Loss: 5.023012638092041 | BCE Loss: 1.0013487339019775\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.055539131164551 | KNN Loss: 5.017012119293213 | BCE Loss: 1.0385267734527588\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.042890548706055 | KNN Loss: 5.024782657623291 | BCE Loss: 1.0181081295013428\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 5.986381530761719 | KNN Loss: 5.009949684143066 | BCE Loss: 0.9764318466186523\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.0711774826049805 | KNN Loss: 5.002415180206299 | BCE Loss: 1.0687620639801025\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.051699638366699 | KNN Loss: 5.023964881896973 | BCE Loss: 1.027734637260437\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.0658135414123535 | KNN Loss: 5.053754806518555 | BCE Loss: 1.0120587348937988\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.058282375335693 | KNN Loss: 5.0267486572265625 | BCE Loss: 1.0315337181091309\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.035552024841309 | KNN Loss: 5.019130706787109 | BCE Loss: 1.0164213180541992\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.1028289794921875 | KNN Loss: 5.039942264556885 | BCE Loss: 1.0628867149353027\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 6.063777923583984 | KNN Loss: 5.036470890045166 | BCE Loss: 1.0273072719573975\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 5.9996209144592285 | KNN Loss: 4.996884346008301 | BCE Loss: 1.0027365684509277\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.093599796295166 | KNN Loss: 5.056035041809082 | BCE Loss: 1.037564754486084\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.028576850891113 | KNN Loss: 5.01560640335083 | BCE Loss: 1.0129705667495728\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.013218402862549 | KNN Loss: 5.009519100189209 | BCE Loss: 1.0036991834640503\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.031942367553711 | KNN Loss: 5.037975311279297 | BCE Loss: 0.9939670562744141\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.051914691925049 | KNN Loss: 5.023040294647217 | BCE Loss: 1.0288745164871216\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.058680534362793 | KNN Loss: 5.033227443695068 | BCE Loss: 1.0254533290863037\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.0494866371154785 | KNN Loss: 5.00883674621582 | BCE Loss: 1.0406498908996582\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.032046318054199 | KNN Loss: 5.011595249176025 | BCE Loss: 1.0204511880874634\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.038715362548828 | KNN Loss: 5.022261142730713 | BCE Loss: 1.0164539813995361\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.041706562042236 | KNN Loss: 5.024456977844238 | BCE Loss: 1.0172494649887085\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.144840717315674 | KNN Loss: 5.0721917152404785 | BCE Loss: 1.0726488828659058\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.095646858215332 | KNN Loss: 5.048330307006836 | BCE Loss: 1.047316312789917\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.052713394165039 | KNN Loss: 5.034103870391846 | BCE Loss: 1.0186092853546143\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 5.999431133270264 | KNN Loss: 5.000045299530029 | BCE Loss: 0.9993857741355896\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.062972068786621 | KNN Loss: 5.03041410446167 | BCE Loss: 1.032557725906372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 6.021491050720215 | KNN Loss: 5.0070929527282715 | BCE Loss: 1.0143979787826538\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.053244113922119 | KNN Loss: 5.023173809051514 | BCE Loss: 1.030070424079895\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.028314113616943 | KNN Loss: 5.002669811248779 | BCE Loss: 1.0256441831588745\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.0571746826171875 | KNN Loss: 5.023507118225098 | BCE Loss: 1.0336676836013794\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.043197154998779 | KNN Loss: 5.002577781677246 | BCE Loss: 1.0406192541122437\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.066444396972656 | KNN Loss: 5.035834312438965 | BCE Loss: 1.0306100845336914\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.048348426818848 | KNN Loss: 5.005783557891846 | BCE Loss: 1.0425646305084229\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.046239852905273 | KNN Loss: 5.009491920471191 | BCE Loss: 1.0367481708526611\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.011214733123779 | KNN Loss: 5.007266998291016 | BCE Loss: 1.0039477348327637\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.064173698425293 | KNN Loss: 5.024593353271484 | BCE Loss: 1.0395804643630981\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.050786972045898 | KNN Loss: 5.031852722167969 | BCE Loss: 1.0189342498779297\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.061434745788574 | KNN Loss: 5.028921127319336 | BCE Loss: 1.0325133800506592\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.007888317108154 | KNN Loss: 5.0023345947265625 | BCE Loss: 1.0055537223815918\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.0692033767700195 | KNN Loss: 5.023143291473389 | BCE Loss: 1.0460598468780518\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 6.04879093170166 | KNN Loss: 5.026124954223633 | BCE Loss: 1.0226659774780273\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 6.060330390930176 | KNN Loss: 5.038830280303955 | BCE Loss: 1.0215003490447998\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 5.993500709533691 | KNN Loss: 5.001711845397949 | BCE Loss: 0.991788923740387\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.00239372253418 | KNN Loss: 5.006153583526611 | BCE Loss: 0.9962401390075684\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.040767669677734 | KNN Loss: 5.013383865356445 | BCE Loss: 1.02738356590271\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.029926300048828 | KNN Loss: 5.0191874504089355 | BCE Loss: 1.0107388496398926\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 6.022691249847412 | KNN Loss: 5.009434223175049 | BCE Loss: 1.0132571458816528\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.0385661125183105 | KNN Loss: 5.023752689361572 | BCE Loss: 1.0148134231567383\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 5.991153717041016 | KNN Loss: 4.997949123382568 | BCE Loss: 0.9932048320770264\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.020719051361084 | KNN Loss: 4.995429039001465 | BCE Loss: 1.0252900123596191\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.040125846862793 | KNN Loss: 5.016557693481445 | BCE Loss: 1.0235681533813477\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.052886962890625 | KNN Loss: 5.010366916656494 | BCE Loss: 1.04252028465271\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.033508777618408 | KNN Loss: 4.9932541847229 | BCE Loss: 1.0402545928955078\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 6.0647430419921875 | KNN Loss: 5.003952503204346 | BCE Loss: 1.060790777206421\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.037700653076172 | KNN Loss: 5.007076263427734 | BCE Loss: 1.0306246280670166\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.031159400939941 | KNN Loss: 5.015500545501709 | BCE Loss: 1.0156586170196533\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.0205183029174805 | KNN Loss: 4.999868869781494 | BCE Loss: 1.0206496715545654\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.057313442230225 | KNN Loss: 5.014285087585449 | BCE Loss: 1.0430283546447754\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.053755760192871 | KNN Loss: 5.019804000854492 | BCE Loss: 1.0339518785476685\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.073861122131348 | KNN Loss: 5.0358500480651855 | BCE Loss: 1.038011074066162\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.047030448913574 | KNN Loss: 5.0118327140808105 | BCE Loss: 1.0351974964141846\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.1011247634887695 | KNN Loss: 5.063002586364746 | BCE Loss: 1.0381220579147339\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.05302095413208 | KNN Loss: 5.023298263549805 | BCE Loss: 1.0297226905822754\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.037856578826904 | KNN Loss: 5.012962341308594 | BCE Loss: 1.024894118309021\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.042295455932617 | KNN Loss: 5.028087615966797 | BCE Loss: 1.0142078399658203\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.065713882446289 | KNN Loss: 5.016061782836914 | BCE Loss: 1.0496519804000854\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.008195400238037 | KNN Loss: 5.006676197052002 | BCE Loss: 1.0015193223953247\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.047017574310303 | KNN Loss: 5.015683174133301 | BCE Loss: 1.031334400177002\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.043045997619629 | KNN Loss: 5.022884368896484 | BCE Loss: 1.0201618671417236\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.059536457061768 | KNN Loss: 5.013050556182861 | BCE Loss: 1.0464859008789062\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.046322822570801 | KNN Loss: 5.001131534576416 | BCE Loss: 1.0451910495758057\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.058135032653809 | KNN Loss: 5.035236358642578 | BCE Loss: 1.0228986740112305\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.052731990814209 | KNN Loss: 5.006804943084717 | BCE Loss: 1.0459270477294922\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.030921936035156 | KNN Loss: 4.995608329772949 | BCE Loss: 1.035313367843628\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.074466705322266 | KNN Loss: 5.0293121337890625 | BCE Loss: 1.0451548099517822\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.007657527923584 | KNN Loss: 5.015195369720459 | BCE Loss: 0.9924620389938354\n",
      "Epoch   191: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.032101154327393 | KNN Loss: 5.012224197387695 | BCE Loss: 1.0198769569396973\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.0184149742126465 | KNN Loss: 5.0002570152282715 | BCE Loss: 1.0181578397750854\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.048042297363281 | KNN Loss: 5.0390095710754395 | BCE Loss: 1.009032964706421\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.043973922729492 | KNN Loss: 5.005698204040527 | BCE Loss: 1.0382754802703857\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.046195030212402 | KNN Loss: 5.038152694702148 | BCE Loss: 1.0080420970916748\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.069378852844238 | KNN Loss: 5.027383327484131 | BCE Loss: 1.0419957637786865\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.047574043273926 | KNN Loss: 5.013880729675293 | BCE Loss: 1.0336933135986328\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 5.995448112487793 | KNN Loss: 5.005004405975342 | BCE Loss: 0.9904437065124512\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.034067630767822 | KNN Loss: 5.030254364013672 | BCE Loss: 1.0038132667541504\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.097605228424072 | KNN Loss: 5.037286758422852 | BCE Loss: 1.0603185892105103\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.047884464263916 | KNN Loss: 5.014118194580078 | BCE Loss: 1.033766269683838\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.043388366699219 | KNN Loss: 5.035515308380127 | BCE Loss: 1.0078731775283813\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.031242370605469 | KNN Loss: 5.020861625671387 | BCE Loss: 1.010380744934082\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.031148433685303 | KNN Loss: 5.020708084106445 | BCE Loss: 1.0104402303695679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.106555938720703 | KNN Loss: 5.02431583404541 | BCE Loss: 1.0822398662567139\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.028464317321777 | KNN Loss: 4.996397495269775 | BCE Loss: 1.032067060470581\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.021909713745117 | KNN Loss: 5.008660793304443 | BCE Loss: 1.0132488012313843\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.032464027404785 | KNN Loss: 5.013736724853516 | BCE Loss: 1.018727421760559\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.052937030792236 | KNN Loss: 5.023896217346191 | BCE Loss: 1.0290409326553345\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.097586631774902 | KNN Loss: 5.05354642868042 | BCE Loss: 1.0440400838851929\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.056812286376953 | KNN Loss: 5.034637451171875 | BCE Loss: 1.0221748352050781\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.045169830322266 | KNN Loss: 5.015415668487549 | BCE Loss: 1.0297541618347168\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.0342116355896 | KNN Loss: 5.030445098876953 | BCE Loss: 1.003766655921936\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.099937915802002 | KNN Loss: 5.061765670776367 | BCE Loss: 1.0381721258163452\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.049963474273682 | KNN Loss: 5.021061897277832 | BCE Loss: 1.0289015769958496\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.036280632019043 | KNN Loss: 5.035048484802246 | BCE Loss: 1.0012319087982178\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.0713605880737305 | KNN Loss: 5.0406317710876465 | BCE Loss: 1.0307285785675049\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.061792373657227 | KNN Loss: 5.037013053894043 | BCE Loss: 1.0247790813446045\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.040859222412109 | KNN Loss: 5.039163589477539 | BCE Loss: 1.0016956329345703\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.040627479553223 | KNN Loss: 5.027234077453613 | BCE Loss: 1.013393521308899\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 6.044467926025391 | KNN Loss: 4.995476722717285 | BCE Loss: 1.0489909648895264\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.0588459968566895 | KNN Loss: 5.029212951660156 | BCE Loss: 1.0296330451965332\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.053886890411377 | KNN Loss: 5.0378642082214355 | BCE Loss: 1.0160226821899414\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.067022323608398 | KNN Loss: 5.017805576324463 | BCE Loss: 1.0492165088653564\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 6.04889440536499 | KNN Loss: 5.034313678741455 | BCE Loss: 1.0145806074142456\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.075550556182861 | KNN Loss: 5.025628566741943 | BCE Loss: 1.049921989440918\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 6.081308841705322 | KNN Loss: 5.025352954864502 | BCE Loss: 1.0559557676315308\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.090813636779785 | KNN Loss: 5.040356159210205 | BCE Loss: 1.0504577159881592\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.041729927062988 | KNN Loss: 5.022977828979492 | BCE Loss: 1.018752098083496\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.03678035736084 | KNN Loss: 5.003047466278076 | BCE Loss: 1.0337326526641846\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.04879093170166 | KNN Loss: 5.004646301269531 | BCE Loss: 1.044144630432129\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.111222267150879 | KNN Loss: 5.075247764587402 | BCE Loss: 1.0359745025634766\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.025581359863281 | KNN Loss: 5.007230758666992 | BCE Loss: 1.0183507204055786\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.080224514007568 | KNN Loss: 5.0384440422058105 | BCE Loss: 1.0417804718017578\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.063081741333008 | KNN Loss: 5.00787353515625 | BCE Loss: 1.0552079677581787\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.070659637451172 | KNN Loss: 5.012103080749512 | BCE Loss: 1.0585567951202393\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.023324012756348 | KNN Loss: 5.000298976898193 | BCE Loss: 1.0230250358581543\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.061347961425781 | KNN Loss: 5.026093006134033 | BCE Loss: 1.0352550745010376\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.092432022094727 | KNN Loss: 5.048925876617432 | BCE Loss: 1.0435059070587158\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.058531284332275 | KNN Loss: 5.023681163787842 | BCE Loss: 1.0348501205444336\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 6.032527446746826 | KNN Loss: 5.001437664031982 | BCE Loss: 1.0310899019241333\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 6.066108703613281 | KNN Loss: 5.024468421936035 | BCE Loss: 1.041640043258667\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.036453723907471 | KNN Loss: 5.00255012512207 | BCE Loss: 1.0339035987854004\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.093850135803223 | KNN Loss: 5.045182228088379 | BCE Loss: 1.0486679077148438\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.075042247772217 | KNN Loss: 5.041141510009766 | BCE Loss: 1.0339007377624512\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.026071548461914 | KNN Loss: 5.011755466461182 | BCE Loss: 1.0143163204193115\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.063324928283691 | KNN Loss: 5.0529093742370605 | BCE Loss: 1.0104156732559204\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.047944068908691 | KNN Loss: 5.022781848907471 | BCE Loss: 1.0251623392105103\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.080084800720215 | KNN Loss: 5.039910316467285 | BCE Loss: 1.0401744842529297\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.086329460144043 | KNN Loss: 5.041367530822754 | BCE Loss: 1.0449621677398682\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.058259010314941 | KNN Loss: 5.021031379699707 | BCE Loss: 1.0372276306152344\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.038195610046387 | KNN Loss: 5.025269031524658 | BCE Loss: 1.0129268169403076\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 6.075897693634033 | KNN Loss: 5.033741474151611 | BCE Loss: 1.0421563386917114\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.039374828338623 | KNN Loss: 5.032650947570801 | BCE Loss: 1.0067238807678223\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.047318458557129 | KNN Loss: 5.012608051300049 | BCE Loss: 1.0347105264663696\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.026312828063965 | KNN Loss: 5.010164737701416 | BCE Loss: 1.0161482095718384\n",
      "Epoch   202: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 6.077171325683594 | KNN Loss: 5.041715145111084 | BCE Loss: 1.0354564189910889\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.035404205322266 | KNN Loss: 5.02341890335083 | BCE Loss: 1.0119850635528564\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.090408802032471 | KNN Loss: 5.056407928466797 | BCE Loss: 1.0340008735656738\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.024205207824707 | KNN Loss: 5.01265811920166 | BCE Loss: 1.0115470886230469\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.089175224304199 | KNN Loss: 5.07118034362793 | BCE Loss: 1.0179948806762695\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.086363792419434 | KNN Loss: 5.0687994956970215 | BCE Loss: 1.017564058303833\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.0455451011657715 | KNN Loss: 5.020627975463867 | BCE Loss: 1.0249171257019043\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.045997142791748 | KNN Loss: 5.015114784240723 | BCE Loss: 1.0308823585510254\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 6.038097381591797 | KNN Loss: 5.002070903778076 | BCE Loss: 1.0360262393951416\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.072006702423096 | KNN Loss: 5.019594669342041 | BCE Loss: 1.0524121522903442\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 6.072785377502441 | KNN Loss: 5.047630310058594 | BCE Loss: 1.0251553058624268\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 6.04788875579834 | KNN Loss: 5.010958671569824 | BCE Loss: 1.0369300842285156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.083514213562012 | KNN Loss: 5.053910255432129 | BCE Loss: 1.0296037197113037\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.088666915893555 | KNN Loss: 5.049191951751709 | BCE Loss: 1.0394752025604248\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.0593414306640625 | KNN Loss: 5.023741245269775 | BCE Loss: 1.0356003046035767\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.06182861328125 | KNN Loss: 5.030941009521484 | BCE Loss: 1.030887484550476\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 6.031280517578125 | KNN Loss: 4.998959541320801 | BCE Loss: 1.0323209762573242\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.077281951904297 | KNN Loss: 5.042796611785889 | BCE Loss: 1.0344853401184082\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.034902572631836 | KNN Loss: 4.999104976654053 | BCE Loss: 1.0357975959777832\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.070522308349609 | KNN Loss: 5.002176284790039 | BCE Loss: 1.0683460235595703\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.011622428894043 | KNN Loss: 4.9966864585876465 | BCE Loss: 1.0149357318878174\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.035425662994385 | KNN Loss: 5.032670497894287 | BCE Loss: 1.0027551651000977\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.046057224273682 | KNN Loss: 5.018322944641113 | BCE Loss: 1.0277342796325684\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.043561935424805 | KNN Loss: 5.003602504730225 | BCE Loss: 1.03995943069458\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.040437698364258 | KNN Loss: 5.006021022796631 | BCE Loss: 1.034416675567627\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 6.088728904724121 | KNN Loss: 5.018284797668457 | BCE Loss: 1.070444107055664\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 6.054477214813232 | KNN Loss: 5.034593105316162 | BCE Loss: 1.0198839902877808\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.060120582580566 | KNN Loss: 5.030207633972168 | BCE Loss: 1.0299129486083984\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.047418594360352 | KNN Loss: 5.012483596801758 | BCE Loss: 1.0349349975585938\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.116900444030762 | KNN Loss: 5.081569671630859 | BCE Loss: 1.0353307723999023\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.071928024291992 | KNN Loss: 5.025681495666504 | BCE Loss: 1.0462466478347778\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.081353187561035 | KNN Loss: 5.039397239685059 | BCE Loss: 1.0419559478759766\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.0624895095825195 | KNN Loss: 5.013537883758545 | BCE Loss: 1.0489516258239746\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.01731014251709 | KNN Loss: 5.000920295715332 | BCE Loss: 1.0163898468017578\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.001064300537109 | KNN Loss: 4.997294902801514 | BCE Loss: 1.0037691593170166\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.065333366394043 | KNN Loss: 5.040231227874756 | BCE Loss: 1.025101900100708\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.064716339111328 | KNN Loss: 5.004921913146973 | BCE Loss: 1.0597941875457764\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.0159807205200195 | KNN Loss: 5.013230800628662 | BCE Loss: 1.002750039100647\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.043786525726318 | KNN Loss: 5.011272430419922 | BCE Loss: 1.032513976097107\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.054434776306152 | KNN Loss: 5.007559776306152 | BCE Loss: 1.0468748807907104\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.027446269989014 | KNN Loss: 5.017609119415283 | BCE Loss: 1.009837031364441\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.069504737854004 | KNN Loss: 5.047638416290283 | BCE Loss: 1.0218660831451416\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.0578203201293945 | KNN Loss: 5.005544662475586 | BCE Loss: 1.0522758960723877\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.0550994873046875 | KNN Loss: 5.024165630340576 | BCE Loss: 1.0309336185455322\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.032783508300781 | KNN Loss: 5.010949611663818 | BCE Loss: 1.021834135055542\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.057732105255127 | KNN Loss: 5.004302978515625 | BCE Loss: 1.053429126739502\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.037108421325684 | KNN Loss: 5.00454044342041 | BCE Loss: 1.0325682163238525\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.068443298339844 | KNN Loss: 5.033738136291504 | BCE Loss: 1.034705400466919\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.01989221572876 | KNN Loss: 5.037625789642334 | BCE Loss: 0.9822665452957153\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 6.073913097381592 | KNN Loss: 5.03018856048584 | BCE Loss: 1.0437244176864624\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.0570759773254395 | KNN Loss: 5.018862247467041 | BCE Loss: 1.0382137298583984\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.119185447692871 | KNN Loss: 5.076766014099121 | BCE Loss: 1.04241943359375\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.055049896240234 | KNN Loss: 5.024272918701172 | BCE Loss: 1.0307769775390625\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.089716911315918 | KNN Loss: 5.055447101593018 | BCE Loss: 1.0342698097229004\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.060086250305176 | KNN Loss: 5.028217792510986 | BCE Loss: 1.0318682193756104\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.041779041290283 | KNN Loss: 5.008732795715332 | BCE Loss: 1.0330462455749512\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.088501453399658 | KNN Loss: 5.0412702560424805 | BCE Loss: 1.0472311973571777\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.082307815551758 | KNN Loss: 5.057700157165527 | BCE Loss: 1.0246078968048096\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.026432991027832 | KNN Loss: 5.0182647705078125 | BCE Loss: 1.0081679821014404\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.044219017028809 | KNN Loss: 5.037203788757324 | BCE Loss: 1.0070152282714844\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.047377586364746 | KNN Loss: 5.015532970428467 | BCE Loss: 1.0318446159362793\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.05766487121582 | KNN Loss: 5.036960124969482 | BCE Loss: 1.0207046270370483\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.038216590881348 | KNN Loss: 5.026216506958008 | BCE Loss: 1.012000322341919\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.040207386016846 | KNN Loss: 5.019248008728027 | BCE Loss: 1.0209593772888184\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.092436790466309 | KNN Loss: 5.0440778732299805 | BCE Loss: 1.0483591556549072\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.053022861480713 | KNN Loss: 5.011666297912598 | BCE Loss: 1.0413565635681152\n",
      "Epoch   213: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.022195816040039 | KNN Loss: 5.014846324920654 | BCE Loss: 1.0073497295379639\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.026180744171143 | KNN Loss: 5.010799407958984 | BCE Loss: 1.0153813362121582\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.068460941314697 | KNN Loss: 5.013418674468994 | BCE Loss: 1.0550422668457031\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.02752161026001 | KNN Loss: 5.016881942749023 | BCE Loss: 1.0106397867202759\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.093572616577148 | KNN Loss: 5.045530796051025 | BCE Loss: 1.048041582107544\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.033465385437012 | KNN Loss: 5.013357162475586 | BCE Loss: 1.0201079845428467\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 5.98347806930542 | KNN Loss: 4.98628568649292 | BCE Loss: 0.9971923828125\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.068385124206543 | KNN Loss: 5.034430980682373 | BCE Loss: 1.03395414352417\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.028826713562012 | KNN Loss: 5.0320305824279785 | BCE Loss: 0.9967959523200989\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.109823703765869 | KNN Loss: 5.069585800170898 | BCE Loss: 1.0402380228042603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.049067497253418 | KNN Loss: 5.015918731689453 | BCE Loss: 1.0331487655639648\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.085432052612305 | KNN Loss: 5.0448527336120605 | BCE Loss: 1.040579080581665\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.0416951179504395 | KNN Loss: 5.0226287841796875 | BCE Loss: 1.0190664529800415\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.1185407638549805 | KNN Loss: 5.061473846435547 | BCE Loss: 1.0570666790008545\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.062761306762695 | KNN Loss: 5.032864093780518 | BCE Loss: 1.0298972129821777\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.04835319519043 | KNN Loss: 5.0264973640441895 | BCE Loss: 1.0218558311462402\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.0505146980285645 | KNN Loss: 5.015153408050537 | BCE Loss: 1.0353612899780273\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.068530082702637 | KNN Loss: 5.039743423461914 | BCE Loss: 1.0287867784500122\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.016397953033447 | KNN Loss: 5.0171799659729 | BCE Loss: 0.9992178678512573\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.059908866882324 | KNN Loss: 5.054983615875244 | BCE Loss: 1.0049254894256592\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.086610317230225 | KNN Loss: 5.041870594024658 | BCE Loss: 1.0447397232055664\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.028046607971191 | KNN Loss: 5.024948596954346 | BCE Loss: 1.0030977725982666\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.046136856079102 | KNN Loss: 5.018966197967529 | BCE Loss: 1.0271704196929932\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.076059341430664 | KNN Loss: 5.0212225914001465 | BCE Loss: 1.0548369884490967\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 6.029183864593506 | KNN Loss: 5.0124616622924805 | BCE Loss: 1.0167222023010254\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.047752380371094 | KNN Loss: 5.009812831878662 | BCE Loss: 1.0379395484924316\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.035503387451172 | KNN Loss: 5.019741058349609 | BCE Loss: 1.015762209892273\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.025083541870117 | KNN Loss: 5.000861167907715 | BCE Loss: 1.0242221355438232\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.01310920715332 | KNN Loss: 5.0244460105896 | BCE Loss: 0.9886629581451416\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.061446666717529 | KNN Loss: 5.016556262969971 | BCE Loss: 1.044890284538269\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.084510803222656 | KNN Loss: 5.042727470397949 | BCE Loss: 1.041783332824707\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.0707173347473145 | KNN Loss: 5.009411334991455 | BCE Loss: 1.0613058805465698\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.034037113189697 | KNN Loss: 5.003876686096191 | BCE Loss: 1.0301605463027954\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.0488691329956055 | KNN Loss: 5.038837432861328 | BCE Loss: 1.0100315809249878\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.060771942138672 | KNN Loss: 5.018649101257324 | BCE Loss: 1.0421229600906372\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.013474464416504 | KNN Loss: 5.004821300506592 | BCE Loss: 1.008653163909912\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.103116989135742 | KNN Loss: 5.038111686706543 | BCE Loss: 1.0650054216384888\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.055301666259766 | KNN Loss: 5.013545989990234 | BCE Loss: 1.0417555570602417\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.044320106506348 | KNN Loss: 5.039819240570068 | BCE Loss: 1.0045008659362793\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.038282871246338 | KNN Loss: 5.0240678787231445 | BCE Loss: 1.0142149925231934\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.046065807342529 | KNN Loss: 5.028393745422363 | BCE Loss: 1.017672061920166\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.020353317260742 | KNN Loss: 4.995956897735596 | BCE Loss: 1.0243966579437256\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.101366996765137 | KNN Loss: 5.048354148864746 | BCE Loss: 1.0530129671096802\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 5.995882034301758 | KNN Loss: 5.015685558319092 | BCE Loss: 0.9801962971687317\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.016706943511963 | KNN Loss: 5.007360935211182 | BCE Loss: 1.0093460083007812\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.080904006958008 | KNN Loss: 5.03688907623291 | BCE Loss: 1.0440149307250977\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.057222366333008 | KNN Loss: 5.027396202087402 | BCE Loss: 1.0298261642456055\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.052018165588379 | KNN Loss: 5.019043445587158 | BCE Loss: 1.0329747200012207\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.079516410827637 | KNN Loss: 5.022515296936035 | BCE Loss: 1.0570011138916016\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.041499137878418 | KNN Loss: 5.018978118896484 | BCE Loss: 1.0225210189819336\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.083606719970703 | KNN Loss: 5.050784111022949 | BCE Loss: 1.0328223705291748\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 6.036220550537109 | KNN Loss: 5.020285606384277 | BCE Loss: 1.015934705734253\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.037883758544922 | KNN Loss: 5.00730562210083 | BCE Loss: 1.0305781364440918\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.025708198547363 | KNN Loss: 4.994803428649902 | BCE Loss: 1.03090500831604\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.086883544921875 | KNN Loss: 5.059432506561279 | BCE Loss: 1.0274512767791748\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.054090976715088 | KNN Loss: 5.023024559020996 | BCE Loss: 1.0310664176940918\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.022032260894775 | KNN Loss: 5.019377708435059 | BCE Loss: 1.0026545524597168\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.068986415863037 | KNN Loss: 5.007248878479004 | BCE Loss: 1.0617375373840332\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.071686744689941 | KNN Loss: 5.0559306144714355 | BCE Loss: 1.0157561302185059\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.037836074829102 | KNN Loss: 5.015230655670166 | BCE Loss: 1.0226051807403564\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.042936325073242 | KNN Loss: 5.027310848236084 | BCE Loss: 1.0156257152557373\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.076136589050293 | KNN Loss: 5.022625923156738 | BCE Loss: 1.0535107851028442\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.030517578125 | KNN Loss: 5.019840240478516 | BCE Loss: 1.0106772184371948\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 6.058180332183838 | KNN Loss: 5.006566047668457 | BCE Loss: 1.0516142845153809\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.0620574951171875 | KNN Loss: 5.010732173919678 | BCE Loss: 1.0513253211975098\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 6.091155529022217 | KNN Loss: 5.0488433837890625 | BCE Loss: 1.0423120260238647\n",
      "Epoch   224: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.037858009338379 | KNN Loss: 5.003337860107422 | BCE Loss: 1.034519910812378\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.066689491271973 | KNN Loss: 5.028011798858643 | BCE Loss: 1.038677453994751\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.046611785888672 | KNN Loss: 5.038692474365234 | BCE Loss: 1.0079193115234375\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.089634418487549 | KNN Loss: 5.060027122497559 | BCE Loss: 1.0296071767807007\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.062349319458008 | KNN Loss: 5.020864963531494 | BCE Loss: 1.0414843559265137\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.047049045562744 | KNN Loss: 5.0233964920043945 | BCE Loss: 1.0236525535583496\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.05592155456543 | KNN Loss: 5.039541244506836 | BCE Loss: 1.0163800716400146\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.056911468505859 | KNN Loss: 5.033239841461182 | BCE Loss: 1.0236717462539673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.049723148345947 | KNN Loss: 5.023898601531982 | BCE Loss: 1.0258245468139648\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.085808753967285 | KNN Loss: 5.028022766113281 | BCE Loss: 1.057785987854004\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.072114944458008 | KNN Loss: 5.026945114135742 | BCE Loss: 1.0451695919036865\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.075882911682129 | KNN Loss: 5.018072605133057 | BCE Loss: 1.0578104257583618\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.062229156494141 | KNN Loss: 5.0176897048950195 | BCE Loss: 1.044539451599121\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 6.046594619750977 | KNN Loss: 5.011147499084473 | BCE Loss: 1.035447120666504\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 6.030889511108398 | KNN Loss: 5.00692892074585 | BCE Loss: 1.0239605903625488\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.036469459533691 | KNN Loss: 5.012301445007324 | BCE Loss: 1.0241682529449463\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.050502777099609 | KNN Loss: 5.034198760986328 | BCE Loss: 1.0163038969039917\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 6.044290065765381 | KNN Loss: 5.0120744705200195 | BCE Loss: 1.0322155952453613\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.050313472747803 | KNN Loss: 5.026412487030029 | BCE Loss: 1.0239009857177734\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.029687404632568 | KNN Loss: 5.026607513427734 | BCE Loss: 1.003079891204834\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.068363189697266 | KNN Loss: 5.035914421081543 | BCE Loss: 1.0324488878250122\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.053308010101318 | KNN Loss: 5.006133079528809 | BCE Loss: 1.0471749305725098\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.068633079528809 | KNN Loss: 5.006373405456543 | BCE Loss: 1.0622599124908447\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.155967712402344 | KNN Loss: 5.098956108093262 | BCE Loss: 1.057011604309082\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.060131549835205 | KNN Loss: 5.022382736206055 | BCE Loss: 1.0377488136291504\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.096694469451904 | KNN Loss: 5.047614097595215 | BCE Loss: 1.0490803718566895\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 6.053646087646484 | KNN Loss: 5.036348342895508 | BCE Loss: 1.0172978639602661\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.023031711578369 | KNN Loss: 5.0013298988342285 | BCE Loss: 1.0217019319534302\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.067808628082275 | KNN Loss: 5.029386520385742 | BCE Loss: 1.0384221076965332\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.02396821975708 | KNN Loss: 5.012950897216797 | BCE Loss: 1.0110172033309937\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.034543514251709 | KNN Loss: 5.022781848907471 | BCE Loss: 1.0117616653442383\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.038510322570801 | KNN Loss: 5.00404691696167 | BCE Loss: 1.0344635248184204\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.02947473526001 | KNN Loss: 5.008362770080566 | BCE Loss: 1.021112084388733\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 6.027121067047119 | KNN Loss: 5.018608570098877 | BCE Loss: 1.0085124969482422\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.094376087188721 | KNN Loss: 5.038267135620117 | BCE Loss: 1.056108832359314\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.05251932144165 | KNN Loss: 5.020865440368652 | BCE Loss: 1.031653881072998\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.023360252380371 | KNN Loss: 5.022146224975586 | BCE Loss: 1.0012142658233643\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.025740623474121 | KNN Loss: 5.005189895629883 | BCE Loss: 1.0205507278442383\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.027397155761719 | KNN Loss: 5.014281272888184 | BCE Loss: 1.0131160020828247\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.060524940490723 | KNN Loss: 5.024275302886963 | BCE Loss: 1.0362496376037598\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.007772922515869 | KNN Loss: 5.003871917724609 | BCE Loss: 1.0039011240005493\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.10177755355835 | KNN Loss: 5.043286323547363 | BCE Loss: 1.0584911108016968\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.0439863204956055 | KNN Loss: 5.034524440765381 | BCE Loss: 1.0094616413116455\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.015190124511719 | KNN Loss: 5.0059590339660645 | BCE Loss: 1.0092308521270752\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.058183670043945 | KNN Loss: 5.015194416046143 | BCE Loss: 1.0429890155792236\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.028803825378418 | KNN Loss: 5.014415264129639 | BCE Loss: 1.0143883228302002\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.00827169418335 | KNN Loss: 4.987403392791748 | BCE Loss: 1.0208683013916016\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 6.041015148162842 | KNN Loss: 5.010536193847656 | BCE Loss: 1.0304789543151855\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.068960189819336 | KNN Loss: 5.037559509277344 | BCE Loss: 1.0314007997512817\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.054401874542236 | KNN Loss: 5.0327534675598145 | BCE Loss: 1.0216485261917114\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.032172203063965 | KNN Loss: 5.029485702514648 | BCE Loss: 1.0026865005493164\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.023268699645996 | KNN Loss: 5.002636432647705 | BCE Loss: 1.020632266998291\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.053842544555664 | KNN Loss: 5.040486812591553 | BCE Loss: 1.0133556127548218\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.040189743041992 | KNN Loss: 5.010738849639893 | BCE Loss: 1.0294508934020996\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.061730861663818 | KNN Loss: 5.042994499206543 | BCE Loss: 1.0187363624572754\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.05794620513916 | KNN Loss: 5.039051532745361 | BCE Loss: 1.0188947916030884\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.1215715408325195 | KNN Loss: 5.088340759277344 | BCE Loss: 1.0332307815551758\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.045446395874023 | KNN Loss: 5.007833003997803 | BCE Loss: 1.0376131534576416\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.052273750305176 | KNN Loss: 5.021100997924805 | BCE Loss: 1.0311729907989502\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.062076568603516 | KNN Loss: 5.032166481018066 | BCE Loss: 1.0299103260040283\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.056755065917969 | KNN Loss: 5.028105735778809 | BCE Loss: 1.0286495685577393\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.043239593505859 | KNN Loss: 5.0218186378479 | BCE Loss: 1.021421194076538\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.02096700668335 | KNN Loss: 4.995932579040527 | BCE Loss: 1.0250344276428223\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.000059127807617 | KNN Loss: 5.003035545349121 | BCE Loss: 0.9970234632492065\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.08500862121582 | KNN Loss: 5.039757251739502 | BCE Loss: 1.0452516078948975\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.087139129638672 | KNN Loss: 5.045721530914307 | BCE Loss: 1.0414174795150757\n",
      "Epoch   235: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.0495405197143555 | KNN Loss: 5.022101402282715 | BCE Loss: 1.0274393558502197\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.0708417892456055 | KNN Loss: 5.032347202301025 | BCE Loss: 1.03849458694458\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.044769763946533 | KNN Loss: 5.021145343780518 | BCE Loss: 1.0236244201660156\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.014212608337402 | KNN Loss: 4.983388900756836 | BCE Loss: 1.0308235883712769\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.036728858947754 | KNN Loss: 5.01326847076416 | BCE Loss: 1.0234601497650146\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.035811901092529 | KNN Loss: 5.01731014251709 | BCE Loss: 1.0185017585754395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.0289106369018555 | KNN Loss: 5.006335258483887 | BCE Loss: 1.0225754976272583\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.004672527313232 | KNN Loss: 4.996206760406494 | BCE Loss: 1.0084657669067383\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.0486836433410645 | KNN Loss: 5.033788204193115 | BCE Loss: 1.0148953199386597\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.032046794891357 | KNN Loss: 5.002834796905518 | BCE Loss: 1.0292118787765503\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.051270961761475 | KNN Loss: 5.013434410095215 | BCE Loss: 1.0378365516662598\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 6.066123008728027 | KNN Loss: 5.046825885772705 | BCE Loss: 1.0192968845367432\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.078777313232422 | KNN Loss: 5.017185688018799 | BCE Loss: 1.061591625213623\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 6.027895927429199 | KNN Loss: 5.010066032409668 | BCE Loss: 1.0178301334381104\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.113422393798828 | KNN Loss: 5.077729225158691 | BCE Loss: 1.0356931686401367\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.044999122619629 | KNN Loss: 5.023993015289307 | BCE Loss: 1.0210063457489014\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.023125171661377 | KNN Loss: 5.0114593505859375 | BCE Loss: 1.011665940284729\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.046763896942139 | KNN Loss: 5.001842021942139 | BCE Loss: 1.0449217557907104\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.01002311706543 | KNN Loss: 5.025489807128906 | BCE Loss: 0.9845333695411682\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.126365661621094 | KNN Loss: 5.070616722106934 | BCE Loss: 1.0557489395141602\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.015342712402344 | KNN Loss: 5.013068675994873 | BCE Loss: 1.0022737979888916\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.0499982833862305 | KNN Loss: 5.025143146514893 | BCE Loss: 1.024855375289917\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.060205936431885 | KNN Loss: 5.004400730133057 | BCE Loss: 1.0558053255081177\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.075597763061523 | KNN Loss: 5.025442123413086 | BCE Loss: 1.0501554012298584\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.044348239898682 | KNN Loss: 5.023435115814209 | BCE Loss: 1.020913004875183\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.064610004425049 | KNN Loss: 5.005895137786865 | BCE Loss: 1.0587149858474731\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.04069709777832 | KNN Loss: 5.026330471038818 | BCE Loss: 1.0143665075302124\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.028188705444336 | KNN Loss: 4.999929428100586 | BCE Loss: 1.028259038925171\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.0214996337890625 | KNN Loss: 5.0042243003845215 | BCE Loss: 1.0172755718231201\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.093865394592285 | KNN Loss: 5.038959503173828 | BCE Loss: 1.0549060106277466\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.044773101806641 | KNN Loss: 5.0206780433654785 | BCE Loss: 1.024094820022583\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.043795585632324 | KNN Loss: 5.010472774505615 | BCE Loss: 1.033323049545288\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.07560920715332 | KNN Loss: 5.041006088256836 | BCE Loss: 1.034603238105774\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.0217766761779785 | KNN Loss: 5.021891117095947 | BCE Loss: 0.9998856782913208\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.04998254776001 | KNN Loss: 5.01416540145874 | BCE Loss: 1.0358171463012695\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.045364856719971 | KNN Loss: 5.024081230163574 | BCE Loss: 1.021283745765686\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.072684288024902 | KNN Loss: 5.051220893859863 | BCE Loss: 1.0214636325836182\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 6.026122093200684 | KNN Loss: 5.002884864807129 | BCE Loss: 1.0232371091842651\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.020051002502441 | KNN Loss: 5.006381988525391 | BCE Loss: 1.0136692523956299\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.008033275604248 | KNN Loss: 5.007110595703125 | BCE Loss: 1.000922679901123\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.070344924926758 | KNN Loss: 5.031744480133057 | BCE Loss: 1.038600206375122\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.045927047729492 | KNN Loss: 5.016598701477051 | BCE Loss: 1.0293283462524414\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.079407691955566 | KNN Loss: 5.021839141845703 | BCE Loss: 1.0575685501098633\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.10798978805542 | KNN Loss: 5.0880937576293945 | BCE Loss: 1.0198959112167358\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.044318675994873 | KNN Loss: 5.04234504699707 | BCE Loss: 1.0019736289978027\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.050018310546875 | KNN Loss: 5.0248284339904785 | BCE Loss: 1.0251896381378174\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.048128128051758 | KNN Loss: 5.007122993469238 | BCE Loss: 1.0410053730010986\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.037679672241211 | KNN Loss: 5.012798309326172 | BCE Loss: 1.024881362915039\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.045479774475098 | KNN Loss: 5.030066013336182 | BCE Loss: 1.0154138803482056\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.07565975189209 | KNN Loss: 5.035146713256836 | BCE Loss: 1.0405128002166748\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.046783924102783 | KNN Loss: 5.002508640289307 | BCE Loss: 1.0442752838134766\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.052330017089844 | KNN Loss: 5.016750812530518 | BCE Loss: 1.0355794429779053\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.0889997482299805 | KNN Loss: 5.0351409912109375 | BCE Loss: 1.053858995437622\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.046941757202148 | KNN Loss: 5.004931449890137 | BCE Loss: 1.0420103073120117\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.076817512512207 | KNN Loss: 5.0421319007873535 | BCE Loss: 1.0346856117248535\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.097963333129883 | KNN Loss: 5.062154769897461 | BCE Loss: 1.0358086824417114\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.0596747398376465 | KNN Loss: 5.023123741149902 | BCE Loss: 1.0365511178970337\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.072229385375977 | KNN Loss: 5.020047664642334 | BCE Loss: 1.0521817207336426\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.104496002197266 | KNN Loss: 5.058646202087402 | BCE Loss: 1.0458498001098633\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.056572914123535 | KNN Loss: 5.012791156768799 | BCE Loss: 1.0437817573547363\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.035406112670898 | KNN Loss: 5.009303092956543 | BCE Loss: 1.026103138923645\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.04230260848999 | KNN Loss: 5.014406681060791 | BCE Loss: 1.0278960466384888\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.066738128662109 | KNN Loss: 5.034696578979492 | BCE Loss: 1.032041311264038\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.052738189697266 | KNN Loss: 5.026546001434326 | BCE Loss: 1.0261919498443604\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.036171913146973 | KNN Loss: 5.016811847686768 | BCE Loss: 1.019359827041626\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.035096168518066 | KNN Loss: 5.010473728179932 | BCE Loss: 1.0246226787567139\n",
      "Epoch   246: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.034124851226807 | KNN Loss: 5.005871295928955 | BCE Loss: 1.0282535552978516\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.047517776489258 | KNN Loss: 5.008591175079346 | BCE Loss: 1.0389264822006226\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.058612823486328 | KNN Loss: 5.039615154266357 | BCE Loss: 1.0189975500106812\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.049010276794434 | KNN Loss: 5.005484580993652 | BCE Loss: 1.0435254573822021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.067867279052734 | KNN Loss: 5.038124084472656 | BCE Loss: 1.0297430753707886\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.068826675415039 | KNN Loss: 5.039900302886963 | BCE Loss: 1.0289264917373657\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.0842437744140625 | KNN Loss: 5.02943229675293 | BCE Loss: 1.054811716079712\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.1016740798950195 | KNN Loss: 5.055055618286133 | BCE Loss: 1.0466185808181763\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 6.04471492767334 | KNN Loss: 5.0164361000061035 | BCE Loss: 1.0282788276672363\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.029384613037109 | KNN Loss: 5.002326965332031 | BCE Loss: 1.0270575284957886\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.017473220825195 | KNN Loss: 5.021862506866455 | BCE Loss: 0.9956104755401611\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.046443462371826 | KNN Loss: 5.009698867797852 | BCE Loss: 1.0367445945739746\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.032130718231201 | KNN Loss: 5.004482746124268 | BCE Loss: 1.0276479721069336\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.016802787780762 | KNN Loss: 4.995169162750244 | BCE Loss: 1.0216338634490967\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.016219615936279 | KNN Loss: 5.009206771850586 | BCE Loss: 1.0070128440856934\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.088659286499023 | KNN Loss: 5.043699264526367 | BCE Loss: 1.0449597835540771\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.090234756469727 | KNN Loss: 5.041722774505615 | BCE Loss: 1.0485118627548218\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.022512912750244 | KNN Loss: 5.001894474029541 | BCE Loss: 1.0206184387207031\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.043044090270996 | KNN Loss: 5.0204854011535645 | BCE Loss: 1.0225586891174316\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.045528888702393 | KNN Loss: 5.029918193817139 | BCE Loss: 1.015610694885254\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.064443588256836 | KNN Loss: 5.023403644561768 | BCE Loss: 1.0410401821136475\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.04841423034668 | KNN Loss: 5.010070323944092 | BCE Loss: 1.038344144821167\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.0835442543029785 | KNN Loss: 5.040292739868164 | BCE Loss: 1.043251395225525\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.1268110275268555 | KNN Loss: 5.092546463012695 | BCE Loss: 1.034264326095581\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.095445156097412 | KNN Loss: 5.067204475402832 | BCE Loss: 1.0282405614852905\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.036539554595947 | KNN Loss: 5.00260066986084 | BCE Loss: 1.0339387655258179\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.043910980224609 | KNN Loss: 5.014985084533691 | BCE Loss: 1.028925895690918\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.02390718460083 | KNN Loss: 5.006210803985596 | BCE Loss: 1.0176963806152344\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.018665313720703 | KNN Loss: 5.00664758682251 | BCE Loss: 1.0120179653167725\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.046285629272461 | KNN Loss: 5.028992652893066 | BCE Loss: 1.0172927379608154\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.029277324676514 | KNN Loss: 4.994330406188965 | BCE Loss: 1.0349470376968384\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.041827201843262 | KNN Loss: 5.009848117828369 | BCE Loss: 1.0319790840148926\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.024454116821289 | KNN Loss: 5.0216965675354 | BCE Loss: 1.0027575492858887\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.071115493774414 | KNN Loss: 5.030429840087891 | BCE Loss: 1.0406856536865234\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.02152681350708 | KNN Loss: 4.995720863342285 | BCE Loss: 1.025805950164795\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.073230743408203 | KNN Loss: 5.03647518157959 | BCE Loss: 1.0367558002471924\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.066833019256592 | KNN Loss: 5.034986972808838 | BCE Loss: 1.031846046447754\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.04392671585083 | KNN Loss: 5.008235454559326 | BCE Loss: 1.0356911420822144\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.042726516723633 | KNN Loss: 5.010530471801758 | BCE Loss: 1.032195806503296\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.028756141662598 | KNN Loss: 5.014716148376465 | BCE Loss: 1.014040231704712\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.0390801429748535 | KNN Loss: 5.023582458496094 | BCE Loss: 1.0154975652694702\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 6.101011276245117 | KNN Loss: 5.047927379608154 | BCE Loss: 1.053083896636963\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.117345809936523 | KNN Loss: 5.060916423797607 | BCE Loss: 1.0564295053482056\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.048762798309326 | KNN Loss: 5.020900726318359 | BCE Loss: 1.0278621912002563\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.021364212036133 | KNN Loss: 5.004150390625 | BCE Loss: 1.0172138214111328\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.076418876647949 | KNN Loss: 5.023019313812256 | BCE Loss: 1.0533993244171143\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.066224098205566 | KNN Loss: 5.036245822906494 | BCE Loss: 1.0299782752990723\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.017544269561768 | KNN Loss: 5.0080037117004395 | BCE Loss: 1.0095404386520386\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.117093563079834 | KNN Loss: 5.0384135246276855 | BCE Loss: 1.078680157661438\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.004349708557129 | KNN Loss: 5.004537105560303 | BCE Loss: 0.9998127818107605\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.030442714691162 | KNN Loss: 5.021560192108154 | BCE Loss: 1.0088825225830078\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.0667314529418945 | KNN Loss: 5.021081447601318 | BCE Loss: 1.0456500053405762\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.073591232299805 | KNN Loss: 5.02314567565918 | BCE Loss: 1.050445556640625\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.060829162597656 | KNN Loss: 5.03943395614624 | BCE Loss: 1.021395206451416\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.084198951721191 | KNN Loss: 5.043850898742676 | BCE Loss: 1.0403478145599365\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.082912445068359 | KNN Loss: 5.034680366516113 | BCE Loss: 1.048231840133667\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.060053825378418 | KNN Loss: 5.0422282218933105 | BCE Loss: 1.0178254842758179\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.001671314239502 | KNN Loss: 5.003368854522705 | BCE Loss: 0.9983023405075073\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.094132423400879 | KNN Loss: 5.036768436431885 | BCE Loss: 1.0573639869689941\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.061141490936279 | KNN Loss: 5.026083469390869 | BCE Loss: 1.0350580215454102\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.047497272491455 | KNN Loss: 5.049643516540527 | BCE Loss: 0.9978536367416382\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.039776802062988 | KNN Loss: 5.009089946746826 | BCE Loss: 1.0306870937347412\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.051368713378906 | KNN Loss: 5.021419048309326 | BCE Loss: 1.0299497842788696\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.084878921508789 | KNN Loss: 5.057353496551514 | BCE Loss: 1.027525544166565\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.041512966156006 | KNN Loss: 5.0053277015686035 | BCE Loss: 1.0361851453781128\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.028257369995117 | KNN Loss: 5.014132976531982 | BCE Loss: 1.0141242742538452\n",
      "Epoch   257: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.027918338775635 | KNN Loss: 5.007829666137695 | BCE Loss: 1.0200886726379395\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.045586109161377 | KNN Loss: 5.029235363006592 | BCE Loss: 1.0163507461547852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.052661895751953 | KNN Loss: 5.01886510848999 | BCE Loss: 1.0337966680526733\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.035187721252441 | KNN Loss: 5.028391361236572 | BCE Loss: 1.0067964792251587\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.048271179199219 | KNN Loss: 5.015400409698486 | BCE Loss: 1.032870888710022\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.053013801574707 | KNN Loss: 5.009747505187988 | BCE Loss: 1.0432660579681396\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.090051174163818 | KNN Loss: 5.046652317047119 | BCE Loss: 1.0433989763259888\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.05143404006958 | KNN Loss: 5.032796382904053 | BCE Loss: 1.0186375379562378\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.054821968078613 | KNN Loss: 5.006478786468506 | BCE Loss: 1.0483429431915283\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.065473556518555 | KNN Loss: 5.016833305358887 | BCE Loss: 1.048640251159668\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.029543399810791 | KNN Loss: 4.99937105178833 | BCE Loss: 1.030172348022461\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.023493766784668 | KNN Loss: 5.008440971374512 | BCE Loss: 1.0150525569915771\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.026236534118652 | KNN Loss: 5.010025501251221 | BCE Loss: 1.0162110328674316\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.020216941833496 | KNN Loss: 5.00502347946167 | BCE Loss: 1.015193223953247\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.044168949127197 | KNN Loss: 5.010866165161133 | BCE Loss: 1.033302903175354\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.034847736358643 | KNN Loss: 5.01443338394165 | BCE Loss: 1.0204143524169922\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.040857315063477 | KNN Loss: 5.01798677444458 | BCE Loss: 1.0228705406188965\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.10629415512085 | KNN Loss: 5.05567741394043 | BCE Loss: 1.0506168603897095\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.057299613952637 | KNN Loss: 5.038064479827881 | BCE Loss: 1.0192351341247559\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.019350528717041 | KNN Loss: 5.021425247192383 | BCE Loss: 0.9979254603385925\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.022984504699707 | KNN Loss: 5.003526210784912 | BCE Loss: 1.0194584131240845\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.08015251159668 | KNN Loss: 5.029877662658691 | BCE Loss: 1.0502746105194092\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.074440002441406 | KNN Loss: 5.02697229385376 | BCE Loss: 1.0474674701690674\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.096611499786377 | KNN Loss: 5.040372371673584 | BCE Loss: 1.0562390089035034\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.031227111816406 | KNN Loss: 5.011875629425049 | BCE Loss: 1.019351601600647\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.028564453125 | KNN Loss: 5.029160499572754 | BCE Loss: 0.9994040727615356\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.052323341369629 | KNN Loss: 5.016423225402832 | BCE Loss: 1.035900354385376\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.0583086013793945 | KNN Loss: 5.025243282318115 | BCE Loss: 1.0330653190612793\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.040415287017822 | KNN Loss: 5.027685642242432 | BCE Loss: 1.012729525566101\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.02725887298584 | KNN Loss: 5.011843204498291 | BCE Loss: 1.0154154300689697\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.073968887329102 | KNN Loss: 5.010555744171143 | BCE Loss: 1.0634132623672485\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.070016384124756 | KNN Loss: 5.022877216339111 | BCE Loss: 1.047139048576355\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.046334743499756 | KNN Loss: 5.02287483215332 | BCE Loss: 1.023460030555725\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.082751274108887 | KNN Loss: 5.062679767608643 | BCE Loss: 1.020071268081665\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.115697383880615 | KNN Loss: 5.066854953765869 | BCE Loss: 1.048842430114746\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.04555082321167 | KNN Loss: 5.029269218444824 | BCE Loss: 1.0162816047668457\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.043322563171387 | KNN Loss: 5.015890121459961 | BCE Loss: 1.0274326801300049\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.055675029754639 | KNN Loss: 5.024306297302246 | BCE Loss: 1.0313688516616821\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.088744163513184 | KNN Loss: 5.061924457550049 | BCE Loss: 1.0268194675445557\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.102878570556641 | KNN Loss: 5.048202037811279 | BCE Loss: 1.0546764135360718\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.049125671386719 | KNN Loss: 5.024979114532471 | BCE Loss: 1.0241467952728271\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.044780254364014 | KNN Loss: 5.018504619598389 | BCE Loss: 1.026275634765625\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.095717430114746 | KNN Loss: 5.083493232727051 | BCE Loss: 1.0122244358062744\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.127429962158203 | KNN Loss: 5.079196453094482 | BCE Loss: 1.0482337474822998\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.064242362976074 | KNN Loss: 5.0268354415893555 | BCE Loss: 1.0374069213867188\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.034296035766602 | KNN Loss: 5.005862712860107 | BCE Loss: 1.028433084487915\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.065623760223389 | KNN Loss: 5.0494184494018555 | BCE Loss: 1.0162054300308228\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.013697624206543 | KNN Loss: 4.999370098114014 | BCE Loss: 1.0143277645111084\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.017690658569336 | KNN Loss: 5.005612850189209 | BCE Loss: 1.0120776891708374\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.024918079376221 | KNN Loss: 5.020552158355713 | BCE Loss: 1.0043659210205078\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.062338829040527 | KNN Loss: 5.015758991241455 | BCE Loss: 1.0465800762176514\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.014214992523193 | KNN Loss: 5.01600456237793 | BCE Loss: 0.9982102513313293\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.071228504180908 | KNN Loss: 5.0383501052856445 | BCE Loss: 1.0328782796859741\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.036726474761963 | KNN Loss: 4.994795322418213 | BCE Loss: 1.0419310331344604\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.073111534118652 | KNN Loss: 5.007190227508545 | BCE Loss: 1.0659215450286865\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.0363688468933105 | KNN Loss: 5.0232319831848145 | BCE Loss: 1.013136863708496\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.0671305656433105 | KNN Loss: 5.021193504333496 | BCE Loss: 1.045937180519104\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.071532726287842 | KNN Loss: 5.033026218414307 | BCE Loss: 1.0385066270828247\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.045025825500488 | KNN Loss: 5.017062187194824 | BCE Loss: 1.027963399887085\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.0458173751831055 | KNN Loss: 5.037643909454346 | BCE Loss: 1.0081732273101807\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.1537275314331055 | KNN Loss: 5.0597710609436035 | BCE Loss: 1.093956470489502\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.040351390838623 | KNN Loss: 5.019480228424072 | BCE Loss: 1.0208711624145508\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.047369003295898 | KNN Loss: 5.038352966308594 | BCE Loss: 1.0090157985687256\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.0418620109558105 | KNN Loss: 5.018252849578857 | BCE Loss: 1.0236090421676636\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.074051856994629 | KNN Loss: 5.030394554138184 | BCE Loss: 1.0436574220657349\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.03629207611084 | KNN Loss: 5.015356063842773 | BCE Loss: 1.0209362506866455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   268: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 6.013206481933594 | KNN Loss: 5.007236957550049 | BCE Loss: 1.0059692859649658\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.022522926330566 | KNN Loss: 5.009215831756592 | BCE Loss: 1.0133068561553955\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.049413681030273 | KNN Loss: 5.02718448638916 | BCE Loss: 1.0222289562225342\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.103693962097168 | KNN Loss: 5.0423359870910645 | BCE Loss: 1.0613579750061035\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.030168533325195 | KNN Loss: 5.006434440612793 | BCE Loss: 1.0237340927124023\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.044207572937012 | KNN Loss: 5.010908603668213 | BCE Loss: 1.033299207687378\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 5.994176864624023 | KNN Loss: 5.00463342666626 | BCE Loss: 0.9895434975624084\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.0349297523498535 | KNN Loss: 5.005988121032715 | BCE Loss: 1.0289417505264282\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 6.079403877258301 | KNN Loss: 5.045726776123047 | BCE Loss: 1.033677101135254\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.056532859802246 | KNN Loss: 5.021758079528809 | BCE Loss: 1.0347747802734375\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.053322792053223 | KNN Loss: 5.013913154602051 | BCE Loss: 1.0394095182418823\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 6.011056423187256 | KNN Loss: 5.002281188964844 | BCE Loss: 1.008775234222412\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.032607555389404 | KNN Loss: 5.009050369262695 | BCE Loss: 1.0235570669174194\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.019542217254639 | KNN Loss: 4.999312877655029 | BCE Loss: 1.0202292203903198\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.059484958648682 | KNN Loss: 5.044007778167725 | BCE Loss: 1.0154772996902466\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.018637180328369 | KNN Loss: 5.008871078491211 | BCE Loss: 1.0097662210464478\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 6.024334907531738 | KNN Loss: 5.022029876708984 | BCE Loss: 1.0023047924041748\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.092966556549072 | KNN Loss: 5.026393413543701 | BCE Loss: 1.066573143005371\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.095448017120361 | KNN Loss: 5.034985542297363 | BCE Loss: 1.0604625940322876\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.057207107543945 | KNN Loss: 5.0119428634643555 | BCE Loss: 1.0452640056610107\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.05290412902832 | KNN Loss: 5.0159687995910645 | BCE Loss: 1.0369350910186768\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.042898178100586 | KNN Loss: 5.002690315246582 | BCE Loss: 1.040208101272583\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.049524784088135 | KNN Loss: 5.023284912109375 | BCE Loss: 1.0262397527694702\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.070706844329834 | KNN Loss: 5.036726474761963 | BCE Loss: 1.033980369567871\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 6.052243232727051 | KNN Loss: 5.010976314544678 | BCE Loss: 1.041266679763794\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.061478137969971 | KNN Loss: 5.0160603523254395 | BCE Loss: 1.0454179048538208\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.029525279998779 | KNN Loss: 4.998814582824707 | BCE Loss: 1.0307106971740723\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.059001445770264 | KNN Loss: 5.036451816558838 | BCE Loss: 1.0225497484207153\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.104270935058594 | KNN Loss: 5.022153377532959 | BCE Loss: 1.0821175575256348\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.0918731689453125 | KNN Loss: 5.030655860900879 | BCE Loss: 1.0612175464630127\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.052000999450684 | KNN Loss: 5.022739410400391 | BCE Loss: 1.029261827468872\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.085760116577148 | KNN Loss: 5.01847505569458 | BCE Loss: 1.0672848224639893\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.065649032592773 | KNN Loss: 5.0106072425842285 | BCE Loss: 1.0550419092178345\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 6.072781562805176 | KNN Loss: 5.032931804656982 | BCE Loss: 1.0398499965667725\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 6.0125555992126465 | KNN Loss: 5.005288124084473 | BCE Loss: 1.0072674751281738\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.088066577911377 | KNN Loss: 5.045989036560059 | BCE Loss: 1.0420774221420288\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 6.082952499389648 | KNN Loss: 5.068764686584473 | BCE Loss: 1.0141875743865967\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.055889129638672 | KNN Loss: 5.035420894622803 | BCE Loss: 1.0204683542251587\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.029663562774658 | KNN Loss: 5.004522323608398 | BCE Loss: 1.0251412391662598\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.052826404571533 | KNN Loss: 5.011626720428467 | BCE Loss: 1.0411995649337769\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.025524139404297 | KNN Loss: 5.0034685134887695 | BCE Loss: 1.0220553874969482\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.045287609100342 | KNN Loss: 5.015053749084473 | BCE Loss: 1.0302338600158691\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.069424152374268 | KNN Loss: 5.046650409698486 | BCE Loss: 1.0227738618850708\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.055096626281738 | KNN Loss: 5.03017520904541 | BCE Loss: 1.0249214172363281\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.017105579376221 | KNN Loss: 4.998730182647705 | BCE Loss: 1.018375277519226\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.030315399169922 | KNN Loss: 5.020058631896973 | BCE Loss: 1.0102565288543701\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 6.015414237976074 | KNN Loss: 4.992578506469727 | BCE Loss: 1.0228354930877686\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.097314834594727 | KNN Loss: 5.03275728225708 | BCE Loss: 1.0645573139190674\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.012730121612549 | KNN Loss: 5.024069309234619 | BCE Loss: 0.9886608719825745\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 6.03007698059082 | KNN Loss: 5.020381450653076 | BCE Loss: 1.0096957683563232\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.028557777404785 | KNN Loss: 5.018268585205078 | BCE Loss: 1.010289192199707\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.06544303894043 | KNN Loss: 5.032418727874756 | BCE Loss: 1.0330244302749634\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.033234596252441 | KNN Loss: 5.018874168395996 | BCE Loss: 1.0143601894378662\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.067780494689941 | KNN Loss: 5.013932704925537 | BCE Loss: 1.0538476705551147\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.031735420227051 | KNN Loss: 5.00512170791626 | BCE Loss: 1.0266139507293701\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.087111473083496 | KNN Loss: 5.063083648681641 | BCE Loss: 1.0240278244018555\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.05708122253418 | KNN Loss: 5.033044338226318 | BCE Loss: 1.0240366458892822\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.034895420074463 | KNN Loss: 5.015866279602051 | BCE Loss: 1.019029140472412\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.025409698486328 | KNN Loss: 5.0181121826171875 | BCE Loss: 1.0072972774505615\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.058246612548828 | KNN Loss: 5.020777225494385 | BCE Loss: 1.037469506263733\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.076416015625 | KNN Loss: 5.042434215545654 | BCE Loss: 1.0339816808700562\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.041825771331787 | KNN Loss: 5.01745080947876 | BCE Loss: 1.0243748426437378\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.076974868774414 | KNN Loss: 5.036007881164551 | BCE Loss: 1.0409669876098633\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.005478382110596 | KNN Loss: 4.996987819671631 | BCE Loss: 1.0084905624389648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.059895992279053 | KNN Loss: 5.015424728393555 | BCE Loss: 1.044471263885498\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.043234348297119 | KNN Loss: 5.009065628051758 | BCE Loss: 1.0341687202453613\n",
      "Epoch   279: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.080356597900391 | KNN Loss: 5.077766418457031 | BCE Loss: 1.0025901794433594\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.06008768081665 | KNN Loss: 5.022638320922852 | BCE Loss: 1.0374494791030884\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.027736186981201 | KNN Loss: 5.014758586883545 | BCE Loss: 1.0129776000976562\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.010735034942627 | KNN Loss: 5.001363754272461 | BCE Loss: 1.0093711614608765\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.073187351226807 | KNN Loss: 5.014804840087891 | BCE Loss: 1.058382511138916\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.077358722686768 | KNN Loss: 5.034072399139404 | BCE Loss: 1.0432863235473633\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.035873889923096 | KNN Loss: 5.020387649536133 | BCE Loss: 1.015486240386963\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.022911548614502 | KNN Loss: 5.006218433380127 | BCE Loss: 1.016693115234375\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.007701396942139 | KNN Loss: 5.003747463226318 | BCE Loss: 1.0039539337158203\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.0277886390686035 | KNN Loss: 5.007392406463623 | BCE Loss: 1.0203962326049805\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.061620235443115 | KNN Loss: 5.039178848266602 | BCE Loss: 1.0224413871765137\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.022605895996094 | KNN Loss: 4.989254474639893 | BCE Loss: 1.0333513021469116\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 5.990658760070801 | KNN Loss: 4.993818283081055 | BCE Loss: 0.9968404173851013\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.055307388305664 | KNN Loss: 5.038330554962158 | BCE Loss: 1.0169769525527954\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.061488151550293 | KNN Loss: 5.006991863250732 | BCE Loss: 1.0544962882995605\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.012540817260742 | KNN Loss: 5.022277355194092 | BCE Loss: 0.9902632236480713\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.049021244049072 | KNN Loss: 5.015749931335449 | BCE Loss: 1.0332714319229126\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.0606536865234375 | KNN Loss: 5.01275634765625 | BCE Loss: 1.0478971004486084\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.047908306121826 | KNN Loss: 5.013298034667969 | BCE Loss: 1.0346101522445679\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.055758476257324 | KNN Loss: 5.020505428314209 | BCE Loss: 1.0352530479431152\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 6.083759307861328 | KNN Loss: 5.030511379241943 | BCE Loss: 1.0532476902008057\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.036585807800293 | KNN Loss: 5.005706787109375 | BCE Loss: 1.030879259109497\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.0361175537109375 | KNN Loss: 5.024367332458496 | BCE Loss: 1.0117502212524414\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.0709404945373535 | KNN Loss: 5.004031658172607 | BCE Loss: 1.066908836364746\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.075898170471191 | KNN Loss: 5.030830383300781 | BCE Loss: 1.0450677871704102\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.032273769378662 | KNN Loss: 5.008895397186279 | BCE Loss: 1.0233782529830933\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.064970016479492 | KNN Loss: 5.040012836456299 | BCE Loss: 1.0249571800231934\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.026405334472656 | KNN Loss: 4.998433589935303 | BCE Loss: 1.0279719829559326\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.030740737915039 | KNN Loss: 5.017145156860352 | BCE Loss: 1.0135953426361084\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.056682109832764 | KNN Loss: 5.023058891296387 | BCE Loss: 1.0336233377456665\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.060354232788086 | KNN Loss: 5.024039268493652 | BCE Loss: 1.0363147258758545\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.018631935119629 | KNN Loss: 5.007567882537842 | BCE Loss: 1.011064052581787\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.112077713012695 | KNN Loss: 5.047010898590088 | BCE Loss: 1.065066933631897\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.0765838623046875 | KNN Loss: 5.021701335906982 | BCE Loss: 1.0548827648162842\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.074185371398926 | KNN Loss: 5.035585403442383 | BCE Loss: 1.038599967956543\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.041463851928711 | KNN Loss: 5.020386695861816 | BCE Loss: 1.0210771560668945\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.071634769439697 | KNN Loss: 5.0312604904174805 | BCE Loss: 1.0403742790222168\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.038787364959717 | KNN Loss: 5.009098052978516 | BCE Loss: 1.0296893119812012\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.0183424949646 | KNN Loss: 5.0197529792785645 | BCE Loss: 0.9985893964767456\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.035752296447754 | KNN Loss: 4.9990997314453125 | BCE Loss: 1.0366523265838623\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.068567276000977 | KNN Loss: 5.014739990234375 | BCE Loss: 1.0538275241851807\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.037162780761719 | KNN Loss: 4.999857425689697 | BCE Loss: 1.0373051166534424\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.046535968780518 | KNN Loss: 5.009169578552246 | BCE Loss: 1.037366271018982\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 6.047029495239258 | KNN Loss: 5.018399238586426 | BCE Loss: 1.0286304950714111\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.076244354248047 | KNN Loss: 5.028717994689941 | BCE Loss: 1.0475265979766846\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.059116363525391 | KNN Loss: 5.0038065910339355 | BCE Loss: 1.055309772491455\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.013949394226074 | KNN Loss: 5.003213405609131 | BCE Loss: 1.0107362270355225\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.041365623474121 | KNN Loss: 5.00429630279541 | BCE Loss: 1.037069320678711\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.050428867340088 | KNN Loss: 5.007437705993652 | BCE Loss: 1.0429911613464355\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.056562423706055 | KNN Loss: 5.009183406829834 | BCE Loss: 1.0473792552947998\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.0363006591796875 | KNN Loss: 5.0164947509765625 | BCE Loss: 1.019806146621704\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 6.049518585205078 | KNN Loss: 5.0241780281066895 | BCE Loss: 1.0253403186798096\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.064354419708252 | KNN Loss: 5.003819942474365 | BCE Loss: 1.0605344772338867\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.056921005249023 | KNN Loss: 5.0222272872924805 | BCE Loss: 1.034693717956543\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 5.994723796844482 | KNN Loss: 4.998632907867432 | BCE Loss: 0.9960908889770508\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.051967620849609 | KNN Loss: 5.02603006362915 | BCE Loss: 1.025937795639038\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.065465927124023 | KNN Loss: 5.0099616050720215 | BCE Loss: 1.055504560470581\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 6.0458574295043945 | KNN Loss: 5.028371810913086 | BCE Loss: 1.0174853801727295\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.055652141571045 | KNN Loss: 5.039638996124268 | BCE Loss: 1.0160131454467773\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.019604206085205 | KNN Loss: 5.0097222328186035 | BCE Loss: 1.0098819732666016\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.047737121582031 | KNN Loss: 5.015654563903809 | BCE Loss: 1.032082438468933\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.03201150894165 | KNN Loss: 5.029513835906982 | BCE Loss: 1.0024975538253784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.102818965911865 | KNN Loss: 5.045796871185303 | BCE Loss: 1.0570220947265625\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.023585319519043 | KNN Loss: 5.017012119293213 | BCE Loss: 1.0065734386444092\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.042482376098633 | KNN Loss: 5.0027384757995605 | BCE Loss: 1.0397436618804932\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 6.041851043701172 | KNN Loss: 5.0196146965026855 | BCE Loss: 1.0222365856170654\n",
      "Epoch   290: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.042211055755615 | KNN Loss: 5.008718490600586 | BCE Loss: 1.0334926843643188\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.054404258728027 | KNN Loss: 5.028841018676758 | BCE Loss: 1.0255634784698486\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.045506477355957 | KNN Loss: 5.015398025512695 | BCE Loss: 1.0301082134246826\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.074329376220703 | KNN Loss: 5.047173023223877 | BCE Loss: 1.0271565914154053\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.064871788024902 | KNN Loss: 5.023103713989258 | BCE Loss: 1.0417678356170654\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.019288063049316 | KNN Loss: 5.007492542266846 | BCE Loss: 1.0117952823638916\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.110579013824463 | KNN Loss: 5.04812479019165 | BCE Loss: 1.0624542236328125\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.025887966156006 | KNN Loss: 5.014899730682373 | BCE Loss: 1.0109882354736328\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.06744909286499 | KNN Loss: 5.041655540466309 | BCE Loss: 1.0257935523986816\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.051743030548096 | KNN Loss: 5.022996425628662 | BCE Loss: 1.0287466049194336\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 6.052803993225098 | KNN Loss: 5.021444320678711 | BCE Loss: 1.0313597917556763\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.0391082763671875 | KNN Loss: 5.045592784881592 | BCE Loss: 0.9935153722763062\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.093559741973877 | KNN Loss: 5.040220260620117 | BCE Loss: 1.0533394813537598\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.0192766189575195 | KNN Loss: 5.000032901763916 | BCE Loss: 1.019243597984314\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.087070941925049 | KNN Loss: 5.039976596832275 | BCE Loss: 1.0470942258834839\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.070977210998535 | KNN Loss: 5.0565314292907715 | BCE Loss: 1.0144455432891846\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.019527435302734 | KNN Loss: 5.003913879394531 | BCE Loss: 1.0156135559082031\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.051421165466309 | KNN Loss: 5.0069169998168945 | BCE Loss: 1.044503927230835\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.072486877441406 | KNN Loss: 5.014467239379883 | BCE Loss: 1.0580195188522339\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.065767765045166 | KNN Loss: 5.044770240783691 | BCE Loss: 1.020997405052185\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.095609664916992 | KNN Loss: 5.020909309387207 | BCE Loss: 1.074700117111206\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.087934970855713 | KNN Loss: 5.0388407707214355 | BCE Loss: 1.049094319343567\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.018092155456543 | KNN Loss: 5.0253586769104 | BCE Loss: 0.9927332401275635\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.0132269859313965 | KNN Loss: 5.009649276733398 | BCE Loss: 1.0035778284072876\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.054218769073486 | KNN Loss: 5.012592792510986 | BCE Loss: 1.0416258573532104\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.075139999389648 | KNN Loss: 5.047227382659912 | BCE Loss: 1.0279128551483154\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.065732002258301 | KNN Loss: 5.022439479827881 | BCE Loss: 1.04329252243042\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 6.072319030761719 | KNN Loss: 5.028665542602539 | BCE Loss: 1.0436532497406006\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.024357795715332 | KNN Loss: 5.012379169464111 | BCE Loss: 1.0119787454605103\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.012887477874756 | KNN Loss: 5.015069484710693 | BCE Loss: 0.9978181719779968\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.108933925628662 | KNN Loss: 5.061251640319824 | BCE Loss: 1.0476824045181274\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.029467582702637 | KNN Loss: 5.029712200164795 | BCE Loss: 0.9997555613517761\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.089232444763184 | KNN Loss: 5.022446632385254 | BCE Loss: 1.0667858123779297\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.040470123291016 | KNN Loss: 5.033385753631592 | BCE Loss: 1.0070844888687134\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.059294700622559 | KNN Loss: 5.007602691650391 | BCE Loss: 1.0516921281814575\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.121284484863281 | KNN Loss: 5.050648212432861 | BCE Loss: 1.070636510848999\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.044890403747559 | KNN Loss: 5.016448020935059 | BCE Loss: 1.028442144393921\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.088115692138672 | KNN Loss: 5.0586042404174805 | BCE Loss: 1.0295113325119019\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.135964393615723 | KNN Loss: 5.074105262756348 | BCE Loss: 1.061858892440796\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.111667633056641 | KNN Loss: 5.0605363845825195 | BCE Loss: 1.0511313676834106\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.021305561065674 | KNN Loss: 5.012998580932617 | BCE Loss: 1.0083070993423462\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 6.019296169281006 | KNN Loss: 5.004317760467529 | BCE Loss: 1.0149785280227661\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.0455803871154785 | KNN Loss: 5.027395248413086 | BCE Loss: 1.0181851387023926\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.042536735534668 | KNN Loss: 5.015659809112549 | BCE Loss: 1.02687668800354\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.028619766235352 | KNN Loss: 5.0231709480285645 | BCE Loss: 1.005448818206787\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.087310791015625 | KNN Loss: 5.029754638671875 | BCE Loss: 1.0575560331344604\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.020977020263672 | KNN Loss: 5.00667142868042 | BCE Loss: 1.0143053531646729\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.060649394989014 | KNN Loss: 5.028323173522949 | BCE Loss: 1.032326340675354\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.06916618347168 | KNN Loss: 5.039727687835693 | BCE Loss: 1.0294387340545654\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.041186332702637 | KNN Loss: 5.03035831451416 | BCE Loss: 1.0108282566070557\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.026403427124023 | KNN Loss: 5.011746406555176 | BCE Loss: 1.0146572589874268\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.04461145401001 | KNN Loss: 5.031166076660156 | BCE Loss: 1.0134453773498535\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.054196357727051 | KNN Loss: 5.016106605529785 | BCE Loss: 1.0380895137786865\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.030196666717529 | KNN Loss: 5.001287460327148 | BCE Loss: 1.0289092063903809\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.025598049163818 | KNN Loss: 5.005827903747559 | BCE Loss: 1.0197701454162598\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.100233554840088 | KNN Loss: 5.069855213165283 | BCE Loss: 1.0303783416748047\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.02454948425293 | KNN Loss: 5.0032196044921875 | BCE Loss: 1.021329641342163\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.095844268798828 | KNN Loss: 5.035069465637207 | BCE Loss: 1.0607746839523315\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.032984256744385 | KNN Loss: 5.019059181213379 | BCE Loss: 1.0139249563217163\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.060919761657715 | KNN Loss: 5.015013694763184 | BCE Loss: 1.0459058284759521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.036981105804443 | KNN Loss: 5.023275852203369 | BCE Loss: 1.0137051343917847\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.062219142913818 | KNN Loss: 5.021985054016113 | BCE Loss: 1.040234088897705\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.0905866622924805 | KNN Loss: 5.0359578132629395 | BCE Loss: 1.054628610610962\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.052672386169434 | KNN Loss: 5.053261756896973 | BCE Loss: 0.9994104504585266\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.05092716217041 | KNN Loss: 5.018309593200684 | BCE Loss: 1.032617449760437\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.086071968078613 | KNN Loss: 5.0401153564453125 | BCE Loss: 1.0459568500518799\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.045450687408447 | KNN Loss: 5.013944625854492 | BCE Loss: 1.031506061553955\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.048126220703125 | KNN Loss: 5.019384860992432 | BCE Loss: 1.0287412405014038\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.066461563110352 | KNN Loss: 5.02886438369751 | BCE Loss: 1.037597417831421\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.031203269958496 | KNN Loss: 5.002529621124268 | BCE Loss: 1.028673768043518\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.077434539794922 | KNN Loss: 5.026083469390869 | BCE Loss: 1.0513511896133423\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.0253190994262695 | KNN Loss: 5.0141282081604 | BCE Loss: 1.01119065284729\n",
      "Epoch   302: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 6.07440185546875 | KNN Loss: 5.0068159103393555 | BCE Loss: 1.0675861835479736\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.013733863830566 | KNN Loss: 5.002475738525391 | BCE Loss: 1.0112581253051758\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.036709785461426 | KNN Loss: 5.014036655426025 | BCE Loss: 1.02267324924469\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.027768135070801 | KNN Loss: 5.0235795974731445 | BCE Loss: 1.0041882991790771\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.076437473297119 | KNN Loss: 5.038492202758789 | BCE Loss: 1.0379451513290405\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.071012020111084 | KNN Loss: 5.035245418548584 | BCE Loss: 1.0357666015625\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.056724548339844 | KNN Loss: 5.033827304840088 | BCE Loss: 1.0228970050811768\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.020272731781006 | KNN Loss: 5.004128456115723 | BCE Loss: 1.0161443948745728\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.0408453941345215 | KNN Loss: 5.0167059898376465 | BCE Loss: 1.024139404296875\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.063942909240723 | KNN Loss: 5.011059761047363 | BCE Loss: 1.0528833866119385\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.059478759765625 | KNN Loss: 5.0251641273498535 | BCE Loss: 1.0343146324157715\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.064971446990967 | KNN Loss: 5.042863845825195 | BCE Loss: 1.0221076011657715\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.024240016937256 | KNN Loss: 5.034214019775391 | BCE Loss: 0.9900259375572205\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.05018424987793 | KNN Loss: 5.022712707519531 | BCE Loss: 1.0274717807769775\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.041947364807129 | KNN Loss: 5.0137200355529785 | BCE Loss: 1.0282273292541504\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.034825325012207 | KNN Loss: 5.015380382537842 | BCE Loss: 1.0194447040557861\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.044190883636475 | KNN Loss: 5.032702922821045 | BCE Loss: 1.0114879608154297\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.073862552642822 | KNN Loss: 5.018253326416016 | BCE Loss: 1.055609107017517\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.035199165344238 | KNN Loss: 5.01816463470459 | BCE Loss: 1.0170345306396484\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.011901378631592 | KNN Loss: 4.999789237976074 | BCE Loss: 1.0121122598648071\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.0721116065979 | KNN Loss: 5.033445835113525 | BCE Loss: 1.0386658906936646\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.067348003387451 | KNN Loss: 5.03102445602417 | BCE Loss: 1.0363234281539917\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.028343200683594 | KNN Loss: 5.008307933807373 | BCE Loss: 1.0200355052947998\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.042128086090088 | KNN Loss: 5.014705181121826 | BCE Loss: 1.0274229049682617\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.053956508636475 | KNN Loss: 5.035833358764648 | BCE Loss: 1.0181230306625366\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.043735980987549 | KNN Loss: 5.024149417877197 | BCE Loss: 1.019586443901062\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.0479326248168945 | KNN Loss: 5.030824661254883 | BCE Loss: 1.0171079635620117\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 6.077847480773926 | KNN Loss: 5.013274669647217 | BCE Loss: 1.064572811126709\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.053034782409668 | KNN Loss: 5.0187201499938965 | BCE Loss: 1.0343148708343506\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.0559539794921875 | KNN Loss: 5.04453706741333 | BCE Loss: 1.0114167928695679\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.067037105560303 | KNN Loss: 5.012750148773193 | BCE Loss: 1.0542869567871094\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.073441505432129 | KNN Loss: 5.050731182098389 | BCE Loss: 1.0227105617523193\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.054771900177002 | KNN Loss: 5.0098114013671875 | BCE Loss: 1.044960379600525\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.014939308166504 | KNN Loss: 5.001875400543213 | BCE Loss: 1.013063907623291\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.048717498779297 | KNN Loss: 5.027247905731201 | BCE Loss: 1.0214693546295166\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.042865753173828 | KNN Loss: 5.012171268463135 | BCE Loss: 1.0306947231292725\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 6.083430290222168 | KNN Loss: 5.069971561431885 | BCE Loss: 1.0134589672088623\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.047683238983154 | KNN Loss: 5.017643928527832 | BCE Loss: 1.0300393104553223\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.043306350708008 | KNN Loss: 5.023772716522217 | BCE Loss: 1.019533395767212\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.053869724273682 | KNN Loss: 4.993379592895508 | BCE Loss: 1.0604901313781738\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.043780326843262 | KNN Loss: 5.034783363342285 | BCE Loss: 1.0089967250823975\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.020828723907471 | KNN Loss: 5.028060436248779 | BCE Loss: 0.9927682876586914\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.039841651916504 | KNN Loss: 5.017117500305176 | BCE Loss: 1.0227243900299072\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.037879943847656 | KNN Loss: 5.022788047790527 | BCE Loss: 1.0150917768478394\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.012174129486084 | KNN Loss: 5.008536338806152 | BCE Loss: 1.003637671470642\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 6.092486381530762 | KNN Loss: 5.0332770347595215 | BCE Loss: 1.0592094659805298\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.052298545837402 | KNN Loss: 5.017378807067871 | BCE Loss: 1.0349197387695312\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.04959774017334 | KNN Loss: 5.020813465118408 | BCE Loss: 1.0287845134735107\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.053538799285889 | KNN Loss: 5.013792514801025 | BCE Loss: 1.0397464036941528\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.04603385925293 | KNN Loss: 5.026123523712158 | BCE Loss: 1.019910216331482\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.061511039733887 | KNN Loss: 5.012678146362305 | BCE Loss: 1.0488331317901611\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.0680952072143555 | KNN Loss: 5.027962684631348 | BCE Loss: 1.0401326417922974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.025239944458008 | KNN Loss: 5.018309116363525 | BCE Loss: 1.0069305896759033\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.069162845611572 | KNN Loss: 5.013923645019531 | BCE Loss: 1.055239200592041\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.052623271942139 | KNN Loss: 5.007540225982666 | BCE Loss: 1.0450830459594727\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.046937942504883 | KNN Loss: 5.0088791847229 | BCE Loss: 1.0380585193634033\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.105905055999756 | KNN Loss: 5.031917095184326 | BCE Loss: 1.0739879608154297\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.012267589569092 | KNN Loss: 5.016502857208252 | BCE Loss: 0.9957647919654846\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.043435573577881 | KNN Loss: 5.0229902267456055 | BCE Loss: 1.020445466041565\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.070878028869629 | KNN Loss: 5.058707237243652 | BCE Loss: 1.0121705532073975\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.044066905975342 | KNN Loss: 5.00289249420166 | BCE Loss: 1.0411745309829712\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.031013488769531 | KNN Loss: 5.0113139152526855 | BCE Loss: 1.0196996927261353\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.070019721984863 | KNN Loss: 5.022110462188721 | BCE Loss: 1.0479093790054321\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 6.08098840713501 | KNN Loss: 5.053665637969971 | BCE Loss: 1.0273228883743286\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.042635440826416 | KNN Loss: 5.017575740814209 | BCE Loss: 1.0250598192214966\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.03598690032959 | KNN Loss: 5.003549098968506 | BCE Loss: 1.0324375629425049\n",
      "Epoch   313: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.027447700500488 | KNN Loss: 5.017852783203125 | BCE Loss: 1.0095951557159424\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.061573505401611 | KNN Loss: 5.026549816131592 | BCE Loss: 1.0350236892700195\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.04942512512207 | KNN Loss: 5.007948875427246 | BCE Loss: 1.0414763689041138\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.075387001037598 | KNN Loss: 5.028931617736816 | BCE Loss: 1.0464555025100708\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.058907985687256 | KNN Loss: 5.045913219451904 | BCE Loss: 1.0129947662353516\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.0531768798828125 | KNN Loss: 5.025282382965088 | BCE Loss: 1.0278942584991455\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 6.094829559326172 | KNN Loss: 5.04766845703125 | BCE Loss: 1.0471612215042114\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.0519700050354 | KNN Loss: 5.039328575134277 | BCE Loss: 1.012641429901123\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.01784610748291 | KNN Loss: 5.021339416503906 | BCE Loss: 0.9965068101882935\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.0386457443237305 | KNN Loss: 5.008639335632324 | BCE Loss: 1.0300061702728271\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.028270244598389 | KNN Loss: 5.02752161026001 | BCE Loss: 1.0007487535476685\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.028249740600586 | KNN Loss: 5.000094890594482 | BCE Loss: 1.028154969215393\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.060606002807617 | KNN Loss: 5.020242214202881 | BCE Loss: 1.0403636693954468\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.035256385803223 | KNN Loss: 5.014928817749023 | BCE Loss: 1.0203278064727783\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.043639183044434 | KNN Loss: 5.024011611938477 | BCE Loss: 1.0196274518966675\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 6.027106285095215 | KNN Loss: 5.004883766174316 | BCE Loss: 1.022222638130188\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.0574235916137695 | KNN Loss: 5.038798809051514 | BCE Loss: 1.018625020980835\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.049027919769287 | KNN Loss: 5.015681266784668 | BCE Loss: 1.0333466529846191\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.109131813049316 | KNN Loss: 5.080692768096924 | BCE Loss: 1.0284392833709717\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.0871262550354 | KNN Loss: 5.057374477386475 | BCE Loss: 1.0297517776489258\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.089860916137695 | KNN Loss: 5.048546314239502 | BCE Loss: 1.0413148403167725\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.0456647872924805 | KNN Loss: 5.012157440185547 | BCE Loss: 1.0335073471069336\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 6.0321455001831055 | KNN Loss: 5.019795894622803 | BCE Loss: 1.0123493671417236\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.0537109375 | KNN Loss: 5.020545959472656 | BCE Loss: 1.0331652164459229\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.050029754638672 | KNN Loss: 5.032943248748779 | BCE Loss: 1.0170865058898926\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.039912223815918 | KNN Loss: 5.033507347106934 | BCE Loss: 1.006404995918274\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.084776878356934 | KNN Loss: 5.007493495941162 | BCE Loss: 1.0772833824157715\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.000478267669678 | KNN Loss: 5.000014781951904 | BCE Loss: 1.0004634857177734\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 6.0650715827941895 | KNN Loss: 5.022642135620117 | BCE Loss: 1.0424293279647827\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.070314884185791 | KNN Loss: 5.02963399887085 | BCE Loss: 1.0406807661056519\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.051080703735352 | KNN Loss: 5.016254901885986 | BCE Loss: 1.0348259210586548\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.107757568359375 | KNN Loss: 5.076343059539795 | BCE Loss: 1.03141450881958\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.057873725891113 | KNN Loss: 5.01713752746582 | BCE Loss: 1.040736436843872\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.051662445068359 | KNN Loss: 5.034546375274658 | BCE Loss: 1.0171163082122803\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.072315692901611 | KNN Loss: 5.022807598114014 | BCE Loss: 1.049507975578308\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 6.0356316566467285 | KNN Loss: 5.026154041290283 | BCE Loss: 1.0094776153564453\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.070153713226318 | KNN Loss: 5.04015588760376 | BCE Loss: 1.0299978256225586\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.037807464599609 | KNN Loss: 5.019278049468994 | BCE Loss: 1.0185291767120361\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.04555082321167 | KNN Loss: 4.999724864959717 | BCE Loss: 1.0458259582519531\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.0686869621276855 | KNN Loss: 5.006175518035889 | BCE Loss: 1.0625114440917969\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.068382263183594 | KNN Loss: 5.027460098266602 | BCE Loss: 1.0409221649169922\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.036293029785156 | KNN Loss: 5.001648902893066 | BCE Loss: 1.0346442461013794\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.089815139770508 | KNN Loss: 5.052317142486572 | BCE Loss: 1.037498116493225\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.0400848388671875 | KNN Loss: 5.040058612823486 | BCE Loss: 1.0000264644622803\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.049461841583252 | KNN Loss: 5.014319896697998 | BCE Loss: 1.035141944885254\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.047154426574707 | KNN Loss: 5.029546737670898 | BCE Loss: 1.0176074504852295\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.036104679107666 | KNN Loss: 5.040327548980713 | BCE Loss: 0.9957772493362427\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.037811756134033 | KNN Loss: 5.02260684967041 | BCE Loss: 1.015204906463623\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.0625505447387695 | KNN Loss: 5.033786296844482 | BCE Loss: 1.028764247894287\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.126852035522461 | KNN Loss: 5.093165397644043 | BCE Loss: 1.033686637878418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.049729824066162 | KNN Loss: 5.013769149780273 | BCE Loss: 1.0359607934951782\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.026420593261719 | KNN Loss: 5.003382205963135 | BCE Loss: 1.0230382680892944\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.021750450134277 | KNN Loss: 5.003814220428467 | BCE Loss: 1.0179364681243896\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 6.07747745513916 | KNN Loss: 5.028421401977539 | BCE Loss: 1.049055814743042\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.00756311416626 | KNN Loss: 5.012650489807129 | BCE Loss: 0.9949126243591309\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.00161600112915 | KNN Loss: 5.0099663734436035 | BCE Loss: 0.9916495084762573\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.0433759689331055 | KNN Loss: 5.0170512199401855 | BCE Loss: 1.02632474899292\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.0418853759765625 | KNN Loss: 5.008951187133789 | BCE Loss: 1.0329339504241943\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 5.999304294586182 | KNN Loss: 4.9955949783325195 | BCE Loss: 1.0037091970443726\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.038366794586182 | KNN Loss: 5.011818885803223 | BCE Loss: 1.026547908782959\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.0414557456970215 | KNN Loss: 5.004242897033691 | BCE Loss: 1.0372129678726196\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.061627388000488 | KNN Loss: 5.035081386566162 | BCE Loss: 1.0265458822250366\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.072811126708984 | KNN Loss: 5.060746669769287 | BCE Loss: 1.0120646953582764\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.006964206695557 | KNN Loss: 5.019890785217285 | BCE Loss: 0.9870736002922058\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.036017417907715 | KNN Loss: 5.017366886138916 | BCE Loss: 1.0186502933502197\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 6.14544153213501 | KNN Loss: 5.091526031494141 | BCE Loss: 1.0539156198501587\n",
      "Epoch   324: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.046372890472412 | KNN Loss: 4.999756336212158 | BCE Loss: 1.046616554260254\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 6.070623874664307 | KNN Loss: 5.0118327140808105 | BCE Loss: 1.0587912797927856\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.126018524169922 | KNN Loss: 5.078532695770264 | BCE Loss: 1.047485589981079\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.033027648925781 | KNN Loss: 5.007419109344482 | BCE Loss: 1.0256085395812988\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.062534332275391 | KNN Loss: 5.0223894119262695 | BCE Loss: 1.0401448011398315\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.081514358520508 | KNN Loss: 5.0531439781188965 | BCE Loss: 1.0283701419830322\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.069737434387207 | KNN Loss: 5.0317254066467285 | BCE Loss: 1.0380122661590576\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.075965881347656 | KNN Loss: 5.01141881942749 | BCE Loss: 1.064547061920166\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.00553035736084 | KNN Loss: 5.0027289390563965 | BCE Loss: 1.002801537513733\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.042879104614258 | KNN Loss: 5.019902229309082 | BCE Loss: 1.0229768753051758\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.056703090667725 | KNN Loss: 5.009912014007568 | BCE Loss: 1.0467911958694458\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.009418487548828 | KNN Loss: 4.997000217437744 | BCE Loss: 1.012418270111084\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.032948970794678 | KNN Loss: 5.017574787139893 | BCE Loss: 1.0153743028640747\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.059697151184082 | KNN Loss: 5.029928684234619 | BCE Loss: 1.029768705368042\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.040548801422119 | KNN Loss: 5.008519649505615 | BCE Loss: 1.032029151916504\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.0159993171691895 | KNN Loss: 5.012012481689453 | BCE Loss: 1.0039869546890259\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.055938720703125 | KNN Loss: 5.030123233795166 | BCE Loss: 1.025815486907959\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.036801338195801 | KNN Loss: 5.011068344116211 | BCE Loss: 1.0257327556610107\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.004575729370117 | KNN Loss: 5.005134582519531 | BCE Loss: 0.9994412660598755\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.09824800491333 | KNN Loss: 5.067761421203613 | BCE Loss: 1.0304865837097168\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.043858528137207 | KNN Loss: 5.014740467071533 | BCE Loss: 1.0291178226470947\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.113725662231445 | KNN Loss: 5.052059650421143 | BCE Loss: 1.0616662502288818\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.025655269622803 | KNN Loss: 5.030121326446533 | BCE Loss: 0.9955340623855591\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.062760353088379 | KNN Loss: 5.031172752380371 | BCE Loss: 1.031587839126587\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.0949625968933105 | KNN Loss: 5.063107013702393 | BCE Loss: 1.0318557024002075\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.042557716369629 | KNN Loss: 5.012404918670654 | BCE Loss: 1.0301529169082642\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.02929162979126 | KNN Loss: 4.998110771179199 | BCE Loss: 1.03118097782135\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.083099365234375 | KNN Loss: 5.052859306335449 | BCE Loss: 1.0302401781082153\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 6.035325527191162 | KNN Loss: 5.0252461433410645 | BCE Loss: 1.0100793838500977\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 6.045645236968994 | KNN Loss: 5.010076522827148 | BCE Loss: 1.0355685949325562\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.043161869049072 | KNN Loss: 5.027719497680664 | BCE Loss: 1.0154423713684082\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.05457878112793 | KNN Loss: 5.026736259460449 | BCE Loss: 1.0278425216674805\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.016495704650879 | KNN Loss: 5.018899440765381 | BCE Loss: 0.9975961446762085\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.0375566482543945 | KNN Loss: 5.039651393890381 | BCE Loss: 0.9979052543640137\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.079200744628906 | KNN Loss: 5.018968105316162 | BCE Loss: 1.0602328777313232\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.052776336669922 | KNN Loss: 5.034230709075928 | BCE Loss: 1.0185458660125732\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.05214262008667 | KNN Loss: 5.028036117553711 | BCE Loss: 1.0241066217422485\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.072385311126709 | KNN Loss: 5.024572372436523 | BCE Loss: 1.0478129386901855\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.091211795806885 | KNN Loss: 5.057060718536377 | BCE Loss: 1.0341510772705078\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.045061111450195 | KNN Loss: 5.044749736785889 | BCE Loss: 1.0003111362457275\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.046177864074707 | KNN Loss: 5.001716613769531 | BCE Loss: 1.0444614887237549\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.067970275878906 | KNN Loss: 5.026516437530518 | BCE Loss: 1.0414540767669678\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.0484089851379395 | KNN Loss: 5.048917770385742 | BCE Loss: 0.9994910359382629\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.036762237548828 | KNN Loss: 5.014769077301025 | BCE Loss: 1.0219933986663818\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.091329574584961 | KNN Loss: 5.0567402839660645 | BCE Loss: 1.0345890522003174\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.037003517150879 | KNN Loss: 5.031452178955078 | BCE Loss: 1.0055515766143799\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.072051525115967 | KNN Loss: 5.036065578460693 | BCE Loss: 1.035986065864563\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.051937103271484 | KNN Loss: 5.012433052062988 | BCE Loss: 1.0395039319992065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.071387767791748 | KNN Loss: 5.019378185272217 | BCE Loss: 1.0520094633102417\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.028693675994873 | KNN Loss: 4.997572898864746 | BCE Loss: 1.031120777130127\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.041281700134277 | KNN Loss: 5.037398338317871 | BCE Loss: 1.0038833618164062\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.046102046966553 | KNN Loss: 5.016395568847656 | BCE Loss: 1.0297064781188965\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.025871753692627 | KNN Loss: 5.012014865875244 | BCE Loss: 1.0138567686080933\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.04915714263916 | KNN Loss: 5.024508476257324 | BCE Loss: 1.0246484279632568\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.047740936279297 | KNN Loss: 5.001276016235352 | BCE Loss: 1.0464651584625244\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.0127716064453125 | KNN Loss: 4.994142055511475 | BCE Loss: 1.018629550933838\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.107542991638184 | KNN Loss: 5.042057514190674 | BCE Loss: 1.0654855966567993\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.054754734039307 | KNN Loss: 5.024099826812744 | BCE Loss: 1.030654788017273\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.004961013793945 | KNN Loss: 5.00670862197876 | BCE Loss: 0.9982521533966064\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.025688171386719 | KNN Loss: 5.00901985168457 | BCE Loss: 1.0166685581207275\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.047210693359375 | KNN Loss: 5.01035737991333 | BCE Loss: 1.0368531942367554\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.050999641418457 | KNN Loss: 5.026730060577393 | BCE Loss: 1.0242693424224854\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.080705642700195 | KNN Loss: 5.025048732757568 | BCE Loss: 1.055656909942627\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.0267014503479 | KNN Loss: 5.014566898345947 | BCE Loss: 1.0121345520019531\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.062887668609619 | KNN Loss: 5.03218412399292 | BCE Loss: 1.0307034254074097\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.058279514312744 | KNN Loss: 5.034213542938232 | BCE Loss: 1.0240660905838013\n",
      "Epoch   335: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.048798561096191 | KNN Loss: 5.026066780090332 | BCE Loss: 1.022731900215149\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.045741081237793 | KNN Loss: 5.045697212219238 | BCE Loss: 1.0000441074371338\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.058757781982422 | KNN Loss: 5.043405055999756 | BCE Loss: 1.0153529644012451\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.077846050262451 | KNN Loss: 5.009679794311523 | BCE Loss: 1.0681662559509277\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.051772117614746 | KNN Loss: 5.014404296875 | BCE Loss: 1.037367582321167\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.093633651733398 | KNN Loss: 5.041413307189941 | BCE Loss: 1.0522205829620361\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.044781684875488 | KNN Loss: 5.008561611175537 | BCE Loss: 1.0362199544906616\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.049834728240967 | KNN Loss: 5.04321813583374 | BCE Loss: 1.0066165924072266\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.0308074951171875 | KNN Loss: 4.992635726928711 | BCE Loss: 1.0381720066070557\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.0442609786987305 | KNN Loss: 5.012207984924316 | BCE Loss: 1.0320531129837036\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.028719902038574 | KNN Loss: 5.001203536987305 | BCE Loss: 1.0275163650512695\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.068330764770508 | KNN Loss: 5.011507034301758 | BCE Loss: 1.056823492050171\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.074806213378906 | KNN Loss: 5.050811767578125 | BCE Loss: 1.0239946842193604\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.050399303436279 | KNN Loss: 5.041229248046875 | BCE Loss: 1.0091700553894043\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.039079666137695 | KNN Loss: 5.017033100128174 | BCE Loss: 1.022046446800232\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 6.047593116760254 | KNN Loss: 5.027466297149658 | BCE Loss: 1.0201265811920166\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.063623428344727 | KNN Loss: 5.009006500244141 | BCE Loss: 1.054617166519165\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.039805889129639 | KNN Loss: 5.030732154846191 | BCE Loss: 1.0090736150741577\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.032458782196045 | KNN Loss: 5.029972076416016 | BCE Loss: 1.0024865865707397\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.062507629394531 | KNN Loss: 5.048474311828613 | BCE Loss: 1.014033317565918\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.03872537612915 | KNN Loss: 5.016585826873779 | BCE Loss: 1.0221396684646606\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.057422637939453 | KNN Loss: 5.016900539398193 | BCE Loss: 1.0405222177505493\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 6.025485992431641 | KNN Loss: 5.012266635894775 | BCE Loss: 1.0132191181182861\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 6.041808605194092 | KNN Loss: 5.0175886154174805 | BCE Loss: 1.0242198705673218\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.0660400390625 | KNN Loss: 5.050931930541992 | BCE Loss: 1.0151079893112183\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.079126834869385 | KNN Loss: 5.022706031799316 | BCE Loss: 1.0564208030700684\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.032374382019043 | KNN Loss: 5.005075454711914 | BCE Loss: 1.0272988080978394\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.055863380432129 | KNN Loss: 5.037696361541748 | BCE Loss: 1.0181667804718018\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.039156913757324 | KNN Loss: 5.022947311401367 | BCE Loss: 1.0162097215652466\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.020340442657471 | KNN Loss: 5.012333869934082 | BCE Loss: 1.0080065727233887\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.042414665222168 | KNN Loss: 5.024189472198486 | BCE Loss: 1.0182249546051025\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 6.028319358825684 | KNN Loss: 5.011401653289795 | BCE Loss: 1.0169179439544678\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.08076286315918 | KNN Loss: 5.038576602935791 | BCE Loss: 1.0421860218048096\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.023082256317139 | KNN Loss: 5.00599479675293 | BCE Loss: 1.0170873403549194\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.04049015045166 | KNN Loss: 5.022247314453125 | BCE Loss: 1.0182428359985352\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.028397560119629 | KNN Loss: 5.013246536254883 | BCE Loss: 1.0151512622833252\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.024706840515137 | KNN Loss: 5.003787517547607 | BCE Loss: 1.0209195613861084\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.020440101623535 | KNN Loss: 5.0027055740356445 | BCE Loss: 1.0177346467971802\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.102806091308594 | KNN Loss: 5.042683124542236 | BCE Loss: 1.0601232051849365\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.000248908996582 | KNN Loss: 4.989625930786133 | BCE Loss: 1.0106228590011597\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.070989608764648 | KNN Loss: 5.037417411804199 | BCE Loss: 1.0335719585418701\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.074270248413086 | KNN Loss: 5.004042148590088 | BCE Loss: 1.070227861404419\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.030674934387207 | KNN Loss: 5.006056308746338 | BCE Loss: 1.0246187448501587\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.129167556762695 | KNN Loss: 5.061956882476807 | BCE Loss: 1.0672104358673096\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.025997638702393 | KNN Loss: 4.99524450302124 | BCE Loss: 1.0307531356811523\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.036126613616943 | KNN Loss: 5.017806053161621 | BCE Loss: 1.0183204412460327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.048254013061523 | KNN Loss: 5.027513027191162 | BCE Loss: 1.0207409858703613\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.025027751922607 | KNN Loss: 5.022772789001465 | BCE Loss: 1.0022549629211426\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.032893180847168 | KNN Loss: 5.025766372680664 | BCE Loss: 1.007127046585083\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.0258469581604 | KNN Loss: 5.015528678894043 | BCE Loss: 1.010318398475647\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.054093360900879 | KNN Loss: 5.032325267791748 | BCE Loss: 1.0217678546905518\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.032695770263672 | KNN Loss: 5.011592864990234 | BCE Loss: 1.0211031436920166\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.042458534240723 | KNN Loss: 5.025774002075195 | BCE Loss: 1.0166842937469482\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.083264350891113 | KNN Loss: 5.021119117736816 | BCE Loss: 1.0621452331542969\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.041421890258789 | KNN Loss: 5.011429309844971 | BCE Loss: 1.0299925804138184\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.107577800750732 | KNN Loss: 5.060722351074219 | BCE Loss: 1.0468553304672241\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.076041221618652 | KNN Loss: 5.026497840881348 | BCE Loss: 1.0495433807373047\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.066440582275391 | KNN Loss: 5.029392242431641 | BCE Loss: 1.037048578262329\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 5.999200820922852 | KNN Loss: 5.002486705780029 | BCE Loss: 0.9967139959335327\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.050088882446289 | KNN Loss: 5.016936779022217 | BCE Loss: 1.0331518650054932\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.031348705291748 | KNN Loss: 5.002933025360107 | BCE Loss: 1.0284156799316406\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.03823184967041 | KNN Loss: 5.005031585693359 | BCE Loss: 1.0332001447677612\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.068343162536621 | KNN Loss: 5.029643535614014 | BCE Loss: 1.0386993885040283\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.085450172424316 | KNN Loss: 5.035137176513672 | BCE Loss: 1.0503129959106445\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.057559490203857 | KNN Loss: 5.024886131286621 | BCE Loss: 1.0326733589172363\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.024308681488037 | KNN Loss: 5.018478870391846 | BCE Loss: 1.0058298110961914\n",
      "Epoch   346: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.044095993041992 | KNN Loss: 5.015532493591309 | BCE Loss: 1.0285636186599731\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 6.041912078857422 | KNN Loss: 5.014963150024414 | BCE Loss: 1.0269486904144287\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.070413589477539 | KNN Loss: 5.024502277374268 | BCE Loss: 1.0459113121032715\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.037322521209717 | KNN Loss: 5.027315139770508 | BCE Loss: 1.0100075006484985\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.076766014099121 | KNN Loss: 5.0296101570129395 | BCE Loss: 1.0471558570861816\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.035231590270996 | KNN Loss: 5.004673480987549 | BCE Loss: 1.0305583477020264\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 6.033302307128906 | KNN Loss: 5.011370658874512 | BCE Loss: 1.0219318866729736\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.074922561645508 | KNN Loss: 5.042324066162109 | BCE Loss: 1.0325984954833984\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.022605895996094 | KNN Loss: 4.9997358322143555 | BCE Loss: 1.0228699445724487\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.028109073638916 | KNN Loss: 5.005277633666992 | BCE Loss: 1.0228314399719238\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.084873676300049 | KNN Loss: 5.055262088775635 | BCE Loss: 1.0296117067337036\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.0909600257873535 | KNN Loss: 5.047516345977783 | BCE Loss: 1.0434436798095703\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.030233383178711 | KNN Loss: 5.0176897048950195 | BCE Loss: 1.0125435590744019\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.025045871734619 | KNN Loss: 4.992163181304932 | BCE Loss: 1.0328826904296875\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.049388408660889 | KNN Loss: 5.020139694213867 | BCE Loss: 1.0292487144470215\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.054729461669922 | KNN Loss: 5.0229668617248535 | BCE Loss: 1.0317623615264893\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.05206298828125 | KNN Loss: 4.999765396118164 | BCE Loss: 1.0522973537445068\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.051525592803955 | KNN Loss: 5.011303901672363 | BCE Loss: 1.0402215719223022\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.07235860824585 | KNN Loss: 4.99686861038208 | BCE Loss: 1.0754899978637695\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.026215553283691 | KNN Loss: 5.0139241218566895 | BCE Loss: 1.0122911930084229\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.014370441436768 | KNN Loss: 5.028018951416016 | BCE Loss: 0.9863514304161072\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.046622276306152 | KNN Loss: 5.012941837310791 | BCE Loss: 1.0336802005767822\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 6.071671009063721 | KNN Loss: 5.0322370529174805 | BCE Loss: 1.0394339561462402\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.0251898765563965 | KNN Loss: 5.013038158416748 | BCE Loss: 1.0121517181396484\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.040556907653809 | KNN Loss: 5.0026140213012695 | BCE Loss: 1.037942886352539\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.041147708892822 | KNN Loss: 5.010458946228027 | BCE Loss: 1.0306886434555054\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.03780460357666 | KNN Loss: 5.011499881744385 | BCE Loss: 1.0263046026229858\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.060786724090576 | KNN Loss: 5.001706123352051 | BCE Loss: 1.0590806007385254\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.024733543395996 | KNN Loss: 5.006161212921143 | BCE Loss: 1.018572211265564\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.15443229675293 | KNN Loss: 5.074987888336182 | BCE Loss: 1.079444169998169\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.02982759475708 | KNN Loss: 5.033844470977783 | BCE Loss: 0.9959830045700073\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.034383773803711 | KNN Loss: 5.002432346343994 | BCE Loss: 1.031951665878296\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.068992614746094 | KNN Loss: 5.0269951820373535 | BCE Loss: 1.0419976711273193\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.031630039215088 | KNN Loss: 5.015060901641846 | BCE Loss: 1.0165691375732422\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.064864158630371 | KNN Loss: 5.022407531738281 | BCE Loss: 1.0424566268920898\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.061373710632324 | KNN Loss: 5.021553993225098 | BCE Loss: 1.0398199558258057\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.051481246948242 | KNN Loss: 5.006434917449951 | BCE Loss: 1.0450465679168701\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.033313751220703 | KNN Loss: 5.00264835357666 | BCE Loss: 1.0306651592254639\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.038344860076904 | KNN Loss: 5.037077903747559 | BCE Loss: 1.0012668371200562\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.057853698730469 | KNN Loss: 5.012848377227783 | BCE Loss: 1.0450055599212646\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.079245090484619 | KNN Loss: 5.042172908782959 | BCE Loss: 1.0370721817016602\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.063772201538086 | KNN Loss: 5.043957233428955 | BCE Loss: 1.0198147296905518\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.015728950500488 | KNN Loss: 5.000889301300049 | BCE Loss: 1.0148394107818604\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 6.02066707611084 | KNN Loss: 4.991961479187012 | BCE Loss: 1.0287058353424072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.060959815979004 | KNN Loss: 5.019456386566162 | BCE Loss: 1.041503667831421\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.05232048034668 | KNN Loss: 5.015036582946777 | BCE Loss: 1.0372841358184814\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.028977394104004 | KNN Loss: 5.006956100463867 | BCE Loss: 1.0220212936401367\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.021738052368164 | KNN Loss: 5.001918792724609 | BCE Loss: 1.0198190212249756\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.05396842956543 | KNN Loss: 5.034595012664795 | BCE Loss: 1.0193736553192139\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.030961036682129 | KNN Loss: 5.007833957672119 | BCE Loss: 1.0231270790100098\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.057252883911133 | KNN Loss: 5.042801380157471 | BCE Loss: 1.0144516229629517\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.0871782302856445 | KNN Loss: 5.049203395843506 | BCE Loss: 1.0379748344421387\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.065191268920898 | KNN Loss: 5.008555889129639 | BCE Loss: 1.0566351413726807\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.0532097816467285 | KNN Loss: 5.0417585372924805 | BCE Loss: 1.011451244354248\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.053790092468262 | KNN Loss: 5.0075249671936035 | BCE Loss: 1.0462650060653687\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.033571243286133 | KNN Loss: 5.011856555938721 | BCE Loss: 1.0217149257659912\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.069452285766602 | KNN Loss: 5.045340538024902 | BCE Loss: 1.0241117477416992\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.01875114440918 | KNN Loss: 5.008429527282715 | BCE Loss: 1.010321855545044\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.041097164154053 | KNN Loss: 5.018346786499023 | BCE Loss: 1.0227504968643188\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.101858139038086 | KNN Loss: 5.063903331756592 | BCE Loss: 1.0379550457000732\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.0343017578125 | KNN Loss: 5.012048244476318 | BCE Loss: 1.0222535133361816\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.060749053955078 | KNN Loss: 5.024966239929199 | BCE Loss: 1.0357826948165894\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.046664237976074 | KNN Loss: 5.0237040519714355 | BCE Loss: 1.0229601860046387\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.047224044799805 | KNN Loss: 5.0162458419799805 | BCE Loss: 1.0309779644012451\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.045461654663086 | KNN Loss: 4.992047309875488 | BCE Loss: 1.0534141063690186\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.060501575469971 | KNN Loss: 5.03076696395874 | BCE Loss: 1.0297346115112305\n",
      "Epoch   357: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.002388954162598 | KNN Loss: 5.003348350524902 | BCE Loss: 0.999040424823761\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.046623229980469 | KNN Loss: 5.027341842651367 | BCE Loss: 1.019281268119812\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.029852867126465 | KNN Loss: 4.993971824645996 | BCE Loss: 1.0358810424804688\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 6.082001209259033 | KNN Loss: 5.044112682342529 | BCE Loss: 1.0378886461257935\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.083282470703125 | KNN Loss: 5.024980068206787 | BCE Loss: 1.0583022832870483\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.106441497802734 | KNN Loss: 5.076463222503662 | BCE Loss: 1.0299781560897827\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.076299667358398 | KNN Loss: 5.01251745223999 | BCE Loss: 1.0637822151184082\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.02175235748291 | KNN Loss: 5.01194953918457 | BCE Loss: 1.0098026990890503\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.119169235229492 | KNN Loss: 5.051033020019531 | BCE Loss: 1.0681359767913818\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.0515828132629395 | KNN Loss: 5.0191240310668945 | BCE Loss: 1.032458782196045\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.086819648742676 | KNN Loss: 5.03883171081543 | BCE Loss: 1.0479880571365356\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.048833847045898 | KNN Loss: 5.034567832946777 | BCE Loss: 1.014266014099121\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.024589538574219 | KNN Loss: 5.021578788757324 | BCE Loss: 1.0030107498168945\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.006126403808594 | KNN Loss: 5.014883995056152 | BCE Loss: 0.991242527961731\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.063333511352539 | KNN Loss: 5.045355796813965 | BCE Loss: 1.0179777145385742\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 6.066222190856934 | KNN Loss: 5.042652606964111 | BCE Loss: 1.0235693454742432\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.050857067108154 | KNN Loss: 5.010931491851807 | BCE Loss: 1.0399255752563477\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.0747785568237305 | KNN Loss: 5.042933940887451 | BCE Loss: 1.0318447351455688\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.043826580047607 | KNN Loss: 5.007847785949707 | BCE Loss: 1.03597891330719\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.115577220916748 | KNN Loss: 5.068195819854736 | BCE Loss: 1.0473812818527222\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.06203556060791 | KNN Loss: 5.033153533935547 | BCE Loss: 1.0288817882537842\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.026805877685547 | KNN Loss: 5.007671356201172 | BCE Loss: 1.019134521484375\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.0541815757751465 | KNN Loss: 5.031768321990967 | BCE Loss: 1.0224132537841797\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 5.998111724853516 | KNN Loss: 5.002734184265137 | BCE Loss: 0.995377779006958\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.0356268882751465 | KNN Loss: 5.016613006591797 | BCE Loss: 1.0190138816833496\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 6.035379409790039 | KNN Loss: 5.027553558349609 | BCE Loss: 1.0078258514404297\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.067195892333984 | KNN Loss: 5.015133857727051 | BCE Loss: 1.0520617961883545\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.0836992263793945 | KNN Loss: 5.04252290725708 | BCE Loss: 1.0411763191223145\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.04470682144165 | KNN Loss: 5.037856578826904 | BCE Loss: 1.006850242614746\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.064887046813965 | KNN Loss: 5.031988620758057 | BCE Loss: 1.0328984260559082\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.048362731933594 | KNN Loss: 5.006307601928711 | BCE Loss: 1.0420551300048828\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.048273086547852 | KNN Loss: 5.027645111083984 | BCE Loss: 1.0206282138824463\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.030652046203613 | KNN Loss: 5.001218795776367 | BCE Loss: 1.029433012008667\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.09724235534668 | KNN Loss: 5.018418312072754 | BCE Loss: 1.0788240432739258\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.056696891784668 | KNN Loss: 5.017795562744141 | BCE Loss: 1.0389012098312378\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.063121795654297 | KNN Loss: 5.036794662475586 | BCE Loss: 1.026327133178711\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.021885871887207 | KNN Loss: 5.0148749351501465 | BCE Loss: 1.0070111751556396\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.079441070556641 | KNN Loss: 5.036004543304443 | BCE Loss: 1.0434365272521973\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.088801383972168 | KNN Loss: 5.009215831756592 | BCE Loss: 1.0795857906341553\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.071599006652832 | KNN Loss: 5.028268337249756 | BCE Loss: 1.0433309078216553\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.019905090332031 | KNN Loss: 5.015150547027588 | BCE Loss: 1.0047547817230225\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.037533760070801 | KNN Loss: 5.006888389587402 | BCE Loss: 1.0306452512741089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.016395568847656 | KNN Loss: 5.00892972946167 | BCE Loss: 1.0074658393859863\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.087184429168701 | KNN Loss: 5.036960124969482 | BCE Loss: 1.0502244234085083\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.091559886932373 | KNN Loss: 5.040876388549805 | BCE Loss: 1.0506833791732788\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.043370246887207 | KNN Loss: 5.012822151184082 | BCE Loss: 1.030548095703125\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.068790912628174 | KNN Loss: 5.006211280822754 | BCE Loss: 1.0625797510147095\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.036464214324951 | KNN Loss: 5.012161731719971 | BCE Loss: 1.0243024826049805\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.040223121643066 | KNN Loss: 5.032845973968506 | BCE Loss: 1.0073773860931396\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.0594682693481445 | KNN Loss: 5.018750190734863 | BCE Loss: 1.0407178401947021\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.052292823791504 | KNN Loss: 5.0116376876831055 | BCE Loss: 1.0406551361083984\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.026268005371094 | KNN Loss: 4.998318195343018 | BCE Loss: 1.0279496908187866\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 6.104626655578613 | KNN Loss: 5.035254955291748 | BCE Loss: 1.0693719387054443\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.059512138366699 | KNN Loss: 5.016645431518555 | BCE Loss: 1.0428667068481445\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.092108249664307 | KNN Loss: 5.024013042449951 | BCE Loss: 1.0680952072143555\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.054080963134766 | KNN Loss: 5.028152942657471 | BCE Loss: 1.0259277820587158\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.0352373123168945 | KNN Loss: 5.015511512756348 | BCE Loss: 1.0197257995605469\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 6.009418964385986 | KNN Loss: 5.001994609832764 | BCE Loss: 1.0074243545532227\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.046642303466797 | KNN Loss: 5.015045642852783 | BCE Loss: 1.0315968990325928\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.022565841674805 | KNN Loss: 5.0208001136779785 | BCE Loss: 1.0017659664154053\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.037078857421875 | KNN Loss: 5.009750843048096 | BCE Loss: 1.0273281335830688\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.036035537719727 | KNN Loss: 5.020005702972412 | BCE Loss: 1.0160295963287354\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.058365821838379 | KNN Loss: 5.024927616119385 | BCE Loss: 1.0334382057189941\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.039070129394531 | KNN Loss: 5.020517349243164 | BCE Loss: 1.0185526609420776\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.054354667663574 | KNN Loss: 5.020836353302002 | BCE Loss: 1.0335184335708618\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.009209156036377 | KNN Loss: 5.000483512878418 | BCE Loss: 1.008725643157959\n",
      "Epoch   368: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.039636135101318 | KNN Loss: 4.996489524841309 | BCE Loss: 1.0431464910507202\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.095559120178223 | KNN Loss: 5.039543628692627 | BCE Loss: 1.0560157299041748\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.042229652404785 | KNN Loss: 5.007366180419922 | BCE Loss: 1.0348632335662842\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.117978096008301 | KNN Loss: 5.097829341888428 | BCE Loss: 1.020148754119873\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.027133941650391 | KNN Loss: 4.998732566833496 | BCE Loss: 1.0284016132354736\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.054430961608887 | KNN Loss: 4.993175506591797 | BCE Loss: 1.0612554550170898\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.041506767272949 | KNN Loss: 4.999057292938232 | BCE Loss: 1.042449712753296\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.08477783203125 | KNN Loss: 5.035281658172607 | BCE Loss: 1.0494964122772217\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.047356128692627 | KNN Loss: 5.009182929992676 | BCE Loss: 1.0381731986999512\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.101672172546387 | KNN Loss: 5.054128170013428 | BCE Loss: 1.0475438833236694\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.053337097167969 | KNN Loss: 5.050782680511475 | BCE Loss: 1.002554178237915\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.021744728088379 | KNN Loss: 4.999919891357422 | BCE Loss: 1.0218250751495361\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.074338912963867 | KNN Loss: 5.023115158081055 | BCE Loss: 1.0512235164642334\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.0459442138671875 | KNN Loss: 5.009047985076904 | BCE Loss: 1.0368961095809937\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.016267776489258 | KNN Loss: 5.011324882507324 | BCE Loss: 1.0049428939819336\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.052900791168213 | KNN Loss: 5.029863357543945 | BCE Loss: 1.0230374336242676\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.03237771987915 | KNN Loss: 5.010571002960205 | BCE Loss: 1.0218067169189453\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.037567138671875 | KNN Loss: 5.003603458404541 | BCE Loss: 1.0339634418487549\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.079022407531738 | KNN Loss: 5.031894207000732 | BCE Loss: 1.0471279621124268\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.074799060821533 | KNN Loss: 5.040234565734863 | BCE Loss: 1.0345643758773804\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.044737815856934 | KNN Loss: 5.010112285614014 | BCE Loss: 1.0346252918243408\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.07365083694458 | KNN Loss: 5.015780925750732 | BCE Loss: 1.0578699111938477\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 6.014113903045654 | KNN Loss: 5.002994537353516 | BCE Loss: 1.0111193656921387\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.094331741333008 | KNN Loss: 5.057304859161377 | BCE Loss: 1.0370268821716309\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.078929901123047 | KNN Loss: 5.01978063583374 | BCE Loss: 1.0591495037078857\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.056543827056885 | KNN Loss: 5.009823322296143 | BCE Loss: 1.0467205047607422\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.0536088943481445 | KNN Loss: 5.015394687652588 | BCE Loss: 1.038214087486267\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.060602188110352 | KNN Loss: 5.030781269073486 | BCE Loss: 1.0298207998275757\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.133131980895996 | KNN Loss: 5.104419231414795 | BCE Loss: 1.028712511062622\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.035366058349609 | KNN Loss: 5.008887767791748 | BCE Loss: 1.0264785289764404\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.064019203186035 | KNN Loss: 5.030896186828613 | BCE Loss: 1.0331230163574219\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.035182476043701 | KNN Loss: 5.006217956542969 | BCE Loss: 1.028964638710022\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.051308631896973 | KNN Loss: 5.037776470184326 | BCE Loss: 1.013532042503357\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 6.054917812347412 | KNN Loss: 5.0307698249816895 | BCE Loss: 1.0241481065750122\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.052241325378418 | KNN Loss: 5.026772499084473 | BCE Loss: 1.0254690647125244\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.041375160217285 | KNN Loss: 5.031061172485352 | BCE Loss: 1.0103139877319336\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.056702613830566 | KNN Loss: 5.025594711303711 | BCE Loss: 1.0311079025268555\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.029645919799805 | KNN Loss: 5.008049964904785 | BCE Loss: 1.0215957164764404\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.080207347869873 | KNN Loss: 5.036876201629639 | BCE Loss: 1.0433310270309448\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.032661437988281 | KNN Loss: 5.013493061065674 | BCE Loss: 1.0191683769226074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 6.045515060424805 | KNN Loss: 5.020077228546143 | BCE Loss: 1.0254377126693726\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.053659439086914 | KNN Loss: 5.0265398025512695 | BCE Loss: 1.0271198749542236\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.048740386962891 | KNN Loss: 5.017756938934326 | BCE Loss: 1.0309836864471436\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 5.997547149658203 | KNN Loss: 4.988187789916992 | BCE Loss: 1.00935959815979\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.039884567260742 | KNN Loss: 5.011584281921387 | BCE Loss: 1.0283002853393555\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.013166904449463 | KNN Loss: 5.000722408294678 | BCE Loss: 1.0124443769454956\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.109011650085449 | KNN Loss: 5.045729160308838 | BCE Loss: 1.0632823705673218\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.010015487670898 | KNN Loss: 5.004150867462158 | BCE Loss: 1.0058646202087402\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.09904146194458 | KNN Loss: 5.07194185256958 | BCE Loss: 1.0270994901657104\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.074002265930176 | KNN Loss: 5.031317710876465 | BCE Loss: 1.0426843166351318\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.077730655670166 | KNN Loss: 5.045365810394287 | BCE Loss: 1.0323647260665894\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.014588832855225 | KNN Loss: 5.011413097381592 | BCE Loss: 1.0031757354736328\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.02664041519165 | KNN Loss: 5.022400856018066 | BCE Loss: 1.0042396783828735\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.052464485168457 | KNN Loss: 5.010443210601807 | BCE Loss: 1.0420212745666504\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.014567852020264 | KNN Loss: 5.009294033050537 | BCE Loss: 1.0052738189697266\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.074967384338379 | KNN Loss: 5.036977291107178 | BCE Loss: 1.037989854812622\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.035946369171143 | KNN Loss: 5.001849174499512 | BCE Loss: 1.0340970754623413\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.014097213745117 | KNN Loss: 5.009374141693115 | BCE Loss: 1.004723072052002\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.042848110198975 | KNN Loss: 5.027076244354248 | BCE Loss: 1.0157719850540161\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.070590019226074 | KNN Loss: 5.059263229370117 | BCE Loss: 1.011326789855957\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.005934238433838 | KNN Loss: 5.013401985168457 | BCE Loss: 0.9925322532653809\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.042518615722656 | KNN Loss: 5.019575119018555 | BCE Loss: 1.0229432582855225\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.058956146240234 | KNN Loss: 5.018575191497803 | BCE Loss: 1.0403807163238525\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.009153366088867 | KNN Loss: 5.002438545227051 | BCE Loss: 1.0067145824432373\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.0365800857543945 | KNN Loss: 5.007543087005615 | BCE Loss: 1.0290369987487793\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.045253276824951 | KNN Loss: 5.01218843460083 | BCE Loss: 1.033064842224121\n",
      "Epoch   379: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.047073841094971 | KNN Loss: 5.02770471572876 | BCE Loss: 1.0193692445755005\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.0635786056518555 | KNN Loss: 5.028350353240967 | BCE Loss: 1.0352282524108887\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.024852275848389 | KNN Loss: 4.999822616577148 | BCE Loss: 1.0250295400619507\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.05442476272583 | KNN Loss: 5.029299259185791 | BCE Loss: 1.0251256227493286\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.0718793869018555 | KNN Loss: 5.046504974365234 | BCE Loss: 1.025374412536621\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.029984951019287 | KNN Loss: 5.02180290222168 | BCE Loss: 1.0081819295883179\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.065420150756836 | KNN Loss: 5.025346279144287 | BCE Loss: 1.0400736331939697\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.007901191711426 | KNN Loss: 5.004244327545166 | BCE Loss: 1.0036568641662598\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.0312652587890625 | KNN Loss: 5.010674476623535 | BCE Loss: 1.0205910205841064\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.057506084442139 | KNN Loss: 5.018333435058594 | BCE Loss: 1.0391727685928345\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.098437309265137 | KNN Loss: 5.053648948669434 | BCE Loss: 1.044788122177124\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.069593906402588 | KNN Loss: 5.018086910247803 | BCE Loss: 1.0515068769454956\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.053492546081543 | KNN Loss: 5.019806385040283 | BCE Loss: 1.0336862802505493\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.0634236335754395 | KNN Loss: 5.031261920928955 | BCE Loss: 1.032161831855774\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.041839122772217 | KNN Loss: 5.008822917938232 | BCE Loss: 1.0330162048339844\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.0875444412231445 | KNN Loss: 5.062533378601074 | BCE Loss: 1.0250113010406494\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.04661750793457 | KNN Loss: 5.038767337799072 | BCE Loss: 1.0078504085540771\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.114701271057129 | KNN Loss: 5.065908432006836 | BCE Loss: 1.0487929582595825\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.048977375030518 | KNN Loss: 5.032375812530518 | BCE Loss: 1.0166015625\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.073034286499023 | KNN Loss: 5.047434329986572 | BCE Loss: 1.0255999565124512\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.072860240936279 | KNN Loss: 5.041691780090332 | BCE Loss: 1.0311684608459473\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.090051651000977 | KNN Loss: 5.061866760253906 | BCE Loss: 1.0281848907470703\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.003108501434326 | KNN Loss: 5.010519027709961 | BCE Loss: 0.9925892949104309\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.048459053039551 | KNN Loss: 5.008125305175781 | BCE Loss: 1.0403339862823486\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 6.0264716148376465 | KNN Loss: 5.003861904144287 | BCE Loss: 1.0226095914840698\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.088168144226074 | KNN Loss: 5.045701026916504 | BCE Loss: 1.0424668788909912\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.084429740905762 | KNN Loss: 5.035264015197754 | BCE Loss: 1.0491654872894287\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.035761833190918 | KNN Loss: 5.015687942504883 | BCE Loss: 1.0200741291046143\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.028364658355713 | KNN Loss: 5.00139045715332 | BCE Loss: 1.0269742012023926\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.083067893981934 | KNN Loss: 5.039173126220703 | BCE Loss: 1.0438947677612305\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.069481372833252 | KNN Loss: 5.037965297698975 | BCE Loss: 1.0315160751342773\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 6.017617225646973 | KNN Loss: 4.98294734954834 | BCE Loss: 1.0346699953079224\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.011693954467773 | KNN Loss: 5.011427879333496 | BCE Loss: 1.0002660751342773\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 6.061941146850586 | KNN Loss: 5.019097328186035 | BCE Loss: 1.0428439378738403\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.07802152633667 | KNN Loss: 5.020860195159912 | BCE Loss: 1.0571612119674683\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.025576591491699 | KNN Loss: 5.008238792419434 | BCE Loss: 1.0173380374908447\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.029482841491699 | KNN Loss: 5.0259480476379395 | BCE Loss: 1.0035347938537598\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.069194793701172 | KNN Loss: 5.034031867980957 | BCE Loss: 1.0351629257202148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.061944961547852 | KNN Loss: 5.025825023651123 | BCE Loss: 1.0361201763153076\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.045200347900391 | KNN Loss: 5.004926681518555 | BCE Loss: 1.0402734279632568\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.037946701049805 | KNN Loss: 5.001579761505127 | BCE Loss: 1.0363671779632568\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.0641961097717285 | KNN Loss: 5.021745204925537 | BCE Loss: 1.0424509048461914\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.062648773193359 | KNN Loss: 5.033629894256592 | BCE Loss: 1.0290188789367676\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 6.101574897766113 | KNN Loss: 5.0872697830200195 | BCE Loss: 1.0143052339553833\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.044249057769775 | KNN Loss: 5.012156963348389 | BCE Loss: 1.0320922136306763\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.064578056335449 | KNN Loss: 5.041496276855469 | BCE Loss: 1.0230817794799805\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.073346138000488 | KNN Loss: 5.011788368225098 | BCE Loss: 1.0615578889846802\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.087193489074707 | KNN Loss: 5.056354522705078 | BCE Loss: 1.030839204788208\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 6.0334978103637695 | KNN Loss: 5.00779914855957 | BCE Loss: 1.0256989002227783\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.060941219329834 | KNN Loss: 5.009857654571533 | BCE Loss: 1.0510835647583008\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.066157817840576 | KNN Loss: 5.023751735687256 | BCE Loss: 1.0424062013626099\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.024090766906738 | KNN Loss: 4.992259502410889 | BCE Loss: 1.0318315029144287\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.013864517211914 | KNN Loss: 5.008224010467529 | BCE Loss: 1.0056407451629639\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.061502933502197 | KNN Loss: 5.0408244132995605 | BCE Loss: 1.0206785202026367\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.033271312713623 | KNN Loss: 5.013005256652832 | BCE Loss: 1.0202659368515015\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.0223774909973145 | KNN Loss: 5.009361743927002 | BCE Loss: 1.013015866279602\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.060529708862305 | KNN Loss: 5.035036563873291 | BCE Loss: 1.0254930257797241\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.041840553283691 | KNN Loss: 5.011987209320068 | BCE Loss: 1.029853105545044\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.061392307281494 | KNN Loss: 5.008618354797363 | BCE Loss: 1.0527739524841309\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.013566493988037 | KNN Loss: 5.002280235290527 | BCE Loss: 1.0112862586975098\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.031975269317627 | KNN Loss: 5.015583038330078 | BCE Loss: 1.0163922309875488\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.071194171905518 | KNN Loss: 5.047719478607178 | BCE Loss: 1.0234746932983398\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.045065879821777 | KNN Loss: 5.010830879211426 | BCE Loss: 1.0342347621917725\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.033148765563965 | KNN Loss: 5.0189619064331055 | BCE Loss: 1.0141870975494385\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 6.0497636795043945 | KNN Loss: 5.026942729949951 | BCE Loss: 1.022821068763733\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 6.060608386993408 | KNN Loss: 5.0101470947265625 | BCE Loss: 1.0504614114761353\n",
      "Epoch   390: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 5.995587348937988 | KNN Loss: 4.98931360244751 | BCE Loss: 1.0062739849090576\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.0052313804626465 | KNN Loss: 4.993599891662598 | BCE Loss: 1.0116314888000488\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 6.001868724822998 | KNN Loss: 4.994540214538574 | BCE Loss: 1.0073285102844238\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 6.040839195251465 | KNN Loss: 5.030219078063965 | BCE Loss: 1.0106199979782104\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.0422749519348145 | KNN Loss: 5.012555122375488 | BCE Loss: 1.0297199487686157\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.008204460144043 | KNN Loss: 4.998312950134277 | BCE Loss: 1.0098912715911865\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.040166854858398 | KNN Loss: 5.015329360961914 | BCE Loss: 1.0248372554779053\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.062597751617432 | KNN Loss: 5.01215124130249 | BCE Loss: 1.0504465103149414\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.050347328186035 | KNN Loss: 5.032081127166748 | BCE Loss: 1.018266201019287\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.078457355499268 | KNN Loss: 5.04913330078125 | BCE Loss: 1.029323935508728\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.073657035827637 | KNN Loss: 5.014306545257568 | BCE Loss: 1.0593502521514893\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.087918281555176 | KNN Loss: 5.047760009765625 | BCE Loss: 1.0401585102081299\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.013660430908203 | KNN Loss: 5.0081610679626465 | BCE Loss: 1.0054993629455566\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.037636756896973 | KNN Loss: 5.004792213439941 | BCE Loss: 1.0328447818756104\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.059194564819336 | KNN Loss: 5.017151355743408 | BCE Loss: 1.0420432090759277\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.04245138168335 | KNN Loss: 5.017241954803467 | BCE Loss: 1.0252095460891724\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.031278610229492 | KNN Loss: 5.017979145050049 | BCE Loss: 1.0132994651794434\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.03006649017334 | KNN Loss: 5.005934238433838 | BCE Loss: 1.024132251739502\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.02805233001709 | KNN Loss: 5.004425525665283 | BCE Loss: 1.0236270427703857\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.058751106262207 | KNN Loss: 5.024182319641113 | BCE Loss: 1.0345685482025146\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.026390075683594 | KNN Loss: 4.995317459106445 | BCE Loss: 1.0310723781585693\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.041276454925537 | KNN Loss: 5.036342144012451 | BCE Loss: 1.0049344301223755\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.054059982299805 | KNN Loss: 5.020349502563477 | BCE Loss: 1.0337104797363281\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 6.060100078582764 | KNN Loss: 5.020279407501221 | BCE Loss: 1.0398207902908325\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 5.99747371673584 | KNN Loss: 5.007059097290039 | BCE Loss: 0.9904148578643799\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 5.998079776763916 | KNN Loss: 5.004147529602051 | BCE Loss: 0.9939322471618652\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.045592308044434 | KNN Loss: 5.010330677032471 | BCE Loss: 1.035261869430542\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.02935266494751 | KNN Loss: 5.017512798309326 | BCE Loss: 1.0118399858474731\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.051909446716309 | KNN Loss: 5.03156852722168 | BCE Loss: 1.020340919494629\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.077633380889893 | KNN Loss: 5.031114101409912 | BCE Loss: 1.04651939868927\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 6.043929100036621 | KNN Loss: 5.018515586853027 | BCE Loss: 1.0254137516021729\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.038161754608154 | KNN Loss: 5.017807960510254 | BCE Loss: 1.0203537940979004\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.065421104431152 | KNN Loss: 5.0259222984313965 | BCE Loss: 1.039499044418335\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 6.048975944519043 | KNN Loss: 5.033814430236816 | BCE Loss: 1.0151616334915161\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.040678024291992 | KNN Loss: 5.034295558929443 | BCE Loss: 1.0063824653625488\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 6.052947044372559 | KNN Loss: 5.013899326324463 | BCE Loss: 1.0390474796295166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.055603504180908 | KNN Loss: 5.023997783660889 | BCE Loss: 1.031605839729309\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.07103157043457 | KNN Loss: 5.035933017730713 | BCE Loss: 1.035098671913147\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.050051212310791 | KNN Loss: 5.040304183959961 | BCE Loss: 1.0097471475601196\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.0619001388549805 | KNN Loss: 5.027299404144287 | BCE Loss: 1.0346004962921143\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.040177345275879 | KNN Loss: 5.004186630249023 | BCE Loss: 1.0359904766082764\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 6.036218166351318 | KNN Loss: 5.019582271575928 | BCE Loss: 1.0166358947753906\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.113120079040527 | KNN Loss: 5.0549774169921875 | BCE Loss: 1.058142900466919\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.069126605987549 | KNN Loss: 5.014901161193848 | BCE Loss: 1.0542255640029907\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.042846202850342 | KNN Loss: 5.022580623626709 | BCE Loss: 1.0202655792236328\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.097151756286621 | KNN Loss: 5.053920745849609 | BCE Loss: 1.0432307720184326\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.114148139953613 | KNN Loss: 5.063167095184326 | BCE Loss: 1.0509812831878662\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.042416572570801 | KNN Loss: 5.002665996551514 | BCE Loss: 1.039750576019287\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.061568737030029 | KNN Loss: 5.015774250030518 | BCE Loss: 1.0457944869995117\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.045351982116699 | KNN Loss: 5.017272472381592 | BCE Loss: 1.0280795097351074\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.062453746795654 | KNN Loss: 5.004256725311279 | BCE Loss: 1.058197021484375\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.088916301727295 | KNN Loss: 5.0300822257995605 | BCE Loss: 1.0588340759277344\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.056126594543457 | KNN Loss: 5.037510871887207 | BCE Loss: 1.0186158418655396\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.0384202003479 | KNN Loss: 5.025258541107178 | BCE Loss: 1.0131617784500122\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 6.053344249725342 | KNN Loss: 5.0201497077941895 | BCE Loss: 1.033194661140442\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 6.107579231262207 | KNN Loss: 5.045083522796631 | BCE Loss: 1.0624957084655762\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.053584098815918 | KNN Loss: 5.006280422210693 | BCE Loss: 1.0473039150238037\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.085338592529297 | KNN Loss: 5.042332649230957 | BCE Loss: 1.0430059432983398\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.034784317016602 | KNN Loss: 4.993460178375244 | BCE Loss: 1.0413241386413574\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.080167293548584 | KNN Loss: 5.026695251464844 | BCE Loss: 1.0534720420837402\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.044152736663818 | KNN Loss: 5.021468639373779 | BCE Loss: 1.022684097290039\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.089541912078857 | KNN Loss: 5.058892726898193 | BCE Loss: 1.0306490659713745\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.050278663635254 | KNN Loss: 5.037415027618408 | BCE Loss: 1.0128638744354248\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.048066139221191 | KNN Loss: 5.021871566772461 | BCE Loss: 1.0261945724487305\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.098593235015869 | KNN Loss: 5.043240547180176 | BCE Loss: 1.0553526878356934\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.03419303894043 | KNN Loss: 5.009135723114014 | BCE Loss: 1.0250575542449951\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.0299391746521 | KNN Loss: 5.005064964294434 | BCE Loss: 1.024874210357666\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.039600372314453 | KNN Loss: 5.013155937194824 | BCE Loss: 1.026444435119629\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.092226982116699 | KNN Loss: 5.017810344696045 | BCE Loss: 1.0744163990020752\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.065613746643066 | KNN Loss: 5.039989471435547 | BCE Loss: 1.0256245136260986\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.030900955200195 | KNN Loss: 5.021457195281982 | BCE Loss: 1.0094438791275024\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.0197577476501465 | KNN Loss: 5.004899501800537 | BCE Loss: 1.0148582458496094\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.070273399353027 | KNN Loss: 5.026381015777588 | BCE Loss: 1.0438921451568604\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.060840606689453 | KNN Loss: 5.019704341888428 | BCE Loss: 1.0411365032196045\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.12811803817749 | KNN Loss: 5.1051435470581055 | BCE Loss: 1.0229744911193848\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.071969509124756 | KNN Loss: 5.036181926727295 | BCE Loss: 1.0357874631881714\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.039661407470703 | KNN Loss: 5.008401393890381 | BCE Loss: 1.0312597751617432\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.04005241394043 | KNN Loss: 5.014551162719727 | BCE Loss: 1.0255012512207031\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.05877161026001 | KNN Loss: 5.017934799194336 | BCE Loss: 1.0408368110656738\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 6.050034046173096 | KNN Loss: 5.033634662628174 | BCE Loss: 1.0163995027542114\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.031033992767334 | KNN Loss: 5.013583183288574 | BCE Loss: 1.0174506902694702\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.073516368865967 | KNN Loss: 5.025972366333008 | BCE Loss: 1.047544002532959\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.050821304321289 | KNN Loss: 5.030144691467285 | BCE Loss: 1.0206767320632935\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.050577163696289 | KNN Loss: 5.014805793762207 | BCE Loss: 1.0357712507247925\n",
      "Epoch   404: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.016096115112305 | KNN Loss: 5.0186920166015625 | BCE Loss: 0.997404158115387\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.029911041259766 | KNN Loss: 5.016079902648926 | BCE Loss: 1.0138309001922607\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.062729835510254 | KNN Loss: 5.009336948394775 | BCE Loss: 1.0533931255340576\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.075563430786133 | KNN Loss: 5.034688472747803 | BCE Loss: 1.0408751964569092\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.063472747802734 | KNN Loss: 5.044548988342285 | BCE Loss: 1.0189235210418701\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.109175682067871 | KNN Loss: 5.049001693725586 | BCE Loss: 1.0601742267608643\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.057200908660889 | KNN Loss: 5.018696308135986 | BCE Loss: 1.0385044813156128\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 6.054343223571777 | KNN Loss: 5.032655239105225 | BCE Loss: 1.0216878652572632\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.09906005859375 | KNN Loss: 5.067444801330566 | BCE Loss: 1.0316150188446045\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.040041923522949 | KNN Loss: 5.02402400970459 | BCE Loss: 1.0160179138183594\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.043436050415039 | KNN Loss: 5.013406276702881 | BCE Loss: 1.0300297737121582\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.040167331695557 | KNN Loss: 5.014225006103516 | BCE Loss: 1.0259422063827515\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 6.03060245513916 | KNN Loss: 5.004419326782227 | BCE Loss: 1.0261828899383545\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 6.082871437072754 | KNN Loss: 5.059320449829102 | BCE Loss: 1.0235509872436523\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.040215015411377 | KNN Loss: 5.002564907073975 | BCE Loss: 1.0376501083374023\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 5.998062610626221 | KNN Loss: 5.011204719543457 | BCE Loss: 0.9868578910827637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 6.100532054901123 | KNN Loss: 5.059808254241943 | BCE Loss: 1.0407239198684692\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.067277431488037 | KNN Loss: 5.031207084655762 | BCE Loss: 1.0360702276229858\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.052545070648193 | KNN Loss: 5.044075012207031 | BCE Loss: 1.0084699392318726\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.014760971069336 | KNN Loss: 5.004143714904785 | BCE Loss: 1.0106174945831299\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.0492448806762695 | KNN Loss: 5.033742904663086 | BCE Loss: 1.0155020952224731\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.045732021331787 | KNN Loss: 5.014837741851807 | BCE Loss: 1.03089439868927\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.054770469665527 | KNN Loss: 5.023788928985596 | BCE Loss: 1.0309815406799316\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.05964469909668 | KNN Loss: 5.023557662963867 | BCE Loss: 1.0360872745513916\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.050106525421143 | KNN Loss: 5.012585639953613 | BCE Loss: 1.0375207662582397\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.061785697937012 | KNN Loss: 5.039277076721191 | BCE Loss: 1.0225083827972412\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.0712080001831055 | KNN Loss: 5.031280517578125 | BCE Loss: 1.0399277210235596\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.0380401611328125 | KNN Loss: 5.012675762176514 | BCE Loss: 1.0253641605377197\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.06749963760376 | KNN Loss: 5.033283233642578 | BCE Loss: 1.0342164039611816\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.037065029144287 | KNN Loss: 5.022711277008057 | BCE Loss: 1.0143537521362305\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.024760723114014 | KNN Loss: 5.0155510902404785 | BCE Loss: 1.0092097520828247\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.036380767822266 | KNN Loss: 5.025659084320068 | BCE Loss: 1.0107219219207764\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.0345540046691895 | KNN Loss: 5.013445854187012 | BCE Loss: 1.0211082696914673\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.0720624923706055 | KNN Loss: 5.053412437438965 | BCE Loss: 1.0186501741409302\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.062595844268799 | KNN Loss: 5.018912315368652 | BCE Loss: 1.0436835289001465\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.101166248321533 | KNN Loss: 5.0370378494262695 | BCE Loss: 1.0641285181045532\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.038803577423096 | KNN Loss: 5.006653308868408 | BCE Loss: 1.032150387763977\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 6.063437461853027 | KNN Loss: 5.020511627197266 | BCE Loss: 1.0429260730743408\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.051222324371338 | KNN Loss: 5.025977611541748 | BCE Loss: 1.0252447128295898\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.041739463806152 | KNN Loss: 5.0070295333862305 | BCE Loss: 1.034710168838501\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.062007904052734 | KNN Loss: 5.032549858093262 | BCE Loss: 1.0294578075408936\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.075466632843018 | KNN Loss: 5.015500545501709 | BCE Loss: 1.0599660873413086\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.043235778808594 | KNN Loss: 5.050410270690918 | BCE Loss: 0.9928253293037415\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.035236358642578 | KNN Loss: 4.9968719482421875 | BCE Loss: 1.0383641719818115\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.024693489074707 | KNN Loss: 5.016875267028809 | BCE Loss: 1.0078184604644775\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.075742721557617 | KNN Loss: 5.022049903869629 | BCE Loss: 1.0536930561065674\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.079108238220215 | KNN Loss: 5.044046878814697 | BCE Loss: 1.0350615978240967\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 6.044431209564209 | KNN Loss: 5.013936996459961 | BCE Loss: 1.030494213104248\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.047662258148193 | KNN Loss: 5.030614852905273 | BCE Loss: 1.01704740524292\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.044629096984863 | KNN Loss: 5.039708137512207 | BCE Loss: 1.0049211978912354\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.027551174163818 | KNN Loss: 5.002351760864258 | BCE Loss: 1.0251994132995605\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.011651039123535 | KNN Loss: 5.00517463684082 | BCE Loss: 1.0064761638641357\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.046773910522461 | KNN Loss: 5.016477584838867 | BCE Loss: 1.0302965641021729\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.084816932678223 | KNN Loss: 5.052754878997803 | BCE Loss: 1.032062292098999\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.050015926361084 | KNN Loss: 5.019044876098633 | BCE Loss: 1.0309710502624512\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.050512313842773 | KNN Loss: 5.019155025482178 | BCE Loss: 1.0313574075698853\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.075767993927002 | KNN Loss: 5.061083793640137 | BCE Loss: 1.0146842002868652\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.028502941131592 | KNN Loss: 5.019012451171875 | BCE Loss: 1.0094906091690063\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.110786437988281 | KNN Loss: 5.040450572967529 | BCE Loss: 1.070335865020752\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.073225021362305 | KNN Loss: 5.033499717712402 | BCE Loss: 1.0397250652313232\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.040840148925781 | KNN Loss: 5.0128936767578125 | BCE Loss: 1.0279467105865479\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.036919116973877 | KNN Loss: 5.0119829177856445 | BCE Loss: 1.0249361991882324\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.106192111968994 | KNN Loss: 5.081071853637695 | BCE Loss: 1.0251202583312988\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.102290153503418 | KNN Loss: 5.0428056716918945 | BCE Loss: 1.0594842433929443\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.0931077003479 | KNN Loss: 5.048427581787109 | BCE Loss: 1.044680118560791\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.046392440795898 | KNN Loss: 5.025757312774658 | BCE Loss: 1.0206350088119507\n",
      "Epoch   415: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.034459114074707 | KNN Loss: 5.018728733062744 | BCE Loss: 1.0157302618026733\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.05734920501709 | KNN Loss: 5.016202926635742 | BCE Loss: 1.0411465167999268\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.062978267669678 | KNN Loss: 5.03347110748291 | BCE Loss: 1.029507040977478\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.077401638031006 | KNN Loss: 5.034542560577393 | BCE Loss: 1.0428591966629028\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.05539608001709 | KNN Loss: 5.032081604003906 | BCE Loss: 1.0233144760131836\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.062282562255859 | KNN Loss: 5.023451805114746 | BCE Loss: 1.0388308763504028\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.015743732452393 | KNN Loss: 5.000344753265381 | BCE Loss: 1.0153990983963013\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.035057067871094 | KNN Loss: 5.011124134063721 | BCE Loss: 1.0239331722259521\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.045287609100342 | KNN Loss: 5.034475803375244 | BCE Loss: 1.010811686515808\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.068943977355957 | KNN Loss: 5.005478858947754 | BCE Loss: 1.0634653568267822\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 6.051257610321045 | KNN Loss: 5.022475242614746 | BCE Loss: 1.0287823677062988\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.033243179321289 | KNN Loss: 5.008194446563721 | BCE Loss: 1.0250484943389893\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.084299087524414 | KNN Loss: 5.021905899047852 | BCE Loss: 1.0623929500579834\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.067098140716553 | KNN Loss: 5.026833534240723 | BCE Loss: 1.0402644872665405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.073976516723633 | KNN Loss: 5.0424933433532715 | BCE Loss: 1.0314830541610718\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.073605060577393 | KNN Loss: 5.048844337463379 | BCE Loss: 1.0247606039047241\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.070494651794434 | KNN Loss: 5.025375843048096 | BCE Loss: 1.045119047164917\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.064312934875488 | KNN Loss: 5.021456241607666 | BCE Loss: 1.0428569316864014\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.0180864334106445 | KNN Loss: 5.001406192779541 | BCE Loss: 1.0166804790496826\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.026587963104248 | KNN Loss: 5.023806571960449 | BCE Loss: 1.0027815103530884\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.039917469024658 | KNN Loss: 5.013638973236084 | BCE Loss: 1.0262783765792847\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.0996174812316895 | KNN Loss: 5.03252649307251 | BCE Loss: 1.0670909881591797\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 5.998686790466309 | KNN Loss: 5.001165390014648 | BCE Loss: 0.9975212216377258\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.049278259277344 | KNN Loss: 5.012186527252197 | BCE Loss: 1.0370914936065674\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.0438408851623535 | KNN Loss: 5.012692451477051 | BCE Loss: 1.0311484336853027\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.04312801361084 | KNN Loss: 5.013448238372803 | BCE Loss: 1.029679536819458\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.115392684936523 | KNN Loss: 5.092822074890137 | BCE Loss: 1.0225707292556763\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.051994323730469 | KNN Loss: 5.041045188903809 | BCE Loss: 1.0109493732452393\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.067131042480469 | KNN Loss: 5.026435375213623 | BCE Loss: 1.0406957864761353\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.126505374908447 | KNN Loss: 5.075436592102051 | BCE Loss: 1.051068902015686\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.046178340911865 | KNN Loss: 5.033326625823975 | BCE Loss: 1.0128518342971802\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.057257652282715 | KNN Loss: 5.020512580871582 | BCE Loss: 1.0367448329925537\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.030368328094482 | KNN Loss: 5.012256622314453 | BCE Loss: 1.0181115865707397\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.08786153793335 | KNN Loss: 5.0332794189453125 | BCE Loss: 1.054582118988037\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.069299697875977 | KNN Loss: 5.0430755615234375 | BCE Loss: 1.0262243747711182\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.052928447723389 | KNN Loss: 5.013548374176025 | BCE Loss: 1.0393800735473633\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.070379257202148 | KNN Loss: 5.023226737976074 | BCE Loss: 1.0471527576446533\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.0456414222717285 | KNN Loss: 5.018248081207275 | BCE Loss: 1.0273934602737427\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.0535125732421875 | KNN Loss: 5.018261432647705 | BCE Loss: 1.0352509021759033\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.005340576171875 | KNN Loss: 5.0008463859558105 | BCE Loss: 1.0044944286346436\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.09956169128418 | KNN Loss: 5.033548355102539 | BCE Loss: 1.0660130977630615\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.023932933807373 | KNN Loss: 5.0105767250061035 | BCE Loss: 1.0133562088012695\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.080935955047607 | KNN Loss: 5.0187177658081055 | BCE Loss: 1.062218189239502\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.068757057189941 | KNN Loss: 5.033229351043701 | BCE Loss: 1.0355274677276611\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.033815860748291 | KNN Loss: 5.019948482513428 | BCE Loss: 1.0138673782348633\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.08425760269165 | KNN Loss: 5.04848051071167 | BCE Loss: 1.03577721118927\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 6.009710788726807 | KNN Loss: 4.990375995635986 | BCE Loss: 1.0193347930908203\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.024935722351074 | KNN Loss: 5.00649356842041 | BCE Loss: 1.0184420347213745\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.075386047363281 | KNN Loss: 5.017378330230713 | BCE Loss: 1.0580079555511475\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.037493705749512 | KNN Loss: 5.0159711837768555 | BCE Loss: 1.0215227603912354\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.0964765548706055 | KNN Loss: 5.062888145446777 | BCE Loss: 1.0335882902145386\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.0975189208984375 | KNN Loss: 5.063205718994141 | BCE Loss: 1.0343130826950073\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.046192646026611 | KNN Loss: 5.037384986877441 | BCE Loss: 1.00880765914917\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.034409999847412 | KNN Loss: 5.008790969848633 | BCE Loss: 1.0256191492080688\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.050683975219727 | KNN Loss: 5.019374847412109 | BCE Loss: 1.0313092470169067\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.063807964324951 | KNN Loss: 5.0144219398498535 | BCE Loss: 1.0493860244750977\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.034806728363037 | KNN Loss: 5.002774238586426 | BCE Loss: 1.0320323705673218\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.026242733001709 | KNN Loss: 5.002858638763428 | BCE Loss: 1.0233842134475708\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.058009624481201 | KNN Loss: 5.01425313949585 | BCE Loss: 1.043756365776062\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 5.997819900512695 | KNN Loss: 5.016757965087891 | BCE Loss: 0.9810616970062256\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 6.101841926574707 | KNN Loss: 5.07564640045166 | BCE Loss: 1.0261955261230469\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.041313171386719 | KNN Loss: 5.017210483551025 | BCE Loss: 1.0241026878356934\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.113307952880859 | KNN Loss: 5.059319972991943 | BCE Loss: 1.053987979888916\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.048368453979492 | KNN Loss: 5.025550365447998 | BCE Loss: 1.0228179693222046\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.024661064147949 | KNN Loss: 5.007286071777344 | BCE Loss: 1.017375111579895\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.033902645111084 | KNN Loss: 5.0185160636901855 | BCE Loss: 1.0153865814208984\n",
      "Epoch   426: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.104689598083496 | KNN Loss: 5.077276706695557 | BCE Loss: 1.0274131298065186\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.090522766113281 | KNN Loss: 5.05617094039917 | BCE Loss: 1.0343515872955322\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.075517177581787 | KNN Loss: 5.052299499511719 | BCE Loss: 1.0232176780700684\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.043127059936523 | KNN Loss: 5.001810073852539 | BCE Loss: 1.0413172245025635\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.1001362800598145 | KNN Loss: 5.031892776489258 | BCE Loss: 1.0682436227798462\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 6.037461280822754 | KNN Loss: 5.00875186920166 | BCE Loss: 1.0287091732025146\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.06233024597168 | KNN Loss: 5.049135208129883 | BCE Loss: 1.0131951570510864\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.1032185554504395 | KNN Loss: 5.045176982879639 | BCE Loss: 1.0580415725708008\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.01021671295166 | KNN Loss: 4.985289096832275 | BCE Loss: 1.0249277353286743\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.06523323059082 | KNN Loss: 5.015614032745361 | BCE Loss: 1.049619436264038\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.025649070739746 | KNN Loss: 4.998655796051025 | BCE Loss: 1.0269930362701416\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.036847114562988 | KNN Loss: 5.013617515563965 | BCE Loss: 1.0232293605804443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.076841354370117 | KNN Loss: 5.057008266448975 | BCE Loss: 1.0198332071304321\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.023669242858887 | KNN Loss: 5.001410961151123 | BCE Loss: 1.0222581624984741\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.071459770202637 | KNN Loss: 5.016894817352295 | BCE Loss: 1.0545647144317627\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.073071479797363 | KNN Loss: 5.026872158050537 | BCE Loss: 1.0461993217468262\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.101875305175781 | KNN Loss: 5.075786590576172 | BCE Loss: 1.0260885953903198\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.150151252746582 | KNN Loss: 5.045929431915283 | BCE Loss: 1.1042218208312988\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.076177597045898 | KNN Loss: 5.016018390655518 | BCE Loss: 1.0601592063903809\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.046045303344727 | KNN Loss: 5.01049280166626 | BCE Loss: 1.0355522632598877\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.035295009613037 | KNN Loss: 5.018273830413818 | BCE Loss: 1.0170212984085083\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.064949035644531 | KNN Loss: 5.033724308013916 | BCE Loss: 1.0312247276306152\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.054836750030518 | KNN Loss: 5.031427383422852 | BCE Loss: 1.0234094858169556\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.02860164642334 | KNN Loss: 5.019764423370361 | BCE Loss: 1.0088369846343994\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.065349102020264 | KNN Loss: 5.012959003448486 | BCE Loss: 1.0523900985717773\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.059947490692139 | KNN Loss: 5.036473274230957 | BCE Loss: 1.023474097251892\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.028505802154541 | KNN Loss: 5.0059285163879395 | BCE Loss: 1.0225774049758911\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.0164713859558105 | KNN Loss: 5.0141730308532715 | BCE Loss: 1.002298355102539\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.084497451782227 | KNN Loss: 5.0269927978515625 | BCE Loss: 1.057504653930664\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.034752368927002 | KNN Loss: 5.008485317230225 | BCE Loss: 1.0262670516967773\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.025240898132324 | KNN Loss: 5.016820907592773 | BCE Loss: 1.0084197521209717\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.070988655090332 | KNN Loss: 5.034268379211426 | BCE Loss: 1.0367200374603271\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 6.033541679382324 | KNN Loss: 5.015058994293213 | BCE Loss: 1.0184824466705322\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.088195323944092 | KNN Loss: 5.039948463439941 | BCE Loss: 1.0482468605041504\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.0384840965271 | KNN Loss: 5.004681587219238 | BCE Loss: 1.0338026285171509\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.023303031921387 | KNN Loss: 4.9970927238464355 | BCE Loss: 1.0262101888656616\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.009810447692871 | KNN Loss: 5.008566856384277 | BCE Loss: 1.0012433528900146\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.011170387268066 | KNN Loss: 5.011013984680176 | BCE Loss: 1.0001564025878906\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 6.070141792297363 | KNN Loss: 5.034421443939209 | BCE Loss: 1.0357201099395752\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.010356903076172 | KNN Loss: 5.003599166870117 | BCE Loss: 1.0067578554153442\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.028592586517334 | KNN Loss: 4.995532989501953 | BCE Loss: 1.0330597162246704\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.01652717590332 | KNN Loss: 5.012327671051025 | BCE Loss: 1.0041992664337158\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.077027320861816 | KNN Loss: 5.026473045349121 | BCE Loss: 1.0505540370941162\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.047306060791016 | KNN Loss: 5.008033752441406 | BCE Loss: 1.0392720699310303\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.048237323760986 | KNN Loss: 5.017251491546631 | BCE Loss: 1.030985951423645\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.052923679351807 | KNN Loss: 5.0201849937438965 | BCE Loss: 1.0327386856079102\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.028801918029785 | KNN Loss: 5.010068416595459 | BCE Loss: 1.0187336206436157\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.057022571563721 | KNN Loss: 5.020491123199463 | BCE Loss: 1.0365315675735474\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.015382289886475 | KNN Loss: 5.001540184020996 | BCE Loss: 1.013841986656189\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.021585941314697 | KNN Loss: 4.996038913726807 | BCE Loss: 1.0255470275878906\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.051430702209473 | KNN Loss: 5.032647132873535 | BCE Loss: 1.018783450126648\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.069609642028809 | KNN Loss: 5.022974967956543 | BCE Loss: 1.0466346740722656\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.012518405914307 | KNN Loss: 5.022435665130615 | BCE Loss: 0.9900826811790466\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.0546135902404785 | KNN Loss: 5.032315254211426 | BCE Loss: 1.0222982168197632\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.043013572692871 | KNN Loss: 4.999469757080078 | BCE Loss: 1.043543815612793\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.037383079528809 | KNN Loss: 5.001426696777344 | BCE Loss: 1.0359561443328857\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 6.031790256500244 | KNN Loss: 4.998778343200684 | BCE Loss: 1.033011794090271\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.037388801574707 | KNN Loss: 5.021296977996826 | BCE Loss: 1.0160918235778809\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.068703651428223 | KNN Loss: 5.038283824920654 | BCE Loss: 1.0304200649261475\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.092152118682861 | KNN Loss: 5.0430827140808105 | BCE Loss: 1.0490694046020508\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.056014537811279 | KNN Loss: 5.034860610961914 | BCE Loss: 1.0211540460586548\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.028387069702148 | KNN Loss: 5.03226900100708 | BCE Loss: 0.9961183071136475\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.035615921020508 | KNN Loss: 5.022088050842285 | BCE Loss: 1.0135278701782227\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.024939060211182 | KNN Loss: 5.002897262573242 | BCE Loss: 1.0220417976379395\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.070836067199707 | KNN Loss: 5.031658172607422 | BCE Loss: 1.039177656173706\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.047730445861816 | KNN Loss: 5.001031875610352 | BCE Loss: 1.046698808670044\n",
      "Epoch   437: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 6.020482063293457 | KNN Loss: 5.014023780822754 | BCE Loss: 1.0064582824707031\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.05974006652832 | KNN Loss: 5.015624523162842 | BCE Loss: 1.0441153049468994\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.066458702087402 | KNN Loss: 5.039115905761719 | BCE Loss: 1.0273429155349731\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.044635772705078 | KNN Loss: 5.027426242828369 | BCE Loss: 1.0172096490859985\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.079850196838379 | KNN Loss: 5.043691158294678 | BCE Loss: 1.0361589193344116\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.041481971740723 | KNN Loss: 5.003951549530029 | BCE Loss: 1.0375301837921143\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.075479507446289 | KNN Loss: 5.0238752365112305 | BCE Loss: 1.0516045093536377\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.0527119636535645 | KNN Loss: 5.0286760330200195 | BCE Loss: 1.024035930633545\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.042864799499512 | KNN Loss: 5.015194416046143 | BCE Loss: 1.0276702642440796\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.051826477050781 | KNN Loss: 5.01162052154541 | BCE Loss: 1.040205717086792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.008277893066406 | KNN Loss: 4.991856098175049 | BCE Loss: 1.0164217948913574\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.041222095489502 | KNN Loss: 5.026523113250732 | BCE Loss: 1.0146989822387695\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.041961669921875 | KNN Loss: 5.00853967666626 | BCE Loss: 1.0334219932556152\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.048149585723877 | KNN Loss: 5.025080680847168 | BCE Loss: 1.0230690240859985\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.0717058181762695 | KNN Loss: 5.030248641967773 | BCE Loss: 1.041456937789917\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.019771099090576 | KNN Loss: 5.012335300445557 | BCE Loss: 1.0074357986450195\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.0255889892578125 | KNN Loss: 5.029683589935303 | BCE Loss: 0.9959052801132202\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.047815322875977 | KNN Loss: 5.0191755294799805 | BCE Loss: 1.0286399126052856\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.0499186515808105 | KNN Loss: 5.038600921630859 | BCE Loss: 1.0113178491592407\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.125396728515625 | KNN Loss: 5.083043098449707 | BCE Loss: 1.0423533916473389\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.043344497680664 | KNN Loss: 5.0201945304870605 | BCE Loss: 1.0231497287750244\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.101332664489746 | KNN Loss: 5.032806873321533 | BCE Loss: 1.0685255527496338\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.056816101074219 | KNN Loss: 5.00474739074707 | BCE Loss: 1.0520689487457275\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.034820556640625 | KNN Loss: 5.004284381866455 | BCE Loss: 1.0305359363555908\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.0251665115356445 | KNN Loss: 5.0138750076293945 | BCE Loss: 1.011291742324829\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.053305149078369 | KNN Loss: 5.034394264221191 | BCE Loss: 1.0189108848571777\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.149953365325928 | KNN Loss: 5.082412242889404 | BCE Loss: 1.067541241645813\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.052364349365234 | KNN Loss: 5.025867938995361 | BCE Loss: 1.026496410369873\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.036973476409912 | KNN Loss: 5.009385585784912 | BCE Loss: 1.027587890625\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.014930248260498 | KNN Loss: 5.001840114593506 | BCE Loss: 1.0130901336669922\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.034017562866211 | KNN Loss: 5.015202522277832 | BCE Loss: 1.018815279006958\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.0425825119018555 | KNN Loss: 5.04492712020874 | BCE Loss: 0.9976552128791809\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.053928852081299 | KNN Loss: 5.0403218269348145 | BCE Loss: 1.013607144355774\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.082996845245361 | KNN Loss: 5.032905101776123 | BCE Loss: 1.0500917434692383\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.060543060302734 | KNN Loss: 5.022770881652832 | BCE Loss: 1.0377721786499023\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.055331707000732 | KNN Loss: 5.034236431121826 | BCE Loss: 1.0210952758789062\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.127524375915527 | KNN Loss: 5.050675392150879 | BCE Loss: 1.0768489837646484\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 6.031689643859863 | KNN Loss: 5.01220703125 | BCE Loss: 1.0194826126098633\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.047727584838867 | KNN Loss: 5.017082214355469 | BCE Loss: 1.0306452512741089\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.043213844299316 | KNN Loss: 5.022350788116455 | BCE Loss: 1.0208628177642822\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.066335678100586 | KNN Loss: 5.04908561706543 | BCE Loss: 1.0172502994537354\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.004911422729492 | KNN Loss: 5.003107070922852 | BCE Loss: 1.0018044710159302\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.044449329376221 | KNN Loss: 5.019258499145508 | BCE Loss: 1.025190830230713\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.068915843963623 | KNN Loss: 5.023101806640625 | BCE Loss: 1.045814037322998\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.069122314453125 | KNN Loss: 5.027724266052246 | BCE Loss: 1.041398048400879\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.058666706085205 | KNN Loss: 5.01972770690918 | BCE Loss: 1.0389389991760254\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.044608116149902 | KNN Loss: 5.022158622741699 | BCE Loss: 1.0224494934082031\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.077118396759033 | KNN Loss: 5.027145862579346 | BCE Loss: 1.0499725341796875\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.054958343505859 | KNN Loss: 5.020273685455322 | BCE Loss: 1.0346847772598267\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.008262634277344 | KNN Loss: 5.001351356506348 | BCE Loss: 1.006911039352417\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.009008407592773 | KNN Loss: 5.0124125480651855 | BCE Loss: 0.9965956807136536\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.0598039627075195 | KNN Loss: 5.017751693725586 | BCE Loss: 1.0420520305633545\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.061548233032227 | KNN Loss: 5.017724990844727 | BCE Loss: 1.043823003768921\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.041595458984375 | KNN Loss: 5.026947021484375 | BCE Loss: 1.014648675918579\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.076788902282715 | KNN Loss: 5.057151794433594 | BCE Loss: 1.019636869430542\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.027362823486328 | KNN Loss: 5.010335922241211 | BCE Loss: 1.0170267820358276\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.05528450012207 | KNN Loss: 5.021243095397949 | BCE Loss: 1.0340416431427002\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.059003829956055 | KNN Loss: 5.020688533782959 | BCE Loss: 1.0383150577545166\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 6.012365341186523 | KNN Loss: 5.005373001098633 | BCE Loss: 1.0069921016693115\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.082882881164551 | KNN Loss: 5.030796051025391 | BCE Loss: 1.0520869493484497\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.0433735847473145 | KNN Loss: 5.010431289672852 | BCE Loss: 1.0329424142837524\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.017457008361816 | KNN Loss: 5.004854202270508 | BCE Loss: 1.0126025676727295\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.071154594421387 | KNN Loss: 5.026689052581787 | BCE Loss: 1.0444656610488892\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.058832168579102 | KNN Loss: 5.044139385223389 | BCE Loss: 1.0146925449371338\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 6.055754661560059 | KNN Loss: 5.023595333099365 | BCE Loss: 1.032159447669983\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 6.0397138595581055 | KNN Loss: 5.019975185394287 | BCE Loss: 1.0197386741638184\n",
      "Epoch   448: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.050179481506348 | KNN Loss: 5.016088008880615 | BCE Loss: 1.0340917110443115\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.035785675048828 | KNN Loss: 5.009734630584717 | BCE Loss: 1.0260512828826904\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.017702102661133 | KNN Loss: 5.010304927825928 | BCE Loss: 1.007396936416626\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.050520896911621 | KNN Loss: 5.020082473754883 | BCE Loss: 1.0304384231567383\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.029397964477539 | KNN Loss: 5.007993221282959 | BCE Loss: 1.02140474319458\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.045258522033691 | KNN Loss: 5.01408576965332 | BCE Loss: 1.0311729907989502\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.04004430770874 | KNN Loss: 5.010822296142578 | BCE Loss: 1.0292218923568726\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.055142879486084 | KNN Loss: 5.0185770988464355 | BCE Loss: 1.0365657806396484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.052653789520264 | KNN Loss: 5.023716449737549 | BCE Loss: 1.0289373397827148\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 6.122894287109375 | KNN Loss: 5.069304943084717 | BCE Loss: 1.0535892248153687\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 6.049287796020508 | KNN Loss: 5.017678737640381 | BCE Loss: 1.031609296798706\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 6.062922477722168 | KNN Loss: 5.031908988952637 | BCE Loss: 1.0310134887695312\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.0268168449401855 | KNN Loss: 5.01405668258667 | BCE Loss: 1.0127602815628052\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.042359352111816 | KNN Loss: 5.026082992553711 | BCE Loss: 1.0162763595581055\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.0299601554870605 | KNN Loss: 5.010076522827148 | BCE Loss: 1.0198837518692017\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.058023929595947 | KNN Loss: 5.025526523590088 | BCE Loss: 1.0324974060058594\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 5.989882469177246 | KNN Loss: 5.004345417022705 | BCE Loss: 0.985537052154541\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.068272113800049 | KNN Loss: 5.062596797943115 | BCE Loss: 1.0056753158569336\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.044963836669922 | KNN Loss: 4.995088577270508 | BCE Loss: 1.0498753786087036\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.029229164123535 | KNN Loss: 5.005948066711426 | BCE Loss: 1.0232810974121094\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 6.034907817840576 | KNN Loss: 5.0001020431518555 | BCE Loss: 1.0348057746887207\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 6.062431335449219 | KNN Loss: 5.02971076965332 | BCE Loss: 1.0327203273773193\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 6.0606842041015625 | KNN Loss: 5.019529342651367 | BCE Loss: 1.0411550998687744\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.026573181152344 | KNN Loss: 4.998036861419678 | BCE Loss: 1.0285362005233765\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.084871768951416 | KNN Loss: 5.02569580078125 | BCE Loss: 1.059175968170166\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.04152250289917 | KNN Loss: 5.001679420471191 | BCE Loss: 1.039843201637268\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.059747695922852 | KNN Loss: 5.020242691040039 | BCE Loss: 1.039505124092102\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.052830696105957 | KNN Loss: 5.013400554656982 | BCE Loss: 1.0394303798675537\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 5.997838973999023 | KNN Loss: 4.992154598236084 | BCE Loss: 1.005684494972229\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.046781539916992 | KNN Loss: 5.017162322998047 | BCE Loss: 1.0296193361282349\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.036942481994629 | KNN Loss: 5.015824794769287 | BCE Loss: 1.0211174488067627\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.048921585083008 | KNN Loss: 5.008326530456543 | BCE Loss: 1.040595293045044\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.0636396408081055 | KNN Loss: 5.01915979385376 | BCE Loss: 1.0444796085357666\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.085853576660156 | KNN Loss: 5.054268836975098 | BCE Loss: 1.0315847396850586\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.063719272613525 | KNN Loss: 5.026493072509766 | BCE Loss: 1.0372263193130493\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.033979415893555 | KNN Loss: 5.026904582977295 | BCE Loss: 1.0070750713348389\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 6.039639472961426 | KNN Loss: 5.0043487548828125 | BCE Loss: 1.0352907180786133\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.101785659790039 | KNN Loss: 5.0261125564575195 | BCE Loss: 1.0756733417510986\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 6.0517401695251465 | KNN Loss: 5.032716274261475 | BCE Loss: 1.0190238952636719\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 6.01090669631958 | KNN Loss: 5.006887912750244 | BCE Loss: 1.004018783569336\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.026637077331543 | KNN Loss: 4.991508483886719 | BCE Loss: 1.0351283550262451\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.032438278198242 | KNN Loss: 5.0029778480529785 | BCE Loss: 1.0294605493545532\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.0676798820495605 | KNN Loss: 5.0306077003479 | BCE Loss: 1.0370721817016602\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.073553085327148 | KNN Loss: 5.015198230743408 | BCE Loss: 1.0583548545837402\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.025750160217285 | KNN Loss: 5.004255294799805 | BCE Loss: 1.0214946269989014\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.039186954498291 | KNN Loss: 5.049084186553955 | BCE Loss: 0.9901025891304016\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.0245771408081055 | KNN Loss: 4.99681282043457 | BCE Loss: 1.027764081954956\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.0554375648498535 | KNN Loss: 5.038564682006836 | BCE Loss: 1.0168728828430176\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.025597095489502 | KNN Loss: 5.027839660644531 | BCE Loss: 0.9977575540542603\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.058576583862305 | KNN Loss: 5.026867866516113 | BCE Loss: 1.0317087173461914\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.0094218254089355 | KNN Loss: 5.013257026672363 | BCE Loss: 0.9961647987365723\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.03434944152832 | KNN Loss: 5.018540859222412 | BCE Loss: 1.015808343887329\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 6.051346778869629 | KNN Loss: 5.015580654144287 | BCE Loss: 1.0357661247253418\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 6.109063148498535 | KNN Loss: 5.039582252502441 | BCE Loss: 1.0694806575775146\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.023468494415283 | KNN Loss: 5.010806560516357 | BCE Loss: 1.0126619338989258\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.089086055755615 | KNN Loss: 5.0266923904418945 | BCE Loss: 1.0623935461044312\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.047788619995117 | KNN Loss: 5.051022529602051 | BCE Loss: 0.9967659115791321\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.042385101318359 | KNN Loss: 5.0277419090271 | BCE Loss: 1.0146434307098389\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.052073955535889 | KNN Loss: 5.025931358337402 | BCE Loss: 1.0261425971984863\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.010393142700195 | KNN Loss: 4.995589256286621 | BCE Loss: 1.0148036479949951\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.045421600341797 | KNN Loss: 5.018679618835449 | BCE Loss: 1.0267422199249268\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.0289998054504395 | KNN Loss: 5.003572463989258 | BCE Loss: 1.0254274606704712\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.058507919311523 | KNN Loss: 5.032559394836426 | BCE Loss: 1.0259487628936768\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.072445869445801 | KNN Loss: 5.030196189880371 | BCE Loss: 1.0422495603561401\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.083282470703125 | KNN Loss: 5.017295837402344 | BCE Loss: 1.0659866333007812\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.06870698928833 | KNN Loss: 5.0204243659973145 | BCE Loss: 1.048282504081726\n",
      "Epoch   459: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.037017822265625 | KNN Loss: 5.018016815185547 | BCE Loss: 1.019000768661499\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.042252063751221 | KNN Loss: 5.022651672363281 | BCE Loss: 1.0196003913879395\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.082536697387695 | KNN Loss: 5.041085243225098 | BCE Loss: 1.0414512157440186\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.0074920654296875 | KNN Loss: 5.008162498474121 | BCE Loss: 0.9993293881416321\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.04011344909668 | KNN Loss: 5.009034633636475 | BCE Loss: 1.0310789346694946\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.038084030151367 | KNN Loss: 5.016907215118408 | BCE Loss: 1.0211765766143799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.033576965332031 | KNN Loss: 5.009700775146484 | BCE Loss: 1.0238761901855469\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.110397815704346 | KNN Loss: 5.0677337646484375 | BCE Loss: 1.0426640510559082\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 6.038633346557617 | KNN Loss: 5.005651950836182 | BCE Loss: 1.0329811573028564\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.037411689758301 | KNN Loss: 5.013095855712891 | BCE Loss: 1.024315595626831\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.04649019241333 | KNN Loss: 5.004507541656494 | BCE Loss: 1.041982650756836\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.074123859405518 | KNN Loss: 5.028656005859375 | BCE Loss: 1.045467734336853\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.081554889678955 | KNN Loss: 5.041596412658691 | BCE Loss: 1.0399584770202637\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.045753479003906 | KNN Loss: 5.027709007263184 | BCE Loss: 1.0180442333221436\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.060490608215332 | KNN Loss: 5.027528762817383 | BCE Loss: 1.0329618453979492\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.089875221252441 | KNN Loss: 5.0446577072143555 | BCE Loss: 1.045217752456665\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.0562005043029785 | KNN Loss: 5.019097805023193 | BCE Loss: 1.0371026992797852\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.062216758728027 | KNN Loss: 5.014285564422607 | BCE Loss: 1.04793119430542\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.013789176940918 | KNN Loss: 5.008432865142822 | BCE Loss: 1.0053563117980957\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.068832874298096 | KNN Loss: 5.029958248138428 | BCE Loss: 1.0388747453689575\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.0517377853393555 | KNN Loss: 5.028598785400391 | BCE Loss: 1.0231387615203857\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.031818389892578 | KNN Loss: 5.016471862792969 | BCE Loss: 1.0153467655181885\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.012781143188477 | KNN Loss: 5.012218475341797 | BCE Loss: 1.0005624294281006\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.038726806640625 | KNN Loss: 5.002991676330566 | BCE Loss: 1.035735011100769\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 6.061642646789551 | KNN Loss: 5.04458475112915 | BCE Loss: 1.0170578956604004\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.0753655433654785 | KNN Loss: 5.027590274810791 | BCE Loss: 1.0477752685546875\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.094563961029053 | KNN Loss: 5.062161445617676 | BCE Loss: 1.032402515411377\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.01836633682251 | KNN Loss: 5.012591361999512 | BCE Loss: 1.005774974822998\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.047426223754883 | KNN Loss: 5.02990198135376 | BCE Loss: 1.0175244808197021\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.028704643249512 | KNN Loss: 5.035836219787598 | BCE Loss: 0.9928684234619141\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.05530309677124 | KNN Loss: 5.029850959777832 | BCE Loss: 1.0254521369934082\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 6.068756103515625 | KNN Loss: 5.0380401611328125 | BCE Loss: 1.0307157039642334\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.033045768737793 | KNN Loss: 5.016739845275879 | BCE Loss: 1.016305685043335\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.031776428222656 | KNN Loss: 5.0071587562561035 | BCE Loss: 1.0246176719665527\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.027760028839111 | KNN Loss: 5.011147975921631 | BCE Loss: 1.0166120529174805\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.018155097961426 | KNN Loss: 5.0010175704956055 | BCE Loss: 1.0171377658843994\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.032442569732666 | KNN Loss: 5.015876293182373 | BCE Loss: 1.016566276550293\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.040865898132324 | KNN Loss: 5.034226894378662 | BCE Loss: 1.0066392421722412\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.06777286529541 | KNN Loss: 5.0367937088012695 | BCE Loss: 1.0309793949127197\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.054003715515137 | KNN Loss: 5.0352935791015625 | BCE Loss: 1.0187100172042847\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.0338544845581055 | KNN Loss: 5.003449440002441 | BCE Loss: 1.030405044555664\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.066935062408447 | KNN Loss: 5.056751728057861 | BCE Loss: 1.010183334350586\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.12516975402832 | KNN Loss: 5.054999351501465 | BCE Loss: 1.0701704025268555\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 6.054708480834961 | KNN Loss: 5.024243354797363 | BCE Loss: 1.0304648876190186\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.024521827697754 | KNN Loss: 5.001305103302002 | BCE Loss: 1.0232164859771729\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.08027458190918 | KNN Loss: 5.017616271972656 | BCE Loss: 1.0626585483551025\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.09822416305542 | KNN Loss: 5.038902759552002 | BCE Loss: 1.0593215227127075\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.03536319732666 | KNN Loss: 5.016729354858398 | BCE Loss: 1.0186336040496826\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.041073799133301 | KNN Loss: 5.006129264831543 | BCE Loss: 1.0349445343017578\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.067204475402832 | KNN Loss: 5.015450477600098 | BCE Loss: 1.0517539978027344\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 6.025479316711426 | KNN Loss: 5.009814739227295 | BCE Loss: 1.0156643390655518\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.067027568817139 | KNN Loss: 5.038728713989258 | BCE Loss: 1.0282989740371704\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.0959272384643555 | KNN Loss: 5.054360389709473 | BCE Loss: 1.0415668487548828\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.067912578582764 | KNN Loss: 5.019392967224121 | BCE Loss: 1.0485197305679321\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.062770843505859 | KNN Loss: 5.026101589202881 | BCE Loss: 1.036669135093689\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.0299224853515625 | KNN Loss: 5.003222942352295 | BCE Loss: 1.026699423789978\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.0916900634765625 | KNN Loss: 5.077066421508789 | BCE Loss: 1.0146234035491943\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.054492473602295 | KNN Loss: 5.019484043121338 | BCE Loss: 1.035008430480957\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.017423629760742 | KNN Loss: 5.006501197814941 | BCE Loss: 1.0109221935272217\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.0416154861450195 | KNN Loss: 5.004275798797607 | BCE Loss: 1.037339687347412\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.0563507080078125 | KNN Loss: 5.033310890197754 | BCE Loss: 1.0230395793914795\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.010687828063965 | KNN Loss: 5.010196208953857 | BCE Loss: 1.000491738319397\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.067668914794922 | KNN Loss: 5.037190914154053 | BCE Loss: 1.03047776222229\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.072877883911133 | KNN Loss: 5.024429798126221 | BCE Loss: 1.048448085784912\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.0265655517578125 | KNN Loss: 5.004122257232666 | BCE Loss: 1.022443413734436\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.018028259277344 | KNN Loss: 5.014825344085693 | BCE Loss: 1.0032029151916504\n",
      "Epoch   470: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.027817726135254 | KNN Loss: 5.011858940124512 | BCE Loss: 1.0159590244293213\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.086718559265137 | KNN Loss: 5.035441875457764 | BCE Loss: 1.051276683807373\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.054335594177246 | KNN Loss: 5.016458034515381 | BCE Loss: 1.0378773212432861\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.0685224533081055 | KNN Loss: 5.034399032592773 | BCE Loss: 1.0341235399246216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.034987926483154 | KNN Loss: 5.00231409072876 | BCE Loss: 1.0326738357543945\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.0556440353393555 | KNN Loss: 5.0288166999816895 | BCE Loss: 1.0268275737762451\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.021641731262207 | KNN Loss: 5.002894401550293 | BCE Loss: 1.018747329711914\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.066617012023926 | KNN Loss: 5.026043891906738 | BCE Loss: 1.040573239326477\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.042706489562988 | KNN Loss: 5.02046012878418 | BCE Loss: 1.0222465991973877\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.03903865814209 | KNN Loss: 5.010788917541504 | BCE Loss: 1.0282495021820068\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.0509467124938965 | KNN Loss: 5.029558181762695 | BCE Loss: 1.0213885307312012\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.046949863433838 | KNN Loss: 5.011531352996826 | BCE Loss: 1.0354183912277222\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.034066200256348 | KNN Loss: 5.016493797302246 | BCE Loss: 1.0175724029541016\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.0534162521362305 | KNN Loss: 5.0182647705078125 | BCE Loss: 1.0351513624191284\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.033771514892578 | KNN Loss: 5.029760360717773 | BCE Loss: 1.0040112733840942\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.050756454467773 | KNN Loss: 5.004673004150391 | BCE Loss: 1.0460833311080933\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.0338239669799805 | KNN Loss: 5.014169216156006 | BCE Loss: 1.0196549892425537\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.025573253631592 | KNN Loss: 5.01906681060791 | BCE Loss: 1.0065064430236816\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.053648948669434 | KNN Loss: 5.022495746612549 | BCE Loss: 1.0311529636383057\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.047246932983398 | KNN Loss: 5.002492427825928 | BCE Loss: 1.0447542667388916\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.039307594299316 | KNN Loss: 5.034867286682129 | BCE Loss: 1.0044405460357666\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.041071891784668 | KNN Loss: 5.0090813636779785 | BCE Loss: 1.0319905281066895\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.075135707855225 | KNN Loss: 5.020216464996338 | BCE Loss: 1.0549193620681763\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.02895975112915 | KNN Loss: 5.020210266113281 | BCE Loss: 1.0087494850158691\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.101337432861328 | KNN Loss: 5.050699234008789 | BCE Loss: 1.0506383180618286\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.03277063369751 | KNN Loss: 5.002374649047852 | BCE Loss: 1.0303961038589478\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.04735803604126 | KNN Loss: 5.005050182342529 | BCE Loss: 1.0423078536987305\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.080410957336426 | KNN Loss: 5.08044958114624 | BCE Loss: 0.9999614953994751\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.0248613357543945 | KNN Loss: 5.007169246673584 | BCE Loss: 1.0176923274993896\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.0447468757629395 | KNN Loss: 5.024531841278076 | BCE Loss: 1.0202150344848633\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.104501724243164 | KNN Loss: 5.032838344573975 | BCE Loss: 1.0716633796691895\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.053284645080566 | KNN Loss: 5.022937774658203 | BCE Loss: 1.0303466320037842\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.0370988845825195 | KNN Loss: 5.000469207763672 | BCE Loss: 1.0366294384002686\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.042874336242676 | KNN Loss: 5.007723808288574 | BCE Loss: 1.035150408744812\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.030363082885742 | KNN Loss: 5.023688316345215 | BCE Loss: 1.0066750049591064\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.001755714416504 | KNN Loss: 5.004125595092773 | BCE Loss: 0.9976303577423096\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.0285444259643555 | KNN Loss: 5.007994174957275 | BCE Loss: 1.0205503702163696\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.050103187561035 | KNN Loss: 5.019429683685303 | BCE Loss: 1.0306737422943115\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.026785850524902 | KNN Loss: 5.018344402313232 | BCE Loss: 1.0084413290023804\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.033322334289551 | KNN Loss: 4.994568824768066 | BCE Loss: 1.0387533903121948\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.056765556335449 | KNN Loss: 5.020605087280273 | BCE Loss: 1.0361604690551758\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.0461039543151855 | KNN Loss: 5.03074836730957 | BCE Loss: 1.0153555870056152\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.064896106719971 | KNN Loss: 5.024940013885498 | BCE Loss: 1.0399560928344727\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.039397239685059 | KNN Loss: 5.008128643035889 | BCE Loss: 1.0312683582305908\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.022252559661865 | KNN Loss: 5.034415245056152 | BCE Loss: 0.9878371953964233\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.036865234375 | KNN Loss: 4.995289325714111 | BCE Loss: 1.0415759086608887\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.0784993171691895 | KNN Loss: 5.054257869720459 | BCE Loss: 1.0242414474487305\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.093906879425049 | KNN Loss: 5.0434112548828125 | BCE Loss: 1.0504957437515259\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.104393005371094 | KNN Loss: 5.024735450744629 | BCE Loss: 1.0796575546264648\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.0647125244140625 | KNN Loss: 5.022246837615967 | BCE Loss: 1.0424654483795166\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.056758403778076 | KNN Loss: 5.011549472808838 | BCE Loss: 1.0452090501785278\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.046398639678955 | KNN Loss: 5.019364833831787 | BCE Loss: 1.0270339250564575\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.027062892913818 | KNN Loss: 5.008392810821533 | BCE Loss: 1.0186702013015747\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.071051597595215 | KNN Loss: 5.04164457321167 | BCE Loss: 1.029407262802124\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.094932556152344 | KNN Loss: 5.072340965270996 | BCE Loss: 1.0225917100906372\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 6.048106670379639 | KNN Loss: 5.021732807159424 | BCE Loss: 1.0263738632202148\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.117671012878418 | KNN Loss: 5.084163665771484 | BCE Loss: 1.0335071086883545\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.116022109985352 | KNN Loss: 5.069571495056152 | BCE Loss: 1.0464508533477783\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.041328430175781 | KNN Loss: 5.039618015289307 | BCE Loss: 1.0017101764678955\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.061783790588379 | KNN Loss: 5.0320329666137695 | BCE Loss: 1.0297508239746094\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.032535076141357 | KNN Loss: 4.996962070465088 | BCE Loss: 1.0355730056762695\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.039243698120117 | KNN Loss: 5.0132975578308105 | BCE Loss: 1.0259459018707275\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.019862174987793 | KNN Loss: 5.00042200088501 | BCE Loss: 1.0194401741027832\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.03891658782959 | KNN Loss: 5.01266622543335 | BCE Loss: 1.0262503623962402\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 6.045454978942871 | KNN Loss: 5.015328884124756 | BCE Loss: 1.0301260948181152\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.078444957733154 | KNN Loss: 5.021947383880615 | BCE Loss: 1.056497573852539\n",
      "Epoch   481: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.011929512023926 | KNN Loss: 4.99990701675415 | BCE Loss: 1.0120224952697754\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.093661785125732 | KNN Loss: 5.074915409088135 | BCE Loss: 1.018746256828308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.034120082855225 | KNN Loss: 5.0099592208862305 | BCE Loss: 1.0241607427597046\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.052731513977051 | KNN Loss: 5.0125813484191895 | BCE Loss: 1.0401499271392822\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.090795040130615 | KNN Loss: 5.0254292488098145 | BCE Loss: 1.0653659105300903\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.014037132263184 | KNN Loss: 5.001946926116943 | BCE Loss: 1.0120902061462402\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.0226545333862305 | KNN Loss: 5.007966995239258 | BCE Loss: 1.0146877765655518\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.066821098327637 | KNN Loss: 5.047447204589844 | BCE Loss: 1.019373893737793\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.072332859039307 | KNN Loss: 5.006659507751465 | BCE Loss: 1.0656733512878418\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.0557942390441895 | KNN Loss: 5.0212788581848145 | BCE Loss: 1.0345152616500854\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.071021556854248 | KNN Loss: 5.054277420043945 | BCE Loss: 1.0167441368103027\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.04716682434082 | KNN Loss: 5.040501117706299 | BCE Loss: 1.0066657066345215\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.037487030029297 | KNN Loss: 5.012407302856445 | BCE Loss: 1.0250799655914307\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.077849388122559 | KNN Loss: 5.024058818817139 | BCE Loss: 1.05379056930542\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.058778762817383 | KNN Loss: 5.013197422027588 | BCE Loss: 1.0455812215805054\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.019198417663574 | KNN Loss: 5.000685214996338 | BCE Loss: 1.0185132026672363\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.023955821990967 | KNN Loss: 5.022160530090332 | BCE Loss: 1.0017954111099243\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.06773567199707 | KNN Loss: 5.018429279327393 | BCE Loss: 1.0493061542510986\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.0872111320495605 | KNN Loss: 5.051792621612549 | BCE Loss: 1.0354185104370117\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.021790981292725 | KNN Loss: 5.014396667480469 | BCE Loss: 1.0073943138122559\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.0741682052612305 | KNN Loss: 5.037402629852295 | BCE Loss: 1.0367655754089355\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.011818885803223 | KNN Loss: 5.008322238922119 | BCE Loss: 1.0034966468811035\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.050553321838379 | KNN Loss: 5.011416435241699 | BCE Loss: 1.0391368865966797\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.01450252532959 | KNN Loss: 5.010674953460693 | BCE Loss: 1.003827452659607\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 6.037971496582031 | KNN Loss: 4.999650955200195 | BCE Loss: 1.0383206605911255\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.03471565246582 | KNN Loss: 5.011875629425049 | BCE Loss: 1.0228400230407715\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.027814865112305 | KNN Loss: 5.026355743408203 | BCE Loss: 1.0014591217041016\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.02992057800293 | KNN Loss: 5.013920783996582 | BCE Loss: 1.0159997940063477\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 6.061901092529297 | KNN Loss: 5.040279865264893 | BCE Loss: 1.0216212272644043\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.0372796058654785 | KNN Loss: 5.0100417137146 | BCE Loss: 1.027237892150879\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.070749282836914 | KNN Loss: 5.023712158203125 | BCE Loss: 1.04703688621521\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.038900852203369 | KNN Loss: 5.02934455871582 | BCE Loss: 1.0095562934875488\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.015384674072266 | KNN Loss: 5.016516208648682 | BCE Loss: 0.9988683462142944\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.074211120605469 | KNN Loss: 5.023532867431641 | BCE Loss: 1.0506783723831177\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.0739970207214355 | KNN Loss: 5.0228729248046875 | BCE Loss: 1.051124095916748\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.057178974151611 | KNN Loss: 5.011502265930176 | BCE Loss: 1.045676589012146\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.052457332611084 | KNN Loss: 5.044246196746826 | BCE Loss: 1.0082111358642578\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 5.992215156555176 | KNN Loss: 4.993653297424316 | BCE Loss: 0.9985620379447937\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.047983169555664 | KNN Loss: 5.013064384460449 | BCE Loss: 1.0349189043045044\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.0353498458862305 | KNN Loss: 5.0050859451293945 | BCE Loss: 1.0302636623382568\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.0691328048706055 | KNN Loss: 5.046080112457275 | BCE Loss: 1.02305269241333\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.024463653564453 | KNN Loss: 5.003634452819824 | BCE Loss: 1.020829200744629\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 6.034557342529297 | KNN Loss: 5.0166335105896 | BCE Loss: 1.0179240703582764\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.036547660827637 | KNN Loss: 5.008998870849609 | BCE Loss: 1.0275490283966064\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.052149772644043 | KNN Loss: 5.038741111755371 | BCE Loss: 1.0134084224700928\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.08419132232666 | KNN Loss: 5.034666538238525 | BCE Loss: 1.0495250225067139\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.070148944854736 | KNN Loss: 5.033183574676514 | BCE Loss: 1.0369654893875122\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.079798698425293 | KNN Loss: 5.048016548156738 | BCE Loss: 1.0317821502685547\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.0557637214660645 | KNN Loss: 5.011883735656738 | BCE Loss: 1.0438798666000366\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.084042072296143 | KNN Loss: 5.0249505043029785 | BCE Loss: 1.059091567993164\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.044636249542236 | KNN Loss: 5.0135650634765625 | BCE Loss: 1.0310710668563843\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 6.025520324707031 | KNN Loss: 5.024666786193848 | BCE Loss: 1.0008536577224731\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.050991058349609 | KNN Loss: 5.011805057525635 | BCE Loss: 1.039185881614685\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.0456695556640625 | KNN Loss: 5.038508415222168 | BCE Loss: 1.007161259651184\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.068131446838379 | KNN Loss: 5.019636154174805 | BCE Loss: 1.0484954118728638\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.060100078582764 | KNN Loss: 5.025411128997803 | BCE Loss: 1.0346888303756714\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.025854587554932 | KNN Loss: 5.01669979095459 | BCE Loss: 1.0091549158096313\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.056558609008789 | KNN Loss: 5.01628303527832 | BCE Loss: 1.0402755737304688\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.043072700500488 | KNN Loss: 5.008307933807373 | BCE Loss: 1.0347647666931152\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.0788068771362305 | KNN Loss: 5.02323579788208 | BCE Loss: 1.0555710792541504\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.033645153045654 | KNN Loss: 5.012506008148193 | BCE Loss: 1.021139144897461\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.02791690826416 | KNN Loss: 4.991006374359131 | BCE Loss: 1.0369104146957397\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.040767192840576 | KNN Loss: 5.014641761779785 | BCE Loss: 1.026125431060791\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.058036804199219 | KNN Loss: 5.038846492767334 | BCE Loss: 1.0191905498504639\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 5.99112606048584 | KNN Loss: 4.993773460388184 | BCE Loss: 0.9973523616790771\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.0789642333984375 | KNN Loss: 5.005762577056885 | BCE Loss: 1.0732018947601318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.100092887878418 | KNN Loss: 5.060546875 | BCE Loss: 1.039546012878418\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.032808303833008 | KNN Loss: 5.01011848449707 | BCE Loss: 1.022689938545227\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.03571891784668 | KNN Loss: 5.011983871459961 | BCE Loss: 1.0237348079681396\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.067623138427734 | KNN Loss: 5.043232440948486 | BCE Loss: 1.024390697479248\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.050644874572754 | KNN Loss: 5.029106616973877 | BCE Loss: 1.0215381383895874\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 5.991954803466797 | KNN Loss: 5.000690460205078 | BCE Loss: 0.9912642240524292\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.029138565063477 | KNN Loss: 5.0149712562561035 | BCE Loss: 1.014167070388794\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.039982795715332 | KNN Loss: 4.9985671043396 | BCE Loss: 1.0414159297943115\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.045782089233398 | KNN Loss: 5.02004337310791 | BCE Loss: 1.0257388353347778\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.0280303955078125 | KNN Loss: 5.013181209564209 | BCE Loss: 1.0148489475250244\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.040862560272217 | KNN Loss: 5.019313812255859 | BCE Loss: 1.021548867225647\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.042139053344727 | KNN Loss: 5.023141860961914 | BCE Loss: 1.0189971923828125\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.084764003753662 | KNN Loss: 5.0664143562316895 | BCE Loss: 1.0183497667312622\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.036713600158691 | KNN Loss: 4.998556137084961 | BCE Loss: 1.0381572246551514\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.066699028015137 | KNN Loss: 5.014860153198242 | BCE Loss: 1.051838994026184\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.045475006103516 | KNN Loss: 5.019083499908447 | BCE Loss: 1.0263912677764893\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 6.05193567276001 | KNN Loss: 5.026317596435547 | BCE Loss: 1.025618076324463\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.031466484069824 | KNN Loss: 5.01408052444458 | BCE Loss: 1.0173861980438232\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 6.031022071838379 | KNN Loss: 5.010586738586426 | BCE Loss: 1.0204355716705322\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.075562953948975 | KNN Loss: 5.017381191253662 | BCE Loss: 1.058181881904602\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.058551788330078 | KNN Loss: 5.029508113861084 | BCE Loss: 1.0290436744689941\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.079808712005615 | KNN Loss: 5.010982990264893 | BCE Loss: 1.0688257217407227\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.028964519500732 | KNN Loss: 5.027425289154053 | BCE Loss: 1.0015392303466797\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.082101821899414 | KNN Loss: 5.0442891120910645 | BCE Loss: 1.0378128290176392\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.044614791870117 | KNN Loss: 5.021523475646973 | BCE Loss: 1.0230915546417236\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.113393306732178 | KNN Loss: 5.060148239135742 | BCE Loss: 1.053244948387146\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.068212509155273 | KNN Loss: 5.0296950340271 | BCE Loss: 1.0385174751281738\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.056225776672363 | KNN Loss: 5.041078567504883 | BCE Loss: 1.0151472091674805\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.043546676635742 | KNN Loss: 5.030698776245117 | BCE Loss: 1.012847900390625\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.018394947052002 | KNN Loss: 5.009581565856934 | BCE Loss: 1.008813500404358\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 6.053135871887207 | KNN Loss: 5.012762069702148 | BCE Loss: 1.0403738021850586\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.0513153076171875 | KNN Loss: 5.019475936889648 | BCE Loss: 1.0318396091461182\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.041977405548096 | KNN Loss: 5.026862621307373 | BCE Loss: 1.015114665031433\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.02728271484375 | KNN Loss: 5.001636028289795 | BCE Loss: 1.0256469249725342\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.064998626708984 | KNN Loss: 5.0211896896362305 | BCE Loss: 1.0438090562820435\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.05989933013916 | KNN Loss: 5.022341251373291 | BCE Loss: 1.0375583171844482\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.037078380584717 | KNN Loss: 5.0066633224487305 | BCE Loss: 1.0304151773452759\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.077582836151123 | KNN Loss: 5.027459621429443 | BCE Loss: 1.0501232147216797\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.0230865478515625 | KNN Loss: 5.011548042297363 | BCE Loss: 1.0115383863449097\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.030332565307617 | KNN Loss: 5.012918472290039 | BCE Loss: 1.017413854598999\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.023695945739746 | KNN Loss: 5.014730453491211 | BCE Loss: 1.008965253829956\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.095553874969482 | KNN Loss: 5.036186695098877 | BCE Loss: 1.0593671798706055\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.022611618041992 | KNN Loss: 5.014405250549316 | BCE Loss: 1.0082063674926758\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.062873840332031 | KNN Loss: 5.014240741729736 | BCE Loss: 1.048633337020874\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.047286033630371 | KNN Loss: 5.025399684906006 | BCE Loss: 1.0218861103057861\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.028432369232178 | KNN Loss: 5.003688812255859 | BCE Loss: 1.0247434377670288\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.055245399475098 | KNN Loss: 5.010349750518799 | BCE Loss: 1.0448956489562988\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.045013904571533 | KNN Loss: 5.009430408477783 | BCE Loss: 1.03558349609375\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0212,  4.1637,  2.2793,  2.4290,  2.3282,  0.7648,  2.8325,  1.9331,\n",
      "          1.9076,  2.1568,  2.3861,  2.4074,  0.6492,  1.8556,  1.3833,  1.1246,\n",
      "          2.4802,  3.3786,  2.0662,  2.5517,  1.3852,  3.1902,  2.0022,  2.8558,\n",
      "          2.2824,  1.8604,  2.3070,  1.5351,  1.6248,  0.4055, -0.2656,  0.5114,\n",
      "          0.3085,  1.0195,  1.6012,  1.5376,  1.1399,  3.5268,  0.9174,  1.3869,\n",
      "          1.0135, -0.7747, -0.2560,  2.5865,  2.2172,  0.8223, -0.1455,  0.1383,\n",
      "          1.5418,  2.3296,  1.9156,  0.1940,  1.0721,  0.4884, -0.6085,  1.2113,\n",
      "          1.6052,  1.4865,  0.9008,  1.9952,  0.6163,  0.9264,  0.1962,  1.7886,\n",
      "          1.4019,  1.8067, -1.8601,  0.4402,  2.3858,  1.7615,  2.2435,  0.4592,\n",
      "          1.4922,  2.6230,  2.1548,  1.4235,  0.3650,  0.8210,  0.3345,  1.1884,\n",
      "          0.1844,  0.4736,  1.4085, -0.2858, -0.0079, -1.0756, -2.3881, -0.1343,\n",
      "          0.5750, -1.8682,  0.5242, -0.0994, -0.5039, -0.8989,  0.6663,  1.3529,\n",
      "         -0.6817, -0.5918,  0.3579,  1.2842,  0.7434, -1.1823,  0.9915,  1.2582,\n",
      "         -1.1929, -1.1760, -0.1551,  0.0347, -1.0285, -1.8232, -0.4213, -2.9350,\n",
      "         -0.4303,  1.8618,  1.6693, -0.2139, -0.6941,  0.2559,  1.6389, -2.7314,\n",
      "          0.1056, -0.0623,  0.5256, -0.9204,  0.0247, -0.7994, -0.9841,  0.9759,\n",
      "          0.2418, -0.5038,  0.4742, -0.7136, -1.2287, -0.2594, -0.7321,  0.9534,\n",
      "         -0.4203,  0.0978, -2.0036, -1.0337, -1.3861,  0.6273, -1.9824, -1.0210,\n",
      "         -0.9406, -0.6093, -1.5818, -1.1479, -2.3365, -0.9508, -1.3268, -0.3025,\n",
      "         -1.8255,  0.4839, -1.7202, -0.5865, -3.3508,  0.2867, -0.1708, -0.6946,\n",
      "         -2.2883, -1.6394, -1.1807, -1.3610, -2.4750, -2.4789, -3.3422]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.3508, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.1637, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dce358be39441395135ea4ef1a0aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 79.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f76c035cb4c4108812af8e109cafd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285f1a0d9dd04c1fafb3018794a43966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6d039e51c1496f88ad3e5feca9d415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "layer 9: 0.0\n",
      "layer 10: 0.0\n",
      "Epoch: 00 | Batch: 000 / 029 | Total loss: 9.631 | Reg loss: 0.014 | Tree loss: 9.631 | Accuracy: 0.000000 | 6.98 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 029 | Total loss: 9.625 | Reg loss: 0.013 | Tree loss: 9.625 | Accuracy: 0.000000 | 6.279 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 029 | Total loss: 9.619 | Reg loss: 0.012 | Tree loss: 9.619 | Accuracy: 0.000000 | 6.027 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 029 | Total loss: 9.614 | Reg loss: 0.011 | Tree loss: 9.614 | Accuracy: 0.000000 | 5.884 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 029 | Total loss: 9.608 | Reg loss: 0.010 | Tree loss: 9.608 | Accuracy: 0.000000 | 6.131 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 029 | Total loss: 9.602 | Reg loss: 0.009 | Tree loss: 9.602 | Accuracy: 0.000000 | 6.273 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 029 | Total loss: 9.596 | Reg loss: 0.009 | Tree loss: 9.596 | Accuracy: 0.019531 | 6.313 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 029 | Total loss: 9.590 | Reg loss: 0.008 | Tree loss: 9.590 | Accuracy: 0.048828 | 6.311 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 029 | Total loss: 9.585 | Reg loss: 0.008 | Tree loss: 9.585 | Accuracy: 0.109375 | 6.313 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 029 | Total loss: 9.579 | Reg loss: 0.007 | Tree loss: 9.579 | Accuracy: 0.203125 | 6.315 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 029 | Total loss: 9.575 | Reg loss: 0.007 | Tree loss: 9.575 | Accuracy: 0.294922 | 6.349 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 029 | Total loss: 9.568 | Reg loss: 0.007 | Tree loss: 9.568 | Accuracy: 0.416016 | 6.404 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 029 | Total loss: 9.565 | Reg loss: 0.007 | Tree loss: 9.565 | Accuracy: 0.476562 | 6.455 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 029 | Total loss: 9.560 | Reg loss: 0.007 | Tree loss: 9.560 | Accuracy: 0.466797 | 6.514 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 029 | Total loss: 9.554 | Reg loss: 0.007 | Tree loss: 9.554 | Accuracy: 0.513672 | 6.565 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 029 | Total loss: 9.550 | Reg loss: 0.008 | Tree loss: 9.550 | Accuracy: 0.466797 | 6.607 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 029 | Total loss: 9.546 | Reg loss: 0.008 | Tree loss: 9.546 | Accuracy: 0.478516 | 6.643 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 029 | Total loss: 9.542 | Reg loss: 0.008 | Tree loss: 9.542 | Accuracy: 0.494141 | 6.675 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 029 | Total loss: 9.535 | Reg loss: 0.008 | Tree loss: 9.535 | Accuracy: 0.494141 | 6.703 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 029 | Total loss: 9.530 | Reg loss: 0.009 | Tree loss: 9.530 | Accuracy: 0.490234 | 6.728 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 029 | Total loss: 9.530 | Reg loss: 0.009 | Tree loss: 9.530 | Accuracy: 0.478516 | 6.747 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 029 | Total loss: 9.525 | Reg loss: 0.009 | Tree loss: 9.525 | Accuracy: 0.472656 | 6.764 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 029 | Total loss: 9.517 | Reg loss: 0.009 | Tree loss: 9.517 | Accuracy: 0.511719 | 6.78 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 029 | Total loss: 9.514 | Reg loss: 0.010 | Tree loss: 9.514 | Accuracy: 0.492188 | 6.79 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 029 | Total loss: 9.511 | Reg loss: 0.010 | Tree loss: 9.511 | Accuracy: 0.490234 | 6.793 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 029 | Total loss: 9.505 | Reg loss: 0.010 | Tree loss: 9.505 | Accuracy: 0.501953 | 6.732 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 029 | Total loss: 9.504 | Reg loss: 0.010 | Tree loss: 9.504 | Accuracy: 0.460938 | 6.672 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 029 | Total loss: 9.499 | Reg loss: 0.011 | Tree loss: 9.499 | Accuracy: 0.498047 | 6.614 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 029 | Total loss: 9.489 | Reg loss: 0.011 | Tree loss: 9.489 | Accuracy: 0.514793 | 6.618 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 029 | Total loss: 9.538 | Reg loss: 0.005 | Tree loss: 9.538 | Accuracy: 0.468750 | 6.902 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 029 | Total loss: 9.535 | Reg loss: 0.005 | Tree loss: 9.535 | Accuracy: 0.498047 | 6.911 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 029 | Total loss: 9.528 | Reg loss: 0.006 | Tree loss: 9.528 | Accuracy: 0.494141 | 6.921 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 029 | Total loss: 9.527 | Reg loss: 0.006 | Tree loss: 9.527 | Accuracy: 0.480469 | 6.866 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 029 | Total loss: 9.521 | Reg loss: 0.006 | Tree loss: 9.521 | Accuracy: 0.486328 | 6.808 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 029 | Total loss: 9.516 | Reg loss: 0.006 | Tree loss: 9.516 | Accuracy: 0.490234 | 6.747 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 029 | Total loss: 9.510 | Reg loss: 0.007 | Tree loss: 9.510 | Accuracy: 0.482422 | 6.696 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 029 | Total loss: 9.506 | Reg loss: 0.007 | Tree loss: 9.506 | Accuracy: 0.457031 | 6.643 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 029 | Total loss: 9.497 | Reg loss: 0.007 | Tree loss: 9.497 | Accuracy: 0.521484 | 6.594 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 029 | Total loss: 9.493 | Reg loss: 0.008 | Tree loss: 9.493 | Accuracy: 0.525391 | 6.547 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 029 | Total loss: 9.491 | Reg loss: 0.008 | Tree loss: 9.491 | Accuracy: 0.474609 | 6.503 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 029 | Total loss: 9.488 | Reg loss: 0.009 | Tree loss: 9.488 | Accuracy: 0.482422 | 6.459 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 029 | Total loss: 9.480 | Reg loss: 0.009 | Tree loss: 9.480 | Accuracy: 0.486328 | 6.454 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 029 | Total loss: 9.480 | Reg loss: 0.010 | Tree loss: 9.480 | Accuracy: 0.451172 | 6.463 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 029 | Total loss: 9.471 | Reg loss: 0.010 | Tree loss: 9.471 | Accuracy: 0.529297 | 6.473 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 029 | Total loss: 9.467 | Reg loss: 0.010 | Tree loss: 9.467 | Accuracy: 0.505859 | 6.485 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 029 | Total loss: 9.462 | Reg loss: 0.011 | Tree loss: 9.462 | Accuracy: 0.535156 | 6.495 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 029 | Total loss: 9.460 | Reg loss: 0.011 | Tree loss: 9.460 | Accuracy: 0.505859 | 6.506 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 029 | Total loss: 9.458 | Reg loss: 0.012 | Tree loss: 9.458 | Accuracy: 0.458984 | 6.517 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 029 | Total loss: 9.451 | Reg loss: 0.012 | Tree loss: 9.451 | Accuracy: 0.472656 | 6.528 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 029 | Total loss: 9.450 | Reg loss: 0.012 | Tree loss: 9.450 | Accuracy: 0.482422 | 6.54 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 029 | Total loss: 9.439 | Reg loss: 0.013 | Tree loss: 9.439 | Accuracy: 0.513672 | 6.552 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 029 | Total loss: 9.439 | Reg loss: 0.013 | Tree loss: 9.439 | Accuracy: 0.478516 | 6.562 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 029 | Total loss: 9.435 | Reg loss: 0.013 | Tree loss: 9.435 | Accuracy: 0.474609 | 6.572 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 029 | Total loss: 9.423 | Reg loss: 0.014 | Tree loss: 9.423 | Accuracy: 0.542969 | 6.581 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 029 | Total loss: 9.421 | Reg loss: 0.014 | Tree loss: 9.421 | Accuracy: 0.525391 | 6.554 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 029 | Total loss: 9.418 | Reg loss: 0.014 | Tree loss: 9.418 | Accuracy: 0.496094 | 6.526 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 029 | Total loss: 9.415 | Reg loss: 0.015 | Tree loss: 9.415 | Accuracy: 0.509766 | 6.497 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 029 | Total loss: 9.410 | Reg loss: 0.015 | Tree loss: 9.410 | Accuracy: 0.502959 | 6.468 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 029 | Total loss: 9.477 | Reg loss: 0.008 | Tree loss: 9.477 | Accuracy: 0.476562 | 6.682 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 029 | Total loss: 9.473 | Reg loss: 0.008 | Tree loss: 9.473 | Accuracy: 0.496094 | 6.693 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 002 / 029 | Total loss: 9.463 | Reg loss: 0.009 | Tree loss: 9.463 | Accuracy: 0.529297 | 6.701 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 029 | Total loss: 9.463 | Reg loss: 0.009 | Tree loss: 9.463 | Accuracy: 0.492188 | 6.708 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 029 | Total loss: 9.457 | Reg loss: 0.009 | Tree loss: 9.457 | Accuracy: 0.474609 | 6.715 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 029 | Total loss: 9.450 | Reg loss: 0.009 | Tree loss: 9.450 | Accuracy: 0.505859 | 6.721 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 029 | Total loss: 9.443 | Reg loss: 0.010 | Tree loss: 9.443 | Accuracy: 0.498047 | 6.728 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 029 | Total loss: 9.443 | Reg loss: 0.010 | Tree loss: 9.443 | Accuracy: 0.458984 | 6.733 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 029 | Total loss: 9.431 | Reg loss: 0.010 | Tree loss: 9.431 | Accuracy: 0.517578 | 6.74 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 029 | Total loss: 9.427 | Reg loss: 0.011 | Tree loss: 9.427 | Accuracy: 0.484375 | 6.746 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 029 | Total loss: 9.421 | Reg loss: 0.011 | Tree loss: 9.421 | Accuracy: 0.486328 | 6.731 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 029 | Total loss: 9.415 | Reg loss: 0.012 | Tree loss: 9.415 | Accuracy: 0.464844 | 6.7 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 029 | Total loss: 9.405 | Reg loss: 0.012 | Tree loss: 9.405 | Accuracy: 0.503906 | 6.673 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 029 | Total loss: 9.399 | Reg loss: 0.013 | Tree loss: 9.399 | Accuracy: 0.490234 | 6.647 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 029 | Total loss: 9.386 | Reg loss: 0.013 | Tree loss: 9.386 | Accuracy: 0.509766 | 6.621 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 029 | Total loss: 9.385 | Reg loss: 0.014 | Tree loss: 9.385 | Accuracy: 0.500000 | 6.595 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 029 | Total loss: 9.376 | Reg loss: 0.014 | Tree loss: 9.376 | Accuracy: 0.511719 | 6.571 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 029 | Total loss: 9.372 | Reg loss: 0.015 | Tree loss: 9.372 | Accuracy: 0.488281 | 6.546 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 029 | Total loss: 9.369 | Reg loss: 0.015 | Tree loss: 9.369 | Accuracy: 0.488281 | 6.523 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 029 | Total loss: 9.353 | Reg loss: 0.016 | Tree loss: 9.353 | Accuracy: 0.488281 | 6.502 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 029 | Total loss: 9.347 | Reg loss: 0.016 | Tree loss: 9.347 | Accuracy: 0.513672 | 6.51 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 029 | Total loss: 9.336 | Reg loss: 0.016 | Tree loss: 9.336 | Accuracy: 0.486328 | 6.518 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 029 | Total loss: 9.332 | Reg loss: 0.017 | Tree loss: 9.332 | Accuracy: 0.464844 | 6.525 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 029 | Total loss: 9.320 | Reg loss: 0.017 | Tree loss: 9.320 | Accuracy: 0.494141 | 6.533 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 029 | Total loss: 9.308 | Reg loss: 0.018 | Tree loss: 9.308 | Accuracy: 0.498047 | 6.535 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 029 | Total loss: 9.301 | Reg loss: 0.018 | Tree loss: 9.301 | Accuracy: 0.505859 | 6.52 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 029 | Total loss: 9.282 | Reg loss: 0.019 | Tree loss: 9.282 | Accuracy: 0.498047 | 6.503 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 029 | Total loss: 9.282 | Reg loss: 0.019 | Tree loss: 9.282 | Accuracy: 0.486328 | 6.486 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 029 | Total loss: 9.269 | Reg loss: 0.019 | Tree loss: 9.269 | Accuracy: 0.520710 | 6.467 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 029 | Total loss: 9.396 | Reg loss: 0.011 | Tree loss: 9.396 | Accuracy: 0.533203 | 6.62 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 029 | Total loss: 9.399 | Reg loss: 0.011 | Tree loss: 9.399 | Accuracy: 0.490234 | 6.626 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 029 | Total loss: 9.394 | Reg loss: 0.012 | Tree loss: 9.394 | Accuracy: 0.470703 | 6.631 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 029 | Total loss: 9.381 | Reg loss: 0.012 | Tree loss: 9.381 | Accuracy: 0.501953 | 6.637 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 029 | Total loss: 9.375 | Reg loss: 0.012 | Tree loss: 9.375 | Accuracy: 0.503906 | 6.642 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 029 | Total loss: 9.360 | Reg loss: 0.012 | Tree loss: 9.360 | Accuracy: 0.525391 | 6.647 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 029 | Total loss: 9.346 | Reg loss: 0.013 | Tree loss: 9.346 | Accuracy: 0.525391 | 6.652 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 029 | Total loss: 9.335 | Reg loss: 0.013 | Tree loss: 9.335 | Accuracy: 0.519531 | 6.656 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 029 | Total loss: 9.334 | Reg loss: 0.013 | Tree loss: 9.334 | Accuracy: 0.505859 | 6.66 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 029 | Total loss: 9.310 | Reg loss: 0.014 | Tree loss: 9.310 | Accuracy: 0.542969 | 6.661 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 029 | Total loss: 9.310 | Reg loss: 0.014 | Tree loss: 9.310 | Accuracy: 0.505859 | 6.664 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 029 | Total loss: 9.302 | Reg loss: 0.015 | Tree loss: 9.302 | Accuracy: 0.478516 | 6.667 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 029 | Total loss: 9.288 | Reg loss: 0.015 | Tree loss: 9.288 | Accuracy: 0.472656 | 6.67 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 029 | Total loss: 9.274 | Reg loss: 0.016 | Tree loss: 9.274 | Accuracy: 0.476562 | 6.673 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 029 | Total loss: 9.264 | Reg loss: 0.016 | Tree loss: 9.264 | Accuracy: 0.498047 | 6.676 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 029 | Total loss: 9.253 | Reg loss: 0.017 | Tree loss: 9.253 | Accuracy: 0.474609 | 6.679 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 029 | Total loss: 9.245 | Reg loss: 0.017 | Tree loss: 9.245 | Accuracy: 0.457031 | 6.682 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 029 | Total loss: 9.217 | Reg loss: 0.018 | Tree loss: 9.217 | Accuracy: 0.507812 | 6.676 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 029 | Total loss: 9.208 | Reg loss: 0.018 | Tree loss: 9.208 | Accuracy: 0.447266 | 6.658 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 029 | Total loss: 9.205 | Reg loss: 0.018 | Tree loss: 9.205 | Accuracy: 0.486328 | 6.639 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 029 | Total loss: 9.173 | Reg loss: 0.019 | Tree loss: 9.173 | Accuracy: 0.515625 | 6.622 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 029 | Total loss: 9.177 | Reg loss: 0.019 | Tree loss: 9.177 | Accuracy: 0.451172 | 6.606 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 029 | Total loss: 9.149 | Reg loss: 0.020 | Tree loss: 9.149 | Accuracy: 0.494141 | 6.59 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 029 | Total loss: 9.135 | Reg loss: 0.020 | Tree loss: 9.135 | Accuracy: 0.484375 | 6.574 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 029 | Total loss: 9.114 | Reg loss: 0.021 | Tree loss: 9.114 | Accuracy: 0.482422 | 6.558 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 029 | Total loss: 9.094 | Reg loss: 0.021 | Tree loss: 9.094 | Accuracy: 0.482422 | 6.55 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 029 | Total loss: 9.086 | Reg loss: 0.022 | Tree loss: 9.086 | Accuracy: 0.457031 | 6.54 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 029 | Total loss: 9.063 | Reg loss: 0.022 | Tree loss: 9.063 | Accuracy: 0.523438 | 6.53 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 029 | Total loss: 9.050 | Reg loss: 0.022 | Tree loss: 9.050 | Accuracy: 0.504931 | 6.537 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 029 | Total loss: 9.283 | Reg loss: 0.014 | Tree loss: 9.283 | Accuracy: 0.509766 | 6.669 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 029 | Total loss: 9.274 | Reg loss: 0.014 | Tree loss: 9.274 | Accuracy: 0.519531 | 6.673 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 029 | Total loss: 9.268 | Reg loss: 0.014 | Tree loss: 9.268 | Accuracy: 0.488281 | 6.677 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 029 | Total loss: 9.262 | Reg loss: 0.014 | Tree loss: 9.262 | Accuracy: 0.457031 | 6.68 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 029 | Total loss: 9.249 | Reg loss: 0.015 | Tree loss: 9.249 | Accuracy: 0.449219 | 6.684 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 005 / 029 | Total loss: 9.220 | Reg loss: 0.015 | Tree loss: 9.220 | Accuracy: 0.505859 | 6.687 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 029 | Total loss: 9.204 | Reg loss: 0.015 | Tree loss: 9.204 | Accuracy: 0.521484 | 6.69 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 029 | Total loss: 9.183 | Reg loss: 0.015 | Tree loss: 9.183 | Accuracy: 0.527344 | 6.693 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 029 | Total loss: 9.179 | Reg loss: 0.016 | Tree loss: 9.179 | Accuracy: 0.476562 | 6.695 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 029 | Total loss: 9.157 | Reg loss: 0.016 | Tree loss: 9.157 | Accuracy: 0.498047 | 6.695 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 029 | Total loss: 9.139 | Reg loss: 0.017 | Tree loss: 9.139 | Accuracy: 0.515625 | 6.696 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 029 | Total loss: 9.125 | Reg loss: 0.017 | Tree loss: 9.125 | Accuracy: 0.482422 | 6.697 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 029 | Total loss: 9.099 | Reg loss: 0.017 | Tree loss: 9.099 | Accuracy: 0.515625 | 6.697 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 029 | Total loss: 9.093 | Reg loss: 0.018 | Tree loss: 9.093 | Accuracy: 0.464844 | 6.698 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 029 | Total loss: 9.070 | Reg loss: 0.018 | Tree loss: 9.070 | Accuracy: 0.486328 | 6.699 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 029 | Total loss: 9.037 | Reg loss: 0.019 | Tree loss: 9.037 | Accuracy: 0.519531 | 6.7 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 029 | Total loss: 9.030 | Reg loss: 0.019 | Tree loss: 9.030 | Accuracy: 0.451172 | 6.701 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 029 | Total loss: 8.997 | Reg loss: 0.020 | Tree loss: 8.997 | Accuracy: 0.494141 | 6.702 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 029 | Total loss: 8.976 | Reg loss: 0.020 | Tree loss: 8.976 | Accuracy: 0.517578 | 6.705 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 029 | Total loss: 8.958 | Reg loss: 0.021 | Tree loss: 8.958 | Accuracy: 0.523438 | 6.709 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 029 | Total loss: 8.945 | Reg loss: 0.021 | Tree loss: 8.945 | Accuracy: 0.470703 | 6.712 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 029 | Total loss: 8.919 | Reg loss: 0.022 | Tree loss: 8.919 | Accuracy: 0.501953 | 6.716 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 029 | Total loss: 8.909 | Reg loss: 0.022 | Tree loss: 8.909 | Accuracy: 0.482422 | 6.701 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 029 | Total loss: 8.856 | Reg loss: 0.022 | Tree loss: 8.856 | Accuracy: 0.500000 | 6.687 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 029 | Total loss: 8.858 | Reg loss: 0.023 | Tree loss: 8.858 | Accuracy: 0.496094 | 6.667 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 029 | Total loss: 8.851 | Reg loss: 0.023 | Tree loss: 8.851 | Accuracy: 0.468750 | 6.659 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 029 | Total loss: 8.803 | Reg loss: 0.024 | Tree loss: 8.803 | Accuracy: 0.492188 | 6.649 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 029 | Total loss: 8.762 | Reg loss: 0.024 | Tree loss: 8.762 | Accuracy: 0.496094 | 6.654 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 029 | Total loss: 8.760 | Reg loss: 0.024 | Tree loss: 8.760 | Accuracy: 0.473373 | 6.658 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 029 | Total loss: 9.114 | Reg loss: 0.017 | Tree loss: 9.114 | Accuracy: 0.492188 | 6.851 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 029 | Total loss: 9.113 | Reg loss: 0.017 | Tree loss: 9.113 | Accuracy: 0.453125 | 6.854 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 029 | Total loss: 9.077 | Reg loss: 0.017 | Tree loss: 9.077 | Accuracy: 0.537109 | 6.856 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 029 | Total loss: 9.069 | Reg loss: 0.017 | Tree loss: 9.069 | Accuracy: 0.470703 | 6.858 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 029 | Total loss: 9.041 | Reg loss: 0.017 | Tree loss: 9.041 | Accuracy: 0.533203 | 6.859 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 029 | Total loss: 9.012 | Reg loss: 0.017 | Tree loss: 9.012 | Accuracy: 0.517578 | 6.859 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 029 | Total loss: 9.005 | Reg loss: 0.018 | Tree loss: 9.005 | Accuracy: 0.476562 | 6.858 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 029 | Total loss: 8.964 | Reg loss: 0.018 | Tree loss: 8.964 | Accuracy: 0.513672 | 6.856 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 029 | Total loss: 8.942 | Reg loss: 0.018 | Tree loss: 8.942 | Accuracy: 0.507812 | 6.855 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 029 | Total loss: 8.917 | Reg loss: 0.018 | Tree loss: 8.917 | Accuracy: 0.476562 | 6.854 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 029 | Total loss: 8.901 | Reg loss: 0.019 | Tree loss: 8.901 | Accuracy: 0.501953 | 6.854 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 029 | Total loss: 8.849 | Reg loss: 0.019 | Tree loss: 8.849 | Accuracy: 0.533203 | 6.853 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 029 | Total loss: 8.860 | Reg loss: 0.020 | Tree loss: 8.860 | Accuracy: 0.466797 | 6.853 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 029 | Total loss: 8.833 | Reg loss: 0.020 | Tree loss: 8.833 | Accuracy: 0.457031 | 6.854 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 029 | Total loss: 8.790 | Reg loss: 0.020 | Tree loss: 8.790 | Accuracy: 0.480469 | 6.855 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 029 | Total loss: 8.764 | Reg loss: 0.021 | Tree loss: 8.764 | Accuracy: 0.511719 | 6.857 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 029 | Total loss: 8.752 | Reg loss: 0.021 | Tree loss: 8.752 | Accuracy: 0.511719 | 6.859 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 029 | Total loss: 8.723 | Reg loss: 0.022 | Tree loss: 8.723 | Accuracy: 0.505859 | 6.861 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 029 | Total loss: 8.716 | Reg loss: 0.022 | Tree loss: 8.716 | Accuracy: 0.460938 | 6.863 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 029 | Total loss: 8.652 | Reg loss: 0.022 | Tree loss: 8.652 | Accuracy: 0.523438 | 6.86 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 029 | Total loss: 8.648 | Reg loss: 0.023 | Tree loss: 8.648 | Accuracy: 0.476562 | 6.843 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 029 | Total loss: 8.620 | Reg loss: 0.023 | Tree loss: 8.620 | Accuracy: 0.490234 | 6.835 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 029 | Total loss: 8.606 | Reg loss: 0.024 | Tree loss: 8.606 | Accuracy: 0.482422 | 6.833 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 029 | Total loss: 8.556 | Reg loss: 0.024 | Tree loss: 8.556 | Accuracy: 0.515625 | 6.835 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 029 | Total loss: 8.549 | Reg loss: 0.024 | Tree loss: 8.549 | Accuracy: 0.507812 | 6.837 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 029 | Total loss: 8.500 | Reg loss: 0.025 | Tree loss: 8.500 | Accuracy: 0.498047 | 6.838 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 029 | Total loss: 8.493 | Reg loss: 0.025 | Tree loss: 8.493 | Accuracy: 0.484375 | 6.84 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 029 | Total loss: 8.475 | Reg loss: 0.025 | Tree loss: 8.475 | Accuracy: 0.478516 | 6.841 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 029 | Total loss: 8.484 | Reg loss: 0.026 | Tree loss: 8.484 | Accuracy: 0.455621 | 6.842 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 029 | Total loss: 8.877 | Reg loss: 0.019 | Tree loss: 8.877 | Accuracy: 0.494141 | 6.972 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 029 | Total loss: 8.867 | Reg loss: 0.019 | Tree loss: 8.867 | Accuracy: 0.455078 | 6.973 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 029 | Total loss: 8.815 | Reg loss: 0.019 | Tree loss: 8.815 | Accuracy: 0.501953 | 6.973 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 029 | Total loss: 8.806 | Reg loss: 0.019 | Tree loss: 8.806 | Accuracy: 0.470703 | 6.974 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 029 | Total loss: 8.789 | Reg loss: 0.019 | Tree loss: 8.789 | Accuracy: 0.472656 | 6.975 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 029 | Total loss: 8.740 | Reg loss: 0.019 | Tree loss: 8.740 | Accuracy: 0.525391 | 6.976 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 029 | Total loss: 8.733 | Reg loss: 0.020 | Tree loss: 8.733 | Accuracy: 0.486328 | 6.976 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 029 | Total loss: 8.714 | Reg loss: 0.020 | Tree loss: 8.714 | Accuracy: 0.472656 | 6.977 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 008 / 029 | Total loss: 8.658 | Reg loss: 0.020 | Tree loss: 8.658 | Accuracy: 0.505859 | 6.978 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 029 | Total loss: 8.648 | Reg loss: 0.020 | Tree loss: 8.648 | Accuracy: 0.494141 | 6.979 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 029 | Total loss: 8.626 | Reg loss: 0.021 | Tree loss: 8.626 | Accuracy: 0.474609 | 6.979 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 029 | Total loss: 8.596 | Reg loss: 0.021 | Tree loss: 8.596 | Accuracy: 0.503906 | 6.98 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 029 | Total loss: 8.548 | Reg loss: 0.021 | Tree loss: 8.548 | Accuracy: 0.535156 | 6.979 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 029 | Total loss: 8.515 | Reg loss: 0.022 | Tree loss: 8.515 | Accuracy: 0.509766 | 6.978 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 029 | Total loss: 8.493 | Reg loss: 0.022 | Tree loss: 8.493 | Accuracy: 0.472656 | 6.978 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 029 | Total loss: 8.478 | Reg loss: 0.022 | Tree loss: 8.478 | Accuracy: 0.466797 | 6.976 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 029 | Total loss: 8.419 | Reg loss: 0.023 | Tree loss: 8.419 | Accuracy: 0.503906 | 6.963 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 029 | Total loss: 8.387 | Reg loss: 0.023 | Tree loss: 8.387 | Accuracy: 0.513672 | 6.955 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 029 | Total loss: 8.354 | Reg loss: 0.023 | Tree loss: 8.354 | Accuracy: 0.507812 | 6.955 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 029 | Total loss: 8.352 | Reg loss: 0.024 | Tree loss: 8.352 | Accuracy: 0.503906 | 6.954 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 029 | Total loss: 8.306 | Reg loss: 0.024 | Tree loss: 8.306 | Accuracy: 0.537109 | 6.952 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 029 | Total loss: 8.311 | Reg loss: 0.024 | Tree loss: 8.311 | Accuracy: 0.480469 | 6.951 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 029 | Total loss: 8.280 | Reg loss: 0.025 | Tree loss: 8.280 | Accuracy: 0.478516 | 6.949 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 029 | Total loss: 8.237 | Reg loss: 0.025 | Tree loss: 8.237 | Accuracy: 0.494141 | 6.948 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 029 | Total loss: 8.208 | Reg loss: 0.025 | Tree loss: 8.208 | Accuracy: 0.517578 | 6.946 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 029 | Total loss: 8.190 | Reg loss: 0.026 | Tree loss: 8.190 | Accuracy: 0.476562 | 6.945 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 029 | Total loss: 8.184 | Reg loss: 0.026 | Tree loss: 8.184 | Accuracy: 0.476562 | 6.944 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 029 | Total loss: 8.169 | Reg loss: 0.026 | Tree loss: 8.169 | Accuracy: 0.460938 | 6.944 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 029 | Total loss: 8.077 | Reg loss: 0.027 | Tree loss: 8.077 | Accuracy: 0.536489 | 6.945 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 029 | Total loss: 8.609 | Reg loss: 0.021 | Tree loss: 8.609 | Accuracy: 0.488281 | 7.074 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 029 | Total loss: 8.544 | Reg loss: 0.021 | Tree loss: 8.544 | Accuracy: 0.498047 | 7.074 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 029 | Total loss: 8.528 | Reg loss: 0.021 | Tree loss: 8.528 | Accuracy: 0.515625 | 7.073 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 029 | Total loss: 8.503 | Reg loss: 0.021 | Tree loss: 8.503 | Accuracy: 0.509766 | 7.072 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 029 | Total loss: 8.491 | Reg loss: 0.021 | Tree loss: 8.491 | Accuracy: 0.519531 | 7.072 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 029 | Total loss: 8.438 | Reg loss: 0.021 | Tree loss: 8.438 | Accuracy: 0.523438 | 7.072 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 029 | Total loss: 8.442 | Reg loss: 0.021 | Tree loss: 8.442 | Accuracy: 0.492188 | 7.071 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 029 | Total loss: 8.402 | Reg loss: 0.022 | Tree loss: 8.402 | Accuracy: 0.498047 | 7.071 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 029 | Total loss: 8.362 | Reg loss: 0.022 | Tree loss: 8.362 | Accuracy: 0.494141 | 7.07 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 029 | Total loss: 8.327 | Reg loss: 0.022 | Tree loss: 8.327 | Accuracy: 0.478516 | 7.069 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 029 | Total loss: 8.304 | Reg loss: 0.022 | Tree loss: 8.304 | Accuracy: 0.501953 | 7.069 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 029 | Total loss: 8.274 | Reg loss: 0.023 | Tree loss: 8.274 | Accuracy: 0.488281 | 7.059 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 029 | Total loss: 8.220 | Reg loss: 0.023 | Tree loss: 8.220 | Accuracy: 0.500000 | 7.049 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 029 | Total loss: 8.203 | Reg loss: 0.023 | Tree loss: 8.203 | Accuracy: 0.474609 | 7.037 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 029 | Total loss: 8.160 | Reg loss: 0.024 | Tree loss: 8.160 | Accuracy: 0.500000 | 7.03 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 029 | Total loss: 8.132 | Reg loss: 0.024 | Tree loss: 8.132 | Accuracy: 0.468750 | 7.03 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 029 | Total loss: 8.083 | Reg loss: 0.024 | Tree loss: 8.083 | Accuracy: 0.488281 | 7.031 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 029 | Total loss: 8.093 | Reg loss: 0.025 | Tree loss: 8.093 | Accuracy: 0.470703 | 7.031 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 029 | Total loss: 8.031 | Reg loss: 0.025 | Tree loss: 8.031 | Accuracy: 0.480469 | 7.029 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 029 | Total loss: 8.003 | Reg loss: 0.025 | Tree loss: 8.003 | Accuracy: 0.517578 | 7.027 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 029 | Total loss: 7.976 | Reg loss: 0.026 | Tree loss: 7.976 | Accuracy: 0.515625 | 7.024 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 029 | Total loss: 7.956 | Reg loss: 0.026 | Tree loss: 7.956 | Accuracy: 0.501953 | 7.021 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 029 | Total loss: 7.936 | Reg loss: 0.026 | Tree loss: 7.936 | Accuracy: 0.496094 | 7.019 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 029 | Total loss: 7.926 | Reg loss: 0.027 | Tree loss: 7.926 | Accuracy: 0.490234 | 7.018 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 029 | Total loss: 7.896 | Reg loss: 0.027 | Tree loss: 7.896 | Accuracy: 0.496094 | 7.019 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 029 | Total loss: 7.845 | Reg loss: 0.027 | Tree loss: 7.845 | Accuracy: 0.474609 | 7.02 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 029 | Total loss: 7.827 | Reg loss: 0.028 | Tree loss: 7.827 | Accuracy: 0.488281 | 7.021 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 029 | Total loss: 7.814 | Reg loss: 0.028 | Tree loss: 7.814 | Accuracy: 0.458984 | 7.022 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 029 | Total loss: 7.749 | Reg loss: 0.028 | Tree loss: 7.749 | Accuracy: 0.500986 | 7.023 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 029 | Total loss: 8.301 | Reg loss: 0.022 | Tree loss: 8.301 | Accuracy: 0.488281 | 7.128 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 029 | Total loss: 8.260 | Reg loss: 0.022 | Tree loss: 8.260 | Accuracy: 0.507812 | 7.128 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 029 | Total loss: 8.219 | Reg loss: 0.022 | Tree loss: 8.219 | Accuracy: 0.484375 | 7.128 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 029 | Total loss: 8.173 | Reg loss: 0.023 | Tree loss: 8.173 | Accuracy: 0.513672 | 7.128 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 029 | Total loss: 8.151 | Reg loss: 0.023 | Tree loss: 8.151 | Accuracy: 0.507812 | 7.128 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 029 | Total loss: 8.117 | Reg loss: 0.023 | Tree loss: 8.117 | Accuracy: 0.490234 | 7.127 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 029 | Total loss: 8.090 | Reg loss: 0.023 | Tree loss: 8.090 | Accuracy: 0.490234 | 7.126 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 029 | Total loss: 8.089 | Reg loss: 0.023 | Tree loss: 8.089 | Accuracy: 0.480469 | 7.122 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 029 | Total loss: 8.028 | Reg loss: 0.024 | Tree loss: 8.028 | Accuracy: 0.500000 | 7.113 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 029 | Total loss: 8.015 | Reg loss: 0.024 | Tree loss: 8.015 | Accuracy: 0.466797 | 7.105 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 029 | Total loss: 7.962 | Reg loss: 0.024 | Tree loss: 7.962 | Accuracy: 0.490234 | 7.105 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 011 / 029 | Total loss: 7.915 | Reg loss: 0.024 | Tree loss: 7.915 | Accuracy: 0.509766 | 7.1 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 029 | Total loss: 7.900 | Reg loss: 0.025 | Tree loss: 7.900 | Accuracy: 0.503906 | 7.092 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 029 | Total loss: 7.852 | Reg loss: 0.025 | Tree loss: 7.852 | Accuracy: 0.484375 | 7.084 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 029 | Total loss: 7.857 | Reg loss: 0.025 | Tree loss: 7.857 | Accuracy: 0.474609 | 7.084 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 029 | Total loss: 7.794 | Reg loss: 0.026 | Tree loss: 7.794 | Accuracy: 0.496094 | 7.083 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 029 | Total loss: 7.736 | Reg loss: 0.026 | Tree loss: 7.736 | Accuracy: 0.513672 | 7.081 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 029 | Total loss: 7.737 | Reg loss: 0.026 | Tree loss: 7.737 | Accuracy: 0.474609 | 7.08 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 029 | Total loss: 7.729 | Reg loss: 0.027 | Tree loss: 7.729 | Accuracy: 0.480469 | 7.078 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 029 | Total loss: 7.696 | Reg loss: 0.027 | Tree loss: 7.696 | Accuracy: 0.476562 | 7.077 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 029 | Total loss: 7.631 | Reg loss: 0.027 | Tree loss: 7.631 | Accuracy: 0.523438 | 7.076 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 029 | Total loss: 7.622 | Reg loss: 0.028 | Tree loss: 7.622 | Accuracy: 0.503906 | 7.076 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 029 | Total loss: 7.632 | Reg loss: 0.028 | Tree loss: 7.632 | Accuracy: 0.466797 | 7.075 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 029 | Total loss: 7.581 | Reg loss: 0.028 | Tree loss: 7.581 | Accuracy: 0.498047 | 7.075 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 029 | Total loss: 7.521 | Reg loss: 0.029 | Tree loss: 7.521 | Accuracy: 0.521484 | 7.074 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 029 | Total loss: 7.518 | Reg loss: 0.029 | Tree loss: 7.518 | Accuracy: 0.505859 | 7.074 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 029 | Total loss: 7.461 | Reg loss: 0.029 | Tree loss: 7.461 | Accuracy: 0.509766 | 7.074 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 029 | Total loss: 7.491 | Reg loss: 0.030 | Tree loss: 7.491 | Accuracy: 0.472656 | 7.074 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 029 | Total loss: 7.452 | Reg loss: 0.030 | Tree loss: 7.452 | Accuracy: 0.491124 | 7.073 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 029 | Total loss: 7.955 | Reg loss: 0.024 | Tree loss: 7.955 | Accuracy: 0.486328 | 7.137 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 029 | Total loss: 7.930 | Reg loss: 0.024 | Tree loss: 7.930 | Accuracy: 0.474609 | 7.137 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 029 | Total loss: 7.868 | Reg loss: 0.024 | Tree loss: 7.868 | Accuracy: 0.517578 | 7.136 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 029 | Total loss: 7.870 | Reg loss: 0.025 | Tree loss: 7.870 | Accuracy: 0.490234 | 7.136 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 029 | Total loss: 7.841 | Reg loss: 0.025 | Tree loss: 7.841 | Accuracy: 0.517578 | 7.135 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 029 | Total loss: 7.809 | Reg loss: 0.025 | Tree loss: 7.809 | Accuracy: 0.523438 | 7.133 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 029 | Total loss: 7.784 | Reg loss: 0.025 | Tree loss: 7.784 | Accuracy: 0.480469 | 7.125 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 029 | Total loss: 7.753 | Reg loss: 0.025 | Tree loss: 7.753 | Accuracy: 0.486328 | 7.118 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 029 | Total loss: 7.733 | Reg loss: 0.025 | Tree loss: 7.733 | Accuracy: 0.464844 | 7.117 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 029 | Total loss: 7.645 | Reg loss: 0.026 | Tree loss: 7.645 | Accuracy: 0.470703 | 7.115 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 029 | Total loss: 7.629 | Reg loss: 0.026 | Tree loss: 7.629 | Accuracy: 0.453125 | 7.113 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 029 | Total loss: 7.575 | Reg loss: 0.026 | Tree loss: 7.575 | Accuracy: 0.519531 | 7.11 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 029 | Total loss: 7.580 | Reg loss: 0.027 | Tree loss: 7.580 | Accuracy: 0.468750 | 7.102 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 029 | Total loss: 7.542 | Reg loss: 0.027 | Tree loss: 7.542 | Accuracy: 0.496094 | 7.094 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 029 | Total loss: 7.520 | Reg loss: 0.027 | Tree loss: 7.520 | Accuracy: 0.498047 | 7.085 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 029 | Total loss: 7.457 | Reg loss: 0.028 | Tree loss: 7.457 | Accuracy: 0.521484 | 7.077 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 029 | Total loss: 7.406 | Reg loss: 0.028 | Tree loss: 7.406 | Accuracy: 0.531250 | 7.068 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 029 | Total loss: 7.398 | Reg loss: 0.028 | Tree loss: 7.398 | Accuracy: 0.513672 | 7.06 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 029 | Total loss: 7.359 | Reg loss: 0.029 | Tree loss: 7.359 | Accuracy: 0.501953 | 7.052 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 029 | Total loss: 7.342 | Reg loss: 0.029 | Tree loss: 7.342 | Accuracy: 0.507812 | 7.044 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 029 | Total loss: 7.317 | Reg loss: 0.029 | Tree loss: 7.317 | Accuracy: 0.451172 | 7.044 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 029 | Total loss: 7.280 | Reg loss: 0.030 | Tree loss: 7.280 | Accuracy: 0.494141 | 7.044 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 029 | Total loss: 7.265 | Reg loss: 0.030 | Tree loss: 7.265 | Accuracy: 0.544922 | 7.043 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 029 | Total loss: 7.224 | Reg loss: 0.030 | Tree loss: 7.224 | Accuracy: 0.517578 | 7.043 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 029 | Total loss: 7.243 | Reg loss: 0.030 | Tree loss: 7.243 | Accuracy: 0.490234 | 7.042 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 029 | Total loss: 7.187 | Reg loss: 0.031 | Tree loss: 7.187 | Accuracy: 0.480469 | 7.041 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 029 | Total loss: 7.214 | Reg loss: 0.031 | Tree loss: 7.214 | Accuracy: 0.425781 | 7.04 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 029 | Total loss: 7.149 | Reg loss: 0.031 | Tree loss: 7.149 | Accuracy: 0.488281 | 7.04 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 029 | Total loss: 7.099 | Reg loss: 0.032 | Tree loss: 7.099 | Accuracy: 0.508876 | 7.04 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 029 | Total loss: 7.615 | Reg loss: 0.026 | Tree loss: 7.615 | Accuracy: 0.494141 | 7.118 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 029 | Total loss: 7.603 | Reg loss: 0.026 | Tree loss: 7.603 | Accuracy: 0.486328 | 7.118 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 029 | Total loss: 7.583 | Reg loss: 0.026 | Tree loss: 7.583 | Accuracy: 0.468750 | 7.112 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 029 | Total loss: 7.539 | Reg loss: 0.026 | Tree loss: 7.539 | Accuracy: 0.488281 | 7.104 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 029 | Total loss: 7.505 | Reg loss: 0.026 | Tree loss: 7.505 | Accuracy: 0.505859 | 7.096 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 029 | Total loss: 7.479 | Reg loss: 0.027 | Tree loss: 7.479 | Accuracy: 0.476562 | 7.088 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 029 | Total loss: 7.482 | Reg loss: 0.027 | Tree loss: 7.482 | Accuracy: 0.451172 | 7.08 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 029 | Total loss: 7.425 | Reg loss: 0.027 | Tree loss: 7.425 | Accuracy: 0.470703 | 7.072 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 029 | Total loss: 7.372 | Reg loss: 0.027 | Tree loss: 7.372 | Accuracy: 0.468750 | 7.065 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 029 | Total loss: 7.348 | Reg loss: 0.027 | Tree loss: 7.348 | Accuracy: 0.511719 | 7.057 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 029 | Total loss: 7.301 | Reg loss: 0.028 | Tree loss: 7.301 | Accuracy: 0.517578 | 7.05 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 029 | Total loss: 7.250 | Reg loss: 0.028 | Tree loss: 7.250 | Accuracy: 0.523438 | 7.042 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 029 | Total loss: 7.235 | Reg loss: 0.028 | Tree loss: 7.235 | Accuracy: 0.496094 | 7.035 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 029 | Total loss: 7.194 | Reg loss: 0.028 | Tree loss: 7.194 | Accuracy: 0.474609 | 7.036 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 014 / 029 | Total loss: 7.176 | Reg loss: 0.029 | Tree loss: 7.176 | Accuracy: 0.480469 | 7.037 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 029 | Total loss: 7.123 | Reg loss: 0.029 | Tree loss: 7.123 | Accuracy: 0.472656 | 7.036 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 029 | Total loss: 7.111 | Reg loss: 0.029 | Tree loss: 7.111 | Accuracy: 0.523438 | 7.03 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 029 | Total loss: 7.099 | Reg loss: 0.030 | Tree loss: 7.099 | Accuracy: 0.464844 | 7.024 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 029 | Total loss: 7.043 | Reg loss: 0.030 | Tree loss: 7.043 | Accuracy: 0.521484 | 7.018 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 029 | Total loss: 7.013 | Reg loss: 0.030 | Tree loss: 7.013 | Accuracy: 0.505859 | 7.018 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 029 | Total loss: 7.020 | Reg loss: 0.030 | Tree loss: 7.020 | Accuracy: 0.476562 | 7.018 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 029 | Total loss: 7.014 | Reg loss: 0.031 | Tree loss: 7.014 | Accuracy: 0.490234 | 7.019 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 029 | Total loss: 6.946 | Reg loss: 0.031 | Tree loss: 6.946 | Accuracy: 0.500000 | 7.019 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 029 | Total loss: 6.900 | Reg loss: 0.031 | Tree loss: 6.900 | Accuracy: 0.527344 | 7.02 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 029 | Total loss: 6.890 | Reg loss: 0.032 | Tree loss: 6.890 | Accuracy: 0.494141 | 7.021 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 029 | Total loss: 6.853 | Reg loss: 0.032 | Tree loss: 6.853 | Accuracy: 0.525391 | 7.022 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 029 | Total loss: 6.857 | Reg loss: 0.032 | Tree loss: 6.857 | Accuracy: 0.500000 | 7.023 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 029 | Total loss: 6.824 | Reg loss: 0.032 | Tree loss: 6.824 | Accuracy: 0.500000 | 7.023 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 029 | Total loss: 6.822 | Reg loss: 0.033 | Tree loss: 6.822 | Accuracy: 0.491124 | 7.024 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 029 | Total loss: 7.292 | Reg loss: 0.028 | Tree loss: 7.292 | Accuracy: 0.498047 | 7.137 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 029 | Total loss: 7.282 | Reg loss: 0.028 | Tree loss: 7.282 | Accuracy: 0.494141 | 7.136 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 029 | Total loss: 7.220 | Reg loss: 0.028 | Tree loss: 7.220 | Accuracy: 0.503906 | 7.135 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 029 | Total loss: 7.200 | Reg loss: 0.028 | Tree loss: 7.200 | Accuracy: 0.490234 | 7.132 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 029 | Total loss: 7.198 | Reg loss: 0.028 | Tree loss: 7.198 | Accuracy: 0.482422 | 7.125 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 029 | Total loss: 7.157 | Reg loss: 0.028 | Tree loss: 7.157 | Accuracy: 0.500000 | 7.118 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 029 | Total loss: 7.095 | Reg loss: 0.028 | Tree loss: 7.095 | Accuracy: 0.457031 | 7.111 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 029 | Total loss: 7.029 | Reg loss: 0.028 | Tree loss: 7.029 | Accuracy: 0.539062 | 7.103 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 029 | Total loss: 7.070 | Reg loss: 0.029 | Tree loss: 7.070 | Accuracy: 0.490234 | 7.096 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 029 | Total loss: 7.086 | Reg loss: 0.029 | Tree loss: 7.086 | Accuracy: 0.466797 | 7.089 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 029 | Total loss: 6.993 | Reg loss: 0.029 | Tree loss: 6.993 | Accuracy: 0.486328 | 7.083 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 029 | Total loss: 6.952 | Reg loss: 0.029 | Tree loss: 6.952 | Accuracy: 0.455078 | 7.078 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 029 | Total loss: 6.914 | Reg loss: 0.029 | Tree loss: 6.914 | Accuracy: 0.488281 | 7.072 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 029 | Total loss: 6.882 | Reg loss: 0.030 | Tree loss: 6.882 | Accuracy: 0.498047 | 7.066 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 029 | Total loss: 6.880 | Reg loss: 0.030 | Tree loss: 6.880 | Accuracy: 0.468750 | 7.06 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 029 | Total loss: 6.828 | Reg loss: 0.030 | Tree loss: 6.828 | Accuracy: 0.503906 | 7.06 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 029 | Total loss: 6.799 | Reg loss: 0.030 | Tree loss: 6.799 | Accuracy: 0.496094 | 7.06 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 029 | Total loss: 6.782 | Reg loss: 0.031 | Tree loss: 6.782 | Accuracy: 0.474609 | 7.061 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 029 | Total loss: 6.735 | Reg loss: 0.031 | Tree loss: 6.735 | Accuracy: 0.501953 | 7.061 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 029 | Total loss: 6.753 | Reg loss: 0.031 | Tree loss: 6.753 | Accuracy: 0.472656 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 029 | Total loss: 6.655 | Reg loss: 0.031 | Tree loss: 6.655 | Accuracy: 0.558594 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 029 | Total loss: 6.680 | Reg loss: 0.032 | Tree loss: 6.680 | Accuracy: 0.494141 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 029 | Total loss: 6.624 | Reg loss: 0.032 | Tree loss: 6.624 | Accuracy: 0.529297 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 029 | Total loss: 6.647 | Reg loss: 0.032 | Tree loss: 6.647 | Accuracy: 0.505859 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 029 | Total loss: 6.598 | Reg loss: 0.032 | Tree loss: 6.598 | Accuracy: 0.505859 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 029 | Total loss: 6.574 | Reg loss: 0.033 | Tree loss: 6.574 | Accuracy: 0.480469 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 029 | Total loss: 6.555 | Reg loss: 0.033 | Tree loss: 6.555 | Accuracy: 0.494141 | 7.061 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 029 | Total loss: 6.519 | Reg loss: 0.033 | Tree loss: 6.519 | Accuracy: 0.480469 | 7.061 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 029 | Total loss: 6.530 | Reg loss: 0.033 | Tree loss: 6.530 | Accuracy: 0.500986 | 7.061 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 029 | Total loss: 6.980 | Reg loss: 0.029 | Tree loss: 6.980 | Accuracy: 0.488281 | 7.102 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 029 | Total loss: 6.973 | Reg loss: 0.029 | Tree loss: 6.973 | Accuracy: 0.490234 | 7.103 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 029 | Total loss: 6.945 | Reg loss: 0.029 | Tree loss: 6.945 | Accuracy: 0.496094 | 7.103 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 029 | Total loss: 6.888 | Reg loss: 0.029 | Tree loss: 6.888 | Accuracy: 0.525391 | 7.103 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 029 | Total loss: 6.856 | Reg loss: 0.029 | Tree loss: 6.856 | Accuracy: 0.511719 | 7.103 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 029 | Total loss: 6.842 | Reg loss: 0.029 | Tree loss: 6.842 | Accuracy: 0.486328 | 7.102 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 029 | Total loss: 6.802 | Reg loss: 0.029 | Tree loss: 6.802 | Accuracy: 0.490234 | 7.098 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 029 | Total loss: 6.752 | Reg loss: 0.030 | Tree loss: 6.752 | Accuracy: 0.519531 | 7.093 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 029 | Total loss: 6.705 | Reg loss: 0.030 | Tree loss: 6.705 | Accuracy: 0.513672 | 7.094 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 029 | Total loss: 6.706 | Reg loss: 0.030 | Tree loss: 6.706 | Accuracy: 0.476562 | 7.095 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 029 | Total loss: 6.670 | Reg loss: 0.030 | Tree loss: 6.670 | Accuracy: 0.482422 | 7.096 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 029 | Total loss: 6.628 | Reg loss: 0.030 | Tree loss: 6.628 | Accuracy: 0.511719 | 7.097 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 029 | Total loss: 6.630 | Reg loss: 0.030 | Tree loss: 6.630 | Accuracy: 0.470703 | 7.093 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 029 | Total loss: 6.561 | Reg loss: 0.031 | Tree loss: 6.561 | Accuracy: 0.531250 | 7.087 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 029 | Total loss: 6.572 | Reg loss: 0.031 | Tree loss: 6.572 | Accuracy: 0.501953 | 7.081 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 029 | Total loss: 6.533 | Reg loss: 0.031 | Tree loss: 6.533 | Accuracy: 0.458984 | 7.075 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 029 | Total loss: 6.475 | Reg loss: 0.031 | Tree loss: 6.475 | Accuracy: 0.488281 | 7.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 017 / 029 | Total loss: 6.456 | Reg loss: 0.031 | Tree loss: 6.456 | Accuracy: 0.539062 | 7.064 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 029 | Total loss: 6.432 | Reg loss: 0.032 | Tree loss: 6.432 | Accuracy: 0.519531 | 7.063 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 029 | Total loss: 6.425 | Reg loss: 0.032 | Tree loss: 6.425 | Accuracy: 0.490234 | 7.063 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 029 | Total loss: 6.404 | Reg loss: 0.032 | Tree loss: 6.404 | Accuracy: 0.474609 | 7.063 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 029 | Total loss: 6.411 | Reg loss: 0.032 | Tree loss: 6.411 | Accuracy: 0.462891 | 7.063 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 029 | Total loss: 6.394 | Reg loss: 0.033 | Tree loss: 6.394 | Accuracy: 0.474609 | 7.064 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 029 | Total loss: 6.369 | Reg loss: 0.033 | Tree loss: 6.369 | Accuracy: 0.460938 | 7.064 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 029 | Total loss: 6.289 | Reg loss: 0.033 | Tree loss: 6.289 | Accuracy: 0.541016 | 7.064 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 029 | Total loss: 6.309 | Reg loss: 0.033 | Tree loss: 6.309 | Accuracy: 0.482422 | 7.065 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 029 | Total loss: 6.305 | Reg loss: 0.033 | Tree loss: 6.305 | Accuracy: 0.449219 | 7.065 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 029 | Total loss: 6.257 | Reg loss: 0.033 | Tree loss: 6.257 | Accuracy: 0.490234 | 7.065 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 029 | Total loss: 6.229 | Reg loss: 0.034 | Tree loss: 6.229 | Accuracy: 0.477318 | 7.066 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 029 | Total loss: 6.660 | Reg loss: 0.030 | Tree loss: 6.660 | Accuracy: 0.517578 | 7.14 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 029 | Total loss: 6.678 | Reg loss: 0.030 | Tree loss: 6.678 | Accuracy: 0.482422 | 7.136 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 029 | Total loss: 6.595 | Reg loss: 0.030 | Tree loss: 6.595 | Accuracy: 0.505859 | 7.135 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 029 | Total loss: 6.597 | Reg loss: 0.030 | Tree loss: 6.597 | Accuracy: 0.476562 | 7.134 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 029 | Total loss: 6.542 | Reg loss: 0.030 | Tree loss: 6.542 | Accuracy: 0.505859 | 7.133 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 029 | Total loss: 6.539 | Reg loss: 0.030 | Tree loss: 6.539 | Accuracy: 0.476562 | 7.131 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 029 | Total loss: 6.497 | Reg loss: 0.030 | Tree loss: 6.497 | Accuracy: 0.478516 | 7.128 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 029 | Total loss: 6.422 | Reg loss: 0.030 | Tree loss: 6.422 | Accuracy: 0.511719 | 7.127 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 029 | Total loss: 6.437 | Reg loss: 0.031 | Tree loss: 6.437 | Accuracy: 0.478516 | 7.126 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 029 | Total loss: 6.431 | Reg loss: 0.031 | Tree loss: 6.431 | Accuracy: 0.466797 | 7.126 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 029 | Total loss: 6.349 | Reg loss: 0.031 | Tree loss: 6.349 | Accuracy: 0.517578 | 7.126 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 029 | Total loss: 6.378 | Reg loss: 0.031 | Tree loss: 6.378 | Accuracy: 0.507812 | 7.126 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 029 | Total loss: 6.300 | Reg loss: 0.031 | Tree loss: 6.300 | Accuracy: 0.515625 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 029 | Total loss: 6.280 | Reg loss: 0.031 | Tree loss: 6.280 | Accuracy: 0.492188 | 7.118 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 029 | Total loss: 6.246 | Reg loss: 0.031 | Tree loss: 6.246 | Accuracy: 0.515625 | 7.118 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 029 | Total loss: 6.300 | Reg loss: 0.032 | Tree loss: 6.300 | Accuracy: 0.460938 | 7.119 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 029 | Total loss: 6.225 | Reg loss: 0.032 | Tree loss: 6.225 | Accuracy: 0.484375 | 7.12 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 029 | Total loss: 6.196 | Reg loss: 0.032 | Tree loss: 6.196 | Accuracy: 0.492188 | 7.12 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 029 | Total loss: 6.155 | Reg loss: 0.032 | Tree loss: 6.155 | Accuracy: 0.513672 | 7.121 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 029 | Total loss: 6.162 | Reg loss: 0.032 | Tree loss: 6.162 | Accuracy: 0.492188 | 7.121 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 029 | Total loss: 6.175 | Reg loss: 0.033 | Tree loss: 6.175 | Accuracy: 0.464844 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 029 | Total loss: 6.097 | Reg loss: 0.033 | Tree loss: 6.097 | Accuracy: 0.466797 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 029 | Total loss: 6.095 | Reg loss: 0.033 | Tree loss: 6.095 | Accuracy: 0.509766 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 029 | Total loss: 6.056 | Reg loss: 0.033 | Tree loss: 6.056 | Accuracy: 0.529297 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 029 | Total loss: 6.040 | Reg loss: 0.033 | Tree loss: 6.040 | Accuracy: 0.509766 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 029 | Total loss: 6.066 | Reg loss: 0.033 | Tree loss: 6.066 | Accuracy: 0.472656 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 029 | Total loss: 6.011 | Reg loss: 0.034 | Tree loss: 6.011 | Accuracy: 0.472656 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 029 | Total loss: 5.974 | Reg loss: 0.034 | Tree loss: 5.974 | Accuracy: 0.476562 | 7.122 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 029 | Total loss: 5.926 | Reg loss: 0.034 | Tree loss: 5.926 | Accuracy: 0.506903 | 7.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 029 | Total loss: 6.374 | Reg loss: 0.031 | Tree loss: 6.374 | Accuracy: 0.472656 | 7.133 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 029 | Total loss: 6.345 | Reg loss: 0.031 | Tree loss: 6.345 | Accuracy: 0.492188 | 7.133 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 029 | Total loss: 6.297 | Reg loss: 0.031 | Tree loss: 6.297 | Accuracy: 0.515625 | 7.132 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 029 | Total loss: 6.297 | Reg loss: 0.031 | Tree loss: 6.297 | Accuracy: 0.466797 | 7.13 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 029 | Total loss: 6.237 | Reg loss: 0.031 | Tree loss: 6.237 | Accuracy: 0.470703 | 7.129 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 029 | Total loss: 6.251 | Reg loss: 0.031 | Tree loss: 6.251 | Accuracy: 0.494141 | 7.127 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 029 | Total loss: 6.213 | Reg loss: 0.031 | Tree loss: 6.213 | Accuracy: 0.484375 | 7.125 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 029 | Total loss: 6.216 | Reg loss: 0.031 | Tree loss: 6.216 | Accuracy: 0.478516 | 7.124 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 029 | Total loss: 6.118 | Reg loss: 0.031 | Tree loss: 6.118 | Accuracy: 0.511719 | 7.124 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 029 | Total loss: 6.144 | Reg loss: 0.031 | Tree loss: 6.144 | Accuracy: 0.482422 | 7.125 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 029 | Total loss: 6.091 | Reg loss: 0.032 | Tree loss: 6.091 | Accuracy: 0.521484 | 7.125 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 029 | Total loss: 6.095 | Reg loss: 0.032 | Tree loss: 6.095 | Accuracy: 0.509766 | 7.126 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 029 | Total loss: 6.017 | Reg loss: 0.032 | Tree loss: 6.017 | Accuracy: 0.511719 | 7.125 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 029 | Total loss: 6.058 | Reg loss: 0.032 | Tree loss: 6.058 | Accuracy: 0.492188 | 7.122 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 029 | Total loss: 5.984 | Reg loss: 0.032 | Tree loss: 5.984 | Accuracy: 0.515625 | 7.118 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 029 | Total loss: 5.963 | Reg loss: 0.032 | Tree loss: 5.963 | Accuracy: 0.500000 | 7.118 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 029 | Total loss: 5.919 | Reg loss: 0.032 | Tree loss: 5.919 | Accuracy: 0.521484 | 7.119 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 029 | Total loss: 5.939 | Reg loss: 0.033 | Tree loss: 5.939 | Accuracy: 0.488281 | 7.12 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 029 | Total loss: 5.909 | Reg loss: 0.033 | Tree loss: 5.909 | Accuracy: 0.476562 | 7.12 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 029 | Total loss: 5.882 | Reg loss: 0.033 | Tree loss: 5.882 | Accuracy: 0.498047 | 7.121 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 020 / 029 | Total loss: 5.871 | Reg loss: 0.033 | Tree loss: 5.871 | Accuracy: 0.500000 | 7.121 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 029 | Total loss: 5.831 | Reg loss: 0.033 | Tree loss: 5.831 | Accuracy: 0.513672 | 7.122 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 029 | Total loss: 5.839 | Reg loss: 0.033 | Tree loss: 5.839 | Accuracy: 0.455078 | 7.122 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 029 | Total loss: 5.776 | Reg loss: 0.033 | Tree loss: 5.776 | Accuracy: 0.498047 | 7.123 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 029 | Total loss: 5.814 | Reg loss: 0.034 | Tree loss: 5.814 | Accuracy: 0.480469 | 7.123 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 029 | Total loss: 5.748 | Reg loss: 0.034 | Tree loss: 5.748 | Accuracy: 0.486328 | 7.123 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 029 | Total loss: 5.721 | Reg loss: 0.034 | Tree loss: 5.721 | Accuracy: 0.468750 | 7.124 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 029 | Total loss: 5.681 | Reg loss: 0.034 | Tree loss: 5.681 | Accuracy: 0.527344 | 7.124 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 029 | Total loss: 5.721 | Reg loss: 0.034 | Tree loss: 5.721 | Accuracy: 0.457594 | 7.123 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 029 | Total loss: 6.039 | Reg loss: 0.031 | Tree loss: 6.039 | Accuracy: 0.513672 | 7.133 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 029 | Total loss: 6.040 | Reg loss: 0.031 | Tree loss: 6.040 | Accuracy: 0.498047 | 7.133 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 029 | Total loss: 5.998 | Reg loss: 0.031 | Tree loss: 5.998 | Accuracy: 0.537109 | 7.133 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 029 | Total loss: 6.009 | Reg loss: 0.032 | Tree loss: 6.009 | Accuracy: 0.474609 | 7.133 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 029 | Total loss: 5.962 | Reg loss: 0.032 | Tree loss: 5.962 | Accuracy: 0.488281 | 7.134 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 029 | Total loss: 5.923 | Reg loss: 0.032 | Tree loss: 5.923 | Accuracy: 0.498047 | 7.134 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 029 | Total loss: 5.947 | Reg loss: 0.032 | Tree loss: 5.947 | Accuracy: 0.472656 | 7.134 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 029 | Total loss: 5.905 | Reg loss: 0.032 | Tree loss: 5.905 | Accuracy: 0.501953 | 7.134 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 029 | Total loss: 5.885 | Reg loss: 0.032 | Tree loss: 5.885 | Accuracy: 0.468750 | 7.134 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 029 | Total loss: 5.855 | Reg loss: 0.032 | Tree loss: 5.855 | Accuracy: 0.496094 | 7.134 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 029 | Total loss: 5.831 | Reg loss: 0.032 | Tree loss: 5.831 | Accuracy: 0.492188 | 7.133 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 029 | Total loss: 5.798 | Reg loss: 0.032 | Tree loss: 5.798 | Accuracy: 0.501953 | 7.133 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 029 | Total loss: 5.776 | Reg loss: 0.032 | Tree loss: 5.776 | Accuracy: 0.496094 | 7.132 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 029 | Total loss: 5.760 | Reg loss: 0.032 | Tree loss: 5.760 | Accuracy: 0.509766 | 7.13 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 029 | Total loss: 5.751 | Reg loss: 0.032 | Tree loss: 5.751 | Accuracy: 0.468750 | 7.124 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 029 | Total loss: 5.705 | Reg loss: 0.033 | Tree loss: 5.705 | Accuracy: 0.490234 | 7.119 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 029 | Total loss: 5.680 | Reg loss: 0.033 | Tree loss: 5.680 | Accuracy: 0.492188 | 7.114 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 029 | Total loss: 5.701 | Reg loss: 0.033 | Tree loss: 5.701 | Accuracy: 0.453125 | 7.109 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 029 | Total loss: 5.656 | Reg loss: 0.033 | Tree loss: 5.656 | Accuracy: 0.482422 | 7.103 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 029 | Total loss: 5.615 | Reg loss: 0.033 | Tree loss: 5.615 | Accuracy: 0.507812 | 7.098 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 029 | Total loss: 5.587 | Reg loss: 0.033 | Tree loss: 5.587 | Accuracy: 0.533203 | 7.093 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 029 | Total loss: 5.603 | Reg loss: 0.033 | Tree loss: 5.603 | Accuracy: 0.474609 | 7.088 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 029 | Total loss: 5.576 | Reg loss: 0.034 | Tree loss: 5.576 | Accuracy: 0.464844 | 7.082 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 029 | Total loss: 5.561 | Reg loss: 0.034 | Tree loss: 5.561 | Accuracy: 0.478516 | 7.081 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 029 | Total loss: 5.535 | Reg loss: 0.034 | Tree loss: 5.535 | Accuracy: 0.488281 | 7.081 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 029 | Total loss: 5.516 | Reg loss: 0.034 | Tree loss: 5.516 | Accuracy: 0.498047 | 7.081 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 029 | Total loss: 5.494 | Reg loss: 0.034 | Tree loss: 5.494 | Accuracy: 0.476562 | 7.08 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 029 | Total loss: 5.450 | Reg loss: 0.034 | Tree loss: 5.450 | Accuracy: 0.496094 | 7.076 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 029 | Total loss: 5.421 | Reg loss: 0.034 | Tree loss: 5.421 | Accuracy: 0.506903 | 7.072 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 029 | Total loss: 5.796 | Reg loss: 0.032 | Tree loss: 5.796 | Accuracy: 0.501953 | 7.088 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 029 | Total loss: 5.790 | Reg loss: 0.032 | Tree loss: 5.790 | Accuracy: 0.470703 | 7.089 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 029 | Total loss: 5.750 | Reg loss: 0.032 | Tree loss: 5.750 | Accuracy: 0.517578 | 7.09 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 029 | Total loss: 5.714 | Reg loss: 0.032 | Tree loss: 5.714 | Accuracy: 0.500000 | 7.091 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 029 | Total loss: 5.688 | Reg loss: 0.032 | Tree loss: 5.688 | Accuracy: 0.498047 | 7.091 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 029 | Total loss: 5.680 | Reg loss: 0.032 | Tree loss: 5.680 | Accuracy: 0.490234 | 7.092 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 029 | Total loss: 5.629 | Reg loss: 0.032 | Tree loss: 5.629 | Accuracy: 0.501953 | 7.092 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 029 | Total loss: 5.625 | Reg loss: 0.032 | Tree loss: 5.625 | Accuracy: 0.492188 | 7.093 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 029 | Total loss: 5.624 | Reg loss: 0.032 | Tree loss: 5.624 | Accuracy: 0.496094 | 7.094 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 029 | Total loss: 5.591 | Reg loss: 0.032 | Tree loss: 5.591 | Accuracy: 0.505859 | 7.094 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 029 | Total loss: 5.592 | Reg loss: 0.032 | Tree loss: 5.592 | Accuracy: 0.472656 | 7.095 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 029 | Total loss: 5.555 | Reg loss: 0.033 | Tree loss: 5.555 | Accuracy: 0.470703 | 7.095 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 029 | Total loss: 5.512 | Reg loss: 0.033 | Tree loss: 5.512 | Accuracy: 0.494141 | 7.096 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 029 | Total loss: 5.534 | Reg loss: 0.033 | Tree loss: 5.534 | Accuracy: 0.476562 | 7.096 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 029 | Total loss: 5.441 | Reg loss: 0.033 | Tree loss: 5.441 | Accuracy: 0.515625 | 7.096 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 029 | Total loss: 5.458 | Reg loss: 0.033 | Tree loss: 5.458 | Accuracy: 0.492188 | 7.095 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 029 | Total loss: 5.465 | Reg loss: 0.033 | Tree loss: 5.465 | Accuracy: 0.458984 | 7.094 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 029 | Total loss: 5.398 | Reg loss: 0.033 | Tree loss: 5.398 | Accuracy: 0.515625 | 7.093 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 029 | Total loss: 5.389 | Reg loss: 0.033 | Tree loss: 5.389 | Accuracy: 0.511719 | 7.092 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 029 | Total loss: 5.384 | Reg loss: 0.033 | Tree loss: 5.384 | Accuracy: 0.490234 | 7.091 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 029 | Total loss: 5.351 | Reg loss: 0.034 | Tree loss: 5.351 | Accuracy: 0.472656 | 7.09 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 029 | Total loss: 5.310 | Reg loss: 0.034 | Tree loss: 5.310 | Accuracy: 0.509766 | 7.09 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 029 | Total loss: 5.345 | Reg loss: 0.034 | Tree loss: 5.345 | Accuracy: 0.458984 | 7.089 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 023 / 029 | Total loss: 5.292 | Reg loss: 0.034 | Tree loss: 5.292 | Accuracy: 0.476562 | 7.085 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 029 | Total loss: 5.268 | Reg loss: 0.034 | Tree loss: 5.268 | Accuracy: 0.474609 | 7.081 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 029 | Total loss: 5.270 | Reg loss: 0.034 | Tree loss: 5.270 | Accuracy: 0.503906 | 7.076 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 029 | Total loss: 5.240 | Reg loss: 0.034 | Tree loss: 5.240 | Accuracy: 0.490234 | 7.073 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 029 | Total loss: 5.221 | Reg loss: 0.034 | Tree loss: 5.221 | Accuracy: 0.486328 | 7.069 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 029 | Total loss: 5.204 | Reg loss: 0.034 | Tree loss: 5.204 | Accuracy: 0.499014 | 7.066 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 029 | Total loss: 5.545 | Reg loss: 0.032 | Tree loss: 5.545 | Accuracy: 0.472656 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 029 | Total loss: 5.504 | Reg loss: 0.032 | Tree loss: 5.504 | Accuracy: 0.484375 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 029 | Total loss: 5.515 | Reg loss: 0.032 | Tree loss: 5.515 | Accuracy: 0.470703 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 029 | Total loss: 5.492 | Reg loss: 0.032 | Tree loss: 5.492 | Accuracy: 0.496094 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 029 | Total loss: 5.474 | Reg loss: 0.032 | Tree loss: 5.474 | Accuracy: 0.476562 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 029 | Total loss: 5.407 | Reg loss: 0.032 | Tree loss: 5.407 | Accuracy: 0.513672 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 029 | Total loss: 5.397 | Reg loss: 0.033 | Tree loss: 5.397 | Accuracy: 0.501953 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 029 | Total loss: 5.380 | Reg loss: 0.033 | Tree loss: 5.380 | Accuracy: 0.482422 | 7.144 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 029 | Total loss: 5.347 | Reg loss: 0.033 | Tree loss: 5.347 | Accuracy: 0.474609 | 7.143 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 029 | Total loss: 5.372 | Reg loss: 0.033 | Tree loss: 5.372 | Accuracy: 0.476562 | 7.143 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 029 | Total loss: 5.325 | Reg loss: 0.033 | Tree loss: 5.325 | Accuracy: 0.460938 | 7.143 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 029 | Total loss: 5.296 | Reg loss: 0.033 | Tree loss: 5.296 | Accuracy: 0.480469 | 7.143 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 029 | Total loss: 5.287 | Reg loss: 0.033 | Tree loss: 5.287 | Accuracy: 0.464844 | 7.143 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 029 | Total loss: 5.255 | Reg loss: 0.033 | Tree loss: 5.255 | Accuracy: 0.494141 | 7.143 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 029 | Total loss: 5.244 | Reg loss: 0.033 | Tree loss: 5.244 | Accuracy: 0.478516 | 7.142 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 029 | Total loss: 5.202 | Reg loss: 0.033 | Tree loss: 5.202 | Accuracy: 0.507812 | 7.142 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 029 | Total loss: 5.175 | Reg loss: 0.033 | Tree loss: 5.175 | Accuracy: 0.490234 | 7.142 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 029 | Total loss: 5.177 | Reg loss: 0.033 | Tree loss: 5.177 | Accuracy: 0.501953 | 7.141 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 029 | Total loss: 5.136 | Reg loss: 0.033 | Tree loss: 5.136 | Accuracy: 0.525391 | 7.139 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 029 | Total loss: 5.102 | Reg loss: 0.034 | Tree loss: 5.102 | Accuracy: 0.523438 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 029 | Total loss: 5.137 | Reg loss: 0.034 | Tree loss: 5.137 | Accuracy: 0.453125 | 7.129 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 029 | Total loss: 5.077 | Reg loss: 0.034 | Tree loss: 5.077 | Accuracy: 0.494141 | 7.13 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 029 | Total loss: 5.084 | Reg loss: 0.034 | Tree loss: 5.084 | Accuracy: 0.435547 | 7.13 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 029 | Total loss: 5.022 | Reg loss: 0.034 | Tree loss: 5.022 | Accuracy: 0.515625 | 7.131 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 029 | Total loss: 5.016 | Reg loss: 0.034 | Tree loss: 5.016 | Accuracy: 0.517578 | 7.131 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 029 | Total loss: 5.014 | Reg loss: 0.034 | Tree loss: 5.014 | Accuracy: 0.505859 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 029 | Total loss: 4.968 | Reg loss: 0.034 | Tree loss: 4.968 | Accuracy: 0.500000 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 029 | Total loss: 5.009 | Reg loss: 0.034 | Tree loss: 5.009 | Accuracy: 0.474609 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 029 | Total loss: 4.986 | Reg loss: 0.034 | Tree loss: 4.986 | Accuracy: 0.477318 | 7.132 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 029 | Total loss: 5.277 | Reg loss: 0.033 | Tree loss: 5.277 | Accuracy: 0.523438 | 7.21 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 029 | Total loss: 5.265 | Reg loss: 0.033 | Tree loss: 5.265 | Accuracy: 0.486328 | 7.209 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 029 | Total loss: 5.231 | Reg loss: 0.033 | Tree loss: 5.231 | Accuracy: 0.511719 | 7.209 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 029 | Total loss: 5.271 | Reg loss: 0.033 | Tree loss: 5.271 | Accuracy: 0.476562 | 7.209 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 029 | Total loss: 5.194 | Reg loss: 0.033 | Tree loss: 5.194 | Accuracy: 0.500000 | 7.209 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 029 | Total loss: 5.199 | Reg loss: 0.033 | Tree loss: 5.199 | Accuracy: 0.464844 | 7.21 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 029 | Total loss: 5.150 | Reg loss: 0.033 | Tree loss: 5.150 | Accuracy: 0.488281 | 7.21 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 029 | Total loss: 5.149 | Reg loss: 0.033 | Tree loss: 5.149 | Accuracy: 0.484375 | 7.21 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 029 | Total loss: 5.096 | Reg loss: 0.033 | Tree loss: 5.096 | Accuracy: 0.484375 | 7.209 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 029 | Total loss: 5.085 | Reg loss: 0.033 | Tree loss: 5.085 | Accuracy: 0.484375 | 7.209 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 029 | Total loss: 5.070 | Reg loss: 0.033 | Tree loss: 5.070 | Accuracy: 0.500000 | 7.206 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 029 | Total loss: 5.070 | Reg loss: 0.033 | Tree loss: 5.070 | Accuracy: 0.505859 | 7.2 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 029 | Total loss: 5.012 | Reg loss: 0.033 | Tree loss: 5.012 | Accuracy: 0.474609 | 7.197 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 029 | Total loss: 5.021 | Reg loss: 0.033 | Tree loss: 5.021 | Accuracy: 0.488281 | 7.198 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 029 | Total loss: 4.948 | Reg loss: 0.033 | Tree loss: 4.948 | Accuracy: 0.541016 | 7.198 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 029 | Total loss: 4.963 | Reg loss: 0.033 | Tree loss: 4.963 | Accuracy: 0.494141 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 029 | Total loss: 4.959 | Reg loss: 0.033 | Tree loss: 4.959 | Accuracy: 0.449219 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 029 | Total loss: 4.952 | Reg loss: 0.034 | Tree loss: 4.952 | Accuracy: 0.480469 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 029 | Total loss: 4.886 | Reg loss: 0.034 | Tree loss: 4.886 | Accuracy: 0.511719 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 029 | Total loss: 4.934 | Reg loss: 0.034 | Tree loss: 4.934 | Accuracy: 0.476562 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 029 | Total loss: 4.855 | Reg loss: 0.034 | Tree loss: 4.855 | Accuracy: 0.515625 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 029 | Total loss: 4.852 | Reg loss: 0.034 | Tree loss: 4.852 | Accuracy: 0.490234 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 029 | Total loss: 4.829 | Reg loss: 0.034 | Tree loss: 4.829 | Accuracy: 0.486328 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 029 | Total loss: 4.859 | Reg loss: 0.034 | Tree loss: 4.859 | Accuracy: 0.462891 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 029 | Total loss: 4.793 | Reg loss: 0.034 | Tree loss: 4.793 | Accuracy: 0.480469 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 029 | Total loss: 4.804 | Reg loss: 0.034 | Tree loss: 4.804 | Accuracy: 0.462891 | 7.199 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 026 / 029 | Total loss: 4.796 | Reg loss: 0.034 | Tree loss: 4.796 | Accuracy: 0.486328 | 7.199 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 029 | Total loss: 4.769 | Reg loss: 0.034 | Tree loss: 4.769 | Accuracy: 0.457031 | 7.198 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 029 | Total loss: 4.751 | Reg loss: 0.034 | Tree loss: 4.751 | Accuracy: 0.491124 | 7.198 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 029 | Total loss: 5.069 | Reg loss: 0.033 | Tree loss: 5.069 | Accuracy: 0.423828 | 7.255 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 029 | Total loss: 5.050 | Reg loss: 0.033 | Tree loss: 5.050 | Accuracy: 0.464844 | 7.255 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 029 | Total loss: 5.010 | Reg loss: 0.033 | Tree loss: 5.010 | Accuracy: 0.466797 | 7.254 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 029 | Total loss: 4.997 | Reg loss: 0.033 | Tree loss: 4.997 | Accuracy: 0.474609 | 7.251 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 029 | Total loss: 4.949 | Reg loss: 0.033 | Tree loss: 4.949 | Accuracy: 0.513672 | 7.247 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 029 | Total loss: 4.967 | Reg loss: 0.033 | Tree loss: 4.967 | Accuracy: 0.478516 | 7.243 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 029 | Total loss: 4.916 | Reg loss: 0.033 | Tree loss: 4.916 | Accuracy: 0.535156 | 7.241 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 029 | Total loss: 4.916 | Reg loss: 0.033 | Tree loss: 4.916 | Accuracy: 0.429688 | 7.238 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 029 | Total loss: 4.864 | Reg loss: 0.033 | Tree loss: 4.864 | Accuracy: 0.513672 | 7.237 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 029 | Total loss: 4.861 | Reg loss: 0.033 | Tree loss: 4.861 | Accuracy: 0.511719 | 7.237 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 029 | Total loss: 4.852 | Reg loss: 0.033 | Tree loss: 4.852 | Accuracy: 0.466797 | 7.237 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 029 | Total loss: 4.813 | Reg loss: 0.033 | Tree loss: 4.813 | Accuracy: 0.511719 | 7.236 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 029 | Total loss: 4.791 | Reg loss: 0.033 | Tree loss: 4.791 | Accuracy: 0.515625 | 7.234 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 029 | Total loss: 4.758 | Reg loss: 0.033 | Tree loss: 4.758 | Accuracy: 0.492188 | 7.233 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 029 | Total loss: 4.810 | Reg loss: 0.033 | Tree loss: 4.810 | Accuracy: 0.455078 | 7.232 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 029 | Total loss: 4.733 | Reg loss: 0.033 | Tree loss: 4.733 | Accuracy: 0.507812 | 7.231 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 029 | Total loss: 4.652 | Reg loss: 0.034 | Tree loss: 4.652 | Accuracy: 0.535156 | 7.23 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 029 | Total loss: 4.687 | Reg loss: 0.034 | Tree loss: 4.687 | Accuracy: 0.470703 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 029 | Total loss: 4.699 | Reg loss: 0.034 | Tree loss: 4.699 | Accuracy: 0.507812 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 029 | Total loss: 4.654 | Reg loss: 0.034 | Tree loss: 4.654 | Accuracy: 0.509766 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 029 | Total loss: 4.684 | Reg loss: 0.034 | Tree loss: 4.684 | Accuracy: 0.462891 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 029 | Total loss: 4.613 | Reg loss: 0.034 | Tree loss: 4.613 | Accuracy: 0.503906 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 029 | Total loss: 4.641 | Reg loss: 0.034 | Tree loss: 4.641 | Accuracy: 0.466797 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 029 | Total loss: 4.599 | Reg loss: 0.034 | Tree loss: 4.599 | Accuracy: 0.478516 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 029 | Total loss: 4.607 | Reg loss: 0.034 | Tree loss: 4.607 | Accuracy: 0.455078 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 029 | Total loss: 4.568 | Reg loss: 0.034 | Tree loss: 4.568 | Accuracy: 0.494141 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 029 | Total loss: 4.543 | Reg loss: 0.034 | Tree loss: 4.543 | Accuracy: 0.513672 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 029 | Total loss: 4.547 | Reg loss: 0.034 | Tree loss: 4.547 | Accuracy: 0.478516 | 7.229 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 029 | Total loss: 4.541 | Reg loss: 0.034 | Tree loss: 4.541 | Accuracy: 0.449704 | 7.228 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 029 | Total loss: 4.776 | Reg loss: 0.033 | Tree loss: 4.776 | Accuracy: 0.519531 | 7.265 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 029 | Total loss: 4.748 | Reg loss: 0.033 | Tree loss: 4.748 | Accuracy: 0.542969 | 7.261 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 029 | Total loss: 4.801 | Reg loss: 0.033 | Tree loss: 4.801 | Accuracy: 0.476562 | 7.258 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 029 | Total loss: 4.749 | Reg loss: 0.033 | Tree loss: 4.749 | Accuracy: 0.457031 | 7.258 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 029 | Total loss: 4.743 | Reg loss: 0.033 | Tree loss: 4.743 | Accuracy: 0.505859 | 7.256 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 029 | Total loss: 4.724 | Reg loss: 0.033 | Tree loss: 4.724 | Accuracy: 0.462891 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 029 | Total loss: 4.708 | Reg loss: 0.033 | Tree loss: 4.708 | Accuracy: 0.478516 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 029 | Total loss: 4.675 | Reg loss: 0.033 | Tree loss: 4.675 | Accuracy: 0.501953 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 029 | Total loss: 4.643 | Reg loss: 0.033 | Tree loss: 4.643 | Accuracy: 0.474609 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 029 | Total loss: 4.608 | Reg loss: 0.033 | Tree loss: 4.608 | Accuracy: 0.494141 | 7.254 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 029 | Total loss: 4.618 | Reg loss: 0.033 | Tree loss: 4.618 | Accuracy: 0.478516 | 7.254 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 029 | Total loss: 4.618 | Reg loss: 0.033 | Tree loss: 4.618 | Accuracy: 0.472656 | 7.254 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 029 | Total loss: 4.601 | Reg loss: 0.033 | Tree loss: 4.601 | Accuracy: 0.433594 | 7.254 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 029 | Total loss: 4.555 | Reg loss: 0.033 | Tree loss: 4.555 | Accuracy: 0.517578 | 7.254 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 029 | Total loss: 4.565 | Reg loss: 0.033 | Tree loss: 4.565 | Accuracy: 0.480469 | 7.254 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 029 | Total loss: 4.488 | Reg loss: 0.034 | Tree loss: 4.488 | Accuracy: 0.488281 | 7.254 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 029 | Total loss: 4.504 | Reg loss: 0.034 | Tree loss: 4.504 | Accuracy: 0.492188 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 029 | Total loss: 4.483 | Reg loss: 0.034 | Tree loss: 4.483 | Accuracy: 0.482422 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 029 | Total loss: 4.408 | Reg loss: 0.034 | Tree loss: 4.408 | Accuracy: 0.523438 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 029 | Total loss: 4.479 | Reg loss: 0.034 | Tree loss: 4.479 | Accuracy: 0.453125 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 029 | Total loss: 4.464 | Reg loss: 0.034 | Tree loss: 4.464 | Accuracy: 0.472656 | 7.253 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 029 | Total loss: 4.437 | Reg loss: 0.034 | Tree loss: 4.437 | Accuracy: 0.470703 | 7.252 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 029 | Total loss: 4.390 | Reg loss: 0.034 | Tree loss: 4.390 | Accuracy: 0.498047 | 7.252 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 029 | Total loss: 4.445 | Reg loss: 0.034 | Tree loss: 4.445 | Accuracy: 0.453125 | 7.252 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 029 | Total loss: 4.386 | Reg loss: 0.034 | Tree loss: 4.386 | Accuracy: 0.474609 | 7.251 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 029 | Total loss: 4.353 | Reg loss: 0.034 | Tree loss: 4.353 | Accuracy: 0.515625 | 7.251 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 029 | Total loss: 4.362 | Reg loss: 0.034 | Tree loss: 4.362 | Accuracy: 0.470703 | 7.25 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 029 | Total loss: 4.330 | Reg loss: 0.034 | Tree loss: 4.330 | Accuracy: 0.492188 | 7.249 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 029 | Total loss: 4.322 | Reg loss: 0.034 | Tree loss: 4.322 | Accuracy: 0.479290 | 7.248 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 029 | Total loss: 4.562 | Reg loss: 0.033 | Tree loss: 4.562 | Accuracy: 0.478516 | 7.257 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 029 | Total loss: 4.575 | Reg loss: 0.033 | Tree loss: 4.575 | Accuracy: 0.494141 | 7.256 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 029 | Total loss: 4.557 | Reg loss: 0.033 | Tree loss: 4.557 | Accuracy: 0.470703 | 7.256 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 029 | Total loss: 4.524 | Reg loss: 0.033 | Tree loss: 4.524 | Accuracy: 0.496094 | 7.254 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 029 | Total loss: 4.488 | Reg loss: 0.033 | Tree loss: 4.488 | Accuracy: 0.492188 | 7.252 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 029 | Total loss: 4.492 | Reg loss: 0.033 | Tree loss: 4.492 | Accuracy: 0.498047 | 7.248 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 029 | Total loss: 4.464 | Reg loss: 0.033 | Tree loss: 4.464 | Accuracy: 0.509766 | 7.244 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 029 | Total loss: 4.418 | Reg loss: 0.033 | Tree loss: 4.418 | Accuracy: 0.507812 | 7.24 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 029 | Total loss: 4.439 | Reg loss: 0.033 | Tree loss: 4.439 | Accuracy: 0.498047 | 7.236 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 029 | Total loss: 4.412 | Reg loss: 0.033 | Tree loss: 4.412 | Accuracy: 0.511719 | 7.232 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 029 | Total loss: 4.381 | Reg loss: 0.033 | Tree loss: 4.381 | Accuracy: 0.486328 | 7.228 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 029 | Total loss: 4.360 | Reg loss: 0.033 | Tree loss: 4.360 | Accuracy: 0.523438 | 7.224 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 029 | Total loss: 4.378 | Reg loss: 0.033 | Tree loss: 4.378 | Accuracy: 0.484375 | 7.22 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 029 | Total loss: 4.332 | Reg loss: 0.033 | Tree loss: 4.332 | Accuracy: 0.511719 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 029 | Total loss: 4.313 | Reg loss: 0.034 | Tree loss: 4.313 | Accuracy: 0.496094 | 7.217 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 029 | Total loss: 4.324 | Reg loss: 0.034 | Tree loss: 4.324 | Accuracy: 0.490234 | 7.217 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 029 | Total loss: 4.305 | Reg loss: 0.034 | Tree loss: 4.305 | Accuracy: 0.511719 | 7.217 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 029 | Total loss: 4.307 | Reg loss: 0.034 | Tree loss: 4.307 | Accuracy: 0.441406 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 029 | Total loss: 4.269 | Reg loss: 0.034 | Tree loss: 4.269 | Accuracy: 0.462891 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 029 | Total loss: 4.240 | Reg loss: 0.034 | Tree loss: 4.240 | Accuracy: 0.472656 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 029 | Total loss: 4.257 | Reg loss: 0.034 | Tree loss: 4.257 | Accuracy: 0.453125 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 029 | Total loss: 4.229 | Reg loss: 0.034 | Tree loss: 4.229 | Accuracy: 0.474609 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 029 | Total loss: 4.188 | Reg loss: 0.034 | Tree loss: 4.188 | Accuracy: 0.478516 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 029 | Total loss: 4.206 | Reg loss: 0.034 | Tree loss: 4.206 | Accuracy: 0.509766 | 7.216 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 029 | Total loss: 4.203 | Reg loss: 0.034 | Tree loss: 4.203 | Accuracy: 0.451172 | 7.215 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 029 | Total loss: 4.152 | Reg loss: 0.034 | Tree loss: 4.152 | Accuracy: 0.457031 | 7.215 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 029 | Total loss: 4.148 | Reg loss: 0.034 | Tree loss: 4.148 | Accuracy: 0.466797 | 7.215 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 029 | Total loss: 4.166 | Reg loss: 0.034 | Tree loss: 4.166 | Accuracy: 0.449219 | 7.214 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 029 | Total loss: 4.114 | Reg loss: 0.034 | Tree loss: 4.114 | Accuracy: 0.489152 | 7.212 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 029 | Total loss: 4.340 | Reg loss: 0.033 | Tree loss: 4.340 | Accuracy: 0.517578 | 7.221 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 029 | Total loss: 4.375 | Reg loss: 0.033 | Tree loss: 4.375 | Accuracy: 0.484375 | 7.221 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 029 | Total loss: 4.349 | Reg loss: 0.033 | Tree loss: 4.349 | Accuracy: 0.460938 | 7.221 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 029 | Total loss: 4.319 | Reg loss: 0.033 | Tree loss: 4.319 | Accuracy: 0.507812 | 7.221 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 029 | Total loss: 4.313 | Reg loss: 0.033 | Tree loss: 4.313 | Accuracy: 0.488281 | 7.22 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 029 | Total loss: 4.249 | Reg loss: 0.033 | Tree loss: 4.249 | Accuracy: 0.521484 | 7.218 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 029 | Total loss: 4.256 | Reg loss: 0.033 | Tree loss: 4.256 | Accuracy: 0.488281 | 7.217 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 029 | Total loss: 4.288 | Reg loss: 0.033 | Tree loss: 4.288 | Accuracy: 0.460938 | 7.216 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 029 | Total loss: 4.244 | Reg loss: 0.033 | Tree loss: 4.244 | Accuracy: 0.460938 | 7.215 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 029 | Total loss: 4.221 | Reg loss: 0.033 | Tree loss: 4.221 | Accuracy: 0.482422 | 7.215 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 029 | Total loss: 4.191 | Reg loss: 0.033 | Tree loss: 4.191 | Accuracy: 0.498047 | 7.214 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 029 | Total loss: 4.166 | Reg loss: 0.033 | Tree loss: 4.166 | Accuracy: 0.492188 | 7.214 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 029 | Total loss: 4.185 | Reg loss: 0.033 | Tree loss: 4.185 | Accuracy: 0.441406 | 7.214 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 029 | Total loss: 4.168 | Reg loss: 0.033 | Tree loss: 4.168 | Accuracy: 0.447266 | 7.211 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 029 | Total loss: 4.114 | Reg loss: 0.033 | Tree loss: 4.114 | Accuracy: 0.500000 | 7.207 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 029 | Total loss: 4.113 | Reg loss: 0.034 | Tree loss: 4.113 | Accuracy: 0.441406 | 7.203 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 029 | Total loss: 4.040 | Reg loss: 0.034 | Tree loss: 4.040 | Accuracy: 0.556641 | 7.2 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 029 | Total loss: 4.064 | Reg loss: 0.034 | Tree loss: 4.064 | Accuracy: 0.507812 | 7.196 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 029 | Total loss: 4.058 | Reg loss: 0.034 | Tree loss: 4.058 | Accuracy: 0.505859 | 7.192 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 029 | Total loss: 4.041 | Reg loss: 0.034 | Tree loss: 4.041 | Accuracy: 0.494141 | 7.189 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 029 | Total loss: 4.027 | Reg loss: 0.034 | Tree loss: 4.027 | Accuracy: 0.490234 | 7.185 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 029 | Total loss: 4.015 | Reg loss: 0.034 | Tree loss: 4.015 | Accuracy: 0.458984 | 7.181 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 029 | Total loss: 3.979 | Reg loss: 0.034 | Tree loss: 3.979 | Accuracy: 0.474609 | 7.182 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 029 | Total loss: 3.984 | Reg loss: 0.034 | Tree loss: 3.984 | Accuracy: 0.470703 | 7.182 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 029 | Total loss: 3.985 | Reg loss: 0.034 | Tree loss: 3.985 | Accuracy: 0.451172 | 7.182 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 029 | Total loss: 3.960 | Reg loss: 0.034 | Tree loss: 3.960 | Accuracy: 0.486328 | 7.183 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 029 | Total loss: 3.967 | Reg loss: 0.034 | Tree loss: 3.967 | Accuracy: 0.470703 | 7.183 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 029 | Total loss: 3.953 | Reg loss: 0.034 | Tree loss: 3.953 | Accuracy: 0.439453 | 7.181 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 029 | Total loss: 3.913 | Reg loss: 0.034 | Tree loss: 3.913 | Accuracy: 0.479290 | 7.178 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 029 | Total loss: 4.205 | Reg loss: 0.033 | Tree loss: 4.205 | Accuracy: 0.458984 | 7.198 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 029 | Total loss: 4.163 | Reg loss: 0.033 | Tree loss: 4.163 | Accuracy: 0.466797 | 7.197 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 029 | Total loss: 4.122 | Reg loss: 0.033 | Tree loss: 4.122 | Accuracy: 0.500000 | 7.197 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 029 | Total loss: 4.156 | Reg loss: 0.033 | Tree loss: 4.156 | Accuracy: 0.449219 | 7.197 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 029 | Total loss: 4.087 | Reg loss: 0.033 | Tree loss: 4.087 | Accuracy: 0.480469 | 7.197 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 029 | Total loss: 4.086 | Reg loss: 0.033 | Tree loss: 4.086 | Accuracy: 0.478516 | 7.197 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 029 | Total loss: 4.053 | Reg loss: 0.033 | Tree loss: 4.053 | Accuracy: 0.503906 | 7.197 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 029 | Total loss: 4.045 | Reg loss: 0.033 | Tree loss: 4.045 | Accuracy: 0.474609 | 7.197 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 029 | Total loss: 4.007 | Reg loss: 0.033 | Tree loss: 4.007 | Accuracy: 0.474609 | 7.196 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 029 | Total loss: 4.005 | Reg loss: 0.033 | Tree loss: 4.005 | Accuracy: 0.494141 | 7.196 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 029 | Total loss: 4.006 | Reg loss: 0.033 | Tree loss: 4.006 | Accuracy: 0.476562 | 7.196 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 029 | Total loss: 3.930 | Reg loss: 0.033 | Tree loss: 3.930 | Accuracy: 0.517578 | 7.196 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 029 | Total loss: 3.960 | Reg loss: 0.033 | Tree loss: 3.960 | Accuracy: 0.457031 | 7.195 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 029 | Total loss: 3.960 | Reg loss: 0.033 | Tree loss: 3.960 | Accuracy: 0.466797 | 7.195 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 029 | Total loss: 3.900 | Reg loss: 0.033 | Tree loss: 3.900 | Accuracy: 0.511719 | 7.194 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 029 | Total loss: 3.925 | Reg loss: 0.033 | Tree loss: 3.925 | Accuracy: 0.466797 | 7.194 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 029 | Total loss: 3.883 | Reg loss: 0.034 | Tree loss: 3.883 | Accuracy: 0.490234 | 7.194 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 029 | Total loss: 3.917 | Reg loss: 0.034 | Tree loss: 3.917 | Accuracy: 0.494141 | 7.194 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 029 | Total loss: 3.877 | Reg loss: 0.034 | Tree loss: 3.877 | Accuracy: 0.478516 | 7.193 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 029 | Total loss: 3.883 | Reg loss: 0.034 | Tree loss: 3.883 | Accuracy: 0.457031 | 7.192 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 029 | Total loss: 3.828 | Reg loss: 0.034 | Tree loss: 3.828 | Accuracy: 0.503906 | 7.188 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 029 | Total loss: 3.776 | Reg loss: 0.034 | Tree loss: 3.776 | Accuracy: 0.541016 | 7.184 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 029 | Total loss: 3.812 | Reg loss: 0.034 | Tree loss: 3.812 | Accuracy: 0.474609 | 7.181 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 029 | Total loss: 3.796 | Reg loss: 0.034 | Tree loss: 3.796 | Accuracy: 0.494141 | 7.177 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 029 | Total loss: 3.765 | Reg loss: 0.034 | Tree loss: 3.765 | Accuracy: 0.531250 | 7.174 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 029 | Total loss: 3.804 | Reg loss: 0.034 | Tree loss: 3.804 | Accuracy: 0.472656 | 7.171 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 029 | Total loss: 3.771 | Reg loss: 0.034 | Tree loss: 3.771 | Accuracy: 0.447266 | 7.169 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 029 | Total loss: 3.713 | Reg loss: 0.034 | Tree loss: 3.713 | Accuracy: 0.482422 | 7.166 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 029 | Total loss: 3.732 | Reg loss: 0.034 | Tree loss: 3.732 | Accuracy: 0.461538 | 7.166 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 029 | Total loss: 3.969 | Reg loss: 0.033 | Tree loss: 3.969 | Accuracy: 0.507812 | 7.211 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 029 | Total loss: 3.928 | Reg loss: 0.033 | Tree loss: 3.928 | Accuracy: 0.492188 | 7.21 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 029 | Total loss: 3.986 | Reg loss: 0.033 | Tree loss: 3.986 | Accuracy: 0.470703 | 7.21 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 029 | Total loss: 3.901 | Reg loss: 0.033 | Tree loss: 3.901 | Accuracy: 0.509766 | 7.21 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 029 | Total loss: 3.900 | Reg loss: 0.033 | Tree loss: 3.900 | Accuracy: 0.498047 | 7.21 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 029 | Total loss: 3.872 | Reg loss: 0.033 | Tree loss: 3.872 | Accuracy: 0.515625 | 7.209 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 029 | Total loss: 3.873 | Reg loss: 0.033 | Tree loss: 3.873 | Accuracy: 0.494141 | 7.209 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 029 | Total loss: 3.863 | Reg loss: 0.033 | Tree loss: 3.863 | Accuracy: 0.488281 | 7.209 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 029 | Total loss: 3.868 | Reg loss: 0.033 | Tree loss: 3.868 | Accuracy: 0.482422 | 7.209 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 029 | Total loss: 3.832 | Reg loss: 0.033 | Tree loss: 3.832 | Accuracy: 0.460938 | 7.209 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 029 | Total loss: 3.784 | Reg loss: 0.033 | Tree loss: 3.784 | Accuracy: 0.500000 | 7.208 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 029 | Total loss: 3.768 | Reg loss: 0.033 | Tree loss: 3.768 | Accuracy: 0.480469 | 7.208 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 029 | Total loss: 3.793 | Reg loss: 0.033 | Tree loss: 3.793 | Accuracy: 0.451172 | 7.208 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 029 | Total loss: 3.742 | Reg loss: 0.033 | Tree loss: 3.742 | Accuracy: 0.492188 | 7.207 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 029 | Total loss: 3.708 | Reg loss: 0.033 | Tree loss: 3.708 | Accuracy: 0.523438 | 7.207 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 029 | Total loss: 3.720 | Reg loss: 0.033 | Tree loss: 3.720 | Accuracy: 0.494141 | 7.207 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 029 | Total loss: 3.661 | Reg loss: 0.033 | Tree loss: 3.661 | Accuracy: 0.507812 | 7.207 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 029 | Total loss: 3.676 | Reg loss: 0.033 | Tree loss: 3.676 | Accuracy: 0.501953 | 7.207 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 029 | Total loss: 3.669 | Reg loss: 0.034 | Tree loss: 3.669 | Accuracy: 0.490234 | 7.206 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 029 | Total loss: 3.680 | Reg loss: 0.034 | Tree loss: 3.680 | Accuracy: 0.472656 | 7.202 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 029 | Total loss: 3.663 | Reg loss: 0.034 | Tree loss: 3.663 | Accuracy: 0.455078 | 7.197 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 029 | Total loss: 3.649 | Reg loss: 0.034 | Tree loss: 3.649 | Accuracy: 0.482422 | 7.195 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 029 | Total loss: 3.622 | Reg loss: 0.034 | Tree loss: 3.622 | Accuracy: 0.478516 | 7.195 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 029 | Total loss: 3.640 | Reg loss: 0.034 | Tree loss: 3.640 | Accuracy: 0.484375 | 7.195 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 029 | Total loss: 3.597 | Reg loss: 0.034 | Tree loss: 3.597 | Accuracy: 0.458984 | 7.196 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 029 | Total loss: 3.580 | Reg loss: 0.034 | Tree loss: 3.580 | Accuracy: 0.458984 | 7.196 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 029 | Total loss: 3.589 | Reg loss: 0.034 | Tree loss: 3.589 | Accuracy: 0.455078 | 7.196 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 029 | Total loss: 3.578 | Reg loss: 0.034 | Tree loss: 3.578 | Accuracy: 0.488281 | 7.196 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 029 | Total loss: 3.566 | Reg loss: 0.034 | Tree loss: 3.566 | Accuracy: 0.475345 | 7.196 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 029 | Total loss: 3.737 | Reg loss: 0.033 | Tree loss: 3.737 | Accuracy: 0.537109 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 029 | Total loss: 3.753 | Reg loss: 0.033 | Tree loss: 3.753 | Accuracy: 0.498047 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 029 | Total loss: 3.760 | Reg loss: 0.033 | Tree loss: 3.760 | Accuracy: 0.455078 | 7.244 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 003 / 029 | Total loss: 3.726 | Reg loss: 0.033 | Tree loss: 3.726 | Accuracy: 0.480469 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 029 | Total loss: 3.731 | Reg loss: 0.033 | Tree loss: 3.731 | Accuracy: 0.498047 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 029 | Total loss: 3.719 | Reg loss: 0.033 | Tree loss: 3.719 | Accuracy: 0.501953 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 029 | Total loss: 3.697 | Reg loss: 0.033 | Tree loss: 3.697 | Accuracy: 0.464844 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 029 | Total loss: 3.673 | Reg loss: 0.033 | Tree loss: 3.673 | Accuracy: 0.464844 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 029 | Total loss: 3.655 | Reg loss: 0.033 | Tree loss: 3.655 | Accuracy: 0.507812 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 029 | Total loss: 3.626 | Reg loss: 0.033 | Tree loss: 3.626 | Accuracy: 0.507812 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 029 | Total loss: 3.593 | Reg loss: 0.033 | Tree loss: 3.593 | Accuracy: 0.501953 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 029 | Total loss: 3.604 | Reg loss: 0.033 | Tree loss: 3.604 | Accuracy: 0.505859 | 7.244 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 029 | Total loss: 3.586 | Reg loss: 0.033 | Tree loss: 3.586 | Accuracy: 0.503906 | 7.241 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 029 | Total loss: 3.570 | Reg loss: 0.033 | Tree loss: 3.570 | Accuracy: 0.503906 | 7.237 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 029 | Total loss: 3.558 | Reg loss: 0.033 | Tree loss: 3.558 | Accuracy: 0.472656 | 7.237 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 029 | Total loss: 3.569 | Reg loss: 0.033 | Tree loss: 3.569 | Accuracy: 0.462891 | 7.237 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 029 | Total loss: 3.542 | Reg loss: 0.033 | Tree loss: 3.542 | Accuracy: 0.466797 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 029 | Total loss: 3.540 | Reg loss: 0.033 | Tree loss: 3.540 | Accuracy: 0.472656 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 029 | Total loss: 3.483 | Reg loss: 0.033 | Tree loss: 3.483 | Accuracy: 0.501953 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 029 | Total loss: 3.517 | Reg loss: 0.033 | Tree loss: 3.517 | Accuracy: 0.462891 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 029 | Total loss: 3.461 | Reg loss: 0.034 | Tree loss: 3.461 | Accuracy: 0.474609 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 029 | Total loss: 3.450 | Reg loss: 0.034 | Tree loss: 3.450 | Accuracy: 0.501953 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 029 | Total loss: 3.458 | Reg loss: 0.034 | Tree loss: 3.458 | Accuracy: 0.460938 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 029 | Total loss: 3.393 | Reg loss: 0.034 | Tree loss: 3.393 | Accuracy: 0.525391 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 029 | Total loss: 3.412 | Reg loss: 0.034 | Tree loss: 3.412 | Accuracy: 0.476562 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 029 | Total loss: 3.396 | Reg loss: 0.034 | Tree loss: 3.396 | Accuracy: 0.482422 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 029 | Total loss: 3.405 | Reg loss: 0.034 | Tree loss: 3.405 | Accuracy: 0.470703 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 029 | Total loss: 3.387 | Reg loss: 0.034 | Tree loss: 3.387 | Accuracy: 0.474609 | 7.238 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 029 | Total loss: 3.369 | Reg loss: 0.034 | Tree loss: 3.369 | Accuracy: 0.483235 | 7.238 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 029 | Total loss: 3.581 | Reg loss: 0.033 | Tree loss: 3.581 | Accuracy: 0.492188 | 7.254 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 029 | Total loss: 3.584 | Reg loss: 0.033 | Tree loss: 3.584 | Accuracy: 0.490234 | 7.254 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 029 | Total loss: 3.585 | Reg loss: 0.033 | Tree loss: 3.585 | Accuracy: 0.470703 | 7.254 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 029 | Total loss: 3.560 | Reg loss: 0.033 | Tree loss: 3.560 | Accuracy: 0.501953 | 7.254 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 029 | Total loss: 3.551 | Reg loss: 0.033 | Tree loss: 3.551 | Accuracy: 0.480469 | 7.253 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 029 | Total loss: 3.502 | Reg loss: 0.033 | Tree loss: 3.502 | Accuracy: 0.517578 | 7.253 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 029 | Total loss: 3.510 | Reg loss: 0.033 | Tree loss: 3.510 | Accuracy: 0.484375 | 7.253 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 029 | Total loss: 3.507 | Reg loss: 0.033 | Tree loss: 3.507 | Accuracy: 0.474609 | 7.253 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 029 | Total loss: 3.463 | Reg loss: 0.033 | Tree loss: 3.463 | Accuracy: 0.525391 | 7.252 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 029 | Total loss: 3.459 | Reg loss: 0.033 | Tree loss: 3.459 | Accuracy: 0.480469 | 7.25 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 029 | Total loss: 3.438 | Reg loss: 0.033 | Tree loss: 3.438 | Accuracy: 0.480469 | 7.25 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 029 | Total loss: 3.414 | Reg loss: 0.033 | Tree loss: 3.414 | Accuracy: 0.503906 | 7.248 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 029 | Total loss: 3.404 | Reg loss: 0.033 | Tree loss: 3.404 | Accuracy: 0.468750 | 7.246 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 029 | Total loss: 3.413 | Reg loss: 0.033 | Tree loss: 3.413 | Accuracy: 0.486328 | 7.243 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 029 | Total loss: 3.382 | Reg loss: 0.033 | Tree loss: 3.382 | Accuracy: 0.490234 | 7.243 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 029 | Total loss: 3.384 | Reg loss: 0.033 | Tree loss: 3.384 | Accuracy: 0.470703 | 7.242 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 029 | Total loss: 3.345 | Reg loss: 0.033 | Tree loss: 3.345 | Accuracy: 0.501953 | 7.242 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 029 | Total loss: 3.353 | Reg loss: 0.033 | Tree loss: 3.353 | Accuracy: 0.470703 | 7.241 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 029 | Total loss: 3.306 | Reg loss: 0.033 | Tree loss: 3.306 | Accuracy: 0.496094 | 7.24 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 029 | Total loss: 3.297 | Reg loss: 0.033 | Tree loss: 3.297 | Accuracy: 0.478516 | 7.24 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 029 | Total loss: 3.277 | Reg loss: 0.033 | Tree loss: 3.277 | Accuracy: 0.482422 | 7.239 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 029 | Total loss: 3.292 | Reg loss: 0.034 | Tree loss: 3.292 | Accuracy: 0.480469 | 7.239 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 029 | Total loss: 3.279 | Reg loss: 0.034 | Tree loss: 3.279 | Accuracy: 0.449219 | 7.238 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 029 | Total loss: 3.252 | Reg loss: 0.034 | Tree loss: 3.252 | Accuracy: 0.500000 | 7.238 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 029 | Total loss: 3.224 | Reg loss: 0.034 | Tree loss: 3.224 | Accuracy: 0.484375 | 7.238 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 029 | Total loss: 3.217 | Reg loss: 0.034 | Tree loss: 3.217 | Accuracy: 0.507812 | 7.237 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 029 | Total loss: 3.235 | Reg loss: 0.034 | Tree loss: 3.235 | Accuracy: 0.494141 | 7.237 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 029 | Total loss: 3.217 | Reg loss: 0.034 | Tree loss: 3.217 | Accuracy: 0.494141 | 7.237 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 029 | Total loss: 3.189 | Reg loss: 0.034 | Tree loss: 3.189 | Accuracy: 0.483235 | 7.237 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 029 | Total loss: 3.408 | Reg loss: 0.032 | Tree loss: 3.408 | Accuracy: 0.548828 | 7.288 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 029 | Total loss: 3.433 | Reg loss: 0.032 | Tree loss: 3.433 | Accuracy: 0.476562 | 7.285 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 029 | Total loss: 3.410 | Reg loss: 0.032 | Tree loss: 3.410 | Accuracy: 0.453125 | 7.282 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 029 | Total loss: 3.378 | Reg loss: 0.032 | Tree loss: 3.378 | Accuracy: 0.494141 | 7.278 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 029 | Total loss: 3.389 | Reg loss: 0.032 | Tree loss: 3.389 | Accuracy: 0.464844 | 7.275 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 029 | Total loss: 3.354 | Reg loss: 0.033 | Tree loss: 3.354 | Accuracy: 0.484375 | 7.272 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 006 / 029 | Total loss: 3.364 | Reg loss: 0.033 | Tree loss: 3.364 | Accuracy: 0.460938 | 7.269 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 029 | Total loss: 3.279 | Reg loss: 0.033 | Tree loss: 3.279 | Accuracy: 0.517578 | 7.266 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 029 | Total loss: 3.290 | Reg loss: 0.033 | Tree loss: 3.290 | Accuracy: 0.498047 | 7.266 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 029 | Total loss: 3.281 | Reg loss: 0.033 | Tree loss: 3.281 | Accuracy: 0.492188 | 7.265 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 029 | Total loss: 3.300 | Reg loss: 0.033 | Tree loss: 3.300 | Accuracy: 0.462891 | 7.264 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 029 | Total loss: 3.257 | Reg loss: 0.033 | Tree loss: 3.257 | Accuracy: 0.488281 | 7.263 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 029 | Total loss: 3.265 | Reg loss: 0.033 | Tree loss: 3.265 | Accuracy: 0.462891 | 7.263 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 029 | Total loss: 3.238 | Reg loss: 0.033 | Tree loss: 3.238 | Accuracy: 0.462891 | 7.262 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 029 | Total loss: 3.247 | Reg loss: 0.033 | Tree loss: 3.247 | Accuracy: 0.441406 | 7.261 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 029 | Total loss: 3.199 | Reg loss: 0.033 | Tree loss: 3.199 | Accuracy: 0.498047 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 029 | Total loss: 3.184 | Reg loss: 0.033 | Tree loss: 3.184 | Accuracy: 0.494141 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 029 | Total loss: 3.155 | Reg loss: 0.033 | Tree loss: 3.155 | Accuracy: 0.490234 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 029 | Total loss: 3.152 | Reg loss: 0.033 | Tree loss: 3.152 | Accuracy: 0.501953 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 029 | Total loss: 3.118 | Reg loss: 0.033 | Tree loss: 3.118 | Accuracy: 0.515625 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 029 | Total loss: 3.111 | Reg loss: 0.033 | Tree loss: 3.111 | Accuracy: 0.513672 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 029 | Total loss: 3.080 | Reg loss: 0.033 | Tree loss: 3.080 | Accuracy: 0.539062 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 029 | Total loss: 3.064 | Reg loss: 0.034 | Tree loss: 3.064 | Accuracy: 0.541016 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 029 | Total loss: 3.116 | Reg loss: 0.034 | Tree loss: 3.116 | Accuracy: 0.466797 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 029 | Total loss: 3.036 | Reg loss: 0.034 | Tree loss: 3.036 | Accuracy: 0.517578 | 7.26 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 029 | Total loss: 3.048 | Reg loss: 0.034 | Tree loss: 3.048 | Accuracy: 0.511719 | 7.259 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 029 | Total loss: 3.071 | Reg loss: 0.034 | Tree loss: 3.071 | Accuracy: 0.449219 | 7.259 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 029 | Total loss: 3.047 | Reg loss: 0.034 | Tree loss: 3.047 | Accuracy: 0.482422 | 7.259 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 029 | Total loss: 3.013 | Reg loss: 0.034 | Tree loss: 3.013 | Accuracy: 0.524655 | 7.259 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 029 | Total loss: 3.272 | Reg loss: 0.032 | Tree loss: 3.272 | Accuracy: 0.488281 | 7.287 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 029 | Total loss: 3.220 | Reg loss: 0.032 | Tree loss: 3.220 | Accuracy: 0.523438 | 7.285 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 029 | Total loss: 3.224 | Reg loss: 0.032 | Tree loss: 3.224 | Accuracy: 0.494141 | 7.282 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 029 | Total loss: 3.228 | Reg loss: 0.032 | Tree loss: 3.228 | Accuracy: 0.496094 | 7.281 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 029 | Total loss: 3.197 | Reg loss: 0.032 | Tree loss: 3.197 | Accuracy: 0.474609 | 7.278 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 029 | Total loss: 3.220 | Reg loss: 0.032 | Tree loss: 3.220 | Accuracy: 0.466797 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 029 | Total loss: 3.220 | Reg loss: 0.032 | Tree loss: 3.220 | Accuracy: 0.441406 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 029 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.531250 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 029 | Total loss: 3.172 | Reg loss: 0.032 | Tree loss: 3.172 | Accuracy: 0.488281 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 029 | Total loss: 3.122 | Reg loss: 0.033 | Tree loss: 3.122 | Accuracy: 0.472656 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 029 | Total loss: 3.142 | Reg loss: 0.033 | Tree loss: 3.142 | Accuracy: 0.455078 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 029 | Total loss: 3.089 | Reg loss: 0.033 | Tree loss: 3.089 | Accuracy: 0.498047 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 029 | Total loss: 3.051 | Reg loss: 0.033 | Tree loss: 3.051 | Accuracy: 0.531250 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 029 | Total loss: 3.055 | Reg loss: 0.033 | Tree loss: 3.055 | Accuracy: 0.521484 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 029 | Total loss: 3.002 | Reg loss: 0.033 | Tree loss: 3.002 | Accuracy: 0.556641 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 029 | Total loss: 3.005 | Reg loss: 0.033 | Tree loss: 3.005 | Accuracy: 0.521484 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 029 | Total loss: 3.010 | Reg loss: 0.033 | Tree loss: 3.010 | Accuracy: 0.519531 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 029 | Total loss: 3.009 | Reg loss: 0.033 | Tree loss: 3.009 | Accuracy: 0.505859 | 7.279 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 029 | Total loss: 2.955 | Reg loss: 0.033 | Tree loss: 2.955 | Accuracy: 0.519531 | 7.278 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 029 | Total loss: 2.997 | Reg loss: 0.033 | Tree loss: 2.997 | Accuracy: 0.457031 | 7.278 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 029 | Total loss: 2.971 | Reg loss: 0.033 | Tree loss: 2.971 | Accuracy: 0.474609 | 7.278 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 029 | Total loss: 2.962 | Reg loss: 0.033 | Tree loss: 2.962 | Accuracy: 0.478516 | 7.278 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 029 | Total loss: 2.924 | Reg loss: 0.034 | Tree loss: 2.924 | Accuracy: 0.525391 | 7.277 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 029 | Total loss: 2.944 | Reg loss: 0.034 | Tree loss: 2.944 | Accuracy: 0.464844 | 7.277 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 029 | Total loss: 2.924 | Reg loss: 0.034 | Tree loss: 2.924 | Accuracy: 0.476562 | 7.276 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 029 | Total loss: 2.920 | Reg loss: 0.034 | Tree loss: 2.920 | Accuracy: 0.447266 | 7.276 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 029 | Total loss: 2.889 | Reg loss: 0.034 | Tree loss: 2.889 | Accuracy: 0.503906 | 7.276 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 029 | Total loss: 2.863 | Reg loss: 0.034 | Tree loss: 2.863 | Accuracy: 0.501953 | 7.275 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 029 | Total loss: 2.887 | Reg loss: 0.034 | Tree loss: 2.887 | Accuracy: 0.483235 | 7.275 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 029 | Total loss: 3.123 | Reg loss: 0.032 | Tree loss: 3.123 | Accuracy: 0.472656 | 7.281 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 029 | Total loss: 3.100 | Reg loss: 0.032 | Tree loss: 3.100 | Accuracy: 0.488281 | 7.281 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 029 | Total loss: 3.086 | Reg loss: 0.032 | Tree loss: 3.086 | Accuracy: 0.490234 | 7.28 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 029 | Total loss: 3.041 | Reg loss: 0.032 | Tree loss: 3.041 | Accuracy: 0.517578 | 7.278 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 029 | Total loss: 3.067 | Reg loss: 0.032 | Tree loss: 3.067 | Accuracy: 0.490234 | 7.275 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 029 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.480469 | 7.272 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 029 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.496094 | 7.269 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 029 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.503906 | 7.266 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 029 | Total loss: 2.978 | Reg loss: 0.033 | Tree loss: 2.978 | Accuracy: 0.498047 | 7.263 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Batch: 009 / 029 | Total loss: 2.975 | Reg loss: 0.033 | Tree loss: 2.975 | Accuracy: 0.478516 | 7.261 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 029 | Total loss: 2.923 | Reg loss: 0.033 | Tree loss: 2.923 | Accuracy: 0.480469 | 7.258 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 029 | Total loss: 2.941 | Reg loss: 0.033 | Tree loss: 2.941 | Accuracy: 0.486328 | 7.258 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 029 | Total loss: 2.934 | Reg loss: 0.033 | Tree loss: 2.934 | Accuracy: 0.488281 | 7.257 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 029 | Total loss: 2.901 | Reg loss: 0.033 | Tree loss: 2.901 | Accuracy: 0.494141 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 029 | Total loss: 2.920 | Reg loss: 0.033 | Tree loss: 2.920 | Accuracy: 0.458984 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 029 | Total loss: 2.895 | Reg loss: 0.033 | Tree loss: 2.895 | Accuracy: 0.496094 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 029 | Total loss: 2.892 | Reg loss: 0.033 | Tree loss: 2.892 | Accuracy: 0.476562 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 029 | Total loss: 2.825 | Reg loss: 0.033 | Tree loss: 2.825 | Accuracy: 0.519531 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 029 | Total loss: 2.814 | Reg loss: 0.033 | Tree loss: 2.814 | Accuracy: 0.525391 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 029 | Total loss: 2.829 | Reg loss: 0.033 | Tree loss: 2.829 | Accuracy: 0.480469 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 029 | Total loss: 2.799 | Reg loss: 0.033 | Tree loss: 2.799 | Accuracy: 0.509766 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 029 | Total loss: 2.817 | Reg loss: 0.033 | Tree loss: 2.817 | Accuracy: 0.458984 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 029 | Total loss: 2.786 | Reg loss: 0.034 | Tree loss: 2.786 | Accuracy: 0.490234 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 029 | Total loss: 2.721 | Reg loss: 0.034 | Tree loss: 2.721 | Accuracy: 0.533203 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 029 | Total loss: 2.762 | Reg loss: 0.034 | Tree loss: 2.762 | Accuracy: 0.488281 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 029 | Total loss: 2.758 | Reg loss: 0.034 | Tree loss: 2.758 | Accuracy: 0.480469 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 029 | Total loss: 2.719 | Reg loss: 0.034 | Tree loss: 2.719 | Accuracy: 0.517578 | 7.256 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 029 | Total loss: 2.688 | Reg loss: 0.034 | Tree loss: 2.688 | Accuracy: 0.521484 | 7.255 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 029 | Total loss: 2.730 | Reg loss: 0.034 | Tree loss: 2.730 | Accuracy: 0.510848 | 7.255 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 029 | Total loss: 2.989 | Reg loss: 0.032 | Tree loss: 2.989 | Accuracy: 0.462891 | 7.26 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 029 | Total loss: 2.960 | Reg loss: 0.032 | Tree loss: 2.960 | Accuracy: 0.509766 | 7.26 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 029 | Total loss: 2.929 | Reg loss: 0.032 | Tree loss: 2.929 | Accuracy: 0.505859 | 7.26 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 029 | Total loss: 2.884 | Reg loss: 0.032 | Tree loss: 2.884 | Accuracy: 0.519531 | 7.259 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 029 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.472656 | 7.259 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 029 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.503906 | 7.258 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 029 | Total loss: 2.843 | Reg loss: 0.033 | Tree loss: 2.843 | Accuracy: 0.519531 | 7.257 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 029 | Total loss: 2.820 | Reg loss: 0.033 | Tree loss: 2.820 | Accuracy: 0.533203 | 7.256 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 029 | Total loss: 2.826 | Reg loss: 0.033 | Tree loss: 2.826 | Accuracy: 0.519531 | 7.255 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 029 | Total loss: 2.818 | Reg loss: 0.033 | Tree loss: 2.818 | Accuracy: 0.531250 | 7.254 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 029 | Total loss: 2.807 | Reg loss: 0.033 | Tree loss: 2.807 | Accuracy: 0.486328 | 7.252 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 029 | Total loss: 2.797 | Reg loss: 0.033 | Tree loss: 2.797 | Accuracy: 0.478516 | 7.25 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 029 | Total loss: 2.771 | Reg loss: 0.033 | Tree loss: 2.771 | Accuracy: 0.478516 | 7.247 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 029 | Total loss: 2.769 | Reg loss: 0.033 | Tree loss: 2.769 | Accuracy: 0.476562 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 029 | Total loss: 2.719 | Reg loss: 0.033 | Tree loss: 2.719 | Accuracy: 0.513672 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 029 | Total loss: 2.715 | Reg loss: 0.033 | Tree loss: 2.715 | Accuracy: 0.527344 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 029 | Total loss: 2.728 | Reg loss: 0.033 | Tree loss: 2.728 | Accuracy: 0.462891 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 029 | Total loss: 2.701 | Reg loss: 0.033 | Tree loss: 2.701 | Accuracy: 0.492188 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 029 | Total loss: 2.710 | Reg loss: 0.033 | Tree loss: 2.710 | Accuracy: 0.468750 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 029 | Total loss: 2.671 | Reg loss: 0.033 | Tree loss: 2.671 | Accuracy: 0.503906 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 029 | Total loss: 2.689 | Reg loss: 0.033 | Tree loss: 2.689 | Accuracy: 0.480469 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 029 | Total loss: 2.645 | Reg loss: 0.034 | Tree loss: 2.645 | Accuracy: 0.517578 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 029 | Total loss: 2.644 | Reg loss: 0.034 | Tree loss: 2.644 | Accuracy: 0.488281 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 029 | Total loss: 2.628 | Reg loss: 0.034 | Tree loss: 2.628 | Accuracy: 0.490234 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 029 | Total loss: 2.622 | Reg loss: 0.034 | Tree loss: 2.622 | Accuracy: 0.453125 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 029 | Total loss: 2.628 | Reg loss: 0.034 | Tree loss: 2.628 | Accuracy: 0.476562 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 029 | Total loss: 2.619 | Reg loss: 0.034 | Tree loss: 2.619 | Accuracy: 0.466797 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 029 | Total loss: 2.605 | Reg loss: 0.034 | Tree loss: 2.605 | Accuracy: 0.476562 | 7.245 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 029 | Total loss: 2.564 | Reg loss: 0.034 | Tree loss: 2.564 | Accuracy: 0.516765 | 7.242 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 029 | Total loss: 2.840 | Reg loss: 0.033 | Tree loss: 2.840 | Accuracy: 0.480469 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 029 | Total loss: 2.792 | Reg loss: 0.033 | Tree loss: 2.792 | Accuracy: 0.490234 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 029 | Total loss: 2.808 | Reg loss: 0.033 | Tree loss: 2.808 | Accuracy: 0.500000 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 029 | Total loss: 2.770 | Reg loss: 0.033 | Tree loss: 2.770 | Accuracy: 0.484375 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 029 | Total loss: 2.768 | Reg loss: 0.033 | Tree loss: 2.768 | Accuracy: 0.527344 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 029 | Total loss: 2.740 | Reg loss: 0.033 | Tree loss: 2.740 | Accuracy: 0.531250 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 029 | Total loss: 2.722 | Reg loss: 0.033 | Tree loss: 2.722 | Accuracy: 0.500000 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 029 | Total loss: 2.698 | Reg loss: 0.033 | Tree loss: 2.698 | Accuracy: 0.498047 | 7.25 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 029 | Total loss: 2.706 | Reg loss: 0.033 | Tree loss: 2.706 | Accuracy: 0.496094 | 7.249 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 029 | Total loss: 2.691 | Reg loss: 0.033 | Tree loss: 2.691 | Accuracy: 0.476562 | 7.248 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 029 | Total loss: 2.659 | Reg loss: 0.033 | Tree loss: 2.659 | Accuracy: 0.505859 | 7.248 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 029 | Total loss: 2.655 | Reg loss: 0.033 | Tree loss: 2.655 | Accuracy: 0.494141 | 7.247 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 012 / 029 | Total loss: 2.630 | Reg loss: 0.033 | Tree loss: 2.630 | Accuracy: 0.492188 | 7.244 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 029 | Total loss: 2.649 | Reg loss: 0.033 | Tree loss: 2.649 | Accuracy: 0.484375 | 7.241 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 029 | Total loss: 2.619 | Reg loss: 0.033 | Tree loss: 2.619 | Accuracy: 0.494141 | 7.239 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 029 | Total loss: 2.576 | Reg loss: 0.033 | Tree loss: 2.576 | Accuracy: 0.521484 | 7.236 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 029 | Total loss: 2.602 | Reg loss: 0.033 | Tree loss: 2.602 | Accuracy: 0.470703 | 7.233 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 029 | Total loss: 2.591 | Reg loss: 0.033 | Tree loss: 2.591 | Accuracy: 0.474609 | 7.23 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 029 | Total loss: 2.559 | Reg loss: 0.033 | Tree loss: 2.559 | Accuracy: 0.501953 | 7.228 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 029 | Total loss: 2.527 | Reg loss: 0.033 | Tree loss: 2.527 | Accuracy: 0.521484 | 7.225 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 029 | Total loss: 2.550 | Reg loss: 0.033 | Tree loss: 2.550 | Accuracy: 0.472656 | 7.225 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 029 | Total loss: 2.553 | Reg loss: 0.033 | Tree loss: 2.553 | Accuracy: 0.449219 | 7.225 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 029 | Total loss: 2.535 | Reg loss: 0.034 | Tree loss: 2.535 | Accuracy: 0.490234 | 7.225 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 029 | Total loss: 2.523 | Reg loss: 0.034 | Tree loss: 2.523 | Accuracy: 0.468750 | 7.225 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 029 | Total loss: 2.509 | Reg loss: 0.034 | Tree loss: 2.509 | Accuracy: 0.474609 | 7.225 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 029 | Total loss: 2.472 | Reg loss: 0.034 | Tree loss: 2.472 | Accuracy: 0.486328 | 7.224 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 029 | Total loss: 2.473 | Reg loss: 0.034 | Tree loss: 2.473 | Accuracy: 0.501953 | 7.224 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 029 | Total loss: 2.457 | Reg loss: 0.034 | Tree loss: 2.457 | Accuracy: 0.542969 | 7.222 sec/iter\n",
      "Epoch: 31 | Batch: 028 / 029 | Total loss: 2.447 | Reg loss: 0.034 | Tree loss: 2.447 | Accuracy: 0.500986 | 7.219 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 029 | Total loss: 2.738 | Reg loss: 0.033 | Tree loss: 2.738 | Accuracy: 0.480469 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 029 | Total loss: 2.661 | Reg loss: 0.033 | Tree loss: 2.661 | Accuracy: 0.509766 | 7.229 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 029 | Total loss: 2.696 | Reg loss: 0.033 | Tree loss: 2.696 | Accuracy: 0.466797 | 7.229 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 029 | Total loss: 2.649 | Reg loss: 0.033 | Tree loss: 2.649 | Accuracy: 0.492188 | 7.229 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 029 | Total loss: 2.647 | Reg loss: 0.033 | Tree loss: 2.647 | Accuracy: 0.496094 | 7.229 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 029 | Total loss: 2.636 | Reg loss: 0.033 | Tree loss: 2.636 | Accuracy: 0.484375 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 029 | Total loss: 2.619 | Reg loss: 0.033 | Tree loss: 2.619 | Accuracy: 0.496094 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 029 | Total loss: 2.547 | Reg loss: 0.033 | Tree loss: 2.547 | Accuracy: 0.531250 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 029 | Total loss: 2.555 | Reg loss: 0.033 | Tree loss: 2.555 | Accuracy: 0.496094 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 029 | Total loss: 2.576 | Reg loss: 0.033 | Tree loss: 2.576 | Accuracy: 0.486328 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 029 | Total loss: 2.546 | Reg loss: 0.033 | Tree loss: 2.546 | Accuracy: 0.507812 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 029 | Total loss: 2.542 | Reg loss: 0.033 | Tree loss: 2.542 | Accuracy: 0.492188 | 7.228 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 029 | Total loss: 2.524 | Reg loss: 0.033 | Tree loss: 2.524 | Accuracy: 0.507812 | 7.227 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 029 | Total loss: 2.509 | Reg loss: 0.033 | Tree loss: 2.509 | Accuracy: 0.492188 | 7.227 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 029 | Total loss: 2.478 | Reg loss: 0.033 | Tree loss: 2.478 | Accuracy: 0.515625 | 7.226 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 029 | Total loss: 2.481 | Reg loss: 0.033 | Tree loss: 2.481 | Accuracy: 0.482422 | 7.226 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 029 | Total loss: 2.484 | Reg loss: 0.033 | Tree loss: 2.484 | Accuracy: 0.474609 | 7.226 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 029 | Total loss: 2.447 | Reg loss: 0.033 | Tree loss: 2.447 | Accuracy: 0.511719 | 7.225 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 029 | Total loss: 2.429 | Reg loss: 0.033 | Tree loss: 2.429 | Accuracy: 0.496094 | 7.224 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 029 | Total loss: 2.424 | Reg loss: 0.033 | Tree loss: 2.424 | Accuracy: 0.507812 | 7.222 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 029 | Total loss: 2.433 | Reg loss: 0.033 | Tree loss: 2.433 | Accuracy: 0.501953 | 7.219 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 029 | Total loss: 2.451 | Reg loss: 0.033 | Tree loss: 2.451 | Accuracy: 0.453125 | 7.216 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 029 | Total loss: 2.413 | Reg loss: 0.033 | Tree loss: 2.413 | Accuracy: 0.476562 | 7.214 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 029 | Total loss: 2.395 | Reg loss: 0.034 | Tree loss: 2.395 | Accuracy: 0.484375 | 7.211 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 029 | Total loss: 2.390 | Reg loss: 0.034 | Tree loss: 2.390 | Accuracy: 0.478516 | 7.209 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 029 | Total loss: 2.376 | Reg loss: 0.034 | Tree loss: 2.376 | Accuracy: 0.482422 | 7.206 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 029 | Total loss: 2.344 | Reg loss: 0.034 | Tree loss: 2.344 | Accuracy: 0.511719 | 7.205 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 029 | Total loss: 2.340 | Reg loss: 0.034 | Tree loss: 2.340 | Accuracy: 0.513672 | 7.203 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 029 | Total loss: 2.333 | Reg loss: 0.034 | Tree loss: 2.333 | Accuracy: 0.502959 | 7.201 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 029 | Total loss: 2.575 | Reg loss: 0.033 | Tree loss: 2.575 | Accuracy: 0.507812 | 7.217 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 029 | Total loss: 2.551 | Reg loss: 0.033 | Tree loss: 2.551 | Accuracy: 0.513672 | 7.217 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 029 | Total loss: 2.581 | Reg loss: 0.033 | Tree loss: 2.581 | Accuracy: 0.498047 | 7.217 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 029 | Total loss: 2.525 | Reg loss: 0.033 | Tree loss: 2.525 | Accuracy: 0.521484 | 7.217 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 029 | Total loss: 2.532 | Reg loss: 0.033 | Tree loss: 2.532 | Accuracy: 0.494141 | 7.217 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 029 | Total loss: 2.500 | Reg loss: 0.033 | Tree loss: 2.500 | Accuracy: 0.503906 | 7.217 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 029 | Total loss: 2.472 | Reg loss: 0.033 | Tree loss: 2.472 | Accuracy: 0.503906 | 7.216 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 029 | Total loss: 2.437 | Reg loss: 0.033 | Tree loss: 2.437 | Accuracy: 0.552734 | 7.216 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 029 | Total loss: 2.475 | Reg loss: 0.033 | Tree loss: 2.475 | Accuracy: 0.486328 | 7.216 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 029 | Total loss: 2.442 | Reg loss: 0.033 | Tree loss: 2.442 | Accuracy: 0.503906 | 7.216 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 029 | Total loss: 2.427 | Reg loss: 0.033 | Tree loss: 2.427 | Accuracy: 0.501953 | 7.216 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 029 | Total loss: 2.442 | Reg loss: 0.033 | Tree loss: 2.442 | Accuracy: 0.480469 | 7.215 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 029 | Total loss: 2.414 | Reg loss: 0.033 | Tree loss: 2.414 | Accuracy: 0.496094 | 7.215 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 029 | Total loss: 2.441 | Reg loss: 0.033 | Tree loss: 2.441 | Accuracy: 0.435547 | 7.215 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 029 | Total loss: 2.374 | Reg loss: 0.033 | Tree loss: 2.374 | Accuracy: 0.513672 | 7.215 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 015 / 029 | Total loss: 2.383 | Reg loss: 0.033 | Tree loss: 2.383 | Accuracy: 0.496094 | 7.214 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 029 | Total loss: 2.373 | Reg loss: 0.033 | Tree loss: 2.373 | Accuracy: 0.496094 | 7.214 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 029 | Total loss: 2.327 | Reg loss: 0.033 | Tree loss: 2.327 | Accuracy: 0.509766 | 7.214 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 029 | Total loss: 2.359 | Reg loss: 0.033 | Tree loss: 2.359 | Accuracy: 0.464844 | 7.214 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 029 | Total loss: 2.319 | Reg loss: 0.033 | Tree loss: 2.319 | Accuracy: 0.496094 | 7.213 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 029 | Total loss: 2.351 | Reg loss: 0.033 | Tree loss: 2.351 | Accuracy: 0.447266 | 7.213 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 029 | Total loss: 2.312 | Reg loss: 0.033 | Tree loss: 2.312 | Accuracy: 0.484375 | 7.213 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 029 | Total loss: 2.287 | Reg loss: 0.033 | Tree loss: 2.287 | Accuracy: 0.505859 | 7.213 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 029 | Total loss: 2.281 | Reg loss: 0.033 | Tree loss: 2.281 | Accuracy: 0.505859 | 7.212 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 029 | Total loss: 2.284 | Reg loss: 0.033 | Tree loss: 2.284 | Accuracy: 0.492188 | 7.208 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 029 | Total loss: 2.269 | Reg loss: 0.034 | Tree loss: 2.269 | Accuracy: 0.488281 | 7.207 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 029 | Total loss: 2.270 | Reg loss: 0.034 | Tree loss: 2.270 | Accuracy: 0.494141 | 7.207 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 029 | Total loss: 2.254 | Reg loss: 0.034 | Tree loss: 2.254 | Accuracy: 0.480469 | 7.207 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 029 | Total loss: 2.263 | Reg loss: 0.034 | Tree loss: 2.263 | Accuracy: 0.457594 | 7.207 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 029 | Total loss: 2.485 | Reg loss: 0.033 | Tree loss: 2.485 | Accuracy: 0.472656 | 7.257 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 029 | Total loss: 2.459 | Reg loss: 0.033 | Tree loss: 2.459 | Accuracy: 0.488281 | 7.257 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 029 | Total loss: 2.411 | Reg loss: 0.033 | Tree loss: 2.411 | Accuracy: 0.509766 | 7.257 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 029 | Total loss: 2.405 | Reg loss: 0.033 | Tree loss: 2.405 | Accuracy: 0.515625 | 7.257 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 029 | Total loss: 2.418 | Reg loss: 0.033 | Tree loss: 2.418 | Accuracy: 0.486328 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 029 | Total loss: 2.372 | Reg loss: 0.033 | Tree loss: 2.372 | Accuracy: 0.544922 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 029 | Total loss: 2.411 | Reg loss: 0.033 | Tree loss: 2.411 | Accuracy: 0.476562 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 029 | Total loss: 2.395 | Reg loss: 0.033 | Tree loss: 2.395 | Accuracy: 0.507812 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 029 | Total loss: 2.391 | Reg loss: 0.033 | Tree loss: 2.391 | Accuracy: 0.484375 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 029 | Total loss: 2.342 | Reg loss: 0.033 | Tree loss: 2.342 | Accuracy: 0.496094 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 029 | Total loss: 2.361 | Reg loss: 0.033 | Tree loss: 2.361 | Accuracy: 0.466797 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 029 | Total loss: 2.307 | Reg loss: 0.033 | Tree loss: 2.307 | Accuracy: 0.521484 | 7.256 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 029 | Total loss: 2.301 | Reg loss: 0.033 | Tree loss: 2.301 | Accuracy: 0.537109 | 7.255 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 029 | Total loss: 2.317 | Reg loss: 0.033 | Tree loss: 2.317 | Accuracy: 0.482422 | 7.254 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 029 | Total loss: 2.326 | Reg loss: 0.033 | Tree loss: 2.326 | Accuracy: 0.451172 | 7.252 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 029 | Total loss: 2.276 | Reg loss: 0.033 | Tree loss: 2.276 | Accuracy: 0.503906 | 7.25 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 029 | Total loss: 2.261 | Reg loss: 0.033 | Tree loss: 2.261 | Accuracy: 0.492188 | 7.25 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 029 | Total loss: 2.246 | Reg loss: 0.033 | Tree loss: 2.246 | Accuracy: 0.525391 | 7.25 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 029 | Total loss: 2.227 | Reg loss: 0.033 | Tree loss: 2.227 | Accuracy: 0.527344 | 7.249 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 029 | Total loss: 2.232 | Reg loss: 0.033 | Tree loss: 2.232 | Accuracy: 0.476562 | 7.249 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 029 | Total loss: 2.235 | Reg loss: 0.033 | Tree loss: 2.235 | Accuracy: 0.468750 | 7.248 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 029 | Total loss: 2.207 | Reg loss: 0.033 | Tree loss: 2.207 | Accuracy: 0.480469 | 7.247 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 029 | Total loss: 2.195 | Reg loss: 0.033 | Tree loss: 2.195 | Accuracy: 0.519531 | 7.246 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 029 | Total loss: 2.203 | Reg loss: 0.033 | Tree loss: 2.203 | Accuracy: 0.482422 | 7.246 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 029 | Total loss: 2.189 | Reg loss: 0.033 | Tree loss: 2.189 | Accuracy: 0.501953 | 7.245 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 029 | Total loss: 2.170 | Reg loss: 0.033 | Tree loss: 2.170 | Accuracy: 0.494141 | 7.245 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 029 | Total loss: 2.175 | Reg loss: 0.033 | Tree loss: 2.175 | Accuracy: 0.490234 | 7.245 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 029 | Total loss: 2.195 | Reg loss: 0.033 | Tree loss: 2.195 | Accuracy: 0.449219 | 7.245 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 029 | Total loss: 2.163 | Reg loss: 0.034 | Tree loss: 2.163 | Accuracy: 0.479290 | 7.245 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 029 | Total loss: 2.383 | Reg loss: 0.032 | Tree loss: 2.383 | Accuracy: 0.486328 | 7.289 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 029 | Total loss: 2.365 | Reg loss: 0.032 | Tree loss: 2.365 | Accuracy: 0.507812 | 7.289 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 029 | Total loss: 2.378 | Reg loss: 0.032 | Tree loss: 2.378 | Accuracy: 0.464844 | 7.289 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 029 | Total loss: 2.300 | Reg loss: 0.032 | Tree loss: 2.300 | Accuracy: 0.552734 | 7.289 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 029 | Total loss: 2.344 | Reg loss: 0.032 | Tree loss: 2.344 | Accuracy: 0.460938 | 7.288 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 029 | Total loss: 2.321 | Reg loss: 0.033 | Tree loss: 2.321 | Accuracy: 0.505859 | 7.286 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 029 | Total loss: 2.325 | Reg loss: 0.033 | Tree loss: 2.325 | Accuracy: 0.472656 | 7.286 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 029 | Total loss: 2.269 | Reg loss: 0.033 | Tree loss: 2.269 | Accuracy: 0.513672 | 7.285 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 029 | Total loss: 2.254 | Reg loss: 0.033 | Tree loss: 2.254 | Accuracy: 0.527344 | 7.283 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 029 | Total loss: 2.258 | Reg loss: 0.033 | Tree loss: 2.258 | Accuracy: 0.503906 | 7.281 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 029 | Total loss: 2.232 | Reg loss: 0.033 | Tree loss: 2.232 | Accuracy: 0.525391 | 7.28 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 029 | Total loss: 2.245 | Reg loss: 0.033 | Tree loss: 2.245 | Accuracy: 0.474609 | 7.28 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 029 | Total loss: 2.237 | Reg loss: 0.033 | Tree loss: 2.237 | Accuracy: 0.511719 | 7.279 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 029 | Total loss: 2.194 | Reg loss: 0.033 | Tree loss: 2.194 | Accuracy: 0.501953 | 7.279 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 029 | Total loss: 2.212 | Reg loss: 0.033 | Tree loss: 2.212 | Accuracy: 0.490234 | 7.278 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 029 | Total loss: 2.215 | Reg loss: 0.033 | Tree loss: 2.215 | Accuracy: 0.476562 | 7.278 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 029 | Total loss: 2.169 | Reg loss: 0.033 | Tree loss: 2.169 | Accuracy: 0.488281 | 7.277 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 029 | Total loss: 2.172 | Reg loss: 0.033 | Tree loss: 2.172 | Accuracy: 0.488281 | 7.277 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 018 / 029 | Total loss: 2.140 | Reg loss: 0.033 | Tree loss: 2.140 | Accuracy: 0.529297 | 7.277 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 029 | Total loss: 2.138 | Reg loss: 0.033 | Tree loss: 2.138 | Accuracy: 0.511719 | 7.276 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 029 | Total loss: 2.119 | Reg loss: 0.033 | Tree loss: 2.119 | Accuracy: 0.496094 | 7.276 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 029 | Total loss: 2.119 | Reg loss: 0.033 | Tree loss: 2.119 | Accuracy: 0.505859 | 7.276 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 029 | Total loss: 2.136 | Reg loss: 0.033 | Tree loss: 2.136 | Accuracy: 0.460938 | 7.275 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 029 | Total loss: 2.111 | Reg loss: 0.033 | Tree loss: 2.111 | Accuracy: 0.488281 | 7.275 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 029 | Total loss: 2.116 | Reg loss: 0.033 | Tree loss: 2.116 | Accuracy: 0.470703 | 7.275 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 029 | Total loss: 2.078 | Reg loss: 0.033 | Tree loss: 2.078 | Accuracy: 0.529297 | 7.275 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 029 | Total loss: 2.098 | Reg loss: 0.033 | Tree loss: 2.098 | Accuracy: 0.458984 | 7.275 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 029 | Total loss: 2.113 | Reg loss: 0.033 | Tree loss: 2.113 | Accuracy: 0.453125 | 7.274 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 029 | Total loss: 2.073 | Reg loss: 0.033 | Tree loss: 2.073 | Accuracy: 0.475345 | 7.274 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 029 | Total loss: 2.323 | Reg loss: 0.032 | Tree loss: 2.323 | Accuracy: 0.474609 | 7.296 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 029 | Total loss: 2.315 | Reg loss: 0.032 | Tree loss: 2.315 | Accuracy: 0.457031 | 7.294 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 029 | Total loss: 2.284 | Reg loss: 0.032 | Tree loss: 2.284 | Accuracy: 0.484375 | 7.292 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 029 | Total loss: 2.235 | Reg loss: 0.032 | Tree loss: 2.235 | Accuracy: 0.500000 | 7.292 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 029 | Total loss: 2.229 | Reg loss: 0.032 | Tree loss: 2.229 | Accuracy: 0.505859 | 7.291 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 029 | Total loss: 2.216 | Reg loss: 0.032 | Tree loss: 2.216 | Accuracy: 0.511719 | 7.29 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 029 | Total loss: 2.192 | Reg loss: 0.032 | Tree loss: 2.192 | Accuracy: 0.490234 | 7.288 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 029 | Total loss: 2.220 | Reg loss: 0.032 | Tree loss: 2.220 | Accuracy: 0.472656 | 7.286 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 029 | Total loss: 2.155 | Reg loss: 0.032 | Tree loss: 2.155 | Accuracy: 0.541016 | 7.283 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 029 | Total loss: 2.187 | Reg loss: 0.033 | Tree loss: 2.187 | Accuracy: 0.478516 | 7.281 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 029 | Total loss: 2.184 | Reg loss: 0.033 | Tree loss: 2.184 | Accuracy: 0.470703 | 7.278 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 029 | Total loss: 2.149 | Reg loss: 0.033 | Tree loss: 2.149 | Accuracy: 0.501953 | 7.278 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 029 | Total loss: 2.133 | Reg loss: 0.033 | Tree loss: 2.133 | Accuracy: 0.517578 | 7.277 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 029 | Total loss: 2.127 | Reg loss: 0.033 | Tree loss: 2.127 | Accuracy: 0.494141 | 7.277 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 029 | Total loss: 2.118 | Reg loss: 0.033 | Tree loss: 2.118 | Accuracy: 0.501953 | 7.276 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 029 | Total loss: 2.130 | Reg loss: 0.033 | Tree loss: 2.130 | Accuracy: 0.480469 | 7.276 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 029 | Total loss: 2.092 | Reg loss: 0.033 | Tree loss: 2.092 | Accuracy: 0.523438 | 7.276 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 029 | Total loss: 2.082 | Reg loss: 0.033 | Tree loss: 2.082 | Accuracy: 0.507812 | 7.275 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 029 | Total loss: 2.071 | Reg loss: 0.033 | Tree loss: 2.071 | Accuracy: 0.492188 | 7.275 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 029 | Total loss: 2.058 | Reg loss: 0.033 | Tree loss: 2.058 | Accuracy: 0.492188 | 7.275 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 029 | Total loss: 2.081 | Reg loss: 0.033 | Tree loss: 2.081 | Accuracy: 0.451172 | 7.274 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 029 | Total loss: 2.070 | Reg loss: 0.033 | Tree loss: 2.070 | Accuracy: 0.474609 | 7.274 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 029 | Total loss: 2.045 | Reg loss: 0.033 | Tree loss: 2.045 | Accuracy: 0.505859 | 7.274 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 029 | Total loss: 2.021 | Reg loss: 0.033 | Tree loss: 2.021 | Accuracy: 0.507812 | 7.274 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 029 | Total loss: 2.025 | Reg loss: 0.033 | Tree loss: 2.025 | Accuracy: 0.505859 | 7.273 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 029 | Total loss: 1.975 | Reg loss: 0.033 | Tree loss: 1.975 | Accuracy: 0.554688 | 7.273 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 029 | Total loss: 2.028 | Reg loss: 0.033 | Tree loss: 2.028 | Accuracy: 0.478516 | 7.273 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 029 | Total loss: 2.044 | Reg loss: 0.033 | Tree loss: 2.044 | Accuracy: 0.462891 | 7.273 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 029 | Total loss: 2.000 | Reg loss: 0.033 | Tree loss: 2.000 | Accuracy: 0.493097 | 7.273 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 029 | Total loss: 2.221 | Reg loss: 0.032 | Tree loss: 2.221 | Accuracy: 0.470703 | 7.282 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 029 | Total loss: 2.198 | Reg loss: 0.032 | Tree loss: 2.198 | Accuracy: 0.492188 | 7.28 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 029 | Total loss: 2.211 | Reg loss: 0.032 | Tree loss: 2.211 | Accuracy: 0.476562 | 7.28 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 029 | Total loss: 2.215 | Reg loss: 0.032 | Tree loss: 2.215 | Accuracy: 0.445312 | 7.28 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 029 | Total loss: 2.149 | Reg loss: 0.032 | Tree loss: 2.149 | Accuracy: 0.505859 | 7.28 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 029 | Total loss: 2.142 | Reg loss: 0.032 | Tree loss: 2.142 | Accuracy: 0.523438 | 7.28 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 029 | Total loss: 2.138 | Reg loss: 0.032 | Tree loss: 2.138 | Accuracy: 0.484375 | 7.28 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 029 | Total loss: 2.101 | Reg loss: 0.032 | Tree loss: 2.101 | Accuracy: 0.544922 | 7.28 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 029 | Total loss: 2.132 | Reg loss: 0.032 | Tree loss: 2.132 | Accuracy: 0.503906 | 7.279 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 029 | Total loss: 2.112 | Reg loss: 0.032 | Tree loss: 2.112 | Accuracy: 0.476562 | 7.277 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 029 | Total loss: 2.085 | Reg loss: 0.032 | Tree loss: 2.085 | Accuracy: 0.498047 | 7.277 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 029 | Total loss: 2.107 | Reg loss: 0.032 | Tree loss: 2.107 | Accuracy: 0.472656 | 7.277 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 029 | Total loss: 2.090 | Reg loss: 0.032 | Tree loss: 2.090 | Accuracy: 0.474609 | 7.276 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 029 | Total loss: 2.051 | Reg loss: 0.033 | Tree loss: 2.051 | Accuracy: 0.501953 | 7.275 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 029 | Total loss: 2.063 | Reg loss: 0.033 | Tree loss: 2.063 | Accuracy: 0.472656 | 7.275 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 029 | Total loss: 2.036 | Reg loss: 0.033 | Tree loss: 2.036 | Accuracy: 0.513672 | 7.274 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 029 | Total loss: 2.011 | Reg loss: 0.033 | Tree loss: 2.011 | Accuracy: 0.500000 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 029 | Total loss: 2.005 | Reg loss: 0.033 | Tree loss: 2.005 | Accuracy: 0.525391 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 029 | Total loss: 2.019 | Reg loss: 0.033 | Tree loss: 2.019 | Accuracy: 0.478516 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 029 | Total loss: 1.978 | Reg loss: 0.033 | Tree loss: 1.978 | Accuracy: 0.519531 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 029 | Total loss: 2.007 | Reg loss: 0.033 | Tree loss: 2.007 | Accuracy: 0.472656 | 7.273 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 021 / 029 | Total loss: 1.983 | Reg loss: 0.033 | Tree loss: 1.983 | Accuracy: 0.500000 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 029 | Total loss: 1.973 | Reg loss: 0.033 | Tree loss: 1.973 | Accuracy: 0.488281 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 029 | Total loss: 1.905 | Reg loss: 0.033 | Tree loss: 1.905 | Accuracy: 0.544922 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 029 | Total loss: 1.933 | Reg loss: 0.033 | Tree loss: 1.933 | Accuracy: 0.521484 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 029 | Total loss: 1.949 | Reg loss: 0.033 | Tree loss: 1.949 | Accuracy: 0.484375 | 7.273 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 029 | Total loss: 1.971 | Reg loss: 0.033 | Tree loss: 1.971 | Accuracy: 0.460938 | 7.272 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 029 | Total loss: 1.955 | Reg loss: 0.033 | Tree loss: 1.955 | Accuracy: 0.474609 | 7.272 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 029 | Total loss: 1.931 | Reg loss: 0.033 | Tree loss: 1.931 | Accuracy: 0.504931 | 7.272 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 029 | Total loss: 2.166 | Reg loss: 0.032 | Tree loss: 2.166 | Accuracy: 0.478516 | 7.278 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 029 | Total loss: 2.141 | Reg loss: 0.032 | Tree loss: 2.141 | Accuracy: 0.464844 | 7.278 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 029 | Total loss: 2.102 | Reg loss: 0.032 | Tree loss: 2.102 | Accuracy: 0.523438 | 7.277 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 029 | Total loss: 2.079 | Reg loss: 0.032 | Tree loss: 2.079 | Accuracy: 0.525391 | 7.276 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 029 | Total loss: 2.103 | Reg loss: 0.032 | Tree loss: 2.103 | Accuracy: 0.478516 | 7.276 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 029 | Total loss: 2.066 | Reg loss: 0.032 | Tree loss: 2.066 | Accuracy: 0.525391 | 7.275 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 029 | Total loss: 2.090 | Reg loss: 0.032 | Tree loss: 2.090 | Accuracy: 0.501953 | 7.275 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 029 | Total loss: 2.050 | Reg loss: 0.032 | Tree loss: 2.050 | Accuracy: 0.498047 | 7.274 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 029 | Total loss: 2.036 | Reg loss: 0.032 | Tree loss: 2.036 | Accuracy: 0.500000 | 7.274 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 029 | Total loss: 2.014 | Reg loss: 0.032 | Tree loss: 2.014 | Accuracy: 0.509766 | 7.272 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 029 | Total loss: 2.028 | Reg loss: 0.032 | Tree loss: 2.028 | Accuracy: 0.480469 | 7.27 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 029 | Total loss: 2.006 | Reg loss: 0.032 | Tree loss: 2.006 | Accuracy: 0.507812 | 7.268 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 029 | Total loss: 2.034 | Reg loss: 0.032 | Tree loss: 2.034 | Accuracy: 0.462891 | 7.266 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 029 | Total loss: 1.991 | Reg loss: 0.032 | Tree loss: 1.991 | Accuracy: 0.500000 | 7.264 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 029 | Total loss: 1.985 | Reg loss: 0.032 | Tree loss: 1.985 | Accuracy: 0.498047 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 029 | Total loss: 1.978 | Reg loss: 0.032 | Tree loss: 1.978 | Accuracy: 0.470703 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 029 | Total loss: 1.980 | Reg loss: 0.033 | Tree loss: 1.980 | Accuracy: 0.476562 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 029 | Total loss: 1.936 | Reg loss: 0.033 | Tree loss: 1.936 | Accuracy: 0.509766 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 029 | Total loss: 1.953 | Reg loss: 0.033 | Tree loss: 1.953 | Accuracy: 0.484375 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 029 | Total loss: 1.907 | Reg loss: 0.033 | Tree loss: 1.907 | Accuracy: 0.521484 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 029 | Total loss: 1.943 | Reg loss: 0.033 | Tree loss: 1.943 | Accuracy: 0.494141 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 029 | Total loss: 1.941 | Reg loss: 0.033 | Tree loss: 1.941 | Accuracy: 0.476562 | 7.263 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 029 | Total loss: 1.921 | Reg loss: 0.033 | Tree loss: 1.921 | Accuracy: 0.488281 | 7.262 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 029 | Total loss: 1.892 | Reg loss: 0.033 | Tree loss: 1.892 | Accuracy: 0.500000 | 7.262 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 029 | Total loss: 1.897 | Reg loss: 0.033 | Tree loss: 1.897 | Accuracy: 0.498047 | 7.262 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 029 | Total loss: 1.900 | Reg loss: 0.033 | Tree loss: 1.900 | Accuracy: 0.488281 | 7.262 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 029 | Total loss: 1.887 | Reg loss: 0.033 | Tree loss: 1.887 | Accuracy: 0.480469 | 7.262 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 029 | Total loss: 1.865 | Reg loss: 0.033 | Tree loss: 1.865 | Accuracy: 0.496094 | 7.261 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 029 | Total loss: 1.865 | Reg loss: 0.033 | Tree loss: 1.865 | Accuracy: 0.493097 | 7.261 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 029 | Total loss: 2.092 | Reg loss: 0.032 | Tree loss: 2.092 | Accuracy: 0.457031 | 7.266 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 029 | Total loss: 2.047 | Reg loss: 0.032 | Tree loss: 2.047 | Accuracy: 0.500000 | 7.265 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 029 | Total loss: 2.085 | Reg loss: 0.032 | Tree loss: 2.085 | Accuracy: 0.458984 | 7.265 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 029 | Total loss: 2.062 | Reg loss: 0.032 | Tree loss: 2.062 | Accuracy: 0.478516 | 7.264 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 029 | Total loss: 2.005 | Reg loss: 0.032 | Tree loss: 2.005 | Accuracy: 0.523438 | 7.264 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 029 | Total loss: 2.018 | Reg loss: 0.032 | Tree loss: 2.018 | Accuracy: 0.498047 | 7.263 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 029 | Total loss: 2.022 | Reg loss: 0.032 | Tree loss: 2.022 | Accuracy: 0.470703 | 7.263 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 029 | Total loss: 1.987 | Reg loss: 0.032 | Tree loss: 1.987 | Accuracy: 0.486328 | 7.262 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 029 | Total loss: 1.995 | Reg loss: 0.032 | Tree loss: 1.995 | Accuracy: 0.449219 | 7.262 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 029 | Total loss: 1.958 | Reg loss: 0.032 | Tree loss: 1.958 | Accuracy: 0.507812 | 7.261 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 029 | Total loss: 1.980 | Reg loss: 0.032 | Tree loss: 1.980 | Accuracy: 0.498047 | 7.261 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 029 | Total loss: 1.944 | Reg loss: 0.032 | Tree loss: 1.944 | Accuracy: 0.509766 | 7.261 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 029 | Total loss: 1.953 | Reg loss: 0.032 | Tree loss: 1.953 | Accuracy: 0.484375 | 7.261 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 029 | Total loss: 1.919 | Reg loss: 0.032 | Tree loss: 1.919 | Accuracy: 0.496094 | 7.259 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 029 | Total loss: 1.906 | Reg loss: 0.032 | Tree loss: 1.906 | Accuracy: 0.503906 | 7.257 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 029 | Total loss: 1.928 | Reg loss: 0.032 | Tree loss: 1.928 | Accuracy: 0.501953 | 7.255 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 029 | Total loss: 1.908 | Reg loss: 0.032 | Tree loss: 1.908 | Accuracy: 0.507812 | 7.253 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 029 | Total loss: 1.891 | Reg loss: 0.032 | Tree loss: 1.891 | Accuracy: 0.501953 | 7.25 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 029 | Total loss: 1.890 | Reg loss: 0.032 | Tree loss: 1.890 | Accuracy: 0.486328 | 7.248 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 029 | Total loss: 1.830 | Reg loss: 0.032 | Tree loss: 1.830 | Accuracy: 0.556641 | 7.246 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 029 | Total loss: 1.871 | Reg loss: 0.033 | Tree loss: 1.871 | Accuracy: 0.492188 | 7.244 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 029 | Total loss: 1.860 | Reg loss: 0.033 | Tree loss: 1.860 | Accuracy: 0.507812 | 7.242 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 029 | Total loss: 1.859 | Reg loss: 0.033 | Tree loss: 1.859 | Accuracy: 0.500000 | 7.242 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 029 | Total loss: 1.861 | Reg loss: 0.033 | Tree loss: 1.861 | Accuracy: 0.468750 | 7.242 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 024 / 029 | Total loss: 1.821 | Reg loss: 0.033 | Tree loss: 1.821 | Accuracy: 0.517578 | 7.242 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 029 | Total loss: 1.846 | Reg loss: 0.033 | Tree loss: 1.846 | Accuracy: 0.472656 | 7.242 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 029 | Total loss: 1.835 | Reg loss: 0.033 | Tree loss: 1.835 | Accuracy: 0.482422 | 7.242 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 029 | Total loss: 1.822 | Reg loss: 0.033 | Tree loss: 1.822 | Accuracy: 0.496094 | 7.241 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 029 | Total loss: 1.814 | Reg loss: 0.033 | Tree loss: 1.814 | Accuracy: 0.518738 | 7.24 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 029 | Total loss: 2.019 | Reg loss: 0.032 | Tree loss: 2.019 | Accuracy: 0.474609 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 029 | Total loss: 1.997 | Reg loss: 0.032 | Tree loss: 1.997 | Accuracy: 0.503906 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 029 | Total loss: 1.983 | Reg loss: 0.032 | Tree loss: 1.983 | Accuracy: 0.507812 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 029 | Total loss: 1.990 | Reg loss: 0.032 | Tree loss: 1.990 | Accuracy: 0.501953 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 029 | Total loss: 1.970 | Reg loss: 0.032 | Tree loss: 1.970 | Accuracy: 0.488281 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 029 | Total loss: 1.969 | Reg loss: 0.032 | Tree loss: 1.969 | Accuracy: 0.488281 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 029 | Total loss: 1.959 | Reg loss: 0.032 | Tree loss: 1.959 | Accuracy: 0.490234 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 029 | Total loss: 1.913 | Reg loss: 0.032 | Tree loss: 1.913 | Accuracy: 0.521484 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 029 | Total loss: 1.918 | Reg loss: 0.032 | Tree loss: 1.918 | Accuracy: 0.517578 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 029 | Total loss: 1.908 | Reg loss: 0.032 | Tree loss: 1.908 | Accuracy: 0.500000 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 029 | Total loss: 1.906 | Reg loss: 0.032 | Tree loss: 1.906 | Accuracy: 0.501953 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 029 | Total loss: 1.895 | Reg loss: 0.032 | Tree loss: 1.895 | Accuracy: 0.494141 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 029 | Total loss: 1.894 | Reg loss: 0.032 | Tree loss: 1.894 | Accuracy: 0.490234 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 029 | Total loss: 1.888 | Reg loss: 0.032 | Tree loss: 1.888 | Accuracy: 0.474609 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 029 | Total loss: 1.885 | Reg loss: 0.032 | Tree loss: 1.885 | Accuracy: 0.501953 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 029 | Total loss: 1.859 | Reg loss: 0.032 | Tree loss: 1.859 | Accuracy: 0.494141 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 029 | Total loss: 1.811 | Reg loss: 0.032 | Tree loss: 1.811 | Accuracy: 0.523438 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 029 | Total loss: 1.827 | Reg loss: 0.032 | Tree loss: 1.827 | Accuracy: 0.513672 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 029 | Total loss: 1.824 | Reg loss: 0.032 | Tree loss: 1.824 | Accuracy: 0.501953 | 7.247 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 029 | Total loss: 1.821 | Reg loss: 0.032 | Tree loss: 1.821 | Accuracy: 0.503906 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 029 | Total loss: 1.836 | Reg loss: 0.032 | Tree loss: 1.836 | Accuracy: 0.455078 | 7.246 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 029 | Total loss: 1.821 | Reg loss: 0.032 | Tree loss: 1.821 | Accuracy: 0.484375 | 7.244 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 029 | Total loss: 1.818 | Reg loss: 0.032 | Tree loss: 1.818 | Accuracy: 0.482422 | 7.242 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 029 | Total loss: 1.836 | Reg loss: 0.033 | Tree loss: 1.836 | Accuracy: 0.464844 | 7.24 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 029 | Total loss: 1.787 | Reg loss: 0.033 | Tree loss: 1.787 | Accuracy: 0.490234 | 7.239 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 029 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.488281 | 7.238 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 029 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.496094 | 7.237 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 029 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.500000 | 7.235 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 029 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.477318 | 7.235 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 029 | Total loss: 1.932 | Reg loss: 0.032 | Tree loss: 1.932 | Accuracy: 0.492188 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 029 | Total loss: 1.924 | Reg loss: 0.032 | Tree loss: 1.924 | Accuracy: 0.515625 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 029 | Total loss: 1.957 | Reg loss: 0.032 | Tree loss: 1.957 | Accuracy: 0.501953 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 029 | Total loss: 1.928 | Reg loss: 0.032 | Tree loss: 1.928 | Accuracy: 0.484375 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 029 | Total loss: 1.923 | Reg loss: 0.032 | Tree loss: 1.923 | Accuracy: 0.468750 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 029 | Total loss: 1.922 | Reg loss: 0.032 | Tree loss: 1.922 | Accuracy: 0.460938 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 029 | Total loss: 1.901 | Reg loss: 0.032 | Tree loss: 1.901 | Accuracy: 0.494141 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 029 | Total loss: 1.896 | Reg loss: 0.032 | Tree loss: 1.896 | Accuracy: 0.476562 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 029 | Total loss: 1.880 | Reg loss: 0.032 | Tree loss: 1.880 | Accuracy: 0.498047 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 029 | Total loss: 1.855 | Reg loss: 0.032 | Tree loss: 1.855 | Accuracy: 0.509766 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 029 | Total loss: 1.891 | Reg loss: 0.032 | Tree loss: 1.891 | Accuracy: 0.482422 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 029 | Total loss: 1.834 | Reg loss: 0.032 | Tree loss: 1.834 | Accuracy: 0.521484 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 029 | Total loss: 1.820 | Reg loss: 0.032 | Tree loss: 1.820 | Accuracy: 0.515625 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 029 | Total loss: 1.828 | Reg loss: 0.032 | Tree loss: 1.828 | Accuracy: 0.501953 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 029 | Total loss: 1.824 | Reg loss: 0.032 | Tree loss: 1.824 | Accuracy: 0.480469 | 7.241 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 029 | Total loss: 1.816 | Reg loss: 0.032 | Tree loss: 1.816 | Accuracy: 0.480469 | 7.24 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 029 | Total loss: 1.791 | Reg loss: 0.032 | Tree loss: 1.791 | Accuracy: 0.505859 | 7.24 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 029 | Total loss: 1.818 | Reg loss: 0.032 | Tree loss: 1.818 | Accuracy: 0.480469 | 7.24 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 029 | Total loss: 1.790 | Reg loss: 0.032 | Tree loss: 1.790 | Accuracy: 0.478516 | 7.24 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 029 | Total loss: 1.804 | Reg loss: 0.032 | Tree loss: 1.804 | Accuracy: 0.451172 | 7.239 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 029 | Total loss: 1.759 | Reg loss: 0.032 | Tree loss: 1.759 | Accuracy: 0.519531 | 7.239 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 029 | Total loss: 1.783 | Reg loss: 0.032 | Tree loss: 1.783 | Accuracy: 0.460938 | 7.239 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 029 | Total loss: 1.743 | Reg loss: 0.032 | Tree loss: 1.743 | Accuracy: 0.505859 | 7.239 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 029 | Total loss: 1.733 | Reg loss: 0.032 | Tree loss: 1.733 | Accuracy: 0.511719 | 7.236 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 029 | Total loss: 1.753 | Reg loss: 0.032 | Tree loss: 1.753 | Accuracy: 0.490234 | 7.234 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 029 | Total loss: 1.740 | Reg loss: 0.032 | Tree loss: 1.740 | Accuracy: 0.507812 | 7.232 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 029 | Total loss: 1.711 | Reg loss: 0.032 | Tree loss: 1.711 | Accuracy: 0.531250 | 7.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 027 / 029 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.509766 | 7.229 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 029 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.495069 | 7.229 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 029 | Total loss: 1.904 | Reg loss: 0.032 | Tree loss: 1.904 | Accuracy: 0.482422 | 7.235 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 029 | Total loss: 1.875 | Reg loss: 0.032 | Tree loss: 1.875 | Accuracy: 0.513672 | 7.235 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 029 | Total loss: 1.846 | Reg loss: 0.032 | Tree loss: 1.846 | Accuracy: 0.535156 | 7.235 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 029 | Total loss: 1.867 | Reg loss: 0.032 | Tree loss: 1.867 | Accuracy: 0.527344 | 7.235 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 029 | Total loss: 1.870 | Reg loss: 0.032 | Tree loss: 1.870 | Accuracy: 0.507812 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 029 | Total loss: 1.825 | Reg loss: 0.032 | Tree loss: 1.825 | Accuracy: 0.537109 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 029 | Total loss: 1.838 | Reg loss: 0.032 | Tree loss: 1.838 | Accuracy: 0.517578 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 029 | Total loss: 1.847 | Reg loss: 0.032 | Tree loss: 1.847 | Accuracy: 0.480469 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 029 | Total loss: 1.820 | Reg loss: 0.032 | Tree loss: 1.820 | Accuracy: 0.490234 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 029 | Total loss: 1.820 | Reg loss: 0.032 | Tree loss: 1.820 | Accuracy: 0.494141 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 029 | Total loss: 1.813 | Reg loss: 0.032 | Tree loss: 1.813 | Accuracy: 0.496094 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 029 | Total loss: 1.785 | Reg loss: 0.032 | Tree loss: 1.785 | Accuracy: 0.523438 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 029 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.460938 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 029 | Total loss: 1.793 | Reg loss: 0.032 | Tree loss: 1.793 | Accuracy: 0.484375 | 7.236 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 029 | Total loss: 1.796 | Reg loss: 0.032 | Tree loss: 1.796 | Accuracy: 0.468750 | 7.235 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 029 | Total loss: 1.801 | Reg loss: 0.032 | Tree loss: 1.801 | Accuracy: 0.453125 | 7.235 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 029 | Total loss: 1.749 | Reg loss: 0.032 | Tree loss: 1.749 | Accuracy: 0.511719 | 7.234 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 029 | Total loss: 1.759 | Reg loss: 0.032 | Tree loss: 1.759 | Accuracy: 0.480469 | 7.234 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 029 | Total loss: 1.723 | Reg loss: 0.032 | Tree loss: 1.723 | Accuracy: 0.521484 | 7.233 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 029 | Total loss: 1.767 | Reg loss: 0.032 | Tree loss: 1.767 | Accuracy: 0.455078 | 7.233 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 029 | Total loss: 1.702 | Reg loss: 0.032 | Tree loss: 1.702 | Accuracy: 0.507812 | 7.233 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 029 | Total loss: 1.755 | Reg loss: 0.032 | Tree loss: 1.755 | Accuracy: 0.472656 | 7.233 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 029 | Total loss: 1.729 | Reg loss: 0.032 | Tree loss: 1.729 | Accuracy: 0.484375 | 7.233 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 029 | Total loss: 1.692 | Reg loss: 0.032 | Tree loss: 1.692 | Accuracy: 0.509766 | 7.232 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 029 | Total loss: 1.687 | Reg loss: 0.032 | Tree loss: 1.687 | Accuracy: 0.511719 | 7.232 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 029 | Total loss: 1.706 | Reg loss: 0.032 | Tree loss: 1.706 | Accuracy: 0.472656 | 7.229 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 029 | Total loss: 1.713 | Reg loss: 0.032 | Tree loss: 1.713 | Accuracy: 0.484375 | 7.228 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 029 | Total loss: 1.698 | Reg loss: 0.032 | Tree loss: 1.698 | Accuracy: 0.476562 | 7.228 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 029 | Total loss: 1.695 | Reg loss: 0.032 | Tree loss: 1.695 | Accuracy: 0.471400 | 7.228 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 029 | Total loss: 1.852 | Reg loss: 0.032 | Tree loss: 1.852 | Accuracy: 0.501953 | 7.233 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 029 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.527344 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 029 | Total loss: 1.869 | Reg loss: 0.032 | Tree loss: 1.869 | Accuracy: 0.460938 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 029 | Total loss: 1.844 | Reg loss: 0.032 | Tree loss: 1.844 | Accuracy: 0.492188 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 029 | Total loss: 1.822 | Reg loss: 0.032 | Tree loss: 1.822 | Accuracy: 0.500000 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 029 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.480469 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 029 | Total loss: 1.799 | Reg loss: 0.032 | Tree loss: 1.799 | Accuracy: 0.488281 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 029 | Total loss: 1.795 | Reg loss: 0.032 | Tree loss: 1.795 | Accuracy: 0.507812 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 029 | Total loss: 1.792 | Reg loss: 0.032 | Tree loss: 1.792 | Accuracy: 0.480469 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 029 | Total loss: 1.799 | Reg loss: 0.032 | Tree loss: 1.799 | Accuracy: 0.476562 | 7.234 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 029 | Total loss: 1.778 | Reg loss: 0.032 | Tree loss: 1.778 | Accuracy: 0.474609 | 7.233 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 029 | Total loss: 1.742 | Reg loss: 0.032 | Tree loss: 1.742 | Accuracy: 0.507812 | 7.233 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 029 | Total loss: 1.753 | Reg loss: 0.032 | Tree loss: 1.753 | Accuracy: 0.478516 | 7.232 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 029 | Total loss: 1.765 | Reg loss: 0.032 | Tree loss: 1.765 | Accuracy: 0.470703 | 7.231 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 029 | Total loss: 1.758 | Reg loss: 0.032 | Tree loss: 1.758 | Accuracy: 0.472656 | 7.231 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 029 | Total loss: 1.736 | Reg loss: 0.032 | Tree loss: 1.736 | Accuracy: 0.480469 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 029 | Total loss: 1.672 | Reg loss: 0.032 | Tree loss: 1.672 | Accuracy: 0.568359 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 029 | Total loss: 1.719 | Reg loss: 0.032 | Tree loss: 1.719 | Accuracy: 0.484375 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 029 | Total loss: 1.708 | Reg loss: 0.032 | Tree loss: 1.708 | Accuracy: 0.492188 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 029 | Total loss: 1.690 | Reg loss: 0.032 | Tree loss: 1.690 | Accuracy: 0.500000 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 029 | Total loss: 1.691 | Reg loss: 0.032 | Tree loss: 1.691 | Accuracy: 0.494141 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 029 | Total loss: 1.670 | Reg loss: 0.032 | Tree loss: 1.670 | Accuracy: 0.519531 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 029 | Total loss: 1.643 | Reg loss: 0.032 | Tree loss: 1.643 | Accuracy: 0.544922 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 029 | Total loss: 1.638 | Reg loss: 0.032 | Tree loss: 1.638 | Accuracy: 0.527344 | 7.23 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 029 | Total loss: 1.678 | Reg loss: 0.032 | Tree loss: 1.678 | Accuracy: 0.482422 | 7.229 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 029 | Total loss: 1.699 | Reg loss: 0.032 | Tree loss: 1.699 | Accuracy: 0.457031 | 7.227 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 029 | Total loss: 1.634 | Reg loss: 0.032 | Tree loss: 1.634 | Accuracy: 0.498047 | 7.226 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 029 | Total loss: 1.670 | Reg loss: 0.032 | Tree loss: 1.670 | Accuracy: 0.470703 | 7.226 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 029 | Total loss: 1.639 | Reg loss: 0.032 | Tree loss: 1.639 | Accuracy: 0.493097 | 7.226 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 029 | Total loss: 1.809 | Reg loss: 0.032 | Tree loss: 1.809 | Accuracy: 0.496094 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 029 | Total loss: 1.811 | Reg loss: 0.032 | Tree loss: 1.811 | Accuracy: 0.494141 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 029 | Total loss: 1.804 | Reg loss: 0.032 | Tree loss: 1.804 | Accuracy: 0.484375 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 029 | Total loss: 1.793 | Reg loss: 0.032 | Tree loss: 1.793 | Accuracy: 0.496094 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 029 | Total loss: 1.799 | Reg loss: 0.032 | Tree loss: 1.799 | Accuracy: 0.503906 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 029 | Total loss: 1.768 | Reg loss: 0.032 | Tree loss: 1.768 | Accuracy: 0.513672 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 029 | Total loss: 1.762 | Reg loss: 0.032 | Tree loss: 1.762 | Accuracy: 0.515625 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 029 | Total loss: 1.721 | Reg loss: 0.032 | Tree loss: 1.721 | Accuracy: 0.546875 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 029 | Total loss: 1.749 | Reg loss: 0.032 | Tree loss: 1.749 | Accuracy: 0.482422 | 7.235 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 029 | Total loss: 1.754 | Reg loss: 0.032 | Tree loss: 1.754 | Accuracy: 0.453125 | 7.234 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 029 | Total loss: 1.714 | Reg loss: 0.032 | Tree loss: 1.714 | Accuracy: 0.523438 | 7.234 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 029 | Total loss: 1.703 | Reg loss: 0.032 | Tree loss: 1.703 | Accuracy: 0.527344 | 7.233 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 029 | Total loss: 1.716 | Reg loss: 0.032 | Tree loss: 1.716 | Accuracy: 0.490234 | 7.233 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 029 | Total loss: 1.742 | Reg loss: 0.032 | Tree loss: 1.742 | Accuracy: 0.457031 | 7.233 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 029 | Total loss: 1.695 | Reg loss: 0.032 | Tree loss: 1.695 | Accuracy: 0.500000 | 7.232 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 029 | Total loss: 1.665 | Reg loss: 0.032 | Tree loss: 1.665 | Accuracy: 0.500000 | 7.232 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 029 | Total loss: 1.684 | Reg loss: 0.032 | Tree loss: 1.684 | Accuracy: 0.476562 | 7.232 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 029 | Total loss: 1.685 | Reg loss: 0.032 | Tree loss: 1.685 | Accuracy: 0.480469 | 7.232 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 029 | Total loss: 1.658 | Reg loss: 0.032 | Tree loss: 1.658 | Accuracy: 0.500000 | 7.232 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 029 | Total loss: 1.619 | Reg loss: 0.032 | Tree loss: 1.619 | Accuracy: 0.539062 | 7.231 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 029 | Total loss: 1.673 | Reg loss: 0.032 | Tree loss: 1.673 | Accuracy: 0.468750 | 7.231 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 029 | Total loss: 1.676 | Reg loss: 0.032 | Tree loss: 1.676 | Accuracy: 0.458984 | 7.231 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 029 | Total loss: 1.648 | Reg loss: 0.032 | Tree loss: 1.648 | Accuracy: 0.496094 | 7.23 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 029 | Total loss: 1.642 | Reg loss: 0.032 | Tree loss: 1.642 | Accuracy: 0.492188 | 7.228 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 029 | Total loss: 1.650 | Reg loss: 0.032 | Tree loss: 1.650 | Accuracy: 0.486328 | 7.226 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 029 | Total loss: 1.623 | Reg loss: 0.032 | Tree loss: 1.623 | Accuracy: 0.490234 | 7.224 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 029 | Total loss: 1.620 | Reg loss: 0.032 | Tree loss: 1.620 | Accuracy: 0.498047 | 7.222 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 029 | Total loss: 1.629 | Reg loss: 0.032 | Tree loss: 1.629 | Accuracy: 0.486328 | 7.221 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 029 | Total loss: 1.622 | Reg loss: 0.032 | Tree loss: 1.622 | Accuracy: 0.475345 | 7.221 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 029 | Total loss: 1.750 | Reg loss: 0.032 | Tree loss: 1.750 | Accuracy: 0.523438 | 7.23 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 029 | Total loss: 1.799 | Reg loss: 0.032 | Tree loss: 1.799 | Accuracy: 0.470703 | 7.23 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 029 | Total loss: 1.789 | Reg loss: 0.032 | Tree loss: 1.789 | Accuracy: 0.478516 | 7.23 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 029 | Total loss: 1.749 | Reg loss: 0.032 | Tree loss: 1.749 | Accuracy: 0.498047 | 7.23 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 029 | Total loss: 1.745 | Reg loss: 0.032 | Tree loss: 1.745 | Accuracy: 0.498047 | 7.23 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 029 | Total loss: 1.737 | Reg loss: 0.032 | Tree loss: 1.737 | Accuracy: 0.503906 | 7.23 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 029 | Total loss: 1.714 | Reg loss: 0.032 | Tree loss: 1.714 | Accuracy: 0.531250 | 7.229 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 029 | Total loss: 1.715 | Reg loss: 0.032 | Tree loss: 1.715 | Accuracy: 0.500000 | 7.229 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 029 | Total loss: 1.720 | Reg loss: 0.032 | Tree loss: 1.720 | Accuracy: 0.482422 | 7.228 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 029 | Total loss: 1.710 | Reg loss: 0.032 | Tree loss: 1.710 | Accuracy: 0.494141 | 7.228 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 029 | Total loss: 1.679 | Reg loss: 0.032 | Tree loss: 1.679 | Accuracy: 0.511719 | 7.227 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 029 | Total loss: 1.698 | Reg loss: 0.032 | Tree loss: 1.698 | Accuracy: 0.492188 | 7.227 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 029 | Total loss: 1.675 | Reg loss: 0.032 | Tree loss: 1.675 | Accuracy: 0.494141 | 7.227 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 029 | Total loss: 1.676 | Reg loss: 0.032 | Tree loss: 1.676 | Accuracy: 0.472656 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 029 | Total loss: 1.703 | Reg loss: 0.032 | Tree loss: 1.703 | Accuracy: 0.441406 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 029 | Total loss: 1.661 | Reg loss: 0.032 | Tree loss: 1.661 | Accuracy: 0.482422 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 029 | Total loss: 1.638 | Reg loss: 0.032 | Tree loss: 1.638 | Accuracy: 0.509766 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 029 | Total loss: 1.625 | Reg loss: 0.032 | Tree loss: 1.625 | Accuracy: 0.507812 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 029 | Total loss: 1.615 | Reg loss: 0.032 | Tree loss: 1.615 | Accuracy: 0.503906 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 029 | Total loss: 1.613 | Reg loss: 0.032 | Tree loss: 1.613 | Accuracy: 0.515625 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 029 | Total loss: 1.639 | Reg loss: 0.032 | Tree loss: 1.639 | Accuracy: 0.478516 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 029 | Total loss: 1.608 | Reg loss: 0.032 | Tree loss: 1.608 | Accuracy: 0.492188 | 7.226 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 029 | Total loss: 1.596 | Reg loss: 0.032 | Tree loss: 1.596 | Accuracy: 0.521484 | 7.225 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 029 | Total loss: 1.619 | Reg loss: 0.032 | Tree loss: 1.619 | Accuracy: 0.476562 | 7.224 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 029 | Total loss: 1.611 | Reg loss: 0.032 | Tree loss: 1.611 | Accuracy: 0.466797 | 7.222 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 029 | Total loss: 1.607 | Reg loss: 0.032 | Tree loss: 1.607 | Accuracy: 0.482422 | 7.22 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 029 | Total loss: 1.589 | Reg loss: 0.032 | Tree loss: 1.589 | Accuracy: 0.507812 | 7.219 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 029 | Total loss: 1.600 | Reg loss: 0.032 | Tree loss: 1.600 | Accuracy: 0.478516 | 7.218 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 029 | Total loss: 1.570 | Reg loss: 0.032 | Tree loss: 1.570 | Accuracy: 0.516765 | 7.218 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 029 | Total loss: 1.784 | Reg loss: 0.031 | Tree loss: 1.784 | Accuracy: 0.453125 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 029 | Total loss: 1.751 | Reg loss: 0.031 | Tree loss: 1.751 | Accuracy: 0.498047 | 7.223 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 002 / 029 | Total loss: 1.733 | Reg loss: 0.031 | Tree loss: 1.733 | Accuracy: 0.498047 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 029 | Total loss: 1.718 | Reg loss: 0.031 | Tree loss: 1.718 | Accuracy: 0.507812 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 029 | Total loss: 1.700 | Reg loss: 0.031 | Tree loss: 1.700 | Accuracy: 0.496094 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 029 | Total loss: 1.721 | Reg loss: 0.031 | Tree loss: 1.721 | Accuracy: 0.476562 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 029 | Total loss: 1.709 | Reg loss: 0.032 | Tree loss: 1.709 | Accuracy: 0.498047 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 029 | Total loss: 1.693 | Reg loss: 0.032 | Tree loss: 1.693 | Accuracy: 0.478516 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 029 | Total loss: 1.690 | Reg loss: 0.032 | Tree loss: 1.690 | Accuracy: 0.478516 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 029 | Total loss: 1.648 | Reg loss: 0.032 | Tree loss: 1.648 | Accuracy: 0.533203 | 7.223 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 029 | Total loss: 1.660 | Reg loss: 0.032 | Tree loss: 1.660 | Accuracy: 0.486328 | 7.222 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 029 | Total loss: 1.666 | Reg loss: 0.032 | Tree loss: 1.666 | Accuracy: 0.474609 | 7.222 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 029 | Total loss: 1.638 | Reg loss: 0.032 | Tree loss: 1.638 | Accuracy: 0.505859 | 7.221 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 029 | Total loss: 1.649 | Reg loss: 0.032 | Tree loss: 1.649 | Accuracy: 0.490234 | 7.221 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 029 | Total loss: 1.628 | Reg loss: 0.032 | Tree loss: 1.628 | Accuracy: 0.500000 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 029 | Total loss: 1.616 | Reg loss: 0.032 | Tree loss: 1.616 | Accuracy: 0.501953 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 029 | Total loss: 1.610 | Reg loss: 0.032 | Tree loss: 1.610 | Accuracy: 0.509766 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 029 | Total loss: 1.578 | Reg loss: 0.032 | Tree loss: 1.578 | Accuracy: 0.525391 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 029 | Total loss: 1.626 | Reg loss: 0.032 | Tree loss: 1.626 | Accuracy: 0.466797 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 029 | Total loss: 1.577 | Reg loss: 0.032 | Tree loss: 1.577 | Accuracy: 0.531250 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 029 | Total loss: 1.583 | Reg loss: 0.032 | Tree loss: 1.583 | Accuracy: 0.492188 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 029 | Total loss: 1.603 | Reg loss: 0.032 | Tree loss: 1.603 | Accuracy: 0.480469 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 029 | Total loss: 1.584 | Reg loss: 0.032 | Tree loss: 1.584 | Accuracy: 0.482422 | 7.22 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 029 | Total loss: 1.561 | Reg loss: 0.032 | Tree loss: 1.561 | Accuracy: 0.503906 | 7.219 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 029 | Total loss: 1.562 | Reg loss: 0.032 | Tree loss: 1.562 | Accuracy: 0.505859 | 7.218 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 029 | Total loss: 1.570 | Reg loss: 0.032 | Tree loss: 1.570 | Accuracy: 0.490234 | 7.216 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 029 | Total loss: 1.543 | Reg loss: 0.032 | Tree loss: 1.543 | Accuracy: 0.515625 | 7.217 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 029 | Total loss: 1.564 | Reg loss: 0.032 | Tree loss: 1.564 | Accuracy: 0.488281 | 7.215 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 029 | Total loss: 1.585 | Reg loss: 0.032 | Tree loss: 1.585 | Accuracy: 0.463511 | 7.214 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 029 | Total loss: 1.722 | Reg loss: 0.031 | Tree loss: 1.722 | Accuracy: 0.505859 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 029 | Total loss: 1.694 | Reg loss: 0.031 | Tree loss: 1.694 | Accuracy: 0.513672 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 029 | Total loss: 1.718 | Reg loss: 0.031 | Tree loss: 1.718 | Accuracy: 0.480469 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 029 | Total loss: 1.697 | Reg loss: 0.031 | Tree loss: 1.697 | Accuracy: 0.503906 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 029 | Total loss: 1.664 | Reg loss: 0.031 | Tree loss: 1.664 | Accuracy: 0.523438 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 029 | Total loss: 1.656 | Reg loss: 0.031 | Tree loss: 1.656 | Accuracy: 0.523438 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 029 | Total loss: 1.681 | Reg loss: 0.031 | Tree loss: 1.681 | Accuracy: 0.482422 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 029 | Total loss: 1.660 | Reg loss: 0.031 | Tree loss: 1.660 | Accuracy: 0.494141 | 7.223 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 029 | Total loss: 1.671 | Reg loss: 0.031 | Tree loss: 1.671 | Accuracy: 0.478516 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 029 | Total loss: 1.629 | Reg loss: 0.031 | Tree loss: 1.629 | Accuracy: 0.488281 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 029 | Total loss: 1.653 | Reg loss: 0.031 | Tree loss: 1.653 | Accuracy: 0.462891 | 7.222 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 029 | Total loss: 1.603 | Reg loss: 0.031 | Tree loss: 1.603 | Accuracy: 0.527344 | 7.221 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 029 | Total loss: 1.626 | Reg loss: 0.032 | Tree loss: 1.626 | Accuracy: 0.478516 | 7.221 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 029 | Total loss: 1.616 | Reg loss: 0.032 | Tree loss: 1.616 | Accuracy: 0.472656 | 7.221 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 029 | Total loss: 1.603 | Reg loss: 0.032 | Tree loss: 1.603 | Accuracy: 0.490234 | 7.22 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 029 | Total loss: 1.603 | Reg loss: 0.032 | Tree loss: 1.603 | Accuracy: 0.484375 | 7.22 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 029 | Total loss: 1.613 | Reg loss: 0.032 | Tree loss: 1.613 | Accuracy: 0.468750 | 7.22 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 029 | Total loss: 1.569 | Reg loss: 0.032 | Tree loss: 1.569 | Accuracy: 0.515625 | 7.219 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 029 | Total loss: 1.537 | Reg loss: 0.032 | Tree loss: 1.537 | Accuracy: 0.535156 | 7.219 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 029 | Total loss: 1.560 | Reg loss: 0.032 | Tree loss: 1.560 | Accuracy: 0.501953 | 7.219 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 029 | Total loss: 1.570 | Reg loss: 0.032 | Tree loss: 1.570 | Accuracy: 0.488281 | 7.219 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 029 | Total loss: 1.544 | Reg loss: 0.032 | Tree loss: 1.544 | Accuracy: 0.525391 | 7.219 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 029 | Total loss: 1.556 | Reg loss: 0.032 | Tree loss: 1.556 | Accuracy: 0.488281 | 7.217 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 029 | Total loss: 1.543 | Reg loss: 0.032 | Tree loss: 1.543 | Accuracy: 0.507812 | 7.216 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 029 | Total loss: 1.555 | Reg loss: 0.032 | Tree loss: 1.555 | Accuracy: 0.484375 | 7.214 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 029 | Total loss: 1.563 | Reg loss: 0.032 | Tree loss: 1.563 | Accuracy: 0.464844 | 7.212 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 029 | Total loss: 1.555 | Reg loss: 0.032 | Tree loss: 1.555 | Accuracy: 0.449219 | 7.211 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 029 | Total loss: 1.533 | Reg loss: 0.032 | Tree loss: 1.533 | Accuracy: 0.500000 | 7.209 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 029 | Total loss: 1.534 | Reg loss: 0.032 | Tree loss: 1.534 | Accuracy: 0.493097 | 7.207 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 029 | Total loss: 1.659 | Reg loss: 0.031 | Tree loss: 1.659 | Accuracy: 0.521484 | 7.215 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 029 | Total loss: 1.699 | Reg loss: 0.031 | Tree loss: 1.699 | Accuracy: 0.494141 | 7.215 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 029 | Total loss: 1.683 | Reg loss: 0.031 | Tree loss: 1.683 | Accuracy: 0.470703 | 7.215 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 029 | Total loss: 1.678 | Reg loss: 0.031 | Tree loss: 1.678 | Accuracy: 0.488281 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 029 | Total loss: 1.656 | Reg loss: 0.031 | Tree loss: 1.656 | Accuracy: 0.501953 | 7.216 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 005 / 029 | Total loss: 1.626 | Reg loss: 0.031 | Tree loss: 1.626 | Accuracy: 0.519531 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 029 | Total loss: 1.654 | Reg loss: 0.031 | Tree loss: 1.654 | Accuracy: 0.486328 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 029 | Total loss: 1.614 | Reg loss: 0.031 | Tree loss: 1.614 | Accuracy: 0.521484 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 029 | Total loss: 1.649 | Reg loss: 0.031 | Tree loss: 1.649 | Accuracy: 0.470703 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 029 | Total loss: 1.608 | Reg loss: 0.031 | Tree loss: 1.608 | Accuracy: 0.503906 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 029 | Total loss: 1.607 | Reg loss: 0.031 | Tree loss: 1.607 | Accuracy: 0.505859 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 029 | Total loss: 1.587 | Reg loss: 0.031 | Tree loss: 1.587 | Accuracy: 0.511719 | 7.216 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 029 | Total loss: 1.580 | Reg loss: 0.031 | Tree loss: 1.580 | Accuracy: 0.500000 | 7.215 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 029 | Total loss: 1.575 | Reg loss: 0.031 | Tree loss: 1.575 | Accuracy: 0.511719 | 7.215 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 029 | Total loss: 1.582 | Reg loss: 0.031 | Tree loss: 1.582 | Accuracy: 0.484375 | 7.215 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 029 | Total loss: 1.540 | Reg loss: 0.031 | Tree loss: 1.540 | Accuracy: 0.527344 | 7.214 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 029 | Total loss: 1.561 | Reg loss: 0.032 | Tree loss: 1.561 | Accuracy: 0.490234 | 7.214 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 029 | Total loss: 1.542 | Reg loss: 0.032 | Tree loss: 1.542 | Accuracy: 0.492188 | 7.214 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 029 | Total loss: 1.574 | Reg loss: 0.032 | Tree loss: 1.574 | Accuracy: 0.453125 | 7.213 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 029 | Total loss: 1.573 | Reg loss: 0.032 | Tree loss: 1.573 | Accuracy: 0.455078 | 7.213 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 029 | Total loss: 1.529 | Reg loss: 0.032 | Tree loss: 1.529 | Accuracy: 0.521484 | 7.213 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 029 | Total loss: 1.538 | Reg loss: 0.032 | Tree loss: 1.538 | Accuracy: 0.498047 | 7.213 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 029 | Total loss: 1.537 | Reg loss: 0.032 | Tree loss: 1.537 | Accuracy: 0.480469 | 7.213 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 029 | Total loss: 1.540 | Reg loss: 0.032 | Tree loss: 1.540 | Accuracy: 0.462891 | 7.213 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 029 | Total loss: 1.496 | Reg loss: 0.032 | Tree loss: 1.496 | Accuracy: 0.535156 | 7.211 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 029 | Total loss: 1.530 | Reg loss: 0.032 | Tree loss: 1.530 | Accuracy: 0.482422 | 7.209 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 029 | Total loss: 1.512 | Reg loss: 0.032 | Tree loss: 1.512 | Accuracy: 0.492188 | 7.208 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 029 | Total loss: 1.527 | Reg loss: 0.032 | Tree loss: 1.527 | Accuracy: 0.472656 | 7.206 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 029 | Total loss: 1.518 | Reg loss: 0.032 | Tree loss: 1.518 | Accuracy: 0.477318 | 7.204 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 029 | Total loss: 1.676 | Reg loss: 0.031 | Tree loss: 1.676 | Accuracy: 0.464844 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 029 | Total loss: 1.643 | Reg loss: 0.031 | Tree loss: 1.643 | Accuracy: 0.511719 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 029 | Total loss: 1.629 | Reg loss: 0.031 | Tree loss: 1.629 | Accuracy: 0.533203 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 029 | Total loss: 1.637 | Reg loss: 0.031 | Tree loss: 1.637 | Accuracy: 0.494141 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 029 | Total loss: 1.627 | Reg loss: 0.031 | Tree loss: 1.627 | Accuracy: 0.494141 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 029 | Total loss: 1.592 | Reg loss: 0.031 | Tree loss: 1.592 | Accuracy: 0.523438 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 029 | Total loss: 1.623 | Reg loss: 0.031 | Tree loss: 1.623 | Accuracy: 0.494141 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 029 | Total loss: 1.628 | Reg loss: 0.031 | Tree loss: 1.628 | Accuracy: 0.468750 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 029 | Total loss: 1.578 | Reg loss: 0.031 | Tree loss: 1.578 | Accuracy: 0.496094 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 029 | Total loss: 1.625 | Reg loss: 0.031 | Tree loss: 1.625 | Accuracy: 0.451172 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 029 | Total loss: 1.597 | Reg loss: 0.031 | Tree loss: 1.597 | Accuracy: 0.492188 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 029 | Total loss: 1.552 | Reg loss: 0.031 | Tree loss: 1.552 | Accuracy: 0.505859 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 029 | Total loss: 1.558 | Reg loss: 0.031 | Tree loss: 1.558 | Accuracy: 0.496094 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 029 | Total loss: 1.541 | Reg loss: 0.031 | Tree loss: 1.541 | Accuracy: 0.511719 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 029 | Total loss: 1.572 | Reg loss: 0.031 | Tree loss: 1.572 | Accuracy: 0.470703 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 029 | Total loss: 1.564 | Reg loss: 0.031 | Tree loss: 1.564 | Accuracy: 0.478516 | 7.21 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 029 | Total loss: 1.558 | Reg loss: 0.031 | Tree loss: 1.558 | Accuracy: 0.476562 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 029 | Total loss: 1.535 | Reg loss: 0.031 | Tree loss: 1.535 | Accuracy: 0.500000 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 029 | Total loss: 1.516 | Reg loss: 0.031 | Tree loss: 1.516 | Accuracy: 0.527344 | 7.209 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 029 | Total loss: 1.531 | Reg loss: 0.031 | Tree loss: 1.531 | Accuracy: 0.480469 | 7.208 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 029 | Total loss: 1.515 | Reg loss: 0.032 | Tree loss: 1.515 | Accuracy: 0.507812 | 7.208 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 029 | Total loss: 1.502 | Reg loss: 0.032 | Tree loss: 1.502 | Accuracy: 0.515625 | 7.208 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 029 | Total loss: 1.505 | Reg loss: 0.032 | Tree loss: 1.505 | Accuracy: 0.492188 | 7.207 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 029 | Total loss: 1.537 | Reg loss: 0.032 | Tree loss: 1.537 | Accuracy: 0.445312 | 7.207 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 029 | Total loss: 1.472 | Reg loss: 0.032 | Tree loss: 1.472 | Accuracy: 0.527344 | 7.207 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 029 | Total loss: 1.489 | Reg loss: 0.032 | Tree loss: 1.489 | Accuracy: 0.503906 | 7.205 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 029 | Total loss: 1.481 | Reg loss: 0.032 | Tree loss: 1.481 | Accuracy: 0.507812 | 7.204 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 029 | Total loss: 1.510 | Reg loss: 0.032 | Tree loss: 1.510 | Accuracy: 0.460938 | 7.202 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 029 | Total loss: 1.477 | Reg loss: 0.032 | Tree loss: 1.477 | Accuracy: 0.500986 | 7.201 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 029 | Total loss: 1.642 | Reg loss: 0.031 | Tree loss: 1.642 | Accuracy: 0.476562 | 7.204 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 029 | Total loss: 1.650 | Reg loss: 0.031 | Tree loss: 1.650 | Accuracy: 0.486328 | 7.205 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 029 | Total loss: 1.597 | Reg loss: 0.031 | Tree loss: 1.597 | Accuracy: 0.507812 | 7.205 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 029 | Total loss: 1.627 | Reg loss: 0.031 | Tree loss: 1.627 | Accuracy: 0.496094 | 7.205 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 029 | Total loss: 1.620 | Reg loss: 0.031 | Tree loss: 1.620 | Accuracy: 0.478516 | 7.205 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 029 | Total loss: 1.600 | Reg loss: 0.031 | Tree loss: 1.600 | Accuracy: 0.490234 | 7.205 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 029 | Total loss: 1.592 | Reg loss: 0.031 | Tree loss: 1.592 | Accuracy: 0.476562 | 7.205 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 029 | Total loss: 1.580 | Reg loss: 0.031 | Tree loss: 1.580 | Accuracy: 0.507812 | 7.205 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 008 / 029 | Total loss: 1.577 | Reg loss: 0.031 | Tree loss: 1.577 | Accuracy: 0.484375 | 7.205 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 029 | Total loss: 1.571 | Reg loss: 0.031 | Tree loss: 1.571 | Accuracy: 0.490234 | 7.204 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 029 | Total loss: 1.573 | Reg loss: 0.031 | Tree loss: 1.573 | Accuracy: 0.464844 | 7.204 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 029 | Total loss: 1.537 | Reg loss: 0.031 | Tree loss: 1.537 | Accuracy: 0.519531 | 7.204 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 029 | Total loss: 1.553 | Reg loss: 0.031 | Tree loss: 1.553 | Accuracy: 0.488281 | 7.203 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 029 | Total loss: 1.534 | Reg loss: 0.031 | Tree loss: 1.534 | Accuracy: 0.500000 | 7.203 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 029 | Total loss: 1.495 | Reg loss: 0.031 | Tree loss: 1.495 | Accuracy: 0.542969 | 7.203 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 029 | Total loss: 1.521 | Reg loss: 0.031 | Tree loss: 1.521 | Accuracy: 0.496094 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 029 | Total loss: 1.525 | Reg loss: 0.031 | Tree loss: 1.525 | Accuracy: 0.484375 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 029 | Total loss: 1.521 | Reg loss: 0.031 | Tree loss: 1.521 | Accuracy: 0.492188 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 029 | Total loss: 1.497 | Reg loss: 0.031 | Tree loss: 1.497 | Accuracy: 0.498047 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 029 | Total loss: 1.511 | Reg loss: 0.031 | Tree loss: 1.511 | Accuracy: 0.474609 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 029 | Total loss: 1.463 | Reg loss: 0.031 | Tree loss: 1.463 | Accuracy: 0.539062 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 029 | Total loss: 1.513 | Reg loss: 0.031 | Tree loss: 1.513 | Accuracy: 0.466797 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 029 | Total loss: 1.470 | Reg loss: 0.031 | Tree loss: 1.470 | Accuracy: 0.523438 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 029 | Total loss: 1.485 | Reg loss: 0.032 | Tree loss: 1.485 | Accuracy: 0.498047 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 029 | Total loss: 1.526 | Reg loss: 0.032 | Tree loss: 1.526 | Accuracy: 0.435547 | 7.202 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 029 | Total loss: 1.482 | Reg loss: 0.032 | Tree loss: 1.482 | Accuracy: 0.486328 | 7.201 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 029 | Total loss: 1.467 | Reg loss: 0.032 | Tree loss: 1.467 | Accuracy: 0.501953 | 7.2 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 029 | Total loss: 1.449 | Reg loss: 0.032 | Tree loss: 1.449 | Accuracy: 0.519531 | 7.198 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 029 | Total loss: 1.454 | Reg loss: 0.032 | Tree loss: 1.454 | Accuracy: 0.506903 | 7.198 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 029 | Total loss: 1.618 | Reg loss: 0.031 | Tree loss: 1.618 | Accuracy: 0.511719 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 029 | Total loss: 1.602 | Reg loss: 0.031 | Tree loss: 1.602 | Accuracy: 0.505859 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 029 | Total loss: 1.638 | Reg loss: 0.031 | Tree loss: 1.638 | Accuracy: 0.457031 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 029 | Total loss: 1.596 | Reg loss: 0.031 | Tree loss: 1.596 | Accuracy: 0.494141 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 029 | Total loss: 1.578 | Reg loss: 0.031 | Tree loss: 1.578 | Accuracy: 0.498047 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 029 | Total loss: 1.591 | Reg loss: 0.031 | Tree loss: 1.591 | Accuracy: 0.466797 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 029 | Total loss: 1.597 | Reg loss: 0.031 | Tree loss: 1.597 | Accuracy: 0.462891 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 029 | Total loss: 1.548 | Reg loss: 0.031 | Tree loss: 1.548 | Accuracy: 0.505859 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 029 | Total loss: 1.552 | Reg loss: 0.031 | Tree loss: 1.552 | Accuracy: 0.517578 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 029 | Total loss: 1.542 | Reg loss: 0.031 | Tree loss: 1.542 | Accuracy: 0.484375 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 029 | Total loss: 1.552 | Reg loss: 0.031 | Tree loss: 1.552 | Accuracy: 0.478516 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 029 | Total loss: 1.522 | Reg loss: 0.031 | Tree loss: 1.522 | Accuracy: 0.492188 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 029 | Total loss: 1.525 | Reg loss: 0.031 | Tree loss: 1.525 | Accuracy: 0.490234 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 029 | Total loss: 1.528 | Reg loss: 0.031 | Tree loss: 1.528 | Accuracy: 0.490234 | 7.201 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 029 | Total loss: 1.534 | Reg loss: 0.031 | Tree loss: 1.534 | Accuracy: 0.476562 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 029 | Total loss: 1.484 | Reg loss: 0.031 | Tree loss: 1.484 | Accuracy: 0.501953 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 029 | Total loss: 1.485 | Reg loss: 0.031 | Tree loss: 1.485 | Accuracy: 0.515625 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 029 | Total loss: 1.493 | Reg loss: 0.031 | Tree loss: 1.493 | Accuracy: 0.503906 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 029 | Total loss: 1.462 | Reg loss: 0.031 | Tree loss: 1.462 | Accuracy: 0.521484 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 029 | Total loss: 1.480 | Reg loss: 0.031 | Tree loss: 1.480 | Accuracy: 0.498047 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 029 | Total loss: 1.468 | Reg loss: 0.031 | Tree loss: 1.468 | Accuracy: 0.500000 | 7.2 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 029 | Total loss: 1.483 | Reg loss: 0.031 | Tree loss: 1.483 | Accuracy: 0.484375 | 7.199 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 029 | Total loss: 1.478 | Reg loss: 0.031 | Tree loss: 1.478 | Accuracy: 0.474609 | 7.199 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 029 | Total loss: 1.445 | Reg loss: 0.031 | Tree loss: 1.445 | Accuracy: 0.533203 | 7.199 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 029 | Total loss: 1.486 | Reg loss: 0.031 | Tree loss: 1.486 | Accuracy: 0.464844 | 7.198 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 029 | Total loss: 1.460 | Reg loss: 0.031 | Tree loss: 1.460 | Accuracy: 0.494141 | 7.197 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 029 | Total loss: 1.410 | Reg loss: 0.032 | Tree loss: 1.410 | Accuracy: 0.535156 | 7.195 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 029 | Total loss: 1.463 | Reg loss: 0.032 | Tree loss: 1.463 | Accuracy: 0.466797 | 7.194 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 029 | Total loss: 1.429 | Reg loss: 0.032 | Tree loss: 1.429 | Accuracy: 0.506903 | 7.192 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 029 | Total loss: 1.602 | Reg loss: 0.031 | Tree loss: 1.602 | Accuracy: 0.500000 | 7.194 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 029 | Total loss: 1.576 | Reg loss: 0.031 | Tree loss: 1.576 | Accuracy: 0.507812 | 7.193 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 029 | Total loss: 1.583 | Reg loss: 0.031 | Tree loss: 1.583 | Accuracy: 0.470703 | 7.191 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 029 | Total loss: 1.574 | Reg loss: 0.031 | Tree loss: 1.574 | Accuracy: 0.496094 | 7.19 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 029 | Total loss: 1.553 | Reg loss: 0.031 | Tree loss: 1.553 | Accuracy: 0.501953 | 7.19 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 029 | Total loss: 1.584 | Reg loss: 0.031 | Tree loss: 1.584 | Accuracy: 0.484375 | 7.19 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 029 | Total loss: 1.581 | Reg loss: 0.031 | Tree loss: 1.581 | Accuracy: 0.464844 | 7.19 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 029 | Total loss: 1.506 | Reg loss: 0.031 | Tree loss: 1.506 | Accuracy: 0.525391 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 029 | Total loss: 1.515 | Reg loss: 0.031 | Tree loss: 1.515 | Accuracy: 0.511719 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 029 | Total loss: 1.504 | Reg loss: 0.031 | Tree loss: 1.504 | Accuracy: 0.505859 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 029 | Total loss: 1.519 | Reg loss: 0.031 | Tree loss: 1.519 | Accuracy: 0.500000 | 7.189 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Batch: 011 / 029 | Total loss: 1.511 | Reg loss: 0.031 | Tree loss: 1.511 | Accuracy: 0.511719 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 029 | Total loss: 1.489 | Reg loss: 0.031 | Tree loss: 1.489 | Accuracy: 0.521484 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 029 | Total loss: 1.527 | Reg loss: 0.031 | Tree loss: 1.527 | Accuracy: 0.460938 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 029 | Total loss: 1.511 | Reg loss: 0.031 | Tree loss: 1.511 | Accuracy: 0.472656 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 029 | Total loss: 1.517 | Reg loss: 0.031 | Tree loss: 1.517 | Accuracy: 0.468750 | 7.189 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 029 | Total loss: 1.469 | Reg loss: 0.031 | Tree loss: 1.469 | Accuracy: 0.519531 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 029 | Total loss: 1.482 | Reg loss: 0.031 | Tree loss: 1.482 | Accuracy: 0.484375 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 029 | Total loss: 1.481 | Reg loss: 0.031 | Tree loss: 1.481 | Accuracy: 0.480469 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 029 | Total loss: 1.468 | Reg loss: 0.031 | Tree loss: 1.468 | Accuracy: 0.492188 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 029 | Total loss: 1.488 | Reg loss: 0.031 | Tree loss: 1.488 | Accuracy: 0.451172 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 029 | Total loss: 1.423 | Reg loss: 0.031 | Tree loss: 1.423 | Accuracy: 0.521484 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 029 | Total loss: 1.432 | Reg loss: 0.031 | Tree loss: 1.432 | Accuracy: 0.509766 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 029 | Total loss: 1.448 | Reg loss: 0.031 | Tree loss: 1.448 | Accuracy: 0.490234 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 029 | Total loss: 1.445 | Reg loss: 0.031 | Tree loss: 1.445 | Accuracy: 0.490234 | 7.188 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 029 | Total loss: 1.453 | Reg loss: 0.031 | Tree loss: 1.453 | Accuracy: 0.480469 | 7.187 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 029 | Total loss: 1.429 | Reg loss: 0.031 | Tree loss: 1.429 | Accuracy: 0.509766 | 7.187 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 029 | Total loss: 1.425 | Reg loss: 0.031 | Tree loss: 1.425 | Accuracy: 0.498047 | 7.186 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 029 | Total loss: 1.420 | Reg loss: 0.031 | Tree loss: 1.420 | Accuracy: 0.500986 | 7.184 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 029 | Total loss: 1.572 | Reg loss: 0.031 | Tree loss: 1.572 | Accuracy: 0.498047 | 7.19 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 029 | Total loss: 1.564 | Reg loss: 0.031 | Tree loss: 1.564 | Accuracy: 0.486328 | 7.19 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 029 | Total loss: 1.584 | Reg loss: 0.031 | Tree loss: 1.584 | Accuracy: 0.460938 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 029 | Total loss: 1.548 | Reg loss: 0.031 | Tree loss: 1.548 | Accuracy: 0.505859 | 7.188 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 029 | Total loss: 1.500 | Reg loss: 0.031 | Tree loss: 1.500 | Accuracy: 0.544922 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 029 | Total loss: 1.549 | Reg loss: 0.031 | Tree loss: 1.549 | Accuracy: 0.496094 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 029 | Total loss: 1.541 | Reg loss: 0.031 | Tree loss: 1.541 | Accuracy: 0.472656 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 029 | Total loss: 1.529 | Reg loss: 0.031 | Tree loss: 1.529 | Accuracy: 0.470703 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 029 | Total loss: 1.519 | Reg loss: 0.031 | Tree loss: 1.519 | Accuracy: 0.498047 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 029 | Total loss: 1.520 | Reg loss: 0.031 | Tree loss: 1.520 | Accuracy: 0.478516 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 029 | Total loss: 1.528 | Reg loss: 0.031 | Tree loss: 1.528 | Accuracy: 0.460938 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 029 | Total loss: 1.466 | Reg loss: 0.031 | Tree loss: 1.466 | Accuracy: 0.515625 | 7.189 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 029 | Total loss: 1.463 | Reg loss: 0.031 | Tree loss: 1.463 | Accuracy: 0.511719 | 7.188 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 029 | Total loss: 1.485 | Reg loss: 0.031 | Tree loss: 1.485 | Accuracy: 0.494141 | 7.188 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 029 | Total loss: 1.486 | Reg loss: 0.031 | Tree loss: 1.486 | Accuracy: 0.492188 | 7.187 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 029 | Total loss: 1.472 | Reg loss: 0.031 | Tree loss: 1.472 | Accuracy: 0.492188 | 7.187 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 029 | Total loss: 1.437 | Reg loss: 0.031 | Tree loss: 1.437 | Accuracy: 0.533203 | 7.187 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 029 | Total loss: 1.449 | Reg loss: 0.031 | Tree loss: 1.449 | Accuracy: 0.509766 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 029 | Total loss: 1.452 | Reg loss: 0.031 | Tree loss: 1.452 | Accuracy: 0.494141 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 029 | Total loss: 1.448 | Reg loss: 0.031 | Tree loss: 1.448 | Accuracy: 0.498047 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 029 | Total loss: 1.497 | Reg loss: 0.031 | Tree loss: 1.497 | Accuracy: 0.425781 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 029 | Total loss: 1.435 | Reg loss: 0.031 | Tree loss: 1.435 | Accuracy: 0.513672 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 029 | Total loss: 1.421 | Reg loss: 0.031 | Tree loss: 1.421 | Accuracy: 0.505859 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 029 | Total loss: 1.435 | Reg loss: 0.031 | Tree loss: 1.435 | Accuracy: 0.486328 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 029 | Total loss: 1.444 | Reg loss: 0.031 | Tree loss: 1.444 | Accuracy: 0.482422 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 029 | Total loss: 1.437 | Reg loss: 0.031 | Tree loss: 1.437 | Accuracy: 0.466797 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 029 | Total loss: 1.440 | Reg loss: 0.031 | Tree loss: 1.440 | Accuracy: 0.474609 | 7.186 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 029 | Total loss: 1.395 | Reg loss: 0.031 | Tree loss: 1.395 | Accuracy: 0.525391 | 7.185 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 029 | Total loss: 1.384 | Reg loss: 0.031 | Tree loss: 1.384 | Accuracy: 0.538462 | 7.183 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 029 | Total loss: 1.570 | Reg loss: 0.031 | Tree loss: 1.570 | Accuracy: 0.468750 | 7.19 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 029 | Total loss: 1.548 | Reg loss: 0.031 | Tree loss: 1.548 | Accuracy: 0.494141 | 7.19 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 029 | Total loss: 1.534 | Reg loss: 0.031 | Tree loss: 1.534 | Accuracy: 0.498047 | 7.189 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 029 | Total loss: 1.531 | Reg loss: 0.031 | Tree loss: 1.531 | Accuracy: 0.501953 | 7.189 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 029 | Total loss: 1.520 | Reg loss: 0.031 | Tree loss: 1.520 | Accuracy: 0.488281 | 7.189 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 029 | Total loss: 1.496 | Reg loss: 0.031 | Tree loss: 1.496 | Accuracy: 0.513672 | 7.189 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 029 | Total loss: 1.485 | Reg loss: 0.031 | Tree loss: 1.485 | Accuracy: 0.529297 | 7.189 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 029 | Total loss: 1.548 | Reg loss: 0.031 | Tree loss: 1.548 | Accuracy: 0.449219 | 7.189 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 029 | Total loss: 1.509 | Reg loss: 0.031 | Tree loss: 1.509 | Accuracy: 0.472656 | 7.189 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 029 | Total loss: 1.491 | Reg loss: 0.031 | Tree loss: 1.491 | Accuracy: 0.505859 | 7.188 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 029 | Total loss: 1.486 | Reg loss: 0.031 | Tree loss: 1.486 | Accuracy: 0.488281 | 7.188 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 029 | Total loss: 1.471 | Reg loss: 0.031 | Tree loss: 1.471 | Accuracy: 0.505859 | 7.188 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 029 | Total loss: 1.478 | Reg loss: 0.031 | Tree loss: 1.478 | Accuracy: 0.490234 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 029 | Total loss: 1.470 | Reg loss: 0.031 | Tree loss: 1.470 | Accuracy: 0.478516 | 7.187 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 014 / 029 | Total loss: 1.454 | Reg loss: 0.031 | Tree loss: 1.454 | Accuracy: 0.503906 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 029 | Total loss: 1.452 | Reg loss: 0.031 | Tree loss: 1.452 | Accuracy: 0.500000 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 029 | Total loss: 1.430 | Reg loss: 0.031 | Tree loss: 1.430 | Accuracy: 0.521484 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 029 | Total loss: 1.454 | Reg loss: 0.031 | Tree loss: 1.454 | Accuracy: 0.488281 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 029 | Total loss: 1.443 | Reg loss: 0.031 | Tree loss: 1.443 | Accuracy: 0.488281 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 029 | Total loss: 1.456 | Reg loss: 0.031 | Tree loss: 1.456 | Accuracy: 0.472656 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 029 | Total loss: 1.463 | Reg loss: 0.031 | Tree loss: 1.463 | Accuracy: 0.451172 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 029 | Total loss: 1.423 | Reg loss: 0.031 | Tree loss: 1.423 | Accuracy: 0.498047 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 029 | Total loss: 1.451 | Reg loss: 0.031 | Tree loss: 1.451 | Accuracy: 0.453125 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 029 | Total loss: 1.409 | Reg loss: 0.031 | Tree loss: 1.409 | Accuracy: 0.511719 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 029 | Total loss: 1.399 | Reg loss: 0.031 | Tree loss: 1.399 | Accuracy: 0.505859 | 7.187 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 029 | Total loss: 1.410 | Reg loss: 0.031 | Tree loss: 1.410 | Accuracy: 0.507812 | 7.186 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 029 | Total loss: 1.369 | Reg loss: 0.031 | Tree loss: 1.369 | Accuracy: 0.531250 | 7.185 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 029 | Total loss: 1.369 | Reg loss: 0.031 | Tree loss: 1.369 | Accuracy: 0.541016 | 7.183 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 029 | Total loss: 1.414 | Reg loss: 0.031 | Tree loss: 1.414 | Accuracy: 0.473373 | 7.182 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 029 | Total loss: 1.557 | Reg loss: 0.031 | Tree loss: 1.557 | Accuracy: 0.480469 | 7.19 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 029 | Total loss: 1.526 | Reg loss: 0.031 | Tree loss: 1.526 | Accuracy: 0.503906 | 7.189 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 029 | Total loss: 1.527 | Reg loss: 0.031 | Tree loss: 1.527 | Accuracy: 0.496094 | 7.188 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 029 | Total loss: 1.523 | Reg loss: 0.031 | Tree loss: 1.523 | Accuracy: 0.488281 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 029 | Total loss: 1.471 | Reg loss: 0.031 | Tree loss: 1.471 | Accuracy: 0.542969 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 029 | Total loss: 1.505 | Reg loss: 0.031 | Tree loss: 1.505 | Accuracy: 0.496094 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 029 | Total loss: 1.509 | Reg loss: 0.031 | Tree loss: 1.509 | Accuracy: 0.492188 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 029 | Total loss: 1.505 | Reg loss: 0.031 | Tree loss: 1.505 | Accuracy: 0.468750 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 029 | Total loss: 1.490 | Reg loss: 0.031 | Tree loss: 1.490 | Accuracy: 0.484375 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 029 | Total loss: 1.469 | Reg loss: 0.031 | Tree loss: 1.469 | Accuracy: 0.511719 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 029 | Total loss: 1.493 | Reg loss: 0.031 | Tree loss: 1.493 | Accuracy: 0.464844 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 029 | Total loss: 1.483 | Reg loss: 0.031 | Tree loss: 1.483 | Accuracy: 0.455078 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 029 | Total loss: 1.476 | Reg loss: 0.031 | Tree loss: 1.476 | Accuracy: 0.486328 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 029 | Total loss: 1.466 | Reg loss: 0.031 | Tree loss: 1.466 | Accuracy: 0.484375 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 029 | Total loss: 1.462 | Reg loss: 0.031 | Tree loss: 1.462 | Accuracy: 0.490234 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 029 | Total loss: 1.430 | Reg loss: 0.031 | Tree loss: 1.430 | Accuracy: 0.501953 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 029 | Total loss: 1.410 | Reg loss: 0.031 | Tree loss: 1.410 | Accuracy: 0.517578 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 029 | Total loss: 1.425 | Reg loss: 0.031 | Tree loss: 1.425 | Accuracy: 0.492188 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 029 | Total loss: 1.423 | Reg loss: 0.031 | Tree loss: 1.423 | Accuracy: 0.498047 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 029 | Total loss: 1.397 | Reg loss: 0.031 | Tree loss: 1.397 | Accuracy: 0.509766 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 029 | Total loss: 1.423 | Reg loss: 0.031 | Tree loss: 1.423 | Accuracy: 0.480469 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 029 | Total loss: 1.431 | Reg loss: 0.031 | Tree loss: 1.431 | Accuracy: 0.453125 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 029 | Total loss: 1.392 | Reg loss: 0.031 | Tree loss: 1.392 | Accuracy: 0.515625 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 029 | Total loss: 1.381 | Reg loss: 0.031 | Tree loss: 1.381 | Accuracy: 0.511719 | 7.187 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 029 | Total loss: 1.402 | Reg loss: 0.031 | Tree loss: 1.402 | Accuracy: 0.486328 | 7.186 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 029 | Total loss: 1.359 | Reg loss: 0.031 | Tree loss: 1.359 | Accuracy: 0.535156 | 7.184 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 029 | Total loss: 1.398 | Reg loss: 0.031 | Tree loss: 1.398 | Accuracy: 0.482422 | 7.183 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 029 | Total loss: 1.362 | Reg loss: 0.031 | Tree loss: 1.362 | Accuracy: 0.533203 | 7.182 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 029 | Total loss: 1.406 | Reg loss: 0.031 | Tree loss: 1.406 | Accuracy: 0.469428 | 7.18 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 029 | Total loss: 1.537 | Reg loss: 0.031 | Tree loss: 1.537 | Accuracy: 0.500000 | 7.185 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 029 | Total loss: 1.503 | Reg loss: 0.031 | Tree loss: 1.503 | Accuracy: 0.542969 | 7.185 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 029 | Total loss: 1.552 | Reg loss: 0.031 | Tree loss: 1.552 | Accuracy: 0.449219 | 7.185 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 029 | Total loss: 1.526 | Reg loss: 0.031 | Tree loss: 1.526 | Accuracy: 0.470703 | 7.184 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 029 | Total loss: 1.468 | Reg loss: 0.031 | Tree loss: 1.468 | Accuracy: 0.519531 | 7.184 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 029 | Total loss: 1.520 | Reg loss: 0.031 | Tree loss: 1.520 | Accuracy: 0.472656 | 7.183 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 029 | Total loss: 1.459 | Reg loss: 0.031 | Tree loss: 1.459 | Accuracy: 0.535156 | 7.183 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 029 | Total loss: 1.492 | Reg loss: 0.031 | Tree loss: 1.492 | Accuracy: 0.496094 | 7.183 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 029 | Total loss: 1.458 | Reg loss: 0.031 | Tree loss: 1.458 | Accuracy: 0.521484 | 7.182 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 029 | Total loss: 1.464 | Reg loss: 0.031 | Tree loss: 1.464 | Accuracy: 0.488281 | 7.182 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 029 | Total loss: 1.456 | Reg loss: 0.031 | Tree loss: 1.456 | Accuracy: 0.492188 | 7.182 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 029 | Total loss: 1.458 | Reg loss: 0.031 | Tree loss: 1.458 | Accuracy: 0.458984 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 029 | Total loss: 1.468 | Reg loss: 0.031 | Tree loss: 1.468 | Accuracy: 0.457031 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 029 | Total loss: 1.413 | Reg loss: 0.031 | Tree loss: 1.413 | Accuracy: 0.531250 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 029 | Total loss: 1.410 | Reg loss: 0.031 | Tree loss: 1.410 | Accuracy: 0.507812 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 029 | Total loss: 1.457 | Reg loss: 0.031 | Tree loss: 1.457 | Accuracy: 0.447266 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 029 | Total loss: 1.414 | Reg loss: 0.031 | Tree loss: 1.414 | Accuracy: 0.490234 | 7.181 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 017 / 029 | Total loss: 1.415 | Reg loss: 0.031 | Tree loss: 1.415 | Accuracy: 0.484375 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 029 | Total loss: 1.399 | Reg loss: 0.031 | Tree loss: 1.399 | Accuracy: 0.503906 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 029 | Total loss: 1.394 | Reg loss: 0.031 | Tree loss: 1.394 | Accuracy: 0.503906 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 029 | Total loss: 1.399 | Reg loss: 0.031 | Tree loss: 1.399 | Accuracy: 0.496094 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 029 | Total loss: 1.400 | Reg loss: 0.031 | Tree loss: 1.400 | Accuracy: 0.498047 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 029 | Total loss: 1.369 | Reg loss: 0.031 | Tree loss: 1.369 | Accuracy: 0.523438 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 029 | Total loss: 1.405 | Reg loss: 0.031 | Tree loss: 1.405 | Accuracy: 0.460938 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 029 | Total loss: 1.390 | Reg loss: 0.031 | Tree loss: 1.390 | Accuracy: 0.474609 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 029 | Total loss: 1.407 | Reg loss: 0.031 | Tree loss: 1.407 | Accuracy: 0.464844 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 029 | Total loss: 1.355 | Reg loss: 0.031 | Tree loss: 1.355 | Accuracy: 0.513672 | 7.181 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 029 | Total loss: 1.364 | Reg loss: 0.031 | Tree loss: 1.364 | Accuracy: 0.515625 | 7.179 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 029 | Total loss: 1.360 | Reg loss: 0.031 | Tree loss: 1.360 | Accuracy: 0.512821 | 7.178 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 029 | Total loss: 1.477 | Reg loss: 0.031 | Tree loss: 1.477 | Accuracy: 0.539062 | 7.182 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 029 | Total loss: 1.476 | Reg loss: 0.031 | Tree loss: 1.476 | Accuracy: 0.525391 | 7.183 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 029 | Total loss: 1.516 | Reg loss: 0.031 | Tree loss: 1.516 | Accuracy: 0.490234 | 7.183 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 029 | Total loss: 1.447 | Reg loss: 0.031 | Tree loss: 1.447 | Accuracy: 0.537109 | 7.182 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 029 | Total loss: 1.491 | Reg loss: 0.031 | Tree loss: 1.491 | Accuracy: 0.488281 | 7.18 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 029 | Total loss: 1.484 | Reg loss: 0.031 | Tree loss: 1.484 | Accuracy: 0.488281 | 7.18 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 029 | Total loss: 1.487 | Reg loss: 0.031 | Tree loss: 1.487 | Accuracy: 0.480469 | 7.18 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 029 | Total loss: 1.465 | Reg loss: 0.031 | Tree loss: 1.465 | Accuracy: 0.492188 | 7.18 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 029 | Total loss: 1.452 | Reg loss: 0.031 | Tree loss: 1.452 | Accuracy: 0.498047 | 7.179 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 029 | Total loss: 1.470 | Reg loss: 0.031 | Tree loss: 1.470 | Accuracy: 0.470703 | 7.179 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 029 | Total loss: 1.439 | Reg loss: 0.031 | Tree loss: 1.439 | Accuracy: 0.494141 | 7.179 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 029 | Total loss: 1.430 | Reg loss: 0.031 | Tree loss: 1.430 | Accuracy: 0.513672 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 029 | Total loss: 1.441 | Reg loss: 0.031 | Tree loss: 1.441 | Accuracy: 0.496094 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 029 | Total loss: 1.436 | Reg loss: 0.031 | Tree loss: 1.436 | Accuracy: 0.464844 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 029 | Total loss: 1.410 | Reg loss: 0.031 | Tree loss: 1.410 | Accuracy: 0.498047 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 029 | Total loss: 1.452 | Reg loss: 0.031 | Tree loss: 1.452 | Accuracy: 0.458984 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 029 | Total loss: 1.413 | Reg loss: 0.031 | Tree loss: 1.413 | Accuracy: 0.494141 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 029 | Total loss: 1.404 | Reg loss: 0.031 | Tree loss: 1.404 | Accuracy: 0.472656 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 029 | Total loss: 1.372 | Reg loss: 0.031 | Tree loss: 1.372 | Accuracy: 0.523438 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 029 | Total loss: 1.373 | Reg loss: 0.031 | Tree loss: 1.373 | Accuracy: 0.517578 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 029 | Total loss: 1.418 | Reg loss: 0.031 | Tree loss: 1.418 | Accuracy: 0.449219 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 029 | Total loss: 1.370 | Reg loss: 0.031 | Tree loss: 1.370 | Accuracy: 0.507812 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 029 | Total loss: 1.380 | Reg loss: 0.031 | Tree loss: 1.380 | Accuracy: 0.492188 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 029 | Total loss: 1.362 | Reg loss: 0.031 | Tree loss: 1.362 | Accuracy: 0.507812 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 029 | Total loss: 1.413 | Reg loss: 0.031 | Tree loss: 1.413 | Accuracy: 0.445312 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 029 | Total loss: 1.360 | Reg loss: 0.031 | Tree loss: 1.360 | Accuracy: 0.507812 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 029 | Total loss: 1.367 | Reg loss: 0.031 | Tree loss: 1.367 | Accuracy: 0.505859 | 7.178 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 029 | Total loss: 1.387 | Reg loss: 0.031 | Tree loss: 1.387 | Accuracy: 0.462891 | 7.177 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 029 | Total loss: 1.348 | Reg loss: 0.031 | Tree loss: 1.348 | Accuracy: 0.510848 | 7.175 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 029 | Total loss: 1.528 | Reg loss: 0.031 | Tree loss: 1.528 | Accuracy: 0.462891 | 7.18 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 029 | Total loss: 1.450 | Reg loss: 0.031 | Tree loss: 1.450 | Accuracy: 0.541016 | 7.18 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 029 | Total loss: 1.482 | Reg loss: 0.031 | Tree loss: 1.482 | Accuracy: 0.503906 | 7.18 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 029 | Total loss: 1.508 | Reg loss: 0.031 | Tree loss: 1.508 | Accuracy: 0.439453 | 7.18 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 029 | Total loss: 1.470 | Reg loss: 0.031 | Tree loss: 1.470 | Accuracy: 0.501953 | 7.179 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 029 | Total loss: 1.482 | Reg loss: 0.031 | Tree loss: 1.482 | Accuracy: 0.472656 | 7.177 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 029 | Total loss: 1.423 | Reg loss: 0.031 | Tree loss: 1.423 | Accuracy: 0.548828 | 7.176 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 029 | Total loss: 1.447 | Reg loss: 0.031 | Tree loss: 1.447 | Accuracy: 0.490234 | 7.175 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 029 | Total loss: 1.415 | Reg loss: 0.031 | Tree loss: 1.415 | Accuracy: 0.529297 | 7.173 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 029 | Total loss: 1.448 | Reg loss: 0.031 | Tree loss: 1.448 | Accuracy: 0.476562 | 7.172 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 029 | Total loss: 1.430 | Reg loss: 0.031 | Tree loss: 1.430 | Accuracy: 0.494141 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 029 | Total loss: 1.401 | Reg loss: 0.031 | Tree loss: 1.401 | Accuracy: 0.517578 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 029 | Total loss: 1.443 | Reg loss: 0.031 | Tree loss: 1.443 | Accuracy: 0.445312 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 029 | Total loss: 1.440 | Reg loss: 0.031 | Tree loss: 1.440 | Accuracy: 0.470703 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 029 | Total loss: 1.412 | Reg loss: 0.031 | Tree loss: 1.412 | Accuracy: 0.478516 | 7.171 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 029 | Total loss: 1.413 | Reg loss: 0.031 | Tree loss: 1.413 | Accuracy: 0.490234 | 7.171 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 029 | Total loss: 1.363 | Reg loss: 0.031 | Tree loss: 1.363 | Accuracy: 0.544922 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 029 | Total loss: 1.400 | Reg loss: 0.031 | Tree loss: 1.400 | Accuracy: 0.476562 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 029 | Total loss: 1.373 | Reg loss: 0.031 | Tree loss: 1.373 | Accuracy: 0.507812 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 029 | Total loss: 1.389 | Reg loss: 0.031 | Tree loss: 1.389 | Accuracy: 0.484375 | 7.17 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 020 / 029 | Total loss: 1.366 | Reg loss: 0.031 | Tree loss: 1.366 | Accuracy: 0.511719 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 029 | Total loss: 1.366 | Reg loss: 0.031 | Tree loss: 1.366 | Accuracy: 0.500000 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 029 | Total loss: 1.364 | Reg loss: 0.031 | Tree loss: 1.364 | Accuracy: 0.500000 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 029 | Total loss: 1.368 | Reg loss: 0.031 | Tree loss: 1.368 | Accuracy: 0.490234 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 029 | Total loss: 1.376 | Reg loss: 0.031 | Tree loss: 1.376 | Accuracy: 0.486328 | 7.17 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 029 | Total loss: 1.369 | Reg loss: 0.031 | Tree loss: 1.369 | Accuracy: 0.494141 | 7.169 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 029 | Total loss: 1.371 | Reg loss: 0.031 | Tree loss: 1.371 | Accuracy: 0.476562 | 7.168 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 029 | Total loss: 1.317 | Reg loss: 0.031 | Tree loss: 1.317 | Accuracy: 0.533203 | 7.167 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 029 | Total loss: 1.369 | Reg loss: 0.031 | Tree loss: 1.369 | Accuracy: 0.463511 | 7.166 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 029 | Total loss: 1.492 | Reg loss: 0.031 | Tree loss: 1.492 | Accuracy: 0.494141 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 029 | Total loss: 1.503 | Reg loss: 0.031 | Tree loss: 1.503 | Accuracy: 0.484375 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 029 | Total loss: 1.474 | Reg loss: 0.031 | Tree loss: 1.474 | Accuracy: 0.496094 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 029 | Total loss: 1.457 | Reg loss: 0.031 | Tree loss: 1.457 | Accuracy: 0.511719 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 029 | Total loss: 1.490 | Reg loss: 0.031 | Tree loss: 1.490 | Accuracy: 0.464844 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 029 | Total loss: 1.465 | Reg loss: 0.031 | Tree loss: 1.465 | Accuracy: 0.470703 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 029 | Total loss: 1.453 | Reg loss: 0.031 | Tree loss: 1.453 | Accuracy: 0.488281 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 029 | Total loss: 1.459 | Reg loss: 0.031 | Tree loss: 1.459 | Accuracy: 0.470703 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 029 | Total loss: 1.437 | Reg loss: 0.031 | Tree loss: 1.437 | Accuracy: 0.490234 | 7.171 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 029 | Total loss: 1.433 | Reg loss: 0.031 | Tree loss: 1.433 | Accuracy: 0.488281 | 7.17 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 029 | Total loss: 1.387 | Reg loss: 0.031 | Tree loss: 1.387 | Accuracy: 0.539062 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 029 | Total loss: 1.375 | Reg loss: 0.031 | Tree loss: 1.375 | Accuracy: 0.523438 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 029 | Total loss: 1.391 | Reg loss: 0.031 | Tree loss: 1.391 | Accuracy: 0.505859 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 029 | Total loss: 1.414 | Reg loss: 0.031 | Tree loss: 1.414 | Accuracy: 0.478516 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 029 | Total loss: 1.389 | Reg loss: 0.031 | Tree loss: 1.389 | Accuracy: 0.490234 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 029 | Total loss: 1.391 | Reg loss: 0.031 | Tree loss: 1.391 | Accuracy: 0.490234 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 029 | Total loss: 1.375 | Reg loss: 0.031 | Tree loss: 1.375 | Accuracy: 0.496094 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 029 | Total loss: 1.367 | Reg loss: 0.031 | Tree loss: 1.367 | Accuracy: 0.503906 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 029 | Total loss: 1.387 | Reg loss: 0.031 | Tree loss: 1.387 | Accuracy: 0.472656 | 7.169 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 029 | Total loss: 1.394 | Reg loss: 0.031 | Tree loss: 1.394 | Accuracy: 0.462891 | 7.168 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 029 | Total loss: 1.342 | Reg loss: 0.031 | Tree loss: 1.342 | Accuracy: 0.525391 | 7.168 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 029 | Total loss: 1.376 | Reg loss: 0.031 | Tree loss: 1.376 | Accuracy: 0.472656 | 7.168 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 029 | Total loss: 1.330 | Reg loss: 0.031 | Tree loss: 1.330 | Accuracy: 0.542969 | 7.168 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 029 | Total loss: 1.342 | Reg loss: 0.031 | Tree loss: 1.342 | Accuracy: 0.513672 | 7.168 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 029 | Total loss: 1.347 | Reg loss: 0.031 | Tree loss: 1.347 | Accuracy: 0.496094 | 7.168 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 029 | Total loss: 1.350 | Reg loss: 0.031 | Tree loss: 1.350 | Accuracy: 0.492188 | 7.167 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 029 | Total loss: 1.355 | Reg loss: 0.031 | Tree loss: 1.355 | Accuracy: 0.480469 | 7.167 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 029 | Total loss: 1.345 | Reg loss: 0.031 | Tree loss: 1.345 | Accuracy: 0.496094 | 7.166 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 029 | Total loss: 1.342 | Reg loss: 0.031 | Tree loss: 1.342 | Accuracy: 0.491124 | 7.165 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 029 | Total loss: 1.486 | Reg loss: 0.030 | Tree loss: 1.486 | Accuracy: 0.486328 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 029 | Total loss: 1.486 | Reg loss: 0.030 | Tree loss: 1.486 | Accuracy: 0.474609 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 029 | Total loss: 1.491 | Reg loss: 0.030 | Tree loss: 1.491 | Accuracy: 0.484375 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 029 | Total loss: 1.446 | Reg loss: 0.030 | Tree loss: 1.446 | Accuracy: 0.507812 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 029 | Total loss: 1.462 | Reg loss: 0.030 | Tree loss: 1.462 | Accuracy: 0.492188 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 029 | Total loss: 1.420 | Reg loss: 0.030 | Tree loss: 1.420 | Accuracy: 0.515625 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 029 | Total loss: 1.421 | Reg loss: 0.030 | Tree loss: 1.421 | Accuracy: 0.515625 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 029 | Total loss: 1.401 | Reg loss: 0.030 | Tree loss: 1.401 | Accuracy: 0.531250 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 029 | Total loss: 1.415 | Reg loss: 0.030 | Tree loss: 1.415 | Accuracy: 0.486328 | 7.17 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 029 | Total loss: 1.409 | Reg loss: 0.030 | Tree loss: 1.409 | Accuracy: 0.513672 | 7.169 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 029 | Total loss: 1.419 | Reg loss: 0.031 | Tree loss: 1.419 | Accuracy: 0.482422 | 7.167 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 029 | Total loss: 1.378 | Reg loss: 0.031 | Tree loss: 1.378 | Accuracy: 0.527344 | 7.166 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 029 | Total loss: 1.372 | Reg loss: 0.031 | Tree loss: 1.372 | Accuracy: 0.523438 | 7.165 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 029 | Total loss: 1.382 | Reg loss: 0.031 | Tree loss: 1.382 | Accuracy: 0.503906 | 7.163 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 029 | Total loss: 1.368 | Reg loss: 0.031 | Tree loss: 1.368 | Accuracy: 0.523438 | 7.162 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 029 | Total loss: 1.398 | Reg loss: 0.031 | Tree loss: 1.398 | Accuracy: 0.460938 | 7.16 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 029 | Total loss: 1.371 | Reg loss: 0.031 | Tree loss: 1.371 | Accuracy: 0.509766 | 7.159 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 029 | Total loss: 1.360 | Reg loss: 0.031 | Tree loss: 1.360 | Accuracy: 0.509766 | 7.158 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 029 | Total loss: 1.395 | Reg loss: 0.031 | Tree loss: 1.395 | Accuracy: 0.453125 | 7.156 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 029 | Total loss: 1.343 | Reg loss: 0.031 | Tree loss: 1.343 | Accuracy: 0.509766 | 7.156 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 029 | Total loss: 1.374 | Reg loss: 0.031 | Tree loss: 1.374 | Accuracy: 0.464844 | 7.156 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 029 | Total loss: 1.353 | Reg loss: 0.031 | Tree loss: 1.353 | Accuracy: 0.494141 | 7.156 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 029 | Total loss: 1.328 | Reg loss: 0.031 | Tree loss: 1.328 | Accuracy: 0.511719 | 7.156 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 023 / 029 | Total loss: 1.347 | Reg loss: 0.031 | Tree loss: 1.347 | Accuracy: 0.488281 | 7.156 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 029 | Total loss: 1.349 | Reg loss: 0.031 | Tree loss: 1.349 | Accuracy: 0.482422 | 7.156 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 029 | Total loss: 1.354 | Reg loss: 0.031 | Tree loss: 1.354 | Accuracy: 0.466797 | 7.156 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 029 | Total loss: 1.349 | Reg loss: 0.031 | Tree loss: 1.349 | Accuracy: 0.474609 | 7.154 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 029 | Total loss: 1.356 | Reg loss: 0.031 | Tree loss: 1.356 | Accuracy: 0.451172 | 7.153 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 029 | Total loss: 1.336 | Reg loss: 0.031 | Tree loss: 1.336 | Accuracy: 0.487179 | 7.153 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 029 | Total loss: 1.460 | Reg loss: 0.030 | Tree loss: 1.460 | Accuracy: 0.498047 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 029 | Total loss: 1.450 | Reg loss: 0.030 | Tree loss: 1.450 | Accuracy: 0.519531 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 029 | Total loss: 1.458 | Reg loss: 0.030 | Tree loss: 1.458 | Accuracy: 0.498047 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 029 | Total loss: 1.477 | Reg loss: 0.030 | Tree loss: 1.477 | Accuracy: 0.468750 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 029 | Total loss: 1.437 | Reg loss: 0.030 | Tree loss: 1.437 | Accuracy: 0.527344 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 029 | Total loss: 1.410 | Reg loss: 0.030 | Tree loss: 1.410 | Accuracy: 0.515625 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 029 | Total loss: 1.463 | Reg loss: 0.030 | Tree loss: 1.463 | Accuracy: 0.455078 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 029 | Total loss: 1.369 | Reg loss: 0.030 | Tree loss: 1.369 | Accuracy: 0.550781 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 029 | Total loss: 1.403 | Reg loss: 0.030 | Tree loss: 1.403 | Accuracy: 0.488281 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 029 | Total loss: 1.402 | Reg loss: 0.030 | Tree loss: 1.402 | Accuracy: 0.484375 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 029 | Total loss: 1.391 | Reg loss: 0.030 | Tree loss: 1.391 | Accuracy: 0.505859 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 029 | Total loss: 1.378 | Reg loss: 0.030 | Tree loss: 1.378 | Accuracy: 0.511719 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 029 | Total loss: 1.380 | Reg loss: 0.030 | Tree loss: 1.380 | Accuracy: 0.498047 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 029 | Total loss: 1.373 | Reg loss: 0.030 | Tree loss: 1.373 | Accuracy: 0.503906 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 029 | Total loss: 1.396 | Reg loss: 0.031 | Tree loss: 1.396 | Accuracy: 0.474609 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 029 | Total loss: 1.404 | Reg loss: 0.031 | Tree loss: 1.404 | Accuracy: 0.457031 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 029 | Total loss: 1.372 | Reg loss: 0.031 | Tree loss: 1.372 | Accuracy: 0.488281 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 029 | Total loss: 1.361 | Reg loss: 0.031 | Tree loss: 1.361 | Accuracy: 0.488281 | 7.158 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 029 | Total loss: 1.325 | Reg loss: 0.031 | Tree loss: 1.325 | Accuracy: 0.537109 | 7.156 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 029 | Total loss: 1.329 | Reg loss: 0.031 | Tree loss: 1.329 | Accuracy: 0.505859 | 7.155 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 029 | Total loss: 1.355 | Reg loss: 0.031 | Tree loss: 1.355 | Accuracy: 0.478516 | 7.154 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 029 | Total loss: 1.351 | Reg loss: 0.031 | Tree loss: 1.351 | Accuracy: 0.478516 | 7.153 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 029 | Total loss: 1.345 | Reg loss: 0.031 | Tree loss: 1.345 | Accuracy: 0.476562 | 7.153 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 029 | Total loss: 1.341 | Reg loss: 0.031 | Tree loss: 1.341 | Accuracy: 0.496094 | 7.153 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 029 | Total loss: 1.350 | Reg loss: 0.031 | Tree loss: 1.350 | Accuracy: 0.476562 | 7.153 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 029 | Total loss: 1.337 | Reg loss: 0.031 | Tree loss: 1.337 | Accuracy: 0.484375 | 7.151 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 029 | Total loss: 1.345 | Reg loss: 0.031 | Tree loss: 1.345 | Accuracy: 0.474609 | 7.15 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 029 | Total loss: 1.317 | Reg loss: 0.031 | Tree loss: 1.317 | Accuracy: 0.490234 | 7.149 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 029 | Total loss: 1.304 | Reg loss: 0.031 | Tree loss: 1.304 | Accuracy: 0.500986 | 7.147 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 029 | Total loss: 1.440 | Reg loss: 0.030 | Tree loss: 1.440 | Accuracy: 0.513672 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 029 | Total loss: 1.465 | Reg loss: 0.030 | Tree loss: 1.465 | Accuracy: 0.486328 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 029 | Total loss: 1.434 | Reg loss: 0.030 | Tree loss: 1.434 | Accuracy: 0.507812 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 029 | Total loss: 1.434 | Reg loss: 0.030 | Tree loss: 1.434 | Accuracy: 0.501953 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 029 | Total loss: 1.452 | Reg loss: 0.030 | Tree loss: 1.452 | Accuracy: 0.482422 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 029 | Total loss: 1.413 | Reg loss: 0.030 | Tree loss: 1.413 | Accuracy: 0.515625 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 029 | Total loss: 1.395 | Reg loss: 0.030 | Tree loss: 1.395 | Accuracy: 0.527344 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 029 | Total loss: 1.414 | Reg loss: 0.030 | Tree loss: 1.414 | Accuracy: 0.470703 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 029 | Total loss: 1.400 | Reg loss: 0.030 | Tree loss: 1.400 | Accuracy: 0.500000 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 029 | Total loss: 1.387 | Reg loss: 0.030 | Tree loss: 1.387 | Accuracy: 0.501953 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 029 | Total loss: 1.402 | Reg loss: 0.030 | Tree loss: 1.402 | Accuracy: 0.466797 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 029 | Total loss: 1.405 | Reg loss: 0.030 | Tree loss: 1.405 | Accuracy: 0.482422 | 7.152 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 029 | Total loss: 1.388 | Reg loss: 0.030 | Tree loss: 1.388 | Accuracy: 0.478516 | 7.151 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 029 | Total loss: 1.370 | Reg loss: 0.030 | Tree loss: 1.370 | Accuracy: 0.482422 | 7.151 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 029 | Total loss: 1.358 | Reg loss: 0.030 | Tree loss: 1.358 | Accuracy: 0.500000 | 7.151 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 029 | Total loss: 1.381 | Reg loss: 0.030 | Tree loss: 1.381 | Accuracy: 0.468750 | 7.151 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 029 | Total loss: 1.350 | Reg loss: 0.030 | Tree loss: 1.350 | Accuracy: 0.503906 | 7.151 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 029 | Total loss: 1.340 | Reg loss: 0.031 | Tree loss: 1.340 | Accuracy: 0.503906 | 7.15 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 029 | Total loss: 1.373 | Reg loss: 0.031 | Tree loss: 1.373 | Accuracy: 0.455078 | 7.15 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 029 | Total loss: 1.351 | Reg loss: 0.031 | Tree loss: 1.351 | Accuracy: 0.490234 | 7.15 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 029 | Total loss: 1.357 | Reg loss: 0.031 | Tree loss: 1.357 | Accuracy: 0.480469 | 7.149 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 029 | Total loss: 1.334 | Reg loss: 0.031 | Tree loss: 1.334 | Accuracy: 0.496094 | 7.148 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 029 | Total loss: 1.324 | Reg loss: 0.031 | Tree loss: 1.324 | Accuracy: 0.505859 | 7.147 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 029 | Total loss: 1.316 | Reg loss: 0.031 | Tree loss: 1.316 | Accuracy: 0.503906 | 7.145 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 029 | Total loss: 1.310 | Reg loss: 0.031 | Tree loss: 1.310 | Accuracy: 0.496094 | 7.144 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 029 | Total loss: 1.315 | Reg loss: 0.031 | Tree loss: 1.315 | Accuracy: 0.488281 | 7.143 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 026 / 029 | Total loss: 1.294 | Reg loss: 0.031 | Tree loss: 1.294 | Accuracy: 0.515625 | 7.141 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 029 | Total loss: 1.311 | Reg loss: 0.031 | Tree loss: 1.311 | Accuracy: 0.494141 | 7.141 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 029 | Total loss: 1.306 | Reg loss: 0.031 | Tree loss: 1.306 | Accuracy: 0.512821 | 7.14 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 029 | Total loss: 1.454 | Reg loss: 0.030 | Tree loss: 1.454 | Accuracy: 0.486328 | 7.144 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 029 | Total loss: 1.415 | Reg loss: 0.030 | Tree loss: 1.415 | Accuracy: 0.533203 | 7.145 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 029 | Total loss: 1.453 | Reg loss: 0.030 | Tree loss: 1.453 | Accuracy: 0.490234 | 7.145 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 029 | Total loss: 1.421 | Reg loss: 0.030 | Tree loss: 1.421 | Accuracy: 0.490234 | 7.145 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 029 | Total loss: 1.412 | Reg loss: 0.030 | Tree loss: 1.412 | Accuracy: 0.519531 | 7.145 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 029 | Total loss: 1.408 | Reg loss: 0.030 | Tree loss: 1.408 | Accuracy: 0.496094 | 7.145 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 029 | Total loss: 1.404 | Reg loss: 0.030 | Tree loss: 1.404 | Accuracy: 0.492188 | 7.145 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 029 | Total loss: 1.392 | Reg loss: 0.030 | Tree loss: 1.392 | Accuracy: 0.507812 | 7.144 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 029 | Total loss: 1.388 | Reg loss: 0.030 | Tree loss: 1.388 | Accuracy: 0.507812 | 7.144 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 029 | Total loss: 1.427 | Reg loss: 0.030 | Tree loss: 1.427 | Accuracy: 0.435547 | 7.144 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 029 | Total loss: 1.403 | Reg loss: 0.030 | Tree loss: 1.403 | Accuracy: 0.460938 | 7.144 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 029 | Total loss: 1.391 | Reg loss: 0.030 | Tree loss: 1.391 | Accuracy: 0.480469 | 7.144 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 029 | Total loss: 1.378 | Reg loss: 0.030 | Tree loss: 1.378 | Accuracy: 0.496094 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 029 | Total loss: 1.356 | Reg loss: 0.030 | Tree loss: 1.356 | Accuracy: 0.498047 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 029 | Total loss: 1.359 | Reg loss: 0.030 | Tree loss: 1.359 | Accuracy: 0.476562 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 029 | Total loss: 1.334 | Reg loss: 0.030 | Tree loss: 1.334 | Accuracy: 0.511719 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 029 | Total loss: 1.333 | Reg loss: 0.030 | Tree loss: 1.333 | Accuracy: 0.511719 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 029 | Total loss: 1.328 | Reg loss: 0.030 | Tree loss: 1.328 | Accuracy: 0.509766 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 029 | Total loss: 1.340 | Reg loss: 0.030 | Tree loss: 1.340 | Accuracy: 0.478516 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 029 | Total loss: 1.342 | Reg loss: 0.030 | Tree loss: 1.342 | Accuracy: 0.476562 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 029 | Total loss: 1.310 | Reg loss: 0.031 | Tree loss: 1.310 | Accuracy: 0.515625 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 029 | Total loss: 1.329 | Reg loss: 0.031 | Tree loss: 1.329 | Accuracy: 0.492188 | 7.143 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 029 | Total loss: 1.319 | Reg loss: 0.031 | Tree loss: 1.319 | Accuracy: 0.488281 | 7.142 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 029 | Total loss: 1.343 | Reg loss: 0.031 | Tree loss: 1.343 | Accuracy: 0.458984 | 7.142 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 029 | Total loss: 1.293 | Reg loss: 0.031 | Tree loss: 1.293 | Accuracy: 0.535156 | 7.142 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 029 | Total loss: 1.303 | Reg loss: 0.031 | Tree loss: 1.303 | Accuracy: 0.513672 | 7.142 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 029 | Total loss: 1.342 | Reg loss: 0.031 | Tree loss: 1.342 | Accuracy: 0.464844 | 7.14 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 029 | Total loss: 1.276 | Reg loss: 0.031 | Tree loss: 1.276 | Accuracy: 0.527344 | 7.14 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 029 | Total loss: 1.316 | Reg loss: 0.031 | Tree loss: 1.316 | Accuracy: 0.477318 | 7.14 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 029 | Total loss: 1.448 | Reg loss: 0.030 | Tree loss: 1.448 | Accuracy: 0.488281 | 7.143 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 029 | Total loss: 1.391 | Reg loss: 0.030 | Tree loss: 1.391 | Accuracy: 0.525391 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 029 | Total loss: 1.425 | Reg loss: 0.030 | Tree loss: 1.425 | Accuracy: 0.482422 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 029 | Total loss: 1.449 | Reg loss: 0.030 | Tree loss: 1.449 | Accuracy: 0.449219 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 029 | Total loss: 1.413 | Reg loss: 0.030 | Tree loss: 1.413 | Accuracy: 0.482422 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 029 | Total loss: 1.412 | Reg loss: 0.030 | Tree loss: 1.412 | Accuracy: 0.498047 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 029 | Total loss: 1.395 | Reg loss: 0.030 | Tree loss: 1.395 | Accuracy: 0.486328 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 029 | Total loss: 1.391 | Reg loss: 0.030 | Tree loss: 1.391 | Accuracy: 0.492188 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 029 | Total loss: 1.369 | Reg loss: 0.030 | Tree loss: 1.369 | Accuracy: 0.511719 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 029 | Total loss: 1.364 | Reg loss: 0.030 | Tree loss: 1.364 | Accuracy: 0.515625 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 029 | Total loss: 1.385 | Reg loss: 0.030 | Tree loss: 1.385 | Accuracy: 0.496094 | 7.144 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 029 | Total loss: 1.379 | Reg loss: 0.030 | Tree loss: 1.379 | Accuracy: 0.488281 | 7.143 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 029 | Total loss: 1.379 | Reg loss: 0.030 | Tree loss: 1.379 | Accuracy: 0.482422 | 7.143 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 029 | Total loss: 1.351 | Reg loss: 0.030 | Tree loss: 1.351 | Accuracy: 0.505859 | 7.143 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 029 | Total loss: 1.374 | Reg loss: 0.030 | Tree loss: 1.374 | Accuracy: 0.464844 | 7.143 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 029 | Total loss: 1.341 | Reg loss: 0.030 | Tree loss: 1.341 | Accuracy: 0.503906 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 029 | Total loss: 1.330 | Reg loss: 0.030 | Tree loss: 1.330 | Accuracy: 0.509766 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 029 | Total loss: 1.321 | Reg loss: 0.030 | Tree loss: 1.321 | Accuracy: 0.505859 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 029 | Total loss: 1.321 | Reg loss: 0.030 | Tree loss: 1.321 | Accuracy: 0.503906 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 029 | Total loss: 1.321 | Reg loss: 0.030 | Tree loss: 1.321 | Accuracy: 0.496094 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 029 | Total loss: 1.341 | Reg loss: 0.030 | Tree loss: 1.341 | Accuracy: 0.457031 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 029 | Total loss: 1.332 | Reg loss: 0.030 | Tree loss: 1.332 | Accuracy: 0.462891 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 029 | Total loss: 1.323 | Reg loss: 0.030 | Tree loss: 1.323 | Accuracy: 0.496094 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 029 | Total loss: 1.323 | Reg loss: 0.031 | Tree loss: 1.323 | Accuracy: 0.486328 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 029 | Total loss: 1.279 | Reg loss: 0.031 | Tree loss: 1.279 | Accuracy: 0.529297 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 029 | Total loss: 1.302 | Reg loss: 0.031 | Tree loss: 1.302 | Accuracy: 0.500000 | 7.142 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 029 | Total loss: 1.293 | Reg loss: 0.031 | Tree loss: 1.293 | Accuracy: 0.501953 | 7.14 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 029 | Total loss: 1.292 | Reg loss: 0.031 | Tree loss: 1.292 | Accuracy: 0.501953 | 7.139 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 029 | Total loss: 1.288 | Reg loss: 0.031 | Tree loss: 1.288 | Accuracy: 0.508876 | 7.14 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 029 | Total loss: 1.443 | Reg loss: 0.030 | Tree loss: 1.443 | Accuracy: 0.488281 | 7.143 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 029 | Total loss: 1.445 | Reg loss: 0.030 | Tree loss: 1.445 | Accuracy: 0.457031 | 7.143 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 029 | Total loss: 1.436 | Reg loss: 0.030 | Tree loss: 1.436 | Accuracy: 0.492188 | 7.143 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 029 | Total loss: 1.441 | Reg loss: 0.030 | Tree loss: 1.441 | Accuracy: 0.451172 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 029 | Total loss: 1.412 | Reg loss: 0.030 | Tree loss: 1.412 | Accuracy: 0.486328 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 029 | Total loss: 1.401 | Reg loss: 0.030 | Tree loss: 1.401 | Accuracy: 0.490234 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 029 | Total loss: 1.386 | Reg loss: 0.030 | Tree loss: 1.386 | Accuracy: 0.501953 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 029 | Total loss: 1.345 | Reg loss: 0.030 | Tree loss: 1.345 | Accuracy: 0.548828 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 029 | Total loss: 1.356 | Reg loss: 0.030 | Tree loss: 1.356 | Accuracy: 0.521484 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 029 | Total loss: 1.368 | Reg loss: 0.030 | Tree loss: 1.368 | Accuracy: 0.500000 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 029 | Total loss: 1.403 | Reg loss: 0.030 | Tree loss: 1.403 | Accuracy: 0.439453 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 029 | Total loss: 1.342 | Reg loss: 0.030 | Tree loss: 1.342 | Accuracy: 0.515625 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 029 | Total loss: 1.359 | Reg loss: 0.030 | Tree loss: 1.359 | Accuracy: 0.480469 | 7.144 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 029 | Total loss: 1.327 | Reg loss: 0.030 | Tree loss: 1.327 | Accuracy: 0.523438 | 7.143 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 029 | Total loss: 1.341 | Reg loss: 0.030 | Tree loss: 1.341 | Accuracy: 0.503906 | 7.143 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 029 | Total loss: 1.345 | Reg loss: 0.030 | Tree loss: 1.345 | Accuracy: 0.476562 | 7.143 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 029 | Total loss: 1.331 | Reg loss: 0.030 | Tree loss: 1.331 | Accuracy: 0.494141 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 029 | Total loss: 1.335 | Reg loss: 0.030 | Tree loss: 1.335 | Accuracy: 0.480469 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 029 | Total loss: 1.305 | Reg loss: 0.030 | Tree loss: 1.305 | Accuracy: 0.519531 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 029 | Total loss: 1.293 | Reg loss: 0.030 | Tree loss: 1.293 | Accuracy: 0.515625 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 029 | Total loss: 1.307 | Reg loss: 0.030 | Tree loss: 1.307 | Accuracy: 0.488281 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 029 | Total loss: 1.312 | Reg loss: 0.030 | Tree loss: 1.312 | Accuracy: 0.503906 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 029 | Total loss: 1.334 | Reg loss: 0.030 | Tree loss: 1.334 | Accuracy: 0.458984 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 029 | Total loss: 1.297 | Reg loss: 0.030 | Tree loss: 1.297 | Accuracy: 0.500000 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 029 | Total loss: 1.270 | Reg loss: 0.030 | Tree loss: 1.270 | Accuracy: 0.535156 | 7.142 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 029 | Total loss: 1.282 | Reg loss: 0.030 | Tree loss: 1.282 | Accuracy: 0.513672 | 7.141 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 029 | Total loss: 1.297 | Reg loss: 0.031 | Tree loss: 1.297 | Accuracy: 0.482422 | 7.14 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 029 | Total loss: 1.287 | Reg loss: 0.031 | Tree loss: 1.287 | Accuracy: 0.498047 | 7.139 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 029 | Total loss: 1.315 | Reg loss: 0.031 | Tree loss: 1.315 | Accuracy: 0.465483 | 7.138 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 029 | Total loss: 1.416 | Reg loss: 0.030 | Tree loss: 1.416 | Accuracy: 0.490234 | 7.144 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 029 | Total loss: 1.424 | Reg loss: 0.030 | Tree loss: 1.424 | Accuracy: 0.494141 | 7.144 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 029 | Total loss: 1.400 | Reg loss: 0.030 | Tree loss: 1.400 | Accuracy: 0.505859 | 7.144 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 029 | Total loss: 1.440 | Reg loss: 0.030 | Tree loss: 1.440 | Accuracy: 0.462891 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 029 | Total loss: 1.406 | Reg loss: 0.030 | Tree loss: 1.406 | Accuracy: 0.494141 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 029 | Total loss: 1.368 | Reg loss: 0.030 | Tree loss: 1.368 | Accuracy: 0.509766 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 029 | Total loss: 1.406 | Reg loss: 0.030 | Tree loss: 1.406 | Accuracy: 0.457031 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 029 | Total loss: 1.406 | Reg loss: 0.030 | Tree loss: 1.406 | Accuracy: 0.468750 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 029 | Total loss: 1.375 | Reg loss: 0.030 | Tree loss: 1.375 | Accuracy: 0.490234 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 029 | Total loss: 1.355 | Reg loss: 0.030 | Tree loss: 1.355 | Accuracy: 0.509766 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 029 | Total loss: 1.385 | Reg loss: 0.030 | Tree loss: 1.385 | Accuracy: 0.474609 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 029 | Total loss: 1.343 | Reg loss: 0.030 | Tree loss: 1.343 | Accuracy: 0.492188 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 029 | Total loss: 1.363 | Reg loss: 0.030 | Tree loss: 1.363 | Accuracy: 0.464844 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 029 | Total loss: 1.346 | Reg loss: 0.030 | Tree loss: 1.346 | Accuracy: 0.498047 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 029 | Total loss: 1.369 | Reg loss: 0.030 | Tree loss: 1.369 | Accuracy: 0.457031 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 029 | Total loss: 1.331 | Reg loss: 0.030 | Tree loss: 1.331 | Accuracy: 0.490234 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 029 | Total loss: 1.311 | Reg loss: 0.030 | Tree loss: 1.311 | Accuracy: 0.492188 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 029 | Total loss: 1.325 | Reg loss: 0.030 | Tree loss: 1.325 | Accuracy: 0.492188 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 029 | Total loss: 1.294 | Reg loss: 0.030 | Tree loss: 1.294 | Accuracy: 0.511719 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 029 | Total loss: 1.281 | Reg loss: 0.030 | Tree loss: 1.281 | Accuracy: 0.525391 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 029 | Total loss: 1.295 | Reg loss: 0.030 | Tree loss: 1.295 | Accuracy: 0.515625 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 029 | Total loss: 1.287 | Reg loss: 0.030 | Tree loss: 1.287 | Accuracy: 0.535156 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 029 | Total loss: 1.305 | Reg loss: 0.030 | Tree loss: 1.305 | Accuracy: 0.478516 | 7.143 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 029 | Total loss: 1.295 | Reg loss: 0.030 | Tree loss: 1.295 | Accuracy: 0.501953 | 7.142 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 029 | Total loss: 1.281 | Reg loss: 0.030 | Tree loss: 1.281 | Accuracy: 0.498047 | 7.141 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 029 | Total loss: 1.283 | Reg loss: 0.030 | Tree loss: 1.283 | Accuracy: 0.498047 | 7.14 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 029 | Total loss: 1.269 | Reg loss: 0.030 | Tree loss: 1.269 | Accuracy: 0.523438 | 7.14 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 029 | Total loss: 1.273 | Reg loss: 0.030 | Tree loss: 1.273 | Accuracy: 0.513672 | 7.139 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 029 | Total loss: 1.280 | Reg loss: 0.030 | Tree loss: 1.280 | Accuracy: 0.487179 | 7.138 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 029 | Total loss: 1.435 | Reg loss: 0.030 | Tree loss: 1.435 | Accuracy: 0.474609 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 029 | Total loss: 1.386 | Reg loss: 0.030 | Tree loss: 1.386 | Accuracy: 0.525391 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 029 | Total loss: 1.413 | Reg loss: 0.030 | Tree loss: 1.413 | Accuracy: 0.501953 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 029 | Total loss: 1.413 | Reg loss: 0.030 | Tree loss: 1.413 | Accuracy: 0.466797 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 029 | Total loss: 1.393 | Reg loss: 0.030 | Tree loss: 1.393 | Accuracy: 0.511719 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 029 | Total loss: 1.368 | Reg loss: 0.030 | Tree loss: 1.368 | Accuracy: 0.509766 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 029 | Total loss: 1.338 | Reg loss: 0.030 | Tree loss: 1.338 | Accuracy: 0.537109 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 029 | Total loss: 1.377 | Reg loss: 0.030 | Tree loss: 1.377 | Accuracy: 0.482422 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 029 | Total loss: 1.382 | Reg loss: 0.030 | Tree loss: 1.382 | Accuracy: 0.484375 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 029 | Total loss: 1.342 | Reg loss: 0.030 | Tree loss: 1.342 | Accuracy: 0.500000 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 029 | Total loss: 1.354 | Reg loss: 0.030 | Tree loss: 1.354 | Accuracy: 0.486328 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 029 | Total loss: 1.359 | Reg loss: 0.030 | Tree loss: 1.359 | Accuracy: 0.478516 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 029 | Total loss: 1.397 | Reg loss: 0.030 | Tree loss: 1.397 | Accuracy: 0.433594 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 029 | Total loss: 1.304 | Reg loss: 0.030 | Tree loss: 1.304 | Accuracy: 0.527344 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 029 | Total loss: 1.358 | Reg loss: 0.030 | Tree loss: 1.358 | Accuracy: 0.464844 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 029 | Total loss: 1.311 | Reg loss: 0.030 | Tree loss: 1.311 | Accuracy: 0.509766 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 029 | Total loss: 1.319 | Reg loss: 0.030 | Tree loss: 1.319 | Accuracy: 0.488281 | 7.143 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 029 | Total loss: 1.287 | Reg loss: 0.030 | Tree loss: 1.287 | Accuracy: 0.507812 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 029 | Total loss: 1.330 | Reg loss: 0.030 | Tree loss: 1.330 | Accuracy: 0.460938 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 029 | Total loss: 1.326 | Reg loss: 0.030 | Tree loss: 1.326 | Accuracy: 0.472656 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 029 | Total loss: 1.277 | Reg loss: 0.030 | Tree loss: 1.277 | Accuracy: 0.519531 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 029 | Total loss: 1.305 | Reg loss: 0.030 | Tree loss: 1.305 | Accuracy: 0.486328 | 7.142 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 029 | Total loss: 1.300 | Reg loss: 0.030 | Tree loss: 1.300 | Accuracy: 0.480469 | 7.141 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 029 | Total loss: 1.286 | Reg loss: 0.030 | Tree loss: 1.286 | Accuracy: 0.507812 | 7.14 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 029 | Total loss: 1.290 | Reg loss: 0.030 | Tree loss: 1.290 | Accuracy: 0.490234 | 7.139 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 029 | Total loss: 1.259 | Reg loss: 0.030 | Tree loss: 1.259 | Accuracy: 0.517578 | 7.138 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 029 | Total loss: 1.272 | Reg loss: 0.030 | Tree loss: 1.272 | Accuracy: 0.505859 | 7.137 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 029 | Total loss: 1.255 | Reg loss: 0.030 | Tree loss: 1.255 | Accuracy: 0.515625 | 7.135 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 029 | Total loss: 1.281 | Reg loss: 0.030 | Tree loss: 1.281 | Accuracy: 0.485207 | 7.135 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 029 | Total loss: 1.415 | Reg loss: 0.030 | Tree loss: 1.415 | Accuracy: 0.490234 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 029 | Total loss: 1.436 | Reg loss: 0.030 | Tree loss: 1.436 | Accuracy: 0.474609 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 029 | Total loss: 1.401 | Reg loss: 0.030 | Tree loss: 1.401 | Accuracy: 0.470703 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 029 | Total loss: 1.391 | Reg loss: 0.030 | Tree loss: 1.391 | Accuracy: 0.486328 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 029 | Total loss: 1.352 | Reg loss: 0.030 | Tree loss: 1.352 | Accuracy: 0.531250 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 029 | Total loss: 1.370 | Reg loss: 0.030 | Tree loss: 1.370 | Accuracy: 0.490234 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 029 | Total loss: 1.363 | Reg loss: 0.030 | Tree loss: 1.363 | Accuracy: 0.505859 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 029 | Total loss: 1.384 | Reg loss: 0.030 | Tree loss: 1.384 | Accuracy: 0.484375 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 029 | Total loss: 1.370 | Reg loss: 0.030 | Tree loss: 1.370 | Accuracy: 0.474609 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 029 | Total loss: 1.353 | Reg loss: 0.030 | Tree loss: 1.353 | Accuracy: 0.494141 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 029 | Total loss: 1.359 | Reg loss: 0.030 | Tree loss: 1.359 | Accuracy: 0.478516 | 7.137 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 029 | Total loss: 1.355 | Reg loss: 0.030 | Tree loss: 1.355 | Accuracy: 0.482422 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 029 | Total loss: 1.345 | Reg loss: 0.030 | Tree loss: 1.345 | Accuracy: 0.480469 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 029 | Total loss: 1.329 | Reg loss: 0.030 | Tree loss: 1.329 | Accuracy: 0.488281 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 029 | Total loss: 1.305 | Reg loss: 0.030 | Tree loss: 1.305 | Accuracy: 0.511719 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 029 | Total loss: 1.313 | Reg loss: 0.030 | Tree loss: 1.313 | Accuracy: 0.496094 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 029 | Total loss: 1.325 | Reg loss: 0.030 | Tree loss: 1.325 | Accuracy: 0.482422 | 7.135 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 029 | Total loss: 1.303 | Reg loss: 0.030 | Tree loss: 1.303 | Accuracy: 0.496094 | 7.135 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 029 | Total loss: 1.301 | Reg loss: 0.030 | Tree loss: 1.301 | Accuracy: 0.488281 | 7.135 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 029 | Total loss: 1.290 | Reg loss: 0.030 | Tree loss: 1.290 | Accuracy: 0.513672 | 7.135 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 029 | Total loss: 1.308 | Reg loss: 0.030 | Tree loss: 1.308 | Accuracy: 0.480469 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 029 | Total loss: 1.275 | Reg loss: 0.030 | Tree loss: 1.275 | Accuracy: 0.505859 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 029 | Total loss: 1.315 | Reg loss: 0.030 | Tree loss: 1.315 | Accuracy: 0.449219 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 029 | Total loss: 1.293 | Reg loss: 0.030 | Tree loss: 1.293 | Accuracy: 0.490234 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 029 | Total loss: 1.267 | Reg loss: 0.030 | Tree loss: 1.267 | Accuracy: 0.507812 | 7.136 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 029 | Total loss: 1.240 | Reg loss: 0.030 | Tree loss: 1.240 | Accuracy: 0.546875 | 7.135 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 029 | Total loss: 1.264 | Reg loss: 0.030 | Tree loss: 1.264 | Accuracy: 0.500000 | 7.134 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 029 | Total loss: 1.243 | Reg loss: 0.030 | Tree loss: 1.243 | Accuracy: 0.537109 | 7.133 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 029 | Total loss: 1.265 | Reg loss: 0.030 | Tree loss: 1.265 | Accuracy: 0.495069 | 7.133 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 029 | Total loss: 1.378 | Reg loss: 0.030 | Tree loss: 1.378 | Accuracy: 0.521484 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 029 | Total loss: 1.369 | Reg loss: 0.030 | Tree loss: 1.369 | Accuracy: 0.539062 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 029 | Total loss: 1.384 | Reg loss: 0.030 | Tree loss: 1.384 | Accuracy: 0.507812 | 7.135 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 003 / 029 | Total loss: 1.390 | Reg loss: 0.030 | Tree loss: 1.390 | Accuracy: 0.490234 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 029 | Total loss: 1.394 | Reg loss: 0.030 | Tree loss: 1.394 | Accuracy: 0.464844 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 029 | Total loss: 1.400 | Reg loss: 0.030 | Tree loss: 1.400 | Accuracy: 0.451172 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 029 | Total loss: 1.364 | Reg loss: 0.030 | Tree loss: 1.364 | Accuracy: 0.501953 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 029 | Total loss: 1.345 | Reg loss: 0.030 | Tree loss: 1.345 | Accuracy: 0.509766 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 029 | Total loss: 1.328 | Reg loss: 0.030 | Tree loss: 1.328 | Accuracy: 0.535156 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 029 | Total loss: 1.314 | Reg loss: 0.030 | Tree loss: 1.314 | Accuracy: 0.515625 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 029 | Total loss: 1.334 | Reg loss: 0.030 | Tree loss: 1.334 | Accuracy: 0.505859 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 029 | Total loss: 1.334 | Reg loss: 0.030 | Tree loss: 1.334 | Accuracy: 0.486328 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 029 | Total loss: 1.350 | Reg loss: 0.030 | Tree loss: 1.350 | Accuracy: 0.451172 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 029 | Total loss: 1.323 | Reg loss: 0.030 | Tree loss: 1.323 | Accuracy: 0.478516 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 029 | Total loss: 1.313 | Reg loss: 0.030 | Tree loss: 1.313 | Accuracy: 0.494141 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 029 | Total loss: 1.328 | Reg loss: 0.030 | Tree loss: 1.328 | Accuracy: 0.472656 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 029 | Total loss: 1.324 | Reg loss: 0.030 | Tree loss: 1.324 | Accuracy: 0.472656 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 029 | Total loss: 1.306 | Reg loss: 0.030 | Tree loss: 1.306 | Accuracy: 0.478516 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 029 | Total loss: 1.288 | Reg loss: 0.030 | Tree loss: 1.288 | Accuracy: 0.501953 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 029 | Total loss: 1.290 | Reg loss: 0.030 | Tree loss: 1.290 | Accuracy: 0.507812 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 029 | Total loss: 1.303 | Reg loss: 0.030 | Tree loss: 1.303 | Accuracy: 0.480469 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 029 | Total loss: 1.290 | Reg loss: 0.030 | Tree loss: 1.290 | Accuracy: 0.484375 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 029 | Total loss: 1.286 | Reg loss: 0.030 | Tree loss: 1.286 | Accuracy: 0.494141 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 029 | Total loss: 1.276 | Reg loss: 0.030 | Tree loss: 1.276 | Accuracy: 0.496094 | 7.135 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 029 | Total loss: 1.300 | Reg loss: 0.030 | Tree loss: 1.300 | Accuracy: 0.472656 | 7.134 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 029 | Total loss: 1.234 | Reg loss: 0.030 | Tree loss: 1.234 | Accuracy: 0.531250 | 7.133 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 029 | Total loss: 1.266 | Reg loss: 0.030 | Tree loss: 1.266 | Accuracy: 0.507812 | 7.132 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 029 | Total loss: 1.283 | Reg loss: 0.030 | Tree loss: 1.283 | Accuracy: 0.480469 | 7.131 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 029 | Total loss: 1.256 | Reg loss: 0.030 | Tree loss: 1.256 | Accuracy: 0.499014 | 7.13 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 029 | Total loss: 1.398 | Reg loss: 0.030 | Tree loss: 1.398 | Accuracy: 0.496094 | 7.132 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 029 | Total loss: 1.403 | Reg loss: 0.030 | Tree loss: 1.403 | Accuracy: 0.492188 | 7.131 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 029 | Total loss: 1.386 | Reg loss: 0.030 | Tree loss: 1.386 | Accuracy: 0.490234 | 7.13 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 029 | Total loss: 1.378 | Reg loss: 0.030 | Tree loss: 1.378 | Accuracy: 0.500000 | 7.13 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 029 | Total loss: 1.356 | Reg loss: 0.030 | Tree loss: 1.356 | Accuracy: 0.507812 | 7.13 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 029 | Total loss: 1.371 | Reg loss: 0.030 | Tree loss: 1.371 | Accuracy: 0.490234 | 7.13 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 029 | Total loss: 1.375 | Reg loss: 0.030 | Tree loss: 1.375 | Accuracy: 0.480469 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 029 | Total loss: 1.332 | Reg loss: 0.030 | Tree loss: 1.332 | Accuracy: 0.513672 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 029 | Total loss: 1.334 | Reg loss: 0.030 | Tree loss: 1.334 | Accuracy: 0.523438 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 029 | Total loss: 1.357 | Reg loss: 0.030 | Tree loss: 1.357 | Accuracy: 0.451172 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 029 | Total loss: 1.337 | Reg loss: 0.030 | Tree loss: 1.337 | Accuracy: 0.496094 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 029 | Total loss: 1.309 | Reg loss: 0.030 | Tree loss: 1.309 | Accuracy: 0.507812 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 029 | Total loss: 1.304 | Reg loss: 0.030 | Tree loss: 1.304 | Accuracy: 0.507812 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 029 | Total loss: 1.289 | Reg loss: 0.030 | Tree loss: 1.289 | Accuracy: 0.535156 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 029 | Total loss: 1.308 | Reg loss: 0.030 | Tree loss: 1.308 | Accuracy: 0.496094 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 029 | Total loss: 1.297 | Reg loss: 0.030 | Tree loss: 1.297 | Accuracy: 0.515625 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 029 | Total loss: 1.302 | Reg loss: 0.030 | Tree loss: 1.302 | Accuracy: 0.490234 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 029 | Total loss: 1.290 | Reg loss: 0.030 | Tree loss: 1.290 | Accuracy: 0.494141 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 029 | Total loss: 1.311 | Reg loss: 0.030 | Tree loss: 1.311 | Accuracy: 0.466797 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 029 | Total loss: 1.318 | Reg loss: 0.030 | Tree loss: 1.318 | Accuracy: 0.457031 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 029 | Total loss: 1.277 | Reg loss: 0.030 | Tree loss: 1.277 | Accuracy: 0.500000 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 029 | Total loss: 1.247 | Reg loss: 0.030 | Tree loss: 1.247 | Accuracy: 0.529297 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 029 | Total loss: 1.260 | Reg loss: 0.030 | Tree loss: 1.260 | Accuracy: 0.500000 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 029 | Total loss: 1.259 | Reg loss: 0.030 | Tree loss: 1.259 | Accuracy: 0.511719 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 029 | Total loss: 1.277 | Reg loss: 0.030 | Tree loss: 1.277 | Accuracy: 0.490234 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 029 | Total loss: 1.299 | Reg loss: 0.030 | Tree loss: 1.299 | Accuracy: 0.451172 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 029 | Total loss: 1.267 | Reg loss: 0.030 | Tree loss: 1.267 | Accuracy: 0.500000 | 7.129 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 029 | Total loss: 1.269 | Reg loss: 0.030 | Tree loss: 1.269 | Accuracy: 0.482422 | 7.128 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 029 | Total loss: 1.277 | Reg loss: 0.030 | Tree loss: 1.277 | Accuracy: 0.455621 | 7.127 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 029 | Total loss: 1.409 | Reg loss: 0.030 | Tree loss: 1.409 | Accuracy: 0.478516 | 7.13 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 029 | Total loss: 1.388 | Reg loss: 0.030 | Tree loss: 1.388 | Accuracy: 0.474609 | 7.13 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 029 | Total loss: 1.386 | Reg loss: 0.030 | Tree loss: 1.386 | Accuracy: 0.501953 | 7.129 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 029 | Total loss: 1.354 | Reg loss: 0.030 | Tree loss: 1.354 | Accuracy: 0.539062 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 029 | Total loss: 1.375 | Reg loss: 0.030 | Tree loss: 1.375 | Accuracy: 0.494141 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 029 | Total loss: 1.365 | Reg loss: 0.030 | Tree loss: 1.365 | Accuracy: 0.500000 | 7.128 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 006 / 029 | Total loss: 1.375 | Reg loss: 0.030 | Tree loss: 1.375 | Accuracy: 0.464844 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 029 | Total loss: 1.367 | Reg loss: 0.030 | Tree loss: 1.367 | Accuracy: 0.480469 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 029 | Total loss: 1.297 | Reg loss: 0.030 | Tree loss: 1.297 | Accuracy: 0.527344 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 029 | Total loss: 1.343 | Reg loss: 0.030 | Tree loss: 1.343 | Accuracy: 0.482422 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 029 | Total loss: 1.310 | Reg loss: 0.030 | Tree loss: 1.310 | Accuracy: 0.521484 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 029 | Total loss: 1.341 | Reg loss: 0.030 | Tree loss: 1.341 | Accuracy: 0.462891 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 029 | Total loss: 1.328 | Reg loss: 0.030 | Tree loss: 1.328 | Accuracy: 0.470703 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 029 | Total loss: 1.302 | Reg loss: 0.030 | Tree loss: 1.302 | Accuracy: 0.492188 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 029 | Total loss: 1.306 | Reg loss: 0.030 | Tree loss: 1.306 | Accuracy: 0.490234 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 029 | Total loss: 1.289 | Reg loss: 0.030 | Tree loss: 1.289 | Accuracy: 0.503906 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 029 | Total loss: 1.289 | Reg loss: 0.030 | Tree loss: 1.289 | Accuracy: 0.484375 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 029 | Total loss: 1.308 | Reg loss: 0.030 | Tree loss: 1.308 | Accuracy: 0.476562 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 029 | Total loss: 1.268 | Reg loss: 0.030 | Tree loss: 1.268 | Accuracy: 0.505859 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 029 | Total loss: 1.265 | Reg loss: 0.030 | Tree loss: 1.265 | Accuracy: 0.509766 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 029 | Total loss: 1.260 | Reg loss: 0.030 | Tree loss: 1.260 | Accuracy: 0.515625 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 029 | Total loss: 1.287 | Reg loss: 0.030 | Tree loss: 1.287 | Accuracy: 0.482422 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 029 | Total loss: 1.254 | Reg loss: 0.030 | Tree loss: 1.254 | Accuracy: 0.507812 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 029 | Total loss: 1.243 | Reg loss: 0.030 | Tree loss: 1.243 | Accuracy: 0.533203 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 029 | Total loss: 1.251 | Reg loss: 0.030 | Tree loss: 1.251 | Accuracy: 0.503906 | 7.128 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 029 | Total loss: 1.292 | Reg loss: 0.030 | Tree loss: 1.292 | Accuracy: 0.464844 | 7.127 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 029 | Total loss: 1.266 | Reg loss: 0.030 | Tree loss: 1.266 | Accuracy: 0.488281 | 7.126 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 029 | Total loss: 1.265 | Reg loss: 0.030 | Tree loss: 1.265 | Accuracy: 0.478516 | 7.125 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 029 | Total loss: 1.242 | Reg loss: 0.030 | Tree loss: 1.242 | Accuracy: 0.497041 | 7.124 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 029 | Total loss: 1.415 | Reg loss: 0.030 | Tree loss: 1.415 | Accuracy: 0.478516 | 7.128 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 029 | Total loss: 1.381 | Reg loss: 0.030 | Tree loss: 1.381 | Accuracy: 0.498047 | 7.128 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 029 | Total loss: 1.402 | Reg loss: 0.030 | Tree loss: 1.402 | Accuracy: 0.466797 | 7.127 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 029 | Total loss: 1.381 | Reg loss: 0.030 | Tree loss: 1.381 | Accuracy: 0.501953 | 7.126 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 029 | Total loss: 1.346 | Reg loss: 0.030 | Tree loss: 1.346 | Accuracy: 0.505859 | 7.126 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 029 | Total loss: 1.391 | Reg loss: 0.030 | Tree loss: 1.391 | Accuracy: 0.445312 | 7.126 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 029 | Total loss: 1.346 | Reg loss: 0.030 | Tree loss: 1.346 | Accuracy: 0.513672 | 7.126 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 029 | Total loss: 1.328 | Reg loss: 0.030 | Tree loss: 1.328 | Accuracy: 0.498047 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 029 | Total loss: 1.307 | Reg loss: 0.030 | Tree loss: 1.307 | Accuracy: 0.515625 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 029 | Total loss: 1.327 | Reg loss: 0.030 | Tree loss: 1.327 | Accuracy: 0.494141 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 029 | Total loss: 1.317 | Reg loss: 0.030 | Tree loss: 1.317 | Accuracy: 0.492188 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 029 | Total loss: 1.331 | Reg loss: 0.030 | Tree loss: 1.331 | Accuracy: 0.484375 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 029 | Total loss: 1.319 | Reg loss: 0.030 | Tree loss: 1.319 | Accuracy: 0.470703 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 029 | Total loss: 1.350 | Reg loss: 0.030 | Tree loss: 1.350 | Accuracy: 0.433594 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 029 | Total loss: 1.261 | Reg loss: 0.030 | Tree loss: 1.261 | Accuracy: 0.531250 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 029 | Total loss: 1.247 | Reg loss: 0.030 | Tree loss: 1.247 | Accuracy: 0.548828 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 029 | Total loss: 1.283 | Reg loss: 0.030 | Tree loss: 1.283 | Accuracy: 0.503906 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 029 | Total loss: 1.291 | Reg loss: 0.030 | Tree loss: 1.291 | Accuracy: 0.484375 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 029 | Total loss: 1.267 | Reg loss: 0.030 | Tree loss: 1.267 | Accuracy: 0.505859 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 029 | Total loss: 1.255 | Reg loss: 0.030 | Tree loss: 1.255 | Accuracy: 0.533203 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 029 | Total loss: 1.283 | Reg loss: 0.030 | Tree loss: 1.283 | Accuracy: 0.482422 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 029 | Total loss: 1.260 | Reg loss: 0.030 | Tree loss: 1.260 | Accuracy: 0.501953 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 029 | Total loss: 1.267 | Reg loss: 0.030 | Tree loss: 1.267 | Accuracy: 0.486328 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 029 | Total loss: 1.251 | Reg loss: 0.030 | Tree loss: 1.251 | Accuracy: 0.490234 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 029 | Total loss: 1.250 | Reg loss: 0.030 | Tree loss: 1.250 | Accuracy: 0.501953 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 029 | Total loss: 1.259 | Reg loss: 0.030 | Tree loss: 1.259 | Accuracy: 0.492188 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 029 | Total loss: 1.255 | Reg loss: 0.030 | Tree loss: 1.255 | Accuracy: 0.496094 | 7.125 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 029 | Total loss: 1.252 | Reg loss: 0.030 | Tree loss: 1.252 | Accuracy: 0.496094 | 7.124 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 029 | Total loss: 1.261 | Reg loss: 0.030 | Tree loss: 1.261 | Accuracy: 0.479290 | 7.123 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 029 | Total loss: 1.388 | Reg loss: 0.030 | Tree loss: 1.388 | Accuracy: 0.492188 | 7.126 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 029 | Total loss: 1.382 | Reg loss: 0.030 | Tree loss: 1.382 | Accuracy: 0.505859 | 7.126 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 029 | Total loss: 1.360 | Reg loss: 0.030 | Tree loss: 1.360 | Accuracy: 0.501953 | 7.126 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 029 | Total loss: 1.329 | Reg loss: 0.030 | Tree loss: 1.329 | Accuracy: 0.544922 | 7.126 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 029 | Total loss: 1.346 | Reg loss: 0.030 | Tree loss: 1.346 | Accuracy: 0.517578 | 7.125 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 029 | Total loss: 1.364 | Reg loss: 0.030 | Tree loss: 1.364 | Accuracy: 0.472656 | 7.125 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 029 | Total loss: 1.346 | Reg loss: 0.030 | Tree loss: 1.346 | Accuracy: 0.503906 | 7.125 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 029 | Total loss: 1.330 | Reg loss: 0.030 | Tree loss: 1.330 | Accuracy: 0.494141 | 7.124 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 029 | Total loss: 1.337 | Reg loss: 0.030 | Tree loss: 1.337 | Accuracy: 0.476562 | 7.124 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 009 / 029 | Total loss: 1.343 | Reg loss: 0.030 | Tree loss: 1.343 | Accuracy: 0.466797 | 7.124 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 029 | Total loss: 1.330 | Reg loss: 0.030 | Tree loss: 1.330 | Accuracy: 0.458984 | 7.124 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 029 | Total loss: 1.305 | Reg loss: 0.030 | Tree loss: 1.305 | Accuracy: 0.492188 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 029 | Total loss: 1.273 | Reg loss: 0.030 | Tree loss: 1.273 | Accuracy: 0.511719 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 029 | Total loss: 1.323 | Reg loss: 0.030 | Tree loss: 1.323 | Accuracy: 0.458984 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 029 | Total loss: 1.296 | Reg loss: 0.030 | Tree loss: 1.296 | Accuracy: 0.494141 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 029 | Total loss: 1.277 | Reg loss: 0.030 | Tree loss: 1.277 | Accuracy: 0.507812 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 029 | Total loss: 1.260 | Reg loss: 0.030 | Tree loss: 1.260 | Accuracy: 0.519531 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 029 | Total loss: 1.306 | Reg loss: 0.030 | Tree loss: 1.306 | Accuracy: 0.455078 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 029 | Total loss: 1.294 | Reg loss: 0.030 | Tree loss: 1.294 | Accuracy: 0.464844 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 029 | Total loss: 1.264 | Reg loss: 0.030 | Tree loss: 1.264 | Accuracy: 0.503906 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 029 | Total loss: 1.292 | Reg loss: 0.030 | Tree loss: 1.292 | Accuracy: 0.464844 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 029 | Total loss: 1.273 | Reg loss: 0.030 | Tree loss: 1.273 | Accuracy: 0.496094 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 029 | Total loss: 1.250 | Reg loss: 0.030 | Tree loss: 1.250 | Accuracy: 0.515625 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 029 | Total loss: 1.247 | Reg loss: 0.030 | Tree loss: 1.247 | Accuracy: 0.507812 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 029 | Total loss: 1.251 | Reg loss: 0.030 | Tree loss: 1.251 | Accuracy: 0.494141 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 029 | Total loss: 1.206 | Reg loss: 0.030 | Tree loss: 1.206 | Accuracy: 0.548828 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 029 | Total loss: 1.278 | Reg loss: 0.030 | Tree loss: 1.278 | Accuracy: 0.458984 | 7.123 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 029 | Total loss: 1.238 | Reg loss: 0.030 | Tree loss: 1.238 | Accuracy: 0.507812 | 7.122 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 029 | Total loss: 1.244 | Reg loss: 0.030 | Tree loss: 1.244 | Accuracy: 0.495069 | 7.121 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 029 | Total loss: 1.402 | Reg loss: 0.030 | Tree loss: 1.402 | Accuracy: 0.488281 | 7.125 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 029 | Total loss: 1.368 | Reg loss: 0.030 | Tree loss: 1.368 | Accuracy: 0.500000 | 7.125 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 029 | Total loss: 1.375 | Reg loss: 0.030 | Tree loss: 1.375 | Accuracy: 0.492188 | 7.125 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 029 | Total loss: 1.382 | Reg loss: 0.030 | Tree loss: 1.382 | Accuracy: 0.441406 | 7.125 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 029 | Total loss: 1.366 | Reg loss: 0.030 | Tree loss: 1.366 | Accuracy: 0.498047 | 7.124 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 029 | Total loss: 1.370 | Reg loss: 0.030 | Tree loss: 1.370 | Accuracy: 0.470703 | 7.124 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 029 | Total loss: 1.363 | Reg loss: 0.030 | Tree loss: 1.363 | Accuracy: 0.464844 | 7.124 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 029 | Total loss: 1.323 | Reg loss: 0.030 | Tree loss: 1.323 | Accuracy: 0.503906 | 7.124 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 029 | Total loss: 1.339 | Reg loss: 0.030 | Tree loss: 1.339 | Accuracy: 0.494141 | 7.124 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 029 | Total loss: 1.336 | Reg loss: 0.030 | Tree loss: 1.336 | Accuracy: 0.464844 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 029 | Total loss: 1.286 | Reg loss: 0.030 | Tree loss: 1.286 | Accuracy: 0.529297 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 029 | Total loss: 1.290 | Reg loss: 0.030 | Tree loss: 1.290 | Accuracy: 0.519531 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 029 | Total loss: 1.281 | Reg loss: 0.030 | Tree loss: 1.281 | Accuracy: 0.515625 | 7.122 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 029 | Total loss: 1.293 | Reg loss: 0.030 | Tree loss: 1.293 | Accuracy: 0.488281 | 7.122 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 029 | Total loss: 1.275 | Reg loss: 0.030 | Tree loss: 1.275 | Accuracy: 0.509766 | 7.122 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 029 | Total loss: 1.278 | Reg loss: 0.030 | Tree loss: 1.278 | Accuracy: 0.505859 | 7.122 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 029 | Total loss: 1.256 | Reg loss: 0.030 | Tree loss: 1.256 | Accuracy: 0.511719 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 029 | Total loss: 1.308 | Reg loss: 0.030 | Tree loss: 1.308 | Accuracy: 0.441406 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 029 | Total loss: 1.283 | Reg loss: 0.030 | Tree loss: 1.283 | Accuracy: 0.468750 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 029 | Total loss: 1.249 | Reg loss: 0.030 | Tree loss: 1.249 | Accuracy: 0.521484 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 029 | Total loss: 1.260 | Reg loss: 0.030 | Tree loss: 1.260 | Accuracy: 0.503906 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 029 | Total loss: 1.256 | Reg loss: 0.030 | Tree loss: 1.256 | Accuracy: 0.486328 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 029 | Total loss: 1.264 | Reg loss: 0.030 | Tree loss: 1.264 | Accuracy: 0.476562 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 029 | Total loss: 1.260 | Reg loss: 0.030 | Tree loss: 1.260 | Accuracy: 0.486328 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 029 | Total loss: 1.234 | Reg loss: 0.030 | Tree loss: 1.234 | Accuracy: 0.509766 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 029 | Total loss: 1.224 | Reg loss: 0.030 | Tree loss: 1.224 | Accuracy: 0.523438 | 7.123 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 029 | Total loss: 1.231 | Reg loss: 0.030 | Tree loss: 1.231 | Accuracy: 0.515625 | 7.122 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 029 | Total loss: 1.231 | Reg loss: 0.030 | Tree loss: 1.231 | Accuracy: 0.503906 | 7.121 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 029 | Total loss: 1.226 | Reg loss: 0.030 | Tree loss: 1.226 | Accuracy: 0.497041 | 7.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 029 | Total loss: 1.372 | Reg loss: 0.029 | Tree loss: 1.372 | Accuracy: 0.478516 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 029 | Total loss: 1.367 | Reg loss: 0.029 | Tree loss: 1.367 | Accuracy: 0.513672 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 029 | Total loss: 1.374 | Reg loss: 0.029 | Tree loss: 1.374 | Accuracy: 0.486328 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 029 | Total loss: 1.337 | Reg loss: 0.029 | Tree loss: 1.337 | Accuracy: 0.503906 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 029 | Total loss: 1.332 | Reg loss: 0.029 | Tree loss: 1.332 | Accuracy: 0.503906 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 029 | Total loss: 1.352 | Reg loss: 0.029 | Tree loss: 1.352 | Accuracy: 0.472656 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 029 | Total loss: 1.335 | Reg loss: 0.029 | Tree loss: 1.335 | Accuracy: 0.498047 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 029 | Total loss: 1.323 | Reg loss: 0.029 | Tree loss: 1.323 | Accuracy: 0.501953 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 029 | Total loss: 1.311 | Reg loss: 0.030 | Tree loss: 1.311 | Accuracy: 0.501953 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 029 | Total loss: 1.302 | Reg loss: 0.030 | Tree loss: 1.302 | Accuracy: 0.519531 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 029 | Total loss: 1.331 | Reg loss: 0.030 | Tree loss: 1.331 | Accuracy: 0.457031 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 029 | Total loss: 1.357 | Reg loss: 0.030 | Tree loss: 1.357 | Accuracy: 0.437500 | 7.133 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 012 / 029 | Total loss: 1.311 | Reg loss: 0.030 | Tree loss: 1.311 | Accuracy: 0.480469 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 029 | Total loss: 1.268 | Reg loss: 0.030 | Tree loss: 1.268 | Accuracy: 0.523438 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 029 | Total loss: 1.301 | Reg loss: 0.030 | Tree loss: 1.301 | Accuracy: 0.482422 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 029 | Total loss: 1.290 | Reg loss: 0.030 | Tree loss: 1.290 | Accuracy: 0.476562 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 029 | Total loss: 1.264 | Reg loss: 0.030 | Tree loss: 1.264 | Accuracy: 0.507812 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 029 | Total loss: 1.265 | Reg loss: 0.030 | Tree loss: 1.265 | Accuracy: 0.496094 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 029 | Total loss: 1.280 | Reg loss: 0.030 | Tree loss: 1.280 | Accuracy: 0.486328 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 029 | Total loss: 1.277 | Reg loss: 0.030 | Tree loss: 1.277 | Accuracy: 0.480469 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 029 | Total loss: 1.226 | Reg loss: 0.030 | Tree loss: 1.226 | Accuracy: 0.531250 | 7.133 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 029 | Total loss: 1.220 | Reg loss: 0.030 | Tree loss: 1.220 | Accuracy: 0.533203 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 029 | Total loss: 1.259 | Reg loss: 0.030 | Tree loss: 1.259 | Accuracy: 0.486328 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 029 | Total loss: 1.236 | Reg loss: 0.030 | Tree loss: 1.236 | Accuracy: 0.509766 | 7.132 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 029 | Total loss: 1.249 | Reg loss: 0.030 | Tree loss: 1.249 | Accuracy: 0.474609 | 7.131 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 029 | Total loss: 1.243 | Reg loss: 0.030 | Tree loss: 1.243 | Accuracy: 0.496094 | 7.131 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 029 | Total loss: 1.227 | Reg loss: 0.030 | Tree loss: 1.227 | Accuracy: 0.505859 | 7.131 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 029 | Total loss: 1.239 | Reg loss: 0.030 | Tree loss: 1.239 | Accuracy: 0.496094 | 7.131 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 029 | Total loss: 1.234 | Reg loss: 0.030 | Tree loss: 1.234 | Accuracy: 0.491124 | 7.131 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 029 | Total loss: 1.363 | Reg loss: 0.029 | Tree loss: 1.363 | Accuracy: 0.501953 | 7.135 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 029 | Total loss: 1.347 | Reg loss: 0.029 | Tree loss: 1.347 | Accuracy: 0.500000 | 7.134 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 029 | Total loss: 1.395 | Reg loss: 0.029 | Tree loss: 1.395 | Accuracy: 0.441406 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 029 | Total loss: 1.363 | Reg loss: 0.029 | Tree loss: 1.363 | Accuracy: 0.488281 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 029 | Total loss: 1.331 | Reg loss: 0.029 | Tree loss: 1.331 | Accuracy: 0.515625 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 029 | Total loss: 1.339 | Reg loss: 0.029 | Tree loss: 1.339 | Accuracy: 0.500000 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 029 | Total loss: 1.339 | Reg loss: 0.029 | Tree loss: 1.339 | Accuracy: 0.492188 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 029 | Total loss: 1.307 | Reg loss: 0.029 | Tree loss: 1.307 | Accuracy: 0.517578 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 029 | Total loss: 1.286 | Reg loss: 0.029 | Tree loss: 1.286 | Accuracy: 0.521484 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 029 | Total loss: 1.304 | Reg loss: 0.029 | Tree loss: 1.304 | Accuracy: 0.494141 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 029 | Total loss: 1.284 | Reg loss: 0.029 | Tree loss: 1.284 | Accuracy: 0.517578 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 029 | Total loss: 1.316 | Reg loss: 0.030 | Tree loss: 1.316 | Accuracy: 0.484375 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 029 | Total loss: 1.277 | Reg loss: 0.030 | Tree loss: 1.277 | Accuracy: 0.521484 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 029 | Total loss: 1.264 | Reg loss: 0.030 | Tree loss: 1.264 | Accuracy: 0.515625 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 029 | Total loss: 1.268 | Reg loss: 0.030 | Tree loss: 1.268 | Accuracy: 0.513672 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 029 | Total loss: 1.267 | Reg loss: 0.030 | Tree loss: 1.267 | Accuracy: 0.498047 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 029 | Total loss: 1.269 | Reg loss: 0.030 | Tree loss: 1.269 | Accuracy: 0.507812 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 029 | Total loss: 1.295 | Reg loss: 0.030 | Tree loss: 1.295 | Accuracy: 0.464844 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 029 | Total loss: 1.314 | Reg loss: 0.030 | Tree loss: 1.314 | Accuracy: 0.425781 | 7.133 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 029 | Total loss: 1.239 | Reg loss: 0.030 | Tree loss: 1.239 | Accuracy: 0.533203 | 7.132 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 029 | Total loss: 1.252 | Reg loss: 0.030 | Tree loss: 1.252 | Accuracy: 0.488281 | 7.131 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 029 | Total loss: 1.254 | Reg loss: 0.030 | Tree loss: 1.254 | Accuracy: 0.484375 | 7.13 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 029 | Total loss: 1.252 | Reg loss: 0.030 | Tree loss: 1.252 | Accuracy: 0.484375 | 7.129 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 029 | Total loss: 1.245 | Reg loss: 0.030 | Tree loss: 1.245 | Accuracy: 0.486328 | 7.128 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 029 | Total loss: 1.249 | Reg loss: 0.030 | Tree loss: 1.249 | Accuracy: 0.480469 | 7.127 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 029 | Total loss: 1.231 | Reg loss: 0.030 | Tree loss: 1.231 | Accuracy: 0.507812 | 7.127 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 029 | Total loss: 1.236 | Reg loss: 0.030 | Tree loss: 1.236 | Accuracy: 0.474609 | 7.127 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 029 | Total loss: 1.233 | Reg loss: 0.030 | Tree loss: 1.233 | Accuracy: 0.478516 | 7.127 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 029 | Total loss: 1.233 | Reg loss: 0.030 | Tree loss: 1.233 | Accuracy: 0.493097 | 7.127 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 029 | Total loss: 1.346 | Reg loss: 0.029 | Tree loss: 1.346 | Accuracy: 0.519531 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 029 | Total loss: 1.370 | Reg loss: 0.029 | Tree loss: 1.370 | Accuracy: 0.496094 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 029 | Total loss: 1.350 | Reg loss: 0.029 | Tree loss: 1.350 | Accuracy: 0.505859 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 029 | Total loss: 1.331 | Reg loss: 0.029 | Tree loss: 1.331 | Accuracy: 0.513672 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 029 | Total loss: 1.338 | Reg loss: 0.029 | Tree loss: 1.338 | Accuracy: 0.503906 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 029 | Total loss: 1.334 | Reg loss: 0.029 | Tree loss: 1.334 | Accuracy: 0.494141 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 029 | Total loss: 1.328 | Reg loss: 0.029 | Tree loss: 1.328 | Accuracy: 0.474609 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 029 | Total loss: 1.284 | Reg loss: 0.029 | Tree loss: 1.284 | Accuracy: 0.513672 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 029 | Total loss: 1.288 | Reg loss: 0.029 | Tree loss: 1.288 | Accuracy: 0.521484 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 029 | Total loss: 1.292 | Reg loss: 0.029 | Tree loss: 1.292 | Accuracy: 0.519531 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 029 | Total loss: 1.311 | Reg loss: 0.029 | Tree loss: 1.311 | Accuracy: 0.472656 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 029 | Total loss: 1.261 | Reg loss: 0.029 | Tree loss: 1.261 | Accuracy: 0.537109 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 029 | Total loss: 1.304 | Reg loss: 0.029 | Tree loss: 1.304 | Accuracy: 0.462891 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 029 | Total loss: 1.299 | Reg loss: 0.029 | Tree loss: 1.299 | Accuracy: 0.457031 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 029 | Total loss: 1.298 | Reg loss: 0.030 | Tree loss: 1.298 | Accuracy: 0.468750 | 7.132 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 015 / 029 | Total loss: 1.253 | Reg loss: 0.030 | Tree loss: 1.253 | Accuracy: 0.519531 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 029 | Total loss: 1.249 | Reg loss: 0.030 | Tree loss: 1.249 | Accuracy: 0.529297 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 029 | Total loss: 1.262 | Reg loss: 0.030 | Tree loss: 1.262 | Accuracy: 0.496094 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 029 | Total loss: 1.261 | Reg loss: 0.030 | Tree loss: 1.261 | Accuracy: 0.490234 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 029 | Total loss: 1.249 | Reg loss: 0.030 | Tree loss: 1.249 | Accuracy: 0.486328 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 029 | Total loss: 1.281 | Reg loss: 0.030 | Tree loss: 1.281 | Accuracy: 0.458984 | 7.132 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 029 | Total loss: 1.260 | Reg loss: 0.030 | Tree loss: 1.260 | Accuracy: 0.484375 | 7.131 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 029 | Total loss: 1.242 | Reg loss: 0.030 | Tree loss: 1.242 | Accuracy: 0.498047 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 029 | Total loss: 1.231 | Reg loss: 0.030 | Tree loss: 1.231 | Accuracy: 0.509766 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 029 | Total loss: 1.273 | Reg loss: 0.030 | Tree loss: 1.273 | Accuracy: 0.437500 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 029 | Total loss: 1.243 | Reg loss: 0.030 | Tree loss: 1.243 | Accuracy: 0.484375 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 029 | Total loss: 1.214 | Reg loss: 0.030 | Tree loss: 1.214 | Accuracy: 0.503906 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 029 | Total loss: 1.234 | Reg loss: 0.030 | Tree loss: 1.234 | Accuracy: 0.490234 | 7.13 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 029 | Total loss: 1.247 | Reg loss: 0.030 | Tree loss: 1.247 | Accuracy: 0.483235 | 7.13 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 029 | Total loss: 1.376 | Reg loss: 0.029 | Tree loss: 1.376 | Accuracy: 0.466797 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 029 | Total loss: 1.354 | Reg loss: 0.029 | Tree loss: 1.354 | Accuracy: 0.472656 | 7.136 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 029 | Total loss: 1.329 | Reg loss: 0.029 | Tree loss: 1.329 | Accuracy: 0.519531 | 7.136 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 029 | Total loss: 1.349 | Reg loss: 0.029 | Tree loss: 1.349 | Accuracy: 0.488281 | 7.136 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 029 | Total loss: 1.332 | Reg loss: 0.029 | Tree loss: 1.332 | Accuracy: 0.507812 | 7.136 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 029 | Total loss: 1.352 | Reg loss: 0.029 | Tree loss: 1.352 | Accuracy: 0.480469 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 029 | Total loss: 1.310 | Reg loss: 0.029 | Tree loss: 1.310 | Accuracy: 0.507812 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 029 | Total loss: 1.344 | Reg loss: 0.029 | Tree loss: 1.344 | Accuracy: 0.464844 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 029 | Total loss: 1.319 | Reg loss: 0.029 | Tree loss: 1.319 | Accuracy: 0.486328 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 029 | Total loss: 1.309 | Reg loss: 0.029 | Tree loss: 1.309 | Accuracy: 0.490234 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 029 | Total loss: 1.305 | Reg loss: 0.029 | Tree loss: 1.305 | Accuracy: 0.488281 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 029 | Total loss: 1.282 | Reg loss: 0.029 | Tree loss: 1.282 | Accuracy: 0.513672 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 029 | Total loss: 1.301 | Reg loss: 0.029 | Tree loss: 1.301 | Accuracy: 0.464844 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 029 | Total loss: 1.285 | Reg loss: 0.029 | Tree loss: 1.285 | Accuracy: 0.490234 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 029 | Total loss: 1.262 | Reg loss: 0.029 | Tree loss: 1.262 | Accuracy: 0.490234 | 7.137 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 029 | Total loss: 1.274 | Reg loss: 0.029 | Tree loss: 1.274 | Accuracy: 0.482422 | 7.136 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 029 | Total loss: 1.230 | Reg loss: 0.029 | Tree loss: 1.230 | Accuracy: 0.533203 | 7.136 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 029 | Total loss: 1.255 | Reg loss: 0.030 | Tree loss: 1.255 | Accuracy: 0.513672 | 7.136 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 029 | Total loss: 1.259 | Reg loss: 0.030 | Tree loss: 1.259 | Accuracy: 0.478516 | 7.135 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 029 | Total loss: 1.235 | Reg loss: 0.030 | Tree loss: 1.235 | Accuracy: 0.509766 | 7.134 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 029 | Total loss: 1.249 | Reg loss: 0.030 | Tree loss: 1.249 | Accuracy: 0.486328 | 7.133 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 029 | Total loss: 1.233 | Reg loss: 0.030 | Tree loss: 1.233 | Accuracy: 0.513672 | 7.132 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 029 | Total loss: 1.234 | Reg loss: 0.030 | Tree loss: 1.234 | Accuracy: 0.492188 | 7.131 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 029 | Total loss: 1.221 | Reg loss: 0.030 | Tree loss: 1.221 | Accuracy: 0.517578 | 7.13 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 029 | Total loss: 1.218 | Reg loss: 0.030 | Tree loss: 1.218 | Accuracy: 0.494141 | 7.129 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 029 | Total loss: 1.229 | Reg loss: 0.030 | Tree loss: 1.229 | Accuracy: 0.488281 | 7.129 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 029 | Total loss: 1.212 | Reg loss: 0.030 | Tree loss: 1.212 | Accuracy: 0.515625 | 7.129 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 029 | Total loss: 1.219 | Reg loss: 0.030 | Tree loss: 1.219 | Accuracy: 0.507812 | 7.128 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 029 | Total loss: 1.242 | Reg loss: 0.030 | Tree loss: 1.242 | Accuracy: 0.467456 | 7.128 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 029 | Total loss: 1.335 | Reg loss: 0.029 | Tree loss: 1.335 | Accuracy: 0.537109 | 7.132 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 029 | Total loss: 1.365 | Reg loss: 0.029 | Tree loss: 1.365 | Accuracy: 0.474609 | 7.131 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 029 | Total loss: 1.342 | Reg loss: 0.029 | Tree loss: 1.342 | Accuracy: 0.488281 | 7.13 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 029 | Total loss: 1.358 | Reg loss: 0.029 | Tree loss: 1.358 | Accuracy: 0.460938 | 7.129 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 029 | Total loss: 1.317 | Reg loss: 0.029 | Tree loss: 1.317 | Accuracy: 0.521484 | 7.128 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 029 | Total loss: 1.323 | Reg loss: 0.029 | Tree loss: 1.323 | Accuracy: 0.486328 | 7.127 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 029 | Total loss: 1.333 | Reg loss: 0.029 | Tree loss: 1.333 | Accuracy: 0.494141 | 7.126 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 029 | Total loss: 1.307 | Reg loss: 0.029 | Tree loss: 1.307 | Accuracy: 0.503906 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 029 | Total loss: 1.290 | Reg loss: 0.029 | Tree loss: 1.290 | Accuracy: 0.523438 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 029 | Total loss: 1.292 | Reg loss: 0.029 | Tree loss: 1.292 | Accuracy: 0.496094 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 029 | Total loss: 1.287 | Reg loss: 0.029 | Tree loss: 1.287 | Accuracy: 0.500000 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 029 | Total loss: 1.251 | Reg loss: 0.029 | Tree loss: 1.251 | Accuracy: 0.517578 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 029 | Total loss: 1.288 | Reg loss: 0.029 | Tree loss: 1.288 | Accuracy: 0.486328 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 029 | Total loss: 1.300 | Reg loss: 0.029 | Tree loss: 1.300 | Accuracy: 0.451172 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 029 | Total loss: 1.275 | Reg loss: 0.029 | Tree loss: 1.275 | Accuracy: 0.484375 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 029 | Total loss: 1.259 | Reg loss: 0.029 | Tree loss: 1.259 | Accuracy: 0.500000 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 029 | Total loss: 1.257 | Reg loss: 0.029 | Tree loss: 1.257 | Accuracy: 0.501953 | 7.125 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 029 | Total loss: 1.278 | Reg loss: 0.029 | Tree loss: 1.278 | Accuracy: 0.462891 | 7.125 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 018 / 029 | Total loss: 1.277 | Reg loss: 0.029 | Tree loss: 1.277 | Accuracy: 0.457031 | 7.124 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 029 | Total loss: 1.252 | Reg loss: 0.030 | Tree loss: 1.252 | Accuracy: 0.478516 | 7.124 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 029 | Total loss: 1.234 | Reg loss: 0.030 | Tree loss: 1.234 | Accuracy: 0.505859 | 7.124 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 029 | Total loss: 1.241 | Reg loss: 0.030 | Tree loss: 1.241 | Accuracy: 0.490234 | 7.124 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 029 | Total loss: 1.228 | Reg loss: 0.030 | Tree loss: 1.228 | Accuracy: 0.515625 | 7.123 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 029 | Total loss: 1.217 | Reg loss: 0.030 | Tree loss: 1.217 | Accuracy: 0.505859 | 7.122 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 029 | Total loss: 1.214 | Reg loss: 0.030 | Tree loss: 1.214 | Accuracy: 0.519531 | 7.121 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 029 | Total loss: 1.239 | Reg loss: 0.030 | Tree loss: 1.239 | Accuracy: 0.480469 | 7.12 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 029 | Total loss: 1.224 | Reg loss: 0.030 | Tree loss: 1.224 | Accuracy: 0.496094 | 7.119 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 029 | Total loss: 1.214 | Reg loss: 0.030 | Tree loss: 1.214 | Accuracy: 0.503906 | 7.118 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 029 | Total loss: 1.225 | Reg loss: 0.030 | Tree loss: 1.225 | Accuracy: 0.489152 | 7.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 029 | Total loss: 1.368 | Reg loss: 0.029 | Tree loss: 1.368 | Accuracy: 0.470703 | 7.12 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 029 | Total loss: 1.375 | Reg loss: 0.029 | Tree loss: 1.375 | Accuracy: 0.462891 | 7.121 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 029 | Total loss: 1.326 | Reg loss: 0.029 | Tree loss: 1.326 | Accuracy: 0.509766 | 7.121 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 029 | Total loss: 1.347 | Reg loss: 0.029 | Tree loss: 1.347 | Accuracy: 0.486328 | 7.121 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 029 | Total loss: 1.326 | Reg loss: 0.029 | Tree loss: 1.326 | Accuracy: 0.507812 | 7.121 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 029 | Total loss: 1.315 | Reg loss: 0.029 | Tree loss: 1.315 | Accuracy: 0.492188 | 7.121 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 029 | Total loss: 1.316 | Reg loss: 0.029 | Tree loss: 1.316 | Accuracy: 0.505859 | 7.12 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 029 | Total loss: 1.310 | Reg loss: 0.029 | Tree loss: 1.310 | Accuracy: 0.503906 | 7.12 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 029 | Total loss: 1.257 | Reg loss: 0.029 | Tree loss: 1.257 | Accuracy: 0.537109 | 7.119 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 029 | Total loss: 1.270 | Reg loss: 0.029 | Tree loss: 1.270 | Accuracy: 0.511719 | 7.118 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 029 | Total loss: 1.300 | Reg loss: 0.029 | Tree loss: 1.300 | Accuracy: 0.474609 | 7.117 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 029 | Total loss: 1.294 | Reg loss: 0.029 | Tree loss: 1.294 | Accuracy: 0.484375 | 7.116 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 029 | Total loss: 1.259 | Reg loss: 0.029 | Tree loss: 1.259 | Accuracy: 0.503906 | 7.115 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 029 | Total loss: 1.257 | Reg loss: 0.029 | Tree loss: 1.257 | Accuracy: 0.500000 | 7.114 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 029 | Total loss: 1.272 | Reg loss: 0.029 | Tree loss: 1.272 | Accuracy: 0.484375 | 7.113 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 029 | Total loss: 1.255 | Reg loss: 0.029 | Tree loss: 1.255 | Accuracy: 0.494141 | 7.112 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 029 | Total loss: 1.250 | Reg loss: 0.029 | Tree loss: 1.250 | Accuracy: 0.511719 | 7.111 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 029 | Total loss: 1.285 | Reg loss: 0.029 | Tree loss: 1.285 | Accuracy: 0.445312 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 029 | Total loss: 1.267 | Reg loss: 0.029 | Tree loss: 1.267 | Accuracy: 0.464844 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 029 | Total loss: 1.234 | Reg loss: 0.029 | Tree loss: 1.234 | Accuracy: 0.507812 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 029 | Total loss: 1.242 | Reg loss: 0.029 | Tree loss: 1.242 | Accuracy: 0.500000 | 7.109 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 029 | Total loss: 1.225 | Reg loss: 0.030 | Tree loss: 1.225 | Accuracy: 0.515625 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 029 | Total loss: 1.244 | Reg loss: 0.030 | Tree loss: 1.244 | Accuracy: 0.472656 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 029 | Total loss: 1.218 | Reg loss: 0.030 | Tree loss: 1.218 | Accuracy: 0.511719 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 029 | Total loss: 1.236 | Reg loss: 0.030 | Tree loss: 1.236 | Accuracy: 0.486328 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 029 | Total loss: 1.209 | Reg loss: 0.030 | Tree loss: 1.209 | Accuracy: 0.496094 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 029 | Total loss: 1.229 | Reg loss: 0.030 | Tree loss: 1.229 | Accuracy: 0.484375 | 7.11 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 029 | Total loss: 1.221 | Reg loss: 0.030 | Tree loss: 1.221 | Accuracy: 0.500000 | 7.109 sec/iter\n",
      "Epoch: 80 | Batch: 028 / 029 | Total loss: 1.206 | Reg loss: 0.030 | Tree loss: 1.206 | Accuracy: 0.506903 | 7.108 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 029 | Total loss: 1.345 | Reg loss: 0.029 | Tree loss: 1.345 | Accuracy: 0.500000 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 029 | Total loss: 1.372 | Reg loss: 0.029 | Tree loss: 1.372 | Accuracy: 0.447266 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 029 | Total loss: 1.331 | Reg loss: 0.029 | Tree loss: 1.331 | Accuracy: 0.498047 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 029 | Total loss: 1.345 | Reg loss: 0.029 | Tree loss: 1.345 | Accuracy: 0.478516 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 029 | Total loss: 1.321 | Reg loss: 0.029 | Tree loss: 1.321 | Accuracy: 0.488281 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 029 | Total loss: 1.286 | Reg loss: 0.029 | Tree loss: 1.286 | Accuracy: 0.546875 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 029 | Total loss: 1.313 | Reg loss: 0.029 | Tree loss: 1.313 | Accuracy: 0.492188 | 7.112 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 029 | Total loss: 1.304 | Reg loss: 0.029 | Tree loss: 1.304 | Accuracy: 0.494141 | 7.112 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 029 | Total loss: 1.303 | Reg loss: 0.029 | Tree loss: 1.303 | Accuracy: 0.494141 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 029 | Total loss: 1.312 | Reg loss: 0.029 | Tree loss: 1.312 | Accuracy: 0.457031 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 029 | Total loss: 1.295 | Reg loss: 0.029 | Tree loss: 1.295 | Accuracy: 0.462891 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 029 | Total loss: 1.272 | Reg loss: 0.029 | Tree loss: 1.272 | Accuracy: 0.500000 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 029 | Total loss: 1.274 | Reg loss: 0.029 | Tree loss: 1.274 | Accuracy: 0.496094 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 029 | Total loss: 1.256 | Reg loss: 0.029 | Tree loss: 1.256 | Accuracy: 0.503906 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 029 | Total loss: 1.313 | Reg loss: 0.029 | Tree loss: 1.313 | Accuracy: 0.433594 | 7.111 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 029 | Total loss: 1.235 | Reg loss: 0.029 | Tree loss: 1.235 | Accuracy: 0.531250 | 7.11 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.521484 | 7.109 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 029 | Total loss: 1.256 | Reg loss: 0.029 | Tree loss: 1.256 | Accuracy: 0.490234 | 7.108 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 029 | Total loss: 1.249 | Reg loss: 0.029 | Tree loss: 1.249 | Accuracy: 0.494141 | 7.108 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 029 | Total loss: 1.186 | Reg loss: 0.029 | Tree loss: 1.186 | Accuracy: 0.558594 | 7.106 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 029 | Total loss: 1.259 | Reg loss: 0.029 | Tree loss: 1.259 | Accuracy: 0.462891 | 7.105 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Batch: 021 / 029 | Total loss: 1.238 | Reg loss: 0.029 | Tree loss: 1.238 | Accuracy: 0.482422 | 7.105 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 029 | Total loss: 1.252 | Reg loss: 0.029 | Tree loss: 1.252 | Accuracy: 0.460938 | 7.104 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 029 | Total loss: 1.205 | Reg loss: 0.029 | Tree loss: 1.205 | Accuracy: 0.525391 | 7.103 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 029 | Total loss: 1.200 | Reg loss: 0.030 | Tree loss: 1.200 | Accuracy: 0.527344 | 7.103 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 029 | Total loss: 1.224 | Reg loss: 0.030 | Tree loss: 1.224 | Accuracy: 0.490234 | 7.103 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 029 | Total loss: 1.210 | Reg loss: 0.030 | Tree loss: 1.210 | Accuracy: 0.503906 | 7.103 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 029 | Total loss: 1.214 | Reg loss: 0.030 | Tree loss: 1.214 | Accuracy: 0.496094 | 7.102 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 029 | Total loss: 1.214 | Reg loss: 0.030 | Tree loss: 1.214 | Accuracy: 0.495069 | 7.101 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 029 | Total loss: 1.384 | Reg loss: 0.029 | Tree loss: 1.384 | Accuracy: 0.464844 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 029 | Total loss: 1.343 | Reg loss: 0.029 | Tree loss: 1.343 | Accuracy: 0.486328 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 029 | Total loss: 1.315 | Reg loss: 0.029 | Tree loss: 1.315 | Accuracy: 0.515625 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 029 | Total loss: 1.327 | Reg loss: 0.029 | Tree loss: 1.327 | Accuracy: 0.494141 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 029 | Total loss: 1.325 | Reg loss: 0.029 | Tree loss: 1.325 | Accuracy: 0.482422 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 029 | Total loss: 1.328 | Reg loss: 0.029 | Tree loss: 1.328 | Accuracy: 0.466797 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 029 | Total loss: 1.291 | Reg loss: 0.029 | Tree loss: 1.291 | Accuracy: 0.513672 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 029 | Total loss: 1.277 | Reg loss: 0.029 | Tree loss: 1.277 | Accuracy: 0.525391 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 029 | Total loss: 1.321 | Reg loss: 0.029 | Tree loss: 1.321 | Accuracy: 0.464844 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 029 | Total loss: 1.287 | Reg loss: 0.029 | Tree loss: 1.287 | Accuracy: 0.490234 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 029 | Total loss: 1.309 | Reg loss: 0.029 | Tree loss: 1.309 | Accuracy: 0.457031 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 029 | Total loss: 1.275 | Reg loss: 0.029 | Tree loss: 1.275 | Accuracy: 0.511719 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 029 | Total loss: 1.266 | Reg loss: 0.029 | Tree loss: 1.266 | Accuracy: 0.511719 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 029 | Total loss: 1.283 | Reg loss: 0.029 | Tree loss: 1.283 | Accuracy: 0.480469 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.517578 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 029 | Total loss: 1.263 | Reg loss: 0.029 | Tree loss: 1.263 | Accuracy: 0.486328 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.498047 | 7.105 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 029 | Total loss: 1.230 | Reg loss: 0.029 | Tree loss: 1.230 | Accuracy: 0.515625 | 7.104 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 029 | Total loss: 1.258 | Reg loss: 0.029 | Tree loss: 1.258 | Accuracy: 0.472656 | 7.104 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 029 | Total loss: 1.218 | Reg loss: 0.029 | Tree loss: 1.218 | Accuracy: 0.513672 | 7.104 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 029 | Total loss: 1.227 | Reg loss: 0.029 | Tree loss: 1.227 | Accuracy: 0.480469 | 7.104 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 029 | Total loss: 1.211 | Reg loss: 0.029 | Tree loss: 1.211 | Accuracy: 0.523438 | 7.104 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 029 | Total loss: 1.252 | Reg loss: 0.029 | Tree loss: 1.252 | Accuracy: 0.453125 | 7.104 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 029 | Total loss: 1.209 | Reg loss: 0.029 | Tree loss: 1.209 | Accuracy: 0.511719 | 7.103 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 029 | Total loss: 1.190 | Reg loss: 0.029 | Tree loss: 1.190 | Accuracy: 0.531250 | 7.102 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 029 | Total loss: 1.210 | Reg loss: 0.029 | Tree loss: 1.210 | Accuracy: 0.501953 | 7.101 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 029 | Total loss: 1.234 | Reg loss: 0.029 | Tree loss: 1.234 | Accuracy: 0.472656 | 7.1 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 029 | Total loss: 1.239 | Reg loss: 0.030 | Tree loss: 1.239 | Accuracy: 0.462891 | 7.1 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 029 | Total loss: 1.188 | Reg loss: 0.030 | Tree loss: 1.188 | Accuracy: 0.526627 | 7.099 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 029 | Total loss: 1.343 | Reg loss: 0.029 | Tree loss: 1.343 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 029 | Total loss: 1.329 | Reg loss: 0.029 | Tree loss: 1.329 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 029 | Total loss: 1.338 | Reg loss: 0.029 | Tree loss: 1.338 | Accuracy: 0.494141 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 029 | Total loss: 1.302 | Reg loss: 0.029 | Tree loss: 1.302 | Accuracy: 0.525391 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 029 | Total loss: 1.331 | Reg loss: 0.029 | Tree loss: 1.331 | Accuracy: 0.486328 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 029 | Total loss: 1.290 | Reg loss: 0.029 | Tree loss: 1.290 | Accuracy: 0.515625 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 029 | Total loss: 1.302 | Reg loss: 0.029 | Tree loss: 1.302 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 029 | Total loss: 1.308 | Reg loss: 0.029 | Tree loss: 1.308 | Accuracy: 0.482422 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 029 | Total loss: 1.297 | Reg loss: 0.029 | Tree loss: 1.297 | Accuracy: 0.472656 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 029 | Total loss: 1.322 | Reg loss: 0.029 | Tree loss: 1.322 | Accuracy: 0.433594 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 029 | Total loss: 1.268 | Reg loss: 0.029 | Tree loss: 1.268 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 029 | Total loss: 1.263 | Reg loss: 0.029 | Tree loss: 1.263 | Accuracy: 0.505859 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 029 | Total loss: 1.281 | Reg loss: 0.029 | Tree loss: 1.281 | Accuracy: 0.476562 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 029 | Total loss: 1.239 | Reg loss: 0.029 | Tree loss: 1.239 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 029 | Total loss: 1.248 | Reg loss: 0.029 | Tree loss: 1.248 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 029 | Total loss: 1.248 | Reg loss: 0.029 | Tree loss: 1.248 | Accuracy: 0.505859 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 029 | Total loss: 1.241 | Reg loss: 0.029 | Tree loss: 1.241 | Accuracy: 0.498047 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 029 | Total loss: 1.243 | Reg loss: 0.029 | Tree loss: 1.243 | Accuracy: 0.498047 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 029 | Total loss: 1.216 | Reg loss: 0.029 | Tree loss: 1.216 | Accuracy: 0.521484 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 029 | Total loss: 1.213 | Reg loss: 0.029 | Tree loss: 1.213 | Accuracy: 0.507812 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 029 | Total loss: 1.253 | Reg loss: 0.029 | Tree loss: 1.253 | Accuracy: 0.453125 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 029 | Total loss: 1.238 | Reg loss: 0.029 | Tree loss: 1.238 | Accuracy: 0.494141 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 029 | Total loss: 1.235 | Reg loss: 0.029 | Tree loss: 1.235 | Accuracy: 0.478516 | 7.102 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 029 | Total loss: 1.255 | Reg loss: 0.029 | Tree loss: 1.255 | Accuracy: 0.462891 | 7.102 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 024 / 029 | Total loss: 1.200 | Reg loss: 0.029 | Tree loss: 1.200 | Accuracy: 0.505859 | 7.101 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 029 | Total loss: 1.229 | Reg loss: 0.029 | Tree loss: 1.229 | Accuracy: 0.470703 | 7.1 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 029 | Total loss: 1.202 | Reg loss: 0.029 | Tree loss: 1.202 | Accuracy: 0.511719 | 7.099 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 029 | Total loss: 1.175 | Reg loss: 0.029 | Tree loss: 1.175 | Accuracy: 0.548828 | 7.099 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 029 | Total loss: 1.221 | Reg loss: 0.029 | Tree loss: 1.221 | Accuracy: 0.471400 | 7.099 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 029 | Total loss: 1.329 | Reg loss: 0.029 | Tree loss: 1.329 | Accuracy: 0.501953 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 029 | Total loss: 1.350 | Reg loss: 0.029 | Tree loss: 1.350 | Accuracy: 0.490234 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 029 | Total loss: 1.340 | Reg loss: 0.029 | Tree loss: 1.340 | Accuracy: 0.476562 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 029 | Total loss: 1.312 | Reg loss: 0.029 | Tree loss: 1.312 | Accuracy: 0.505859 | 7.109 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 029 | Total loss: 1.330 | Reg loss: 0.029 | Tree loss: 1.330 | Accuracy: 0.478516 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 029 | Total loss: 1.301 | Reg loss: 0.029 | Tree loss: 1.301 | Accuracy: 0.501953 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 029 | Total loss: 1.297 | Reg loss: 0.029 | Tree loss: 1.297 | Accuracy: 0.505859 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 029 | Total loss: 1.315 | Reg loss: 0.029 | Tree loss: 1.315 | Accuracy: 0.468750 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 029 | Total loss: 1.288 | Reg loss: 0.029 | Tree loss: 1.288 | Accuracy: 0.503906 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 029 | Total loss: 1.289 | Reg loss: 0.029 | Tree loss: 1.289 | Accuracy: 0.490234 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 029 | Total loss: 1.280 | Reg loss: 0.029 | Tree loss: 1.280 | Accuracy: 0.478516 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 029 | Total loss: 1.281 | Reg loss: 0.029 | Tree loss: 1.281 | Accuracy: 0.462891 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 029 | Total loss: 1.269 | Reg loss: 0.029 | Tree loss: 1.269 | Accuracy: 0.484375 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 029 | Total loss: 1.275 | Reg loss: 0.029 | Tree loss: 1.275 | Accuracy: 0.480469 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 029 | Total loss: 1.243 | Reg loss: 0.029 | Tree loss: 1.243 | Accuracy: 0.507812 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.509766 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 029 | Total loss: 1.236 | Reg loss: 0.029 | Tree loss: 1.236 | Accuracy: 0.486328 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 029 | Total loss: 1.255 | Reg loss: 0.029 | Tree loss: 1.255 | Accuracy: 0.470703 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 029 | Total loss: 1.233 | Reg loss: 0.029 | Tree loss: 1.233 | Accuracy: 0.484375 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 029 | Total loss: 1.220 | Reg loss: 0.029 | Tree loss: 1.220 | Accuracy: 0.507812 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 029 | Total loss: 1.206 | Reg loss: 0.029 | Tree loss: 1.206 | Accuracy: 0.523438 | 7.108 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 029 | Total loss: 1.209 | Reg loss: 0.029 | Tree loss: 1.209 | Accuracy: 0.509766 | 7.107 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 029 | Total loss: 1.216 | Reg loss: 0.029 | Tree loss: 1.216 | Accuracy: 0.513672 | 7.106 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 029 | Total loss: 1.214 | Reg loss: 0.029 | Tree loss: 1.214 | Accuracy: 0.501953 | 7.105 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 029 | Total loss: 1.201 | Reg loss: 0.029 | Tree loss: 1.201 | Accuracy: 0.500000 | 7.105 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 029 | Total loss: 1.221 | Reg loss: 0.029 | Tree loss: 1.221 | Accuracy: 0.490234 | 7.105 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 029 | Total loss: 1.201 | Reg loss: 0.029 | Tree loss: 1.201 | Accuracy: 0.501953 | 7.105 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 029 | Total loss: 1.199 | Reg loss: 0.029 | Tree loss: 1.199 | Accuracy: 0.498047 | 7.105 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 029 | Total loss: 1.206 | Reg loss: 0.029 | Tree loss: 1.206 | Accuracy: 0.497041 | 7.105 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 029 | Total loss: 1.365 | Reg loss: 0.029 | Tree loss: 1.365 | Accuracy: 0.494141 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 029 | Total loss: 1.356 | Reg loss: 0.029 | Tree loss: 1.356 | Accuracy: 0.476562 | 7.109 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 029 | Total loss: 1.325 | Reg loss: 0.029 | Tree loss: 1.325 | Accuracy: 0.507812 | 7.109 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 029 | Total loss: 1.315 | Reg loss: 0.029 | Tree loss: 1.315 | Accuracy: 0.501953 | 7.109 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 029 | Total loss: 1.295 | Reg loss: 0.029 | Tree loss: 1.295 | Accuracy: 0.529297 | 7.109 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 029 | Total loss: 1.323 | Reg loss: 0.029 | Tree loss: 1.323 | Accuracy: 0.466797 | 7.109 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 029 | Total loss: 1.285 | Reg loss: 0.029 | Tree loss: 1.285 | Accuracy: 0.509766 | 7.109 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 029 | Total loss: 1.281 | Reg loss: 0.029 | Tree loss: 1.281 | Accuracy: 0.482422 | 7.109 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 029 | Total loss: 1.288 | Reg loss: 0.029 | Tree loss: 1.288 | Accuracy: 0.492188 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 029 | Total loss: 1.281 | Reg loss: 0.029 | Tree loss: 1.281 | Accuracy: 0.486328 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 029 | Total loss: 1.274 | Reg loss: 0.029 | Tree loss: 1.274 | Accuracy: 0.482422 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 029 | Total loss: 1.274 | Reg loss: 0.029 | Tree loss: 1.274 | Accuracy: 0.476562 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 029 | Total loss: 1.240 | Reg loss: 0.029 | Tree loss: 1.240 | Accuracy: 0.519531 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 029 | Total loss: 1.249 | Reg loss: 0.029 | Tree loss: 1.249 | Accuracy: 0.511719 | 7.107 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 029 | Total loss: 1.245 | Reg loss: 0.029 | Tree loss: 1.245 | Accuracy: 0.496094 | 7.107 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 029 | Total loss: 1.263 | Reg loss: 0.029 | Tree loss: 1.263 | Accuracy: 0.472656 | 7.107 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 029 | Total loss: 1.215 | Reg loss: 0.029 | Tree loss: 1.215 | Accuracy: 0.511719 | 7.107 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 029 | Total loss: 1.226 | Reg loss: 0.029 | Tree loss: 1.226 | Accuracy: 0.507812 | 7.107 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 029 | Total loss: 1.249 | Reg loss: 0.029 | Tree loss: 1.249 | Accuracy: 0.458984 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 029 | Total loss: 1.244 | Reg loss: 0.029 | Tree loss: 1.244 | Accuracy: 0.462891 | 7.108 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 029 | Total loss: 1.248 | Reg loss: 0.029 | Tree loss: 1.248 | Accuracy: 0.455078 | 7.107 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 029 | Total loss: 1.216 | Reg loss: 0.029 | Tree loss: 1.216 | Accuracy: 0.500000 | 7.107 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 029 | Total loss: 1.227 | Reg loss: 0.029 | Tree loss: 1.227 | Accuracy: 0.494141 | 7.106 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 029 | Total loss: 1.188 | Reg loss: 0.029 | Tree loss: 1.188 | Accuracy: 0.527344 | 7.105 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 029 | Total loss: 1.189 | Reg loss: 0.029 | Tree loss: 1.189 | Accuracy: 0.523438 | 7.105 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 029 | Total loss: 1.223 | Reg loss: 0.029 | Tree loss: 1.223 | Accuracy: 0.478516 | 7.104 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 029 | Total loss: 1.195 | Reg loss: 0.029 | Tree loss: 1.195 | Accuracy: 0.505859 | 7.104 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 027 / 029 | Total loss: 1.179 | Reg loss: 0.029 | Tree loss: 1.179 | Accuracy: 0.527344 | 7.104 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 029 | Total loss: 1.215 | Reg loss: 0.029 | Tree loss: 1.215 | Accuracy: 0.473373 | 7.104 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 029 | Total loss: 1.367 | Reg loss: 0.029 | Tree loss: 1.367 | Accuracy: 0.464844 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 029 | Total loss: 1.322 | Reg loss: 0.029 | Tree loss: 1.322 | Accuracy: 0.492188 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 029 | Total loss: 1.306 | Reg loss: 0.029 | Tree loss: 1.306 | Accuracy: 0.511719 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 029 | Total loss: 1.326 | Reg loss: 0.029 | Tree loss: 1.326 | Accuracy: 0.470703 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 029 | Total loss: 1.328 | Reg loss: 0.029 | Tree loss: 1.328 | Accuracy: 0.462891 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 029 | Total loss: 1.339 | Reg loss: 0.029 | Tree loss: 1.339 | Accuracy: 0.455078 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 029 | Total loss: 1.283 | Reg loss: 0.029 | Tree loss: 1.283 | Accuracy: 0.515625 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 029 | Total loss: 1.301 | Reg loss: 0.029 | Tree loss: 1.301 | Accuracy: 0.472656 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 029 | Total loss: 1.301 | Reg loss: 0.029 | Tree loss: 1.301 | Accuracy: 0.492188 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 029 | Total loss: 1.307 | Reg loss: 0.029 | Tree loss: 1.307 | Accuracy: 0.464844 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 029 | Total loss: 1.249 | Reg loss: 0.029 | Tree loss: 1.249 | Accuracy: 0.519531 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 029 | Total loss: 1.308 | Reg loss: 0.029 | Tree loss: 1.308 | Accuracy: 0.443359 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 029 | Total loss: 1.278 | Reg loss: 0.029 | Tree loss: 1.278 | Accuracy: 0.466797 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 029 | Total loss: 1.268 | Reg loss: 0.029 | Tree loss: 1.268 | Accuracy: 0.480469 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 029 | Total loss: 1.223 | Reg loss: 0.029 | Tree loss: 1.223 | Accuracy: 0.517578 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 029 | Total loss: 1.221 | Reg loss: 0.029 | Tree loss: 1.221 | Accuracy: 0.523438 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 029 | Total loss: 1.225 | Reg loss: 0.029 | Tree loss: 1.225 | Accuracy: 0.507812 | 7.11 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 029 | Total loss: 1.229 | Reg loss: 0.029 | Tree loss: 1.229 | Accuracy: 0.509766 | 7.109 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 029 | Total loss: 1.213 | Reg loss: 0.029 | Tree loss: 1.213 | Accuracy: 0.513672 | 7.109 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 029 | Total loss: 1.212 | Reg loss: 0.029 | Tree loss: 1.212 | Accuracy: 0.503906 | 7.108 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 029 | Total loss: 1.220 | Reg loss: 0.029 | Tree loss: 1.220 | Accuracy: 0.498047 | 7.107 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.525391 | 7.106 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 029 | Total loss: 1.185 | Reg loss: 0.029 | Tree loss: 1.185 | Accuracy: 0.515625 | 7.106 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 029 | Total loss: 1.198 | Reg loss: 0.029 | Tree loss: 1.198 | Accuracy: 0.509766 | 7.106 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 029 | Total loss: 1.184 | Reg loss: 0.029 | Tree loss: 1.184 | Accuracy: 0.519531 | 7.105 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.505859 | 7.104 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 029 | Total loss: 1.208 | Reg loss: 0.029 | Tree loss: 1.208 | Accuracy: 0.470703 | 7.104 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 029 | Total loss: 1.209 | Reg loss: 0.029 | Tree loss: 1.209 | Accuracy: 0.480469 | 7.104 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 029 | Total loss: 1.179 | Reg loss: 0.029 | Tree loss: 1.179 | Accuracy: 0.518738 | 7.104 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 029 | Total loss: 1.335 | Reg loss: 0.029 | Tree loss: 1.335 | Accuracy: 0.494141 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 029 | Total loss: 1.324 | Reg loss: 0.029 | Tree loss: 1.324 | Accuracy: 0.507812 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 029 | Total loss: 1.301 | Reg loss: 0.029 | Tree loss: 1.301 | Accuracy: 0.519531 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 029 | Total loss: 1.332 | Reg loss: 0.029 | Tree loss: 1.332 | Accuracy: 0.478516 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 029 | Total loss: 1.315 | Reg loss: 0.029 | Tree loss: 1.315 | Accuracy: 0.462891 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 029 | Total loss: 1.311 | Reg loss: 0.029 | Tree loss: 1.311 | Accuracy: 0.494141 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 029 | Total loss: 1.282 | Reg loss: 0.029 | Tree loss: 1.282 | Accuracy: 0.519531 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 029 | Total loss: 1.306 | Reg loss: 0.029 | Tree loss: 1.306 | Accuracy: 0.460938 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 029 | Total loss: 1.300 | Reg loss: 0.029 | Tree loss: 1.300 | Accuracy: 0.474609 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 029 | Total loss: 1.260 | Reg loss: 0.029 | Tree loss: 1.260 | Accuracy: 0.517578 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 029 | Total loss: 1.272 | Reg loss: 0.029 | Tree loss: 1.272 | Accuracy: 0.492188 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 029 | Total loss: 1.263 | Reg loss: 0.029 | Tree loss: 1.263 | Accuracy: 0.484375 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 029 | Total loss: 1.243 | Reg loss: 0.029 | Tree loss: 1.243 | Accuracy: 0.517578 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 029 | Total loss: 1.241 | Reg loss: 0.029 | Tree loss: 1.241 | Accuracy: 0.509766 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 029 | Total loss: 1.244 | Reg loss: 0.029 | Tree loss: 1.244 | Accuracy: 0.490234 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 029 | Total loss: 1.264 | Reg loss: 0.029 | Tree loss: 1.264 | Accuracy: 0.447266 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 029 | Total loss: 1.205 | Reg loss: 0.029 | Tree loss: 1.205 | Accuracy: 0.531250 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 029 | Total loss: 1.225 | Reg loss: 0.029 | Tree loss: 1.225 | Accuracy: 0.507812 | 7.109 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 029 | Total loss: 1.228 | Reg loss: 0.029 | Tree loss: 1.228 | Accuracy: 0.492188 | 7.108 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.515625 | 7.107 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 029 | Total loss: 1.200 | Reg loss: 0.029 | Tree loss: 1.200 | Accuracy: 0.515625 | 7.107 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.523438 | 7.106 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 029 | Total loss: 1.185 | Reg loss: 0.029 | Tree loss: 1.185 | Accuracy: 0.509766 | 7.105 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 029 | Total loss: 1.203 | Reg loss: 0.029 | Tree loss: 1.203 | Accuracy: 0.488281 | 7.104 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 029 | Total loss: 1.237 | Reg loss: 0.029 | Tree loss: 1.237 | Accuracy: 0.457031 | 7.103 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 029 | Total loss: 1.192 | Reg loss: 0.029 | Tree loss: 1.192 | Accuracy: 0.500000 | 7.102 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 029 | Total loss: 1.219 | Reg loss: 0.029 | Tree loss: 1.219 | Accuracy: 0.458984 | 7.102 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 029 | Total loss: 1.208 | Reg loss: 0.029 | Tree loss: 1.208 | Accuracy: 0.492188 | 7.102 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 029 | Total loss: 1.219 | Reg loss: 0.029 | Tree loss: 1.219 | Accuracy: 0.469428 | 7.102 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 029 | Total loss: 1.312 | Reg loss: 0.029 | Tree loss: 1.312 | Accuracy: 0.521484 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 029 | Total loss: 1.371 | Reg loss: 0.029 | Tree loss: 1.371 | Accuracy: 0.464844 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 029 | Total loss: 1.311 | Reg loss: 0.029 | Tree loss: 1.311 | Accuracy: 0.500000 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 029 | Total loss: 1.283 | Reg loss: 0.029 | Tree loss: 1.283 | Accuracy: 0.525391 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 029 | Total loss: 1.311 | Reg loss: 0.029 | Tree loss: 1.311 | Accuracy: 0.472656 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 029 | Total loss: 1.326 | Reg loss: 0.029 | Tree loss: 1.326 | Accuracy: 0.470703 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 029 | Total loss: 1.298 | Reg loss: 0.029 | Tree loss: 1.298 | Accuracy: 0.480469 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 029 | Total loss: 1.262 | Reg loss: 0.029 | Tree loss: 1.262 | Accuracy: 0.525391 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 029 | Total loss: 1.307 | Reg loss: 0.029 | Tree loss: 1.307 | Accuracy: 0.466797 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 029 | Total loss: 1.251 | Reg loss: 0.029 | Tree loss: 1.251 | Accuracy: 0.509766 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 029 | Total loss: 1.260 | Reg loss: 0.029 | Tree loss: 1.260 | Accuracy: 0.486328 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 029 | Total loss: 1.268 | Reg loss: 0.029 | Tree loss: 1.268 | Accuracy: 0.476562 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 029 | Total loss: 1.240 | Reg loss: 0.029 | Tree loss: 1.240 | Accuracy: 0.509766 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 029 | Total loss: 1.258 | Reg loss: 0.029 | Tree loss: 1.258 | Accuracy: 0.484375 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 029 | Total loss: 1.226 | Reg loss: 0.029 | Tree loss: 1.226 | Accuracy: 0.521484 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 029 | Total loss: 1.247 | Reg loss: 0.029 | Tree loss: 1.247 | Accuracy: 0.480469 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 029 | Total loss: 1.228 | Reg loss: 0.029 | Tree loss: 1.228 | Accuracy: 0.496094 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 029 | Total loss: 1.262 | Reg loss: 0.029 | Tree loss: 1.262 | Accuracy: 0.451172 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 029 | Total loss: 1.226 | Reg loss: 0.029 | Tree loss: 1.226 | Accuracy: 0.505859 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 029 | Total loss: 1.216 | Reg loss: 0.029 | Tree loss: 1.216 | Accuracy: 0.498047 | 7.106 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.501953 | 7.105 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 029 | Total loss: 1.211 | Reg loss: 0.029 | Tree loss: 1.211 | Accuracy: 0.496094 | 7.105 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 029 | Total loss: 1.196 | Reg loss: 0.029 | Tree loss: 1.196 | Accuracy: 0.511719 | 7.104 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 029 | Total loss: 1.185 | Reg loss: 0.029 | Tree loss: 1.185 | Accuracy: 0.517578 | 7.103 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 029 | Total loss: 1.196 | Reg loss: 0.029 | Tree loss: 1.196 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 029 | Total loss: 1.211 | Reg loss: 0.029 | Tree loss: 1.211 | Accuracy: 0.458984 | 7.101 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 029 | Total loss: 1.202 | Reg loss: 0.029 | Tree loss: 1.202 | Accuracy: 0.490234 | 7.1 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 029 | Total loss: 1.187 | Reg loss: 0.029 | Tree loss: 1.187 | Accuracy: 0.513672 | 7.1 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 029 | Total loss: 1.188 | Reg loss: 0.029 | Tree loss: 1.188 | Accuracy: 0.493097 | 7.1 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 029 | Total loss: 1.365 | Reg loss: 0.029 | Tree loss: 1.365 | Accuracy: 0.447266 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 029 | Total loss: 1.303 | Reg loss: 0.029 | Tree loss: 1.303 | Accuracy: 0.511719 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 029 | Total loss: 1.392 | Reg loss: 0.029 | Tree loss: 1.392 | Accuracy: 0.431641 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 029 | Total loss: 1.320 | Reg loss: 0.029 | Tree loss: 1.320 | Accuracy: 0.486328 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 029 | Total loss: 1.318 | Reg loss: 0.029 | Tree loss: 1.318 | Accuracy: 0.470703 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 029 | Total loss: 1.269 | Reg loss: 0.029 | Tree loss: 1.269 | Accuracy: 0.527344 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 029 | Total loss: 1.319 | Reg loss: 0.029 | Tree loss: 1.319 | Accuracy: 0.472656 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 029 | Total loss: 1.262 | Reg loss: 0.029 | Tree loss: 1.262 | Accuracy: 0.507812 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 029 | Total loss: 1.260 | Reg loss: 0.029 | Tree loss: 1.260 | Accuracy: 0.517578 | 7.103 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 029 | Total loss: 1.287 | Reg loss: 0.029 | Tree loss: 1.287 | Accuracy: 0.482422 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 029 | Total loss: 1.267 | Reg loss: 0.029 | Tree loss: 1.267 | Accuracy: 0.484375 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 029 | Total loss: 1.246 | Reg loss: 0.029 | Tree loss: 1.246 | Accuracy: 0.501953 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 029 | Total loss: 1.229 | Reg loss: 0.029 | Tree loss: 1.229 | Accuracy: 0.515625 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 029 | Total loss: 1.236 | Reg loss: 0.029 | Tree loss: 1.236 | Accuracy: 0.498047 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 029 | Total loss: 1.217 | Reg loss: 0.029 | Tree loss: 1.217 | Accuracy: 0.511719 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 029 | Total loss: 1.228 | Reg loss: 0.029 | Tree loss: 1.228 | Accuracy: 0.503906 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 029 | Total loss: 1.234 | Reg loss: 0.029 | Tree loss: 1.234 | Accuracy: 0.468750 | 7.101 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 029 | Total loss: 1.234 | Reg loss: 0.029 | Tree loss: 1.234 | Accuracy: 0.484375 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 029 | Total loss: 1.201 | Reg loss: 0.029 | Tree loss: 1.201 | Accuracy: 0.505859 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 029 | Total loss: 1.200 | Reg loss: 0.029 | Tree loss: 1.200 | Accuracy: 0.523438 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 029 | Total loss: 1.220 | Reg loss: 0.029 | Tree loss: 1.220 | Accuracy: 0.482422 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 029 | Total loss: 1.215 | Reg loss: 0.029 | Tree loss: 1.215 | Accuracy: 0.482422 | 7.102 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 029 | Total loss: 1.197 | Reg loss: 0.029 | Tree loss: 1.197 | Accuracy: 0.507812 | 7.101 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 029 | Total loss: 1.213 | Reg loss: 0.029 | Tree loss: 1.213 | Accuracy: 0.482422 | 7.101 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.480469 | 7.1 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 029 | Total loss: 1.156 | Reg loss: 0.029 | Tree loss: 1.156 | Accuracy: 0.544922 | 7.1 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 029 | Total loss: 1.213 | Reg loss: 0.029 | Tree loss: 1.213 | Accuracy: 0.468750 | 7.1 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 029 | Total loss: 1.190 | Reg loss: 0.029 | Tree loss: 1.190 | Accuracy: 0.511719 | 7.1 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 029 | Total loss: 1.179 | Reg loss: 0.029 | Tree loss: 1.179 | Accuracy: 0.518738 | 7.099 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 029 | Total loss: 1.322 | Reg loss: 0.029 | Tree loss: 1.322 | Accuracy: 0.496094 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 029 | Total loss: 1.316 | Reg loss: 0.029 | Tree loss: 1.316 | Accuracy: 0.501953 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 029 | Total loss: 1.334 | Reg loss: 0.029 | Tree loss: 1.334 | Accuracy: 0.488281 | 7.103 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 003 / 029 | Total loss: 1.292 | Reg loss: 0.029 | Tree loss: 1.292 | Accuracy: 0.529297 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 029 | Total loss: 1.313 | Reg loss: 0.029 | Tree loss: 1.313 | Accuracy: 0.482422 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 029 | Total loss: 1.292 | Reg loss: 0.029 | Tree loss: 1.292 | Accuracy: 0.482422 | 7.104 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 029 | Total loss: 1.252 | Reg loss: 0.029 | Tree loss: 1.252 | Accuracy: 0.525391 | 7.104 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 029 | Total loss: 1.239 | Reg loss: 0.029 | Tree loss: 1.239 | Accuracy: 0.537109 | 7.104 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 029 | Total loss: 1.269 | Reg loss: 0.029 | Tree loss: 1.269 | Accuracy: 0.498047 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 029 | Total loss: 1.291 | Reg loss: 0.029 | Tree loss: 1.291 | Accuracy: 0.476562 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 029 | Total loss: 1.279 | Reg loss: 0.029 | Tree loss: 1.279 | Accuracy: 0.470703 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 029 | Total loss: 1.235 | Reg loss: 0.029 | Tree loss: 1.235 | Accuracy: 0.523438 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 029 | Total loss: 1.262 | Reg loss: 0.029 | Tree loss: 1.262 | Accuracy: 0.480469 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 029 | Total loss: 1.252 | Reg loss: 0.029 | Tree loss: 1.252 | Accuracy: 0.470703 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 029 | Total loss: 1.255 | Reg loss: 0.029 | Tree loss: 1.255 | Accuracy: 0.462891 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 029 | Total loss: 1.251 | Reg loss: 0.029 | Tree loss: 1.251 | Accuracy: 0.470703 | 7.103 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 029 | Total loss: 1.245 | Reg loss: 0.029 | Tree loss: 1.245 | Accuracy: 0.470703 | 7.102 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 029 | Total loss: 1.234 | Reg loss: 0.029 | Tree loss: 1.234 | Accuracy: 0.466797 | 7.102 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 029 | Total loss: 1.233 | Reg loss: 0.029 | Tree loss: 1.233 | Accuracy: 0.466797 | 7.102 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 029 | Total loss: 1.188 | Reg loss: 0.029 | Tree loss: 1.188 | Accuracy: 0.529297 | 7.102 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 029 | Total loss: 1.228 | Reg loss: 0.029 | Tree loss: 1.228 | Accuracy: 0.462891 | 7.102 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 029 | Total loss: 1.199 | Reg loss: 0.029 | Tree loss: 1.199 | Accuracy: 0.503906 | 7.101 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 029 | Total loss: 1.197 | Reg loss: 0.029 | Tree loss: 1.197 | Accuracy: 0.505859 | 7.101 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 029 | Total loss: 1.170 | Reg loss: 0.029 | Tree loss: 1.170 | Accuracy: 0.541016 | 7.1 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 029 | Total loss: 1.192 | Reg loss: 0.029 | Tree loss: 1.192 | Accuracy: 0.507812 | 7.099 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 029 | Total loss: 1.167 | Reg loss: 0.029 | Tree loss: 1.167 | Accuracy: 0.531250 | 7.098 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 029 | Total loss: 1.172 | Reg loss: 0.029 | Tree loss: 1.172 | Accuracy: 0.523438 | 7.097 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 029 | Total loss: 1.212 | Reg loss: 0.029 | Tree loss: 1.212 | Accuracy: 0.470703 | 7.096 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 029 | Total loss: 1.215 | Reg loss: 0.029 | Tree loss: 1.215 | Accuracy: 0.455621 | 7.096 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 029 | Total loss: 1.338 | Reg loss: 0.029 | Tree loss: 1.338 | Accuracy: 0.496094 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 029 | Total loss: 1.357 | Reg loss: 0.029 | Tree loss: 1.357 | Accuracy: 0.460938 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 029 | Total loss: 1.328 | Reg loss: 0.029 | Tree loss: 1.328 | Accuracy: 0.488281 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 029 | Total loss: 1.309 | Reg loss: 0.029 | Tree loss: 1.309 | Accuracy: 0.482422 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 029 | Total loss: 1.330 | Reg loss: 0.029 | Tree loss: 1.330 | Accuracy: 0.455078 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 029 | Total loss: 1.310 | Reg loss: 0.029 | Tree loss: 1.310 | Accuracy: 0.484375 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 029 | Total loss: 1.284 | Reg loss: 0.029 | Tree loss: 1.284 | Accuracy: 0.505859 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 029 | Total loss: 1.251 | Reg loss: 0.029 | Tree loss: 1.251 | Accuracy: 0.529297 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 029 | Total loss: 1.253 | Reg loss: 0.029 | Tree loss: 1.253 | Accuracy: 0.517578 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 029 | Total loss: 1.265 | Reg loss: 0.029 | Tree loss: 1.265 | Accuracy: 0.480469 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 029 | Total loss: 1.268 | Reg loss: 0.029 | Tree loss: 1.268 | Accuracy: 0.486328 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 029 | Total loss: 1.271 | Reg loss: 0.029 | Tree loss: 1.271 | Accuracy: 0.464844 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 029 | Total loss: 1.255 | Reg loss: 0.029 | Tree loss: 1.255 | Accuracy: 0.490234 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.501953 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 029 | Total loss: 1.247 | Reg loss: 0.029 | Tree loss: 1.247 | Accuracy: 0.484375 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 029 | Total loss: 1.220 | Reg loss: 0.029 | Tree loss: 1.220 | Accuracy: 0.500000 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 029 | Total loss: 1.225 | Reg loss: 0.029 | Tree loss: 1.225 | Accuracy: 0.472656 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.517578 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 029 | Total loss: 1.194 | Reg loss: 0.029 | Tree loss: 1.194 | Accuracy: 0.509766 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 029 | Total loss: 1.209 | Reg loss: 0.029 | Tree loss: 1.209 | Accuracy: 0.488281 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 029 | Total loss: 1.200 | Reg loss: 0.029 | Tree loss: 1.200 | Accuracy: 0.492188 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 029 | Total loss: 1.211 | Reg loss: 0.029 | Tree loss: 1.211 | Accuracy: 0.480469 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 029 | Total loss: 1.213 | Reg loss: 0.029 | Tree loss: 1.213 | Accuracy: 0.470703 | 7.1 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 029 | Total loss: 1.180 | Reg loss: 0.029 | Tree loss: 1.180 | Accuracy: 0.509766 | 7.099 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 029 | Total loss: 1.190 | Reg loss: 0.029 | Tree loss: 1.190 | Accuracy: 0.492188 | 7.099 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 029 | Total loss: 1.174 | Reg loss: 0.029 | Tree loss: 1.174 | Accuracy: 0.523438 | 7.098 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 029 | Total loss: 1.145 | Reg loss: 0.029 | Tree loss: 1.145 | Accuracy: 0.558594 | 7.097 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 029 | Total loss: 1.191 | Reg loss: 0.029 | Tree loss: 1.191 | Accuracy: 0.500000 | 7.097 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 029 | Total loss: 1.187 | Reg loss: 0.029 | Tree loss: 1.187 | Accuracy: 0.489152 | 7.096 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 029 | Total loss: 1.329 | Reg loss: 0.028 | Tree loss: 1.329 | Accuracy: 0.486328 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 029 | Total loss: 1.349 | Reg loss: 0.028 | Tree loss: 1.349 | Accuracy: 0.470703 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 029 | Total loss: 1.315 | Reg loss: 0.028 | Tree loss: 1.315 | Accuracy: 0.494141 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 029 | Total loss: 1.310 | Reg loss: 0.028 | Tree loss: 1.310 | Accuracy: 0.478516 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 029 | Total loss: 1.290 | Reg loss: 0.028 | Tree loss: 1.290 | Accuracy: 0.494141 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 029 | Total loss: 1.282 | Reg loss: 0.028 | Tree loss: 1.282 | Accuracy: 0.507812 | 7.099 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 006 / 029 | Total loss: 1.302 | Reg loss: 0.028 | Tree loss: 1.302 | Accuracy: 0.480469 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 029 | Total loss: 1.271 | Reg loss: 0.028 | Tree loss: 1.271 | Accuracy: 0.496094 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 029 | Total loss: 1.293 | Reg loss: 0.029 | Tree loss: 1.293 | Accuracy: 0.468750 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 029 | Total loss: 1.271 | Reg loss: 0.029 | Tree loss: 1.271 | Accuracy: 0.490234 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 029 | Total loss: 1.250 | Reg loss: 0.029 | Tree loss: 1.250 | Accuracy: 0.501953 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.513672 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 029 | Total loss: 1.221 | Reg loss: 0.029 | Tree loss: 1.221 | Accuracy: 0.515625 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 029 | Total loss: 1.250 | Reg loss: 0.029 | Tree loss: 1.250 | Accuracy: 0.470703 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 029 | Total loss: 1.205 | Reg loss: 0.029 | Tree loss: 1.205 | Accuracy: 0.537109 | 7.099 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 029 | Total loss: 1.261 | Reg loss: 0.029 | Tree loss: 1.261 | Accuracy: 0.439453 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 029 | Total loss: 1.206 | Reg loss: 0.029 | Tree loss: 1.206 | Accuracy: 0.515625 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 029 | Total loss: 1.200 | Reg loss: 0.029 | Tree loss: 1.200 | Accuracy: 0.515625 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 029 | Total loss: 1.199 | Reg loss: 0.029 | Tree loss: 1.199 | Accuracy: 0.515625 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 029 | Total loss: 1.185 | Reg loss: 0.029 | Tree loss: 1.185 | Accuracy: 0.511719 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 029 | Total loss: 1.212 | Reg loss: 0.029 | Tree loss: 1.212 | Accuracy: 0.486328 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 029 | Total loss: 1.207 | Reg loss: 0.029 | Tree loss: 1.207 | Accuracy: 0.474609 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 029 | Total loss: 1.218 | Reg loss: 0.029 | Tree loss: 1.218 | Accuracy: 0.476562 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.474609 | 7.098 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 029 | Total loss: 1.214 | Reg loss: 0.029 | Tree loss: 1.214 | Accuracy: 0.466797 | 7.097 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 029 | Total loss: 1.165 | Reg loss: 0.029 | Tree loss: 1.165 | Accuracy: 0.533203 | 7.096 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 029 | Total loss: 1.203 | Reg loss: 0.029 | Tree loss: 1.203 | Accuracy: 0.468750 | 7.095 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 029 | Total loss: 1.192 | Reg loss: 0.029 | Tree loss: 1.192 | Accuracy: 0.500000 | 7.095 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 029 | Total loss: 1.145 | Reg loss: 0.029 | Tree loss: 1.145 | Accuracy: 0.548323 | 7.094 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 029 | Total loss: 1.307 | Reg loss: 0.028 | Tree loss: 1.307 | Accuracy: 0.503906 | 7.095 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 029 | Total loss: 1.349 | Reg loss: 0.028 | Tree loss: 1.349 | Accuracy: 0.466797 | 7.095 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 029 | Total loss: 1.303 | Reg loss: 0.028 | Tree loss: 1.303 | Accuracy: 0.505859 | 7.095 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 029 | Total loss: 1.330 | Reg loss: 0.028 | Tree loss: 1.330 | Accuracy: 0.468750 | 7.095 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 029 | Total loss: 1.281 | Reg loss: 0.028 | Tree loss: 1.281 | Accuracy: 0.496094 | 7.095 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 029 | Total loss: 1.296 | Reg loss: 0.028 | Tree loss: 1.296 | Accuracy: 0.492188 | 7.095 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 029 | Total loss: 1.288 | Reg loss: 0.028 | Tree loss: 1.288 | Accuracy: 0.478516 | 7.095 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 029 | Total loss: 1.281 | Reg loss: 0.028 | Tree loss: 1.281 | Accuracy: 0.484375 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 029 | Total loss: 1.268 | Reg loss: 0.028 | Tree loss: 1.268 | Accuracy: 0.498047 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 029 | Total loss: 1.251 | Reg loss: 0.028 | Tree loss: 1.251 | Accuracy: 0.505859 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 029 | Total loss: 1.253 | Reg loss: 0.029 | Tree loss: 1.253 | Accuracy: 0.488281 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 029 | Total loss: 1.240 | Reg loss: 0.029 | Tree loss: 1.240 | Accuracy: 0.496094 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.501953 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 029 | Total loss: 1.236 | Reg loss: 0.029 | Tree loss: 1.236 | Accuracy: 0.482422 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 029 | Total loss: 1.216 | Reg loss: 0.029 | Tree loss: 1.216 | Accuracy: 0.509766 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 029 | Total loss: 1.223 | Reg loss: 0.029 | Tree loss: 1.223 | Accuracy: 0.492188 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 029 | Total loss: 1.217 | Reg loss: 0.029 | Tree loss: 1.217 | Accuracy: 0.496094 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 029 | Total loss: 1.215 | Reg loss: 0.029 | Tree loss: 1.215 | Accuracy: 0.496094 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 029 | Total loss: 1.221 | Reg loss: 0.029 | Tree loss: 1.221 | Accuracy: 0.472656 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 029 | Total loss: 1.200 | Reg loss: 0.029 | Tree loss: 1.200 | Accuracy: 0.496094 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 029 | Total loss: 1.210 | Reg loss: 0.029 | Tree loss: 1.210 | Accuracy: 0.478516 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 029 | Total loss: 1.221 | Reg loss: 0.029 | Tree loss: 1.221 | Accuracy: 0.462891 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 029 | Total loss: 1.192 | Reg loss: 0.029 | Tree loss: 1.192 | Accuracy: 0.511719 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 029 | Total loss: 1.184 | Reg loss: 0.029 | Tree loss: 1.184 | Accuracy: 0.517578 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 029 | Total loss: 1.180 | Reg loss: 0.029 | Tree loss: 1.180 | Accuracy: 0.509766 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 029 | Total loss: 1.169 | Reg loss: 0.029 | Tree loss: 1.169 | Accuracy: 0.533203 | 7.094 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 029 | Total loss: 1.180 | Reg loss: 0.029 | Tree loss: 1.180 | Accuracy: 0.498047 | 7.093 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 029 | Total loss: 1.181 | Reg loss: 0.029 | Tree loss: 1.181 | Accuracy: 0.505859 | 7.093 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 029 | Total loss: 1.190 | Reg loss: 0.029 | Tree loss: 1.190 | Accuracy: 0.483235 | 7.092 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 029 | Total loss: 1.279 | Reg loss: 0.028 | Tree loss: 1.279 | Accuracy: 0.560547 | 7.094 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 029 | Total loss: 1.293 | Reg loss: 0.028 | Tree loss: 1.293 | Accuracy: 0.527344 | 7.093 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 029 | Total loss: 1.330 | Reg loss: 0.028 | Tree loss: 1.330 | Accuracy: 0.464844 | 7.093 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 029 | Total loss: 1.285 | Reg loss: 0.028 | Tree loss: 1.285 | Accuracy: 0.511719 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 029 | Total loss: 1.270 | Reg loss: 0.028 | Tree loss: 1.270 | Accuracy: 0.521484 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 029 | Total loss: 1.305 | Reg loss: 0.028 | Tree loss: 1.305 | Accuracy: 0.460938 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 029 | Total loss: 1.274 | Reg loss: 0.028 | Tree loss: 1.274 | Accuracy: 0.492188 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 029 | Total loss: 1.282 | Reg loss: 0.028 | Tree loss: 1.282 | Accuracy: 0.478516 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 029 | Total loss: 1.291 | Reg loss: 0.028 | Tree loss: 1.291 | Accuracy: 0.457031 | 7.092 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 009 / 029 | Total loss: 1.249 | Reg loss: 0.028 | Tree loss: 1.249 | Accuracy: 0.509766 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 029 | Total loss: 1.262 | Reg loss: 0.028 | Tree loss: 1.262 | Accuracy: 0.474609 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 029 | Total loss: 1.238 | Reg loss: 0.028 | Tree loss: 1.238 | Accuracy: 0.498047 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 029 | Total loss: 1.255 | Reg loss: 0.028 | Tree loss: 1.255 | Accuracy: 0.484375 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 029 | Total loss: 1.245 | Reg loss: 0.029 | Tree loss: 1.245 | Accuracy: 0.476562 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 029 | Total loss: 1.226 | Reg loss: 0.029 | Tree loss: 1.226 | Accuracy: 0.511719 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 029 | Total loss: 1.259 | Reg loss: 0.029 | Tree loss: 1.259 | Accuracy: 0.451172 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 029 | Total loss: 1.195 | Reg loss: 0.029 | Tree loss: 1.195 | Accuracy: 0.525391 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 029 | Total loss: 1.213 | Reg loss: 0.029 | Tree loss: 1.213 | Accuracy: 0.484375 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 029 | Total loss: 1.211 | Reg loss: 0.029 | Tree loss: 1.211 | Accuracy: 0.498047 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 029 | Total loss: 1.166 | Reg loss: 0.029 | Tree loss: 1.166 | Accuracy: 0.544922 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 029 | Total loss: 1.206 | Reg loss: 0.029 | Tree loss: 1.206 | Accuracy: 0.474609 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 029 | Total loss: 1.206 | Reg loss: 0.029 | Tree loss: 1.206 | Accuracy: 0.476562 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 029 | Total loss: 1.198 | Reg loss: 0.029 | Tree loss: 1.198 | Accuracy: 0.490234 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 029 | Total loss: 1.168 | Reg loss: 0.029 | Tree loss: 1.168 | Accuracy: 0.529297 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 029 | Total loss: 1.187 | Reg loss: 0.029 | Tree loss: 1.187 | Accuracy: 0.492188 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 029 | Total loss: 1.192 | Reg loss: 0.029 | Tree loss: 1.192 | Accuracy: 0.484375 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 029 | Total loss: 1.183 | Reg loss: 0.029 | Tree loss: 1.183 | Accuracy: 0.500000 | 7.092 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 029 | Total loss: 1.207 | Reg loss: 0.029 | Tree loss: 1.207 | Accuracy: 0.468750 | 7.091 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 029 | Total loss: 1.181 | Reg loss: 0.029 | Tree loss: 1.181 | Accuracy: 0.483235 | 7.09 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 029 | Total loss: 1.352 | Reg loss: 0.028 | Tree loss: 1.352 | Accuracy: 0.443359 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 029 | Total loss: 1.342 | Reg loss: 0.028 | Tree loss: 1.342 | Accuracy: 0.462891 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 029 | Total loss: 1.301 | Reg loss: 0.028 | Tree loss: 1.301 | Accuracy: 0.501953 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 029 | Total loss: 1.324 | Reg loss: 0.028 | Tree loss: 1.324 | Accuracy: 0.474609 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 029 | Total loss: 1.271 | Reg loss: 0.028 | Tree loss: 1.271 | Accuracy: 0.529297 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 029 | Total loss: 1.261 | Reg loss: 0.028 | Tree loss: 1.261 | Accuracy: 0.523438 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 029 | Total loss: 1.250 | Reg loss: 0.028 | Tree loss: 1.250 | Accuracy: 0.517578 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 029 | Total loss: 1.272 | Reg loss: 0.028 | Tree loss: 1.272 | Accuracy: 0.482422 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 029 | Total loss: 1.256 | Reg loss: 0.028 | Tree loss: 1.256 | Accuracy: 0.511719 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 029 | Total loss: 1.267 | Reg loss: 0.028 | Tree loss: 1.267 | Accuracy: 0.478516 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 029 | Total loss: 1.242 | Reg loss: 0.028 | Tree loss: 1.242 | Accuracy: 0.501953 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 029 | Total loss: 1.216 | Reg loss: 0.028 | Tree loss: 1.216 | Accuracy: 0.513672 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 029 | Total loss: 1.216 | Reg loss: 0.028 | Tree loss: 1.216 | Accuracy: 0.500000 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 029 | Total loss: 1.224 | Reg loss: 0.028 | Tree loss: 1.224 | Accuracy: 0.503906 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 029 | Total loss: 1.192 | Reg loss: 0.028 | Tree loss: 1.192 | Accuracy: 0.548828 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 029 | Total loss: 1.214 | Reg loss: 0.029 | Tree loss: 1.214 | Accuracy: 0.490234 | 7.094 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 029 | Total loss: 1.238 | Reg loss: 0.029 | Tree loss: 1.238 | Accuracy: 0.457031 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 029 | Total loss: 1.250 | Reg loss: 0.029 | Tree loss: 1.250 | Accuracy: 0.457031 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 029 | Total loss: 1.234 | Reg loss: 0.029 | Tree loss: 1.234 | Accuracy: 0.457031 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 029 | Total loss: 1.224 | Reg loss: 0.029 | Tree loss: 1.224 | Accuracy: 0.470703 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 029 | Total loss: 1.206 | Reg loss: 0.029 | Tree loss: 1.206 | Accuracy: 0.484375 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 029 | Total loss: 1.190 | Reg loss: 0.029 | Tree loss: 1.190 | Accuracy: 0.509766 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 029 | Total loss: 1.197 | Reg loss: 0.029 | Tree loss: 1.197 | Accuracy: 0.494141 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 029 | Total loss: 1.165 | Reg loss: 0.029 | Tree loss: 1.165 | Accuracy: 0.531250 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 029 | Total loss: 1.180 | Reg loss: 0.029 | Tree loss: 1.180 | Accuracy: 0.501953 | 7.093 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.466797 | 7.092 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 029 | Total loss: 1.182 | Reg loss: 0.029 | Tree loss: 1.182 | Accuracy: 0.509766 | 7.091 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 029 | Total loss: 1.161 | Reg loss: 0.029 | Tree loss: 1.161 | Accuracy: 0.519531 | 7.091 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 029 | Total loss: 1.168 | Reg loss: 0.029 | Tree loss: 1.168 | Accuracy: 0.489152 | 7.09 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 029 | Total loss: 1.324 | Reg loss: 0.028 | Tree loss: 1.324 | Accuracy: 0.494141 | 7.09 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 029 | Total loss: 1.334 | Reg loss: 0.028 | Tree loss: 1.334 | Accuracy: 0.453125 | 7.089 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 029 | Total loss: 1.271 | Reg loss: 0.028 | Tree loss: 1.271 | Accuracy: 0.521484 | 7.088 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 029 | Total loss: 1.260 | Reg loss: 0.028 | Tree loss: 1.260 | Accuracy: 0.535156 | 7.087 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 029 | Total loss: 1.299 | Reg loss: 0.028 | Tree loss: 1.299 | Accuracy: 0.474609 | 7.087 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 029 | Total loss: 1.294 | Reg loss: 0.028 | Tree loss: 1.294 | Accuracy: 0.488281 | 7.086 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 029 | Total loss: 1.293 | Reg loss: 0.028 | Tree loss: 1.293 | Accuracy: 0.478516 | 7.085 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 029 | Total loss: 1.240 | Reg loss: 0.028 | Tree loss: 1.240 | Accuracy: 0.529297 | 7.084 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 029 | Total loss: 1.258 | Reg loss: 0.028 | Tree loss: 1.258 | Accuracy: 0.492188 | 7.084 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 029 | Total loss: 1.221 | Reg loss: 0.028 | Tree loss: 1.221 | Accuracy: 0.525391 | 7.083 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 029 | Total loss: 1.267 | Reg loss: 0.028 | Tree loss: 1.267 | Accuracy: 0.468750 | 7.082 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 029 | Total loss: 1.245 | Reg loss: 0.028 | Tree loss: 1.245 | Accuracy: 0.478516 | 7.081 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Batch: 012 / 029 | Total loss: 1.220 | Reg loss: 0.028 | Tree loss: 1.220 | Accuracy: 0.519531 | 7.08 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 029 | Total loss: 1.212 | Reg loss: 0.028 | Tree loss: 1.212 | Accuracy: 0.498047 | 7.079 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 029 | Total loss: 1.234 | Reg loss: 0.028 | Tree loss: 1.234 | Accuracy: 0.484375 | 7.079 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 029 | Total loss: 1.212 | Reg loss: 0.028 | Tree loss: 1.212 | Accuracy: 0.509766 | 7.078 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 029 | Total loss: 1.249 | Reg loss: 0.028 | Tree loss: 1.249 | Accuracy: 0.457031 | 7.077 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 029 | Total loss: 1.196 | Reg loss: 0.029 | Tree loss: 1.196 | Accuracy: 0.503906 | 7.076 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 029 | Total loss: 1.210 | Reg loss: 0.029 | Tree loss: 1.210 | Accuracy: 0.501953 | 7.075 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 029 | Total loss: 1.205 | Reg loss: 0.029 | Tree loss: 1.205 | Accuracy: 0.490234 | 7.074 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 029 | Total loss: 1.204 | Reg loss: 0.029 | Tree loss: 1.204 | Accuracy: 0.496094 | 7.073 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 029 | Total loss: 1.167 | Reg loss: 0.029 | Tree loss: 1.167 | Accuracy: 0.527344 | 7.073 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 029 | Total loss: 1.208 | Reg loss: 0.029 | Tree loss: 1.208 | Accuracy: 0.478516 | 7.072 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 029 | Total loss: 1.208 | Reg loss: 0.029 | Tree loss: 1.208 | Accuracy: 0.466797 | 7.071 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 029 | Total loss: 1.187 | Reg loss: 0.029 | Tree loss: 1.187 | Accuracy: 0.496094 | 7.07 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 029 | Total loss: 1.194 | Reg loss: 0.029 | Tree loss: 1.194 | Accuracy: 0.478516 | 7.069 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 029 | Total loss: 1.191 | Reg loss: 0.029 | Tree loss: 1.191 | Accuracy: 0.482422 | 7.068 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 029 | Total loss: 1.155 | Reg loss: 0.029 | Tree loss: 1.155 | Accuracy: 0.525391 | 7.068 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 029 | Total loss: 1.184 | Reg loss: 0.029 | Tree loss: 1.184 | Accuracy: 0.477318 | 7.067 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 029 | Total loss: 1.297 | Reg loss: 0.028 | Tree loss: 1.297 | Accuracy: 0.529297 | 7.067 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 029 | Total loss: 1.309 | Reg loss: 0.028 | Tree loss: 1.309 | Accuracy: 0.498047 | 7.066 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 029 | Total loss: 1.278 | Reg loss: 0.028 | Tree loss: 1.278 | Accuracy: 0.537109 | 7.066 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 029 | Total loss: 1.316 | Reg loss: 0.028 | Tree loss: 1.316 | Accuracy: 0.492188 | 7.064 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 029 | Total loss: 1.277 | Reg loss: 0.028 | Tree loss: 1.277 | Accuracy: 0.505859 | 7.064 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 029 | Total loss: 1.275 | Reg loss: 0.028 | Tree loss: 1.275 | Accuracy: 0.517578 | 7.063 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 029 | Total loss: 1.280 | Reg loss: 0.028 | Tree loss: 1.280 | Accuracy: 0.480469 | 7.062 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 029 | Total loss: 1.251 | Reg loss: 0.028 | Tree loss: 1.251 | Accuracy: 0.498047 | 7.062 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 029 | Total loss: 1.255 | Reg loss: 0.028 | Tree loss: 1.255 | Accuracy: 0.501953 | 7.061 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 029 | Total loss: 1.237 | Reg loss: 0.028 | Tree loss: 1.237 | Accuracy: 0.519531 | 7.06 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 029 | Total loss: 1.243 | Reg loss: 0.028 | Tree loss: 1.243 | Accuracy: 0.490234 | 7.06 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 029 | Total loss: 1.241 | Reg loss: 0.028 | Tree loss: 1.241 | Accuracy: 0.494141 | 7.059 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 029 | Total loss: 1.219 | Reg loss: 0.028 | Tree loss: 1.219 | Accuracy: 0.501953 | 7.058 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 029 | Total loss: 1.250 | Reg loss: 0.028 | Tree loss: 1.250 | Accuracy: 0.478516 | 7.057 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 029 | Total loss: 1.212 | Reg loss: 0.028 | Tree loss: 1.212 | Accuracy: 0.500000 | 7.057 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 029 | Total loss: 1.208 | Reg loss: 0.028 | Tree loss: 1.208 | Accuracy: 0.494141 | 7.056 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 029 | Total loss: 1.177 | Reg loss: 0.028 | Tree loss: 1.177 | Accuracy: 0.535156 | 7.055 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 029 | Total loss: 1.213 | Reg loss: 0.028 | Tree loss: 1.213 | Accuracy: 0.480469 | 7.054 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 029 | Total loss: 1.193 | Reg loss: 0.028 | Tree loss: 1.193 | Accuracy: 0.503906 | 7.053 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 029 | Total loss: 1.217 | Reg loss: 0.029 | Tree loss: 1.217 | Accuracy: 0.470703 | 7.052 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 029 | Total loss: 1.232 | Reg loss: 0.029 | Tree loss: 1.232 | Accuracy: 0.445312 | 7.051 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 029 | Total loss: 1.214 | Reg loss: 0.029 | Tree loss: 1.214 | Accuracy: 0.451172 | 7.051 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 029 | Total loss: 1.158 | Reg loss: 0.029 | Tree loss: 1.158 | Accuracy: 0.525391 | 7.05 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 029 | Total loss: 1.194 | Reg loss: 0.029 | Tree loss: 1.194 | Accuracy: 0.484375 | 7.049 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 029 | Total loss: 1.176 | Reg loss: 0.029 | Tree loss: 1.176 | Accuracy: 0.498047 | 7.048 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 029 | Total loss: 1.177 | Reg loss: 0.029 | Tree loss: 1.177 | Accuracy: 0.492188 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 029 | Total loss: 1.228 | Reg loss: 0.029 | Tree loss: 1.228 | Accuracy: 0.429688 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 029 | Total loss: 1.179 | Reg loss: 0.029 | Tree loss: 1.179 | Accuracy: 0.501953 | 7.046 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 029 | Total loss: 1.185 | Reg loss: 0.029 | Tree loss: 1.185 | Accuracy: 0.475345 | 7.045 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 029 | Total loss: 1.323 | Reg loss: 0.028 | Tree loss: 1.323 | Accuracy: 0.480469 | 7.045 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 029 | Total loss: 1.317 | Reg loss: 0.028 | Tree loss: 1.317 | Accuracy: 0.472656 | 7.044 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 029 | Total loss: 1.297 | Reg loss: 0.028 | Tree loss: 1.297 | Accuracy: 0.500000 | 7.044 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 029 | Total loss: 1.312 | Reg loss: 0.028 | Tree loss: 1.312 | Accuracy: 0.470703 | 7.043 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 029 | Total loss: 1.294 | Reg loss: 0.028 | Tree loss: 1.294 | Accuracy: 0.484375 | 7.042 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 029 | Total loss: 1.276 | Reg loss: 0.028 | Tree loss: 1.276 | Accuracy: 0.511719 | 7.041 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 029 | Total loss: 1.245 | Reg loss: 0.028 | Tree loss: 1.245 | Accuracy: 0.519531 | 7.041 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 029 | Total loss: 1.240 | Reg loss: 0.028 | Tree loss: 1.240 | Accuracy: 0.517578 | 7.04 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 029 | Total loss: 1.256 | Reg loss: 0.028 | Tree loss: 1.256 | Accuracy: 0.501953 | 7.04 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 029 | Total loss: 1.214 | Reg loss: 0.028 | Tree loss: 1.214 | Accuracy: 0.533203 | 7.039 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 029 | Total loss: 1.237 | Reg loss: 0.028 | Tree loss: 1.237 | Accuracy: 0.498047 | 7.038 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 029 | Total loss: 1.246 | Reg loss: 0.028 | Tree loss: 1.246 | Accuracy: 0.480469 | 7.038 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 029 | Total loss: 1.199 | Reg loss: 0.028 | Tree loss: 1.199 | Accuracy: 0.539062 | 7.037 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 029 | Total loss: 1.258 | Reg loss: 0.028 | Tree loss: 1.258 | Accuracy: 0.453125 | 7.036 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 029 | Total loss: 1.202 | Reg loss: 0.028 | Tree loss: 1.202 | Accuracy: 0.513672 | 7.035 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Batch: 015 / 029 | Total loss: 1.184 | Reg loss: 0.028 | Tree loss: 1.184 | Accuracy: 0.533203 | 7.035 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 029 | Total loss: 1.194 | Reg loss: 0.028 | Tree loss: 1.194 | Accuracy: 0.519531 | 7.034 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 029 | Total loss: 1.207 | Reg loss: 0.028 | Tree loss: 1.207 | Accuracy: 0.486328 | 7.033 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 029 | Total loss: 1.233 | Reg loss: 0.028 | Tree loss: 1.233 | Accuracy: 0.468750 | 7.032 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 029 | Total loss: 1.229 | Reg loss: 0.028 | Tree loss: 1.229 | Accuracy: 0.462891 | 7.031 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 029 | Total loss: 1.203 | Reg loss: 0.028 | Tree loss: 1.203 | Accuracy: 0.490234 | 7.031 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 029 | Total loss: 1.180 | Reg loss: 0.028 | Tree loss: 1.180 | Accuracy: 0.503906 | 7.03 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 029 | Total loss: 1.208 | Reg loss: 0.029 | Tree loss: 1.208 | Accuracy: 0.458984 | 7.029 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 029 | Total loss: 1.194 | Reg loss: 0.029 | Tree loss: 1.194 | Accuracy: 0.478516 | 7.028 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 029 | Total loss: 1.187 | Reg loss: 0.029 | Tree loss: 1.187 | Accuracy: 0.480469 | 7.027 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 029 | Total loss: 1.150 | Reg loss: 0.029 | Tree loss: 1.150 | Accuracy: 0.539062 | 7.027 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 029 | Total loss: 1.185 | Reg loss: 0.029 | Tree loss: 1.185 | Accuracy: 0.490234 | 7.026 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 029 | Total loss: 1.177 | Reg loss: 0.029 | Tree loss: 1.177 | Accuracy: 0.478516 | 7.025 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 029 | Total loss: 1.193 | Reg loss: 0.029 | Tree loss: 1.193 | Accuracy: 0.465483 | 7.024 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 029 | Total loss: 1.326 | Reg loss: 0.028 | Tree loss: 1.326 | Accuracy: 0.494141 | 7.024 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 029 | Total loss: 1.309 | Reg loss: 0.028 | Tree loss: 1.309 | Accuracy: 0.490234 | 7.024 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 029 | Total loss: 1.288 | Reg loss: 0.028 | Tree loss: 1.288 | Accuracy: 0.488281 | 7.023 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 029 | Total loss: 1.246 | Reg loss: 0.028 | Tree loss: 1.246 | Accuracy: 0.537109 | 7.022 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 029 | Total loss: 1.320 | Reg loss: 0.028 | Tree loss: 1.320 | Accuracy: 0.453125 | 7.022 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 029 | Total loss: 1.301 | Reg loss: 0.028 | Tree loss: 1.301 | Accuracy: 0.466797 | 7.021 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 029 | Total loss: 1.245 | Reg loss: 0.028 | Tree loss: 1.245 | Accuracy: 0.513672 | 7.02 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 029 | Total loss: 1.243 | Reg loss: 0.028 | Tree loss: 1.243 | Accuracy: 0.519531 | 7.019 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 029 | Total loss: 1.234 | Reg loss: 0.028 | Tree loss: 1.234 | Accuracy: 0.515625 | 7.019 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 029 | Total loss: 1.252 | Reg loss: 0.028 | Tree loss: 1.252 | Accuracy: 0.509766 | 7.018 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 029 | Total loss: 1.247 | Reg loss: 0.028 | Tree loss: 1.247 | Accuracy: 0.486328 | 7.018 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 029 | Total loss: 1.220 | Reg loss: 0.028 | Tree loss: 1.220 | Accuracy: 0.517578 | 7.017 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 029 | Total loss: 1.237 | Reg loss: 0.028 | Tree loss: 1.237 | Accuracy: 0.472656 | 7.016 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 029 | Total loss: 1.205 | Reg loss: 0.028 | Tree loss: 1.205 | Accuracy: 0.515625 | 7.016 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 029 | Total loss: 1.201 | Reg loss: 0.028 | Tree loss: 1.201 | Accuracy: 0.500000 | 7.015 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 029 | Total loss: 1.208 | Reg loss: 0.028 | Tree loss: 1.208 | Accuracy: 0.505859 | 7.014 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 029 | Total loss: 1.191 | Reg loss: 0.028 | Tree loss: 1.191 | Accuracy: 0.519531 | 7.014 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 029 | Total loss: 1.238 | Reg loss: 0.028 | Tree loss: 1.238 | Accuracy: 0.453125 | 7.013 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 029 | Total loss: 1.186 | Reg loss: 0.028 | Tree loss: 1.186 | Accuracy: 0.521484 | 7.012 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 029 | Total loss: 1.185 | Reg loss: 0.028 | Tree loss: 1.185 | Accuracy: 0.509766 | 7.011 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 029 | Total loss: 1.195 | Reg loss: 0.028 | Tree loss: 1.195 | Accuracy: 0.486328 | 7.01 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 029 | Total loss: 1.223 | Reg loss: 0.028 | Tree loss: 1.223 | Accuracy: 0.449219 | 7.01 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 029 | Total loss: 1.227 | Reg loss: 0.028 | Tree loss: 1.227 | Accuracy: 0.431641 | 7.009 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 029 | Total loss: 1.177 | Reg loss: 0.028 | Tree loss: 1.177 | Accuracy: 0.500000 | 7.008 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 029 | Total loss: 1.186 | Reg loss: 0.029 | Tree loss: 1.186 | Accuracy: 0.490234 | 7.007 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 029 | Total loss: 1.195 | Reg loss: 0.029 | Tree loss: 1.195 | Accuracy: 0.470703 | 7.006 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 029 | Total loss: 1.163 | Reg loss: 0.029 | Tree loss: 1.163 | Accuracy: 0.513672 | 7.006 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 029 | Total loss: 1.184 | Reg loss: 0.029 | Tree loss: 1.184 | Accuracy: 0.486328 | 7.005 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 029 | Total loss: 1.156 | Reg loss: 0.029 | Tree loss: 1.156 | Accuracy: 0.514793 | 7.004 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c628c202c524a16bc6e4959ed896fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6619fa4ad734363ab61b9ce3bd84e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15b54a428184ac5bf4046988e3b03b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1e1b1afb3044568748dfcd28a532dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 11.886291486291487\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 3465\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "14843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "============== Pattern 1025 ==============\n",
      "============== Pattern 1026 ==============\n",
      "============== Pattern 1027 ==============\n",
      "============== Pattern 1028 ==============\n",
      "============== Pattern 1029 ==============\n",
      "============== Pattern 1030 ==============\n",
      "============== Pattern 1031 ==============\n",
      "============== Pattern 1032 ==============\n",
      "============== Pattern 1033 ==============\n",
      "============== Pattern 1034 ==============\n",
      "============== Pattern 1035 ==============\n",
      "============== Pattern 1036 ==============\n",
      "============== Pattern 1037 ==============\n",
      "============== Pattern 1038 ==============\n",
      "============== Pattern 1039 ==============\n",
      "============== Pattern 1040 ==============\n",
      "============== Pattern 1041 ==============\n",
      "============== Pattern 1042 ==============\n",
      "============== Pattern 1043 ==============\n",
      "============== Pattern 1044 ==============\n",
      "============== Pattern 1045 ==============\n",
      "============== Pattern 1046 ==============\n",
      "============== Pattern 1047 ==============\n",
      "============== Pattern 1048 ==============\n",
      "============== Pattern 1049 ==============\n",
      "============== Pattern 1050 ==============\n",
      "============== Pattern 1051 ==============\n",
      "============== Pattern 1052 ==============\n",
      "============== Pattern 1053 ==============\n",
      "============== Pattern 1054 ==============\n",
      "============== Pattern 1055 ==============\n",
      "============== Pattern 1056 ==============\n",
      "============== Pattern 1057 ==============\n",
      "============== Pattern 1058 ==============\n",
      "============== Pattern 1059 ==============\n",
      "============== Pattern 1060 ==============\n",
      "============== Pattern 1061 ==============\n",
      "============== Pattern 1062 ==============\n",
      "============== Pattern 1063 ==============\n",
      "============== Pattern 1064 ==============\n",
      "============== Pattern 1065 ==============\n",
      "============== Pattern 1066 ==============\n",
      "============== Pattern 1067 ==============\n",
      "============== Pattern 1068 ==============\n",
      "============== Pattern 1069 ==============\n",
      "============== Pattern 1070 ==============\n",
      "============== Pattern 1071 ==============\n",
      "============== Pattern 1072 ==============\n",
      "============== Pattern 1073 ==============\n",
      "============== Pattern 1074 ==============\n",
      "============== Pattern 1075 ==============\n",
      "============== Pattern 1076 ==============\n",
      "============== Pattern 1077 ==============\n",
      "============== Pattern 1078 ==============\n",
      "============== Pattern 1079 ==============\n",
      "============== Pattern 1080 ==============\n",
      "============== Pattern 1081 ==============\n",
      "============== Pattern 1082 ==============\n",
      "============== Pattern 1083 ==============\n",
      "============== Pattern 1084 ==============\n",
      "============== Pattern 1085 ==============\n",
      "============== Pattern 1086 ==============\n",
      "============== Pattern 1087 ==============\n",
      "============== Pattern 1088 ==============\n",
      "============== Pattern 1089 ==============\n",
      "============== Pattern 1090 ==============\n",
      "============== Pattern 1091 ==============\n",
      "============== Pattern 1092 ==============\n",
      "============== Pattern 1093 ==============\n",
      "============== Pattern 1094 ==============\n",
      "============== Pattern 1095 ==============\n",
      "============== Pattern 1096 ==============\n",
      "============== Pattern 1097 ==============\n",
      "============== Pattern 1098 ==============\n",
      "============== Pattern 1099 ==============\n",
      "============== Pattern 1100 ==============\n",
      "============== Pattern 1101 ==============\n",
      "============== Pattern 1102 ==============\n",
      "============== Pattern 1103 ==============\n",
      "============== Pattern 1104 ==============\n",
      "============== Pattern 1105 ==============\n",
      "============== Pattern 1106 ==============\n",
      "============== Pattern 1107 ==============\n",
      "============== Pattern 1108 ==============\n",
      "============== Pattern 1109 ==============\n",
      "============== Pattern 1110 ==============\n",
      "============== Pattern 1111 ==============\n",
      "============== Pattern 1112 ==============\n",
      "============== Pattern 1113 ==============\n",
      "============== Pattern 1114 ==============\n",
      "============== Pattern 1115 ==============\n",
      "============== Pattern 1116 ==============\n",
      "============== Pattern 1117 ==============\n",
      "============== Pattern 1118 ==============\n",
      "============== Pattern 1119 ==============\n",
      "============== Pattern 1120 ==============\n",
      "============== Pattern 1121 ==============\n",
      "============== Pattern 1122 ==============\n",
      "============== Pattern 1123 ==============\n",
      "============== Pattern 1124 ==============\n",
      "============== Pattern 1125 ==============\n",
      "============== Pattern 1126 ==============\n",
      "============== Pattern 1127 ==============\n",
      "============== Pattern 1128 ==============\n",
      "============== Pattern 1129 ==============\n",
      "============== Pattern 1130 ==============\n",
      "============== Pattern 1131 ==============\n",
      "============== Pattern 1132 ==============\n",
      "============== Pattern 1133 ==============\n",
      "============== Pattern 1134 ==============\n",
      "============== Pattern 1135 ==============\n",
      "============== Pattern 1136 ==============\n",
      "============== Pattern 1137 ==============\n",
      "============== Pattern 1138 ==============\n",
      "============== Pattern 1139 ==============\n",
      "============== Pattern 1140 ==============\n",
      "============== Pattern 1141 ==============\n",
      "============== Pattern 1142 ==============\n",
      "============== Pattern 1143 ==============\n",
      "============== Pattern 1144 ==============\n",
      "============== Pattern 1145 ==============\n",
      "============== Pattern 1146 ==============\n",
      "============== Pattern 1147 ==============\n",
      "============== Pattern 1148 ==============\n",
      "============== Pattern 1149 ==============\n",
      "============== Pattern 1150 ==============\n",
      "============== Pattern 1151 ==============\n",
      "============== Pattern 1152 ==============\n",
      "============== Pattern 1153 ==============\n",
      "============== Pattern 1154 ==============\n",
      "============== Pattern 1155 ==============\n",
      "============== Pattern 1156 ==============\n",
      "============== Pattern 1157 ==============\n",
      "============== Pattern 1158 ==============\n",
      "============== Pattern 1159 ==============\n",
      "============== Pattern 1160 ==============\n",
      "============== Pattern 1161 ==============\n",
      "============== Pattern 1162 ==============\n",
      "============== Pattern 1163 ==============\n",
      "============== Pattern 1164 ==============\n",
      "============== Pattern 1165 ==============\n",
      "============== Pattern 1166 ==============\n",
      "============== Pattern 1167 ==============\n",
      "============== Pattern 1168 ==============\n",
      "============== Pattern 1169 ==============\n",
      "============== Pattern 1170 ==============\n",
      "============== Pattern 1171 ==============\n",
      "============== Pattern 1172 ==============\n",
      "============== Pattern 1173 ==============\n",
      "============== Pattern 1174 ==============\n",
      "============== Pattern 1175 ==============\n",
      "============== Pattern 1176 ==============\n",
      "============== Pattern 1177 ==============\n",
      "============== Pattern 1178 ==============\n",
      "============== Pattern 1179 ==============\n",
      "============== Pattern 1180 ==============\n",
      "============== Pattern 1181 ==============\n",
      "============== Pattern 1182 ==============\n",
      "============== Pattern 1183 ==============\n",
      "============== Pattern 1184 ==============\n",
      "============== Pattern 1185 ==============\n",
      "============== Pattern 1186 ==============\n",
      "============== Pattern 1187 ==============\n",
      "============== Pattern 1188 ==============\n",
      "============== Pattern 1189 ==============\n",
      "============== Pattern 1190 ==============\n",
      "============== Pattern 1191 ==============\n",
      "============== Pattern 1192 ==============\n",
      "============== Pattern 1193 ==============\n",
      "============== Pattern 1194 ==============\n",
      "============== Pattern 1195 ==============\n",
      "============== Pattern 1196 ==============\n",
      "============== Pattern 1197 ==============\n",
      "============== Pattern 1198 ==============\n",
      "============== Pattern 1199 ==============\n",
      "============== Pattern 1200 ==============\n",
      "============== Pattern 1201 ==============\n",
      "============== Pattern 1202 ==============\n",
      "============== Pattern 1203 ==============\n",
      "============== Pattern 1204 ==============\n",
      "============== Pattern 1205 ==============\n",
      "============== Pattern 1206 ==============\n",
      "============== Pattern 1207 ==============\n",
      "============== Pattern 1208 ==============\n",
      "============== Pattern 1209 ==============\n",
      "============== Pattern 1210 ==============\n",
      "============== Pattern 1211 ==============\n",
      "============== Pattern 1212 ==============\n",
      "============== Pattern 1213 ==============\n",
      "============== Pattern 1214 ==============\n",
      "============== Pattern 1215 ==============\n",
      "============== Pattern 1216 ==============\n",
      "============== Pattern 1217 ==============\n",
      "============== Pattern 1218 ==============\n",
      "============== Pattern 1219 ==============\n",
      "============== Pattern 1220 ==============\n",
      "============== Pattern 1221 ==============\n",
      "============== Pattern 1222 ==============\n",
      "============== Pattern 1223 ==============\n",
      "============== Pattern 1224 ==============\n",
      "============== Pattern 1225 ==============\n",
      "============== Pattern 1226 ==============\n",
      "============== Pattern 1227 ==============\n",
      "============== Pattern 1228 ==============\n",
      "============== Pattern 1229 ==============\n",
      "============== Pattern 1230 ==============\n",
      "============== Pattern 1231 ==============\n",
      "============== Pattern 1232 ==============\n",
      "============== Pattern 1233 ==============\n",
      "============== Pattern 1234 ==============\n",
      "============== Pattern 1235 ==============\n",
      "============== Pattern 1236 ==============\n",
      "============== Pattern 1237 ==============\n",
      "============== Pattern 1238 ==============\n",
      "============== Pattern 1239 ==============\n",
      "============== Pattern 1240 ==============\n",
      "============== Pattern 1241 ==============\n",
      "============== Pattern 1242 ==============\n",
      "============== Pattern 1243 ==============\n",
      "============== Pattern 1244 ==============\n",
      "============== Pattern 1245 ==============\n",
      "============== Pattern 1246 ==============\n",
      "============== Pattern 1247 ==============\n",
      "============== Pattern 1248 ==============\n",
      "============== Pattern 1249 ==============\n",
      "============== Pattern 1250 ==============\n",
      "============== Pattern 1251 ==============\n",
      "============== Pattern 1252 ==============\n",
      "============== Pattern 1253 ==============\n",
      "============== Pattern 1254 ==============\n",
      "============== Pattern 1255 ==============\n",
      "============== Pattern 1256 ==============\n",
      "============== Pattern 1257 ==============\n",
      "============== Pattern 1258 ==============\n",
      "============== Pattern 1259 ==============\n",
      "============== Pattern 1260 ==============\n",
      "============== Pattern 1261 ==============\n",
      "============== Pattern 1262 ==============\n",
      "============== Pattern 1263 ==============\n",
      "============== Pattern 1264 ==============\n",
      "============== Pattern 1265 ==============\n",
      "============== Pattern 1266 ==============\n",
      "============== Pattern 1267 ==============\n",
      "============== Pattern 1268 ==============\n",
      "============== Pattern 1269 ==============\n",
      "============== Pattern 1270 ==============\n",
      "============== Pattern 1271 ==============\n",
      "============== Pattern 1272 ==============\n",
      "============== Pattern 1273 ==============\n",
      "============== Pattern 1274 ==============\n",
      "============== Pattern 1275 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1276 ==============\n",
      "============== Pattern 1277 ==============\n",
      "============== Pattern 1278 ==============\n",
      "============== Pattern 1279 ==============\n",
      "============== Pattern 1280 ==============\n",
      "============== Pattern 1281 ==============\n",
      "============== Pattern 1282 ==============\n",
      "============== Pattern 1283 ==============\n",
      "============== Pattern 1284 ==============\n",
      "============== Pattern 1285 ==============\n",
      "============== Pattern 1286 ==============\n",
      "============== Pattern 1287 ==============\n",
      "============== Pattern 1288 ==============\n",
      "============== Pattern 1289 ==============\n",
      "============== Pattern 1290 ==============\n",
      "============== Pattern 1291 ==============\n",
      "============== Pattern 1292 ==============\n",
      "============== Pattern 1293 ==============\n",
      "============== Pattern 1294 ==============\n",
      "============== Pattern 1295 ==============\n",
      "============== Pattern 1296 ==============\n",
      "============== Pattern 1297 ==============\n",
      "============== Pattern 1298 ==============\n",
      "============== Pattern 1299 ==============\n",
      "============== Pattern 1300 ==============\n",
      "============== Pattern 1301 ==============\n",
      "============== Pattern 1302 ==============\n",
      "============== Pattern 1303 ==============\n",
      "============== Pattern 1304 ==============\n",
      "============== Pattern 1305 ==============\n",
      "============== Pattern 1306 ==============\n",
      "============== Pattern 1307 ==============\n",
      "============== Pattern 1308 ==============\n",
      "============== Pattern 1309 ==============\n",
      "============== Pattern 1310 ==============\n",
      "============== Pattern 1311 ==============\n",
      "============== Pattern 1312 ==============\n",
      "============== Pattern 1313 ==============\n",
      "============== Pattern 1314 ==============\n",
      "============== Pattern 1315 ==============\n",
      "============== Pattern 1316 ==============\n",
      "============== Pattern 1317 ==============\n",
      "============== Pattern 1318 ==============\n",
      "============== Pattern 1319 ==============\n",
      "============== Pattern 1320 ==============\n",
      "============== Pattern 1321 ==============\n",
      "============== Pattern 1322 ==============\n",
      "============== Pattern 1323 ==============\n",
      "============== Pattern 1324 ==============\n",
      "============== Pattern 1325 ==============\n",
      "============== Pattern 1326 ==============\n",
      "============== Pattern 1327 ==============\n",
      "============== Pattern 1328 ==============\n",
      "============== Pattern 1329 ==============\n",
      "============== Pattern 1330 ==============\n",
      "============== Pattern 1331 ==============\n",
      "============== Pattern 1332 ==============\n",
      "============== Pattern 1333 ==============\n",
      "============== Pattern 1334 ==============\n",
      "============== Pattern 1335 ==============\n",
      "============== Pattern 1336 ==============\n",
      "============== Pattern 1337 ==============\n",
      "============== Pattern 1338 ==============\n",
      "============== Pattern 1339 ==============\n",
      "============== Pattern 1340 ==============\n",
      "============== Pattern 1341 ==============\n",
      "============== Pattern 1342 ==============\n",
      "============== Pattern 1343 ==============\n",
      "============== Pattern 1344 ==============\n",
      "============== Pattern 1345 ==============\n",
      "============== Pattern 1346 ==============\n",
      "============== Pattern 1347 ==============\n",
      "============== Pattern 1348 ==============\n",
      "============== Pattern 1349 ==============\n",
      "============== Pattern 1350 ==============\n",
      "============== Pattern 1351 ==============\n",
      "============== Pattern 1352 ==============\n",
      "============== Pattern 1353 ==============\n",
      "============== Pattern 1354 ==============\n",
      "============== Pattern 1355 ==============\n",
      "============== Pattern 1356 ==============\n",
      "============== Pattern 1357 ==============\n",
      "============== Pattern 1358 ==============\n",
      "============== Pattern 1359 ==============\n",
      "============== Pattern 1360 ==============\n",
      "============== Pattern 1361 ==============\n",
      "============== Pattern 1362 ==============\n",
      "============== Pattern 1363 ==============\n",
      "============== Pattern 1364 ==============\n",
      "============== Pattern 1365 ==============\n",
      "============== Pattern 1366 ==============\n",
      "============== Pattern 1367 ==============\n",
      "============== Pattern 1368 ==============\n",
      "============== Pattern 1369 ==============\n",
      "============== Pattern 1370 ==============\n",
      "============== Pattern 1371 ==============\n",
      "============== Pattern 1372 ==============\n",
      "============== Pattern 1373 ==============\n",
      "============== Pattern 1374 ==============\n",
      "============== Pattern 1375 ==============\n",
      "============== Pattern 1376 ==============\n",
      "============== Pattern 1377 ==============\n",
      "============== Pattern 1378 ==============\n",
      "============== Pattern 1379 ==============\n",
      "============== Pattern 1380 ==============\n",
      "============== Pattern 1381 ==============\n",
      "============== Pattern 1382 ==============\n",
      "============== Pattern 1383 ==============\n",
      "============== Pattern 1384 ==============\n",
      "============== Pattern 1385 ==============\n",
      "============== Pattern 1386 ==============\n",
      "============== Pattern 1387 ==============\n",
      "============== Pattern 1388 ==============\n",
      "============== Pattern 1389 ==============\n",
      "============== Pattern 1390 ==============\n",
      "============== Pattern 1391 ==============\n",
      "============== Pattern 1392 ==============\n",
      "============== Pattern 1393 ==============\n",
      "============== Pattern 1394 ==============\n",
      "============== Pattern 1395 ==============\n",
      "============== Pattern 1396 ==============\n",
      "============== Pattern 1397 ==============\n",
      "============== Pattern 1398 ==============\n",
      "============== Pattern 1399 ==============\n",
      "============== Pattern 1400 ==============\n",
      "============== Pattern 1401 ==============\n",
      "============== Pattern 1402 ==============\n",
      "============== Pattern 1403 ==============\n",
      "============== Pattern 1404 ==============\n",
      "============== Pattern 1405 ==============\n",
      "============== Pattern 1406 ==============\n",
      "============== Pattern 1407 ==============\n",
      "============== Pattern 1408 ==============\n",
      "============== Pattern 1409 ==============\n",
      "============== Pattern 1410 ==============\n",
      "============== Pattern 1411 ==============\n",
      "============== Pattern 1412 ==============\n",
      "============== Pattern 1413 ==============\n",
      "============== Pattern 1414 ==============\n",
      "============== Pattern 1415 ==============\n",
      "============== Pattern 1416 ==============\n",
      "============== Pattern 1417 ==============\n",
      "============== Pattern 1418 ==============\n",
      "============== Pattern 1419 ==============\n",
      "============== Pattern 1420 ==============\n",
      "============== Pattern 1421 ==============\n",
      "============== Pattern 1422 ==============\n",
      "============== Pattern 1423 ==============\n",
      "============== Pattern 1424 ==============\n",
      "============== Pattern 1425 ==============\n",
      "============== Pattern 1426 ==============\n",
      "============== Pattern 1427 ==============\n",
      "============== Pattern 1428 ==============\n",
      "============== Pattern 1429 ==============\n",
      "============== Pattern 1430 ==============\n",
      "============== Pattern 1431 ==============\n",
      "============== Pattern 1432 ==============\n",
      "============== Pattern 1433 ==============\n",
      "============== Pattern 1434 ==============\n",
      "============== Pattern 1435 ==============\n",
      "============== Pattern 1436 ==============\n",
      "============== Pattern 1437 ==============\n",
      "============== Pattern 1438 ==============\n",
      "============== Pattern 1439 ==============\n",
      "============== Pattern 1440 ==============\n",
      "============== Pattern 1441 ==============\n",
      "============== Pattern 1442 ==============\n",
      "============== Pattern 1443 ==============\n",
      "============== Pattern 1444 ==============\n",
      "============== Pattern 1445 ==============\n",
      "============== Pattern 1446 ==============\n",
      "============== Pattern 1447 ==============\n",
      "============== Pattern 1448 ==============\n",
      "============== Pattern 1449 ==============\n",
      "============== Pattern 1450 ==============\n",
      "============== Pattern 1451 ==============\n",
      "============== Pattern 1452 ==============\n",
      "============== Pattern 1453 ==============\n",
      "============== Pattern 1454 ==============\n",
      "============== Pattern 1455 ==============\n",
      "============== Pattern 1456 ==============\n",
      "============== Pattern 1457 ==============\n",
      "============== Pattern 1458 ==============\n",
      "============== Pattern 1459 ==============\n",
      "============== Pattern 1460 ==============\n",
      "============== Pattern 1461 ==============\n",
      "============== Pattern 1462 ==============\n",
      "============== Pattern 1463 ==============\n",
      "============== Pattern 1464 ==============\n",
      "============== Pattern 1465 ==============\n",
      "============== Pattern 1466 ==============\n",
      "============== Pattern 1467 ==============\n",
      "============== Pattern 1468 ==============\n",
      "============== Pattern 1469 ==============\n",
      "============== Pattern 1470 ==============\n",
      "============== Pattern 1471 ==============\n",
      "============== Pattern 1472 ==============\n",
      "============== Pattern 1473 ==============\n",
      "============== Pattern 1474 ==============\n",
      "============== Pattern 1475 ==============\n",
      "============== Pattern 1476 ==============\n",
      "============== Pattern 1477 ==============\n",
      "============== Pattern 1478 ==============\n",
      "============== Pattern 1479 ==============\n",
      "============== Pattern 1480 ==============\n",
      "============== Pattern 1481 ==============\n",
      "============== Pattern 1482 ==============\n",
      "============== Pattern 1483 ==============\n",
      "============== Pattern 1484 ==============\n",
      "============== Pattern 1485 ==============\n",
      "============== Pattern 1486 ==============\n",
      "============== Pattern 1487 ==============\n",
      "============== Pattern 1488 ==============\n",
      "============== Pattern 1489 ==============\n",
      "============== Pattern 1490 ==============\n",
      "============== Pattern 1491 ==============\n",
      "============== Pattern 1492 ==============\n",
      "============== Pattern 1493 ==============\n",
      "============== Pattern 1494 ==============\n",
      "============== Pattern 1495 ==============\n",
      "============== Pattern 1496 ==============\n",
      "============== Pattern 1497 ==============\n",
      "============== Pattern 1498 ==============\n",
      "============== Pattern 1499 ==============\n",
      "============== Pattern 1500 ==============\n",
      "============== Pattern 1501 ==============\n",
      "============== Pattern 1502 ==============\n",
      "============== Pattern 1503 ==============\n",
      "============== Pattern 1504 ==============\n",
      "============== Pattern 1505 ==============\n",
      "============== Pattern 1506 ==============\n",
      "============== Pattern 1507 ==============\n",
      "============== Pattern 1508 ==============\n",
      "============== Pattern 1509 ==============\n",
      "============== Pattern 1510 ==============\n",
      "============== Pattern 1511 ==============\n",
      "============== Pattern 1512 ==============\n",
      "============== Pattern 1513 ==============\n",
      "============== Pattern 1514 ==============\n",
      "============== Pattern 1515 ==============\n",
      "============== Pattern 1516 ==============\n",
      "============== Pattern 1517 ==============\n",
      "============== Pattern 1518 ==============\n",
      "============== Pattern 1519 ==============\n",
      "============== Pattern 1520 ==============\n",
      "============== Pattern 1521 ==============\n",
      "============== Pattern 1522 ==============\n",
      "============== Pattern 1523 ==============\n",
      "============== Pattern 1524 ==============\n",
      "============== Pattern 1525 ==============\n",
      "============== Pattern 1526 ==============\n",
      "============== Pattern 1527 ==============\n",
      "============== Pattern 1528 ==============\n",
      "============== Pattern 1529 ==============\n",
      "============== Pattern 1530 ==============\n",
      "============== Pattern 1531 ==============\n",
      "============== Pattern 1532 ==============\n",
      "============== Pattern 1533 ==============\n",
      "============== Pattern 1534 ==============\n",
      "============== Pattern 1535 ==============\n",
      "============== Pattern 1536 ==============\n",
      "============== Pattern 1537 ==============\n",
      "============== Pattern 1538 ==============\n",
      "============== Pattern 1539 ==============\n",
      "============== Pattern 1540 ==============\n",
      "============== Pattern 1541 ==============\n",
      "============== Pattern 1542 ==============\n",
      "============== Pattern 1543 ==============\n",
      "============== Pattern 1544 ==============\n",
      "============== Pattern 1545 ==============\n",
      "============== Pattern 1546 ==============\n",
      "============== Pattern 1547 ==============\n",
      "============== Pattern 1548 ==============\n",
      "============== Pattern 1549 ==============\n",
      "============== Pattern 1550 ==============\n",
      "============== Pattern 1551 ==============\n",
      "============== Pattern 1552 ==============\n",
      "============== Pattern 1553 ==============\n",
      "============== Pattern 1554 ==============\n",
      "============== Pattern 1555 ==============\n",
      "============== Pattern 1556 ==============\n",
      "============== Pattern 1557 ==============\n",
      "============== Pattern 1558 ==============\n",
      "============== Pattern 1559 ==============\n",
      "============== Pattern 1560 ==============\n",
      "============== Pattern 1561 ==============\n",
      "============== Pattern 1562 ==============\n",
      "============== Pattern 1563 ==============\n",
      "============== Pattern 1564 ==============\n",
      "============== Pattern 1565 ==============\n",
      "============== Pattern 1566 ==============\n",
      "============== Pattern 1567 ==============\n",
      "============== Pattern 1568 ==============\n",
      "============== Pattern 1569 ==============\n",
      "============== Pattern 1570 ==============\n",
      "============== Pattern 1571 ==============\n",
      "============== Pattern 1572 ==============\n",
      "============== Pattern 1573 ==============\n",
      "============== Pattern 1574 ==============\n",
      "============== Pattern 1575 ==============\n",
      "============== Pattern 1576 ==============\n",
      "============== Pattern 1577 ==============\n",
      "============== Pattern 1578 ==============\n",
      "============== Pattern 1579 ==============\n",
      "============== Pattern 1580 ==============\n",
      "============== Pattern 1581 ==============\n",
      "============== Pattern 1582 ==============\n",
      "============== Pattern 1583 ==============\n",
      "============== Pattern 1584 ==============\n",
      "============== Pattern 1585 ==============\n",
      "============== Pattern 1586 ==============\n",
      "============== Pattern 1587 ==============\n",
      "============== Pattern 1588 ==============\n",
      "============== Pattern 1589 ==============\n",
      "============== Pattern 1590 ==============\n",
      "============== Pattern 1591 ==============\n",
      "============== Pattern 1592 ==============\n",
      "============== Pattern 1593 ==============\n",
      "============== Pattern 1594 ==============\n",
      "============== Pattern 1595 ==============\n",
      "============== Pattern 1596 ==============\n",
      "============== Pattern 1597 ==============\n",
      "============== Pattern 1598 ==============\n",
      "============== Pattern 1599 ==============\n",
      "============== Pattern 1600 ==============\n",
      "============== Pattern 1601 ==============\n",
      "============== Pattern 1602 ==============\n",
      "============== Pattern 1603 ==============\n",
      "============== Pattern 1604 ==============\n",
      "============== Pattern 1605 ==============\n",
      "============== Pattern 1606 ==============\n",
      "============== Pattern 1607 ==============\n",
      "============== Pattern 1608 ==============\n",
      "============== Pattern 1609 ==============\n",
      "============== Pattern 1610 ==============\n",
      "============== Pattern 1611 ==============\n",
      "============== Pattern 1612 ==============\n",
      "============== Pattern 1613 ==============\n",
      "============== Pattern 1614 ==============\n",
      "============== Pattern 1615 ==============\n",
      "============== Pattern 1616 ==============\n",
      "============== Pattern 1617 ==============\n",
      "============== Pattern 1618 ==============\n",
      "============== Pattern 1619 ==============\n",
      "============== Pattern 1620 ==============\n",
      "============== Pattern 1621 ==============\n",
      "============== Pattern 1622 ==============\n",
      "============== Pattern 1623 ==============\n",
      "============== Pattern 1624 ==============\n",
      "============== Pattern 1625 ==============\n",
      "============== Pattern 1626 ==============\n",
      "============== Pattern 1627 ==============\n",
      "============== Pattern 1628 ==============\n",
      "============== Pattern 1629 ==============\n",
      "============== Pattern 1630 ==============\n",
      "============== Pattern 1631 ==============\n",
      "============== Pattern 1632 ==============\n",
      "============== Pattern 1633 ==============\n",
      "============== Pattern 1634 ==============\n",
      "============== Pattern 1635 ==============\n",
      "============== Pattern 1636 ==============\n",
      "============== Pattern 1637 ==============\n",
      "============== Pattern 1638 ==============\n",
      "============== Pattern 1639 ==============\n",
      "============== Pattern 1640 ==============\n",
      "============== Pattern 1641 ==============\n",
      "============== Pattern 1642 ==============\n",
      "============== Pattern 1643 ==============\n",
      "============== Pattern 1644 ==============\n",
      "============== Pattern 1645 ==============\n",
      "============== Pattern 1646 ==============\n",
      "============== Pattern 1647 ==============\n",
      "============== Pattern 1648 ==============\n",
      "============== Pattern 1649 ==============\n",
      "============== Pattern 1650 ==============\n",
      "============== Pattern 1651 ==============\n",
      "============== Pattern 1652 ==============\n",
      "============== Pattern 1653 ==============\n",
      "============== Pattern 1654 ==============\n",
      "============== Pattern 1655 ==============\n",
      "============== Pattern 1656 ==============\n",
      "============== Pattern 1657 ==============\n",
      "============== Pattern 1658 ==============\n",
      "============== Pattern 1659 ==============\n",
      "============== Pattern 1660 ==============\n",
      "============== Pattern 1661 ==============\n",
      "============== Pattern 1662 ==============\n",
      "============== Pattern 1663 ==============\n",
      "============== Pattern 1664 ==============\n",
      "============== Pattern 1665 ==============\n",
      "============== Pattern 1666 ==============\n",
      "============== Pattern 1667 ==============\n",
      "============== Pattern 1668 ==============\n",
      "============== Pattern 1669 ==============\n",
      "============== Pattern 1670 ==============\n",
      "============== Pattern 1671 ==============\n",
      "============== Pattern 1672 ==============\n",
      "============== Pattern 1673 ==============\n",
      "============== Pattern 1674 ==============\n",
      "============== Pattern 1675 ==============\n",
      "============== Pattern 1676 ==============\n",
      "============== Pattern 1677 ==============\n",
      "============== Pattern 1678 ==============\n",
      "============== Pattern 1679 ==============\n",
      "============== Pattern 1680 ==============\n",
      "============== Pattern 1681 ==============\n",
      "============== Pattern 1682 ==============\n",
      "============== Pattern 1683 ==============\n",
      "============== Pattern 1684 ==============\n",
      "============== Pattern 1685 ==============\n",
      "============== Pattern 1686 ==============\n",
      "============== Pattern 1687 ==============\n",
      "============== Pattern 1688 ==============\n",
      "============== Pattern 1689 ==============\n",
      "============== Pattern 1690 ==============\n",
      "============== Pattern 1691 ==============\n",
      "============== Pattern 1692 ==============\n",
      "============== Pattern 1693 ==============\n",
      "============== Pattern 1694 ==============\n",
      "============== Pattern 1695 ==============\n",
      "============== Pattern 1696 ==============\n",
      "============== Pattern 1697 ==============\n",
      "============== Pattern 1698 ==============\n",
      "============== Pattern 1699 ==============\n",
      "============== Pattern 1700 ==============\n",
      "============== Pattern 1701 ==============\n",
      "============== Pattern 1702 ==============\n",
      "============== Pattern 1703 ==============\n",
      "============== Pattern 1704 ==============\n",
      "============== Pattern 1705 ==============\n",
      "============== Pattern 1706 ==============\n",
      "============== Pattern 1707 ==============\n",
      "============== Pattern 1708 ==============\n",
      "============== Pattern 1709 ==============\n",
      "============== Pattern 1710 ==============\n",
      "============== Pattern 1711 ==============\n",
      "============== Pattern 1712 ==============\n",
      "============== Pattern 1713 ==============\n",
      "============== Pattern 1714 ==============\n",
      "============== Pattern 1715 ==============\n",
      "============== Pattern 1716 ==============\n",
      "============== Pattern 1717 ==============\n",
      "============== Pattern 1718 ==============\n",
      "============== Pattern 1719 ==============\n",
      "============== Pattern 1720 ==============\n",
      "============== Pattern 1721 ==============\n",
      "============== Pattern 1722 ==============\n",
      "============== Pattern 1723 ==============\n",
      "============== Pattern 1724 ==============\n",
      "============== Pattern 1725 ==============\n",
      "============== Pattern 1726 ==============\n",
      "============== Pattern 1727 ==============\n",
      "============== Pattern 1728 ==============\n",
      "============== Pattern 1729 ==============\n",
      "============== Pattern 1730 ==============\n",
      "============== Pattern 1731 ==============\n",
      "============== Pattern 1732 ==============\n",
      "============== Pattern 1733 ==============\n",
      "============== Pattern 1734 ==============\n",
      "============== Pattern 1735 ==============\n",
      "============== Pattern 1736 ==============\n",
      "============== Pattern 1737 ==============\n",
      "============== Pattern 1738 ==============\n",
      "============== Pattern 1739 ==============\n",
      "============== Pattern 1740 ==============\n",
      "============== Pattern 1741 ==============\n",
      "============== Pattern 1742 ==============\n",
      "============== Pattern 1743 ==============\n",
      "============== Pattern 1744 ==============\n",
      "============== Pattern 1745 ==============\n",
      "============== Pattern 1746 ==============\n",
      "============== Pattern 1747 ==============\n",
      "============== Pattern 1748 ==============\n",
      "============== Pattern 1749 ==============\n",
      "============== Pattern 1750 ==============\n",
      "============== Pattern 1751 ==============\n",
      "============== Pattern 1752 ==============\n",
      "============== Pattern 1753 ==============\n",
      "============== Pattern 1754 ==============\n",
      "============== Pattern 1755 ==============\n",
      "============== Pattern 1756 ==============\n",
      "============== Pattern 1757 ==============\n",
      "============== Pattern 1758 ==============\n",
      "============== Pattern 1759 ==============\n",
      "============== Pattern 1760 ==============\n",
      "============== Pattern 1761 ==============\n",
      "============== Pattern 1762 ==============\n",
      "============== Pattern 1763 ==============\n",
      "============== Pattern 1764 ==============\n",
      "============== Pattern 1765 ==============\n",
      "============== Pattern 1766 ==============\n",
      "============== Pattern 1767 ==============\n",
      "============== Pattern 1768 ==============\n",
      "============== Pattern 1769 ==============\n",
      "============== Pattern 1770 ==============\n",
      "============== Pattern 1771 ==============\n",
      "============== Pattern 1772 ==============\n",
      "============== Pattern 1773 ==============\n",
      "============== Pattern 1774 ==============\n",
      "============== Pattern 1775 ==============\n",
      "============== Pattern 1776 ==============\n",
      "============== Pattern 1777 ==============\n",
      "============== Pattern 1778 ==============\n",
      "============== Pattern 1779 ==============\n",
      "============== Pattern 1780 ==============\n",
      "============== Pattern 1781 ==============\n",
      "============== Pattern 1782 ==============\n",
      "============== Pattern 1783 ==============\n",
      "============== Pattern 1784 ==============\n",
      "============== Pattern 1785 ==============\n",
      "============== Pattern 1786 ==============\n",
      "============== Pattern 1787 ==============\n",
      "============== Pattern 1788 ==============\n",
      "============== Pattern 1789 ==============\n",
      "============== Pattern 1790 ==============\n",
      "============== Pattern 1791 ==============\n",
      "============== Pattern 1792 ==============\n",
      "============== Pattern 1793 ==============\n",
      "============== Pattern 1794 ==============\n",
      "============== Pattern 1795 ==============\n",
      "============== Pattern 1796 ==============\n",
      "============== Pattern 1797 ==============\n",
      "============== Pattern 1798 ==============\n",
      "============== Pattern 1799 ==============\n",
      "============== Pattern 1800 ==============\n",
      "============== Pattern 1801 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1802 ==============\n",
      "============== Pattern 1803 ==============\n",
      "============== Pattern 1804 ==============\n",
      "============== Pattern 1805 ==============\n",
      "============== Pattern 1806 ==============\n",
      "============== Pattern 1807 ==============\n",
      "============== Pattern 1808 ==============\n",
      "============== Pattern 1809 ==============\n",
      "============== Pattern 1810 ==============\n",
      "============== Pattern 1811 ==============\n",
      "============== Pattern 1812 ==============\n",
      "============== Pattern 1813 ==============\n",
      "============== Pattern 1814 ==============\n",
      "============== Pattern 1815 ==============\n",
      "============== Pattern 1816 ==============\n",
      "============== Pattern 1817 ==============\n",
      "============== Pattern 1818 ==============\n",
      "============== Pattern 1819 ==============\n",
      "============== Pattern 1820 ==============\n",
      "============== Pattern 1821 ==============\n",
      "============== Pattern 1822 ==============\n",
      "============== Pattern 1823 ==============\n",
      "============== Pattern 1824 ==============\n",
      "============== Pattern 1825 ==============\n",
      "============== Pattern 1826 ==============\n",
      "============== Pattern 1827 ==============\n",
      "============== Pattern 1828 ==============\n",
      "============== Pattern 1829 ==============\n",
      "============== Pattern 1830 ==============\n",
      "============== Pattern 1831 ==============\n",
      "============== Pattern 1832 ==============\n",
      "============== Pattern 1833 ==============\n",
      "============== Pattern 1834 ==============\n",
      "============== Pattern 1835 ==============\n",
      "============== Pattern 1836 ==============\n",
      "============== Pattern 1837 ==============\n",
      "============== Pattern 1838 ==============\n",
      "============== Pattern 1839 ==============\n",
      "============== Pattern 1840 ==============\n",
      "============== Pattern 1841 ==============\n",
      "============== Pattern 1842 ==============\n",
      "============== Pattern 1843 ==============\n",
      "============== Pattern 1844 ==============\n",
      "============== Pattern 1845 ==============\n",
      "============== Pattern 1846 ==============\n",
      "============== Pattern 1847 ==============\n",
      "============== Pattern 1848 ==============\n",
      "============== Pattern 1849 ==============\n",
      "============== Pattern 1850 ==============\n",
      "============== Pattern 1851 ==============\n",
      "============== Pattern 1852 ==============\n",
      "============== Pattern 1853 ==============\n",
      "============== Pattern 1854 ==============\n",
      "============== Pattern 1855 ==============\n",
      "============== Pattern 1856 ==============\n",
      "============== Pattern 1857 ==============\n",
      "============== Pattern 1858 ==============\n",
      "============== Pattern 1859 ==============\n",
      "============== Pattern 1860 ==============\n",
      "============== Pattern 1861 ==============\n",
      "============== Pattern 1862 ==============\n",
      "============== Pattern 1863 ==============\n",
      "============== Pattern 1864 ==============\n",
      "============== Pattern 1865 ==============\n",
      "============== Pattern 1866 ==============\n",
      "============== Pattern 1867 ==============\n",
      "============== Pattern 1868 ==============\n",
      "============== Pattern 1869 ==============\n",
      "============== Pattern 1870 ==============\n",
      "============== Pattern 1871 ==============\n",
      "============== Pattern 1872 ==============\n",
      "============== Pattern 1873 ==============\n",
      "============== Pattern 1874 ==============\n",
      "============== Pattern 1875 ==============\n",
      "============== Pattern 1876 ==============\n",
      "============== Pattern 1877 ==============\n",
      "============== Pattern 1878 ==============\n",
      "============== Pattern 1879 ==============\n",
      "============== Pattern 1880 ==============\n",
      "============== Pattern 1881 ==============\n",
      "============== Pattern 1882 ==============\n",
      "============== Pattern 1883 ==============\n",
      "============== Pattern 1884 ==============\n",
      "============== Pattern 1885 ==============\n",
      "============== Pattern 1886 ==============\n",
      "============== Pattern 1887 ==============\n",
      "============== Pattern 1888 ==============\n",
      "============== Pattern 1889 ==============\n",
      "============== Pattern 1890 ==============\n",
      "============== Pattern 1891 ==============\n",
      "============== Pattern 1892 ==============\n",
      "============== Pattern 1893 ==============\n",
      "============== Pattern 1894 ==============\n",
      "============== Pattern 1895 ==============\n",
      "============== Pattern 1896 ==============\n",
      "============== Pattern 1897 ==============\n",
      "============== Pattern 1898 ==============\n",
      "============== Pattern 1899 ==============\n",
      "============== Pattern 1900 ==============\n",
      "============== Pattern 1901 ==============\n",
      "============== Pattern 1902 ==============\n",
      "============== Pattern 1903 ==============\n",
      "============== Pattern 1904 ==============\n",
      "============== Pattern 1905 ==============\n",
      "============== Pattern 1906 ==============\n",
      "============== Pattern 1907 ==============\n",
      "============== Pattern 1908 ==============\n",
      "============== Pattern 1909 ==============\n",
      "============== Pattern 1910 ==============\n",
      "============== Pattern 1911 ==============\n",
      "============== Pattern 1912 ==============\n",
      "============== Pattern 1913 ==============\n",
      "============== Pattern 1914 ==============\n",
      "============== Pattern 1915 ==============\n",
      "============== Pattern 1916 ==============\n",
      "============== Pattern 1917 ==============\n",
      "============== Pattern 1918 ==============\n",
      "============== Pattern 1919 ==============\n",
      "============== Pattern 1920 ==============\n",
      "============== Pattern 1921 ==============\n",
      "============== Pattern 1922 ==============\n",
      "============== Pattern 1923 ==============\n",
      "============== Pattern 1924 ==============\n",
      "============== Pattern 1925 ==============\n",
      "============== Pattern 1926 ==============\n",
      "============== Pattern 1927 ==============\n",
      "============== Pattern 1928 ==============\n",
      "============== Pattern 1929 ==============\n",
      "============== Pattern 1930 ==============\n",
      "============== Pattern 1931 ==============\n",
      "============== Pattern 1932 ==============\n",
      "============== Pattern 1933 ==============\n",
      "============== Pattern 1934 ==============\n",
      "============== Pattern 1935 ==============\n",
      "============== Pattern 1936 ==============\n",
      "============== Pattern 1937 ==============\n",
      "============== Pattern 1938 ==============\n",
      "============== Pattern 1939 ==============\n",
      "============== Pattern 1940 ==============\n",
      "============== Pattern 1941 ==============\n",
      "============== Pattern 1942 ==============\n",
      "============== Pattern 1943 ==============\n",
      "============== Pattern 1944 ==============\n",
      "============== Pattern 1945 ==============\n",
      "============== Pattern 1946 ==============\n",
      "============== Pattern 1947 ==============\n",
      "============== Pattern 1948 ==============\n",
      "============== Pattern 1949 ==============\n",
      "============== Pattern 1950 ==============\n",
      "============== Pattern 1951 ==============\n",
      "============== Pattern 1952 ==============\n",
      "============== Pattern 1953 ==============\n",
      "============== Pattern 1954 ==============\n",
      "============== Pattern 1955 ==============\n",
      "============== Pattern 1956 ==============\n",
      "============== Pattern 1957 ==============\n",
      "============== Pattern 1958 ==============\n",
      "============== Pattern 1959 ==============\n",
      "============== Pattern 1960 ==============\n",
      "============== Pattern 1961 ==============\n",
      "============== Pattern 1962 ==============\n",
      "============== Pattern 1963 ==============\n",
      "============== Pattern 1964 ==============\n",
      "============== Pattern 1965 ==============\n",
      "============== Pattern 1966 ==============\n",
      "============== Pattern 1967 ==============\n",
      "============== Pattern 1968 ==============\n",
      "============== Pattern 1969 ==============\n",
      "============== Pattern 1970 ==============\n",
      "============== Pattern 1971 ==============\n",
      "============== Pattern 1972 ==============\n",
      "============== Pattern 1973 ==============\n",
      "============== Pattern 1974 ==============\n",
      "============== Pattern 1975 ==============\n",
      "============== Pattern 1976 ==============\n",
      "============== Pattern 1977 ==============\n",
      "============== Pattern 1978 ==============\n",
      "============== Pattern 1979 ==============\n",
      "============== Pattern 1980 ==============\n",
      "============== Pattern 1981 ==============\n",
      "============== Pattern 1982 ==============\n",
      "============== Pattern 1983 ==============\n",
      "============== Pattern 1984 ==============\n",
      "============== Pattern 1985 ==============\n",
      "============== Pattern 1986 ==============\n",
      "============== Pattern 1987 ==============\n",
      "============== Pattern 1988 ==============\n",
      "============== Pattern 1989 ==============\n",
      "============== Pattern 1990 ==============\n",
      "============== Pattern 1991 ==============\n",
      "============== Pattern 1992 ==============\n",
      "============== Pattern 1993 ==============\n",
      "============== Pattern 1994 ==============\n",
      "============== Pattern 1995 ==============\n",
      "============== Pattern 1996 ==============\n",
      "============== Pattern 1997 ==============\n",
      "============== Pattern 1998 ==============\n",
      "============== Pattern 1999 ==============\n",
      "============== Pattern 2000 ==============\n",
      "============== Pattern 2001 ==============\n",
      "============== Pattern 2002 ==============\n",
      "============== Pattern 2003 ==============\n",
      "============== Pattern 2004 ==============\n",
      "============== Pattern 2005 ==============\n",
      "============== Pattern 2006 ==============\n",
      "============== Pattern 2007 ==============\n",
      "============== Pattern 2008 ==============\n",
      "============== Pattern 2009 ==============\n",
      "============== Pattern 2010 ==============\n",
      "============== Pattern 2011 ==============\n",
      "============== Pattern 2012 ==============\n",
      "============== Pattern 2013 ==============\n",
      "============== Pattern 2014 ==============\n",
      "============== Pattern 2015 ==============\n",
      "============== Pattern 2016 ==============\n",
      "============== Pattern 2017 ==============\n",
      "============== Pattern 2018 ==============\n",
      "============== Pattern 2019 ==============\n",
      "============== Pattern 2020 ==============\n",
      "============== Pattern 2021 ==============\n",
      "============== Pattern 2022 ==============\n",
      "============== Pattern 2023 ==============\n",
      "============== Pattern 2024 ==============\n",
      "============== Pattern 2025 ==============\n",
      "============== Pattern 2026 ==============\n",
      "============== Pattern 2027 ==============\n",
      "============== Pattern 2028 ==============\n",
      "============== Pattern 2029 ==============\n",
      "============== Pattern 2030 ==============\n",
      "============== Pattern 2031 ==============\n",
      "============== Pattern 2032 ==============\n",
      "============== Pattern 2033 ==============\n",
      "============== Pattern 2034 ==============\n",
      "============== Pattern 2035 ==============\n",
      "============== Pattern 2036 ==============\n",
      "============== Pattern 2037 ==============\n",
      "============== Pattern 2038 ==============\n",
      "============== Pattern 2039 ==============\n",
      "============== Pattern 2040 ==============\n",
      "============== Pattern 2041 ==============\n",
      "============== Pattern 2042 ==============\n",
      "============== Pattern 2043 ==============\n",
      "============== Pattern 2044 ==============\n",
      "============== Pattern 2045 ==============\n",
      "============== Pattern 2046 ==============\n",
      "============== Pattern 2047 ==============\n",
      "============== Pattern 2048 ==============\n",
      "============== Pattern 2049 ==============\n",
      "============== Pattern 2050 ==============\n",
      "============== Pattern 2051 ==============\n",
      "============== Pattern 2052 ==============\n",
      "============== Pattern 2053 ==============\n",
      "============== Pattern 2054 ==============\n",
      "============== Pattern 2055 ==============\n",
      "============== Pattern 2056 ==============\n",
      "============== Pattern 2057 ==============\n",
      "============== Pattern 2058 ==============\n",
      "============== Pattern 2059 ==============\n",
      "============== Pattern 2060 ==============\n",
      "============== Pattern 2061 ==============\n",
      "============== Pattern 2062 ==============\n",
      "============== Pattern 2063 ==============\n",
      "============== Pattern 2064 ==============\n",
      "============== Pattern 2065 ==============\n",
      "============== Pattern 2066 ==============\n",
      "============== Pattern 2067 ==============\n",
      "============== Pattern 2068 ==============\n",
      "============== Pattern 2069 ==============\n",
      "============== Pattern 2070 ==============\n",
      "============== Pattern 2071 ==============\n",
      "============== Pattern 2072 ==============\n",
      "============== Pattern 2073 ==============\n",
      "============== Pattern 2074 ==============\n",
      "============== Pattern 2075 ==============\n",
      "============== Pattern 2076 ==============\n",
      "============== Pattern 2077 ==============\n",
      "============== Pattern 2078 ==============\n",
      "============== Pattern 2079 ==============\n",
      "============== Pattern 2080 ==============\n",
      "============== Pattern 2081 ==============\n",
      "============== Pattern 2082 ==============\n",
      "============== Pattern 2083 ==============\n",
      "============== Pattern 2084 ==============\n",
      "============== Pattern 2085 ==============\n",
      "============== Pattern 2086 ==============\n",
      "============== Pattern 2087 ==============\n",
      "============== Pattern 2088 ==============\n",
      "============== Pattern 2089 ==============\n",
      "============== Pattern 2090 ==============\n",
      "============== Pattern 2091 ==============\n",
      "============== Pattern 2092 ==============\n",
      "============== Pattern 2093 ==============\n",
      "============== Pattern 2094 ==============\n",
      "============== Pattern 2095 ==============\n",
      "============== Pattern 2096 ==============\n",
      "============== Pattern 2097 ==============\n",
      "============== Pattern 2098 ==============\n",
      "============== Pattern 2099 ==============\n",
      "============== Pattern 2100 ==============\n",
      "============== Pattern 2101 ==============\n",
      "============== Pattern 2102 ==============\n",
      "============== Pattern 2103 ==============\n",
      "============== Pattern 2104 ==============\n",
      "============== Pattern 2105 ==============\n",
      "============== Pattern 2106 ==============\n",
      "============== Pattern 2107 ==============\n",
      "============== Pattern 2108 ==============\n",
      "============== Pattern 2109 ==============\n",
      "============== Pattern 2110 ==============\n",
      "============== Pattern 2111 ==============\n",
      "============== Pattern 2112 ==============\n",
      "============== Pattern 2113 ==============\n",
      "============== Pattern 2114 ==============\n",
      "============== Pattern 2115 ==============\n",
      "============== Pattern 2116 ==============\n",
      "============== Pattern 2117 ==============\n",
      "============== Pattern 2118 ==============\n",
      "============== Pattern 2119 ==============\n",
      "============== Pattern 2120 ==============\n",
      "============== Pattern 2121 ==============\n",
      "============== Pattern 2122 ==============\n",
      "============== Pattern 2123 ==============\n",
      "============== Pattern 2124 ==============\n",
      "============== Pattern 2125 ==============\n",
      "============== Pattern 2126 ==============\n",
      "============== Pattern 2127 ==============\n",
      "============== Pattern 2128 ==============\n",
      "============== Pattern 2129 ==============\n",
      "============== Pattern 2130 ==============\n",
      "============== Pattern 2131 ==============\n",
      "============== Pattern 2132 ==============\n",
      "============== Pattern 2133 ==============\n",
      "============== Pattern 2134 ==============\n",
      "============== Pattern 2135 ==============\n",
      "============== Pattern 2136 ==============\n",
      "============== Pattern 2137 ==============\n",
      "============== Pattern 2138 ==============\n",
      "============== Pattern 2139 ==============\n",
      "============== Pattern 2140 ==============\n",
      "============== Pattern 2141 ==============\n",
      "============== Pattern 2142 ==============\n",
      "============== Pattern 2143 ==============\n",
      "============== Pattern 2144 ==============\n",
      "============== Pattern 2145 ==============\n",
      "============== Pattern 2146 ==============\n",
      "============== Pattern 2147 ==============\n",
      "============== Pattern 2148 ==============\n",
      "============== Pattern 2149 ==============\n",
      "============== Pattern 2150 ==============\n",
      "============== Pattern 2151 ==============\n",
      "============== Pattern 2152 ==============\n",
      "============== Pattern 2153 ==============\n",
      "============== Pattern 2154 ==============\n",
      "============== Pattern 2155 ==============\n",
      "============== Pattern 2156 ==============\n",
      "============== Pattern 2157 ==============\n",
      "============== Pattern 2158 ==============\n",
      "============== Pattern 2159 ==============\n",
      "============== Pattern 2160 ==============\n",
      "============== Pattern 2161 ==============\n",
      "============== Pattern 2162 ==============\n",
      "============== Pattern 2163 ==============\n",
      "============== Pattern 2164 ==============\n",
      "============== Pattern 2165 ==============\n",
      "============== Pattern 2166 ==============\n",
      "============== Pattern 2167 ==============\n",
      "============== Pattern 2168 ==============\n",
      "============== Pattern 2169 ==============\n",
      "============== Pattern 2170 ==============\n",
      "============== Pattern 2171 ==============\n",
      "============== Pattern 2172 ==============\n",
      "============== Pattern 2173 ==============\n",
      "============== Pattern 2174 ==============\n",
      "============== Pattern 2175 ==============\n",
      "============== Pattern 2176 ==============\n",
      "============== Pattern 2177 ==============\n",
      "============== Pattern 2178 ==============\n",
      "============== Pattern 2179 ==============\n",
      "============== Pattern 2180 ==============\n",
      "============== Pattern 2181 ==============\n",
      "============== Pattern 2182 ==============\n",
      "============== Pattern 2183 ==============\n",
      "============== Pattern 2184 ==============\n",
      "============== Pattern 2185 ==============\n",
      "============== Pattern 2186 ==============\n",
      "============== Pattern 2187 ==============\n",
      "============== Pattern 2188 ==============\n",
      "============== Pattern 2189 ==============\n",
      "============== Pattern 2190 ==============\n",
      "============== Pattern 2191 ==============\n",
      "============== Pattern 2192 ==============\n",
      "============== Pattern 2193 ==============\n",
      "============== Pattern 2194 ==============\n",
      "============== Pattern 2195 ==============\n",
      "============== Pattern 2196 ==============\n",
      "============== Pattern 2197 ==============\n",
      "============== Pattern 2198 ==============\n",
      "============== Pattern 2199 ==============\n",
      "============== Pattern 2200 ==============\n",
      "============== Pattern 2201 ==============\n",
      "============== Pattern 2202 ==============\n",
      "============== Pattern 2203 ==============\n",
      "============== Pattern 2204 ==============\n",
      "============== Pattern 2205 ==============\n",
      "============== Pattern 2206 ==============\n",
      "============== Pattern 2207 ==============\n",
      "============== Pattern 2208 ==============\n",
      "============== Pattern 2209 ==============\n",
      "============== Pattern 2210 ==============\n",
      "============== Pattern 2211 ==============\n",
      "============== Pattern 2212 ==============\n",
      "============== Pattern 2213 ==============\n",
      "============== Pattern 2214 ==============\n",
      "============== Pattern 2215 ==============\n",
      "============== Pattern 2216 ==============\n",
      "============== Pattern 2217 ==============\n",
      "============== Pattern 2218 ==============\n",
      "============== Pattern 2219 ==============\n",
      "============== Pattern 2220 ==============\n",
      "============== Pattern 2221 ==============\n",
      "============== Pattern 2222 ==============\n",
      "============== Pattern 2223 ==============\n",
      "============== Pattern 2224 ==============\n",
      "============== Pattern 2225 ==============\n",
      "============== Pattern 2226 ==============\n",
      "============== Pattern 2227 ==============\n",
      "============== Pattern 2228 ==============\n",
      "============== Pattern 2229 ==============\n",
      "============== Pattern 2230 ==============\n",
      "============== Pattern 2231 ==============\n",
      "============== Pattern 2232 ==============\n",
      "============== Pattern 2233 ==============\n",
      "============== Pattern 2234 ==============\n",
      "============== Pattern 2235 ==============\n",
      "============== Pattern 2236 ==============\n",
      "============== Pattern 2237 ==============\n",
      "============== Pattern 2238 ==============\n",
      "============== Pattern 2239 ==============\n",
      "============== Pattern 2240 ==============\n",
      "============== Pattern 2241 ==============\n",
      "============== Pattern 2242 ==============\n",
      "============== Pattern 2243 ==============\n",
      "============== Pattern 2244 ==============\n",
      "============== Pattern 2245 ==============\n",
      "============== Pattern 2246 ==============\n",
      "============== Pattern 2247 ==============\n",
      "============== Pattern 2248 ==============\n",
      "============== Pattern 2249 ==============\n",
      "============== Pattern 2250 ==============\n",
      "============== Pattern 2251 ==============\n",
      "============== Pattern 2252 ==============\n",
      "============== Pattern 2253 ==============\n",
      "============== Pattern 2254 ==============\n",
      "============== Pattern 2255 ==============\n",
      "============== Pattern 2256 ==============\n",
      "============== Pattern 2257 ==============\n",
      "============== Pattern 2258 ==============\n",
      "============== Pattern 2259 ==============\n",
      "============== Pattern 2260 ==============\n",
      "============== Pattern 2261 ==============\n",
      "============== Pattern 2262 ==============\n",
      "============== Pattern 2263 ==============\n",
      "============== Pattern 2264 ==============\n",
      "============== Pattern 2265 ==============\n",
      "============== Pattern 2266 ==============\n",
      "============== Pattern 2267 ==============\n",
      "============== Pattern 2268 ==============\n",
      "============== Pattern 2269 ==============\n",
      "============== Pattern 2270 ==============\n",
      "============== Pattern 2271 ==============\n",
      "============== Pattern 2272 ==============\n",
      "============== Pattern 2273 ==============\n",
      "============== Pattern 2274 ==============\n",
      "============== Pattern 2275 ==============\n",
      "============== Pattern 2276 ==============\n",
      "============== Pattern 2277 ==============\n",
      "============== Pattern 2278 ==============\n",
      "============== Pattern 2279 ==============\n",
      "============== Pattern 2280 ==============\n",
      "============== Pattern 2281 ==============\n",
      "============== Pattern 2282 ==============\n",
      "============== Pattern 2283 ==============\n",
      "============== Pattern 2284 ==============\n",
      "============== Pattern 2285 ==============\n",
      "============== Pattern 2286 ==============\n",
      "============== Pattern 2287 ==============\n",
      "============== Pattern 2288 ==============\n",
      "============== Pattern 2289 ==============\n",
      "============== Pattern 2290 ==============\n",
      "============== Pattern 2291 ==============\n",
      "============== Pattern 2292 ==============\n",
      "============== Pattern 2293 ==============\n",
      "============== Pattern 2294 ==============\n",
      "============== Pattern 2295 ==============\n",
      "============== Pattern 2296 ==============\n",
      "============== Pattern 2297 ==============\n",
      "============== Pattern 2298 ==============\n",
      "============== Pattern 2299 ==============\n",
      "============== Pattern 2300 ==============\n",
      "============== Pattern 2301 ==============\n",
      "============== Pattern 2302 ==============\n",
      "============== Pattern 2303 ==============\n",
      "============== Pattern 2304 ==============\n",
      "============== Pattern 2305 ==============\n",
      "============== Pattern 2306 ==============\n",
      "============== Pattern 2307 ==============\n",
      "============== Pattern 2308 ==============\n",
      "============== Pattern 2309 ==============\n",
      "============== Pattern 2310 ==============\n",
      "============== Pattern 2311 ==============\n",
      "============== Pattern 2312 ==============\n",
      "============== Pattern 2313 ==============\n",
      "============== Pattern 2314 ==============\n",
      "============== Pattern 2315 ==============\n",
      "============== Pattern 2316 ==============\n",
      "============== Pattern 2317 ==============\n",
      "============== Pattern 2318 ==============\n",
      "============== Pattern 2319 ==============\n",
      "============== Pattern 2320 ==============\n",
      "============== Pattern 2321 ==============\n",
      "============== Pattern 2322 ==============\n",
      "============== Pattern 2323 ==============\n",
      "============== Pattern 2324 ==============\n",
      "============== Pattern 2325 ==============\n",
      "============== Pattern 2326 ==============\n",
      "============== Pattern 2327 ==============\n",
      "============== Pattern 2328 ==============\n",
      "============== Pattern 2329 ==============\n",
      "============== Pattern 2330 ==============\n",
      "============== Pattern 2331 ==============\n",
      "============== Pattern 2332 ==============\n",
      "============== Pattern 2333 ==============\n",
      "============== Pattern 2334 ==============\n",
      "============== Pattern 2335 ==============\n",
      "============== Pattern 2336 ==============\n",
      "============== Pattern 2337 ==============\n",
      "============== Pattern 2338 ==============\n",
      "============== Pattern 2339 ==============\n",
      "============== Pattern 2340 ==============\n",
      "============== Pattern 2341 ==============\n",
      "============== Pattern 2342 ==============\n",
      "============== Pattern 2343 ==============\n",
      "============== Pattern 2344 ==============\n",
      "============== Pattern 2345 ==============\n",
      "============== Pattern 2346 ==============\n",
      "============== Pattern 2347 ==============\n",
      "============== Pattern 2348 ==============\n",
      "============== Pattern 2349 ==============\n",
      "============== Pattern 2350 ==============\n",
      "============== Pattern 2351 ==============\n",
      "============== Pattern 2352 ==============\n",
      "============== Pattern 2353 ==============\n",
      "============== Pattern 2354 ==============\n",
      "============== Pattern 2355 ==============\n",
      "============== Pattern 2356 ==============\n",
      "============== Pattern 2357 ==============\n",
      "============== Pattern 2358 ==============\n",
      "============== Pattern 2359 ==============\n",
      "============== Pattern 2360 ==============\n",
      "============== Pattern 2361 ==============\n",
      "============== Pattern 2362 ==============\n",
      "============== Pattern 2363 ==============\n",
      "============== Pattern 2364 ==============\n",
      "============== Pattern 2365 ==============\n",
      "============== Pattern 2366 ==============\n",
      "============== Pattern 2367 ==============\n",
      "============== Pattern 2368 ==============\n",
      "============== Pattern 2369 ==============\n",
      "============== Pattern 2370 ==============\n",
      "============== Pattern 2371 ==============\n",
      "============== Pattern 2372 ==============\n",
      "============== Pattern 2373 ==============\n",
      "============== Pattern 2374 ==============\n",
      "============== Pattern 2375 ==============\n",
      "============== Pattern 2376 ==============\n",
      "============== Pattern 2377 ==============\n",
      "============== Pattern 2378 ==============\n",
      "============== Pattern 2379 ==============\n",
      "============== Pattern 2380 ==============\n",
      "============== Pattern 2381 ==============\n",
      "============== Pattern 2382 ==============\n",
      "============== Pattern 2383 ==============\n",
      "============== Pattern 2384 ==============\n",
      "============== Pattern 2385 ==============\n",
      "============== Pattern 2386 ==============\n",
      "============== Pattern 2387 ==============\n",
      "============== Pattern 2388 ==============\n",
      "============== Pattern 2389 ==============\n",
      "============== Pattern 2390 ==============\n",
      "============== Pattern 2391 ==============\n",
      "============== Pattern 2392 ==============\n",
      "============== Pattern 2393 ==============\n",
      "============== Pattern 2394 ==============\n",
      "============== Pattern 2395 ==============\n",
      "============== Pattern 2396 ==============\n",
      "============== Pattern 2397 ==============\n",
      "============== Pattern 2398 ==============\n",
      "============== Pattern 2399 ==============\n",
      "============== Pattern 2400 ==============\n",
      "============== Pattern 2401 ==============\n",
      "============== Pattern 2402 ==============\n",
      "============== Pattern 2403 ==============\n",
      "============== Pattern 2404 ==============\n",
      "============== Pattern 2405 ==============\n",
      "============== Pattern 2406 ==============\n",
      "============== Pattern 2407 ==============\n",
      "============== Pattern 2408 ==============\n",
      "============== Pattern 2409 ==============\n",
      "============== Pattern 2410 ==============\n",
      "============== Pattern 2411 ==============\n",
      "============== Pattern 2412 ==============\n",
      "============== Pattern 2413 ==============\n",
      "============== Pattern 2414 ==============\n",
      "============== Pattern 2415 ==============\n",
      "============== Pattern 2416 ==============\n",
      "============== Pattern 2417 ==============\n",
      "============== Pattern 2418 ==============\n",
      "============== Pattern 2419 ==============\n",
      "============== Pattern 2420 ==============\n",
      "============== Pattern 2421 ==============\n",
      "============== Pattern 2422 ==============\n",
      "============== Pattern 2423 ==============\n",
      "============== Pattern 2424 ==============\n",
      "============== Pattern 2425 ==============\n",
      "============== Pattern 2426 ==============\n",
      "============== Pattern 2427 ==============\n",
      "============== Pattern 2428 ==============\n",
      "============== Pattern 2429 ==============\n",
      "============== Pattern 2430 ==============\n",
      "============== Pattern 2431 ==============\n",
      "============== Pattern 2432 ==============\n",
      "============== Pattern 2433 ==============\n",
      "============== Pattern 2434 ==============\n",
      "============== Pattern 2435 ==============\n",
      "============== Pattern 2436 ==============\n",
      "============== Pattern 2437 ==============\n",
      "============== Pattern 2438 ==============\n",
      "============== Pattern 2439 ==============\n",
      "============== Pattern 2440 ==============\n",
      "============== Pattern 2441 ==============\n",
      "============== Pattern 2442 ==============\n",
      "============== Pattern 2443 ==============\n",
      "============== Pattern 2444 ==============\n",
      "============== Pattern 2445 ==============\n",
      "============== Pattern 2446 ==============\n",
      "============== Pattern 2447 ==============\n",
      "============== Pattern 2448 ==============\n",
      "============== Pattern 2449 ==============\n",
      "============== Pattern 2450 ==============\n",
      "============== Pattern 2451 ==============\n",
      "============== Pattern 2452 ==============\n",
      "============== Pattern 2453 ==============\n",
      "============== Pattern 2454 ==============\n",
      "============== Pattern 2455 ==============\n",
      "============== Pattern 2456 ==============\n",
      "============== Pattern 2457 ==============\n",
      "============== Pattern 2458 ==============\n",
      "============== Pattern 2459 ==============\n",
      "============== Pattern 2460 ==============\n",
      "============== Pattern 2461 ==============\n",
      "============== Pattern 2462 ==============\n",
      "============== Pattern 2463 ==============\n",
      "============== Pattern 2464 ==============\n",
      "============== Pattern 2465 ==============\n",
      "============== Pattern 2466 ==============\n",
      "============== Pattern 2467 ==============\n",
      "============== Pattern 2468 ==============\n",
      "============== Pattern 2469 ==============\n",
      "============== Pattern 2470 ==============\n",
      "============== Pattern 2471 ==============\n",
      "============== Pattern 2472 ==============\n",
      "============== Pattern 2473 ==============\n",
      "============== Pattern 2474 ==============\n",
      "============== Pattern 2475 ==============\n",
      "============== Pattern 2476 ==============\n",
      "============== Pattern 2477 ==============\n",
      "============== Pattern 2478 ==============\n",
      "============== Pattern 2479 ==============\n",
      "============== Pattern 2480 ==============\n",
      "============== Pattern 2481 ==============\n",
      "============== Pattern 2482 ==============\n",
      "============== Pattern 2483 ==============\n",
      "============== Pattern 2484 ==============\n",
      "============== Pattern 2485 ==============\n",
      "============== Pattern 2486 ==============\n",
      "============== Pattern 2487 ==============\n",
      "============== Pattern 2488 ==============\n",
      "============== Pattern 2489 ==============\n",
      "============== Pattern 2490 ==============\n",
      "============== Pattern 2491 ==============\n",
      "============== Pattern 2492 ==============\n",
      "============== Pattern 2493 ==============\n",
      "============== Pattern 2494 ==============\n",
      "============== Pattern 2495 ==============\n",
      "============== Pattern 2496 ==============\n",
      "============== Pattern 2497 ==============\n",
      "============== Pattern 2498 ==============\n",
      "============== Pattern 2499 ==============\n",
      "============== Pattern 2500 ==============\n",
      "============== Pattern 2501 ==============\n",
      "============== Pattern 2502 ==============\n",
      "============== Pattern 2503 ==============\n",
      "============== Pattern 2504 ==============\n",
      "============== Pattern 2505 ==============\n",
      "============== Pattern 2506 ==============\n",
      "============== Pattern 2507 ==============\n",
      "============== Pattern 2508 ==============\n",
      "============== Pattern 2509 ==============\n",
      "============== Pattern 2510 ==============\n",
      "============== Pattern 2511 ==============\n",
      "============== Pattern 2512 ==============\n",
      "============== Pattern 2513 ==============\n",
      "============== Pattern 2514 ==============\n",
      "============== Pattern 2515 ==============\n",
      "============== Pattern 2516 ==============\n",
      "============== Pattern 2517 ==============\n",
      "============== Pattern 2518 ==============\n",
      "============== Pattern 2519 ==============\n",
      "============== Pattern 2520 ==============\n",
      "============== Pattern 2521 ==============\n",
      "============== Pattern 2522 ==============\n",
      "============== Pattern 2523 ==============\n",
      "============== Pattern 2524 ==============\n",
      "============== Pattern 2525 ==============\n",
      "============== Pattern 2526 ==============\n",
      "============== Pattern 2527 ==============\n",
      "============== Pattern 2528 ==============\n",
      "============== Pattern 2529 ==============\n",
      "============== Pattern 2530 ==============\n",
      "============== Pattern 2531 ==============\n",
      "============== Pattern 2532 ==============\n",
      "============== Pattern 2533 ==============\n",
      "============== Pattern 2534 ==============\n",
      "============== Pattern 2535 ==============\n",
      "============== Pattern 2536 ==============\n",
      "============== Pattern 2537 ==============\n",
      "============== Pattern 2538 ==============\n",
      "============== Pattern 2539 ==============\n",
      "============== Pattern 2540 ==============\n",
      "============== Pattern 2541 ==============\n",
      "============== Pattern 2542 ==============\n",
      "============== Pattern 2543 ==============\n",
      "============== Pattern 2544 ==============\n",
      "============== Pattern 2545 ==============\n",
      "============== Pattern 2546 ==============\n",
      "============== Pattern 2547 ==============\n",
      "============== Pattern 2548 ==============\n",
      "============== Pattern 2549 ==============\n",
      "============== Pattern 2550 ==============\n",
      "============== Pattern 2551 ==============\n",
      "============== Pattern 2552 ==============\n",
      "============== Pattern 2553 ==============\n",
      "============== Pattern 2554 ==============\n",
      "============== Pattern 2555 ==============\n",
      "============== Pattern 2556 ==============\n",
      "============== Pattern 2557 ==============\n",
      "============== Pattern 2558 ==============\n",
      "============== Pattern 2559 ==============\n",
      "============== Pattern 2560 ==============\n",
      "============== Pattern 2561 ==============\n",
      "============== Pattern 2562 ==============\n",
      "============== Pattern 2563 ==============\n",
      "============== Pattern 2564 ==============\n",
      "============== Pattern 2565 ==============\n",
      "============== Pattern 2566 ==============\n",
      "============== Pattern 2567 ==============\n",
      "============== Pattern 2568 ==============\n",
      "============== Pattern 2569 ==============\n",
      "============== Pattern 2570 ==============\n",
      "============== Pattern 2571 ==============\n",
      "============== Pattern 2572 ==============\n",
      "============== Pattern 2573 ==============\n",
      "============== Pattern 2574 ==============\n",
      "============== Pattern 2575 ==============\n",
      "============== Pattern 2576 ==============\n",
      "============== Pattern 2577 ==============\n",
      "============== Pattern 2578 ==============\n",
      "============== Pattern 2579 ==============\n",
      "============== Pattern 2580 ==============\n",
      "============== Pattern 2581 ==============\n",
      "============== Pattern 2582 ==============\n",
      "============== Pattern 2583 ==============\n",
      "============== Pattern 2584 ==============\n",
      "============== Pattern 2585 ==============\n",
      "============== Pattern 2586 ==============\n",
      "============== Pattern 2587 ==============\n",
      "============== Pattern 2588 ==============\n",
      "============== Pattern 2589 ==============\n",
      "============== Pattern 2590 ==============\n",
      "============== Pattern 2591 ==============\n",
      "============== Pattern 2592 ==============\n",
      "============== Pattern 2593 ==============\n",
      "============== Pattern 2594 ==============\n",
      "============== Pattern 2595 ==============\n",
      "============== Pattern 2596 ==============\n",
      "============== Pattern 2597 ==============\n",
      "============== Pattern 2598 ==============\n",
      "============== Pattern 2599 ==============\n",
      "============== Pattern 2600 ==============\n",
      "============== Pattern 2601 ==============\n",
      "============== Pattern 2602 ==============\n",
      "============== Pattern 2603 ==============\n",
      "============== Pattern 2604 ==============\n",
      "============== Pattern 2605 ==============\n",
      "============== Pattern 2606 ==============\n",
      "============== Pattern 2607 ==============\n",
      "============== Pattern 2608 ==============\n",
      "============== Pattern 2609 ==============\n",
      "============== Pattern 2610 ==============\n",
      "============== Pattern 2611 ==============\n",
      "============== Pattern 2612 ==============\n",
      "============== Pattern 2613 ==============\n",
      "============== Pattern 2614 ==============\n",
      "============== Pattern 2615 ==============\n",
      "============== Pattern 2616 ==============\n",
      "============== Pattern 2617 ==============\n",
      "============== Pattern 2618 ==============\n",
      "============== Pattern 2619 ==============\n",
      "============== Pattern 2620 ==============\n",
      "============== Pattern 2621 ==============\n",
      "============== Pattern 2622 ==============\n",
      "============== Pattern 2623 ==============\n",
      "============== Pattern 2624 ==============\n",
      "============== Pattern 2625 ==============\n",
      "============== Pattern 2626 ==============\n",
      "============== Pattern 2627 ==============\n",
      "============== Pattern 2628 ==============\n",
      "============== Pattern 2629 ==============\n",
      "============== Pattern 2630 ==============\n",
      "============== Pattern 2631 ==============\n",
      "============== Pattern 2632 ==============\n",
      "============== Pattern 2633 ==============\n",
      "============== Pattern 2634 ==============\n",
      "============== Pattern 2635 ==============\n",
      "============== Pattern 2636 ==============\n",
      "============== Pattern 2637 ==============\n",
      "============== Pattern 2638 ==============\n",
      "============== Pattern 2639 ==============\n",
      "============== Pattern 2640 ==============\n",
      "============== Pattern 2641 ==============\n",
      "============== Pattern 2642 ==============\n",
      "============== Pattern 2643 ==============\n",
      "============== Pattern 2644 ==============\n",
      "============== Pattern 2645 ==============\n",
      "============== Pattern 2646 ==============\n",
      "============== Pattern 2647 ==============\n",
      "============== Pattern 2648 ==============\n",
      "============== Pattern 2649 ==============\n",
      "============== Pattern 2650 ==============\n",
      "============== Pattern 2651 ==============\n",
      "============== Pattern 2652 ==============\n",
      "============== Pattern 2653 ==============\n",
      "============== Pattern 2654 ==============\n",
      "============== Pattern 2655 ==============\n",
      "============== Pattern 2656 ==============\n",
      "============== Pattern 2657 ==============\n",
      "============== Pattern 2658 ==============\n",
      "============== Pattern 2659 ==============\n",
      "============== Pattern 2660 ==============\n",
      "============== Pattern 2661 ==============\n",
      "============== Pattern 2662 ==============\n",
      "============== Pattern 2663 ==============\n",
      "============== Pattern 2664 ==============\n",
      "============== Pattern 2665 ==============\n",
      "============== Pattern 2666 ==============\n",
      "============== Pattern 2667 ==============\n",
      "============== Pattern 2668 ==============\n",
      "============== Pattern 2669 ==============\n",
      "============== Pattern 2670 ==============\n",
      "============== Pattern 2671 ==============\n",
      "============== Pattern 2672 ==============\n",
      "============== Pattern 2673 ==============\n",
      "============== Pattern 2674 ==============\n",
      "============== Pattern 2675 ==============\n",
      "============== Pattern 2676 ==============\n",
      "============== Pattern 2677 ==============\n",
      "============== Pattern 2678 ==============\n",
      "============== Pattern 2679 ==============\n",
      "============== Pattern 2680 ==============\n",
      "============== Pattern 2681 ==============\n",
      "============== Pattern 2682 ==============\n",
      "============== Pattern 2683 ==============\n",
      "============== Pattern 2684 ==============\n",
      "============== Pattern 2685 ==============\n",
      "============== Pattern 2686 ==============\n",
      "============== Pattern 2687 ==============\n",
      "============== Pattern 2688 ==============\n",
      "============== Pattern 2689 ==============\n",
      "============== Pattern 2690 ==============\n",
      "============== Pattern 2691 ==============\n",
      "============== Pattern 2692 ==============\n",
      "============== Pattern 2693 ==============\n",
      "============== Pattern 2694 ==============\n",
      "============== Pattern 2695 ==============\n",
      "============== Pattern 2696 ==============\n",
      "============== Pattern 2697 ==============\n",
      "============== Pattern 2698 ==============\n",
      "============== Pattern 2699 ==============\n",
      "============== Pattern 2700 ==============\n",
      "============== Pattern 2701 ==============\n",
      "============== Pattern 2702 ==============\n",
      "============== Pattern 2703 ==============\n",
      "============== Pattern 2704 ==============\n",
      "============== Pattern 2705 ==============\n",
      "============== Pattern 2706 ==============\n",
      "============== Pattern 2707 ==============\n",
      "============== Pattern 2708 ==============\n",
      "============== Pattern 2709 ==============\n",
      "============== Pattern 2710 ==============\n",
      "============== Pattern 2711 ==============\n",
      "============== Pattern 2712 ==============\n",
      "============== Pattern 2713 ==============\n",
      "============== Pattern 2714 ==============\n",
      "============== Pattern 2715 ==============\n",
      "============== Pattern 2716 ==============\n",
      "============== Pattern 2717 ==============\n",
      "============== Pattern 2718 ==============\n",
      "============== Pattern 2719 ==============\n",
      "============== Pattern 2720 ==============\n",
      "============== Pattern 2721 ==============\n",
      "============== Pattern 2722 ==============\n",
      "============== Pattern 2723 ==============\n",
      "============== Pattern 2724 ==============\n",
      "============== Pattern 2725 ==============\n",
      "============== Pattern 2726 ==============\n",
      "============== Pattern 2727 ==============\n",
      "============== Pattern 2728 ==============\n",
      "============== Pattern 2729 ==============\n",
      "============== Pattern 2730 ==============\n",
      "============== Pattern 2731 ==============\n",
      "============== Pattern 2732 ==============\n",
      "============== Pattern 2733 ==============\n",
      "============== Pattern 2734 ==============\n",
      "============== Pattern 2735 ==============\n",
      "============== Pattern 2736 ==============\n",
      "============== Pattern 2737 ==============\n",
      "============== Pattern 2738 ==============\n",
      "============== Pattern 2739 ==============\n",
      "============== Pattern 2740 ==============\n",
      "============== Pattern 2741 ==============\n",
      "============== Pattern 2742 ==============\n",
      "============== Pattern 2743 ==============\n",
      "============== Pattern 2744 ==============\n",
      "============== Pattern 2745 ==============\n",
      "============== Pattern 2746 ==============\n",
      "============== Pattern 2747 ==============\n",
      "============== Pattern 2748 ==============\n",
      "============== Pattern 2749 ==============\n",
      "============== Pattern 2750 ==============\n",
      "============== Pattern 2751 ==============\n",
      "============== Pattern 2752 ==============\n",
      "============== Pattern 2753 ==============\n",
      "============== Pattern 2754 ==============\n",
      "============== Pattern 2755 ==============\n",
      "============== Pattern 2756 ==============\n",
      "============== Pattern 2757 ==============\n",
      "============== Pattern 2758 ==============\n",
      "============== Pattern 2759 ==============\n",
      "============== Pattern 2760 ==============\n",
      "============== Pattern 2761 ==============\n",
      "============== Pattern 2762 ==============\n",
      "============== Pattern 2763 ==============\n",
      "============== Pattern 2764 ==============\n",
      "============== Pattern 2765 ==============\n",
      "============== Pattern 2766 ==============\n",
      "============== Pattern 2767 ==============\n",
      "============== Pattern 2768 ==============\n",
      "============== Pattern 2769 ==============\n",
      "============== Pattern 2770 ==============\n",
      "============== Pattern 2771 ==============\n",
      "============== Pattern 2772 ==============\n",
      "============== Pattern 2773 ==============\n",
      "============== Pattern 2774 ==============\n",
      "============== Pattern 2775 ==============\n",
      "============== Pattern 2776 ==============\n",
      "============== Pattern 2777 ==============\n",
      "============== Pattern 2778 ==============\n",
      "============== Pattern 2779 ==============\n",
      "============== Pattern 2780 ==============\n",
      "============== Pattern 2781 ==============\n",
      "============== Pattern 2782 ==============\n",
      "============== Pattern 2783 ==============\n",
      "============== Pattern 2784 ==============\n",
      "============== Pattern 2785 ==============\n",
      "============== Pattern 2786 ==============\n",
      "============== Pattern 2787 ==============\n",
      "============== Pattern 2788 ==============\n",
      "============== Pattern 2789 ==============\n",
      "============== Pattern 2790 ==============\n",
      "============== Pattern 2791 ==============\n",
      "============== Pattern 2792 ==============\n",
      "============== Pattern 2793 ==============\n",
      "============== Pattern 2794 ==============\n",
      "============== Pattern 2795 ==============\n",
      "============== Pattern 2796 ==============\n",
      "============== Pattern 2797 ==============\n",
      "============== Pattern 2798 ==============\n",
      "============== Pattern 2799 ==============\n",
      "============== Pattern 2800 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2801 ==============\n",
      "============== Pattern 2802 ==============\n",
      "============== Pattern 2803 ==============\n",
      "============== Pattern 2804 ==============\n",
      "============== Pattern 2805 ==============\n",
      "============== Pattern 2806 ==============\n",
      "============== Pattern 2807 ==============\n",
      "============== Pattern 2808 ==============\n",
      "============== Pattern 2809 ==============\n",
      "============== Pattern 2810 ==============\n",
      "============== Pattern 2811 ==============\n",
      "============== Pattern 2812 ==============\n",
      "============== Pattern 2813 ==============\n",
      "============== Pattern 2814 ==============\n",
      "============== Pattern 2815 ==============\n",
      "============== Pattern 2816 ==============\n",
      "============== Pattern 2817 ==============\n",
      "============== Pattern 2818 ==============\n",
      "============== Pattern 2819 ==============\n",
      "============== Pattern 2820 ==============\n",
      "============== Pattern 2821 ==============\n",
      "============== Pattern 2822 ==============\n",
      "============== Pattern 2823 ==============\n",
      "============== Pattern 2824 ==============\n",
      "============== Pattern 2825 ==============\n",
      "============== Pattern 2826 ==============\n",
      "============== Pattern 2827 ==============\n",
      "============== Pattern 2828 ==============\n",
      "============== Pattern 2829 ==============\n",
      "============== Pattern 2830 ==============\n",
      "============== Pattern 2831 ==============\n",
      "============== Pattern 2832 ==============\n",
      "============== Pattern 2833 ==============\n",
      "============== Pattern 2834 ==============\n",
      "============== Pattern 2835 ==============\n",
      "============== Pattern 2836 ==============\n",
      "============== Pattern 2837 ==============\n",
      "============== Pattern 2838 ==============\n",
      "============== Pattern 2839 ==============\n",
      "============== Pattern 2840 ==============\n",
      "============== Pattern 2841 ==============\n",
      "============== Pattern 2842 ==============\n",
      "============== Pattern 2843 ==============\n",
      "============== Pattern 2844 ==============\n",
      "============== Pattern 2845 ==============\n",
      "============== Pattern 2846 ==============\n",
      "============== Pattern 2847 ==============\n",
      "============== Pattern 2848 ==============\n",
      "============== Pattern 2849 ==============\n",
      "============== Pattern 2850 ==============\n",
      "============== Pattern 2851 ==============\n",
      "============== Pattern 2852 ==============\n",
      "============== Pattern 2853 ==============\n",
      "============== Pattern 2854 ==============\n",
      "============== Pattern 2855 ==============\n",
      "============== Pattern 2856 ==============\n",
      "============== Pattern 2857 ==============\n",
      "============== Pattern 2858 ==============\n",
      "============== Pattern 2859 ==============\n",
      "============== Pattern 2860 ==============\n",
      "============== Pattern 2861 ==============\n",
      "============== Pattern 2862 ==============\n",
      "============== Pattern 2863 ==============\n",
      "============== Pattern 2864 ==============\n",
      "============== Pattern 2865 ==============\n",
      "============== Pattern 2866 ==============\n",
      "============== Pattern 2867 ==============\n",
      "============== Pattern 2868 ==============\n",
      "============== Pattern 2869 ==============\n",
      "============== Pattern 2870 ==============\n",
      "============== Pattern 2871 ==============\n",
      "============== Pattern 2872 ==============\n",
      "============== Pattern 2873 ==============\n",
      "============== Pattern 2874 ==============\n",
      "============== Pattern 2875 ==============\n",
      "============== Pattern 2876 ==============\n",
      "============== Pattern 2877 ==============\n",
      "============== Pattern 2878 ==============\n",
      "============== Pattern 2879 ==============\n",
      "============== Pattern 2880 ==============\n",
      "============== Pattern 2881 ==============\n",
      "============== Pattern 2882 ==============\n",
      "============== Pattern 2883 ==============\n",
      "============== Pattern 2884 ==============\n",
      "============== Pattern 2885 ==============\n",
      "============== Pattern 2886 ==============\n",
      "============== Pattern 2887 ==============\n",
      "============== Pattern 2888 ==============\n",
      "============== Pattern 2889 ==============\n",
      "============== Pattern 2890 ==============\n",
      "============== Pattern 2891 ==============\n",
      "============== Pattern 2892 ==============\n",
      "============== Pattern 2893 ==============\n",
      "============== Pattern 2894 ==============\n",
      "============== Pattern 2895 ==============\n",
      "============== Pattern 2896 ==============\n",
      "============== Pattern 2897 ==============\n",
      "============== Pattern 2898 ==============\n",
      "============== Pattern 2899 ==============\n",
      "============== Pattern 2900 ==============\n",
      "============== Pattern 2901 ==============\n",
      "============== Pattern 2902 ==============\n",
      "============== Pattern 2903 ==============\n",
      "============== Pattern 2904 ==============\n",
      "============== Pattern 2905 ==============\n",
      "============== Pattern 2906 ==============\n",
      "============== Pattern 2907 ==============\n",
      "============== Pattern 2908 ==============\n",
      "============== Pattern 2909 ==============\n",
      "============== Pattern 2910 ==============\n",
      "============== Pattern 2911 ==============\n",
      "============== Pattern 2912 ==============\n",
      "============== Pattern 2913 ==============\n",
      "============== Pattern 2914 ==============\n",
      "============== Pattern 2915 ==============\n",
      "============== Pattern 2916 ==============\n",
      "============== Pattern 2917 ==============\n",
      "============== Pattern 2918 ==============\n",
      "============== Pattern 2919 ==============\n",
      "============== Pattern 2920 ==============\n",
      "============== Pattern 2921 ==============\n",
      "============== Pattern 2922 ==============\n",
      "============== Pattern 2923 ==============\n",
      "============== Pattern 2924 ==============\n",
      "============== Pattern 2925 ==============\n",
      "============== Pattern 2926 ==============\n",
      "============== Pattern 2927 ==============\n",
      "============== Pattern 2928 ==============\n",
      "============== Pattern 2929 ==============\n",
      "============== Pattern 2930 ==============\n",
      "============== Pattern 2931 ==============\n",
      "============== Pattern 2932 ==============\n",
      "============== Pattern 2933 ==============\n",
      "============== Pattern 2934 ==============\n",
      "============== Pattern 2935 ==============\n",
      "============== Pattern 2936 ==============\n",
      "============== Pattern 2937 ==============\n",
      "============== Pattern 2938 ==============\n",
      "============== Pattern 2939 ==============\n",
      "============== Pattern 2940 ==============\n",
      "============== Pattern 2941 ==============\n",
      "============== Pattern 2942 ==============\n",
      "============== Pattern 2943 ==============\n",
      "============== Pattern 2944 ==============\n",
      "============== Pattern 2945 ==============\n",
      "============== Pattern 2946 ==============\n",
      "============== Pattern 2947 ==============\n",
      "============== Pattern 2948 ==============\n",
      "============== Pattern 2949 ==============\n",
      "============== Pattern 2950 ==============\n",
      "============== Pattern 2951 ==============\n",
      "============== Pattern 2952 ==============\n",
      "============== Pattern 2953 ==============\n",
      "============== Pattern 2954 ==============\n",
      "============== Pattern 2955 ==============\n",
      "============== Pattern 2956 ==============\n",
      "============== Pattern 2957 ==============\n",
      "============== Pattern 2958 ==============\n",
      "============== Pattern 2959 ==============\n",
      "============== Pattern 2960 ==============\n",
      "============== Pattern 2961 ==============\n",
      "============== Pattern 2962 ==============\n",
      "============== Pattern 2963 ==============\n",
      "============== Pattern 2964 ==============\n",
      "============== Pattern 2965 ==============\n",
      "============== Pattern 2966 ==============\n",
      "============== Pattern 2967 ==============\n",
      "============== Pattern 2968 ==============\n",
      "============== Pattern 2969 ==============\n",
      "============== Pattern 2970 ==============\n",
      "============== Pattern 2971 ==============\n",
      "============== Pattern 2972 ==============\n",
      "============== Pattern 2973 ==============\n",
      "============== Pattern 2974 ==============\n",
      "============== Pattern 2975 ==============\n",
      "============== Pattern 2976 ==============\n",
      "============== Pattern 2977 ==============\n",
      "============== Pattern 2978 ==============\n",
      "============== Pattern 2979 ==============\n",
      "============== Pattern 2980 ==============\n",
      "============== Pattern 2981 ==============\n",
      "============== Pattern 2982 ==============\n",
      "============== Pattern 2983 ==============\n",
      "============== Pattern 2984 ==============\n",
      "============== Pattern 2985 ==============\n",
      "============== Pattern 2986 ==============\n",
      "============== Pattern 2987 ==============\n",
      "============== Pattern 2988 ==============\n",
      "============== Pattern 2989 ==============\n",
      "============== Pattern 2990 ==============\n",
      "============== Pattern 2991 ==============\n",
      "============== Pattern 2992 ==============\n",
      "============== Pattern 2993 ==============\n",
      "============== Pattern 2994 ==============\n",
      "============== Pattern 2995 ==============\n",
      "============== Pattern 2996 ==============\n",
      "============== Pattern 2997 ==============\n",
      "============== Pattern 2998 ==============\n",
      "============== Pattern 2999 ==============\n",
      "============== Pattern 3000 ==============\n",
      "============== Pattern 3001 ==============\n",
      "============== Pattern 3002 ==============\n",
      "============== Pattern 3003 ==============\n",
      "============== Pattern 3004 ==============\n",
      "============== Pattern 3005 ==============\n",
      "============== Pattern 3006 ==============\n",
      "============== Pattern 3007 ==============\n",
      "============== Pattern 3008 ==============\n",
      "============== Pattern 3009 ==============\n",
      "============== Pattern 3010 ==============\n",
      "============== Pattern 3011 ==============\n",
      "============== Pattern 3012 ==============\n",
      "============== Pattern 3013 ==============\n",
      "============== Pattern 3014 ==============\n",
      "============== Pattern 3015 ==============\n",
      "============== Pattern 3016 ==============\n",
      "============== Pattern 3017 ==============\n",
      "============== Pattern 3018 ==============\n",
      "============== Pattern 3019 ==============\n",
      "============== Pattern 3020 ==============\n",
      "============== Pattern 3021 ==============\n",
      "============== Pattern 3022 ==============\n",
      "============== Pattern 3023 ==============\n",
      "============== Pattern 3024 ==============\n",
      "============== Pattern 3025 ==============\n",
      "============== Pattern 3026 ==============\n",
      "============== Pattern 3027 ==============\n",
      "============== Pattern 3028 ==============\n",
      "============== Pattern 3029 ==============\n",
      "============== Pattern 3030 ==============\n",
      "============== Pattern 3031 ==============\n",
      "============== Pattern 3032 ==============\n",
      "============== Pattern 3033 ==============\n",
      "============== Pattern 3034 ==============\n",
      "============== Pattern 3035 ==============\n",
      "============== Pattern 3036 ==============\n",
      "============== Pattern 3037 ==============\n",
      "============== Pattern 3038 ==============\n",
      "============== Pattern 3039 ==============\n",
      "============== Pattern 3040 ==============\n",
      "============== Pattern 3041 ==============\n",
      "============== Pattern 3042 ==============\n",
      "============== Pattern 3043 ==============\n",
      "============== Pattern 3044 ==============\n",
      "============== Pattern 3045 ==============\n",
      "============== Pattern 3046 ==============\n",
      "============== Pattern 3047 ==============\n",
      "============== Pattern 3048 ==============\n",
      "============== Pattern 3049 ==============\n",
      "============== Pattern 3050 ==============\n",
      "============== Pattern 3051 ==============\n",
      "============== Pattern 3052 ==============\n",
      "============== Pattern 3053 ==============\n",
      "============== Pattern 3054 ==============\n",
      "============== Pattern 3055 ==============\n",
      "============== Pattern 3056 ==============\n",
      "============== Pattern 3057 ==============\n",
      "============== Pattern 3058 ==============\n",
      "============== Pattern 3059 ==============\n",
      "============== Pattern 3060 ==============\n",
      "============== Pattern 3061 ==============\n",
      "============== Pattern 3062 ==============\n",
      "============== Pattern 3063 ==============\n",
      "============== Pattern 3064 ==============\n",
      "============== Pattern 3065 ==============\n",
      "============== Pattern 3066 ==============\n",
      "============== Pattern 3067 ==============\n",
      "============== Pattern 3068 ==============\n",
      "============== Pattern 3069 ==============\n",
      "============== Pattern 3070 ==============\n",
      "============== Pattern 3071 ==============\n",
      "============== Pattern 3072 ==============\n",
      "============== Pattern 3073 ==============\n",
      "============== Pattern 3074 ==============\n",
      "============== Pattern 3075 ==============\n",
      "============== Pattern 3076 ==============\n",
      "============== Pattern 3077 ==============\n",
      "============== Pattern 3078 ==============\n",
      "============== Pattern 3079 ==============\n",
      "============== Pattern 3080 ==============\n",
      "============== Pattern 3081 ==============\n",
      "============== Pattern 3082 ==============\n",
      "============== Pattern 3083 ==============\n",
      "============== Pattern 3084 ==============\n",
      "============== Pattern 3085 ==============\n",
      "============== Pattern 3086 ==============\n",
      "============== Pattern 3087 ==============\n",
      "============== Pattern 3088 ==============\n",
      "============== Pattern 3089 ==============\n",
      "============== Pattern 3090 ==============\n",
      "============== Pattern 3091 ==============\n",
      "============== Pattern 3092 ==============\n",
      "============== Pattern 3093 ==============\n",
      "============== Pattern 3094 ==============\n",
      "============== Pattern 3095 ==============\n",
      "============== Pattern 3096 ==============\n",
      "============== Pattern 3097 ==============\n",
      "============== Pattern 3098 ==============\n",
      "============== Pattern 3099 ==============\n",
      "============== Pattern 3100 ==============\n",
      "============== Pattern 3101 ==============\n",
      "============== Pattern 3102 ==============\n",
      "============== Pattern 3103 ==============\n",
      "============== Pattern 3104 ==============\n",
      "============== Pattern 3105 ==============\n",
      "============== Pattern 3106 ==============\n",
      "============== Pattern 3107 ==============\n",
      "============== Pattern 3108 ==============\n",
      "============== Pattern 3109 ==============\n",
      "============== Pattern 3110 ==============\n",
      "============== Pattern 3111 ==============\n",
      "============== Pattern 3112 ==============\n",
      "============== Pattern 3113 ==============\n",
      "============== Pattern 3114 ==============\n",
      "============== Pattern 3115 ==============\n",
      "============== Pattern 3116 ==============\n",
      "============== Pattern 3117 ==============\n",
      "============== Pattern 3118 ==============\n",
      "============== Pattern 3119 ==============\n",
      "============== Pattern 3120 ==============\n",
      "============== Pattern 3121 ==============\n",
      "============== Pattern 3122 ==============\n",
      "============== Pattern 3123 ==============\n",
      "============== Pattern 3124 ==============\n",
      "============== Pattern 3125 ==============\n",
      "============== Pattern 3126 ==============\n",
      "============== Pattern 3127 ==============\n",
      "============== Pattern 3128 ==============\n",
      "============== Pattern 3129 ==============\n",
      "============== Pattern 3130 ==============\n",
      "============== Pattern 3131 ==============\n",
      "============== Pattern 3132 ==============\n",
      "============== Pattern 3133 ==============\n",
      "============== Pattern 3134 ==============\n",
      "============== Pattern 3135 ==============\n",
      "============== Pattern 3136 ==============\n",
      "============== Pattern 3137 ==============\n",
      "============== Pattern 3138 ==============\n",
      "============== Pattern 3139 ==============\n",
      "============== Pattern 3140 ==============\n",
      "============== Pattern 3141 ==============\n",
      "============== Pattern 3142 ==============\n",
      "============== Pattern 3143 ==============\n",
      "============== Pattern 3144 ==============\n",
      "============== Pattern 3145 ==============\n",
      "============== Pattern 3146 ==============\n",
      "============== Pattern 3147 ==============\n",
      "============== Pattern 3148 ==============\n",
      "============== Pattern 3149 ==============\n",
      "============== Pattern 3150 ==============\n",
      "============== Pattern 3151 ==============\n",
      "============== Pattern 3152 ==============\n",
      "============== Pattern 3153 ==============\n",
      "============== Pattern 3154 ==============\n",
      "============== Pattern 3155 ==============\n",
      "============== Pattern 3156 ==============\n",
      "============== Pattern 3157 ==============\n",
      "============== Pattern 3158 ==============\n",
      "============== Pattern 3159 ==============\n",
      "============== Pattern 3160 ==============\n",
      "============== Pattern 3161 ==============\n",
      "============== Pattern 3162 ==============\n",
      "============== Pattern 3163 ==============\n",
      "============== Pattern 3164 ==============\n",
      "============== Pattern 3165 ==============\n",
      "============== Pattern 3166 ==============\n",
      "============== Pattern 3167 ==============\n",
      "============== Pattern 3168 ==============\n",
      "============== Pattern 3169 ==============\n",
      "============== Pattern 3170 ==============\n",
      "============== Pattern 3171 ==============\n",
      "============== Pattern 3172 ==============\n",
      "============== Pattern 3173 ==============\n",
      "============== Pattern 3174 ==============\n",
      "============== Pattern 3175 ==============\n",
      "============== Pattern 3176 ==============\n",
      "============== Pattern 3177 ==============\n",
      "============== Pattern 3178 ==============\n",
      "============== Pattern 3179 ==============\n",
      "============== Pattern 3180 ==============\n",
      "============== Pattern 3181 ==============\n",
      "============== Pattern 3182 ==============\n",
      "============== Pattern 3183 ==============\n",
      "============== Pattern 3184 ==============\n",
      "============== Pattern 3185 ==============\n",
      "============== Pattern 3186 ==============\n",
      "============== Pattern 3187 ==============\n",
      "============== Pattern 3188 ==============\n",
      "============== Pattern 3189 ==============\n",
      "============== Pattern 3190 ==============\n",
      "============== Pattern 3191 ==============\n",
      "============== Pattern 3192 ==============\n",
      "============== Pattern 3193 ==============\n",
      "============== Pattern 3194 ==============\n",
      "============== Pattern 3195 ==============\n",
      "============== Pattern 3196 ==============\n",
      "============== Pattern 3197 ==============\n",
      "============== Pattern 3198 ==============\n",
      "============== Pattern 3199 ==============\n",
      "============== Pattern 3200 ==============\n",
      "============== Pattern 3201 ==============\n",
      "============== Pattern 3202 ==============\n",
      "============== Pattern 3203 ==============\n",
      "============== Pattern 3204 ==============\n",
      "============== Pattern 3205 ==============\n",
      "============== Pattern 3206 ==============\n",
      "============== Pattern 3207 ==============\n",
      "============== Pattern 3208 ==============\n",
      "============== Pattern 3209 ==============\n",
      "============== Pattern 3210 ==============\n",
      "============== Pattern 3211 ==============\n",
      "============== Pattern 3212 ==============\n",
      "============== Pattern 3213 ==============\n",
      "============== Pattern 3214 ==============\n",
      "============== Pattern 3215 ==============\n",
      "============== Pattern 3216 ==============\n",
      "============== Pattern 3217 ==============\n",
      "============== Pattern 3218 ==============\n",
      "============== Pattern 3219 ==============\n",
      "============== Pattern 3220 ==============\n",
      "============== Pattern 3221 ==============\n",
      "============== Pattern 3222 ==============\n",
      "============== Pattern 3223 ==============\n",
      "============== Pattern 3224 ==============\n",
      "============== Pattern 3225 ==============\n",
      "============== Pattern 3226 ==============\n",
      "============== Pattern 3227 ==============\n",
      "============== Pattern 3228 ==============\n",
      "============== Pattern 3229 ==============\n",
      "============== Pattern 3230 ==============\n",
      "============== Pattern 3231 ==============\n",
      "============== Pattern 3232 ==============\n",
      "============== Pattern 3233 ==============\n",
      "============== Pattern 3234 ==============\n",
      "============== Pattern 3235 ==============\n",
      "============== Pattern 3236 ==============\n",
      "============== Pattern 3237 ==============\n",
      "============== Pattern 3238 ==============\n",
      "============== Pattern 3239 ==============\n",
      "============== Pattern 3240 ==============\n",
      "============== Pattern 3241 ==============\n",
      "============== Pattern 3242 ==============\n",
      "============== Pattern 3243 ==============\n",
      "============== Pattern 3244 ==============\n",
      "============== Pattern 3245 ==============\n",
      "============== Pattern 3246 ==============\n",
      "============== Pattern 3247 ==============\n",
      "============== Pattern 3248 ==============\n",
      "============== Pattern 3249 ==============\n",
      "============== Pattern 3250 ==============\n",
      "============== Pattern 3251 ==============\n",
      "============== Pattern 3252 ==============\n",
      "============== Pattern 3253 ==============\n",
      "============== Pattern 3254 ==============\n",
      "============== Pattern 3255 ==============\n",
      "============== Pattern 3256 ==============\n",
      "============== Pattern 3257 ==============\n",
      "============== Pattern 3258 ==============\n",
      "============== Pattern 3259 ==============\n",
      "============== Pattern 3260 ==============\n",
      "============== Pattern 3261 ==============\n",
      "============== Pattern 3262 ==============\n",
      "============== Pattern 3263 ==============\n",
      "============== Pattern 3264 ==============\n",
      "============== Pattern 3265 ==============\n",
      "============== Pattern 3266 ==============\n",
      "============== Pattern 3267 ==============\n",
      "============== Pattern 3268 ==============\n",
      "============== Pattern 3269 ==============\n",
      "============== Pattern 3270 ==============\n",
      "============== Pattern 3271 ==============\n",
      "============== Pattern 3272 ==============\n",
      "============== Pattern 3273 ==============\n",
      "============== Pattern 3274 ==============\n",
      "============== Pattern 3275 ==============\n",
      "============== Pattern 3276 ==============\n",
      "============== Pattern 3277 ==============\n",
      "============== Pattern 3278 ==============\n",
      "============== Pattern 3279 ==============\n",
      "============== Pattern 3280 ==============\n",
      "============== Pattern 3281 ==============\n",
      "============== Pattern 3282 ==============\n",
      "============== Pattern 3283 ==============\n",
      "============== Pattern 3284 ==============\n",
      "============== Pattern 3285 ==============\n",
      "============== Pattern 3286 ==============\n",
      "============== Pattern 3287 ==============\n",
      "============== Pattern 3288 ==============\n",
      "============== Pattern 3289 ==============\n",
      "============== Pattern 3290 ==============\n",
      "============== Pattern 3291 ==============\n",
      "============== Pattern 3292 ==============\n",
      "============== Pattern 3293 ==============\n",
      "============== Pattern 3294 ==============\n",
      "============== Pattern 3295 ==============\n",
      "============== Pattern 3296 ==============\n",
      "============== Pattern 3297 ==============\n",
      "============== Pattern 3298 ==============\n",
      "============== Pattern 3299 ==============\n",
      "============== Pattern 3300 ==============\n",
      "============== Pattern 3301 ==============\n",
      "============== Pattern 3302 ==============\n",
      "============== Pattern 3303 ==============\n",
      "============== Pattern 3304 ==============\n",
      "============== Pattern 3305 ==============\n",
      "============== Pattern 3306 ==============\n",
      "============== Pattern 3307 ==============\n",
      "============== Pattern 3308 ==============\n",
      "============== Pattern 3309 ==============\n",
      "============== Pattern 3310 ==============\n",
      "============== Pattern 3311 ==============\n",
      "============== Pattern 3312 ==============\n",
      "============== Pattern 3313 ==============\n",
      "============== Pattern 3314 ==============\n",
      "============== Pattern 3315 ==============\n",
      "============== Pattern 3316 ==============\n",
      "============== Pattern 3317 ==============\n",
      "============== Pattern 3318 ==============\n",
      "============== Pattern 3319 ==============\n",
      "============== Pattern 3320 ==============\n",
      "============== Pattern 3321 ==============\n",
      "============== Pattern 3322 ==============\n",
      "============== Pattern 3323 ==============\n",
      "============== Pattern 3324 ==============\n",
      "============== Pattern 3325 ==============\n",
      "============== Pattern 3326 ==============\n",
      "============== Pattern 3327 ==============\n",
      "============== Pattern 3328 ==============\n",
      "============== Pattern 3329 ==============\n",
      "============== Pattern 3330 ==============\n",
      "============== Pattern 3331 ==============\n",
      "============== Pattern 3332 ==============\n",
      "============== Pattern 3333 ==============\n",
      "============== Pattern 3334 ==============\n",
      "============== Pattern 3335 ==============\n",
      "============== Pattern 3336 ==============\n",
      "============== Pattern 3337 ==============\n",
      "============== Pattern 3338 ==============\n",
      "============== Pattern 3339 ==============\n",
      "============== Pattern 3340 ==============\n",
      "============== Pattern 3341 ==============\n",
      "============== Pattern 3342 ==============\n",
      "============== Pattern 3343 ==============\n",
      "============== Pattern 3344 ==============\n",
      "============== Pattern 3345 ==============\n",
      "============== Pattern 3346 ==============\n",
      "============== Pattern 3347 ==============\n",
      "============== Pattern 3348 ==============\n",
      "============== Pattern 3349 ==============\n",
      "============== Pattern 3350 ==============\n",
      "============== Pattern 3351 ==============\n",
      "============== Pattern 3352 ==============\n",
      "============== Pattern 3353 ==============\n",
      "============== Pattern 3354 ==============\n",
      "============== Pattern 3355 ==============\n",
      "============== Pattern 3356 ==============\n",
      "============== Pattern 3357 ==============\n",
      "============== Pattern 3358 ==============\n",
      "============== Pattern 3359 ==============\n",
      "============== Pattern 3360 ==============\n",
      "============== Pattern 3361 ==============\n",
      "============== Pattern 3362 ==============\n",
      "============== Pattern 3363 ==============\n",
      "============== Pattern 3364 ==============\n",
      "============== Pattern 3365 ==============\n",
      "============== Pattern 3366 ==============\n",
      "============== Pattern 3367 ==============\n",
      "============== Pattern 3368 ==============\n",
      "============== Pattern 3369 ==============\n",
      "============== Pattern 3370 ==============\n",
      "============== Pattern 3371 ==============\n",
      "============== Pattern 3372 ==============\n",
      "============== Pattern 3373 ==============\n",
      "============== Pattern 3374 ==============\n",
      "============== Pattern 3375 ==============\n",
      "============== Pattern 3376 ==============\n",
      "============== Pattern 3377 ==============\n",
      "============== Pattern 3378 ==============\n",
      "============== Pattern 3379 ==============\n",
      "============== Pattern 3380 ==============\n",
      "============== Pattern 3381 ==============\n",
      "============== Pattern 3382 ==============\n",
      "============== Pattern 3383 ==============\n",
      "============== Pattern 3384 ==============\n",
      "============== Pattern 3385 ==============\n",
      "============== Pattern 3386 ==============\n",
      "============== Pattern 3387 ==============\n",
      "============== Pattern 3388 ==============\n",
      "============== Pattern 3389 ==============\n",
      "============== Pattern 3390 ==============\n",
      "============== Pattern 3391 ==============\n",
      "============== Pattern 3392 ==============\n",
      "============== Pattern 3393 ==============\n",
      "============== Pattern 3394 ==============\n",
      "============== Pattern 3395 ==============\n",
      "============== Pattern 3396 ==============\n",
      "============== Pattern 3397 ==============\n",
      "============== Pattern 3398 ==============\n",
      "============== Pattern 3399 ==============\n",
      "============== Pattern 3400 ==============\n",
      "============== Pattern 3401 ==============\n",
      "============== Pattern 3402 ==============\n",
      "============== Pattern 3403 ==============\n",
      "============== Pattern 3404 ==============\n",
      "============== Pattern 3405 ==============\n",
      "============== Pattern 3406 ==============\n",
      "============== Pattern 3407 ==============\n",
      "============== Pattern 3408 ==============\n",
      "============== Pattern 3409 ==============\n",
      "============== Pattern 3410 ==============\n",
      "============== Pattern 3411 ==============\n",
      "============== Pattern 3412 ==============\n",
      "============== Pattern 3413 ==============\n",
      "============== Pattern 3414 ==============\n",
      "============== Pattern 3415 ==============\n",
      "============== Pattern 3416 ==============\n",
      "============== Pattern 3417 ==============\n",
      "============== Pattern 3418 ==============\n",
      "============== Pattern 3419 ==============\n",
      "============== Pattern 3420 ==============\n",
      "============== Pattern 3421 ==============\n",
      "============== Pattern 3422 ==============\n",
      "============== Pattern 3423 ==============\n",
      "============== Pattern 3424 ==============\n",
      "============== Pattern 3425 ==============\n",
      "============== Pattern 3426 ==============\n",
      "============== Pattern 3427 ==============\n",
      "============== Pattern 3428 ==============\n",
      "============== Pattern 3429 ==============\n",
      "============== Pattern 3430 ==============\n",
      "============== Pattern 3431 ==============\n",
      "============== Pattern 3432 ==============\n",
      "============== Pattern 3433 ==============\n",
      "============== Pattern 3434 ==============\n",
      "============== Pattern 3435 ==============\n",
      "============== Pattern 3436 ==============\n",
      "============== Pattern 3437 ==============\n",
      "============== Pattern 3438 ==============\n",
      "============== Pattern 3439 ==============\n",
      "============== Pattern 3440 ==============\n",
      "============== Pattern 3441 ==============\n",
      "============== Pattern 3442 ==============\n",
      "============== Pattern 3443 ==============\n",
      "============== Pattern 3444 ==============\n",
      "============== Pattern 3445 ==============\n",
      "============== Pattern 3446 ==============\n",
      "============== Pattern 3447 ==============\n",
      "============== Pattern 3448 ==============\n",
      "============== Pattern 3449 ==============\n",
      "============== Pattern 3450 ==============\n",
      "============== Pattern 3451 ==============\n",
      "============== Pattern 3452 ==============\n",
      "============== Pattern 3453 ==============\n",
      "============== Pattern 3454 ==============\n",
      "============== Pattern 3455 ==============\n",
      "============== Pattern 3456 ==============\n",
      "============== Pattern 3457 ==============\n",
      "============== Pattern 3458 ==============\n",
      "============== Pattern 3459 ==============\n",
      "============== Pattern 3460 ==============\n",
      "============== Pattern 3461 ==============\n",
      "============== Pattern 3462 ==============\n",
      "============== Pattern 3463 ==============\n",
      "============== Pattern 3464 ==============\n",
      "============== Pattern 3465 ==============\n",
      "Average comprehensibility: 60.38787878787879\n",
      "std comprehensibility: 4.175330936554858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var comprehensibility: 17.433388429752068\n",
      "minimum comprehensibility: 30\n",
      "maximum comprehensibility: 68\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
