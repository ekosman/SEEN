{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 32\n",
    "tree_depth = 12\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.161576271057129 | KNN Loss: 6.226352691650391 | BCE Loss: 1.9352238178253174\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.159000396728516 | KNN Loss: 6.226648807525635 | BCE Loss: 1.932352066040039\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.188031196594238 | KNN Loss: 6.226075172424316 | BCE Loss: 1.9619560241699219\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.213977813720703 | KNN Loss: 6.225383758544922 | BCE Loss: 1.9885936975479126\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.180749893188477 | KNN Loss: 6.225130081176758 | BCE Loss: 1.9556200504302979\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.228500366210938 | KNN Loss: 6.224370956420898 | BCE Loss: 2.004129409790039\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.17105484008789 | KNN Loss: 6.22470760345459 | BCE Loss: 1.9463468790054321\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.153498649597168 | KNN Loss: 6.224190711975098 | BCE Loss: 1.9293080568313599\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.09954833984375 | KNN Loss: 6.223996162414551 | BCE Loss: 1.8755519390106201\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.188777923583984 | KNN Loss: 6.223495006561279 | BCE Loss: 1.9652825593948364\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.069656372070312 | KNN Loss: 6.223164081573486 | BCE Loss: 1.846492052078247\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.098255157470703 | KNN Loss: 6.222594738006592 | BCE Loss: 1.8756605386734009\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.092670440673828 | KNN Loss: 6.222278118133545 | BCE Loss: 1.8703924417495728\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.083723068237305 | KNN Loss: 6.222372531890869 | BCE Loss: 1.8613505363464355\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.086665153503418 | KNN Loss: 6.221867561340332 | BCE Loss: 1.8647979497909546\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.068214416503906 | KNN Loss: 6.221030235290527 | BCE Loss: 1.847184419631958\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.065683364868164 | KNN Loss: 6.220468997955322 | BCE Loss: 1.8452142477035522\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.030014991760254 | KNN Loss: 6.219931602478027 | BCE Loss: 1.8100836277008057\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.097970962524414 | KNN Loss: 6.2190470695495605 | BCE Loss: 1.8789243698120117\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.037774085998535 | KNN Loss: 6.218971252441406 | BCE Loss: 1.818802833557129\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.012804985046387 | KNN Loss: 6.217867851257324 | BCE Loss: 1.7949373722076416\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.070337295532227 | KNN Loss: 6.217059135437012 | BCE Loss: 1.853278636932373\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.99405574798584 | KNN Loss: 6.216683387756348 | BCE Loss: 1.7773725986480713\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.9973225593566895 | KNN Loss: 6.214766502380371 | BCE Loss: 1.7825559377670288\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.985340595245361 | KNN Loss: 6.2135162353515625 | BCE Loss: 1.7718243598937988\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.972363471984863 | KNN Loss: 6.212133407592773 | BCE Loss: 1.760230302810669\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.973086833953857 | KNN Loss: 6.211043834686279 | BCE Loss: 1.7620429992675781\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.95546817779541 | KNN Loss: 6.210241317749023 | BCE Loss: 1.7452269792556763\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.930292129516602 | KNN Loss: 6.207434177398682 | BCE Loss: 1.7228577136993408\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.953562259674072 | KNN Loss: 6.205725193023682 | BCE Loss: 1.7478370666503906\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.914057731628418 | KNN Loss: 6.204119682312012 | BCE Loss: 1.7099381685256958\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.8967695236206055 | KNN Loss: 6.2002644538879395 | BCE Loss: 1.6965049505233765\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.849244117736816 | KNN Loss: 6.196889400482178 | BCE Loss: 1.6523544788360596\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.841864585876465 | KNN Loss: 6.198394298553467 | BCE Loss: 1.6434701681137085\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.843667507171631 | KNN Loss: 6.191394329071045 | BCE Loss: 1.652273178100586\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.867297172546387 | KNN Loss: 6.188953399658203 | BCE Loss: 1.6783437728881836\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.810844898223877 | KNN Loss: 6.185260772705078 | BCE Loss: 1.6255842447280884\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.811498641967773 | KNN Loss: 6.179890155792236 | BCE Loss: 1.631608247756958\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.756295680999756 | KNN Loss: 6.174192428588867 | BCE Loss: 1.5821031332015991\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.723996162414551 | KNN Loss: 6.165309429168701 | BCE Loss: 1.55868661403656\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.743303298950195 | KNN Loss: 6.1554179191589355 | BCE Loss: 1.5878853797912598\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.68756628036499 | KNN Loss: 6.148885726928711 | BCE Loss: 1.5386805534362793\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.6908674240112305 | KNN Loss: 6.13879919052124 | BCE Loss: 1.5520682334899902\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.6822357177734375 | KNN Loss: 6.1348958015441895 | BCE Loss: 1.5473401546478271\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.616951942443848 | KNN Loss: 6.105294704437256 | BCE Loss: 1.5116569995880127\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.608304977416992 | KNN Loss: 6.095808506011963 | BCE Loss: 1.5124962329864502\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.554848670959473 | KNN Loss: 6.0756330490112305 | BCE Loss: 1.4792158603668213\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.516475200653076 | KNN Loss: 6.063202381134033 | BCE Loss: 1.453272819519043\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.464408874511719 | KNN Loss: 6.031014442443848 | BCE Loss: 1.433394193649292\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.366040229797363 | KNN Loss: 5.997008323669434 | BCE Loss: 1.3690316677093506\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.350461483001709 | KNN Loss: 5.958778381347656 | BCE Loss: 1.3916832208633423\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.354300498962402 | KNN Loss: 5.932717323303223 | BCE Loss: 1.4215829372406006\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.2316365242004395 | KNN Loss: 5.875561237335205 | BCE Loss: 1.3560752868652344\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.192459583282471 | KNN Loss: 5.841818332672119 | BCE Loss: 1.350641131401062\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.041247367858887 | KNN Loss: 5.767274856567383 | BCE Loss: 1.273972511291504\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.044648170471191 | KNN Loss: 5.734325885772705 | BCE Loss: 1.3103225231170654\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.939826488494873 | KNN Loss: 5.6758131980896 | BCE Loss: 1.2640132904052734\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.89492654800415 | KNN Loss: 5.594135284423828 | BCE Loss: 1.3007912635803223\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.788924217224121 | KNN Loss: 5.521265029907227 | BCE Loss: 1.2676594257354736\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.5871381759643555 | KNN Loss: 5.415383815765381 | BCE Loss: 1.1717543601989746\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.502552509307861 | KNN Loss: 5.3158955574035645 | BCE Loss: 1.1866570711135864\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.439037322998047 | KNN Loss: 5.252435684204102 | BCE Loss: 1.1866014003753662\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.379894256591797 | KNN Loss: 5.159494400024414 | BCE Loss: 1.220400094985962\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.150590896606445 | KNN Loss: 5.030452251434326 | BCE Loss: 1.1201386451721191\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.086978912353516 | KNN Loss: 4.939206123352051 | BCE Loss: 1.147773027420044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 6.008642196655273 | KNN Loss: 4.881716251373291 | BCE Loss: 1.1269259452819824\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.880500793457031 | KNN Loss: 4.756161212921143 | BCE Loss: 1.1243395805358887\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.784660816192627 | KNN Loss: 4.677248954772949 | BCE Loss: 1.1074117422103882\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.713019847869873 | KNN Loss: 4.59433126449585 | BCE Loss: 1.118688702583313\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.579710006713867 | KNN Loss: 4.452706813812256 | BCE Loss: 1.1270031929016113\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.4775896072387695 | KNN Loss: 4.382556438446045 | BCE Loss: 1.0950329303741455\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.459367752075195 | KNN Loss: 4.336392402648926 | BCE Loss: 1.1229753494262695\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.407453536987305 | KNN Loss: 4.294209957122803 | BCE Loss: 1.113243818283081\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.327317237854004 | KNN Loss: 4.237171173095703 | BCE Loss: 1.0901458263397217\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.284808158874512 | KNN Loss: 4.182506561279297 | BCE Loss: 1.1023015975952148\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.225720405578613 | KNN Loss: 4.129440784454346 | BCE Loss: 1.0962798595428467\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.21074104309082 | KNN Loss: 4.090064525604248 | BCE Loss: 1.1206762790679932\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.202098846435547 | KNN Loss: 4.072436809539795 | BCE Loss: 1.1296619176864624\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.125721454620361 | KNN Loss: 4.057973861694336 | BCE Loss: 1.0677475929260254\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.159777641296387 | KNN Loss: 4.0616774559021 | BCE Loss: 1.0981004238128662\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.120649337768555 | KNN Loss: 4.024700164794922 | BCE Loss: 1.0959489345550537\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.064682960510254 | KNN Loss: 3.9634170532226562 | BCE Loss: 1.1012661457061768\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.036537170410156 | KNN Loss: 3.974785089492798 | BCE Loss: 1.0617520809173584\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 5.03837251663208 | KNN Loss: 3.95028018951416 | BCE Loss: 1.08809232711792\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 4.99551248550415 | KNN Loss: 3.924945592880249 | BCE Loss: 1.0705668926239014\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 5.038726329803467 | KNN Loss: 3.978780508041382 | BCE Loss: 1.059945821762085\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.984103202819824 | KNN Loss: 3.92848801612854 | BCE Loss: 1.0556151866912842\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.954278469085693 | KNN Loss: 3.917090654373169 | BCE Loss: 1.0371878147125244\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 5.008635520935059 | KNN Loss: 3.9136080741882324 | BCE Loss: 1.0950274467468262\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 5.031431674957275 | KNN Loss: 3.9454588890075684 | BCE Loss: 1.0859726667404175\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.001941680908203 | KNN Loss: 3.9079935550689697 | BCE Loss: 1.0939483642578125\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 5.008861541748047 | KNN Loss: 3.919282913208008 | BCE Loss: 1.0895785093307495\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.9350481033325195 | KNN Loss: 3.861217975616455 | BCE Loss: 1.0738301277160645\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.926103591918945 | KNN Loss: 3.8586931228637695 | BCE Loss: 1.0674102306365967\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 4.991544246673584 | KNN Loss: 3.9247069358825684 | BCE Loss: 1.066837191581726\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.8959856033325195 | KNN Loss: 3.8312721252441406 | BCE Loss: 1.064713478088379\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.976751804351807 | KNN Loss: 3.9226694107055664 | BCE Loss: 1.0540822744369507\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.985251426696777 | KNN Loss: 3.914769172668457 | BCE Loss: 1.0704823732376099\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.945215702056885 | KNN Loss: 3.8824117183685303 | BCE Loss: 1.0628039836883545\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.933901786804199 | KNN Loss: 3.8351452350616455 | BCE Loss: 1.0987566709518433\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.929329872131348 | KNN Loss: 3.8649511337280273 | BCE Loss: 1.0643789768218994\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.9129533767700195 | KNN Loss: 3.842810869216919 | BCE Loss: 1.0701422691345215\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.944995880126953 | KNN Loss: 3.8772337436676025 | BCE Loss: 1.0677622556686401\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.928296089172363 | KNN Loss: 3.88303542137146 | BCE Loss: 1.0452606678009033\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.942607879638672 | KNN Loss: 3.8782596588134766 | BCE Loss: 1.0643484592437744\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.876293182373047 | KNN Loss: 3.8411076068878174 | BCE Loss: 1.0351853370666504\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.9489641189575195 | KNN Loss: 3.8702962398529053 | BCE Loss: 1.0786681175231934\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.930630683898926 | KNN Loss: 3.8604044914245605 | BCE Loss: 1.0702261924743652\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.885526657104492 | KNN Loss: 3.827153444290161 | BCE Loss: 1.058372974395752\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.956915378570557 | KNN Loss: 3.8856849670410156 | BCE Loss: 1.0712305307388306\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.920953273773193 | KNN Loss: 3.8632142543792725 | BCE Loss: 1.0577389001846313\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.911037445068359 | KNN Loss: 3.8391895294189453 | BCE Loss: 1.071847677230835\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.926963806152344 | KNN Loss: 3.8682327270507812 | BCE Loss: 1.0587313175201416\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.901252746582031 | KNN Loss: 3.847689390182495 | BCE Loss: 1.0535635948181152\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.870549201965332 | KNN Loss: 3.8086609840393066 | BCE Loss: 1.0618884563446045\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.935188293457031 | KNN Loss: 3.848137855529785 | BCE Loss: 1.087050199508667\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.937593460083008 | KNN Loss: 3.860994577407837 | BCE Loss: 1.0765986442565918\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.884746551513672 | KNN Loss: 3.8264923095703125 | BCE Loss: 1.0582542419433594\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.890594005584717 | KNN Loss: 3.8379580974578857 | BCE Loss: 1.052635908126831\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.861795902252197 | KNN Loss: 3.8334462642669678 | BCE Loss: 1.0283496379852295\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.901845932006836 | KNN Loss: 3.849449634552002 | BCE Loss: 1.052396535873413\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.876337051391602 | KNN Loss: 3.8370730876922607 | BCE Loss: 1.0392639636993408\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.902900695800781 | KNN Loss: 3.8224875926971436 | BCE Loss: 1.0804128646850586\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.870331764221191 | KNN Loss: 3.8409881591796875 | BCE Loss: 1.029343843460083\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.863180160522461 | KNN Loss: 3.8099608421325684 | BCE Loss: 1.0532190799713135\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.865755081176758 | KNN Loss: 3.811995506286621 | BCE Loss: 1.0537595748901367\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.855825424194336 | KNN Loss: 3.8051750659942627 | BCE Loss: 1.0506503582000732\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.898254871368408 | KNN Loss: 3.8437013626098633 | BCE Loss: 1.0545536279678345\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.857398986816406 | KNN Loss: 3.819715976715088 | BCE Loss: 1.0376827716827393\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.8727545738220215 | KNN Loss: 3.794776678085327 | BCE Loss: 1.0779780149459839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.889537811279297 | KNN Loss: 3.8321566581726074 | BCE Loss: 1.0573813915252686\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.884041786193848 | KNN Loss: 3.817591905593872 | BCE Loss: 1.0664498805999756\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.904548645019531 | KNN Loss: 3.816315174102783 | BCE Loss: 1.0882335901260376\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.870769023895264 | KNN Loss: 3.8403100967407227 | BCE Loss: 1.030458927154541\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.880009651184082 | KNN Loss: 3.8069984912872314 | BCE Loss: 1.0730112791061401\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.868500709533691 | KNN Loss: 3.8365886211395264 | BCE Loss: 1.0319123268127441\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.897292613983154 | KNN Loss: 3.8595547676086426 | BCE Loss: 1.0377379655838013\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.877297401428223 | KNN Loss: 3.8279037475585938 | BCE Loss: 1.0493934154510498\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.914737224578857 | KNN Loss: 3.843266248703003 | BCE Loss: 1.0714709758758545\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.842731475830078 | KNN Loss: 3.8076331615448 | BCE Loss: 1.0350980758666992\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.9113264083862305 | KNN Loss: 3.837113857269287 | BCE Loss: 1.0742125511169434\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.923915386199951 | KNN Loss: 3.8682243824005127 | BCE Loss: 1.055691123008728\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.853078842163086 | KNN Loss: 3.7848336696624756 | BCE Loss: 1.0682449340820312\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.834965705871582 | KNN Loss: 3.7892813682556152 | BCE Loss: 1.0456843376159668\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.867203235626221 | KNN Loss: 3.8068180084228516 | BCE Loss: 1.0603852272033691\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.870003700256348 | KNN Loss: 3.774616003036499 | BCE Loss: 1.0953876972198486\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.807101249694824 | KNN Loss: 3.7879724502563477 | BCE Loss: 1.0191290378570557\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.8879570960998535 | KNN Loss: 3.808211088180542 | BCE Loss: 1.0797460079193115\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.843871593475342 | KNN Loss: 3.7752201557159424 | BCE Loss: 1.0686514377593994\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.78050422668457 | KNN Loss: 3.7596065998077393 | BCE Loss: 1.020897626876831\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.888187885284424 | KNN Loss: 3.8162500858306885 | BCE Loss: 1.0719376802444458\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.861087799072266 | KNN Loss: 3.8242838382720947 | BCE Loss: 1.0368037223815918\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.90488338470459 | KNN Loss: 3.8397953510284424 | BCE Loss: 1.065087914466858\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.833340644836426 | KNN Loss: 3.779719114303589 | BCE Loss: 1.0536216497421265\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.866458415985107 | KNN Loss: 3.7990901470184326 | BCE Loss: 1.0673682689666748\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.839338302612305 | KNN Loss: 3.7925350666046143 | BCE Loss: 1.0468034744262695\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.816798210144043 | KNN Loss: 3.773843288421631 | BCE Loss: 1.0429551601409912\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.882840156555176 | KNN Loss: 3.815262794494629 | BCE Loss: 1.0675774812698364\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.864630222320557 | KNN Loss: 3.8105928897857666 | BCE Loss: 1.05403733253479\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.8003387451171875 | KNN Loss: 3.7809431552886963 | BCE Loss: 1.019395351409912\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.874795913696289 | KNN Loss: 3.809063196182251 | BCE Loss: 1.0657329559326172\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.832259178161621 | KNN Loss: 3.7657642364501953 | BCE Loss: 1.0664951801300049\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.899240016937256 | KNN Loss: 3.818598508834839 | BCE Loss: 1.0806413888931274\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.902576446533203 | KNN Loss: 3.836513042449951 | BCE Loss: 1.066063404083252\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.817977428436279 | KNN Loss: 3.7665064334869385 | BCE Loss: 1.0514711141586304\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.845800399780273 | KNN Loss: 3.7897214889526367 | BCE Loss: 1.0560791492462158\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.881817817687988 | KNN Loss: 3.8224539756774902 | BCE Loss: 1.059363842010498\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.823411464691162 | KNN Loss: 3.771414279937744 | BCE Loss: 1.051997184753418\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.86308479309082 | KNN Loss: 3.781653642654419 | BCE Loss: 1.0814313888549805\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.8622965812683105 | KNN Loss: 3.8079349994659424 | BCE Loss: 1.0543615818023682\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.835762977600098 | KNN Loss: 3.7734463214874268 | BCE Loss: 1.06231689453125\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.84328031539917 | KNN Loss: 3.7870702743530273 | BCE Loss: 1.056209921836853\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.79514741897583 | KNN Loss: 3.747549295425415 | BCE Loss: 1.0475980043411255\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.828857421875 | KNN Loss: 3.825385332107544 | BCE Loss: 1.003472089767456\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.841480255126953 | KNN Loss: 3.7910897731781006 | BCE Loss: 1.0503904819488525\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.825279235839844 | KNN Loss: 3.791245222091675 | BCE Loss: 1.034034013748169\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.843754768371582 | KNN Loss: 3.7796778678894043 | BCE Loss: 1.0640769004821777\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.824158668518066 | KNN Loss: 3.781897783279419 | BCE Loss: 1.0422606468200684\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.845171928405762 | KNN Loss: 3.815131425857544 | BCE Loss: 1.0300405025482178\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.8795061111450195 | KNN Loss: 3.8215532302856445 | BCE Loss: 1.057952880859375\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.888157367706299 | KNN Loss: 3.815420389175415 | BCE Loss: 1.0727370977401733\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.824688911437988 | KNN Loss: 3.8087031841278076 | BCE Loss: 1.0159859657287598\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.839202880859375 | KNN Loss: 3.8073267936706543 | BCE Loss: 1.0318762063980103\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.851529121398926 | KNN Loss: 3.8110876083374023 | BCE Loss: 1.0404415130615234\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.837698459625244 | KNN Loss: 3.770885467529297 | BCE Loss: 1.0668129920959473\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.810248374938965 | KNN Loss: 3.7914066314697266 | BCE Loss: 1.0188419818878174\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.7850260734558105 | KNN Loss: 3.7370517253875732 | BCE Loss: 1.0479744672775269\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.853157997131348 | KNN Loss: 3.7746739387512207 | BCE Loss: 1.0784838199615479\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.826610088348389 | KNN Loss: 3.7583742141723633 | BCE Loss: 1.0682358741760254\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.846639633178711 | KNN Loss: 3.795459270477295 | BCE Loss: 1.051180124282837\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.809904098510742 | KNN Loss: 3.769592761993408 | BCE Loss: 1.040311574935913\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.818124294281006 | KNN Loss: 3.77836012840271 | BCE Loss: 1.0397640466690063\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.817714691162109 | KNN Loss: 3.766691207885742 | BCE Loss: 1.051023244857788\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.839670658111572 | KNN Loss: 3.79349946975708 | BCE Loss: 1.0461711883544922\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.8174357414245605 | KNN Loss: 3.769158363342285 | BCE Loss: 1.0482773780822754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.769382953643799 | KNN Loss: 3.7356760501861572 | BCE Loss: 1.0337069034576416\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.821622848510742 | KNN Loss: 3.7641007900238037 | BCE Loss: 1.0575222969055176\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.8612380027771 | KNN Loss: 3.805204153060913 | BCE Loss: 1.0560338497161865\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.810745716094971 | KNN Loss: 3.7756741046905518 | BCE Loss: 1.0350717306137085\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.864107608795166 | KNN Loss: 3.8003976345062256 | BCE Loss: 1.0637099742889404\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.850316524505615 | KNN Loss: 3.797315835952759 | BCE Loss: 1.0530006885528564\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.829188346862793 | KNN Loss: 3.7873759269714355 | BCE Loss: 1.0418126583099365\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.849262237548828 | KNN Loss: 3.7826976776123047 | BCE Loss: 1.0665643215179443\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.775193214416504 | KNN Loss: 3.755681037902832 | BCE Loss: 1.0195119380950928\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.793929100036621 | KNN Loss: 3.765399932861328 | BCE Loss: 1.0285289287567139\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.822271347045898 | KNN Loss: 3.752190589904785 | BCE Loss: 1.0700805187225342\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.823601722717285 | KNN Loss: 3.778470516204834 | BCE Loss: 1.045130968093872\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.818547248840332 | KNN Loss: 3.782945394515991 | BCE Loss: 1.03560209274292\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.765495300292969 | KNN Loss: 3.745103120803833 | BCE Loss: 1.0203920602798462\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.839353084564209 | KNN Loss: 3.7845993041992188 | BCE Loss: 1.0547537803649902\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.7993268966674805 | KNN Loss: 3.7511255741119385 | BCE Loss: 1.048201084136963\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.786861419677734 | KNN Loss: 3.757157802581787 | BCE Loss: 1.0297034978866577\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.822481632232666 | KNN Loss: 3.753479242324829 | BCE Loss: 1.0690022706985474\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.8295698165893555 | KNN Loss: 3.800112009048462 | BCE Loss: 1.029457688331604\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.818083763122559 | KNN Loss: 3.793677806854248 | BCE Loss: 1.0244059562683105\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.833139419555664 | KNN Loss: 3.801527738571167 | BCE Loss: 1.031611680984497\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.862397193908691 | KNN Loss: 3.806959867477417 | BCE Loss: 1.0554372072219849\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.805171012878418 | KNN Loss: 3.7612016201019287 | BCE Loss: 1.0439696311950684\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.827118396759033 | KNN Loss: 3.7978765964508057 | BCE Loss: 1.0292418003082275\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.813097953796387 | KNN Loss: 3.769946575164795 | BCE Loss: 1.0431511402130127\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.76663875579834 | KNN Loss: 3.74764084815979 | BCE Loss: 1.0189976692199707\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.771195411682129 | KNN Loss: 3.750666618347168 | BCE Loss: 1.02052903175354\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.7796149253845215 | KNN Loss: 3.7523131370544434 | BCE Loss: 1.0273019075393677\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.7977705001831055 | KNN Loss: 3.7574687004089355 | BCE Loss: 1.0403015613555908\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.806228160858154 | KNN Loss: 3.7779288291931152 | BCE Loss: 1.0282992124557495\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.807708263397217 | KNN Loss: 3.7536518573760986 | BCE Loss: 1.0540565252304077\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.763691425323486 | KNN Loss: 3.7250349521636963 | BCE Loss: 1.0386563539505005\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.7700700759887695 | KNN Loss: 3.7206897735595703 | BCE Loss: 1.0493804216384888\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.819427490234375 | KNN Loss: 3.7651607990264893 | BCE Loss: 1.0542664527893066\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.8198089599609375 | KNN Loss: 3.7916572093963623 | BCE Loss: 1.0281517505645752\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.750650405883789 | KNN Loss: 3.74275803565979 | BCE Loss: 1.00789213180542\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.824601173400879 | KNN Loss: 3.777188777923584 | BCE Loss: 1.0474121570587158\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.8316802978515625 | KNN Loss: 3.777528762817383 | BCE Loss: 1.0541515350341797\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.817474365234375 | KNN Loss: 3.7855939865112305 | BCE Loss: 1.0318803787231445\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.784518241882324 | KNN Loss: 3.7370941638946533 | BCE Loss: 1.0474238395690918\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.84857177734375 | KNN Loss: 3.8077399730682373 | BCE Loss: 1.0408318042755127\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.821722030639648 | KNN Loss: 3.7972705364227295 | BCE Loss: 1.024451494216919\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.772219657897949 | KNN Loss: 3.755826234817505 | BCE Loss: 1.0163933038711548\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.801059722900391 | KNN Loss: 3.720027446746826 | BCE Loss: 1.0810320377349854\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.801346778869629 | KNN Loss: 3.7813053131103516 | BCE Loss: 1.0200417041778564\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.774421215057373 | KNN Loss: 3.7353923320770264 | BCE Loss: 1.0390288829803467\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.792618274688721 | KNN Loss: 3.746082067489624 | BCE Loss: 1.0465362071990967\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.802959442138672 | KNN Loss: 3.761049509048462 | BCE Loss: 1.0419096946716309\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.876058578491211 | KNN Loss: 3.814293384552002 | BCE Loss: 1.061765432357788\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.835247993469238 | KNN Loss: 3.790203094482422 | BCE Loss: 1.0450448989868164\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.777050018310547 | KNN Loss: 3.726741313934326 | BCE Loss: 1.0503089427947998\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.770306587219238 | KNN Loss: 3.7413508892059326 | BCE Loss: 1.0289555788040161\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.771103858947754 | KNN Loss: 3.7442514896392822 | BCE Loss: 1.0268526077270508\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.7965617179870605 | KNN Loss: 3.743614912033081 | BCE Loss: 1.0529468059539795\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.773232460021973 | KNN Loss: 3.7495017051696777 | BCE Loss: 1.0237306356430054\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.830706596374512 | KNN Loss: 3.759716272354126 | BCE Loss: 1.0709905624389648\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.836451530456543 | KNN Loss: 3.7829511165618896 | BCE Loss: 1.0535004138946533\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.850738048553467 | KNN Loss: 3.7831060886383057 | BCE Loss: 1.0676320791244507\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.83161735534668 | KNN Loss: 3.7837014198303223 | BCE Loss: 1.047916054725647\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.79362154006958 | KNN Loss: 3.7480924129486084 | BCE Loss: 1.0455292463302612\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.820904731750488 | KNN Loss: 3.7653982639312744 | BCE Loss: 1.055506706237793\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.761561393737793 | KNN Loss: 3.7411482334136963 | BCE Loss: 1.0204133987426758\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.77779483795166 | KNN Loss: 3.7500054836273193 | BCE Loss: 1.0277893543243408\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.766435146331787 | KNN Loss: 3.724281072616577 | BCE Loss: 1.0421541929244995\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.790122985839844 | KNN Loss: 3.777789354324341 | BCE Loss: 1.012333869934082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.815564155578613 | KNN Loss: 3.7677969932556152 | BCE Loss: 1.0477674007415771\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.830121994018555 | KNN Loss: 3.8136463165283203 | BCE Loss: 1.0164759159088135\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.812733173370361 | KNN Loss: 3.7600300312042236 | BCE Loss: 1.0527031421661377\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.761682510375977 | KNN Loss: 3.7450225353240967 | BCE Loss: 1.0166598558425903\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.831888198852539 | KNN Loss: 3.7563159465789795 | BCE Loss: 1.0755720138549805\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.798451900482178 | KNN Loss: 3.7779626846313477 | BCE Loss: 1.02048921585083\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.813273906707764 | KNN Loss: 3.7830395698547363 | BCE Loss: 1.030234456062317\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.818156719207764 | KNN Loss: 3.7547125816345215 | BCE Loss: 1.0634441375732422\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.774256706237793 | KNN Loss: 3.7343149185180664 | BCE Loss: 1.0399419069290161\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.762454509735107 | KNN Loss: 3.729151964187622 | BCE Loss: 1.0333025455474854\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.799838542938232 | KNN Loss: 3.7809641361236572 | BCE Loss: 1.0188744068145752\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.78891658782959 | KNN Loss: 3.7501981258392334 | BCE Loss: 1.0387182235717773\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.805832386016846 | KNN Loss: 3.7827038764953613 | BCE Loss: 1.0231283903121948\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.779913902282715 | KNN Loss: 3.723233461380005 | BCE Loss: 1.05668044090271\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.79299259185791 | KNN Loss: 3.731950044631958 | BCE Loss: 1.0610426664352417\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.79369592666626 | KNN Loss: 3.758043050765991 | BCE Loss: 1.035652756690979\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.749614715576172 | KNN Loss: 3.7054970264434814 | BCE Loss: 1.04411780834198\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.813160419464111 | KNN Loss: 3.7791190147399902 | BCE Loss: 1.034041404724121\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.79173469543457 | KNN Loss: 3.733708143234253 | BCE Loss: 1.0580264329910278\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.764636039733887 | KNN Loss: 3.7497332096099854 | BCE Loss: 1.0149027109146118\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.77433967590332 | KNN Loss: 3.719616651535034 | BCE Loss: 1.0547230243682861\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.803897380828857 | KNN Loss: 3.7380685806274414 | BCE Loss: 1.065828800201416\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.751542091369629 | KNN Loss: 3.7298972606658936 | BCE Loss: 1.0216445922851562\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.819041728973389 | KNN Loss: 3.785327434539795 | BCE Loss: 1.0337141752243042\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.755051612854004 | KNN Loss: 3.7430124282836914 | BCE Loss: 1.012039303779602\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.771803855895996 | KNN Loss: 3.737961769104004 | BCE Loss: 1.033841848373413\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.760846138000488 | KNN Loss: 3.717733144760132 | BCE Loss: 1.0431129932403564\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.785687446594238 | KNN Loss: 3.722541332244873 | BCE Loss: 1.0631462335586548\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.727904319763184 | KNN Loss: 3.70579195022583 | BCE Loss: 1.022112250328064\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.777746677398682 | KNN Loss: 3.7393763065338135 | BCE Loss: 1.0383702516555786\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.782387733459473 | KNN Loss: 3.7502565383911133 | BCE Loss: 1.032131314277649\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.78834867477417 | KNN Loss: 3.7699925899505615 | BCE Loss: 1.0183559656143188\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.7575554847717285 | KNN Loss: 3.7127840518951416 | BCE Loss: 1.0447713136672974\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.788791656494141 | KNN Loss: 3.758469343185425 | BCE Loss: 1.0303223133087158\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.7874345779418945 | KNN Loss: 3.7556204795837402 | BCE Loss: 1.0318143367767334\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.812707424163818 | KNN Loss: 3.7660882472991943 | BCE Loss: 1.0466190576553345\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.790589332580566 | KNN Loss: 3.7701027393341064 | BCE Loss: 1.0204863548278809\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.754582405090332 | KNN Loss: 3.7328906059265137 | BCE Loss: 1.021691918373108\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.762300491333008 | KNN Loss: 3.728924512863159 | BCE Loss: 1.0333757400512695\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.7607316970825195 | KNN Loss: 3.7305898666381836 | BCE Loss: 1.030142068862915\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.807558059692383 | KNN Loss: 3.7788124084472656 | BCE Loss: 1.0287456512451172\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.776650428771973 | KNN Loss: 3.7374484539031982 | BCE Loss: 1.0392017364501953\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.7400665283203125 | KNN Loss: 3.726372241973877 | BCE Loss: 1.0136945247650146\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.748649597167969 | KNN Loss: 3.7224018573760986 | BCE Loss: 1.026247501373291\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.779972076416016 | KNN Loss: 3.7515716552734375 | BCE Loss: 1.0284006595611572\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.750733375549316 | KNN Loss: 3.732644557952881 | BCE Loss: 1.0180885791778564\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.769721508026123 | KNN Loss: 3.7559280395507812 | BCE Loss: 1.0137935876846313\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.736882209777832 | KNN Loss: 3.725071907043457 | BCE Loss: 1.011810302734375\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.771823883056641 | KNN Loss: 3.738614082336426 | BCE Loss: 1.0332096815109253\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.75351095199585 | KNN Loss: 3.7197022438049316 | BCE Loss: 1.033808708190918\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.744041442871094 | KNN Loss: 3.705940008163452 | BCE Loss: 1.0381011962890625\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.852609634399414 | KNN Loss: 3.789496898651123 | BCE Loss: 1.0631129741668701\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.80284309387207 | KNN Loss: 3.754476547241211 | BCE Loss: 1.0483665466308594\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.782173156738281 | KNN Loss: 3.741091012954712 | BCE Loss: 1.0410823822021484\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.777990341186523 | KNN Loss: 3.7386655807495117 | BCE Loss: 1.0393245220184326\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.779045581817627 | KNN Loss: 3.7194442749023438 | BCE Loss: 1.0596011877059937\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.751890182495117 | KNN Loss: 3.722637891769409 | BCE Loss: 1.029252529144287\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.758376598358154 | KNN Loss: 3.7331433296203613 | BCE Loss: 1.025233268737793\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.757632732391357 | KNN Loss: 3.717907428741455 | BCE Loss: 1.0397253036499023\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.794435501098633 | KNN Loss: 3.75022029876709 | BCE Loss: 1.044215440750122\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.785499095916748 | KNN Loss: 3.7798306941986084 | BCE Loss: 1.0056684017181396\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.846906661987305 | KNN Loss: 3.798511505126953 | BCE Loss: 1.0483951568603516\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.755374908447266 | KNN Loss: 3.720836639404297 | BCE Loss: 1.0345385074615479\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.751225471496582 | KNN Loss: 3.7326529026031494 | BCE Loss: 1.018572449684143\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.797588348388672 | KNN Loss: 3.7474753856658936 | BCE Loss: 1.0501129627227783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.8205156326293945 | KNN Loss: 3.7558465003967285 | BCE Loss: 1.0646693706512451\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.785021781921387 | KNN Loss: 3.7652742862701416 | BCE Loss: 1.0197476148605347\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.803799629211426 | KNN Loss: 3.752368211746216 | BCE Loss: 1.05143141746521\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.7835564613342285 | KNN Loss: 3.739664077758789 | BCE Loss: 1.04389226436615\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.757462024688721 | KNN Loss: 3.723784923553467 | BCE Loss: 1.0336772203445435\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.785367488861084 | KNN Loss: 3.7681097984313965 | BCE Loss: 1.0172576904296875\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.766208171844482 | KNN Loss: 3.7340595722198486 | BCE Loss: 1.0321484804153442\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.766202926635742 | KNN Loss: 3.7332630157470703 | BCE Loss: 1.0329396724700928\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.776920318603516 | KNN Loss: 3.7660021781921387 | BCE Loss: 1.0109179019927979\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.743192672729492 | KNN Loss: 3.7159485816955566 | BCE Loss: 1.0272438526153564\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.732550144195557 | KNN Loss: 3.7105553150177 | BCE Loss: 1.0219948291778564\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.767481803894043 | KNN Loss: 3.737091064453125 | BCE Loss: 1.0303908586502075\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.770728588104248 | KNN Loss: 3.7181460857391357 | BCE Loss: 1.0525825023651123\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.7611589431762695 | KNN Loss: 3.731546640396118 | BCE Loss: 1.0296121835708618\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.763187408447266 | KNN Loss: 3.74898362159729 | BCE Loss: 1.0142037868499756\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.715646266937256 | KNN Loss: 3.682190418243408 | BCE Loss: 1.033455729484558\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.7694549560546875 | KNN Loss: 3.715810537338257 | BCE Loss: 1.0536445379257202\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.742012023925781 | KNN Loss: 3.6878504753112793 | BCE Loss: 1.0541616678237915\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.804348468780518 | KNN Loss: 3.7460107803344727 | BCE Loss: 1.0583378076553345\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.78078031539917 | KNN Loss: 3.7541918754577637 | BCE Loss: 1.0265885591506958\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.739412307739258 | KNN Loss: 3.6908645629882812 | BCE Loss: 1.0485477447509766\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.788997650146484 | KNN Loss: 3.7563109397888184 | BCE Loss: 1.032686471939087\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.795635223388672 | KNN Loss: 3.7492637634277344 | BCE Loss: 1.046371340751648\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.773330211639404 | KNN Loss: 3.7322371006011963 | BCE Loss: 1.0410929918289185\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.751255035400391 | KNN Loss: 3.7295403480529785 | BCE Loss: 1.021714448928833\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.782959938049316 | KNN Loss: 3.7440197467803955 | BCE Loss: 1.038940191268921\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.811871528625488 | KNN Loss: 3.750337839126587 | BCE Loss: 1.0615334510803223\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.799373626708984 | KNN Loss: 3.734769582748413 | BCE Loss: 1.0646042823791504\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.733959197998047 | KNN Loss: 3.714883327484131 | BCE Loss: 1.0190757513046265\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.720541000366211 | KNN Loss: 3.715771436691284 | BCE Loss: 1.0047693252563477\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.77266788482666 | KNN Loss: 3.749039649963379 | BCE Loss: 1.0236282348632812\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.726039886474609 | KNN Loss: 3.703740119934082 | BCE Loss: 1.0222995281219482\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.739189147949219 | KNN Loss: 3.712394952774048 | BCE Loss: 1.026794195175171\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.784185409545898 | KNN Loss: 3.724487066268921 | BCE Loss: 1.059698462486267\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.726279258728027 | KNN Loss: 3.6974239349365234 | BCE Loss: 1.0288550853729248\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.716476917266846 | KNN Loss: 3.7163918018341064 | BCE Loss: 1.0000851154327393\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.725791931152344 | KNN Loss: 3.712578773498535 | BCE Loss: 1.0132131576538086\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.766512393951416 | KNN Loss: 3.733881711959839 | BCE Loss: 1.0326306819915771\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.801314353942871 | KNN Loss: 3.773470640182495 | BCE Loss: 1.027843952178955\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.7747931480407715 | KNN Loss: 3.7428781986236572 | BCE Loss: 1.0319148302078247\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.757339954376221 | KNN Loss: 3.7220380306243896 | BCE Loss: 1.0353018045425415\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.734814643859863 | KNN Loss: 3.7146527767181396 | BCE Loss: 1.0201621055603027\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.769503593444824 | KNN Loss: 3.726518392562866 | BCE Loss: 1.042984962463379\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.733486175537109 | KNN Loss: 3.7203969955444336 | BCE Loss: 1.0130894184112549\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.767561912536621 | KNN Loss: 3.7303144931793213 | BCE Loss: 1.0372471809387207\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.750633239746094 | KNN Loss: 3.7028613090515137 | BCE Loss: 1.0477720499038696\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.7419328689575195 | KNN Loss: 3.729816436767578 | BCE Loss: 1.0121161937713623\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.731246471405029 | KNN Loss: 3.716806173324585 | BCE Loss: 1.0144401788711548\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.745911121368408 | KNN Loss: 3.7318758964538574 | BCE Loss: 1.0140353441238403\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.749516487121582 | KNN Loss: 3.720431327819824 | BCE Loss: 1.0290849208831787\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.756649494171143 | KNN Loss: 3.725825309753418 | BCE Loss: 1.0308241844177246\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.687244892120361 | KNN Loss: 3.6967356204986572 | BCE Loss: 0.9905092716217041\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.791104793548584 | KNN Loss: 3.760154962539673 | BCE Loss: 1.0309498310089111\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.777636528015137 | KNN Loss: 3.733203649520874 | BCE Loss: 1.0444331169128418\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.800805568695068 | KNN Loss: 3.747169256210327 | BCE Loss: 1.0536363124847412\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.761248588562012 | KNN Loss: 3.7179694175720215 | BCE Loss: 1.0432789325714111\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.782377243041992 | KNN Loss: 3.7512307167053223 | BCE Loss: 1.0311466455459595\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.819430828094482 | KNN Loss: 3.7647578716278076 | BCE Loss: 1.0546730756759644\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.741705894470215 | KNN Loss: 3.690408945083618 | BCE Loss: 1.0512968301773071\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.681349277496338 | KNN Loss: 3.6883347034454346 | BCE Loss: 0.9930146336555481\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.731550216674805 | KNN Loss: 3.721431255340576 | BCE Loss: 1.010119080543518\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.82075309753418 | KNN Loss: 3.7634949684143066 | BCE Loss: 1.057258129119873\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.692014694213867 | KNN Loss: 3.695054531097412 | BCE Loss: 0.9969600439071655\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.746405601501465 | KNN Loss: 3.7264370918273926 | BCE Loss: 1.0199685096740723\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.738624572753906 | KNN Loss: 3.6960299015045166 | BCE Loss: 1.0425947904586792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.717559814453125 | KNN Loss: 3.6916518211364746 | BCE Loss: 1.0259077548980713\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.754786014556885 | KNN Loss: 3.7242918014526367 | BCE Loss: 1.030494213104248\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.736239433288574 | KNN Loss: 3.7219812870025635 | BCE Loss: 1.0142581462860107\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.726287841796875 | KNN Loss: 3.7138025760650635 | BCE Loss: 1.0124852657318115\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.714808464050293 | KNN Loss: 3.6771745681762695 | BCE Loss: 1.0376336574554443\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.722078800201416 | KNN Loss: 3.713974952697754 | BCE Loss: 1.008103847503662\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.817697525024414 | KNN Loss: 3.742600917816162 | BCE Loss: 1.075096607208252\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.726203918457031 | KNN Loss: 3.697857618331909 | BCE Loss: 1.0283465385437012\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.821727752685547 | KNN Loss: 3.768184185028076 | BCE Loss: 1.0535438060760498\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.746099948883057 | KNN Loss: 3.737370491027832 | BCE Loss: 1.0087294578552246\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.757940769195557 | KNN Loss: 3.7451040744781494 | BCE Loss: 1.0128365755081177\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.798151969909668 | KNN Loss: 3.7730467319488525 | BCE Loss: 1.0251049995422363\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.68179988861084 | KNN Loss: 3.6825814247131348 | BCE Loss: 0.9992186427116394\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.74098014831543 | KNN Loss: 3.7211437225341797 | BCE Loss: 1.019836187362671\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.7697906494140625 | KNN Loss: 3.7203638553619385 | BCE Loss: 1.0494270324707031\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.784778594970703 | KNN Loss: 3.7741081714630127 | BCE Loss: 1.01067054271698\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.744482040405273 | KNN Loss: 3.708298444747925 | BCE Loss: 1.0361833572387695\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.711940765380859 | KNN Loss: 3.6926932334899902 | BCE Loss: 1.0192476511001587\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.81240177154541 | KNN Loss: 3.8019368648529053 | BCE Loss: 1.0104647874832153\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.751913070678711 | KNN Loss: 3.7567505836486816 | BCE Loss: 0.9951627254486084\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.813933849334717 | KNN Loss: 3.7850186824798584 | BCE Loss: 1.028915286064148\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.765472412109375 | KNN Loss: 3.721510887145996 | BCE Loss: 1.0439614057540894\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.752185821533203 | KNN Loss: 3.7311697006225586 | BCE Loss: 1.0210163593292236\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.718701362609863 | KNN Loss: 3.682391881942749 | BCE Loss: 1.0363094806671143\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.762397289276123 | KNN Loss: 3.7294206619262695 | BCE Loss: 1.032976746559143\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.747930526733398 | KNN Loss: 3.717390298843384 | BCE Loss: 1.0305404663085938\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.749238967895508 | KNN Loss: 3.7403345108032227 | BCE Loss: 1.0089043378829956\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.758047580718994 | KNN Loss: 3.7392220497131348 | BCE Loss: 1.0188254117965698\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.762330055236816 | KNN Loss: 3.717623710632324 | BCE Loss: 1.0447063446044922\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.718276023864746 | KNN Loss: 3.675584077835083 | BCE Loss: 1.0426921844482422\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.797597885131836 | KNN Loss: 3.7516350746154785 | BCE Loss: 1.0459625720977783\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.732754707336426 | KNN Loss: 3.7172656059265137 | BCE Loss: 1.0154889822006226\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.753975868225098 | KNN Loss: 3.740185260772705 | BCE Loss: 1.0137908458709717\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.716461181640625 | KNN Loss: 3.6893603801727295 | BCE Loss: 1.0271010398864746\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.705058574676514 | KNN Loss: 3.714749813079834 | BCE Loss: 0.9903087615966797\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.678909778594971 | KNN Loss: 3.6654562950134277 | BCE Loss: 1.0134533643722534\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.722360610961914 | KNN Loss: 3.7112131118774414 | BCE Loss: 1.0111477375030518\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.769815444946289 | KNN Loss: 3.754650592803955 | BCE Loss: 1.0151646137237549\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.755109786987305 | KNN Loss: 3.7287192344665527 | BCE Loss: 1.0263903141021729\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.724062442779541 | KNN Loss: 3.7037954330444336 | BCE Loss: 1.0202670097351074\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.724128246307373 | KNN Loss: 3.7166454792022705 | BCE Loss: 1.007482886314392\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.7990570068359375 | KNN Loss: 3.735114336013794 | BCE Loss: 1.0639424324035645\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.713596343994141 | KNN Loss: 3.680647373199463 | BCE Loss: 1.0329492092132568\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.770031929016113 | KNN Loss: 3.7353243827819824 | BCE Loss: 1.0347075462341309\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.754793643951416 | KNN Loss: 3.711787462234497 | BCE Loss: 1.0430060625076294\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.735297203063965 | KNN Loss: 3.7122788429260254 | BCE Loss: 1.0230183601379395\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.742241859436035 | KNN Loss: 3.709188938140869 | BCE Loss: 1.0330530405044556\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.675532341003418 | KNN Loss: 3.6851162910461426 | BCE Loss: 0.9904161691665649\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.756410598754883 | KNN Loss: 3.7266204357147217 | BCE Loss: 1.029789924621582\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.824085235595703 | KNN Loss: 3.75666880607605 | BCE Loss: 1.0674166679382324\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.789570331573486 | KNN Loss: 3.7613415718078613 | BCE Loss: 1.028228759765625\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.753697395324707 | KNN Loss: 3.7078285217285156 | BCE Loss: 1.0458691120147705\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.73663330078125 | KNN Loss: 3.6900393962860107 | BCE Loss: 1.0465940237045288\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.710082530975342 | KNN Loss: 3.7069027423858643 | BCE Loss: 1.003179907798767\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.7600579261779785 | KNN Loss: 3.712818145751953 | BCE Loss: 1.047239899635315\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.7457122802734375 | KNN Loss: 3.72810697555542 | BCE Loss: 1.0176055431365967\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.744532585144043 | KNN Loss: 3.7132151126861572 | BCE Loss: 1.0313177108764648\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.725910186767578 | KNN Loss: 3.713244676589966 | BCE Loss: 1.0126657485961914\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.694151878356934 | KNN Loss: 3.695603609085083 | BCE Loss: 0.9985483288764954\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.719414710998535 | KNN Loss: 3.6850359439849854 | BCE Loss: 1.0343788862228394\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.733585357666016 | KNN Loss: 3.7319064140319824 | BCE Loss: 1.0016788244247437\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.815162181854248 | KNN Loss: 3.749387264251709 | BCE Loss: 1.0657750368118286\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.7722930908203125 | KNN Loss: 3.7386767864227295 | BCE Loss: 1.033616304397583\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.733343601226807 | KNN Loss: 3.698188304901123 | BCE Loss: 1.0351554155349731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.70012903213501 | KNN Loss: 3.6739377975463867 | BCE Loss: 1.0261913537979126\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.748657703399658 | KNN Loss: 3.7083072662353516 | BCE Loss: 1.040350317955017\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.758847713470459 | KNN Loss: 3.7303507328033447 | BCE Loss: 1.0284970998764038\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.747636795043945 | KNN Loss: 3.712843894958496 | BCE Loss: 1.0347930192947388\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.830390930175781 | KNN Loss: 3.7853589057922363 | BCE Loss: 1.045032262802124\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.697503089904785 | KNN Loss: 3.700519323348999 | BCE Loss: 0.9969840049743652\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.725960731506348 | KNN Loss: 3.6897521018981934 | BCE Loss: 1.0362083911895752\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.737920761108398 | KNN Loss: 3.7127323150634766 | BCE Loss: 1.0251884460449219\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.715034008026123 | KNN Loss: 3.6846742630004883 | BCE Loss: 1.0303597450256348\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.724421501159668 | KNN Loss: 3.7191476821899414 | BCE Loss: 1.0052740573883057\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.735427379608154 | KNN Loss: 3.706726551055908 | BCE Loss: 1.0287007093429565\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.7168779373168945 | KNN Loss: 3.6996729373931885 | BCE Loss: 1.017204999923706\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.748236656188965 | KNN Loss: 3.7047793865203857 | BCE Loss: 1.04345703125\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.706912040710449 | KNN Loss: 3.686577081680298 | BCE Loss: 1.0203347206115723\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.706333637237549 | KNN Loss: 3.6780295372009277 | BCE Loss: 1.0283042192459106\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.747030258178711 | KNN Loss: 3.717038154602051 | BCE Loss: 1.029991865158081\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.7385406494140625 | KNN Loss: 3.7329587936401367 | BCE Loss: 1.0055820941925049\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.697002410888672 | KNN Loss: 3.6826865673065186 | BCE Loss: 1.0143158435821533\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.7008771896362305 | KNN Loss: 3.6930408477783203 | BCE Loss: 1.0078362226486206\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.7467803955078125 | KNN Loss: 3.735349416732788 | BCE Loss: 1.0114307403564453\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.744200706481934 | KNN Loss: 3.6834373474121094 | BCE Loss: 1.0607631206512451\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.775345325469971 | KNN Loss: 3.7424821853637695 | BCE Loss: 1.0328632593154907\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.716499328613281 | KNN Loss: 3.711120128631592 | BCE Loss: 1.0053790807724\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.755138874053955 | KNN Loss: 3.7147109508514404 | BCE Loss: 1.0404279232025146\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.812994956970215 | KNN Loss: 3.7199952602386475 | BCE Loss: 1.0929996967315674\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.736021518707275 | KNN Loss: 3.6791844367980957 | BCE Loss: 1.0568369626998901\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.716263771057129 | KNN Loss: 3.691453218460083 | BCE Loss: 1.0248103141784668\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.731637954711914 | KNN Loss: 3.718238115310669 | BCE Loss: 1.0133999586105347\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.745863437652588 | KNN Loss: 3.716681718826294 | BCE Loss: 1.029181718826294\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.804919719696045 | KNN Loss: 3.7499821186065674 | BCE Loss: 1.054937481880188\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.7581682205200195 | KNN Loss: 3.7150096893310547 | BCE Loss: 1.0431586503982544\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.736274719238281 | KNN Loss: 3.696425676345825 | BCE Loss: 1.039848804473877\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.742371082305908 | KNN Loss: 3.727567195892334 | BCE Loss: 1.0148038864135742\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.728137016296387 | KNN Loss: 3.7179415225982666 | BCE Loss: 1.0101957321166992\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.780303955078125 | KNN Loss: 3.7413742542266846 | BCE Loss: 1.0389297008514404\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.751629829406738 | KNN Loss: 3.7355942726135254 | BCE Loss: 1.016035795211792\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.753588676452637 | KNN Loss: 3.7274467945098877 | BCE Loss: 1.026141881942749\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.7430219650268555 | KNN Loss: 3.69417405128479 | BCE Loss: 1.0488479137420654\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.692719459533691 | KNN Loss: 3.689587116241455 | BCE Loss: 1.0031324625015259\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.744159698486328 | KNN Loss: 3.710310697555542 | BCE Loss: 1.0338490009307861\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.7523674964904785 | KNN Loss: 3.7262327671051025 | BCE Loss: 1.026134729385376\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.723300457000732 | KNN Loss: 3.6996874809265137 | BCE Loss: 1.0236130952835083\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.728128433227539 | KNN Loss: 3.722003221511841 | BCE Loss: 1.0061250925064087\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.73199462890625 | KNN Loss: 3.705306053161621 | BCE Loss: 1.026688575744629\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.755008697509766 | KNN Loss: 3.7302207946777344 | BCE Loss: 1.0247880220413208\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.766891002655029 | KNN Loss: 3.7242753505706787 | BCE Loss: 1.0426156520843506\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.768071174621582 | KNN Loss: 3.7332024574279785 | BCE Loss: 1.0348689556121826\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.787705421447754 | KNN Loss: 3.734631299972534 | BCE Loss: 1.0530740022659302\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.733119487762451 | KNN Loss: 3.6827232837677 | BCE Loss: 1.0503963232040405\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.6953020095825195 | KNN Loss: 3.6941699981689453 | BCE Loss: 1.0011318922042847\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.756768226623535 | KNN Loss: 3.6962106227874756 | BCE Loss: 1.0605578422546387\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.765570640563965 | KNN Loss: 3.740036725997925 | BCE Loss: 1.025533676147461\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.75908088684082 | KNN Loss: 3.7161905765533447 | BCE Loss: 1.0428905487060547\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.715789318084717 | KNN Loss: 3.6905996799468994 | BCE Loss: 1.0251896381378174\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.721973896026611 | KNN Loss: 3.7109375 | BCE Loss: 1.0110363960266113\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.78448486328125 | KNN Loss: 3.7281970977783203 | BCE Loss: 1.0562878847122192\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.710995197296143 | KNN Loss: 3.7128310203552246 | BCE Loss: 0.9981640577316284\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.761651515960693 | KNN Loss: 3.7231171131134033 | BCE Loss: 1.0385345220565796\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.748871803283691 | KNN Loss: 3.740332841873169 | BCE Loss: 1.0085391998291016\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.814611434936523 | KNN Loss: 3.768671751022339 | BCE Loss: 1.045939564704895\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.798056602478027 | KNN Loss: 3.7339255809783936 | BCE Loss: 1.064131259918213\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.780388832092285 | KNN Loss: 3.732813835144043 | BCE Loss: 1.0475748777389526\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.697556972503662 | KNN Loss: 3.6869077682495117 | BCE Loss: 1.0106492042541504\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.71462345123291 | KNN Loss: 3.699399471282959 | BCE Loss: 1.0152242183685303\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.733119487762451 | KNN Loss: 3.7131643295288086 | BCE Loss: 1.019955039024353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.782723903656006 | KNN Loss: 3.7330589294433594 | BCE Loss: 1.049664855003357\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.758654594421387 | KNN Loss: 3.7373623847961426 | BCE Loss: 1.0212922096252441\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.709909439086914 | KNN Loss: 3.687044382095337 | BCE Loss: 1.022864818572998\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.7103753089904785 | KNN Loss: 3.667794704437256 | BCE Loss: 1.0425806045532227\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.736180305480957 | KNN Loss: 3.7055439949035645 | BCE Loss: 1.0306364297866821\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.759105682373047 | KNN Loss: 3.7183895111083984 | BCE Loss: 1.0407160520553589\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.696104526519775 | KNN Loss: 3.6824448108673096 | BCE Loss: 1.0136598348617554\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.759469985961914 | KNN Loss: 3.7415833473205566 | BCE Loss: 1.017886757850647\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.736485481262207 | KNN Loss: 3.695587396621704 | BCE Loss: 1.0408982038497925\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.754393577575684 | KNN Loss: 3.707207202911377 | BCE Loss: 1.0471861362457275\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.729045867919922 | KNN Loss: 3.724731922149658 | BCE Loss: 1.0043139457702637\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.731782913208008 | KNN Loss: 3.7073862552642822 | BCE Loss: 1.0243966579437256\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.799835205078125 | KNN Loss: 3.746537685394287 | BCE Loss: 1.0532972812652588\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.794945240020752 | KNN Loss: 3.7520720958709717 | BCE Loss: 1.0428731441497803\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.727572917938232 | KNN Loss: 3.70125150680542 | BCE Loss: 1.0263214111328125\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.713799953460693 | KNN Loss: 3.7204740047454834 | BCE Loss: 0.9933260679244995\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.751522541046143 | KNN Loss: 3.707221746444702 | BCE Loss: 1.04430091381073\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.72962760925293 | KNN Loss: 3.704195976257324 | BCE Loss: 1.0254318714141846\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.698792457580566 | KNN Loss: 3.6731679439544678 | BCE Loss: 1.0256245136260986\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.728322505950928 | KNN Loss: 3.6927337646484375 | BCE Loss: 1.0355887413024902\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.742727279663086 | KNN Loss: 3.697431802749634 | BCE Loss: 1.0452955961227417\n",
      "Epoch    90: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.689728260040283 | KNN Loss: 3.684800863265991 | BCE Loss: 1.0049272775650024\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.802255630493164 | KNN Loss: 3.7633509635925293 | BCE Loss: 1.0389046669006348\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.7107157707214355 | KNN Loss: 3.675234317779541 | BCE Loss: 1.035481333732605\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.717042446136475 | KNN Loss: 3.6837100982666016 | BCE Loss: 1.0333324670791626\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.750461101531982 | KNN Loss: 3.733182668685913 | BCE Loss: 1.0172785520553589\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.74264669418335 | KNN Loss: 3.7167165279388428 | BCE Loss: 1.0259302854537964\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.753144264221191 | KNN Loss: 3.724686861038208 | BCE Loss: 1.0284576416015625\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.741106986999512 | KNN Loss: 3.71907901763916 | BCE Loss: 1.0220279693603516\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.750102996826172 | KNN Loss: 3.690080165863037 | BCE Loss: 1.0600228309631348\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.743703365325928 | KNN Loss: 3.7401201725006104 | BCE Loss: 1.0035831928253174\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.696526050567627 | KNN Loss: 3.667259931564331 | BCE Loss: 1.029266119003296\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.740732192993164 | KNN Loss: 3.708570957183838 | BCE Loss: 1.0321611166000366\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.734185695648193 | KNN Loss: 3.6875665187835693 | BCE Loss: 1.0466190576553345\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.769273281097412 | KNN Loss: 3.747706174850464 | BCE Loss: 1.0215672254562378\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.685159206390381 | KNN Loss: 3.6933720111846924 | BCE Loss: 0.9917871952056885\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.758728981018066 | KNN Loss: 3.7203378677368164 | BCE Loss: 1.038390874862671\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.743080139160156 | KNN Loss: 3.7140591144561768 | BCE Loss: 1.0290210247039795\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.69000244140625 | KNN Loss: 3.6907436847686768 | BCE Loss: 0.9992585182189941\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.711522579193115 | KNN Loss: 3.686037540435791 | BCE Loss: 1.0254850387573242\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.7457194328308105 | KNN Loss: 3.712616443634033 | BCE Loss: 1.033103108406067\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.722700595855713 | KNN Loss: 3.706186532974243 | BCE Loss: 1.0165140628814697\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.736930847167969 | KNN Loss: 3.696232318878174 | BCE Loss: 1.0406982898712158\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.808034896850586 | KNN Loss: 3.7712910175323486 | BCE Loss: 1.0367436408996582\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.788117408752441 | KNN Loss: 3.752009630203247 | BCE Loss: 1.0361077785491943\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.721277236938477 | KNN Loss: 3.7076847553253174 | BCE Loss: 1.01359224319458\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.735047340393066 | KNN Loss: 3.7052459716796875 | BCE Loss: 1.029801368713379\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.7514472007751465 | KNN Loss: 3.7048168182373047 | BCE Loss: 1.0466303825378418\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.7457685470581055 | KNN Loss: 3.71909761428833 | BCE Loss: 1.0266711711883545\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.782727241516113 | KNN Loss: 3.7595174312591553 | BCE Loss: 1.0232096910476685\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.712536811828613 | KNN Loss: 3.6825125217437744 | BCE Loss: 1.030024528503418\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.733582496643066 | KNN Loss: 3.683016538619995 | BCE Loss: 1.0505661964416504\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.740841388702393 | KNN Loss: 3.6883277893066406 | BCE Loss: 1.052513599395752\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.748109817504883 | KNN Loss: 3.6999733448028564 | BCE Loss: 1.0481362342834473\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.76182746887207 | KNN Loss: 3.7317869663238525 | BCE Loss: 1.0300402641296387\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.67567777633667 | KNN Loss: 3.6596996784210205 | BCE Loss: 1.015978217124939\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.696389675140381 | KNN Loss: 3.6440024375915527 | BCE Loss: 1.0523872375488281\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.717075824737549 | KNN Loss: 3.669839859008789 | BCE Loss: 1.0472358465194702\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.699669361114502 | KNN Loss: 3.6778042316436768 | BCE Loss: 1.0218651294708252\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.762810707092285 | KNN Loss: 3.7189502716064453 | BCE Loss: 1.0438604354858398\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.698355674743652 | KNN Loss: 3.689150810241699 | BCE Loss: 1.0092048645019531\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.7771148681640625 | KNN Loss: 3.7410082817077637 | BCE Loss: 1.036106824874878\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.754615783691406 | KNN Loss: 3.7253565788269043 | BCE Loss: 1.0292589664459229\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.734837055206299 | KNN Loss: 3.7016448974609375 | BCE Loss: 1.0331922769546509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.726352214813232 | KNN Loss: 3.6945323944091797 | BCE Loss: 1.0318198204040527\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.732864856719971 | KNN Loss: 3.7146449089050293 | BCE Loss: 1.0182199478149414\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.753122329711914 | KNN Loss: 3.708754539489746 | BCE Loss: 1.0443676710128784\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.735610485076904 | KNN Loss: 3.705475330352783 | BCE Loss: 1.030135154724121\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.734410285949707 | KNN Loss: 3.7329373359680176 | BCE Loss: 1.001473069190979\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.749842643737793 | KNN Loss: 3.721266269683838 | BCE Loss: 1.0285766124725342\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.754935264587402 | KNN Loss: 3.708028793334961 | BCE Loss: 1.0469067096710205\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.786403179168701 | KNN Loss: 3.734344005584717 | BCE Loss: 1.052059292793274\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.77133321762085 | KNN Loss: 3.7265546321868896 | BCE Loss: 1.0447784662246704\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.718505382537842 | KNN Loss: 3.671457290649414 | BCE Loss: 1.0470480918884277\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.736928462982178 | KNN Loss: 3.712381362915039 | BCE Loss: 1.0245471000671387\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.741122245788574 | KNN Loss: 3.70434832572937 | BCE Loss: 1.036773681640625\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.730767250061035 | KNN Loss: 3.7027060985565186 | BCE Loss: 1.0280609130859375\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.729012489318848 | KNN Loss: 3.7105355262756348 | BCE Loss: 1.0184770822525024\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.698973655700684 | KNN Loss: 3.695775032043457 | BCE Loss: 1.0031988620758057\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.740843296051025 | KNN Loss: 3.6964635848999023 | BCE Loss: 1.0443798303604126\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.79367208480835 | KNN Loss: 3.724414110183716 | BCE Loss: 1.0692578554153442\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.686649322509766 | KNN Loss: 3.668701410293579 | BCE Loss: 1.0179479122161865\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.705360412597656 | KNN Loss: 3.692216157913208 | BCE Loss: 1.0131440162658691\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.744729995727539 | KNN Loss: 3.7031354904174805 | BCE Loss: 1.0415945053100586\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.769736289978027 | KNN Loss: 3.7330682277679443 | BCE Loss: 1.036668300628662\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.721412658691406 | KNN Loss: 3.707437753677368 | BCE Loss: 1.0139750242233276\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.745357990264893 | KNN Loss: 3.7227611541748047 | BCE Loss: 1.0225969552993774\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.749490261077881 | KNN Loss: 3.7128658294677734 | BCE Loss: 1.0366244316101074\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.740189552307129 | KNN Loss: 3.737748622894287 | BCE Loss: 1.0024406909942627\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.72776985168457 | KNN Loss: 3.708768129348755 | BCE Loss: 1.019001841545105\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.712709426879883 | KNN Loss: 3.686640739440918 | BCE Loss: 1.0260685682296753\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.7690534591674805 | KNN Loss: 3.7289819717407227 | BCE Loss: 1.040071725845337\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.67372465133667 | KNN Loss: 3.655895471572876 | BCE Loss: 1.0178292989730835\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.764252185821533 | KNN Loss: 3.729780673980713 | BCE Loss: 1.0344715118408203\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.731875896453857 | KNN Loss: 3.7053146362304688 | BCE Loss: 1.0265612602233887\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.735562324523926 | KNN Loss: 3.70853328704834 | BCE Loss: 1.0270287990570068\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.705935478210449 | KNN Loss: 3.6757688522338867 | BCE Loss: 1.0301663875579834\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.71663761138916 | KNN Loss: 3.7070932388305664 | BCE Loss: 1.0095446109771729\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.704693794250488 | KNN Loss: 3.668292999267578 | BCE Loss: 1.036400556564331\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.660185813903809 | KNN Loss: 3.67524790763855 | BCE Loss: 0.9849377274513245\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.738310813903809 | KNN Loss: 3.7043285369873047 | BCE Loss: 1.033982276916504\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.727259635925293 | KNN Loss: 3.708747625350952 | BCE Loss: 1.0185120105743408\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.7166571617126465 | KNN Loss: 3.685997247695923 | BCE Loss: 1.0306600332260132\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.696192741394043 | KNN Loss: 3.6765217781066895 | BCE Loss: 1.0196707248687744\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.702043533325195 | KNN Loss: 3.687654733657837 | BCE Loss: 1.0143890380859375\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.723564624786377 | KNN Loss: 3.6921300888061523 | BCE Loss: 1.0314345359802246\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.756223678588867 | KNN Loss: 3.695281982421875 | BCE Loss: 1.060941457748413\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.730936527252197 | KNN Loss: 3.709287405014038 | BCE Loss: 1.0216490030288696\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.7595062255859375 | KNN Loss: 3.74617338180542 | BCE Loss: 1.013332724571228\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.71584939956665 | KNN Loss: 3.6821155548095703 | BCE Loss: 1.0337339639663696\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.752560138702393 | KNN Loss: 3.6960666179656982 | BCE Loss: 1.0564936399459839\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.720202922821045 | KNN Loss: 3.6981146335601807 | BCE Loss: 1.0220882892608643\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.770334243774414 | KNN Loss: 3.7347707748413086 | BCE Loss: 1.035563588142395\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.716876029968262 | KNN Loss: 3.70040225982666 | BCE Loss: 1.0164740085601807\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.710404872894287 | KNN Loss: 3.6922478675842285 | BCE Loss: 1.0181570053100586\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.7277350425720215 | KNN Loss: 3.7115516662597656 | BCE Loss: 1.0161834955215454\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.725357532501221 | KNN Loss: 3.7003560066223145 | BCE Loss: 1.0250015258789062\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.682779312133789 | KNN Loss: 3.6524834632873535 | BCE Loss: 1.030295729637146\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.749124050140381 | KNN Loss: 3.7143325805664062 | BCE Loss: 1.0347914695739746\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.789957046508789 | KNN Loss: 3.7469491958618164 | BCE Loss: 1.0430078506469727\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.709502696990967 | KNN Loss: 3.7169506549835205 | BCE Loss: 0.9925522208213806\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.775236129760742 | KNN Loss: 3.711867570877075 | BCE Loss: 1.063368797302246\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.683737277984619 | KNN Loss: 3.674928903579712 | BCE Loss: 1.0088083744049072\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.805753707885742 | KNN Loss: 3.742480993270874 | BCE Loss: 1.0632728338241577\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.732455253601074 | KNN Loss: 3.711121082305908 | BCE Loss: 1.021334171295166\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.76493501663208 | KNN Loss: 3.7246837615966797 | BCE Loss: 1.0402512550354004\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.7736358642578125 | KNN Loss: 3.7499070167541504 | BCE Loss: 1.023728609085083\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.748161315917969 | KNN Loss: 3.707115888595581 | BCE Loss: 1.0410454273223877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.768698692321777 | KNN Loss: 3.7170653343200684 | BCE Loss: 1.0516332387924194\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.730510234832764 | KNN Loss: 3.7095818519592285 | BCE Loss: 1.0209283828735352\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.746492385864258 | KNN Loss: 3.703134775161743 | BCE Loss: 1.0433573722839355\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.768903732299805 | KNN Loss: 3.7134742736816406 | BCE Loss: 1.055429458618164\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.6883697509765625 | KNN Loss: 3.681579828262329 | BCE Loss: 1.0067898035049438\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.808526515960693 | KNN Loss: 3.7909724712371826 | BCE Loss: 1.0175540447235107\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.7757768630981445 | KNN Loss: 3.7663114070892334 | BCE Loss: 1.009465217590332\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.735389709472656 | KNN Loss: 3.6886401176452637 | BCE Loss: 1.0467495918273926\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.73417854309082 | KNN Loss: 3.708635091781616 | BCE Loss: 1.025543451309204\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.791437149047852 | KNN Loss: 3.7384045124053955 | BCE Loss: 1.0530328750610352\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.680761337280273 | KNN Loss: 3.6673271656036377 | BCE Loss: 1.0134344100952148\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.700211524963379 | KNN Loss: 3.678823232650757 | BCE Loss: 1.021388292312622\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.70326566696167 | KNN Loss: 3.6927831172943115 | BCE Loss: 1.010482668876648\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.68450403213501 | KNN Loss: 3.6613121032714844 | BCE Loss: 1.0231919288635254\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.754860877990723 | KNN Loss: 3.73576283454895 | BCE Loss: 1.019097924232483\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.736850261688232 | KNN Loss: 3.7094969749450684 | BCE Loss: 1.027353286743164\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.768688678741455 | KNN Loss: 3.730713129043579 | BCE Loss: 1.037975549697876\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.784626007080078 | KNN Loss: 3.7798449993133545 | BCE Loss: 1.0047810077667236\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.756643295288086 | KNN Loss: 3.7250776290893555 | BCE Loss: 1.0315654277801514\n",
      "Epoch   111: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.706434726715088 | KNN Loss: 3.6636083126068115 | BCE Loss: 1.042826533317566\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.803709030151367 | KNN Loss: 3.744931697845459 | BCE Loss: 1.0587773323059082\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.77094841003418 | KNN Loss: 3.7304089069366455 | BCE Loss: 1.0405397415161133\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.745872497558594 | KNN Loss: 3.696509599685669 | BCE Loss: 1.0493626594543457\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.786964416503906 | KNN Loss: 3.7291314601898193 | BCE Loss: 1.0578327178955078\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.709189414978027 | KNN Loss: 3.700363874435425 | BCE Loss: 1.0088257789611816\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.709327220916748 | KNN Loss: 3.7272403240203857 | BCE Loss: 0.9820868372917175\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.774532318115234 | KNN Loss: 3.748507261276245 | BCE Loss: 1.0260250568389893\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.739048957824707 | KNN Loss: 3.67875337600708 | BCE Loss: 1.0602953433990479\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.773045063018799 | KNN Loss: 3.7456343173980713 | BCE Loss: 1.027410626411438\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.702356815338135 | KNN Loss: 3.6896395683288574 | BCE Loss: 1.0127172470092773\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.713804244995117 | KNN Loss: 3.69482159614563 | BCE Loss: 1.0189828872680664\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.773279190063477 | KNN Loss: 3.7355594635009766 | BCE Loss: 1.037719964981079\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.7607831954956055 | KNN Loss: 3.7222702503204346 | BCE Loss: 1.038512945175171\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.749243259429932 | KNN Loss: 3.7449791431427 | BCE Loss: 1.0042641162872314\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.707601547241211 | KNN Loss: 3.6816558837890625 | BCE Loss: 1.0259459018707275\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.721920967102051 | KNN Loss: 3.687767744064331 | BCE Loss: 1.0341531038284302\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.7331624031066895 | KNN Loss: 3.6905007362365723 | BCE Loss: 1.0426615476608276\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.735023021697998 | KNN Loss: 3.677856922149658 | BCE Loss: 1.0571659803390503\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.693133354187012 | KNN Loss: 3.677845001220703 | BCE Loss: 1.0152885913848877\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.703895568847656 | KNN Loss: 3.6910314559936523 | BCE Loss: 1.0128638744354248\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.719438552856445 | KNN Loss: 3.680877447128296 | BCE Loss: 1.0385613441467285\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.692508697509766 | KNN Loss: 3.6654584407806396 | BCE Loss: 1.027050256729126\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.728875160217285 | KNN Loss: 3.7112956047058105 | BCE Loss: 1.0175797939300537\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.685100555419922 | KNN Loss: 3.670032262802124 | BCE Loss: 1.0150682926177979\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.73136043548584 | KNN Loss: 3.678621768951416 | BCE Loss: 1.0527386665344238\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.675561904907227 | KNN Loss: 3.669617176055908 | BCE Loss: 1.0059444904327393\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.795833110809326 | KNN Loss: 3.7842342853546143 | BCE Loss: 1.011598825454712\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.6894731521606445 | KNN Loss: 3.669445037841797 | BCE Loss: 1.020027995109558\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.687207221984863 | KNN Loss: 3.6642873287200928 | BCE Loss: 1.0229201316833496\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.724447727203369 | KNN Loss: 3.677262783050537 | BCE Loss: 1.0471848249435425\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.740720748901367 | KNN Loss: 3.718327045440674 | BCE Loss: 1.0223934650421143\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.696652412414551 | KNN Loss: 3.694121837615967 | BCE Loss: 1.002530813217163\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.675917148590088 | KNN Loss: 3.678518533706665 | BCE Loss: 0.9973984360694885\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.745624542236328 | KNN Loss: 3.7259716987609863 | BCE Loss: 1.0196528434753418\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.749109268188477 | KNN Loss: 3.703399896621704 | BCE Loss: 1.0457093715667725\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.784116744995117 | KNN Loss: 3.759648084640503 | BCE Loss: 1.0244685411453247\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.6794586181640625 | KNN Loss: 3.677513599395752 | BCE Loss: 1.001944899559021\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.748473167419434 | KNN Loss: 3.6933863162994385 | BCE Loss: 1.0550870895385742\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.7191386222839355 | KNN Loss: 3.698725700378418 | BCE Loss: 1.0204129219055176\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.705501079559326 | KNN Loss: 3.6871256828308105 | BCE Loss: 1.018375277519226\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.754688739776611 | KNN Loss: 3.714218854904175 | BCE Loss: 1.0404698848724365\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.729948997497559 | KNN Loss: 3.7280235290527344 | BCE Loss: 1.0019254684448242\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.730215072631836 | KNN Loss: 3.6808130741119385 | BCE Loss: 1.0494019985198975\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.676530838012695 | KNN Loss: 3.6633920669555664 | BCE Loss: 1.013139009475708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.721090793609619 | KNN Loss: 3.696495294570923 | BCE Loss: 1.0245953798294067\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.762164115905762 | KNN Loss: 3.715609550476074 | BCE Loss: 1.046554684638977\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.714425086975098 | KNN Loss: 3.6814746856689453 | BCE Loss: 1.032950520515442\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.681241035461426 | KNN Loss: 3.6533665657043457 | BCE Loss: 1.02787446975708\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.733404159545898 | KNN Loss: 3.727436065673828 | BCE Loss: 1.0059682130813599\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.7258687019348145 | KNN Loss: 3.6961257457733154 | BCE Loss: 1.0297428369522095\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.731088638305664 | KNN Loss: 3.7130115032196045 | BCE Loss: 1.0180772542953491\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.739720344543457 | KNN Loss: 3.7130990028381348 | BCE Loss: 1.0266214609146118\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.706398010253906 | KNN Loss: 3.6833372116088867 | BCE Loss: 1.0230610370635986\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.716943740844727 | KNN Loss: 3.6924965381622314 | BCE Loss: 1.024446964263916\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.76361083984375 | KNN Loss: 3.7143852710723877 | BCE Loss: 1.0492253303527832\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.716360092163086 | KNN Loss: 3.6998159885406494 | BCE Loss: 1.016544222831726\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.734194755554199 | KNN Loss: 3.694546699523926 | BCE Loss: 1.0396478176116943\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.788601875305176 | KNN Loss: 3.7326362133026123 | BCE Loss: 1.0559659004211426\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.68626594543457 | KNN Loss: 3.6778275966644287 | BCE Loss: 1.0084385871887207\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.731289863586426 | KNN Loss: 3.7061712741851807 | BCE Loss: 1.0251188278198242\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.755321025848389 | KNN Loss: 3.7259600162506104 | BCE Loss: 1.0293611288070679\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.690409183502197 | KNN Loss: 3.6996147632598877 | BCE Loss: 0.9907944202423096\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.75371789932251 | KNN Loss: 3.712229013442993 | BCE Loss: 1.041488766670227\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.771396636962891 | KNN Loss: 3.7214951515197754 | BCE Loss: 1.0499017238616943\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.719697952270508 | KNN Loss: 3.702058792114258 | BCE Loss: 1.0176390409469604\n",
      "Epoch   122: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.709689617156982 | KNN Loss: 3.688307285308838 | BCE Loss: 1.0213823318481445\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.783854961395264 | KNN Loss: 3.7229104042053223 | BCE Loss: 1.060944676399231\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.748767852783203 | KNN Loss: 3.7271814346313477 | BCE Loss: 1.0215864181518555\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.776495933532715 | KNN Loss: 3.7238590717315674 | BCE Loss: 1.0526368618011475\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.743157386779785 | KNN Loss: 3.697035789489746 | BCE Loss: 1.04612135887146\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.727177143096924 | KNN Loss: 3.698707103729248 | BCE Loss: 1.0284699201583862\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.734362602233887 | KNN Loss: 3.712472677230835 | BCE Loss: 1.0218901634216309\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.681346893310547 | KNN Loss: 3.6612942218780518 | BCE Loss: 1.0200529098510742\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.712116718292236 | KNN Loss: 3.6969261169433594 | BCE Loss: 1.0151904821395874\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.714519023895264 | KNN Loss: 3.6845791339874268 | BCE Loss: 1.0299397706985474\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.7280144691467285 | KNN Loss: 3.6802821159362793 | BCE Loss: 1.0477323532104492\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.701330184936523 | KNN Loss: 3.665877103805542 | BCE Loss: 1.0354528427124023\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.684352874755859 | KNN Loss: 3.6975314617156982 | BCE Loss: 0.9868215918540955\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.692814350128174 | KNN Loss: 3.6698248386383057 | BCE Loss: 1.0229895114898682\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.703419208526611 | KNN Loss: 3.676516532897949 | BCE Loss: 1.0269025564193726\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.746974945068359 | KNN Loss: 3.7103686332702637 | BCE Loss: 1.0366060733795166\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.745547294616699 | KNN Loss: 3.713247537612915 | BCE Loss: 1.0322997570037842\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.758677005767822 | KNN Loss: 3.718940258026123 | BCE Loss: 1.0397367477416992\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.709644794464111 | KNN Loss: 3.6780083179473877 | BCE Loss: 1.0316365957260132\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.734114646911621 | KNN Loss: 3.689025402069092 | BCE Loss: 1.0450894832611084\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.754364490509033 | KNN Loss: 3.7173359394073486 | BCE Loss: 1.037028431892395\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.727652549743652 | KNN Loss: 3.6702687740325928 | BCE Loss: 1.0573840141296387\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.696282386779785 | KNN Loss: 3.6649911403656006 | BCE Loss: 1.0312910079956055\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.787209510803223 | KNN Loss: 3.7408652305603027 | BCE Loss: 1.0463441610336304\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.741744041442871 | KNN Loss: 3.6908512115478516 | BCE Loss: 1.05089271068573\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.7840728759765625 | KNN Loss: 3.7255399227142334 | BCE Loss: 1.05853271484375\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.70941162109375 | KNN Loss: 3.6946628093719482 | BCE Loss: 1.0147485733032227\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.7618408203125 | KNN Loss: 3.7384512424468994 | BCE Loss: 1.0233898162841797\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.675896167755127 | KNN Loss: 3.6849493980407715 | BCE Loss: 0.990946888923645\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.743404865264893 | KNN Loss: 3.7160353660583496 | BCE Loss: 1.0273696184158325\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.729863166809082 | KNN Loss: 3.7037320137023926 | BCE Loss: 1.0261309146881104\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.6794891357421875 | KNN Loss: 3.681551456451416 | BCE Loss: 0.9979378581047058\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.733905792236328 | KNN Loss: 3.6879069805145264 | BCE Loss: 1.0459988117218018\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.722637176513672 | KNN Loss: 3.693615674972534 | BCE Loss: 1.0290212631225586\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.713939666748047 | KNN Loss: 3.7055959701538086 | BCE Loss: 1.0083434581756592\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.712084770202637 | KNN Loss: 3.677989959716797 | BCE Loss: 1.0340945720672607\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.6844964027404785 | KNN Loss: 3.67612624168396 | BCE Loss: 1.008370280265808\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.743521690368652 | KNN Loss: 3.700279951095581 | BCE Loss: 1.0432417392730713\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.699893951416016 | KNN Loss: 3.6876533031463623 | BCE Loss: 1.0122407674789429\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.733729362487793 | KNN Loss: 3.68880033493042 | BCE Loss: 1.044928789138794\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.718907356262207 | KNN Loss: 3.701096534729004 | BCE Loss: 1.0178110599517822\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.72135066986084 | KNN Loss: 3.694094181060791 | BCE Loss: 1.0272562503814697\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.708636283874512 | KNN Loss: 3.69777250289917 | BCE Loss: 1.010864019393921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.71942138671875 | KNN Loss: 3.723304033279419 | BCE Loss: 0.9961174726486206\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.719966888427734 | KNN Loss: 3.6803359985351562 | BCE Loss: 1.039630651473999\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.73471212387085 | KNN Loss: 3.7119104862213135 | BCE Loss: 1.0228015184402466\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.762751579284668 | KNN Loss: 3.7187418937683105 | BCE Loss: 1.0440094470977783\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.7164306640625 | KNN Loss: 3.658285617828369 | BCE Loss: 1.0581451654434204\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.794734954833984 | KNN Loss: 3.733668804168701 | BCE Loss: 1.0610661506652832\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.6592116355896 | KNN Loss: 3.6446800231933594 | BCE Loss: 1.0145316123962402\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.765515327453613 | KNN Loss: 3.7434771060943604 | BCE Loss: 1.022038221359253\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.717893600463867 | KNN Loss: 3.708232879638672 | BCE Loss: 1.0096604824066162\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.752872467041016 | KNN Loss: 3.7225863933563232 | BCE Loss: 1.0302860736846924\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.744200706481934 | KNN Loss: 3.690784215927124 | BCE Loss: 1.0534167289733887\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.737603187561035 | KNN Loss: 3.7023088932037354 | BCE Loss: 1.0352940559387207\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.766245365142822 | KNN Loss: 3.7374627590179443 | BCE Loss: 1.028782606124878\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.727064609527588 | KNN Loss: 3.702915668487549 | BCE Loss: 1.024148941040039\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.705142498016357 | KNN Loss: 3.6755948066711426 | BCE Loss: 1.0295478105545044\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.663322925567627 | KNN Loss: 3.666152238845825 | BCE Loss: 0.9971705079078674\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.757232189178467 | KNN Loss: 3.699685573577881 | BCE Loss: 1.057546615600586\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.695731163024902 | KNN Loss: 3.682331085205078 | BCE Loss: 1.0134000778198242\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.69780969619751 | KNN Loss: 3.6794419288635254 | BCE Loss: 1.0183677673339844\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.739794731140137 | KNN Loss: 3.703425645828247 | BCE Loss: 1.0363693237304688\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.713083267211914 | KNN Loss: 3.674362897872925 | BCE Loss: 1.0387203693389893\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.701778411865234 | KNN Loss: 3.692762851715088 | BCE Loss: 1.0090153217315674\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.680429458618164 | KNN Loss: 3.6590232849121094 | BCE Loss: 1.0214059352874756\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.721742630004883 | KNN Loss: 3.697054624557495 | BCE Loss: 1.0246880054473877\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.748629570007324 | KNN Loss: 3.7271244525909424 | BCE Loss: 1.0215048789978027\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.786900043487549 | KNN Loss: 3.737499952316284 | BCE Loss: 1.0494000911712646\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.79001522064209 | KNN Loss: 3.7516725063323975 | BCE Loss: 1.0383429527282715\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.673459529876709 | KNN Loss: 3.664695978164673 | BCE Loss: 1.0087636709213257\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.749304294586182 | KNN Loss: 3.7071304321289062 | BCE Loss: 1.042173981666565\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.717245101928711 | KNN Loss: 3.675030469894409 | BCE Loss: 1.0422143936157227\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.779754638671875 | KNN Loss: 3.735694408416748 | BCE Loss: 1.044060230255127\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.773966312408447 | KNN Loss: 3.7473652362823486 | BCE Loss: 1.026600956916809\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.716135025024414 | KNN Loss: 3.696472644805908 | BCE Loss: 1.0196623802185059\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.705301284790039 | KNN Loss: 3.6900436878204346 | BCE Loss: 1.0152575969696045\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.731417655944824 | KNN Loss: 3.699774980545044 | BCE Loss: 1.0316426753997803\n",
      "Epoch   135: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.690182209014893 | KNN Loss: 3.672386884689331 | BCE Loss: 1.0177953243255615\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.750256061553955 | KNN Loss: 3.7288260459899902 | BCE Loss: 1.0214300155639648\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.739130020141602 | KNN Loss: 3.7066445350646973 | BCE Loss: 1.0324853658676147\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.713168144226074 | KNN Loss: 3.680799722671509 | BCE Loss: 1.0323684215545654\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.6716437339782715 | KNN Loss: 3.6657330989837646 | BCE Loss: 1.0059107542037964\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.725612640380859 | KNN Loss: 3.6935839653015137 | BCE Loss: 1.0320289134979248\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.732728004455566 | KNN Loss: 3.681788682937622 | BCE Loss: 1.0509393215179443\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.7911787033081055 | KNN Loss: 3.7301690578460693 | BCE Loss: 1.0610096454620361\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.7508015632629395 | KNN Loss: 3.7061593532562256 | BCE Loss: 1.0446420907974243\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.754946708679199 | KNN Loss: 3.736424207687378 | BCE Loss: 1.0185225009918213\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.717967987060547 | KNN Loss: 3.6914114952087402 | BCE Loss: 1.0265567302703857\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.7423415184021 | KNN Loss: 3.7153544425964355 | BCE Loss: 1.0269871950149536\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.75009298324585 | KNN Loss: 3.717728853225708 | BCE Loss: 1.0323641300201416\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.754070281982422 | KNN Loss: 3.7192156314849854 | BCE Loss: 1.0348548889160156\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.6830267906188965 | KNN Loss: 3.676187753677368 | BCE Loss: 1.0068390369415283\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.698432922363281 | KNN Loss: 3.6855392456054688 | BCE Loss: 1.0128936767578125\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.761099338531494 | KNN Loss: 3.7353177070617676 | BCE Loss: 1.0257816314697266\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.755267143249512 | KNN Loss: 3.735778570175171 | BCE Loss: 1.0194886922836304\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.637094020843506 | KNN Loss: 3.6547863483428955 | BCE Loss: 0.9823077917098999\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.71884298324585 | KNN Loss: 3.696380615234375 | BCE Loss: 1.0224623680114746\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.691397666931152 | KNN Loss: 3.6798465251922607 | BCE Loss: 1.0115511417388916\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.677109718322754 | KNN Loss: 3.6462976932525635 | BCE Loss: 1.0308119058609009\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.745250701904297 | KNN Loss: 3.71341872215271 | BCE Loss: 1.0318318605422974\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.718301773071289 | KNN Loss: 3.681735038757324 | BCE Loss: 1.0365667343139648\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.717347145080566 | KNN Loss: 3.695774555206299 | BCE Loss: 1.0215725898742676\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.676313877105713 | KNN Loss: 3.66637921333313 | BCE Loss: 1.0099345445632935\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.741316795349121 | KNN Loss: 3.7148005962371826 | BCE Loss: 1.0265161991119385\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.694052696228027 | KNN Loss: 3.676673650741577 | BCE Loss: 1.0173792839050293\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.7583088874816895 | KNN Loss: 3.717970371246338 | BCE Loss: 1.040338397026062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.730654716491699 | KNN Loss: 3.6917569637298584 | BCE Loss: 1.0388975143432617\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.719101428985596 | KNN Loss: 3.6691977977752686 | BCE Loss: 1.0499035120010376\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.71204948425293 | KNN Loss: 3.706813097000122 | BCE Loss: 1.005236268043518\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.752737045288086 | KNN Loss: 3.7210421562194824 | BCE Loss: 1.0316946506500244\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.721447467803955 | KNN Loss: 3.682311534881592 | BCE Loss: 1.0391359329223633\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.693603515625 | KNN Loss: 3.670137882232666 | BCE Loss: 1.0234657526016235\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.739391326904297 | KNN Loss: 3.7140326499938965 | BCE Loss: 1.0253584384918213\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.694085121154785 | KNN Loss: 3.662022113800049 | BCE Loss: 1.0320627689361572\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.733493328094482 | KNN Loss: 3.7020838260650635 | BCE Loss: 1.031409502029419\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.758627414703369 | KNN Loss: 3.7197957038879395 | BCE Loss: 1.0388318300247192\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.7236833572387695 | KNN Loss: 3.6716372966766357 | BCE Loss: 1.0520458221435547\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.721553802490234 | KNN Loss: 3.6821253299713135 | BCE Loss: 1.039428472518921\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.714529037475586 | KNN Loss: 3.7005321979522705 | BCE Loss: 1.0139968395233154\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.713751792907715 | KNN Loss: 3.67268443107605 | BCE Loss: 1.041067361831665\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.721068859100342 | KNN Loss: 3.676506519317627 | BCE Loss: 1.0445624589920044\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.7015380859375 | KNN Loss: 3.6708953380584717 | BCE Loss: 1.0306428670883179\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.7218918800354 | KNN Loss: 3.682382345199585 | BCE Loss: 1.0395095348358154\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.751238822937012 | KNN Loss: 3.7461769580841064 | BCE Loss: 1.0050616264343262\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.7543625831604 | KNN Loss: 3.7145049571990967 | BCE Loss: 1.0398576259613037\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.749975204467773 | KNN Loss: 3.7089786529541016 | BCE Loss: 1.040996789932251\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.687613010406494 | KNN Loss: 3.680656909942627 | BCE Loss: 1.0069559812545776\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.727524757385254 | KNN Loss: 3.687803268432617 | BCE Loss: 1.0397214889526367\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.714071750640869 | KNN Loss: 3.679046154022217 | BCE Loss: 1.0350255966186523\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.773164749145508 | KNN Loss: 3.74196195602417 | BCE Loss: 1.031202793121338\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.674951076507568 | KNN Loss: 3.6630218029022217 | BCE Loss: 1.0119292736053467\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.722251892089844 | KNN Loss: 3.7011969089508057 | BCE Loss: 1.0210552215576172\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.728478908538818 | KNN Loss: 3.69385027885437 | BCE Loss: 1.0346286296844482\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.738109588623047 | KNN Loss: 3.6896746158599854 | BCE Loss: 1.0484352111816406\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.690898895263672 | KNN Loss: 3.6652262210845947 | BCE Loss: 1.0256726741790771\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.739704132080078 | KNN Loss: 3.7041854858398438 | BCE Loss: 1.0355184078216553\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.7001495361328125 | KNN Loss: 3.655780792236328 | BCE Loss: 1.0443687438964844\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.686253547668457 | KNN Loss: 3.69166898727417 | BCE Loss: 0.9945847392082214\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.731612682342529 | KNN Loss: 3.6988275051116943 | BCE Loss: 1.032785177230835\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.707243919372559 | KNN Loss: 3.7006685733795166 | BCE Loss: 1.006575345993042\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.719593524932861 | KNN Loss: 3.6809325218200684 | BCE Loss: 1.038661003112793\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.678900718688965 | KNN Loss: 3.6670732498168945 | BCE Loss: 1.0118275880813599\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.7051286697387695 | KNN Loss: 3.6750149726867676 | BCE Loss: 1.0301134586334229\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.76984977722168 | KNN Loss: 3.7386069297790527 | BCE Loss: 1.0312427282333374\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.810720443725586 | KNN Loss: 3.750227451324463 | BCE Loss: 1.060492992401123\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.719901084899902 | KNN Loss: 3.686192035675049 | BCE Loss: 1.0337088108062744\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.717548847198486 | KNN Loss: 3.703859329223633 | BCE Loss: 1.0136895179748535\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.731113433837891 | KNN Loss: 3.695239305496216 | BCE Loss: 1.0358741283416748\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.799239158630371 | KNN Loss: 3.781374931335449 | BCE Loss: 1.0178639888763428\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.734645366668701 | KNN Loss: 3.707564115524292 | BCE Loss: 1.0270812511444092\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.6834611892700195 | KNN Loss: 3.671419143676758 | BCE Loss: 1.0120418071746826\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.7564616203308105 | KNN Loss: 3.6772613525390625 | BCE Loss: 1.079200267791748\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.758097171783447 | KNN Loss: 3.7198617458343506 | BCE Loss: 1.0382354259490967\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.725373268127441 | KNN Loss: 3.707838773727417 | BCE Loss: 1.0175344944000244\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.673930644989014 | KNN Loss: 3.6739859580993652 | BCE Loss: 0.9999445676803589\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.667403221130371 | KNN Loss: 3.6655890941619873 | BCE Loss: 1.0018138885498047\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.701854228973389 | KNN Loss: 3.695037603378296 | BCE Loss: 1.0068166255950928\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.71678352355957 | KNN Loss: 3.7028236389160156 | BCE Loss: 1.0139601230621338\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.74375057220459 | KNN Loss: 3.698197603225708 | BCE Loss: 1.0455527305603027\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.699214935302734 | KNN Loss: 3.6729257106781006 | BCE Loss: 1.0262889862060547\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.754975318908691 | KNN Loss: 3.719174385070801 | BCE Loss: 1.0358006954193115\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.738765239715576 | KNN Loss: 3.686089515686035 | BCE Loss: 1.0526758432388306\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.686779499053955 | KNN Loss: 3.6557974815368652 | BCE Loss: 1.0309820175170898\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.69766902923584 | KNN Loss: 3.679241895675659 | BCE Loss: 1.0184268951416016\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.741912364959717 | KNN Loss: 3.733502149581909 | BCE Loss: 1.0084103345870972\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.692337989807129 | KNN Loss: 3.680467367172241 | BCE Loss: 1.0118707418441772\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.690370559692383 | KNN Loss: 3.677279472351074 | BCE Loss: 1.0130908489227295\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.710200309753418 | KNN Loss: 3.6699743270874023 | BCE Loss: 1.0402259826660156\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.81348991394043 | KNN Loss: 3.775226593017578 | BCE Loss: 1.0382635593414307\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.684689521789551 | KNN Loss: 3.6677632331848145 | BCE Loss: 1.0169265270233154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.692320346832275 | KNN Loss: 3.679750680923462 | BCE Loss: 1.0125696659088135\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.74092435836792 | KNN Loss: 3.6927499771118164 | BCE Loss: 1.048174262046814\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.672476768493652 | KNN Loss: 3.6641311645507812 | BCE Loss: 1.0083458423614502\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.692797660827637 | KNN Loss: 3.6731109619140625 | BCE Loss: 1.0196869373321533\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.741195201873779 | KNN Loss: 3.706538438796997 | BCE Loss: 1.0346566438674927\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.726855754852295 | KNN Loss: 3.731053352355957 | BCE Loss: 0.9958022832870483\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.709287643432617 | KNN Loss: 3.697166919708252 | BCE Loss: 1.0121204853057861\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.70134162902832 | KNN Loss: 3.693608522415161 | BCE Loss: 1.0077333450317383\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.735265731811523 | KNN Loss: 3.6949996948242188 | BCE Loss: 1.0402662754058838\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.674315452575684 | KNN Loss: 3.6595041751861572 | BCE Loss: 1.014811396598816\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.71696662902832 | KNN Loss: 3.66658878326416 | BCE Loss: 1.0503778457641602\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.747945785522461 | KNN Loss: 3.720194101333618 | BCE Loss: 1.0277519226074219\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.718086242675781 | KNN Loss: 3.6989309787750244 | BCE Loss: 1.0191552639007568\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.76226806640625 | KNN Loss: 3.7252843379974365 | BCE Loss: 1.0369837284088135\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.704920291900635 | KNN Loss: 3.6701266765594482 | BCE Loss: 1.0347936153411865\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.716487884521484 | KNN Loss: 3.708792209625244 | BCE Loss: 1.0076955556869507\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.718294143676758 | KNN Loss: 3.682333469390869 | BCE Loss: 1.0359604358673096\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.725671768188477 | KNN Loss: 3.679079532623291 | BCE Loss: 1.0465922355651855\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.747546195983887 | KNN Loss: 3.7125742435455322 | BCE Loss: 1.0349721908569336\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.7239460945129395 | KNN Loss: 3.6836159229278564 | BCE Loss: 1.0403302907943726\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.6836934089660645 | KNN Loss: 3.6985363960266113 | BCE Loss: 0.9851570725440979\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.760021209716797 | KNN Loss: 3.705228805541992 | BCE Loss: 1.0547921657562256\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.7077484130859375 | KNN Loss: 3.678868055343628 | BCE Loss: 1.0288801193237305\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.734317302703857 | KNN Loss: 3.7239906787872314 | BCE Loss: 1.0103265047073364\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.797492980957031 | KNN Loss: 3.7435503005981445 | BCE Loss: 1.0539429187774658\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.730110168457031 | KNN Loss: 3.713224411010742 | BCE Loss: 1.0168859958648682\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.69511604309082 | KNN Loss: 3.662231683731079 | BCE Loss: 1.0328845977783203\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.705689907073975 | KNN Loss: 3.669811964035034 | BCE Loss: 1.0358779430389404\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.703641414642334 | KNN Loss: 3.6886088848114014 | BCE Loss: 1.0150325298309326\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.736101150512695 | KNN Loss: 3.704594373703003 | BCE Loss: 1.0315070152282715\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.756057262420654 | KNN Loss: 3.703521251678467 | BCE Loss: 1.052535891532898\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.7435712814331055 | KNN Loss: 3.7242534160614014 | BCE Loss: 1.019317865371704\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.72956657409668 | KNN Loss: 3.71439790725708 | BCE Loss: 1.0151689052581787\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.708808898925781 | KNN Loss: 3.6909141540527344 | BCE Loss: 1.017894983291626\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.768998622894287 | KNN Loss: 3.715961217880249 | BCE Loss: 1.0530375242233276\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.746865272521973 | KNN Loss: 3.7243003845214844 | BCE Loss: 1.0225646495819092\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.717862129211426 | KNN Loss: 3.7042453289031982 | BCE Loss: 1.0136168003082275\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.74752950668335 | KNN Loss: 3.697725534439087 | BCE Loss: 1.0498039722442627\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.766022682189941 | KNN Loss: 3.736611843109131 | BCE Loss: 1.0294110774993896\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.729689121246338 | KNN Loss: 3.7042155265808105 | BCE Loss: 1.025473713874817\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.751676082611084 | KNN Loss: 3.7060623168945312 | BCE Loss: 1.0456137657165527\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.6802215576171875 | KNN Loss: 3.6689162254333496 | BCE Loss: 1.011305332183838\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.711381912231445 | KNN Loss: 3.6917498111724854 | BCE Loss: 1.01963210105896\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.7408647537231445 | KNN Loss: 3.678750991821289 | BCE Loss: 1.062113881111145\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.669760704040527 | KNN Loss: 3.6548678874969482 | BCE Loss: 1.014892578125\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.7159318923950195 | KNN Loss: 3.6870758533477783 | BCE Loss: 1.0288559198379517\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.743867874145508 | KNN Loss: 3.7047057151794434 | BCE Loss: 1.0391621589660645\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.789674758911133 | KNN Loss: 3.7487306594848633 | BCE Loss: 1.0409443378448486\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.708990097045898 | KNN Loss: 3.7029716968536377 | BCE Loss: 1.0060186386108398\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.7300262451171875 | KNN Loss: 3.7052342891693115 | BCE Loss: 1.024791955947876\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.712995529174805 | KNN Loss: 3.698171377182007 | BCE Loss: 1.0148239135742188\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.716156959533691 | KNN Loss: 3.6671135425567627 | BCE Loss: 1.0490431785583496\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.712141990661621 | KNN Loss: 3.6983771324157715 | BCE Loss: 1.0137650966644287\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.72391414642334 | KNN Loss: 3.6869957447052 | BCE Loss: 1.0369181632995605\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.6660871505737305 | KNN Loss: 3.6577956676483154 | BCE Loss: 1.008291482925415\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.766256332397461 | KNN Loss: 3.725874185562134 | BCE Loss: 1.0403823852539062\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.723480701446533 | KNN Loss: 3.6835038661956787 | BCE Loss: 1.0399768352508545\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.679502964019775 | KNN Loss: 3.6710212230682373 | BCE Loss: 1.008481740951538\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.691659927368164 | KNN Loss: 3.6608190536499023 | BCE Loss: 1.0308411121368408\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.6986799240112305 | KNN Loss: 3.666553497314453 | BCE Loss: 1.0321261882781982\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.753454685211182 | KNN Loss: 3.712801218032837 | BCE Loss: 1.0406534671783447\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.725577354431152 | KNN Loss: 3.68916654586792 | BCE Loss: 1.0364108085632324\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.779284477233887 | KNN Loss: 3.722569227218628 | BCE Loss: 1.056715488433838\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.738393783569336 | KNN Loss: 3.6950550079345703 | BCE Loss: 1.0433390140533447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.720521926879883 | KNN Loss: 3.6986353397369385 | BCE Loss: 1.0218863487243652\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.762421607971191 | KNN Loss: 3.699746608734131 | BCE Loss: 1.0626747608184814\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.748936176300049 | KNN Loss: 3.728175640106201 | BCE Loss: 1.0207605361938477\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.747129440307617 | KNN Loss: 3.719137191772461 | BCE Loss: 1.0279922485351562\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.699103355407715 | KNN Loss: 3.689349889755249 | BCE Loss: 1.0097532272338867\n",
      "Epoch   162: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.767461776733398 | KNN Loss: 3.717923402786255 | BCE Loss: 1.0495386123657227\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.740508556365967 | KNN Loss: 3.729621648788452 | BCE Loss: 1.010886788368225\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.665060043334961 | KNN Loss: 3.6605851650238037 | BCE Loss: 1.0044751167297363\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.791553020477295 | KNN Loss: 3.7433011531829834 | BCE Loss: 1.0482518672943115\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.725139617919922 | KNN Loss: 3.6966943740844727 | BCE Loss: 1.0284450054168701\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.700288772583008 | KNN Loss: 3.6752419471740723 | BCE Loss: 1.0250468254089355\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.691120624542236 | KNN Loss: 3.6759743690490723 | BCE Loss: 1.015146255493164\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.742598533630371 | KNN Loss: 3.7085556983947754 | BCE Loss: 1.0340430736541748\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.718167781829834 | KNN Loss: 3.6974234580993652 | BCE Loss: 1.0207443237304688\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.729732990264893 | KNN Loss: 3.693715810775757 | BCE Loss: 1.0360171794891357\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.748534679412842 | KNN Loss: 3.7064919471740723 | BCE Loss: 1.0420427322387695\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.713397979736328 | KNN Loss: 3.6847920417785645 | BCE Loss: 1.0286059379577637\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.763096809387207 | KNN Loss: 3.7288036346435547 | BCE Loss: 1.0342934131622314\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.719020843505859 | KNN Loss: 3.680448293685913 | BCE Loss: 1.0385724306106567\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.735647201538086 | KNN Loss: 3.697523355484009 | BCE Loss: 1.0381240844726562\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.742162704467773 | KNN Loss: 3.6928274631500244 | BCE Loss: 1.049335241317749\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.714532375335693 | KNN Loss: 3.694086790084839 | BCE Loss: 1.020445704460144\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.670981407165527 | KNN Loss: 3.6639811992645264 | BCE Loss: 1.007000207901001\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.766383647918701 | KNN Loss: 3.719996690750122 | BCE Loss: 1.046386957168579\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.708341598510742 | KNN Loss: 3.689953327178955 | BCE Loss: 1.018388271331787\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.6752824783325195 | KNN Loss: 3.6725337505340576 | BCE Loss: 1.0027484893798828\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.723564147949219 | KNN Loss: 3.6959214210510254 | BCE Loss: 1.0276426076889038\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.728710174560547 | KNN Loss: 3.6983442306518555 | BCE Loss: 1.030366063117981\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.686487197875977 | KNN Loss: 3.6814541816711426 | BCE Loss: 1.005033254623413\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.782732009887695 | KNN Loss: 3.740429639816284 | BCE Loss: 1.0423026084899902\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.7224907875061035 | KNN Loss: 3.699907064437866 | BCE Loss: 1.0225836038589478\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.707067012786865 | KNN Loss: 3.683713436126709 | BCE Loss: 1.0233535766601562\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.7097392082214355 | KNN Loss: 3.6825525760650635 | BCE Loss: 1.027186632156372\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.762507915496826 | KNN Loss: 3.705453395843506 | BCE Loss: 1.0570546388626099\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.684085845947266 | KNN Loss: 3.6638848781585693 | BCE Loss: 1.0202010869979858\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.669343948364258 | KNN Loss: 3.6522152423858643 | BCE Loss: 1.0171284675598145\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.670830726623535 | KNN Loss: 3.6904337406158447 | BCE Loss: 0.9803968667984009\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.7313313484191895 | KNN Loss: 3.706721305847168 | BCE Loss: 1.0246100425720215\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.718186855316162 | KNN Loss: 3.684419870376587 | BCE Loss: 1.0337668657302856\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.655152320861816 | KNN Loss: 3.6519174575805664 | BCE Loss: 1.0032349824905396\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.7133917808532715 | KNN Loss: 3.6921141147613525 | BCE Loss: 1.021277666091919\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.683527946472168 | KNN Loss: 3.676076889038086 | BCE Loss: 1.007451057434082\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.750560760498047 | KNN Loss: 3.716642379760742 | BCE Loss: 1.0339186191558838\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.7276082038879395 | KNN Loss: 3.716594934463501 | BCE Loss: 1.011013388633728\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.7144622802734375 | KNN Loss: 3.6634774208068848 | BCE Loss: 1.0509849786758423\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.743499755859375 | KNN Loss: 3.710117816925049 | BCE Loss: 1.0333818197250366\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.694217681884766 | KNN Loss: 3.671435832977295 | BCE Loss: 1.0227820873260498\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.702826976776123 | KNN Loss: 3.6870479583740234 | BCE Loss: 1.01577889919281\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.709184646606445 | KNN Loss: 3.7039434909820557 | BCE Loss: 1.0052410364151\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.701135158538818 | KNN Loss: 3.6475634574890137 | BCE Loss: 1.0535717010498047\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.685024261474609 | KNN Loss: 3.6734440326690674 | BCE Loss: 1.011580228805542\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.7328901290893555 | KNN Loss: 3.6995792388916016 | BCE Loss: 1.033310890197754\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.786848068237305 | KNN Loss: 3.7753732204437256 | BCE Loss: 1.0114750862121582\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.7147345542907715 | KNN Loss: 3.702749252319336 | BCE Loss: 1.0119853019714355\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.765973091125488 | KNN Loss: 3.7208452224731445 | BCE Loss: 1.0451277494430542\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.713104248046875 | KNN Loss: 3.6956377029418945 | BCE Loss: 1.017466425895691\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.759517192840576 | KNN Loss: 3.7421066761016846 | BCE Loss: 1.0174106359481812\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.718950271606445 | KNN Loss: 3.6966164112091064 | BCE Loss: 1.0223337411880493\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.711238861083984 | KNN Loss: 3.680756092071533 | BCE Loss: 1.030482530593872\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.693088531494141 | KNN Loss: 3.6432909965515137 | BCE Loss: 1.049797534942627\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.715569019317627 | KNN Loss: 3.702491521835327 | BCE Loss: 1.0130774974822998\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.7053751945495605 | KNN Loss: 3.697857141494751 | BCE Loss: 1.0075181722640991\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.671955585479736 | KNN Loss: 3.6827774047851562 | BCE Loss: 0.9891782999038696\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.725849151611328 | KNN Loss: 3.6964211463928223 | BCE Loss: 1.0294277667999268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.723074913024902 | KNN Loss: 3.6920275688171387 | BCE Loss: 1.0310475826263428\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.76519775390625 | KNN Loss: 3.7023539543151855 | BCE Loss: 1.0628437995910645\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.6731648445129395 | KNN Loss: 3.6617534160614014 | BCE Loss: 1.011411428451538\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.714776515960693 | KNN Loss: 3.705110549926758 | BCE Loss: 1.0096659660339355\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.775636672973633 | KNN Loss: 3.732318878173828 | BCE Loss: 1.0433180332183838\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.742336273193359 | KNN Loss: 3.7158775329589844 | BCE Loss: 1.026458740234375\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.7466840744018555 | KNN Loss: 3.693270683288574 | BCE Loss: 1.0534135103225708\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.74367618560791 | KNN Loss: 3.695646047592163 | BCE Loss: 1.048029899597168\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.728513240814209 | KNN Loss: 3.7020740509033203 | BCE Loss: 1.0264390707015991\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.684985160827637 | KNN Loss: 3.6673781871795654 | BCE Loss: 1.0176067352294922\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.695503234863281 | KNN Loss: 3.677713632583618 | BCE Loss: 1.017789363861084\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.785646438598633 | KNN Loss: 3.7230045795440674 | BCE Loss: 1.062641978263855\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.704217910766602 | KNN Loss: 3.6752195358276367 | BCE Loss: 1.028998613357544\n",
      "Epoch   174: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.674893379211426 | KNN Loss: 3.634098529815674 | BCE Loss: 1.0407946109771729\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.702464580535889 | KNN Loss: 3.697277307510376 | BCE Loss: 1.0051871538162231\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.742864608764648 | KNN Loss: 3.709547996520996 | BCE Loss: 1.033316731452942\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.749930381774902 | KNN Loss: 3.7218124866485596 | BCE Loss: 1.0281180143356323\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.764241695404053 | KNN Loss: 3.716081142425537 | BCE Loss: 1.0481605529785156\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.756833553314209 | KNN Loss: 3.7076213359832764 | BCE Loss: 1.049212098121643\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.705496788024902 | KNN Loss: 3.6878433227539062 | BCE Loss: 1.017653226852417\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.683723449707031 | KNN Loss: 3.6677772998809814 | BCE Loss: 1.0159461498260498\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.739016532897949 | KNN Loss: 3.6830196380615234 | BCE Loss: 1.0559971332550049\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.720195770263672 | KNN Loss: 3.699742555618286 | BCE Loss: 1.0204532146453857\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.757068634033203 | KNN Loss: 3.7017698287963867 | BCE Loss: 1.0552988052368164\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.716102600097656 | KNN Loss: 3.69347882270813 | BCE Loss: 1.0226235389709473\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.727842807769775 | KNN Loss: 3.7010180950164795 | BCE Loss: 1.026824712753296\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.661556243896484 | KNN Loss: 3.6591339111328125 | BCE Loss: 1.0024223327636719\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.733309268951416 | KNN Loss: 3.6872429847717285 | BCE Loss: 1.0460662841796875\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.664192199707031 | KNN Loss: 3.654963731765747 | BCE Loss: 1.0092283487319946\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.702127456665039 | KNN Loss: 3.678583860397339 | BCE Loss: 1.0235435962677002\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.717728614807129 | KNN Loss: 3.7007830142974854 | BCE Loss: 1.016945719718933\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.702308654785156 | KNN Loss: 3.6785264015197754 | BCE Loss: 1.02378249168396\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.709719657897949 | KNN Loss: 3.70827054977417 | BCE Loss: 1.0014489889144897\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.707798957824707 | KNN Loss: 3.7235538959503174 | BCE Loss: 0.9842450618743896\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.708609104156494 | KNN Loss: 3.681783676147461 | BCE Loss: 1.0268254280090332\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.747941017150879 | KNN Loss: 3.7161169052124023 | BCE Loss: 1.0318243503570557\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.753414630889893 | KNN Loss: 3.7274742126464844 | BCE Loss: 1.0259404182434082\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.7398786544799805 | KNN Loss: 3.700735092163086 | BCE Loss: 1.0391438007354736\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.73445987701416 | KNN Loss: 3.69638991355896 | BCE Loss: 1.038069725036621\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.726452827453613 | KNN Loss: 3.7000749111175537 | BCE Loss: 1.0263776779174805\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.7477617263793945 | KNN Loss: 3.7100727558135986 | BCE Loss: 1.0376887321472168\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.691761016845703 | KNN Loss: 3.6787662506103516 | BCE Loss: 1.012994647026062\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.770728588104248 | KNN Loss: 3.7604830265045166 | BCE Loss: 1.0102455615997314\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.7183637619018555 | KNN Loss: 3.6821506023406982 | BCE Loss: 1.0362131595611572\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.726655006408691 | KNN Loss: 3.6869239807128906 | BCE Loss: 1.0397307872772217\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.714568138122559 | KNN Loss: 3.677253007888794 | BCE Loss: 1.0373148918151855\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.730979919433594 | KNN Loss: 3.6999239921569824 | BCE Loss: 1.0310556888580322\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.705225467681885 | KNN Loss: 3.683685302734375 | BCE Loss: 1.0215402841567993\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.679980278015137 | KNN Loss: 3.65091609954834 | BCE Loss: 1.029064416885376\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.686790943145752 | KNN Loss: 3.66770339012146 | BCE Loss: 1.019087553024292\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.766103267669678 | KNN Loss: 3.7363650798797607 | BCE Loss: 1.0297383069992065\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.766098976135254 | KNN Loss: 3.722616195678711 | BCE Loss: 1.0434826612472534\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.691740989685059 | KNN Loss: 3.6569976806640625 | BCE Loss: 1.0347431898117065\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.639182090759277 | KNN Loss: 3.649282693862915 | BCE Loss: 0.9898992776870728\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.751841068267822 | KNN Loss: 3.7070348262786865 | BCE Loss: 1.0448062419891357\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.692868709564209 | KNN Loss: 3.702153444290161 | BCE Loss: 0.9907150864601135\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.682868480682373 | KNN Loss: 3.6451210975646973 | BCE Loss: 1.0377473831176758\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.709406852722168 | KNN Loss: 3.677989959716797 | BCE Loss: 1.031416654586792\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.744265556335449 | KNN Loss: 3.732166290283203 | BCE Loss: 1.0120995044708252\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.691553115844727 | KNN Loss: 3.6811821460723877 | BCE Loss: 1.0103710889816284\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.7897491455078125 | KNN Loss: 3.730304002761841 | BCE Loss: 1.0594449043273926\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.657769203186035 | KNN Loss: 3.655013084411621 | BCE Loss: 1.002755880355835\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.759815692901611 | KNN Loss: 3.7215213775634766 | BCE Loss: 1.0382943153381348\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.731578350067139 | KNN Loss: 3.6929800510406494 | BCE Loss: 1.0385984182357788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.728199481964111 | KNN Loss: 3.717276096343994 | BCE Loss: 1.0109233856201172\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.724344253540039 | KNN Loss: 3.726048469543457 | BCE Loss: 0.9982956647872925\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.70720100402832 | KNN Loss: 3.6923863887786865 | BCE Loss: 1.0148143768310547\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.755712509155273 | KNN Loss: 3.7218666076660156 | BCE Loss: 1.0338457822799683\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.698261260986328 | KNN Loss: 3.6680076122283936 | BCE Loss: 1.0302538871765137\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.702728271484375 | KNN Loss: 3.6781487464904785 | BCE Loss: 1.0245792865753174\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.738921165466309 | KNN Loss: 3.7188212871551514 | BCE Loss: 1.0200999975204468\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.722631931304932 | KNN Loss: 3.690525770187378 | BCE Loss: 1.0321061611175537\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.6925482749938965 | KNN Loss: 3.6883435249328613 | BCE Loss: 1.0042046308517456\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.688726425170898 | KNN Loss: 3.694211006164551 | BCE Loss: 0.994515597820282\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.752615928649902 | KNN Loss: 3.7207489013671875 | BCE Loss: 1.0318667888641357\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.676548957824707 | KNN Loss: 3.6549131870269775 | BCE Loss: 1.0216357707977295\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.670748710632324 | KNN Loss: 3.6458139419555664 | BCE Loss: 1.0249345302581787\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.707775115966797 | KNN Loss: 3.673668146133423 | BCE Loss: 1.0341070890426636\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.6740570068359375 | KNN Loss: 3.6576080322265625 | BCE Loss: 1.016448736190796\n",
      "Epoch   185: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.776174068450928 | KNN Loss: 3.7280282974243164 | BCE Loss: 1.0481457710266113\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.723488807678223 | KNN Loss: 3.7141990661621094 | BCE Loss: 1.0092898607254028\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.732792854309082 | KNN Loss: 3.685600757598877 | BCE Loss: 1.047192096710205\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.735315799713135 | KNN Loss: 3.7018260955810547 | BCE Loss: 1.03348970413208\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.759486198425293 | KNN Loss: 3.706401824951172 | BCE Loss: 1.0530844926834106\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.694567680358887 | KNN Loss: 3.6798412799835205 | BCE Loss: 1.0147264003753662\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.713287353515625 | KNN Loss: 3.7014918327331543 | BCE Loss: 1.0117957592010498\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.727707386016846 | KNN Loss: 3.6832327842712402 | BCE Loss: 1.0444746017456055\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.694703102111816 | KNN Loss: 3.684080123901367 | BCE Loss: 1.0106229782104492\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.712998390197754 | KNN Loss: 3.697054862976074 | BCE Loss: 1.0159436464309692\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.790750503540039 | KNN Loss: 3.735360860824585 | BCE Loss: 1.0553895235061646\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.673334121704102 | KNN Loss: 3.66003680229187 | BCE Loss: 1.013297438621521\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.7607927322387695 | KNN Loss: 3.725375175476074 | BCE Loss: 1.0354174375534058\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.751489639282227 | KNN Loss: 3.7001960277557373 | BCE Loss: 1.0512938499450684\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.779877662658691 | KNN Loss: 3.7217376232147217 | BCE Loss: 1.0581400394439697\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.68183708190918 | KNN Loss: 3.656240463256836 | BCE Loss: 1.0255963802337646\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.709751605987549 | KNN Loss: 3.69344162940979 | BCE Loss: 1.0163098573684692\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.740145683288574 | KNN Loss: 3.6958351135253906 | BCE Loss: 1.0443103313446045\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.647684574127197 | KNN Loss: 3.6428606510162354 | BCE Loss: 1.004823923110962\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.712928771972656 | KNN Loss: 3.6663146018981934 | BCE Loss: 1.0466140508651733\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.719740390777588 | KNN Loss: 3.7062997817993164 | BCE Loss: 1.013440489768982\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.717162132263184 | KNN Loss: 3.6948304176330566 | BCE Loss: 1.0223314762115479\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.71414041519165 | KNN Loss: 3.682433843612671 | BCE Loss: 1.03170645236969\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.769567012786865 | KNN Loss: 3.7481536865234375 | BCE Loss: 1.0214133262634277\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.6897807121276855 | KNN Loss: 3.673081874847412 | BCE Loss: 1.0166987180709839\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.739435195922852 | KNN Loss: 3.694640636444092 | BCE Loss: 1.0447946786880493\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.729646682739258 | KNN Loss: 3.711106061935425 | BCE Loss: 1.018540620803833\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.800185680389404 | KNN Loss: 3.743016481399536 | BCE Loss: 1.0571691989898682\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.677559852600098 | KNN Loss: 3.6621620655059814 | BCE Loss: 1.015397548675537\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.732706069946289 | KNN Loss: 3.6884942054748535 | BCE Loss: 1.0442116260528564\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.731301307678223 | KNN Loss: 3.6941423416137695 | BCE Loss: 1.0371592044830322\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.7509613037109375 | KNN Loss: 3.7199816703796387 | BCE Loss: 1.030979871749878\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.758157730102539 | KNN Loss: 3.7221670150756836 | BCE Loss: 1.0359907150268555\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.710819244384766 | KNN Loss: 3.6769280433654785 | BCE Loss: 1.033891201019287\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.680109977722168 | KNN Loss: 3.6904852390289307 | BCE Loss: 0.9896247386932373\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.6920294761657715 | KNN Loss: 3.6695070266723633 | BCE Loss: 1.0225225687026978\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.69857120513916 | KNN Loss: 3.6882870197296143 | BCE Loss: 1.0102839469909668\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.691837787628174 | KNN Loss: 3.660317897796631 | BCE Loss: 1.031519889831543\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.688752174377441 | KNN Loss: 3.6598520278930664 | BCE Loss: 1.028900384902954\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.702905654907227 | KNN Loss: 3.666147470474243 | BCE Loss: 1.0367584228515625\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.780794143676758 | KNN Loss: 3.7363452911376953 | BCE Loss: 1.0444486141204834\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.7575788497924805 | KNN Loss: 3.7049472332000732 | BCE Loss: 1.0526313781738281\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.721583366394043 | KNN Loss: 3.690800428390503 | BCE Loss: 1.030782699584961\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.663705825805664 | KNN Loss: 3.657747983932495 | BCE Loss: 1.005957841873169\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.700639247894287 | KNN Loss: 3.6729815006256104 | BCE Loss: 1.0276577472686768\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.718518257141113 | KNN Loss: 3.717525005340576 | BCE Loss: 1.000993013381958\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.764008522033691 | KNN Loss: 3.73305344581604 | BCE Loss: 1.0309548377990723\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.697256565093994 | KNN Loss: 3.675311803817749 | BCE Loss: 1.0219447612762451\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.716055870056152 | KNN Loss: 3.697542667388916 | BCE Loss: 1.0185132026672363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.7431769371032715 | KNN Loss: 3.6902525424957275 | BCE Loss: 1.052924394607544\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.755003452301025 | KNN Loss: 3.7211110591888428 | BCE Loss: 1.0338923931121826\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.743887901306152 | KNN Loss: 3.7114546298980713 | BCE Loss: 1.032433032989502\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.713747024536133 | KNN Loss: 3.69150447845459 | BCE Loss: 1.0222423076629639\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.718604564666748 | KNN Loss: 3.687448501586914 | BCE Loss: 1.0311559438705444\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.720799446105957 | KNN Loss: 3.6797728538513184 | BCE Loss: 1.0410263538360596\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.698553085327148 | KNN Loss: 3.6851823329925537 | BCE Loss: 1.0133705139160156\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.697116374969482 | KNN Loss: 3.6794466972351074 | BCE Loss: 1.0176695585250854\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.7770490646362305 | KNN Loss: 3.7366273403167725 | BCE Loss: 1.040421962738037\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.726621627807617 | KNN Loss: 3.6827404499053955 | BCE Loss: 1.0438810586929321\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.713275909423828 | KNN Loss: 3.7184243202209473 | BCE Loss: 0.9948513507843018\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.659607410430908 | KNN Loss: 3.671856641769409 | BCE Loss: 0.9877508282661438\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.700437545776367 | KNN Loss: 3.672358989715576 | BCE Loss: 1.028078556060791\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.714406490325928 | KNN Loss: 3.700066089630127 | BCE Loss: 1.0143405199050903\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.672237396240234 | KNN Loss: 3.657028913497925 | BCE Loss: 1.0152084827423096\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.6807355880737305 | KNN Loss: 3.656677484512329 | BCE Loss: 1.0240578651428223\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.701646327972412 | KNN Loss: 3.6762473583221436 | BCE Loss: 1.0253989696502686\n",
      "Epoch   196: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.7350263595581055 | KNN Loss: 3.6872596740722656 | BCE Loss: 1.047766923904419\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.689828872680664 | KNN Loss: 3.67533540725708 | BCE Loss: 1.0144932270050049\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.740242958068848 | KNN Loss: 3.6957263946533203 | BCE Loss: 1.0445163249969482\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.7735795974731445 | KNN Loss: 3.727490186691284 | BCE Loss: 1.0460891723632812\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.773605823516846 | KNN Loss: 3.724825382232666 | BCE Loss: 1.0487803220748901\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.751836776733398 | KNN Loss: 3.7216503620147705 | BCE Loss: 1.0301861763000488\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.6571736335754395 | KNN Loss: 3.6603803634643555 | BCE Loss: 0.996793270111084\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.7241621017456055 | KNN Loss: 3.690105438232422 | BCE Loss: 1.0340564250946045\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.700080394744873 | KNN Loss: 3.675065755844116 | BCE Loss: 1.0250145196914673\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.707292556762695 | KNN Loss: 3.6678359508514404 | BCE Loss: 1.039456844329834\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.720742225646973 | KNN Loss: 3.69277286529541 | BCE Loss: 1.0279695987701416\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.699211597442627 | KNN Loss: 3.677663564682007 | BCE Loss: 1.0215480327606201\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.750759124755859 | KNN Loss: 3.702834367752075 | BCE Loss: 1.0479246377944946\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.750398635864258 | KNN Loss: 3.712440252304077 | BCE Loss: 1.0379586219787598\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.764761924743652 | KNN Loss: 3.7241690158843994 | BCE Loss: 1.040592908859253\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.718945026397705 | KNN Loss: 3.6743533611297607 | BCE Loss: 1.0445916652679443\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.738541126251221 | KNN Loss: 3.7256269454956055 | BCE Loss: 1.0129141807556152\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.692558765411377 | KNN Loss: 3.6619646549224854 | BCE Loss: 1.0305941104888916\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.694304466247559 | KNN Loss: 3.6668689250946045 | BCE Loss: 1.027435541152954\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.685295104980469 | KNN Loss: 3.6690762042999268 | BCE Loss: 1.016218900680542\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.714439868927002 | KNN Loss: 3.6994667053222656 | BCE Loss: 1.0149731636047363\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.737588405609131 | KNN Loss: 3.728092670440674 | BCE Loss: 1.0094956159591675\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.6892991065979 | KNN Loss: 3.664045810699463 | BCE Loss: 1.0252532958984375\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.763172626495361 | KNN Loss: 3.715162754058838 | BCE Loss: 1.0480098724365234\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.7079854011535645 | KNN Loss: 3.6968586444854736 | BCE Loss: 1.0111267566680908\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.7400312423706055 | KNN Loss: 3.7072298526763916 | BCE Loss: 1.032801628112793\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.704579830169678 | KNN Loss: 3.6827213764190674 | BCE Loss: 1.0218584537506104\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.68305778503418 | KNN Loss: 3.6785101890563965 | BCE Loss: 1.0045478343963623\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.694608688354492 | KNN Loss: 3.638082265853882 | BCE Loss: 1.0565266609191895\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.733785629272461 | KNN Loss: 3.71370530128479 | BCE Loss: 1.02008056640625\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.727356433868408 | KNN Loss: 3.7218337059020996 | BCE Loss: 1.0055227279663086\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.762096405029297 | KNN Loss: 3.718097448348999 | BCE Loss: 1.043999195098877\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.698535919189453 | KNN Loss: 3.700462818145752 | BCE Loss: 0.998073160648346\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.802628517150879 | KNN Loss: 3.758495569229126 | BCE Loss: 1.0441328287124634\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.67432975769043 | KNN Loss: 3.6523780822753906 | BCE Loss: 1.0219519138336182\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.783246040344238 | KNN Loss: 3.7557499408721924 | BCE Loss: 1.027496337890625\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.730158805847168 | KNN Loss: 3.698951482772827 | BCE Loss: 1.0312072038650513\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.709812641143799 | KNN Loss: 3.6896581649780273 | BCE Loss: 1.020154595375061\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.687094211578369 | KNN Loss: 3.657015323638916 | BCE Loss: 1.0300787687301636\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.766268253326416 | KNN Loss: 3.7271006107330322 | BCE Loss: 1.0391676425933838\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.792723655700684 | KNN Loss: 3.7406280040740967 | BCE Loss: 1.052095651626587\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.717564582824707 | KNN Loss: 3.6954047679901123 | BCE Loss: 1.0221595764160156\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.731183052062988 | KNN Loss: 3.691366195678711 | BCE Loss: 1.039816975593567\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.793118953704834 | KNN Loss: 3.7606582641601562 | BCE Loss: 1.0324606895446777\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.704817771911621 | KNN Loss: 3.6743764877319336 | BCE Loss: 1.0304412841796875\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.733747482299805 | KNN Loss: 3.6902778148651123 | BCE Loss: 1.0434694290161133\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.729358673095703 | KNN Loss: 3.6791117191314697 | BCE Loss: 1.0502468347549438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.670753002166748 | KNN Loss: 3.656561851501465 | BCE Loss: 1.0141910314559937\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.73293399810791 | KNN Loss: 3.679028272628784 | BCE Loss: 1.0539054870605469\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.773617744445801 | KNN Loss: 3.706202745437622 | BCE Loss: 1.0674148797988892\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.658775329589844 | KNN Loss: 3.649751901626587 | BCE Loss: 1.009023666381836\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.724366188049316 | KNN Loss: 3.6884162425994873 | BCE Loss: 1.035949945449829\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.76778507232666 | KNN Loss: 3.7399439811706543 | BCE Loss: 1.027841329574585\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.754150390625 | KNN Loss: 3.6985888481140137 | BCE Loss: 1.0555617809295654\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.8139262199401855 | KNN Loss: 3.739851951599121 | BCE Loss: 1.0740742683410645\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.714527606964111 | KNN Loss: 3.7223904132843018 | BCE Loss: 0.9921373724937439\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.761347770690918 | KNN Loss: 3.7177791595458984 | BCE Loss: 1.0435688495635986\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.757227897644043 | KNN Loss: 3.692568302154541 | BCE Loss: 1.0646593570709229\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.730414390563965 | KNN Loss: 3.676076889038086 | BCE Loss: 1.0543372631072998\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.67023229598999 | KNN Loss: 3.677246570587158 | BCE Loss: 0.9929858446121216\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.68357515335083 | KNN Loss: 3.661238193511963 | BCE Loss: 1.0223369598388672\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.628026485443115 | KNN Loss: 3.6514270305633545 | BCE Loss: 0.9765993356704712\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.72340202331543 | KNN Loss: 3.6926677227020264 | BCE Loss: 1.0307340621948242\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.708316802978516 | KNN Loss: 3.6680009365081787 | BCE Loss: 1.0403156280517578\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.720609664916992 | KNN Loss: 3.7114429473876953 | BCE Loss: 1.0091664791107178\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.68839168548584 | KNN Loss: 3.6904211044311523 | BCE Loss: 0.997970700263977\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.700841903686523 | KNN Loss: 3.6864306926727295 | BCE Loss: 1.0144109725952148\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.704767227172852 | KNN Loss: 3.6953132152557373 | BCE Loss: 1.0094538927078247\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.72037935256958 | KNN Loss: 3.6895484924316406 | BCE Loss: 1.0308308601379395\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.746370315551758 | KNN Loss: 3.6941726207733154 | BCE Loss: 1.0521974563598633\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.697078704833984 | KNN Loss: 3.6732449531555176 | BCE Loss: 1.0238336324691772\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.6811747550964355 | KNN Loss: 3.6588263511657715 | BCE Loss: 1.022348403930664\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.721668720245361 | KNN Loss: 3.691387176513672 | BCE Loss: 1.030281662940979\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.732243537902832 | KNN Loss: 3.7009024620056152 | BCE Loss: 1.031341314315796\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.728067874908447 | KNN Loss: 3.668025016784668 | BCE Loss: 1.0600428581237793\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.710071086883545 | KNN Loss: 3.713606119155884 | BCE Loss: 0.9964651465415955\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.7054853439331055 | KNN Loss: 3.6885945796966553 | BCE Loss: 1.0168910026550293\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.715069770812988 | KNN Loss: 3.6919429302215576 | BCE Loss: 1.0231266021728516\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.712481498718262 | KNN Loss: 3.6870594024658203 | BCE Loss: 1.0254218578338623\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.756687164306641 | KNN Loss: 3.7556941509246826 | BCE Loss: 1.000993013381958\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.751890182495117 | KNN Loss: 3.7087392807006836 | BCE Loss: 1.0431509017944336\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.696911334991455 | KNN Loss: 3.6964404582977295 | BCE Loss: 1.0004709959030151\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.700249671936035 | KNN Loss: 3.6734495162963867 | BCE Loss: 1.0268000364303589\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.7842512130737305 | KNN Loss: 3.754592180252075 | BCE Loss: 1.0296592712402344\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.7143707275390625 | KNN Loss: 3.6917412281036377 | BCE Loss: 1.0226292610168457\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.681562423706055 | KNN Loss: 3.6651499271392822 | BCE Loss: 1.0164122581481934\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.777011394500732 | KNN Loss: 3.7529232501983643 | BCE Loss: 1.0240880250930786\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.675152778625488 | KNN Loss: 3.651603937149048 | BCE Loss: 1.0235487222671509\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.715910911560059 | KNN Loss: 3.6882946491241455 | BCE Loss: 1.0276163816452026\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.756880760192871 | KNN Loss: 3.6889944076538086 | BCE Loss: 1.0678863525390625\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.681959629058838 | KNN Loss: 3.6625192165374756 | BCE Loss: 1.0194405317306519\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.697409152984619 | KNN Loss: 3.6712639331817627 | BCE Loss: 1.026145100593567\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.74564790725708 | KNN Loss: 3.693349838256836 | BCE Loss: 1.0522981882095337\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.743854522705078 | KNN Loss: 3.716356039047241 | BCE Loss: 1.027498722076416\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.692989826202393 | KNN Loss: 3.6643784046173096 | BCE Loss: 1.0286115407943726\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.747042179107666 | KNN Loss: 3.7231762409210205 | BCE Loss: 1.0238659381866455\n",
      "Epoch   212: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.743656158447266 | KNN Loss: 3.7297377586364746 | BCE Loss: 1.013918399810791\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.707322597503662 | KNN Loss: 3.703852653503418 | BCE Loss: 1.0034700632095337\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.727134704589844 | KNN Loss: 3.662637710571289 | BCE Loss: 1.0644968748092651\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.686634063720703 | KNN Loss: 3.6732215881347656 | BCE Loss: 1.013412594795227\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.705198287963867 | KNN Loss: 3.6966209411621094 | BCE Loss: 1.0085771083831787\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.679024696350098 | KNN Loss: 3.6467831134796143 | BCE Loss: 1.0322413444519043\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.725329399108887 | KNN Loss: 3.7196011543273926 | BCE Loss: 1.0057283639907837\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.772213935852051 | KNN Loss: 3.7321200370788574 | BCE Loss: 1.0400936603546143\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.759649753570557 | KNN Loss: 3.693932056427002 | BCE Loss: 1.0657175779342651\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.737588405609131 | KNN Loss: 3.719707489013672 | BCE Loss: 1.0178810358047485\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.7039594650268555 | KNN Loss: 3.673783779144287 | BCE Loss: 1.0301759243011475\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.690310001373291 | KNN Loss: 3.6649887561798096 | BCE Loss: 1.025321364402771\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.727567195892334 | KNN Loss: 3.6758100986480713 | BCE Loss: 1.0517570972442627\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.735476493835449 | KNN Loss: 3.721068859100342 | BCE Loss: 1.0144073963165283\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.744344711303711 | KNN Loss: 3.727823495864868 | BCE Loss: 1.0165212154388428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.733274936676025 | KNN Loss: 3.7063345909118652 | BCE Loss: 1.0269403457641602\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.655884265899658 | KNN Loss: 3.644085645675659 | BCE Loss: 1.011798620223999\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.745719909667969 | KNN Loss: 3.721095323562622 | BCE Loss: 1.0246245861053467\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.760196208953857 | KNN Loss: 3.7233102321624756 | BCE Loss: 1.0368858575820923\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.697086334228516 | KNN Loss: 3.690844774246216 | BCE Loss: 1.006241798400879\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.7418599128723145 | KNN Loss: 3.716752767562866 | BCE Loss: 1.0251071453094482\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.703171730041504 | KNN Loss: 3.688396692276001 | BCE Loss: 1.0147749185562134\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.667035102844238 | KNN Loss: 3.6743946075439453 | BCE Loss: 0.9926406145095825\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.753168106079102 | KNN Loss: 3.7165398597717285 | BCE Loss: 1.036628246307373\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.756199836730957 | KNN Loss: 3.7182278633117676 | BCE Loss: 1.037972092628479\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.701748847961426 | KNN Loss: 3.6898326873779297 | BCE Loss: 1.0119163990020752\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.757732391357422 | KNN Loss: 3.71362566947937 | BCE Loss: 1.0441066026687622\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.747923851013184 | KNN Loss: 3.7078754901885986 | BCE Loss: 1.040048599243164\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.704795837402344 | KNN Loss: 3.7134618759155273 | BCE Loss: 0.9913339614868164\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.740612506866455 | KNN Loss: 3.6974287033081055 | BCE Loss: 1.0431838035583496\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.722346782684326 | KNN Loss: 3.671149492263794 | BCE Loss: 1.0511971712112427\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.719271659851074 | KNN Loss: 3.680091381072998 | BCE Loss: 1.039180040359497\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.693892478942871 | KNN Loss: 3.666783571243286 | BCE Loss: 1.027109146118164\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.650744438171387 | KNN Loss: 3.6481873989105225 | BCE Loss: 1.0025571584701538\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.7086100578308105 | KNN Loss: 3.676224946975708 | BCE Loss: 1.0323851108551025\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.704545021057129 | KNN Loss: 3.675597667694092 | BCE Loss: 1.028947114944458\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.693741321563721 | KNN Loss: 3.6580209732055664 | BCE Loss: 1.0357203483581543\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.689781188964844 | KNN Loss: 3.6984317302703857 | BCE Loss: 0.9913492202758789\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.753336429595947 | KNN Loss: 3.745004415512085 | BCE Loss: 1.0083321332931519\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.727315902709961 | KNN Loss: 3.7050118446350098 | BCE Loss: 1.0223042964935303\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.731077194213867 | KNN Loss: 3.6866378784179688 | BCE Loss: 1.0444390773773193\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.743422508239746 | KNN Loss: 3.7111313343048096 | BCE Loss: 1.0322914123535156\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.693925857543945 | KNN Loss: 3.681873083114624 | BCE Loss: 1.0120525360107422\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.719731330871582 | KNN Loss: 3.690298080444336 | BCE Loss: 1.029433012008667\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.778308868408203 | KNN Loss: 3.7409160137176514 | BCE Loss: 1.0373926162719727\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.723379135131836 | KNN Loss: 3.7238168716430664 | BCE Loss: 0.99956214427948\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.729598045349121 | KNN Loss: 3.679819107055664 | BCE Loss: 1.0497791767120361\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.720246315002441 | KNN Loss: 3.6962051391601562 | BCE Loss: 1.0240411758422852\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.703947067260742 | KNN Loss: 3.702676296234131 | BCE Loss: 1.0012707710266113\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.702233791351318 | KNN Loss: 3.6832690238952637 | BCE Loss: 1.0189647674560547\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.75324821472168 | KNN Loss: 3.7193727493286133 | BCE Loss: 1.0338752269744873\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.727795600891113 | KNN Loss: 3.7082765102386475 | BCE Loss: 1.0195190906524658\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.728589057922363 | KNN Loss: 3.688258171081543 | BCE Loss: 1.0403306484222412\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.716028690338135 | KNN Loss: 3.6763250827789307 | BCE Loss: 1.039703607559204\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.745673656463623 | KNN Loss: 3.7137715816497803 | BCE Loss: 1.0319020748138428\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.693145751953125 | KNN Loss: 3.6646103858947754 | BCE Loss: 1.0285353660583496\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.722005844116211 | KNN Loss: 3.695539712905884 | BCE Loss: 1.0264661312103271\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.706785202026367 | KNN Loss: 3.6854758262634277 | BCE Loss: 1.0213096141815186\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.740988731384277 | KNN Loss: 3.735783576965332 | BCE Loss: 1.0052049160003662\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.709117889404297 | KNN Loss: 3.6984291076660156 | BCE Loss: 1.0106890201568604\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.745073318481445 | KNN Loss: 3.6979310512542725 | BCE Loss: 1.0471422672271729\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.697725772857666 | KNN Loss: 3.6850709915161133 | BCE Loss: 1.0126547813415527\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.754734992980957 | KNN Loss: 3.7004411220550537 | BCE Loss: 1.0542936325073242\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.73567008972168 | KNN Loss: 3.688314199447632 | BCE Loss: 1.0473558902740479\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.702561855316162 | KNN Loss: 3.707977294921875 | BCE Loss: 0.9945845603942871\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.702509880065918 | KNN Loss: 3.688823699951172 | BCE Loss: 1.0136860609054565\n",
      "Epoch   223: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.685016632080078 | KNN Loss: 3.6899826526641846 | BCE Loss: 0.9950339198112488\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.692592620849609 | KNN Loss: 3.6836438179016113 | BCE Loss: 1.008948564529419\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.696958065032959 | KNN Loss: 3.6770639419555664 | BCE Loss: 1.0198941230773926\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.700491905212402 | KNN Loss: 3.6842312812805176 | BCE Loss: 1.0162608623504639\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.797019958496094 | KNN Loss: 3.715949535369873 | BCE Loss: 1.0810704231262207\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.732527732849121 | KNN Loss: 3.6983792781829834 | BCE Loss: 1.0341483354568481\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.738360404968262 | KNN Loss: 3.7135486602783203 | BCE Loss: 1.0248115062713623\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.705672264099121 | KNN Loss: 3.6835269927978516 | BCE Loss: 1.02214515209198\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.739376068115234 | KNN Loss: 3.6788196563720703 | BCE Loss: 1.060556173324585\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.71580696105957 | KNN Loss: 3.6957595348358154 | BCE Loss: 1.0200475454330444\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.760709762573242 | KNN Loss: 3.709242820739746 | BCE Loss: 1.051466703414917\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.708823204040527 | KNN Loss: 3.6949515342712402 | BCE Loss: 1.013871669769287\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.68765926361084 | KNN Loss: 3.673811674118042 | BCE Loss: 1.013847827911377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.690882682800293 | KNN Loss: 3.663172960281372 | BCE Loss: 1.0277094841003418\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.711453914642334 | KNN Loss: 3.674525737762451 | BCE Loss: 1.0369281768798828\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.704532623291016 | KNN Loss: 3.6768031120300293 | BCE Loss: 1.0277293920516968\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.74006986618042 | KNN Loss: 3.720184564590454 | BCE Loss: 1.0198853015899658\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.698287010192871 | KNN Loss: 3.671630859375 | BCE Loss: 1.0266563892364502\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.737943649291992 | KNN Loss: 3.7200491428375244 | BCE Loss: 1.0178947448730469\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.749603748321533 | KNN Loss: 3.713608980178833 | BCE Loss: 1.0359947681427002\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.713703155517578 | KNN Loss: 3.6990466117858887 | BCE Loss: 1.0146563053131104\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.701892852783203 | KNN Loss: 3.6979165077209473 | BCE Loss: 1.003976583480835\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.688283920288086 | KNN Loss: 3.6822681427001953 | BCE Loss: 1.006015658378601\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.7176513671875 | KNN Loss: 3.685919761657715 | BCE Loss: 1.031731367111206\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.718559265136719 | KNN Loss: 3.689162254333496 | BCE Loss: 1.0293967723846436\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.747618198394775 | KNN Loss: 3.69986629486084 | BCE Loss: 1.047751784324646\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.733617782592773 | KNN Loss: 3.6873831748962402 | BCE Loss: 1.0462348461151123\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.681971549987793 | KNN Loss: 3.678022861480713 | BCE Loss: 1.0039489269256592\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.719852447509766 | KNN Loss: 3.68851375579834 | BCE Loss: 1.0313389301300049\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.711493968963623 | KNN Loss: 3.684532880783081 | BCE Loss: 1.0269612073898315\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.72432804107666 | KNN Loss: 3.6812050342559814 | BCE Loss: 1.0431227684020996\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.716117858886719 | KNN Loss: 3.726527690887451 | BCE Loss: 0.9895899295806885\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.708563327789307 | KNN Loss: 3.697434425354004 | BCE Loss: 1.0111289024353027\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.683785438537598 | KNN Loss: 3.6645829677581787 | BCE Loss: 1.019202709197998\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.785923957824707 | KNN Loss: 3.7456557750701904 | BCE Loss: 1.040268063545227\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.738738059997559 | KNN Loss: 3.7098114490509033 | BCE Loss: 1.0289268493652344\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.692151069641113 | KNN Loss: 3.673506736755371 | BCE Loss: 1.018644094467163\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.724865436553955 | KNN Loss: 3.6898295879364014 | BCE Loss: 1.0350358486175537\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.757465362548828 | KNN Loss: 3.7086987495422363 | BCE Loss: 1.0487664937973022\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.726794719696045 | KNN Loss: 3.6849112510681152 | BCE Loss: 1.0418835878372192\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.730010986328125 | KNN Loss: 3.6779043674468994 | BCE Loss: 1.0521063804626465\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.685173988342285 | KNN Loss: 3.662623405456543 | BCE Loss: 1.0225508213043213\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.729887962341309 | KNN Loss: 3.700427532196045 | BCE Loss: 1.0294603109359741\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.692532062530518 | KNN Loss: 3.688845634460449 | BCE Loss: 1.0036864280700684\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.73341178894043 | KNN Loss: 3.700857639312744 | BCE Loss: 1.0325541496276855\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.754526615142822 | KNN Loss: 3.722039222717285 | BCE Loss: 1.0324875116348267\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.735818862915039 | KNN Loss: 3.7162318229675293 | BCE Loss: 1.0195870399475098\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.782741546630859 | KNN Loss: 3.7405953407287598 | BCE Loss: 1.0421463251113892\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.72763204574585 | KNN Loss: 3.693477153778076 | BCE Loss: 1.0341548919677734\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.7262420654296875 | KNN Loss: 3.6938393115997314 | BCE Loss: 1.032402515411377\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.713150978088379 | KNN Loss: 3.675827741622925 | BCE Loss: 1.0373234748840332\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.737785816192627 | KNN Loss: 3.6989808082580566 | BCE Loss: 1.0388050079345703\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.757801055908203 | KNN Loss: 3.7125039100646973 | BCE Loss: 1.0452971458435059\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.742578983306885 | KNN Loss: 3.7102930545806885 | BCE Loss: 1.0322860479354858\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.724058151245117 | KNN Loss: 3.697571039199829 | BCE Loss: 1.026486873626709\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.729092121124268 | KNN Loss: 3.6986308097839355 | BCE Loss: 1.030461311340332\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.713529109954834 | KNN Loss: 3.6832926273345947 | BCE Loss: 1.0302364826202393\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.742351055145264 | KNN Loss: 3.699380874633789 | BCE Loss: 1.0429701805114746\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.718385696411133 | KNN Loss: 3.706603527069092 | BCE Loss: 1.011782169342041\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.715086936950684 | KNN Loss: 3.685175895690918 | BCE Loss: 1.0299108028411865\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.717331886291504 | KNN Loss: 3.7090651988983154 | BCE Loss: 1.0082666873931885\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.759106636047363 | KNN Loss: 3.714160203933716 | BCE Loss: 1.0449466705322266\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.716154098510742 | KNN Loss: 3.698338031768799 | BCE Loss: 1.0178159475326538\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.7415876388549805 | KNN Loss: 3.725123882293701 | BCE Loss: 1.0164635181427002\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.747363090515137 | KNN Loss: 3.724010705947876 | BCE Loss: 1.0233526229858398\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.7183027267456055 | KNN Loss: 3.7123918533325195 | BCE Loss: 1.0059107542037964\n",
      "Epoch   234: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.691076278686523 | KNN Loss: 3.685051441192627 | BCE Loss: 1.0060245990753174\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.702908515930176 | KNN Loss: 3.672783613204956 | BCE Loss: 1.0301251411437988\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.744263648986816 | KNN Loss: 3.7377047538757324 | BCE Loss: 1.0065586566925049\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.724125385284424 | KNN Loss: 3.6824066638946533 | BCE Loss: 1.04171884059906\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.663794040679932 | KNN Loss: 3.661072015762329 | BCE Loss: 1.002722144126892\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.7063775062561035 | KNN Loss: 3.682734966278076 | BCE Loss: 1.023642659187317\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.698531627655029 | KNN Loss: 3.6885130405426025 | BCE Loss: 1.0100185871124268\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.778947830200195 | KNN Loss: 3.7621049880981445 | BCE Loss: 1.0168430805206299\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.694045066833496 | KNN Loss: 3.6868083477020264 | BCE Loss: 1.0072364807128906\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.714990139007568 | KNN Loss: 3.6788833141326904 | BCE Loss: 1.036106824874878\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.720304489135742 | KNN Loss: 3.6966142654418945 | BCE Loss: 1.0236903429031372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.726428985595703 | KNN Loss: 3.70947527885437 | BCE Loss: 1.0169538259506226\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.697216987609863 | KNN Loss: 3.6754963397979736 | BCE Loss: 1.0217208862304688\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.7593536376953125 | KNN Loss: 3.7357773780822754 | BCE Loss: 1.023576021194458\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.7160773277282715 | KNN Loss: 3.661588191986084 | BCE Loss: 1.0544891357421875\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.757218360900879 | KNN Loss: 3.727879762649536 | BCE Loss: 1.0293388366699219\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.703752517700195 | KNN Loss: 3.6653473377227783 | BCE Loss: 1.0384050607681274\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.717349052429199 | KNN Loss: 3.6914241313934326 | BCE Loss: 1.0259249210357666\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.707173824310303 | KNN Loss: 3.6859989166259766 | BCE Loss: 1.0211747884750366\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.759544849395752 | KNN Loss: 3.7437620162963867 | BCE Loss: 1.0157828330993652\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.714629173278809 | KNN Loss: 3.683656930923462 | BCE Loss: 1.0309724807739258\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.767340183258057 | KNN Loss: 3.7055535316467285 | BCE Loss: 1.0617865324020386\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.718673229217529 | KNN Loss: 3.689117670059204 | BCE Loss: 1.0295555591583252\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.716212272644043 | KNN Loss: 3.6967763900756836 | BCE Loss: 1.0194358825683594\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.753353118896484 | KNN Loss: 3.68719220161438 | BCE Loss: 1.066161036491394\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.700554847717285 | KNN Loss: 3.6804051399230957 | BCE Loss: 1.0201499462127686\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.750931262969971 | KNN Loss: 3.7147576808929443 | BCE Loss: 1.0361735820770264\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.705095291137695 | KNN Loss: 3.700845718383789 | BCE Loss: 1.0042494535446167\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.73610782623291 | KNN Loss: 3.7076406478881836 | BCE Loss: 1.0284674167633057\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.704090118408203 | KNN Loss: 3.6919848918914795 | BCE Loss: 1.0121052265167236\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.714825630187988 | KNN Loss: 3.6851017475128174 | BCE Loss: 1.0297236442565918\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.728719711303711 | KNN Loss: 3.7048113346099854 | BCE Loss: 1.0239081382751465\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.703402519226074 | KNN Loss: 3.6725075244903564 | BCE Loss: 1.0308952331542969\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.701202392578125 | KNN Loss: 3.693218231201172 | BCE Loss: 1.0079842805862427\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.736681938171387 | KNN Loss: 3.684666872024536 | BCE Loss: 1.0520150661468506\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.713836669921875 | KNN Loss: 3.7188148498535156 | BCE Loss: 0.995021641254425\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.70842981338501 | KNN Loss: 3.695120096206665 | BCE Loss: 1.0133095979690552\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.6870808601379395 | KNN Loss: 3.680670976638794 | BCE Loss: 1.0064098834991455\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.714890003204346 | KNN Loss: 3.665287733078003 | BCE Loss: 1.0496021509170532\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.722787857055664 | KNN Loss: 3.6842474937438965 | BCE Loss: 1.0385406017303467\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.661325454711914 | KNN Loss: 3.6496589183807373 | BCE Loss: 1.0116665363311768\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.650744438171387 | KNN Loss: 3.6492111682891846 | BCE Loss: 1.001533031463623\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.682699203491211 | KNN Loss: 3.6687991619110107 | BCE Loss: 1.0139000415802002\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.7245941162109375 | KNN Loss: 3.6779098510742188 | BCE Loss: 1.0466840267181396\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.718531608581543 | KNN Loss: 3.696857452392578 | BCE Loss: 1.0216741561889648\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.711318016052246 | KNN Loss: 3.67634654045105 | BCE Loss: 1.0349717140197754\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.717178821563721 | KNN Loss: 3.699981927871704 | BCE Loss: 1.0171968936920166\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.732234001159668 | KNN Loss: 3.6898229122161865 | BCE Loss: 1.0424110889434814\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.712406158447266 | KNN Loss: 3.690119743347168 | BCE Loss: 1.0222864151000977\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.716705799102783 | KNN Loss: 3.6953704357147217 | BCE Loss: 1.0213353633880615\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.70871639251709 | KNN Loss: 3.690326690673828 | BCE Loss: 1.0183899402618408\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.705881118774414 | KNN Loss: 3.682767152786255 | BCE Loss: 1.02311372756958\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.720259666442871 | KNN Loss: 3.6850993633270264 | BCE Loss: 1.0351603031158447\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.70015287399292 | KNN Loss: 3.665039539337158 | BCE Loss: 1.0351133346557617\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.744968414306641 | KNN Loss: 3.7221152782440186 | BCE Loss: 1.022853136062622\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.787284851074219 | KNN Loss: 3.7420153617858887 | BCE Loss: 1.04526948928833\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.699018478393555 | KNN Loss: 3.6737060546875 | BCE Loss: 1.0253124237060547\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.716776371002197 | KNN Loss: 3.7017059326171875 | BCE Loss: 1.0150704383850098\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.7621355056762695 | KNN Loss: 3.72701096534729 | BCE Loss: 1.0351245403289795\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.815053939819336 | KNN Loss: 3.761385917663574 | BCE Loss: 1.0536680221557617\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.769567966461182 | KNN Loss: 3.73146653175354 | BCE Loss: 1.0381014347076416\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.723397254943848 | KNN Loss: 3.6795706748962402 | BCE Loss: 1.0438264608383179\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.708369255065918 | KNN Loss: 3.7049832344055176 | BCE Loss: 1.00338613986969\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.723177909851074 | KNN Loss: 3.6973235607147217 | BCE Loss: 1.0258543491363525\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.668214797973633 | KNN Loss: 3.6605446338653564 | BCE Loss: 1.0076699256896973\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.773609161376953 | KNN Loss: 3.7280654907226562 | BCE Loss: 1.045543909072876\n",
      "Epoch   245: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.739079475402832 | KNN Loss: 3.697800636291504 | BCE Loss: 1.0412790775299072\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.722746849060059 | KNN Loss: 3.720052719116211 | BCE Loss: 1.0026938915252686\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.702648162841797 | KNN Loss: 3.670562744140625 | BCE Loss: 1.0320852994918823\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.709028720855713 | KNN Loss: 3.678671360015869 | BCE Loss: 1.0303573608398438\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.730902671813965 | KNN Loss: 3.6874914169311523 | BCE Loss: 1.0434110164642334\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.709969520568848 | KNN Loss: 3.679328441619873 | BCE Loss: 1.0306410789489746\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.712213516235352 | KNN Loss: 3.7195968627929688 | BCE Loss: 0.9926168322563171\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.677834987640381 | KNN Loss: 3.646061420440674 | BCE Loss: 1.0317736864089966\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.756336212158203 | KNN Loss: 3.7124698162078857 | BCE Loss: 1.0438666343688965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.663125514984131 | KNN Loss: 3.6654231548309326 | BCE Loss: 0.9977021813392639\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.677103042602539 | KNN Loss: 3.6687116622924805 | BCE Loss: 1.0083911418914795\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.751274585723877 | KNN Loss: 3.687795400619507 | BCE Loss: 1.0634791851043701\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.728036403656006 | KNN Loss: 3.6967201232910156 | BCE Loss: 1.0313162803649902\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.705807209014893 | KNN Loss: 3.6932244300842285 | BCE Loss: 1.0125826597213745\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.699333190917969 | KNN Loss: 3.6660399436950684 | BCE Loss: 1.0332930088043213\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.682737827301025 | KNN Loss: 3.663724184036255 | BCE Loss: 1.0190136432647705\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.673327445983887 | KNN Loss: 3.672988176345825 | BCE Loss: 1.0003390312194824\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.682952880859375 | KNN Loss: 3.675184965133667 | BCE Loss: 1.007767677307129\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.704983711242676 | KNN Loss: 3.69222354888916 | BCE Loss: 1.0127604007720947\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.70529842376709 | KNN Loss: 3.6773264408111572 | BCE Loss: 1.027971863746643\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.671018600463867 | KNN Loss: 3.6618645191192627 | BCE Loss: 1.009153962135315\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.735379219055176 | KNN Loss: 3.7178821563720703 | BCE Loss: 1.0174968242645264\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.704809665679932 | KNN Loss: 3.6734533309936523 | BCE Loss: 1.0313562154769897\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.73336124420166 | KNN Loss: 3.7108523845672607 | BCE Loss: 1.0225088596343994\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.730243682861328 | KNN Loss: 3.7020628452301025 | BCE Loss: 1.0281805992126465\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.789278507232666 | KNN Loss: 3.7428231239318848 | BCE Loss: 1.0464552640914917\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.751286029815674 | KNN Loss: 3.6951143741607666 | BCE Loss: 1.0561716556549072\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.72576904296875 | KNN Loss: 3.7123517990112305 | BCE Loss: 1.0134170055389404\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.743669509887695 | KNN Loss: 3.705641508102417 | BCE Loss: 1.0380280017852783\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.711434364318848 | KNN Loss: 3.6824774742126465 | BCE Loss: 1.0289568901062012\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.741942405700684 | KNN Loss: 3.720062732696533 | BCE Loss: 1.0218794345855713\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.730560302734375 | KNN Loss: 3.682600975036621 | BCE Loss: 1.0479594469070435\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.720165729522705 | KNN Loss: 3.705049991607666 | BCE Loss: 1.0151156187057495\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.723333835601807 | KNN Loss: 3.693899631500244 | BCE Loss: 1.0294342041015625\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.7017669677734375 | KNN Loss: 3.700852870941162 | BCE Loss: 1.000914216041565\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.739727020263672 | KNN Loss: 3.6984691619873047 | BCE Loss: 1.0412580966949463\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.72660493850708 | KNN Loss: 3.7061121463775635 | BCE Loss: 1.020492672920227\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.707400798797607 | KNN Loss: 3.6900887489318848 | BCE Loss: 1.0173120498657227\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.652139186859131 | KNN Loss: 3.6614508628845215 | BCE Loss: 0.9906883835792542\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.66153621673584 | KNN Loss: 3.6442768573760986 | BCE Loss: 1.017259120941162\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.7042083740234375 | KNN Loss: 3.698573112487793 | BCE Loss: 1.0056350231170654\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.74934196472168 | KNN Loss: 3.7417685985565186 | BCE Loss: 1.0075732469558716\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.730521202087402 | KNN Loss: 3.6945385932922363 | BCE Loss: 1.035982608795166\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.7499237060546875 | KNN Loss: 3.7162675857543945 | BCE Loss: 1.0336562395095825\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.707297325134277 | KNN Loss: 3.664273262023926 | BCE Loss: 1.0430240631103516\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.7857513427734375 | KNN Loss: 3.745065212249756 | BCE Loss: 1.0406862497329712\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.713489055633545 | KNN Loss: 3.6638362407684326 | BCE Loss: 1.0496526956558228\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.6772661209106445 | KNN Loss: 3.642873525619507 | BCE Loss: 1.0343925952911377\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.7639617919921875 | KNN Loss: 3.745927572250366 | BCE Loss: 1.0180339813232422\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.713727951049805 | KNN Loss: 3.6752426624298096 | BCE Loss: 1.0384852886199951\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.646270275115967 | KNN Loss: 3.6573104858398438 | BCE Loss: 0.9889599084854126\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.739391326904297 | KNN Loss: 3.705259323120117 | BCE Loss: 1.0341320037841797\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.674386501312256 | KNN Loss: 3.66068696975708 | BCE Loss: 1.0136994123458862\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.7367448806762695 | KNN Loss: 3.7181074619293213 | BCE Loss: 1.0186376571655273\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.750115871429443 | KNN Loss: 3.687734603881836 | BCE Loss: 1.0623812675476074\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.680732727050781 | KNN Loss: 3.671315908432007 | BCE Loss: 1.0094165802001953\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.743432998657227 | KNN Loss: 3.7219271659851074 | BCE Loss: 1.0215058326721191\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.678955554962158 | KNN Loss: 3.649719715118408 | BCE Loss: 1.02923583984375\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.685545921325684 | KNN Loss: 3.6703860759735107 | BCE Loss: 1.0151597261428833\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.764513969421387 | KNN Loss: 3.710273504257202 | BCE Loss: 1.0542404651641846\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.742910861968994 | KNN Loss: 3.736077070236206 | BCE Loss: 1.006833791732788\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.665397644042969 | KNN Loss: 3.6607916355133057 | BCE Loss: 1.004606008529663\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.7622151374816895 | KNN Loss: 3.738345146179199 | BCE Loss: 1.0238699913024902\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.707561492919922 | KNN Loss: 3.696885824203491 | BCE Loss: 1.0106756687164307\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.699831008911133 | KNN Loss: 3.686150312423706 | BCE Loss: 1.0136804580688477\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.722085475921631 | KNN Loss: 3.6780714988708496 | BCE Loss: 1.0440139770507812\n",
      "Epoch   256: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.710807800292969 | KNN Loss: 3.674516201019287 | BCE Loss: 1.0362918376922607\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.714507579803467 | KNN Loss: 3.6707253456115723 | BCE Loss: 1.0437822341918945\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.725948333740234 | KNN Loss: 3.6993792057037354 | BCE Loss: 1.026569128036499\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.703185081481934 | KNN Loss: 3.6790695190429688 | BCE Loss: 1.0241155624389648\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.723663330078125 | KNN Loss: 3.703443765640259 | BCE Loss: 1.0202198028564453\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.684787750244141 | KNN Loss: 3.6755855083465576 | BCE Loss: 1.0092021226882935\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.707733154296875 | KNN Loss: 3.6832778453826904 | BCE Loss: 1.0244553089141846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.686240196228027 | KNN Loss: 3.6769490242004395 | BCE Loss: 1.0092909336090088\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.713218688964844 | KNN Loss: 3.6742780208587646 | BCE Loss: 1.0389409065246582\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.69713830947876 | KNN Loss: 3.6589245796203613 | BCE Loss: 1.0382137298583984\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.675715446472168 | KNN Loss: 3.6596076488494873 | BCE Loss: 1.0161077976226807\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.70760440826416 | KNN Loss: 3.6653239727020264 | BCE Loss: 1.042280673980713\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.670154571533203 | KNN Loss: 3.6453938484191895 | BCE Loss: 1.0247609615325928\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.751132965087891 | KNN Loss: 3.7314257621765137 | BCE Loss: 1.019707441329956\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.714669227600098 | KNN Loss: 3.6844139099121094 | BCE Loss: 1.0302553176879883\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.743739128112793 | KNN Loss: 3.684534788131714 | BCE Loss: 1.0592041015625\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.735874176025391 | KNN Loss: 3.703096389770508 | BCE Loss: 1.0327777862548828\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.7390217781066895 | KNN Loss: 3.705599784851074 | BCE Loss: 1.0334221124649048\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.745039463043213 | KNN Loss: 3.7086403369903564 | BCE Loss: 1.0363991260528564\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.7004899978637695 | KNN Loss: 3.6801416873931885 | BCE Loss: 1.0203485488891602\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.716847896575928 | KNN Loss: 3.67071270942688 | BCE Loss: 1.0461351871490479\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.683633327484131 | KNN Loss: 3.672752857208252 | BCE Loss: 1.0108805894851685\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.719758033752441 | KNN Loss: 3.6625092029571533 | BCE Loss: 1.0572489500045776\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.687797546386719 | KNN Loss: 3.659867525100708 | BCE Loss: 1.0279302597045898\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.765385627746582 | KNN Loss: 3.693122148513794 | BCE Loss: 1.0722637176513672\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.681495189666748 | KNN Loss: 3.6713194847106934 | BCE Loss: 1.0101757049560547\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.724170684814453 | KNN Loss: 3.713745355606079 | BCE Loss: 1.010425090789795\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.774931907653809 | KNN Loss: 3.718400239944458 | BCE Loss: 1.0565316677093506\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.728262901306152 | KNN Loss: 3.685037612915039 | BCE Loss: 1.0432252883911133\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.744833946228027 | KNN Loss: 3.7191994190216064 | BCE Loss: 1.0256342887878418\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.670809745788574 | KNN Loss: 3.6574645042419434 | BCE Loss: 1.0133452415466309\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.7142181396484375 | KNN Loss: 3.6920058727264404 | BCE Loss: 1.022212028503418\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.707530975341797 | KNN Loss: 3.6914303302764893 | BCE Loss: 1.016100525856018\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.699942588806152 | KNN Loss: 3.665085554122925 | BCE Loss: 1.034856915473938\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.709735870361328 | KNN Loss: 3.6807239055633545 | BCE Loss: 1.0290117263793945\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.717783451080322 | KNN Loss: 3.676435947418213 | BCE Loss: 1.0413475036621094\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.733901500701904 | KNN Loss: 3.72554874420166 | BCE Loss: 1.0083527565002441\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.7329607009887695 | KNN Loss: 3.7055366039276123 | BCE Loss: 1.0274238586425781\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.751805305480957 | KNN Loss: 3.7200098037719727 | BCE Loss: 1.0317957401275635\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.759636878967285 | KNN Loss: 3.7368853092193604 | BCE Loss: 1.022751808166504\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.685360908508301 | KNN Loss: 3.6686346530914307 | BCE Loss: 1.0167264938354492\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.72137451171875 | KNN Loss: 3.684065580368042 | BCE Loss: 1.037309169769287\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.731828689575195 | KNN Loss: 3.695814847946167 | BCE Loss: 1.0360136032104492\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.748237609863281 | KNN Loss: 3.6909408569335938 | BCE Loss: 1.0572965145111084\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.795987606048584 | KNN Loss: 3.7577896118164062 | BCE Loss: 1.0381979942321777\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.773651123046875 | KNN Loss: 3.715007781982422 | BCE Loss: 1.0586434602737427\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.687007904052734 | KNN Loss: 3.691473960876465 | BCE Loss: 0.9955340623855591\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.71103048324585 | KNN Loss: 3.6812119483947754 | BCE Loss: 1.0298184156417847\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.735400199890137 | KNN Loss: 3.6801400184631348 | BCE Loss: 1.0552599430084229\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.694915294647217 | KNN Loss: 3.645068645477295 | BCE Loss: 1.0498465299606323\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.707962989807129 | KNN Loss: 3.671658992767334 | BCE Loss: 1.0363041162490845\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.7166595458984375 | KNN Loss: 3.7012672424316406 | BCE Loss: 1.0153923034667969\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.676370620727539 | KNN Loss: 3.6683425903320312 | BCE Loss: 1.008028268814087\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.7223358154296875 | KNN Loss: 3.716691017150879 | BCE Loss: 1.0056447982788086\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.713870525360107 | KNN Loss: 3.6927123069763184 | BCE Loss: 1.0211583375930786\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.700026035308838 | KNN Loss: 3.671654462814331 | BCE Loss: 1.0283714532852173\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.753673553466797 | KNN Loss: 3.6968183517456055 | BCE Loss: 1.0568552017211914\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.687211513519287 | KNN Loss: 3.6806373596191406 | BCE Loss: 1.006574273109436\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.700033664703369 | KNN Loss: 3.668444871902466 | BCE Loss: 1.0315886735916138\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.740800857543945 | KNN Loss: 3.7235054969787598 | BCE Loss: 1.017295241355896\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.7266716957092285 | KNN Loss: 3.697282314300537 | BCE Loss: 1.0293892621994019\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.68549919128418 | KNN Loss: 3.6540253162384033 | BCE Loss: 1.0314738750457764\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.722446918487549 | KNN Loss: 3.7005600929260254 | BCE Loss: 1.0218868255615234\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.738491535186768 | KNN Loss: 3.689065456390381 | BCE Loss: 1.0494260787963867\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.733527183532715 | KNN Loss: 3.709144353866577 | BCE Loss: 1.0243828296661377\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.688523769378662 | KNN Loss: 3.6750824451446533 | BCE Loss: 1.0134413242340088\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.686842918395996 | KNN Loss: 3.659358263015747 | BCE Loss: 1.027484655380249\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.774087429046631 | KNN Loss: 3.705625295639038 | BCE Loss: 1.0684621334075928\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.729445457458496 | KNN Loss: 3.67622447013855 | BCE Loss: 1.0532208681106567\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.729843616485596 | KNN Loss: 3.694368839263916 | BCE Loss: 1.0354747772216797\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.672761917114258 | KNN Loss: 3.666893482208252 | BCE Loss: 1.0058681964874268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.690254211425781 | KNN Loss: 3.6842732429504395 | BCE Loss: 1.0059807300567627\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.714166164398193 | KNN Loss: 3.673591375350952 | BCE Loss: 1.0405747890472412\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.695077419281006 | KNN Loss: 3.677628993988037 | BCE Loss: 1.0174484252929688\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.762144088745117 | KNN Loss: 3.7387232780456543 | BCE Loss: 1.0234205722808838\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.707087516784668 | KNN Loss: 3.6951851844787598 | BCE Loss: 1.0119025707244873\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.733908176422119 | KNN Loss: 3.703760862350464 | BCE Loss: 1.0301471948623657\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.743232727050781 | KNN Loss: 3.729060649871826 | BCE Loss: 1.014172077178955\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.718256950378418 | KNN Loss: 3.70161509513855 | BCE Loss: 1.0166418552398682\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.722613334655762 | KNN Loss: 3.7099826335906982 | BCE Loss: 1.012630820274353\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.750042915344238 | KNN Loss: 3.7185862064361572 | BCE Loss: 1.031456708908081\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.710659027099609 | KNN Loss: 3.69191312789917 | BCE Loss: 1.01874577999115\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.699968338012695 | KNN Loss: 3.663673162460327 | BCE Loss: 1.036294937133789\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.704248428344727 | KNN Loss: 3.654634952545166 | BCE Loss: 1.0496134757995605\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.723531246185303 | KNN Loss: 3.6844046115875244 | BCE Loss: 1.0391265153884888\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.704713821411133 | KNN Loss: 3.6849288940429688 | BCE Loss: 1.0197848081588745\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.700428485870361 | KNN Loss: 3.6769378185272217 | BCE Loss: 1.0234906673431396\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.670788288116455 | KNN Loss: 3.6465651988983154 | BCE Loss: 1.0242230892181396\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.706290245056152 | KNN Loss: 3.6790428161621094 | BCE Loss: 1.027247667312622\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.728443622589111 | KNN Loss: 3.703861951828003 | BCE Loss: 1.0245815515518188\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.700162887573242 | KNN Loss: 3.673523426055908 | BCE Loss: 1.0266393423080444\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.689603805541992 | KNN Loss: 3.6785995960235596 | BCE Loss: 1.0110044479370117\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.673904895782471 | KNN Loss: 3.661639928817749 | BCE Loss: 1.0122650861740112\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.712985515594482 | KNN Loss: 3.717270851135254 | BCE Loss: 0.9957147240638733\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.707769393920898 | KNN Loss: 3.6887001991271973 | BCE Loss: 1.019068956375122\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.752262115478516 | KNN Loss: 3.6921589374542236 | BCE Loss: 1.060103178024292\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.7571306228637695 | KNN Loss: 3.7369234561920166 | BCE Loss: 1.020207405090332\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.723186492919922 | KNN Loss: 3.688580274581909 | BCE Loss: 1.0346064567565918\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.750028610229492 | KNN Loss: 3.7095816135406494 | BCE Loss: 1.0404472351074219\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.693886756896973 | KNN Loss: 3.66839337348938 | BCE Loss: 1.0254931449890137\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.734385967254639 | KNN Loss: 3.7093958854675293 | BCE Loss: 1.0249900817871094\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.706839561462402 | KNN Loss: 3.6824920177459717 | BCE Loss: 1.0243476629257202\n",
      "Epoch   273: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.711034774780273 | KNN Loss: 3.6732494831085205 | BCE Loss: 1.0377850532531738\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.704259395599365 | KNN Loss: 3.675572395324707 | BCE Loss: 1.0286868810653687\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.706358909606934 | KNN Loss: 3.693300247192383 | BCE Loss: 1.0130584239959717\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.7456536293029785 | KNN Loss: 3.682596206665039 | BCE Loss: 1.0630574226379395\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.716856002807617 | KNN Loss: 3.6789634227752686 | BCE Loss: 1.0378923416137695\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.735124111175537 | KNN Loss: 3.7083451747894287 | BCE Loss: 1.0267789363861084\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.74442195892334 | KNN Loss: 3.7056851387023926 | BCE Loss: 1.0387365818023682\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.720856189727783 | KNN Loss: 3.690387725830078 | BCE Loss: 1.0304685831069946\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.726278305053711 | KNN Loss: 3.6969006061553955 | BCE Loss: 1.029377818107605\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.69017219543457 | KNN Loss: 3.6769704818725586 | BCE Loss: 1.0132015943527222\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.722629547119141 | KNN Loss: 3.700340509414673 | BCE Loss: 1.0222890377044678\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.71497917175293 | KNN Loss: 3.6703481674194336 | BCE Loss: 1.044630765914917\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.757322311401367 | KNN Loss: 3.7352206707000732 | BCE Loss: 1.022101879119873\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.726691722869873 | KNN Loss: 3.6905741691589355 | BCE Loss: 1.0361175537109375\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.699631690979004 | KNN Loss: 3.6796040534973145 | BCE Loss: 1.0200273990631104\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.708504676818848 | KNN Loss: 3.6837000846862793 | BCE Loss: 1.0248043537139893\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.721561431884766 | KNN Loss: 3.677276611328125 | BCE Loss: 1.0442850589752197\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.717644691467285 | KNN Loss: 3.708991289138794 | BCE Loss: 1.0086532831192017\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.713440895080566 | KNN Loss: 3.6895651817321777 | BCE Loss: 1.0238757133483887\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.745871067047119 | KNN Loss: 3.694253444671631 | BCE Loss: 1.0516176223754883\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.699864864349365 | KNN Loss: 3.6790504455566406 | BCE Loss: 1.0208144187927246\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.708962917327881 | KNN Loss: 3.690802574157715 | BCE Loss: 1.018160343170166\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.717891216278076 | KNN Loss: 3.6889567375183105 | BCE Loss: 1.0289344787597656\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.750123023986816 | KNN Loss: 3.70741605758667 | BCE Loss: 1.0427072048187256\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.696694374084473 | KNN Loss: 3.674527406692505 | BCE Loss: 1.0221672058105469\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.722381591796875 | KNN Loss: 3.7078495025634766 | BCE Loss: 1.0145318508148193\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.708811283111572 | KNN Loss: 3.6685314178466797 | BCE Loss: 1.0402798652648926\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.7191057205200195 | KNN Loss: 3.685349702835083 | BCE Loss: 1.033756136894226\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.73675537109375 | KNN Loss: 3.7149288654327393 | BCE Loss: 1.0218266248703003\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.720863342285156 | KNN Loss: 3.7006547451019287 | BCE Loss: 1.0202088356018066\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.700414657592773 | KNN Loss: 3.668752670288086 | BCE Loss: 1.031661868095398\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.681094646453857 | KNN Loss: 3.6598615646362305 | BCE Loss: 1.021233081817627\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.728196620941162 | KNN Loss: 3.696167230606079 | BCE Loss: 1.0320292711257935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.682867050170898 | KNN Loss: 3.6782047748565674 | BCE Loss: 1.004662275314331\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.756335258483887 | KNN Loss: 3.707280397415161 | BCE Loss: 1.0490546226501465\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.712508201599121 | KNN Loss: 3.673041582107544 | BCE Loss: 1.0394665002822876\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.72740364074707 | KNN Loss: 3.7171902656555176 | BCE Loss: 1.0102134943008423\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.697874069213867 | KNN Loss: 3.6791491508483887 | BCE Loss: 1.018724799156189\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.705450534820557 | KNN Loss: 3.6719131469726562 | BCE Loss: 1.0335373878479004\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.713605880737305 | KNN Loss: 3.6799371242523193 | BCE Loss: 1.0336689949035645\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.705572128295898 | KNN Loss: 3.667717695236206 | BCE Loss: 1.0378541946411133\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.687354564666748 | KNN Loss: 3.6574604511260986 | BCE Loss: 1.0298941135406494\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.681981563568115 | KNN Loss: 3.6573708057403564 | BCE Loss: 1.0246107578277588\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.746398448944092 | KNN Loss: 3.7142884731292725 | BCE Loss: 1.0321099758148193\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.759522914886475 | KNN Loss: 3.6957461833953857 | BCE Loss: 1.0637767314910889\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.735998153686523 | KNN Loss: 3.7194671630859375 | BCE Loss: 1.016530990600586\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.709847927093506 | KNN Loss: 3.6796464920043945 | BCE Loss: 1.0302013158798218\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.686730861663818 | KNN Loss: 3.656306266784668 | BCE Loss: 1.0304245948791504\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.742318153381348 | KNN Loss: 3.720151424407959 | BCE Loss: 1.0221667289733887\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.757104873657227 | KNN Loss: 3.682445526123047 | BCE Loss: 1.0746594667434692\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.718577861785889 | KNN Loss: 3.6626524925231934 | BCE Loss: 1.0559253692626953\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.745880603790283 | KNN Loss: 3.7173945903778076 | BCE Loss: 1.0284860134124756\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.732540607452393 | KNN Loss: 3.702078342437744 | BCE Loss: 1.0304622650146484\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.70814323425293 | KNN Loss: 3.66611385345459 | BCE Loss: 1.0420293807983398\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.693873405456543 | KNN Loss: 3.6795761585235596 | BCE Loss: 1.0142970085144043\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.696648120880127 | KNN Loss: 3.660924196243286 | BCE Loss: 1.0357239246368408\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.748110294342041 | KNN Loss: 3.7448606491088867 | BCE Loss: 1.0032496452331543\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.7363667488098145 | KNN Loss: 3.7191760540008545 | BCE Loss: 1.01719069480896\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.753765106201172 | KNN Loss: 3.713649272918701 | BCE Loss: 1.0401155948638916\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.682204246520996 | KNN Loss: 3.676286220550537 | BCE Loss: 1.005918025970459\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.681175708770752 | KNN Loss: 3.6625688076019287 | BCE Loss: 1.0186069011688232\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.69915246963501 | KNN Loss: 3.67755389213562 | BCE Loss: 1.0215985774993896\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.699954986572266 | KNN Loss: 3.6809563636779785 | BCE Loss: 1.018998384475708\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.705481052398682 | KNN Loss: 3.666815757751465 | BCE Loss: 1.0386652946472168\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.758730888366699 | KNN Loss: 3.704630136489868 | BCE Loss: 1.0541009902954102\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.706918716430664 | KNN Loss: 3.6966333389282227 | BCE Loss: 1.0102851390838623\n",
      "Epoch   284: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.705955982208252 | KNN Loss: 3.668410062789917 | BCE Loss: 1.0375458002090454\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.670576572418213 | KNN Loss: 3.659276008605957 | BCE Loss: 1.0113005638122559\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.660672664642334 | KNN Loss: 3.6547939777374268 | BCE Loss: 1.0058786869049072\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.7106733322143555 | KNN Loss: 3.681835174560547 | BCE Loss: 1.0288382768630981\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.697312355041504 | KNN Loss: 3.6793935298919678 | BCE Loss: 1.0179190635681152\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.692525863647461 | KNN Loss: 3.67142653465271 | BCE Loss: 1.0210994482040405\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.687601566314697 | KNN Loss: 3.673754930496216 | BCE Loss: 1.013846755027771\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.7293243408203125 | KNN Loss: 3.6961474418640137 | BCE Loss: 1.0331768989562988\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.775117874145508 | KNN Loss: 3.7420871257781982 | BCE Loss: 1.03303062915802\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.763422966003418 | KNN Loss: 3.695112943649292 | BCE Loss: 1.068310022354126\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.776978492736816 | KNN Loss: 3.7170257568359375 | BCE Loss: 1.0599524974822998\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.661281585693359 | KNN Loss: 3.6771421432495117 | BCE Loss: 0.9841396808624268\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.6750311851501465 | KNN Loss: 3.649695873260498 | BCE Loss: 1.0253351926803589\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.721164226531982 | KNN Loss: 3.689724922180176 | BCE Loss: 1.0314393043518066\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.6659369468688965 | KNN Loss: 3.6562697887420654 | BCE Loss: 1.0096672773361206\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.742520332336426 | KNN Loss: 3.725064277648926 | BCE Loss: 1.017456293106079\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.747509002685547 | KNN Loss: 3.697167158126831 | BCE Loss: 1.0503418445587158\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.748507499694824 | KNN Loss: 3.726522445678711 | BCE Loss: 1.0219850540161133\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.680328369140625 | KNN Loss: 3.678135633468628 | BCE Loss: 1.002192735671997\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.706331729888916 | KNN Loss: 3.675097703933716 | BCE Loss: 1.0312339067459106\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.730548858642578 | KNN Loss: 3.715196371078491 | BCE Loss: 1.0153526067733765\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.706284046173096 | KNN Loss: 3.7021682262420654 | BCE Loss: 1.0041159391403198\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.696307182312012 | KNN Loss: 3.661076784133911 | BCE Loss: 1.0352301597595215\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.7458624839782715 | KNN Loss: 3.6944468021392822 | BCE Loss: 1.0514156818389893\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.724441051483154 | KNN Loss: 3.683220863342285 | BCE Loss: 1.0412203073501587\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.708182334899902 | KNN Loss: 3.6886184215545654 | BCE Loss: 1.019563913345337\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.731033802032471 | KNN Loss: 3.7109532356262207 | BCE Loss: 1.02008056640625\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.69033670425415 | KNN Loss: 3.689574718475342 | BCE Loss: 1.0007619857788086\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.736522197723389 | KNN Loss: 3.7174205780029297 | BCE Loss: 1.019101619720459\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.721794605255127 | KNN Loss: 3.67574143409729 | BCE Loss: 1.0460532903671265\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.666953086853027 | KNN Loss: 3.677644729614258 | BCE Loss: 0.9893082976341248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.760621547698975 | KNN Loss: 3.7019033432006836 | BCE Loss: 1.0587183237075806\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.719483375549316 | KNN Loss: 3.6946074962615967 | BCE Loss: 1.0248757600784302\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.71480131149292 | KNN Loss: 3.681589126586914 | BCE Loss: 1.0332121849060059\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.73521614074707 | KNN Loss: 3.6880173683166504 | BCE Loss: 1.0471986532211304\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.7194294929504395 | KNN Loss: 3.6698851585388184 | BCE Loss: 1.0495442152023315\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.683517932891846 | KNN Loss: 3.6646318435668945 | BCE Loss: 1.0188862085342407\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.706836223602295 | KNN Loss: 3.6750364303588867 | BCE Loss: 1.0317997932434082\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.745070934295654 | KNN Loss: 3.7051889896392822 | BCE Loss: 1.0398818254470825\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.688241958618164 | KNN Loss: 3.6700668334960938 | BCE Loss: 1.0181753635406494\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.7201104164123535 | KNN Loss: 3.687954902648926 | BCE Loss: 1.0321553945541382\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.703863143920898 | KNN Loss: 3.683476448059082 | BCE Loss: 1.0203869342803955\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.702446937561035 | KNN Loss: 3.6844489574432373 | BCE Loss: 1.0179977416992188\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.780082702636719 | KNN Loss: 3.7525932788848877 | BCE Loss: 1.027489423751831\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.721704483032227 | KNN Loss: 3.674048900604248 | BCE Loss: 1.0476553440093994\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.756807804107666 | KNN Loss: 3.7276768684387207 | BCE Loss: 1.0291309356689453\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.759332180023193 | KNN Loss: 3.7059013843536377 | BCE Loss: 1.0534309148788452\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.664319038391113 | KNN Loss: 3.664017915725708 | BCE Loss: 1.0003010034561157\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.684520721435547 | KNN Loss: 3.659574031829834 | BCE Loss: 1.0249468088150024\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.71218729019165 | KNN Loss: 3.7058820724487305 | BCE Loss: 1.0063053369522095\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.711947441101074 | KNN Loss: 3.687467098236084 | BCE Loss: 1.0244801044464111\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.69362735748291 | KNN Loss: 3.664442539215088 | BCE Loss: 1.0291845798492432\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.696234226226807 | KNN Loss: 3.677669048309326 | BCE Loss: 1.0185651779174805\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.774409294128418 | KNN Loss: 3.7364726066589355 | BCE Loss: 1.0379366874694824\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.717824459075928 | KNN Loss: 3.7141740322113037 | BCE Loss: 1.0036503076553345\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.750131130218506 | KNN Loss: 3.717122793197632 | BCE Loss: 1.033008337020874\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.719317436218262 | KNN Loss: 3.7165379524230957 | BCE Loss: 1.002779483795166\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.674781799316406 | KNN Loss: 3.6719167232513428 | BCE Loss: 1.002864956855774\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.709007263183594 | KNN Loss: 3.6752474308013916 | BCE Loss: 1.0337598323822021\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.749999523162842 | KNN Loss: 3.7055721282958984 | BCE Loss: 1.0444273948669434\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.691364288330078 | KNN Loss: 3.6805810928344727 | BCE Loss: 1.010783314704895\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.741374969482422 | KNN Loss: 3.705942392349243 | BCE Loss: 1.0354328155517578\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.735063552856445 | KNN Loss: 3.7004427909851074 | BCE Loss: 1.034621000289917\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.7357072830200195 | KNN Loss: 3.7307591438293457 | BCE Loss: 1.0049480199813843\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.68257999420166 | KNN Loss: 3.6791696548461914 | BCE Loss: 1.0034103393554688\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.695178985595703 | KNN Loss: 3.654557228088379 | BCE Loss: 1.0406216382980347\n",
      "Epoch   295: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.736467361450195 | KNN Loss: 3.6937389373779297 | BCE Loss: 1.0427285432815552\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.690625190734863 | KNN Loss: 3.683035135269165 | BCE Loss: 1.0075899362564087\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.735119819641113 | KNN Loss: 3.720675230026245 | BCE Loss: 1.0144448280334473\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.718040943145752 | KNN Loss: 3.6986329555511475 | BCE Loss: 1.019408106803894\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.696197509765625 | KNN Loss: 3.680886745452881 | BCE Loss: 1.0153107643127441\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.680873870849609 | KNN Loss: 3.6678922176361084 | BCE Loss: 1.01298189163208\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.743167400360107 | KNN Loss: 3.680790901184082 | BCE Loss: 1.0623764991760254\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.731005668640137 | KNN Loss: 3.7103230953216553 | BCE Loss: 1.0206825733184814\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.723587989807129 | KNN Loss: 3.679875135421753 | BCE Loss: 1.0437126159667969\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.753113269805908 | KNN Loss: 3.7199366092681885 | BCE Loss: 1.0331767797470093\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.740170001983643 | KNN Loss: 3.7073171138763428 | BCE Loss: 1.0328527688980103\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.651919364929199 | KNN Loss: 3.6621103286743164 | BCE Loss: 0.9898091554641724\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.716586112976074 | KNN Loss: 3.7059264183044434 | BCE Loss: 1.0106595754623413\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.729111671447754 | KNN Loss: 3.6931710243225098 | BCE Loss: 1.0359407663345337\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.752419471740723 | KNN Loss: 3.712245225906372 | BCE Loss: 1.0401743650436401\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.711305618286133 | KNN Loss: 3.667884588241577 | BCE Loss: 1.0434212684631348\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.667585849761963 | KNN Loss: 3.6613473892211914 | BCE Loss: 1.0062384605407715\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.688938617706299 | KNN Loss: 3.647824287414551 | BCE Loss: 1.0411142110824585\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.672149181365967 | KNN Loss: 3.675086736679077 | BCE Loss: 0.9970623254776001\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.678842067718506 | KNN Loss: 3.67274808883667 | BCE Loss: 1.0060938596725464\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.719740867614746 | KNN Loss: 3.6858105659484863 | BCE Loss: 1.0339305400848389\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.739957809448242 | KNN Loss: 3.7134969234466553 | BCE Loss: 1.0264610052108765\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.689311504364014 | KNN Loss: 3.6926090717315674 | BCE Loss: 0.9967023134231567\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.699707508087158 | KNN Loss: 3.660176992416382 | BCE Loss: 1.039530634880066\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.728813648223877 | KNN Loss: 3.703242063522339 | BCE Loss: 1.025571584701538\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.6815056800842285 | KNN Loss: 3.67619252204895 | BCE Loss: 1.0053132772445679\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.692913055419922 | KNN Loss: 3.658304214477539 | BCE Loss: 1.0346088409423828\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.732068061828613 | KNN Loss: 3.7218406200408936 | BCE Loss: 1.0102276802062988\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.677789211273193 | KNN Loss: 3.6591615676879883 | BCE Loss: 1.018627643585205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.729122161865234 | KNN Loss: 3.691704273223877 | BCE Loss: 1.0374176502227783\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.758906364440918 | KNN Loss: 3.700498104095459 | BCE Loss: 1.058408498764038\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.719759941101074 | KNN Loss: 3.7201356887817383 | BCE Loss: 0.9996241331100464\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.7085161209106445 | KNN Loss: 3.6874711513519287 | BCE Loss: 1.0210449695587158\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.774001121520996 | KNN Loss: 3.71958327293396 | BCE Loss: 1.0544177293777466\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.717996597290039 | KNN Loss: 3.68058705329895 | BCE Loss: 1.0374093055725098\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.716028213500977 | KNN Loss: 3.7086071968078613 | BCE Loss: 1.0074210166931152\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.70789909362793 | KNN Loss: 3.7055933475494385 | BCE Loss: 1.0023059844970703\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.692590236663818 | KNN Loss: 3.683061361312866 | BCE Loss: 1.0095287561416626\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.703666687011719 | KNN Loss: 3.6827027797698975 | BCE Loss: 1.0209637880325317\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.714098930358887 | KNN Loss: 3.6602556705474854 | BCE Loss: 1.053843379020691\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.707862854003906 | KNN Loss: 3.6647136211395264 | BCE Loss: 1.0431491136550903\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.759330749511719 | KNN Loss: 3.7272276878356934 | BCE Loss: 1.0321030616760254\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.702471733093262 | KNN Loss: 3.6754252910614014 | BCE Loss: 1.0270463228225708\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.671472072601318 | KNN Loss: 3.6614623069763184 | BCE Loss: 1.010009765625\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.688191890716553 | KNN Loss: 3.6721901893615723 | BCE Loss: 1.0160017013549805\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.725583553314209 | KNN Loss: 3.706223487854004 | BCE Loss: 1.0193599462509155\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.6913838386535645 | KNN Loss: 3.6797544956207275 | BCE Loss: 1.011629343032837\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.70332145690918 | KNN Loss: 3.702775239944458 | BCE Loss: 1.0005460977554321\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.729286193847656 | KNN Loss: 3.716193199157715 | BCE Loss: 1.0130927562713623\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.701572418212891 | KNN Loss: 3.6779062747955322 | BCE Loss: 1.0236661434173584\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.759352207183838 | KNN Loss: 3.7410590648651123 | BCE Loss: 1.0182932615280151\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.760814189910889 | KNN Loss: 3.7210793495178223 | BCE Loss: 1.0397348403930664\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.693536758422852 | KNN Loss: 3.6547529697418213 | BCE Loss: 1.0387835502624512\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.75563907623291 | KNN Loss: 3.6812868118286133 | BCE Loss: 1.0743521451950073\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.754985332489014 | KNN Loss: 3.728811740875244 | BCE Loss: 1.0261735916137695\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.660178184509277 | KNN Loss: 3.6531670093536377 | BCE Loss: 1.0070114135742188\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.736717224121094 | KNN Loss: 3.7239818572998047 | BCE Loss: 1.012735366821289\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.72069787979126 | KNN Loss: 3.694777011871338 | BCE Loss: 1.0259208679199219\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.779747009277344 | KNN Loss: 3.750823736190796 | BCE Loss: 1.028923511505127\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.699934482574463 | KNN Loss: 3.6563754081726074 | BCE Loss: 1.043558955192566\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.715264320373535 | KNN Loss: 3.681626558303833 | BCE Loss: 1.0336377620697021\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.713372230529785 | KNN Loss: 3.6905534267425537 | BCE Loss: 1.0228188037872314\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.688728332519531 | KNN Loss: 3.6770987510681152 | BCE Loss: 1.011629343032837\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.71073055267334 | KNN Loss: 3.665717840194702 | BCE Loss: 1.0450129508972168\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.716152667999268 | KNN Loss: 3.683164596557617 | BCE Loss: 1.0329880714416504\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.729836463928223 | KNN Loss: 3.7029290199279785 | BCE Loss: 1.026907205581665\n",
      "Epoch   306: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.768604278564453 | KNN Loss: 3.735074520111084 | BCE Loss: 1.03352952003479\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.717803955078125 | KNN Loss: 3.676671028137207 | BCE Loss: 1.0411328077316284\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.7486491203308105 | KNN Loss: 3.7241909503936768 | BCE Loss: 1.0244580507278442\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.723331451416016 | KNN Loss: 3.7109949588775635 | BCE Loss: 1.0123363733291626\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.687961578369141 | KNN Loss: 3.661806106567383 | BCE Loss: 1.0261552333831787\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.740340709686279 | KNN Loss: 3.6743977069854736 | BCE Loss: 1.0659431219100952\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.69206428527832 | KNN Loss: 3.6756772994995117 | BCE Loss: 1.0163872241973877\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.762086391448975 | KNN Loss: 3.7448949813842773 | BCE Loss: 1.0171914100646973\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.734179496765137 | KNN Loss: 3.7167747020721436 | BCE Loss: 1.017404556274414\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.732156276702881 | KNN Loss: 3.688880443572998 | BCE Loss: 1.0432758331298828\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.747251033782959 | KNN Loss: 3.704185724258423 | BCE Loss: 1.0430653095245361\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.696933746337891 | KNN Loss: 3.686556577682495 | BCE Loss: 1.010377287864685\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.712023735046387 | KNN Loss: 3.6896162033081055 | BCE Loss: 1.0224072933197021\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.718076229095459 | KNN Loss: 3.7056732177734375 | BCE Loss: 1.012403130531311\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.757542610168457 | KNN Loss: 3.735835552215576 | BCE Loss: 1.0217071771621704\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.703093528747559 | KNN Loss: 3.690030097961426 | BCE Loss: 1.0130631923675537\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.750758171081543 | KNN Loss: 3.7155723571777344 | BCE Loss: 1.035185694694519\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.691974639892578 | KNN Loss: 3.6899728775024414 | BCE Loss: 1.0020017623901367\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.706157684326172 | KNN Loss: 3.709155797958374 | BCE Loss: 0.9970017671585083\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.738856315612793 | KNN Loss: 3.7149789333343506 | BCE Loss: 1.0238776206970215\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.7662034034729 | KNN Loss: 3.7014055252075195 | BCE Loss: 1.0647979974746704\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.711397171020508 | KNN Loss: 3.6840121746063232 | BCE Loss: 1.0273851156234741\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.728856086730957 | KNN Loss: 3.694091796875 | BCE Loss: 1.0347644090652466\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.702390670776367 | KNN Loss: 3.6774580478668213 | BCE Loss: 1.024932861328125\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.732126235961914 | KNN Loss: 3.7000012397766113 | BCE Loss: 1.0321247577667236\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.7383222579956055 | KNN Loss: 3.701951742172241 | BCE Loss: 1.0363706350326538\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.747222900390625 | KNN Loss: 3.6968963146209717 | BCE Loss: 1.0503268241882324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.6973490715026855 | KNN Loss: 3.6869826316833496 | BCE Loss: 1.010366439819336\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.762295722961426 | KNN Loss: 3.740694046020508 | BCE Loss: 1.021601915359497\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.765730381011963 | KNN Loss: 3.7262954711914062 | BCE Loss: 1.0394349098205566\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.670647144317627 | KNN Loss: 3.644244432449341 | BCE Loss: 1.0264027118682861\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.692130088806152 | KNN Loss: 3.671787977218628 | BCE Loss: 1.0203418731689453\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.709624767303467 | KNN Loss: 3.69140362739563 | BCE Loss: 1.0182210206985474\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.742819786071777 | KNN Loss: 3.708972930908203 | BCE Loss: 1.0338467359542847\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.745131492614746 | KNN Loss: 3.6836631298065186 | BCE Loss: 1.0614686012268066\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.742173194885254 | KNN Loss: 3.704718828201294 | BCE Loss: 1.0374544858932495\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.70723819732666 | KNN Loss: 3.686823844909668 | BCE Loss: 1.0204143524169922\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.7111968994140625 | KNN Loss: 3.702633857727051 | BCE Loss: 1.0085632801055908\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.756355285644531 | KNN Loss: 3.717113494873047 | BCE Loss: 1.0392415523529053\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.713326454162598 | KNN Loss: 3.6817517280578613 | BCE Loss: 1.0315749645233154\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.7353434562683105 | KNN Loss: 3.714508533477783 | BCE Loss: 1.020835041999817\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.7036542892456055 | KNN Loss: 3.68971848487854 | BCE Loss: 1.0139355659484863\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.745636940002441 | KNN Loss: 3.7071382999420166 | BCE Loss: 1.0384984016418457\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.666481971740723 | KNN Loss: 3.668532133102417 | BCE Loss: 0.9979498386383057\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.772621154785156 | KNN Loss: 3.731745958328247 | BCE Loss: 1.0408751964569092\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.735990524291992 | KNN Loss: 3.713845729827881 | BCE Loss: 1.0221446752548218\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.720157623291016 | KNN Loss: 3.6849002838134766 | BCE Loss: 1.03525710105896\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.700084686279297 | KNN Loss: 3.6989572048187256 | BCE Loss: 1.0011276006698608\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.722208023071289 | KNN Loss: 3.6972062587738037 | BCE Loss: 1.0250020027160645\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.759961128234863 | KNN Loss: 3.7034807205200195 | BCE Loss: 1.0564804077148438\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.7534685134887695 | KNN Loss: 3.738530158996582 | BCE Loss: 1.0149381160736084\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.717840671539307 | KNN Loss: 3.685476779937744 | BCE Loss: 1.0323638916015625\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.756363868713379 | KNN Loss: 3.7250590324401855 | BCE Loss: 1.0313045978546143\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.71618127822876 | KNN Loss: 3.6987287998199463 | BCE Loss: 1.0174524784088135\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.740492820739746 | KNN Loss: 3.703711748123169 | BCE Loss: 1.036780834197998\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.749866008758545 | KNN Loss: 3.6892449855804443 | BCE Loss: 1.0606210231781006\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.670970916748047 | KNN Loss: 3.645430088043213 | BCE Loss: 1.0255409479141235\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.733060836791992 | KNN Loss: 3.699608325958252 | BCE Loss: 1.0334525108337402\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.72356653213501 | KNN Loss: 3.6856448650360107 | BCE Loss: 1.0379215478897095\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.721341133117676 | KNN Loss: 3.67189884185791 | BCE Loss: 1.049442172050476\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.702513694763184 | KNN Loss: 3.686413049697876 | BCE Loss: 1.0161004066467285\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.728390693664551 | KNN Loss: 3.708406686782837 | BCE Loss: 1.0199838876724243\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.738971710205078 | KNN Loss: 3.7016210556030273 | BCE Loss: 1.0373506546020508\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.78846549987793 | KNN Loss: 3.7230777740478516 | BCE Loss: 1.0653876066207886\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.698582649230957 | KNN Loss: 3.673426866531372 | BCE Loss: 1.0251555442810059\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.733575820922852 | KNN Loss: 3.68318247795105 | BCE Loss: 1.0503931045532227\n",
      "Epoch   317: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.697765350341797 | KNN Loss: 3.686530590057373 | BCE Loss: 1.011234998703003\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.672695636749268 | KNN Loss: 3.6673436164855957 | BCE Loss: 1.0053520202636719\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.685015678405762 | KNN Loss: 3.663836717605591 | BCE Loss: 1.02117919921875\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.732848644256592 | KNN Loss: 3.71030592918396 | BCE Loss: 1.0225427150726318\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.708305358886719 | KNN Loss: 3.685680866241455 | BCE Loss: 1.0226244926452637\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.724102020263672 | KNN Loss: 3.703667402267456 | BCE Loss: 1.0204346179962158\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.788209438323975 | KNN Loss: 3.758051633834839 | BCE Loss: 1.0301578044891357\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.73535680770874 | KNN Loss: 3.7287111282348633 | BCE Loss: 1.0066455602645874\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.741523265838623 | KNN Loss: 3.7227370738983154 | BCE Loss: 1.0187861919403076\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.758485794067383 | KNN Loss: 3.742316484451294 | BCE Loss: 1.016169548034668\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.693703651428223 | KNN Loss: 3.656663417816162 | BCE Loss: 1.0370404720306396\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.756160736083984 | KNN Loss: 3.708491086959839 | BCE Loss: 1.0476696491241455\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.7455339431762695 | KNN Loss: 3.7064499855041504 | BCE Loss: 1.0390840768814087\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.744112014770508 | KNN Loss: 3.7001287937164307 | BCE Loss: 1.043982982635498\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.686977863311768 | KNN Loss: 3.671152353286743 | BCE Loss: 1.0158255100250244\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.736767768859863 | KNN Loss: 3.6975784301757812 | BCE Loss: 1.039189338684082\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.745771408081055 | KNN Loss: 3.7166218757629395 | BCE Loss: 1.0291494131088257\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.725762844085693 | KNN Loss: 3.698673725128174 | BCE Loss: 1.0270891189575195\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.724865913391113 | KNN Loss: 3.697223663330078 | BCE Loss: 1.0276424884796143\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.713841915130615 | KNN Loss: 3.6834094524383545 | BCE Loss: 1.0304324626922607\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.652446746826172 | KNN Loss: 3.6696786880493164 | BCE Loss: 0.9827678203582764\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.729379653930664 | KNN Loss: 3.6890830993652344 | BCE Loss: 1.0402963161468506\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.731306076049805 | KNN Loss: 3.682112216949463 | BCE Loss: 1.0491936206817627\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.708759307861328 | KNN Loss: 3.6956284046173096 | BCE Loss: 1.0131306648254395\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.791077136993408 | KNN Loss: 3.7631969451904297 | BCE Loss: 1.0278801918029785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.703286170959473 | KNN Loss: 3.7073047161102295 | BCE Loss: 0.9959813356399536\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.7307538986206055 | KNN Loss: 3.6719863414764404 | BCE Loss: 1.058767318725586\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.744879245758057 | KNN Loss: 3.7323477268218994 | BCE Loss: 1.0125315189361572\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.709120273590088 | KNN Loss: 3.7197728157043457 | BCE Loss: 0.9893473386764526\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.719443321228027 | KNN Loss: 3.6686720848083496 | BCE Loss: 1.0507714748382568\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.702714920043945 | KNN Loss: 3.6722261905670166 | BCE Loss: 1.0304887294769287\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.727326393127441 | KNN Loss: 3.696885824203491 | BCE Loss: 1.0304408073425293\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.694815158843994 | KNN Loss: 3.673060655593872 | BCE Loss: 1.021754503250122\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.675843238830566 | KNN Loss: 3.6644058227539062 | BCE Loss: 1.0114376544952393\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.724254608154297 | KNN Loss: 3.702420473098755 | BCE Loss: 1.021833896636963\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.720667839050293 | KNN Loss: 3.6948800086975098 | BCE Loss: 1.0257880687713623\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.775290012359619 | KNN Loss: 3.6918420791625977 | BCE Loss: 1.0834479331970215\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.7160139083862305 | KNN Loss: 3.6810972690582275 | BCE Loss: 1.034916639328003\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.724029541015625 | KNN Loss: 3.6780693531036377 | BCE Loss: 1.0459599494934082\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.695437908172607 | KNN Loss: 3.6489384174346924 | BCE Loss: 1.046499490737915\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.718958854675293 | KNN Loss: 3.6860570907592773 | BCE Loss: 1.0329020023345947\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.723653793334961 | KNN Loss: 3.719745397567749 | BCE Loss: 1.003908395767212\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.713957786560059 | KNN Loss: 3.7044568061828613 | BCE Loss: 1.0095009803771973\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.785771369934082 | KNN Loss: 3.707120418548584 | BCE Loss: 1.0786511898040771\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.6619768142700195 | KNN Loss: 3.656709909439087 | BCE Loss: 1.0052671432495117\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.725358009338379 | KNN Loss: 3.713346481323242 | BCE Loss: 1.0120112895965576\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.693181037902832 | KNN Loss: 3.6856729984283447 | BCE Loss: 1.0075080394744873\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.7028326988220215 | KNN Loss: 3.6860289573669434 | BCE Loss: 1.0168037414550781\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.736392021179199 | KNN Loss: 3.7029566764831543 | BCE Loss: 1.033435583114624\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.733846187591553 | KNN Loss: 3.7074882984161377 | BCE Loss: 1.026357889175415\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.78626012802124 | KNN Loss: 3.7229361534118652 | BCE Loss: 1.063323974609375\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.796083927154541 | KNN Loss: 3.7701895236968994 | BCE Loss: 1.0258945226669312\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.700044631958008 | KNN Loss: 3.6877989768981934 | BCE Loss: 1.012245774269104\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.707699298858643 | KNN Loss: 3.6749565601348877 | BCE Loss: 1.0327426195144653\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.729903221130371 | KNN Loss: 3.6945905685424805 | BCE Loss: 1.0353124141693115\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.703312873840332 | KNN Loss: 3.685487747192383 | BCE Loss: 1.0178251266479492\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.722302436828613 | KNN Loss: 3.7032644748687744 | BCE Loss: 1.019038200378418\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.729129791259766 | KNN Loss: 3.707113027572632 | BCE Loss: 1.022017002105713\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.711014747619629 | KNN Loss: 3.700568675994873 | BCE Loss: 1.0104458332061768\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.719425201416016 | KNN Loss: 3.699540615081787 | BCE Loss: 1.0198843479156494\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.724591255187988 | KNN Loss: 3.6991167068481445 | BCE Loss: 1.0254745483398438\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.665350914001465 | KNN Loss: 3.6449813842773438 | BCE Loss: 1.0203694105148315\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.760922431945801 | KNN Loss: 3.744399070739746 | BCE Loss: 1.0165233612060547\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.727817058563232 | KNN Loss: 3.6803956031799316 | BCE Loss: 1.0474215745925903\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.76470422744751 | KNN Loss: 3.728712797164917 | BCE Loss: 1.0359913110733032\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.72283935546875 | KNN Loss: 3.671645402908325 | BCE Loss: 1.0511938333511353\n",
      "Epoch   328: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.706590175628662 | KNN Loss: 3.6639435291290283 | BCE Loss: 1.0426465272903442\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.726530075073242 | KNN Loss: 3.6927578449249268 | BCE Loss: 1.0337724685668945\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.727312088012695 | KNN Loss: 3.7132999897003174 | BCE Loss: 1.0140119791030884\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.703510761260986 | KNN Loss: 3.664405107498169 | BCE Loss: 1.039105772972107\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.732407569885254 | KNN Loss: 3.6955814361572266 | BCE Loss: 1.0368258953094482\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.712477207183838 | KNN Loss: 3.6910083293914795 | BCE Loss: 1.0214687585830688\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.737777233123779 | KNN Loss: 3.7335493564605713 | BCE Loss: 1.004227876663208\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.75555944442749 | KNN Loss: 3.727673292160034 | BCE Loss: 1.027886152267456\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.712176322937012 | KNN Loss: 3.7043304443359375 | BCE Loss: 1.0078456401824951\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.71320104598999 | KNN Loss: 3.704464912414551 | BCE Loss: 1.008736252784729\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.698547840118408 | KNN Loss: 3.6802432537078857 | BCE Loss: 1.0183045864105225\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.6715192794799805 | KNN Loss: 3.6724178791046143 | BCE Loss: 0.9991016387939453\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.720345973968506 | KNN Loss: 3.6740775108337402 | BCE Loss: 1.0462685823440552\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.721857070922852 | KNN Loss: 3.6968488693237305 | BCE Loss: 1.0250084400177002\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.748107433319092 | KNN Loss: 3.7165863513946533 | BCE Loss: 1.031521201133728\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.7309675216674805 | KNN Loss: 3.6707334518432617 | BCE Loss: 1.0602339506149292\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.731753349304199 | KNN Loss: 3.693647861480713 | BCE Loss: 1.0381052494049072\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.728177547454834 | KNN Loss: 3.698878526687622 | BCE Loss: 1.029299020767212\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.741522789001465 | KNN Loss: 3.686208963394165 | BCE Loss: 1.0553139448165894\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.731686115264893 | KNN Loss: 3.7186107635498047 | BCE Loss: 1.013075351715088\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.676825523376465 | KNN Loss: 3.660984992980957 | BCE Loss: 1.0158405303955078\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.729189395904541 | KNN Loss: 3.688140392303467 | BCE Loss: 1.0410491228103638\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.715252876281738 | KNN Loss: 3.6665682792663574 | BCE Loss: 1.0486847162246704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.696038246154785 | KNN Loss: 3.683161973953247 | BCE Loss: 1.0128765106201172\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.717602729797363 | KNN Loss: 3.699557304382324 | BCE Loss: 1.01804518699646\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.715530872344971 | KNN Loss: 3.663956880569458 | BCE Loss: 1.0515739917755127\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.771039962768555 | KNN Loss: 3.7389886379241943 | BCE Loss: 1.0320510864257812\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.737974166870117 | KNN Loss: 3.699420213699341 | BCE Loss: 1.0385539531707764\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.757017135620117 | KNN Loss: 3.7121214866638184 | BCE Loss: 1.0448956489562988\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.698191165924072 | KNN Loss: 3.702141523361206 | BCE Loss: 0.9960498213768005\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.745579719543457 | KNN Loss: 3.698697328567505 | BCE Loss: 1.0468826293945312\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.729282379150391 | KNN Loss: 3.713709592819214 | BCE Loss: 1.0155727863311768\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.739006042480469 | KNN Loss: 3.713017225265503 | BCE Loss: 1.025989055633545\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.7393798828125 | KNN Loss: 3.715019702911377 | BCE Loss: 1.0243604183197021\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.697933673858643 | KNN Loss: 3.668915271759033 | BCE Loss: 1.0290182828903198\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.7723236083984375 | KNN Loss: 3.717446804046631 | BCE Loss: 1.054876685142517\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.711971282958984 | KNN Loss: 3.67783784866333 | BCE Loss: 1.0341334342956543\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.71464204788208 | KNN Loss: 3.679292678833008 | BCE Loss: 1.0353493690490723\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.764622688293457 | KNN Loss: 3.7285561561584473 | BCE Loss: 1.0360662937164307\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.690650463104248 | KNN Loss: 3.6641550064086914 | BCE Loss: 1.0264954566955566\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.708690166473389 | KNN Loss: 3.637629985809326 | BCE Loss: 1.0710601806640625\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.707030773162842 | KNN Loss: 3.6955983638763428 | BCE Loss: 1.0114322900772095\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.687189102172852 | KNN Loss: 3.6732289791107178 | BCE Loss: 1.0139598846435547\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.768248558044434 | KNN Loss: 3.7293388843536377 | BCE Loss: 1.038909912109375\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.698413848876953 | KNN Loss: 3.670567035675049 | BCE Loss: 1.0278468132019043\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.7062482833862305 | KNN Loss: 3.682016372680664 | BCE Loss: 1.0242316722869873\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.770749568939209 | KNN Loss: 3.7051491737365723 | BCE Loss: 1.0656005144119263\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.7796101570129395 | KNN Loss: 3.724896192550659 | BCE Loss: 1.0547139644622803\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.691582202911377 | KNN Loss: 3.6668386459350586 | BCE Loss: 1.024743676185608\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.731947898864746 | KNN Loss: 3.6905195713043213 | BCE Loss: 1.0414282083511353\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.716923713684082 | KNN Loss: 3.6944308280944824 | BCE Loss: 1.0224928855895996\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.709305763244629 | KNN Loss: 3.680114269256592 | BCE Loss: 1.029191493988037\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.718334197998047 | KNN Loss: 3.679943323135376 | BCE Loss: 1.0383906364440918\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.771711349487305 | KNN Loss: 3.717263698577881 | BCE Loss: 1.0544477701187134\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.724885940551758 | KNN Loss: 3.6926310062408447 | BCE Loss: 1.0322551727294922\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.674687385559082 | KNN Loss: 3.6587424278259277 | BCE Loss: 1.0159449577331543\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.740647792816162 | KNN Loss: 3.705134153366089 | BCE Loss: 1.0355136394500732\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.693939685821533 | KNN Loss: 3.6706008911132812 | BCE Loss: 1.023338794708252\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.722315788269043 | KNN Loss: 3.7085022926330566 | BCE Loss: 1.0138134956359863\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.685276508331299 | KNN Loss: 3.6883840560913086 | BCE Loss: 0.9968925714492798\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.71312141418457 | KNN Loss: 3.682802438735962 | BCE Loss: 1.0303187370300293\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.713531017303467 | KNN Loss: 3.6879677772521973 | BCE Loss: 1.025563359260559\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.736091613769531 | KNN Loss: 3.678402900695801 | BCE Loss: 1.057688593864441\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.650261402130127 | KNN Loss: 3.6503798961639404 | BCE Loss: 0.9998816847801208\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.722309589385986 | KNN Loss: 3.6858654022216797 | BCE Loss: 1.0364441871643066\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.67563009262085 | KNN Loss: 3.633488655090332 | BCE Loss: 1.0421414375305176\n",
      "Epoch   339: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.705410957336426 | KNN Loss: 3.6761040687561035 | BCE Loss: 1.0293071269989014\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.6652607917785645 | KNN Loss: 3.6543424129486084 | BCE Loss: 1.0109184980392456\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.725170612335205 | KNN Loss: 3.680288791656494 | BCE Loss: 1.0448819398880005\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.705047607421875 | KNN Loss: 3.7141733169555664 | BCE Loss: 0.9908742308616638\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.742724895477295 | KNN Loss: 3.705970525741577 | BCE Loss: 1.0367543697357178\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.72841739654541 | KNN Loss: 3.7004599571228027 | BCE Loss: 1.0279572010040283\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.720518112182617 | KNN Loss: 3.720709800720215 | BCE Loss: 0.9998080730438232\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.700464248657227 | KNN Loss: 3.688354969024658 | BCE Loss: 1.0121092796325684\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.709501266479492 | KNN Loss: 3.667403221130371 | BCE Loss: 1.042098045349121\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.722265720367432 | KNN Loss: 3.6879448890686035 | BCE Loss: 1.0343208312988281\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.756939888000488 | KNN Loss: 3.7275161743164062 | BCE Loss: 1.029423475265503\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.750406265258789 | KNN Loss: 3.7038400173187256 | BCE Loss: 1.0465662479400635\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.7139997482299805 | KNN Loss: 3.6954524517059326 | BCE Loss: 1.018547534942627\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.77656364440918 | KNN Loss: 3.7396037578582764 | BCE Loss: 1.0369596481323242\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.662988185882568 | KNN Loss: 3.6709682941436768 | BCE Loss: 0.9920200705528259\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.717679977416992 | KNN Loss: 3.689892292022705 | BCE Loss: 1.027787446975708\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.716285705566406 | KNN Loss: 3.6794750690460205 | BCE Loss: 1.0368103981018066\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.73763370513916 | KNN Loss: 3.708366632461548 | BCE Loss: 1.0292669534683228\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.663428783416748 | KNN Loss: 3.652263641357422 | BCE Loss: 1.0111651420593262\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.715587615966797 | KNN Loss: 3.6920292377471924 | BCE Loss: 1.0235583782196045\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.771609306335449 | KNN Loss: 3.716658592224121 | BCE Loss: 1.0549507141113281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.716294288635254 | KNN Loss: 3.6879234313964844 | BCE Loss: 1.0283708572387695\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.706413269042969 | KNN Loss: 3.661684513092041 | BCE Loss: 1.0447287559509277\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.7120442390441895 | KNN Loss: 3.714740514755249 | BCE Loss: 0.9973036646842957\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.732468128204346 | KNN Loss: 3.701120376586914 | BCE Loss: 1.0313477516174316\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.715949058532715 | KNN Loss: 3.708346366882324 | BCE Loss: 1.0076026916503906\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.73274564743042 | KNN Loss: 3.7274937629699707 | BCE Loss: 1.0052520036697388\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.6937761306762695 | KNN Loss: 3.668198347091675 | BCE Loss: 1.0255775451660156\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.6882734298706055 | KNN Loss: 3.6559057235717773 | BCE Loss: 1.0323677062988281\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.740067481994629 | KNN Loss: 3.7289083003997803 | BCE Loss: 1.0111589431762695\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.6909708976745605 | KNN Loss: 3.6880693435668945 | BCE Loss: 1.0029014348983765\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.739160537719727 | KNN Loss: 3.7090816497802734 | BCE Loss: 1.0300791263580322\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.700018405914307 | KNN Loss: 3.6657214164733887 | BCE Loss: 1.034296989440918\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.713263034820557 | KNN Loss: 3.6700081825256348 | BCE Loss: 1.0432549715042114\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.723668098449707 | KNN Loss: 3.6898090839385986 | BCE Loss: 1.0338590145111084\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.674607276916504 | KNN Loss: 3.665187120437622 | BCE Loss: 1.0094200372695923\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.697492599487305 | KNN Loss: 3.643033504486084 | BCE Loss: 1.0544592142105103\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.71307373046875 | KNN Loss: 3.6807618141174316 | BCE Loss: 1.0323121547698975\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.727476119995117 | KNN Loss: 3.6921629905700684 | BCE Loss: 1.0353131294250488\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.753187656402588 | KNN Loss: 3.712730884552002 | BCE Loss: 1.040456771850586\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.716843605041504 | KNN Loss: 3.7061944007873535 | BCE Loss: 1.0106492042541504\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.702767372131348 | KNN Loss: 3.717061996459961 | BCE Loss: 0.9857052564620972\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.686798095703125 | KNN Loss: 3.668466091156006 | BCE Loss: 1.0183320045471191\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.7362446784973145 | KNN Loss: 3.7122654914855957 | BCE Loss: 1.0239791870117188\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.687339782714844 | KNN Loss: 3.656503200531006 | BCE Loss: 1.0308364629745483\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.699860095977783 | KNN Loss: 3.6660847663879395 | BCE Loss: 1.0337754487991333\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.689055442810059 | KNN Loss: 3.6424434185028076 | BCE Loss: 1.0466117858886719\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.712453842163086 | KNN Loss: 3.694348096847534 | BCE Loss: 1.0181057453155518\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.720968246459961 | KNN Loss: 3.6941792964935303 | BCE Loss: 1.0267889499664307\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.772540092468262 | KNN Loss: 3.7300026416778564 | BCE Loss: 1.0425372123718262\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.6909027099609375 | KNN Loss: 3.6935768127441406 | BCE Loss: 0.9973257780075073\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.725884437561035 | KNN Loss: 3.695051670074463 | BCE Loss: 1.0308327674865723\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.711266040802002 | KNN Loss: 3.666640043258667 | BCE Loss: 1.0446261167526245\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.70829963684082 | KNN Loss: 3.709461212158203 | BCE Loss: 0.9988384246826172\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.754817485809326 | KNN Loss: 3.6993000507354736 | BCE Loss: 1.0555174350738525\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.670676231384277 | KNN Loss: 3.6494967937469482 | BCE Loss: 1.021179437637329\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.691237449645996 | KNN Loss: 3.7075343132019043 | BCE Loss: 0.9837032556533813\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.737116813659668 | KNN Loss: 3.705258846282959 | BCE Loss: 1.0318577289581299\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.726945400238037 | KNN Loss: 3.6982333660125732 | BCE Loss: 1.0287121534347534\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.742702484130859 | KNN Loss: 3.7251412868499756 | BCE Loss: 1.0175611972808838\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.716327667236328 | KNN Loss: 3.6544861793518066 | BCE Loss: 1.0618417263031006\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.751021862030029 | KNN Loss: 3.7350494861602783 | BCE Loss: 1.0159722566604614\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.697500228881836 | KNN Loss: 3.677902936935425 | BCE Loss: 1.019597053527832\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.699584484100342 | KNN Loss: 3.6806588172912598 | BCE Loss: 1.018925666809082\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.689637184143066 | KNN Loss: 3.6580069065093994 | BCE Loss: 1.0316301584243774\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.778274059295654 | KNN Loss: 3.7340314388275146 | BCE Loss: 1.0442426204681396\n",
      "Epoch   350: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.710536956787109 | KNN Loss: 3.6997745037078857 | BCE Loss: 1.0107626914978027\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.737152576446533 | KNN Loss: 3.6863338947296143 | BCE Loss: 1.050818681716919\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.7058820724487305 | KNN Loss: 3.67672061920166 | BCE Loss: 1.0291612148284912\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.732093334197998 | KNN Loss: 3.689446449279785 | BCE Loss: 1.0426470041275024\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.76494836807251 | KNN Loss: 3.7175652980804443 | BCE Loss: 1.0473830699920654\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.72459602355957 | KNN Loss: 3.6974356174468994 | BCE Loss: 1.02716064453125\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.674625396728516 | KNN Loss: 3.662163734436035 | BCE Loss: 1.0124619007110596\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.774457931518555 | KNN Loss: 3.7282936573028564 | BCE Loss: 1.0461642742156982\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.718044757843018 | KNN Loss: 3.6786551475524902 | BCE Loss: 1.0393896102905273\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.764773368835449 | KNN Loss: 3.7265713214874268 | BCE Loss: 1.0382018089294434\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.74018669128418 | KNN Loss: 3.7157862186431885 | BCE Loss: 1.0244004726409912\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.710744857788086 | KNN Loss: 3.692032814025879 | BCE Loss: 1.0187119245529175\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.737229347229004 | KNN Loss: 3.7092466354370117 | BCE Loss: 1.0279825925827026\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.734267234802246 | KNN Loss: 3.7121853828430176 | BCE Loss: 1.0220818519592285\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.7223711013793945 | KNN Loss: 3.7014780044555664 | BCE Loss: 1.0208933353424072\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.679976463317871 | KNN Loss: 3.6647353172302246 | BCE Loss: 1.0152409076690674\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.709133148193359 | KNN Loss: 3.687237024307251 | BCE Loss: 1.0218961238861084\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.7275872230529785 | KNN Loss: 3.6903457641601562 | BCE Loss: 1.0372413396835327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.737476348876953 | KNN Loss: 3.7125961780548096 | BCE Loss: 1.024880051612854\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.73302698135376 | KNN Loss: 3.67484188079834 | BCE Loss: 1.05818510055542\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.700045108795166 | KNN Loss: 3.6693005561828613 | BCE Loss: 1.0307446718215942\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.69725227355957 | KNN Loss: 3.6881394386291504 | BCE Loss: 1.009113073348999\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.690520763397217 | KNN Loss: 3.6620655059814453 | BCE Loss: 1.0284552574157715\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.747366905212402 | KNN Loss: 3.694300651550293 | BCE Loss: 1.0530664920806885\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.665631294250488 | KNN Loss: 3.6557323932647705 | BCE Loss: 1.0098990201950073\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.743204116821289 | KNN Loss: 3.7181787490844727 | BCE Loss: 1.025025486946106\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.699156284332275 | KNN Loss: 3.6831283569335938 | BCE Loss: 1.0160279273986816\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.704566478729248 | KNN Loss: 3.675431966781616 | BCE Loss: 1.0291345119476318\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.7026448249816895 | KNN Loss: 3.655127763748169 | BCE Loss: 1.0475170612335205\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.785120964050293 | KNN Loss: 3.707195997238159 | BCE Loss: 1.0779249668121338\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.694684028625488 | KNN Loss: 3.664766550064087 | BCE Loss: 1.0299172401428223\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.689613342285156 | KNN Loss: 3.6615142822265625 | BCE Loss: 1.0280990600585938\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.696335792541504 | KNN Loss: 3.657728910446167 | BCE Loss: 1.038607120513916\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.704800128936768 | KNN Loss: 3.6525967121124268 | BCE Loss: 1.0522035360336304\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.726265907287598 | KNN Loss: 3.7053184509277344 | BCE Loss: 1.0209474563598633\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.671009063720703 | KNN Loss: 3.6659390926361084 | BCE Loss: 1.0050700902938843\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.66790246963501 | KNN Loss: 3.659602165222168 | BCE Loss: 1.0083003044128418\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.6692891120910645 | KNN Loss: 3.647477388381958 | BCE Loss: 1.0218117237091064\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.705512046813965 | KNN Loss: 3.6908302307128906 | BCE Loss: 1.0146818161010742\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.722729682922363 | KNN Loss: 3.6756021976470947 | BCE Loss: 1.0471274852752686\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.731653690338135 | KNN Loss: 3.7074434757232666 | BCE Loss: 1.0242100954055786\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.730354309082031 | KNN Loss: 3.691303014755249 | BCE Loss: 1.0390510559082031\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.693233489990234 | KNN Loss: 3.6905722618103027 | BCE Loss: 1.0026614665985107\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.714241981506348 | KNN Loss: 3.666043996810913 | BCE Loss: 1.048197865486145\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.680736541748047 | KNN Loss: 3.6558775901794434 | BCE Loss: 1.0248589515686035\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.713549613952637 | KNN Loss: 3.6997318267822266 | BCE Loss: 1.0138177871704102\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.746233940124512 | KNN Loss: 3.7124643325805664 | BCE Loss: 1.0337697267532349\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.742833137512207 | KNN Loss: 3.6994521617889404 | BCE Loss: 1.0433807373046875\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.705269813537598 | KNN Loss: 3.690047025680542 | BCE Loss: 1.0152225494384766\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.675061225891113 | KNN Loss: 3.6684985160827637 | BCE Loss: 1.0065627098083496\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.730964660644531 | KNN Loss: 3.691007375717163 | BCE Loss: 1.0399571657180786\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.670277118682861 | KNN Loss: 3.661957025527954 | BCE Loss: 1.0083199739456177\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.728460788726807 | KNN Loss: 3.7200632095336914 | BCE Loss: 1.0083976984024048\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.673896312713623 | KNN Loss: 3.661919116973877 | BCE Loss: 1.011977195739746\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.704172134399414 | KNN Loss: 3.6749179363250732 | BCE Loss: 1.0292539596557617\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.681911945343018 | KNN Loss: 3.6632373332977295 | BCE Loss: 1.0186747312545776\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.690723419189453 | KNN Loss: 3.6575767993927 | BCE Loss: 1.0331463813781738\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.728360176086426 | KNN Loss: 3.720167875289917 | BCE Loss: 1.0081921815872192\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.732233047485352 | KNN Loss: 3.6867692470550537 | BCE Loss: 1.0454639196395874\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.728031158447266 | KNN Loss: 3.7082011699676514 | BCE Loss: 1.0198299884796143\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.689388275146484 | KNN Loss: 3.665275812149048 | BCE Loss: 1.024112343788147\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.743792533874512 | KNN Loss: 3.7392032146453857 | BCE Loss: 1.0045890808105469\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.7056708335876465 | KNN Loss: 3.6896262168884277 | BCE Loss: 1.0160446166992188\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.705568790435791 | KNN Loss: 3.6946160793304443 | BCE Loss: 1.0109528303146362\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.697144508361816 | KNN Loss: 3.669577121734619 | BCE Loss: 1.0275671482086182\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.664924621582031 | KNN Loss: 3.663954019546509 | BCE Loss: 1.000970482826233\n",
      "Epoch   361: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.702070236206055 | KNN Loss: 3.672696113586426 | BCE Loss: 1.0293738842010498\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.674999713897705 | KNN Loss: 3.6603870391845703 | BCE Loss: 1.0146126747131348\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.758897304534912 | KNN Loss: 3.6921634674072266 | BCE Loss: 1.0667338371276855\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.718623638153076 | KNN Loss: 3.6804721355438232 | BCE Loss: 1.038151502609253\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.758814334869385 | KNN Loss: 3.7304391860961914 | BCE Loss: 1.0283751487731934\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.72997522354126 | KNN Loss: 3.6753907203674316 | BCE Loss: 1.0545845031738281\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.732349395751953 | KNN Loss: 3.68452525138855 | BCE Loss: 1.0478241443634033\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.73461389541626 | KNN Loss: 3.71232533454895 | BCE Loss: 1.02228844165802\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.700911045074463 | KNN Loss: 3.6819770336151123 | BCE Loss: 1.0189340114593506\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.739334583282471 | KNN Loss: 3.718311071395874 | BCE Loss: 1.0210233926773071\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.735048294067383 | KNN Loss: 3.7040152549743652 | BCE Loss: 1.0310328006744385\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.742199897766113 | KNN Loss: 3.7182059288024902 | BCE Loss: 1.023993730545044\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.681025981903076 | KNN Loss: 3.6570746898651123 | BCE Loss: 1.0239512920379639\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.728014945983887 | KNN Loss: 3.6745095252990723 | BCE Loss: 1.0535054206848145\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.718964576721191 | KNN Loss: 3.6763722896575928 | BCE Loss: 1.0425924062728882\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.678928375244141 | KNN Loss: 3.6544926166534424 | BCE Loss: 1.0244357585906982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.7802228927612305 | KNN Loss: 3.7588999271392822 | BCE Loss: 1.0213230848312378\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.673035144805908 | KNN Loss: 3.671469211578369 | BCE Loss: 1.0015658140182495\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.734552383422852 | KNN Loss: 3.7308170795440674 | BCE Loss: 1.003735065460205\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.733162879943848 | KNN Loss: 3.708827257156372 | BCE Loss: 1.0243357419967651\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.796446323394775 | KNN Loss: 3.7646076679229736 | BCE Loss: 1.0318386554718018\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.76054573059082 | KNN Loss: 3.7119338512420654 | BCE Loss: 1.0486116409301758\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.728576183319092 | KNN Loss: 3.7051239013671875 | BCE Loss: 1.0234522819519043\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.7119059562683105 | KNN Loss: 3.707667589187622 | BCE Loss: 1.0042383670806885\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.741637706756592 | KNN Loss: 3.6946346759796143 | BCE Loss: 1.047003149986267\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.73255729675293 | KNN Loss: 3.6971116065979004 | BCE Loss: 1.0354454517364502\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.6793928146362305 | KNN Loss: 3.674384593963623 | BCE Loss: 1.005008339881897\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.714690208435059 | KNN Loss: 3.669687509536743 | BCE Loss: 1.0450024604797363\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.711467742919922 | KNN Loss: 3.678840398788452 | BCE Loss: 1.0326275825500488\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.684570789337158 | KNN Loss: 3.661036729812622 | BCE Loss: 1.0235339403152466\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.715427875518799 | KNN Loss: 3.67919659614563 | BCE Loss: 1.0362313985824585\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.767772197723389 | KNN Loss: 3.7171173095703125 | BCE Loss: 1.0506548881530762\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.708607196807861 | KNN Loss: 3.6884727478027344 | BCE Loss: 1.020134449005127\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.690361022949219 | KNN Loss: 3.6661274433135986 | BCE Loss: 1.0242334604263306\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.755542755126953 | KNN Loss: 3.7315409183502197 | BCE Loss: 1.0240018367767334\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.6957855224609375 | KNN Loss: 3.6702208518981934 | BCE Loss: 1.0255646705627441\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.780913829803467 | KNN Loss: 3.7275240421295166 | BCE Loss: 1.0533897876739502\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.712298393249512 | KNN Loss: 3.7153220176696777 | BCE Loss: 0.9969763159751892\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.744695663452148 | KNN Loss: 3.710162401199341 | BCE Loss: 1.0345335006713867\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.679270267486572 | KNN Loss: 3.6737234592437744 | BCE Loss: 1.0055466890335083\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.700075149536133 | KNN Loss: 3.6637725830078125 | BCE Loss: 1.0363024473190308\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.64397668838501 | KNN Loss: 3.6440675258636475 | BCE Loss: 0.9999093413352966\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.700615882873535 | KNN Loss: 3.6904025077819824 | BCE Loss: 1.0102131366729736\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.742840766906738 | KNN Loss: 3.6791481971740723 | BCE Loss: 1.063692331314087\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.720312118530273 | KNN Loss: 3.692627429962158 | BCE Loss: 1.0276846885681152\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.7050018310546875 | KNN Loss: 3.6630167961120605 | BCE Loss: 1.041985273361206\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.750126838684082 | KNN Loss: 3.719407796859741 | BCE Loss: 1.0307188034057617\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.7237114906311035 | KNN Loss: 3.7140355110168457 | BCE Loss: 1.0096759796142578\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.70808744430542 | KNN Loss: 3.681471109390259 | BCE Loss: 1.0266164541244507\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.70219087600708 | KNN Loss: 3.660907745361328 | BCE Loss: 1.0412830114364624\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.683387279510498 | KNN Loss: 3.6694843769073486 | BCE Loss: 1.013903021812439\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.715330600738525 | KNN Loss: 3.698382616043091 | BCE Loss: 1.0169479846954346\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.678559303283691 | KNN Loss: 3.675163745880127 | BCE Loss: 1.0033957958221436\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.685189723968506 | KNN Loss: 3.667311906814575 | BCE Loss: 1.0178776979446411\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.7373433113098145 | KNN Loss: 3.7179806232452393 | BCE Loss: 1.0193628072738647\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.735557556152344 | KNN Loss: 3.683004379272461 | BCE Loss: 1.052553415298462\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.693150043487549 | KNN Loss: 3.6513705253601074 | BCE Loss: 1.041779637336731\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.741888046264648 | KNN Loss: 3.6949291229248047 | BCE Loss: 1.0469589233398438\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.701533317565918 | KNN Loss: 3.6882832050323486 | BCE Loss: 1.0132503509521484\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.728582382202148 | KNN Loss: 3.7051703929901123 | BCE Loss: 1.0234119892120361\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.708624362945557 | KNN Loss: 3.677288770675659 | BCE Loss: 1.031335711479187\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.725843906402588 | KNN Loss: 3.702913999557495 | BCE Loss: 1.0229299068450928\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.796957969665527 | KNN Loss: 3.7235357761383057 | BCE Loss: 1.0734224319458008\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.660998344421387 | KNN Loss: 3.649555206298828 | BCE Loss: 1.011443018913269\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.656830787658691 | KNN Loss: 3.6405673027038574 | BCE Loss: 1.016263484954834\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.731692314147949 | KNN Loss: 3.7124722003936768 | BCE Loss: 1.0192203521728516\n",
      "Epoch   372: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.670937538146973 | KNN Loss: 3.654752016067505 | BCE Loss: 1.0161857604980469\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.664675235748291 | KNN Loss: 3.6560282707214355 | BCE Loss: 1.008646845817566\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.698709964752197 | KNN Loss: 3.6792399883270264 | BCE Loss: 1.019469976425171\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.694395065307617 | KNN Loss: 3.672064781188965 | BCE Loss: 1.0223302841186523\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.664607048034668 | KNN Loss: 3.6658198833465576 | BCE Loss: 0.9987872838973999\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.722451686859131 | KNN Loss: 3.6857056617736816 | BCE Loss: 1.0367461442947388\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.685243606567383 | KNN Loss: 3.6512742042541504 | BCE Loss: 1.033969521522522\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.7193193435668945 | KNN Loss: 3.680065631866455 | BCE Loss: 1.0392537117004395\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.706460952758789 | KNN Loss: 3.7040252685546875 | BCE Loss: 1.0024356842041016\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.716490268707275 | KNN Loss: 3.666768789291382 | BCE Loss: 1.0497214794158936\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.7492170333862305 | KNN Loss: 3.689939022064209 | BCE Loss: 1.0592782497406006\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.754422187805176 | KNN Loss: 3.7006447315216064 | BCE Loss: 1.0537774562835693\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.756488800048828 | KNN Loss: 3.706907272338867 | BCE Loss: 1.049581527709961\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.737387657165527 | KNN Loss: 3.70890474319458 | BCE Loss: 1.0284826755523682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.717179298400879 | KNN Loss: 3.6856465339660645 | BCE Loss: 1.0315330028533936\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.699735164642334 | KNN Loss: 3.6713080406188965 | BCE Loss: 1.0284271240234375\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.7407636642456055 | KNN Loss: 3.693535804748535 | BCE Loss: 1.0472276210784912\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.7882795333862305 | KNN Loss: 3.7234835624694824 | BCE Loss: 1.0647960901260376\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.648393630981445 | KNN Loss: 3.6593244075775146 | BCE Loss: 0.9890689849853516\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.685923099517822 | KNN Loss: 3.6749303340911865 | BCE Loss: 1.0109926462173462\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.7249345779418945 | KNN Loss: 3.6860744953155518 | BCE Loss: 1.0388603210449219\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.713101387023926 | KNN Loss: 3.6788272857666016 | BCE Loss: 1.0342738628387451\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.750876426696777 | KNN Loss: 3.7117669582366943 | BCE Loss: 1.0391093492507935\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.6925764083862305 | KNN Loss: 3.667283058166504 | BCE Loss: 1.0252931118011475\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.696351528167725 | KNN Loss: 3.6812472343444824 | BCE Loss: 1.0151042938232422\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.711206436157227 | KNN Loss: 3.6825406551361084 | BCE Loss: 1.0286659002304077\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.718813896179199 | KNN Loss: 3.701951742172241 | BCE Loss: 1.016861915588379\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.719720840454102 | KNN Loss: 3.690568208694458 | BCE Loss: 1.0291526317596436\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.749383926391602 | KNN Loss: 3.7398874759674072 | BCE Loss: 1.0094964504241943\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.6941986083984375 | KNN Loss: 3.659862995147705 | BCE Loss: 1.0343356132507324\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.739967346191406 | KNN Loss: 3.7175779342651367 | BCE Loss: 1.0223896503448486\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.757277965545654 | KNN Loss: 3.7198429107666016 | BCE Loss: 1.0374350547790527\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.699397087097168 | KNN Loss: 3.679175615310669 | BCE Loss: 1.0202217102050781\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.715604305267334 | KNN Loss: 3.711780071258545 | BCE Loss: 1.0038243532180786\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.6990461349487305 | KNN Loss: 3.6973087787628174 | BCE Loss: 1.0017375946044922\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.6900482177734375 | KNN Loss: 3.693817138671875 | BCE Loss: 0.9962310791015625\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.739416122436523 | KNN Loss: 3.7143478393554688 | BCE Loss: 1.0250685214996338\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.709787845611572 | KNN Loss: 3.687196969985962 | BCE Loss: 1.0225908756256104\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.678055763244629 | KNN Loss: 3.643557548522949 | BCE Loss: 1.0344980955123901\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.697011947631836 | KNN Loss: 3.675381898880005 | BCE Loss: 1.0216301679611206\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.775157928466797 | KNN Loss: 3.7376182079315186 | BCE Loss: 1.0375394821166992\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.74888277053833 | KNN Loss: 3.7108378410339355 | BCE Loss: 1.038044810295105\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.713074684143066 | KNN Loss: 3.6853628158569336 | BCE Loss: 1.0277118682861328\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.681051254272461 | KNN Loss: 3.650132179260254 | BCE Loss: 1.0309189558029175\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.685861110687256 | KNN Loss: 3.6756162643432617 | BCE Loss: 1.0102447271347046\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.6811323165893555 | KNN Loss: 3.66292142868042 | BCE Loss: 1.0182108879089355\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.7506513595581055 | KNN Loss: 3.7101969718933105 | BCE Loss: 1.0404545068740845\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.696203231811523 | KNN Loss: 3.6612114906311035 | BCE Loss: 1.0349915027618408\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.73203182220459 | KNN Loss: 3.705780506134033 | BCE Loss: 1.0262515544891357\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.688865661621094 | KNN Loss: 3.6677663326263428 | BCE Loss: 1.0210990905761719\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.683197498321533 | KNN Loss: 3.6888017654418945 | BCE Loss: 0.9943956136703491\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.72636079788208 | KNN Loss: 3.713467597961426 | BCE Loss: 1.0128933191299438\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.785024166107178 | KNN Loss: 3.7238786220550537 | BCE Loss: 1.061145544052124\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.713561058044434 | KNN Loss: 3.6949613094329834 | BCE Loss: 1.0185997486114502\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.75532865524292 | KNN Loss: 3.7084572315216064 | BCE Loss: 1.046871304512024\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.685606479644775 | KNN Loss: 3.657224655151367 | BCE Loss: 1.0283818244934082\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.715156555175781 | KNN Loss: 3.679280996322632 | BCE Loss: 1.0358757972717285\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.696477890014648 | KNN Loss: 3.675245761871338 | BCE Loss: 1.0212323665618896\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.712873458862305 | KNN Loss: 3.6939566135406494 | BCE Loss: 1.0189167261123657\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.651607990264893 | KNN Loss: 3.655735731124878 | BCE Loss: 0.9958723783493042\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.732963562011719 | KNN Loss: 3.702671527862549 | BCE Loss: 1.0302917957305908\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.766266345977783 | KNN Loss: 3.7281107902526855 | BCE Loss: 1.0381556749343872\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.732081413269043 | KNN Loss: 3.6822855472564697 | BCE Loss: 1.0497956275939941\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.718679428100586 | KNN Loss: 3.664874315261841 | BCE Loss: 1.0538051128387451\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.68168830871582 | KNN Loss: 3.663224697113037 | BCE Loss: 1.018463373184204\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.711604118347168 | KNN Loss: 3.6813206672668457 | BCE Loss: 1.0302834510803223\n",
      "Epoch   383: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.747066974639893 | KNN Loss: 3.6906728744506836 | BCE Loss: 1.056394100189209\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.684309005737305 | KNN Loss: 3.6433305740356445 | BCE Loss: 1.0409784317016602\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.688200950622559 | KNN Loss: 3.70485258102417 | BCE Loss: 0.9833483099937439\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.699771881103516 | KNN Loss: 3.6811256408691406 | BCE Loss: 1.0186463594436646\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.709185600280762 | KNN Loss: 3.693378448486328 | BCE Loss: 1.0158071517944336\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.696521282196045 | KNN Loss: 3.6975820064544678 | BCE Loss: 0.9989392161369324\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.719539165496826 | KNN Loss: 3.7311244010925293 | BCE Loss: 0.9884148836135864\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.714693069458008 | KNN Loss: 3.690429449081421 | BCE Loss: 1.0242637395858765\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.725205898284912 | KNN Loss: 3.690488576889038 | BCE Loss: 1.034717321395874\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.7604899406433105 | KNN Loss: 3.7146239280700684 | BCE Loss: 1.0458660125732422\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.728562355041504 | KNN Loss: 3.6948561668395996 | BCE Loss: 1.0337060689926147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.716984748840332 | KNN Loss: 3.6645429134368896 | BCE Loss: 1.052441954612732\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.726771354675293 | KNN Loss: 3.6962080001831055 | BCE Loss: 1.0305631160736084\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.729870796203613 | KNN Loss: 3.7021656036376953 | BCE Loss: 1.0277049541473389\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.705059051513672 | KNN Loss: 3.7195026874542236 | BCE Loss: 0.9855563640594482\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.6992058753967285 | KNN Loss: 3.6730051040649414 | BCE Loss: 1.0262008905410767\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.747584342956543 | KNN Loss: 3.686039447784424 | BCE Loss: 1.0615448951721191\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.733644485473633 | KNN Loss: 3.683284044265747 | BCE Loss: 1.0503602027893066\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.743600845336914 | KNN Loss: 3.7328524589538574 | BCE Loss: 1.0107486248016357\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.691247940063477 | KNN Loss: 3.66861891746521 | BCE Loss: 1.0226292610168457\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.716486930847168 | KNN Loss: 3.701885461807251 | BCE Loss: 1.0146013498306274\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.762838840484619 | KNN Loss: 3.6838595867156982 | BCE Loss: 1.0789793729782104\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.704107284545898 | KNN Loss: 3.704629421234131 | BCE Loss: 0.999477744102478\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.7232666015625 | KNN Loss: 3.7076773643493652 | BCE Loss: 1.0155894756317139\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.771333694458008 | KNN Loss: 3.727053642272949 | BCE Loss: 1.0442802906036377\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.735734939575195 | KNN Loss: 3.734565496444702 | BCE Loss: 1.001169204711914\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.672530174255371 | KNN Loss: 3.663132905960083 | BCE Loss: 1.009397268295288\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.694808006286621 | KNN Loss: 3.666895866394043 | BCE Loss: 1.0279122591018677\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.72629976272583 | KNN Loss: 3.6870856285095215 | BCE Loss: 1.0392141342163086\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.7452473640441895 | KNN Loss: 3.7162551879882812 | BCE Loss: 1.0289922952651978\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.708075046539307 | KNN Loss: 3.696019172668457 | BCE Loss: 1.0120559930801392\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.705479621887207 | KNN Loss: 3.672903299331665 | BCE Loss: 1.032576084136963\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.755402088165283 | KNN Loss: 3.714754581451416 | BCE Loss: 1.0406475067138672\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.716563701629639 | KNN Loss: 3.6854512691497803 | BCE Loss: 1.0311123132705688\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.758461952209473 | KNN Loss: 3.707827091217041 | BCE Loss: 1.0506348609924316\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.743809700012207 | KNN Loss: 3.6797449588775635 | BCE Loss: 1.0640649795532227\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.67783260345459 | KNN Loss: 3.664865493774414 | BCE Loss: 1.0129673480987549\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.725467681884766 | KNN Loss: 3.6898653507232666 | BCE Loss: 1.0356025695800781\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.699772834777832 | KNN Loss: 3.657625436782837 | BCE Loss: 1.042147159576416\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.739768981933594 | KNN Loss: 3.6973488330841064 | BCE Loss: 1.0424199104309082\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.700035572052002 | KNN Loss: 3.6802685260772705 | BCE Loss: 1.0197670459747314\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.694360256195068 | KNN Loss: 3.673419713973999 | BCE Loss: 1.0209405422210693\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.7223100662231445 | KNN Loss: 3.6914210319519043 | BCE Loss: 1.0308889150619507\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.726412773132324 | KNN Loss: 3.672649383544922 | BCE Loss: 1.0537631511688232\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.733325958251953 | KNN Loss: 3.6975016593933105 | BCE Loss: 1.0358242988586426\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.789158344268799 | KNN Loss: 3.757498025894165 | BCE Loss: 1.0316603183746338\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.733778953552246 | KNN Loss: 3.6928529739379883 | BCE Loss: 1.040926218032837\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.757513999938965 | KNN Loss: 3.750483274459839 | BCE Loss: 1.0070306062698364\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.735467910766602 | KNN Loss: 3.712031602859497 | BCE Loss: 1.0234365463256836\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.7126617431640625 | KNN Loss: 3.6814777851104736 | BCE Loss: 1.0311837196350098\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.698016166687012 | KNN Loss: 3.684390068054199 | BCE Loss: 1.013625979423523\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.754269599914551 | KNN Loss: 3.7348217964172363 | BCE Loss: 1.0194475650787354\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.739133358001709 | KNN Loss: 3.715271234512329 | BCE Loss: 1.0238622426986694\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.724674224853516 | KNN Loss: 3.7146191596984863 | BCE Loss: 1.0100550651550293\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.675516128540039 | KNN Loss: 3.6590449810028076 | BCE Loss: 1.0164709091186523\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.679020404815674 | KNN Loss: 3.6826131343841553 | BCE Loss: 0.9964072108268738\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.761332988739014 | KNN Loss: 3.7112839221954346 | BCE Loss: 1.0500491857528687\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.764408111572266 | KNN Loss: 3.7063074111938477 | BCE Loss: 1.0581008195877075\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.713160514831543 | KNN Loss: 3.695091962814331 | BCE Loss: 1.0180683135986328\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.730445861816406 | KNN Loss: 3.71046781539917 | BCE Loss: 1.0199782848358154\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.6876349449157715 | KNN Loss: 3.669354200363159 | BCE Loss: 1.0182808637619019\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.762275695800781 | KNN Loss: 3.7312090396881104 | BCE Loss: 1.0310664176940918\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.714969635009766 | KNN Loss: 3.70426344871521 | BCE Loss: 1.0107059478759766\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.773105621337891 | KNN Loss: 3.7021677494049072 | BCE Loss: 1.0709378719329834\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.692080020904541 | KNN Loss: 3.6752021312713623 | BCE Loss: 1.0168777704238892\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.719936370849609 | KNN Loss: 3.693875789642334 | BCE Loss: 1.0260608196258545\n",
      "Epoch   394: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.704877853393555 | KNN Loss: 3.7017855644226074 | BCE Loss: 1.0030921697616577\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.745431900024414 | KNN Loss: 3.7025697231292725 | BCE Loss: 1.0428621768951416\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.659785270690918 | KNN Loss: 3.6579525470733643 | BCE Loss: 1.0018324851989746\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.78134822845459 | KNN Loss: 3.7053537368774414 | BCE Loss: 1.0759943723678589\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.6979827880859375 | KNN Loss: 3.6699674129486084 | BCE Loss: 1.028015375137329\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.68860387802124 | KNN Loss: 3.6725826263427734 | BCE Loss: 1.0160212516784668\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.767765998840332 | KNN Loss: 3.7338991165161133 | BCE Loss: 1.0338666439056396\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.684314727783203 | KNN Loss: 3.6820526123046875 | BCE Loss: 1.0022621154785156\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.6824469566345215 | KNN Loss: 3.686509132385254 | BCE Loss: 0.9959378242492676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.726590156555176 | KNN Loss: 3.656073808670044 | BCE Loss: 1.0705164670944214\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.750820159912109 | KNN Loss: 3.7186710834503174 | BCE Loss: 1.032149314880371\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.727725028991699 | KNN Loss: 3.7031919956207275 | BCE Loss: 1.0245330333709717\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.721490859985352 | KNN Loss: 3.6906983852386475 | BCE Loss: 1.0307927131652832\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.728784561157227 | KNN Loss: 3.67907977104187 | BCE Loss: 1.049704670906067\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.707558631896973 | KNN Loss: 3.70527720451355 | BCE Loss: 1.0022815465927124\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.707256317138672 | KNN Loss: 3.6947617530822754 | BCE Loss: 1.0124948024749756\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.685079574584961 | KNN Loss: 3.668729543685913 | BCE Loss: 1.0163501501083374\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.700257778167725 | KNN Loss: 3.6781423091888428 | BCE Loss: 1.0221155881881714\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.7024054527282715 | KNN Loss: 3.675652503967285 | BCE Loss: 1.0267530679702759\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.7523956298828125 | KNN Loss: 3.7375407218933105 | BCE Loss: 1.0148546695709229\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.738880157470703 | KNN Loss: 3.7209808826446533 | BCE Loss: 1.0178992748260498\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.659655570983887 | KNN Loss: 3.6593856811523438 | BCE Loss: 1.0002696514129639\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.733838081359863 | KNN Loss: 3.6778674125671387 | BCE Loss: 1.0559706687927246\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.729549407958984 | KNN Loss: 3.702162742614746 | BCE Loss: 1.0273866653442383\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.724446773529053 | KNN Loss: 3.7091526985168457 | BCE Loss: 1.0152941942214966\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.7328996658325195 | KNN Loss: 3.716991424560547 | BCE Loss: 1.0159083604812622\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.708643436431885 | KNN Loss: 3.6838138103485107 | BCE Loss: 1.024829626083374\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.7284345626831055 | KNN Loss: 3.68989634513855 | BCE Loss: 1.0385384559631348\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.695874214172363 | KNN Loss: 3.691901206970215 | BCE Loss: 1.0039727687835693\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.680882930755615 | KNN Loss: 3.6832587718963623 | BCE Loss: 0.9976240396499634\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.732846260070801 | KNN Loss: 3.715203046798706 | BCE Loss: 1.0176433324813843\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.750790119171143 | KNN Loss: 3.7355775833129883 | BCE Loss: 1.0152125358581543\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.73952579498291 | KNN Loss: 3.69307279586792 | BCE Loss: 1.0464531183242798\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.724991798400879 | KNN Loss: 3.691568613052368 | BCE Loss: 1.0334230661392212\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.707198143005371 | KNN Loss: 3.690647602081299 | BCE Loss: 1.0165507793426514\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.687131404876709 | KNN Loss: 3.6652817726135254 | BCE Loss: 1.0218497514724731\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.724280834197998 | KNN Loss: 3.728875160217285 | BCE Loss: 0.9954056739807129\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.726468086242676 | KNN Loss: 3.7092156410217285 | BCE Loss: 1.0172526836395264\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.663634777069092 | KNN Loss: 3.653933048248291 | BCE Loss: 1.0097017288208008\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.713336944580078 | KNN Loss: 3.702873468399048 | BCE Loss: 1.0104632377624512\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.700282096862793 | KNN Loss: 3.672996997833252 | BCE Loss: 1.0272849798202515\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.767345428466797 | KNN Loss: 3.7305078506469727 | BCE Loss: 1.0368375778198242\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.71392297744751 | KNN Loss: 3.693068265914917 | BCE Loss: 1.0208547115325928\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.7291107177734375 | KNN Loss: 3.6854050159454346 | BCE Loss: 1.0437054634094238\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.700526714324951 | KNN Loss: 3.699838876724243 | BCE Loss: 1.000687837600708\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.749020099639893 | KNN Loss: 3.7221221923828125 | BCE Loss: 1.0268977880477905\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.699200630187988 | KNN Loss: 3.63507080078125 | BCE Loss: 1.0641297101974487\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.660882472991943 | KNN Loss: 3.6682345867156982 | BCE Loss: 0.9926480650901794\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.792114734649658 | KNN Loss: 3.7472281455993652 | BCE Loss: 1.0448864698410034\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.741720199584961 | KNN Loss: 3.70747971534729 | BCE Loss: 1.0342406034469604\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.727564811706543 | KNN Loss: 3.7052149772644043 | BCE Loss: 1.0223495960235596\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.699288368225098 | KNN Loss: 3.682849645614624 | BCE Loss: 1.0164389610290527\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.671003341674805 | KNN Loss: 3.6539499759674072 | BCE Loss: 1.017053246498108\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.679287910461426 | KNN Loss: 3.6908624172210693 | BCE Loss: 0.988425612449646\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.694852828979492 | KNN Loss: 3.6735377311706543 | BCE Loss: 1.021315336227417\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.7300214767456055 | KNN Loss: 3.7262141704559326 | BCE Loss: 1.0038070678710938\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.704834938049316 | KNN Loss: 3.672741174697876 | BCE Loss: 1.0320936441421509\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.674432754516602 | KNN Loss: 3.6534032821655273 | BCE Loss: 1.0210295915603638\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.759716033935547 | KNN Loss: 3.6899423599243164 | BCE Loss: 1.06977379322052\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.690542221069336 | KNN Loss: 3.6863913536071777 | BCE Loss: 1.004150629043579\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.678286552429199 | KNN Loss: 3.64370059967041 | BCE Loss: 1.034585952758789\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.673690319061279 | KNN Loss: 3.656388521194458 | BCE Loss: 1.0173017978668213\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.701900482177734 | KNN Loss: 3.678074359893799 | BCE Loss: 1.0238258838653564\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.732482433319092 | KNN Loss: 3.722595691680908 | BCE Loss: 1.0098867416381836\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.734896183013916 | KNN Loss: 3.716799736022949 | BCE Loss: 1.0180964469909668\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.686394214630127 | KNN Loss: 3.6939151287078857 | BCE Loss: 0.9924792647361755\n",
      "Epoch   405: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.697018623352051 | KNN Loss: 3.667490243911743 | BCE Loss: 1.0295284986495972\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.713553428649902 | KNN Loss: 3.6695291996002197 | BCE Loss: 1.0440242290496826\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.679214954376221 | KNN Loss: 3.660705804824829 | BCE Loss: 1.0185091495513916\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.667733192443848 | KNN Loss: 3.6535110473632812 | BCE Loss: 1.0142221450805664\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.712036609649658 | KNN Loss: 3.7057645320892334 | BCE Loss: 1.0062720775604248\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.728217124938965 | KNN Loss: 3.6970620155334473 | BCE Loss: 1.0311552286148071\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.677515029907227 | KNN Loss: 3.6727724075317383 | BCE Loss: 1.0047425031661987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.733777046203613 | KNN Loss: 3.692228317260742 | BCE Loss: 1.0415486097335815\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.715044975280762 | KNN Loss: 3.677948236465454 | BCE Loss: 1.0370965003967285\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.741888046264648 | KNN Loss: 3.7062532901763916 | BCE Loss: 1.0356345176696777\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.699357509613037 | KNN Loss: 3.6756443977355957 | BCE Loss: 1.0237131118774414\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.725636005401611 | KNN Loss: 3.6912457942962646 | BCE Loss: 1.0343900918960571\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.746334075927734 | KNN Loss: 3.6990106105804443 | BCE Loss: 1.047323226928711\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.717850685119629 | KNN Loss: 3.678654193878174 | BCE Loss: 1.039196252822876\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.73098087310791 | KNN Loss: 3.6944801807403564 | BCE Loss: 1.0365009307861328\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.685969352722168 | KNN Loss: 3.681379556655884 | BCE Loss: 1.0045897960662842\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.698275566101074 | KNN Loss: 3.6881086826324463 | BCE Loss: 1.0101666450500488\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.7344865798950195 | KNN Loss: 3.7080187797546387 | BCE Loss: 1.0264678001403809\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.73848295211792 | KNN Loss: 3.687805414199829 | BCE Loss: 1.0506775379180908\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.74466609954834 | KNN Loss: 3.725792646408081 | BCE Loss: 1.0188733339309692\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.737546920776367 | KNN Loss: 3.7025065422058105 | BCE Loss: 1.0350404977798462\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.7313432693481445 | KNN Loss: 3.679884195327759 | BCE Loss: 1.0514591932296753\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.745216369628906 | KNN Loss: 3.746323347091675 | BCE Loss: 0.9988927841186523\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.704803466796875 | KNN Loss: 3.6790225505828857 | BCE Loss: 1.0257806777954102\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.727334976196289 | KNN Loss: 3.7064244747161865 | BCE Loss: 1.020910382270813\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.707070350646973 | KNN Loss: 3.6922261714935303 | BCE Loss: 1.0148439407348633\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.726921081542969 | KNN Loss: 3.6945247650146484 | BCE Loss: 1.0323965549468994\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.723452568054199 | KNN Loss: 3.676833391189575 | BCE Loss: 1.0466194152832031\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.701997756958008 | KNN Loss: 3.670494318008423 | BCE Loss: 1.0315032005310059\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.75797700881958 | KNN Loss: 3.7207632064819336 | BCE Loss: 1.0372138023376465\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.737735748291016 | KNN Loss: 3.707021713256836 | BCE Loss: 1.0307142734527588\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.747194766998291 | KNN Loss: 3.69411563873291 | BCE Loss: 1.0530792474746704\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.680690288543701 | KNN Loss: 3.671778440475464 | BCE Loss: 1.0089118480682373\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.715970993041992 | KNN Loss: 3.679655075073242 | BCE Loss: 1.0363160371780396\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.778445243835449 | KNN Loss: 3.727365016937256 | BCE Loss: 1.0510804653167725\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.681247711181641 | KNN Loss: 3.6661221981048584 | BCE Loss: 1.0151252746582031\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.72549295425415 | KNN Loss: 3.7074878215789795 | BCE Loss: 1.018005132675171\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.77918815612793 | KNN Loss: 3.712836265563965 | BCE Loss: 1.0663518905639648\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.687448024749756 | KNN Loss: 3.653116226196289 | BCE Loss: 1.0343317985534668\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.720242023468018 | KNN Loss: 3.679783344268799 | BCE Loss: 1.0404587984085083\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.689165115356445 | KNN Loss: 3.6682684421539307 | BCE Loss: 1.020896553993225\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.701760768890381 | KNN Loss: 3.6707067489624023 | BCE Loss: 1.031053900718689\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.697478771209717 | KNN Loss: 3.6809139251708984 | BCE Loss: 1.0165648460388184\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.703360080718994 | KNN Loss: 3.691462993621826 | BCE Loss: 1.0118969678878784\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.763141632080078 | KNN Loss: 3.7366180419921875 | BCE Loss: 1.0265233516693115\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.718843936920166 | KNN Loss: 3.6972837448120117 | BCE Loss: 1.0215601921081543\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.662121772766113 | KNN Loss: 3.659759044647217 | BCE Loss: 1.0023629665374756\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.762211322784424 | KNN Loss: 3.7407639026641846 | BCE Loss: 1.0214474201202393\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.734107971191406 | KNN Loss: 3.6899373531341553 | BCE Loss: 1.0441703796386719\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.757140159606934 | KNN Loss: 3.7085516452789307 | BCE Loss: 1.048588752746582\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.719034671783447 | KNN Loss: 3.7003283500671387 | BCE Loss: 1.0187063217163086\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.704378128051758 | KNN Loss: 3.6796655654907227 | BCE Loss: 1.0247124433517456\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.720466136932373 | KNN Loss: 3.694746494293213 | BCE Loss: 1.0257196426391602\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.700921058654785 | KNN Loss: 3.672689914703369 | BCE Loss: 1.028230905532837\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.742773532867432 | KNN Loss: 3.6823513507843018 | BCE Loss: 1.0604221820831299\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.684324264526367 | KNN Loss: 3.6913397312164307 | BCE Loss: 0.9929843544960022\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.736443042755127 | KNN Loss: 3.6957898139953613 | BCE Loss: 1.0406532287597656\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.650997638702393 | KNN Loss: 3.6529176235198975 | BCE Loss: 0.9980799555778503\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.6450676918029785 | KNN Loss: 3.6443228721618652 | BCE Loss: 1.0007449388504028\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.69869327545166 | KNN Loss: 3.681027889251709 | BCE Loss: 1.0176656246185303\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.7132673263549805 | KNN Loss: 3.6837267875671387 | BCE Loss: 1.029540777206421\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.734919548034668 | KNN Loss: 3.6964993476867676 | BCE Loss: 1.0384202003479004\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.704476356506348 | KNN Loss: 3.6976213455200195 | BCE Loss: 1.0068552494049072\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.691680908203125 | KNN Loss: 3.6721363067626953 | BCE Loss: 1.0195448398590088\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.680089473724365 | KNN Loss: 3.666959047317505 | BCE Loss: 1.0131304264068604\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.732084274291992 | KNN Loss: 3.7208797931671143 | BCE Loss: 1.011204481124878\n",
      "Epoch   416: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.7088212966918945 | KNN Loss: 3.707873582839966 | BCE Loss: 1.0009478330612183\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.744556427001953 | KNN Loss: 3.703437089920044 | BCE Loss: 1.0411194562911987\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.666899681091309 | KNN Loss: 3.679136037826538 | BCE Loss: 0.9877638816833496\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.754404067993164 | KNN Loss: 3.713346481323242 | BCE Loss: 1.0410573482513428\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.701419830322266 | KNN Loss: 3.656675338745117 | BCE Loss: 1.044744610786438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.695993423461914 | KNN Loss: 3.6871416568756104 | BCE Loss: 1.0088517665863037\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.7436747550964355 | KNN Loss: 3.7013444900512695 | BCE Loss: 1.042330265045166\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.745013236999512 | KNN Loss: 3.719170570373535 | BCE Loss: 1.0258427858352661\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.722750663757324 | KNN Loss: 3.6956098079681396 | BCE Loss: 1.0271408557891846\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.711954116821289 | KNN Loss: 3.6852829456329346 | BCE Loss: 1.0266709327697754\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.706849098205566 | KNN Loss: 3.688894033432007 | BCE Loss: 1.0179550647735596\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.733284950256348 | KNN Loss: 3.708181381225586 | BCE Loss: 1.0251034498214722\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.721634864807129 | KNN Loss: 3.7080259323120117 | BCE Loss: 1.0136088132858276\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.694787979125977 | KNN Loss: 3.6615262031555176 | BCE Loss: 1.0332615375518799\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.724222660064697 | KNN Loss: 3.716040849685669 | BCE Loss: 1.0081819295883179\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.780613422393799 | KNN Loss: 3.737617254257202 | BCE Loss: 1.0429960489273071\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.7055511474609375 | KNN Loss: 3.6936495304107666 | BCE Loss: 1.0119014978408813\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.735080718994141 | KNN Loss: 3.7000327110290527 | BCE Loss: 1.035048246383667\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.747583866119385 | KNN Loss: 3.713310956954956 | BCE Loss: 1.0342729091644287\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.747742652893066 | KNN Loss: 3.712101697921753 | BCE Loss: 1.0356409549713135\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.681375026702881 | KNN Loss: 3.6611576080322266 | BCE Loss: 1.0202172994613647\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.683518886566162 | KNN Loss: 3.6845078468322754 | BCE Loss: 0.9990111589431763\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.751846790313721 | KNN Loss: 3.7079849243164062 | BCE Loss: 1.043861985206604\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.675738334655762 | KNN Loss: 3.660555839538574 | BCE Loss: 1.0151827335357666\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.688689231872559 | KNN Loss: 3.6607272624969482 | BCE Loss: 1.0279619693756104\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.685837268829346 | KNN Loss: 3.690889835357666 | BCE Loss: 0.9949473142623901\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.754937171936035 | KNN Loss: 3.7253525257110596 | BCE Loss: 1.029584527015686\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.66516637802124 | KNN Loss: 3.6638245582580566 | BCE Loss: 1.001341700553894\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.721796035766602 | KNN Loss: 3.7130110263824463 | BCE Loss: 1.0087848901748657\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.690400123596191 | KNN Loss: 3.660735845565796 | BCE Loss: 1.0296645164489746\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.714160919189453 | KNN Loss: 3.695448637008667 | BCE Loss: 1.0187122821807861\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.692635536193848 | KNN Loss: 3.6898744106292725 | BCE Loss: 1.0027613639831543\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.745235443115234 | KNN Loss: 3.699625253677368 | BCE Loss: 1.0456100702285767\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.716512680053711 | KNN Loss: 3.70998215675354 | BCE Loss: 1.006530523300171\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.706380367279053 | KNN Loss: 3.6754140853881836 | BCE Loss: 1.0309662818908691\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.685381889343262 | KNN Loss: 3.674499034881592 | BCE Loss: 1.010883092880249\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.743930816650391 | KNN Loss: 3.7500643730163574 | BCE Loss: 0.9938666820526123\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.737685680389404 | KNN Loss: 3.6984832286834717 | BCE Loss: 1.0392025709152222\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.697818756103516 | KNN Loss: 3.6822874546051025 | BCE Loss: 1.0155314207077026\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.728899955749512 | KNN Loss: 3.7246570587158203 | BCE Loss: 1.0042428970336914\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.739099025726318 | KNN Loss: 3.7038333415985107 | BCE Loss: 1.035265564918518\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.742751598358154 | KNN Loss: 3.684414863586426 | BCE Loss: 1.0583367347717285\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.680934429168701 | KNN Loss: 3.6718151569366455 | BCE Loss: 1.0091192722320557\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.7290496826171875 | KNN Loss: 3.68255352973938 | BCE Loss: 1.0464961528778076\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.704803466796875 | KNN Loss: 3.6963565349578857 | BCE Loss: 1.0084469318389893\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.749813079833984 | KNN Loss: 3.722656726837158 | BCE Loss: 1.0271563529968262\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.769911289215088 | KNN Loss: 3.7305586338043213 | BCE Loss: 1.039352536201477\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.706431865692139 | KNN Loss: 3.6799416542053223 | BCE Loss: 1.0264902114868164\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.755362033843994 | KNN Loss: 3.7306535243988037 | BCE Loss: 1.0247083902359009\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.721468448638916 | KNN Loss: 3.684612512588501 | BCE Loss: 1.036855936050415\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.745143413543701 | KNN Loss: 3.712489128112793 | BCE Loss: 1.0326542854309082\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.7199225425720215 | KNN Loss: 3.6970787048339844 | BCE Loss: 1.0228439569473267\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.719648361206055 | KNN Loss: 3.6907782554626465 | BCE Loss: 1.0288703441619873\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.718257904052734 | KNN Loss: 3.7154653072357178 | BCE Loss: 1.0027928352355957\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.772231578826904 | KNN Loss: 3.7453858852386475 | BCE Loss: 1.0268456935882568\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.719712734222412 | KNN Loss: 3.6969306468963623 | BCE Loss: 1.0227819681167603\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.749006271362305 | KNN Loss: 3.718662977218628 | BCE Loss: 1.0303434133529663\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.714562892913818 | KNN Loss: 3.6900460720062256 | BCE Loss: 1.0245168209075928\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.719087600708008 | KNN Loss: 3.687965154647827 | BCE Loss: 1.0311222076416016\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.722480773925781 | KNN Loss: 3.6834683418273926 | BCE Loss: 1.0390125513076782\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.744592666625977 | KNN Loss: 3.711427688598633 | BCE Loss: 1.0331648588180542\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.726250648498535 | KNN Loss: 3.6928212642669678 | BCE Loss: 1.0334296226501465\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.714230060577393 | KNN Loss: 3.703092575073242 | BCE Loss: 1.0111373662948608\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.769100189208984 | KNN Loss: 3.69574236869812 | BCE Loss: 1.0733577013015747\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.731431007385254 | KNN Loss: 3.6813759803771973 | BCE Loss: 1.0500550270080566\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.686532974243164 | KNN Loss: 3.6679179668426514 | BCE Loss: 1.0186150074005127\n",
      "Epoch   427: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.713252067565918 | KNN Loss: 3.699268341064453 | BCE Loss: 1.013983964920044\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.739260196685791 | KNN Loss: 3.69962215423584 | BCE Loss: 1.0396380424499512\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.736845970153809 | KNN Loss: 3.745471239089966 | BCE Loss: 0.9913747906684875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.6972222328186035 | KNN Loss: 3.671234130859375 | BCE Loss: 1.0259881019592285\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.695345878601074 | KNN Loss: 3.669992208480835 | BCE Loss: 1.0253534317016602\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.705785751342773 | KNN Loss: 3.69469952583313 | BCE Loss: 1.0110859870910645\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.731802940368652 | KNN Loss: 3.6968953609466553 | BCE Loss: 1.034907579421997\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.780817031860352 | KNN Loss: 3.7310500144958496 | BCE Loss: 1.049767017364502\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.679352283477783 | KNN Loss: 3.636094093322754 | BCE Loss: 1.0432581901550293\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.761282444000244 | KNN Loss: 3.7173852920532227 | BCE Loss: 1.043897032737732\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.656574249267578 | KNN Loss: 3.6647729873657227 | BCE Loss: 0.9918015003204346\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.726189613342285 | KNN Loss: 3.6837029457092285 | BCE Loss: 1.0424869060516357\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.712986946105957 | KNN Loss: 3.682765007019043 | BCE Loss: 1.030221700668335\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.762206077575684 | KNN Loss: 3.6909871101379395 | BCE Loss: 1.0712188482284546\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.702930927276611 | KNN Loss: 3.667290687561035 | BCE Loss: 1.0356401205062866\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.742946624755859 | KNN Loss: 3.691866874694824 | BCE Loss: 1.051079511642456\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.714922904968262 | KNN Loss: 3.713379383087158 | BCE Loss: 1.0015432834625244\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.765157699584961 | KNN Loss: 3.7398834228515625 | BCE Loss: 1.0252745151519775\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.783246994018555 | KNN Loss: 3.7425854206085205 | BCE Loss: 1.040661334991455\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.76089334487915 | KNN Loss: 3.716395139694214 | BCE Loss: 1.044498085975647\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.714860439300537 | KNN Loss: 3.669355630874634 | BCE Loss: 1.0455048084259033\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.705811023712158 | KNN Loss: 3.6866469383239746 | BCE Loss: 1.0191642045974731\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.735241889953613 | KNN Loss: 3.7145538330078125 | BCE Loss: 1.0206880569458008\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.69191837310791 | KNN Loss: 3.6726276874542236 | BCE Loss: 1.0192906856536865\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.709611892700195 | KNN Loss: 3.694485664367676 | BCE Loss: 1.0151259899139404\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.756046772003174 | KNN Loss: 3.6983771324157715 | BCE Loss: 1.057669758796692\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.7246294021606445 | KNN Loss: 3.705361843109131 | BCE Loss: 1.0192674398422241\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.670345783233643 | KNN Loss: 3.652987480163574 | BCE Loss: 1.0173583030700684\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.69340181350708 | KNN Loss: 3.6637489795684814 | BCE Loss: 1.0296528339385986\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.720004081726074 | KNN Loss: 3.665818452835083 | BCE Loss: 1.0541857481002808\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.70294189453125 | KNN Loss: 3.681246757507324 | BCE Loss: 1.0216948986053467\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.7408766746521 | KNN Loss: 3.699732780456543 | BCE Loss: 1.041143774986267\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.674077033996582 | KNN Loss: 3.6839089393615723 | BCE Loss: 0.9901678562164307\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.74332857131958 | KNN Loss: 3.725886583328247 | BCE Loss: 1.017441987991333\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.714797496795654 | KNN Loss: 3.693410634994507 | BCE Loss: 1.0213868618011475\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.683854579925537 | KNN Loss: 3.652304172515869 | BCE Loss: 1.0315505266189575\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.673384666442871 | KNN Loss: 3.6461730003356934 | BCE Loss: 1.0272114276885986\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.753789901733398 | KNN Loss: 3.7434847354888916 | BCE Loss: 1.0103049278259277\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.71812105178833 | KNN Loss: 3.7036101818084717 | BCE Loss: 1.0145108699798584\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.7585296630859375 | KNN Loss: 3.684584140777588 | BCE Loss: 1.0739452838897705\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.754160404205322 | KNN Loss: 3.7161178588867188 | BCE Loss: 1.038042426109314\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.728204727172852 | KNN Loss: 3.700723648071289 | BCE Loss: 1.0274813175201416\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.718982696533203 | KNN Loss: 3.699758291244507 | BCE Loss: 1.0192242860794067\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.71783447265625 | KNN Loss: 3.6578378677368164 | BCE Loss: 1.0599968433380127\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.708187580108643 | KNN Loss: 3.68391752243042 | BCE Loss: 1.0242700576782227\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.74666166305542 | KNN Loss: 3.712759494781494 | BCE Loss: 1.0339021682739258\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.73732852935791 | KNN Loss: 3.710383176803589 | BCE Loss: 1.0269453525543213\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.740416526794434 | KNN Loss: 3.692739725112915 | BCE Loss: 1.0476770401000977\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.700969696044922 | KNN Loss: 3.677031993865967 | BCE Loss: 1.023937702178955\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.704693794250488 | KNN Loss: 3.6897196769714355 | BCE Loss: 1.0149739980697632\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.678576469421387 | KNN Loss: 3.653719425201416 | BCE Loss: 1.0248568058013916\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.791093826293945 | KNN Loss: 3.7591137886047363 | BCE Loss: 1.0319799184799194\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.715156555175781 | KNN Loss: 3.695990562438965 | BCE Loss: 1.0191657543182373\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.693975448608398 | KNN Loss: 3.681584119796753 | BCE Loss: 1.0123913288116455\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.695545673370361 | KNN Loss: 3.6740550994873047 | BCE Loss: 1.021490454673767\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.734808921813965 | KNN Loss: 3.7183313369750977 | BCE Loss: 1.0164777040481567\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.715127468109131 | KNN Loss: 3.692133665084839 | BCE Loss: 1.0229939222335815\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.703690528869629 | KNN Loss: 3.6816861629486084 | BCE Loss: 1.0220043659210205\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.678459644317627 | KNN Loss: 3.655275821685791 | BCE Loss: 1.023183822631836\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.791795253753662 | KNN Loss: 3.7214574813842773 | BCE Loss: 1.0703377723693848\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.757321357727051 | KNN Loss: 3.7080934047698975 | BCE Loss: 1.0492279529571533\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.683877944946289 | KNN Loss: 3.678762912750244 | BCE Loss: 1.0051147937774658\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.665487289428711 | KNN Loss: 3.654109239578247 | BCE Loss: 1.0113779306411743\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.723844051361084 | KNN Loss: 3.720405101776123 | BCE Loss: 1.0034388303756714\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.693728446960449 | KNN Loss: 3.7026214599609375 | BCE Loss: 0.9911069273948669\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.6940155029296875 | KNN Loss: 3.678190231323242 | BCE Loss: 1.0158252716064453\n",
      "Epoch   438: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.6985063552856445 | KNN Loss: 3.6836488246917725 | BCE Loss: 1.0148577690124512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.6569976806640625 | KNN Loss: 3.6538920402526855 | BCE Loss: 1.003105640411377\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.7059736251831055 | KNN Loss: 3.6790621280670166 | BCE Loss: 1.0269112586975098\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.715023517608643 | KNN Loss: 3.6542091369628906 | BCE Loss: 1.060814380645752\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.751033306121826 | KNN Loss: 3.719531774520874 | BCE Loss: 1.0315016508102417\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.732845783233643 | KNN Loss: 3.6994423866271973 | BCE Loss: 1.0334035158157349\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.72944974899292 | KNN Loss: 3.699167013168335 | BCE Loss: 1.030282735824585\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.774263381958008 | KNN Loss: 3.7340073585510254 | BCE Loss: 1.0402560234069824\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.7733049392700195 | KNN Loss: 3.7429163455963135 | BCE Loss: 1.030388355255127\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.771879196166992 | KNN Loss: 3.733809232711792 | BCE Loss: 1.0380698442459106\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.721040725708008 | KNN Loss: 3.683497428894043 | BCE Loss: 1.0375430583953857\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.69603157043457 | KNN Loss: 3.66959285736084 | BCE Loss: 1.0264389514923096\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.684253215789795 | KNN Loss: 3.686579704284668 | BCE Loss: 0.9976733326911926\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.709504127502441 | KNN Loss: 3.696366310119629 | BCE Loss: 1.0131375789642334\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.704084396362305 | KNN Loss: 3.687342882156372 | BCE Loss: 1.016741394996643\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.698765754699707 | KNN Loss: 3.6639833450317383 | BCE Loss: 1.0347821712493896\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.719058513641357 | KNN Loss: 3.6899900436401367 | BCE Loss: 1.0290684700012207\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.7847466468811035 | KNN Loss: 3.736145257949829 | BCE Loss: 1.0486012697219849\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.70247220993042 | KNN Loss: 3.658262014389038 | BCE Loss: 1.0442100763320923\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.739845275878906 | KNN Loss: 3.7097063064575195 | BCE Loss: 1.0301392078399658\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.737174987792969 | KNN Loss: 3.7124695777893066 | BCE Loss: 1.0247056484222412\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.671091556549072 | KNN Loss: 3.6729846000671387 | BCE Loss: 0.9981070756912231\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.723089694976807 | KNN Loss: 3.681997537612915 | BCE Loss: 1.041092038154602\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.721614837646484 | KNN Loss: 3.6956422328948975 | BCE Loss: 1.0259723663330078\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.730920791625977 | KNN Loss: 3.708998203277588 | BCE Loss: 1.0219225883483887\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.7420549392700195 | KNN Loss: 3.7170965671539307 | BCE Loss: 1.0249582529067993\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.709258079528809 | KNN Loss: 3.6629135608673096 | BCE Loss: 1.046344518661499\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.730932235717773 | KNN Loss: 3.715482234954834 | BCE Loss: 1.015450119972229\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.720762252807617 | KNN Loss: 3.705767869949341 | BCE Loss: 1.0149941444396973\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.755665302276611 | KNN Loss: 3.695880174636841 | BCE Loss: 1.0597851276397705\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.703514099121094 | KNN Loss: 3.660578727722168 | BCE Loss: 1.0429351329803467\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.708958625793457 | KNN Loss: 3.6719584465026855 | BCE Loss: 1.0369999408721924\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.712101459503174 | KNN Loss: 3.666067600250244 | BCE Loss: 1.0460338592529297\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.683135032653809 | KNN Loss: 3.6816155910491943 | BCE Loss: 1.0015194416046143\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.731199264526367 | KNN Loss: 3.705491304397583 | BCE Loss: 1.0257079601287842\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.679981231689453 | KNN Loss: 3.654533624649048 | BCE Loss: 1.0254473686218262\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.708478927612305 | KNN Loss: 3.690753221511841 | BCE Loss: 1.0177257061004639\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.690976619720459 | KNN Loss: 3.667544364929199 | BCE Loss: 1.0234323740005493\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.724155902862549 | KNN Loss: 3.696833372116089 | BCE Loss: 1.0273224115371704\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.687455177307129 | KNN Loss: 3.6601035594940186 | BCE Loss: 1.0273517370224\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.704188823699951 | KNN Loss: 3.6820919513702393 | BCE Loss: 1.0220969915390015\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.710626602172852 | KNN Loss: 3.673807144165039 | BCE Loss: 1.0368192195892334\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.678626537322998 | KNN Loss: 3.6479904651641846 | BCE Loss: 1.030636191368103\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.745907783508301 | KNN Loss: 3.711442470550537 | BCE Loss: 1.0344653129577637\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.691450119018555 | KNN Loss: 3.6976845264434814 | BCE Loss: 0.9937658309936523\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.705810070037842 | KNN Loss: 3.6722958087921143 | BCE Loss: 1.033514142036438\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.700430870056152 | KNN Loss: 3.6872966289520264 | BCE Loss: 1.0131341218948364\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.698436260223389 | KNN Loss: 3.6946117877960205 | BCE Loss: 1.0038244724273682\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.699523448944092 | KNN Loss: 3.6859538555145264 | BCE Loss: 1.0135694742202759\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.754169464111328 | KNN Loss: 3.74416184425354 | BCE Loss: 1.0100078582763672\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.69349479675293 | KNN Loss: 3.679257869720459 | BCE Loss: 1.0142369270324707\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.731741905212402 | KNN Loss: 3.7045440673828125 | BCE Loss: 1.0271975994110107\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.750214576721191 | KNN Loss: 3.7390270233154297 | BCE Loss: 1.0111873149871826\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.740513801574707 | KNN Loss: 3.7136197090148926 | BCE Loss: 1.0268943309783936\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.721046447753906 | KNN Loss: 3.6934144496917725 | BCE Loss: 1.0276317596435547\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.708313941955566 | KNN Loss: 3.7019364833831787 | BCE Loss: 1.0063772201538086\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.80119514465332 | KNN Loss: 3.7544748783111572 | BCE Loss: 1.0467205047607422\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.729372978210449 | KNN Loss: 3.722249984741211 | BCE Loss: 1.0071227550506592\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.69675874710083 | KNN Loss: 3.659850597381592 | BCE Loss: 1.0369081497192383\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.708216667175293 | KNN Loss: 3.6863200664520264 | BCE Loss: 1.0218963623046875\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.754777908325195 | KNN Loss: 3.713512420654297 | BCE Loss: 1.0412652492523193\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.725251197814941 | KNN Loss: 3.6853420734405518 | BCE Loss: 1.0399090051651\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.769761085510254 | KNN Loss: 3.710446357727051 | BCE Loss: 1.059314489364624\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.721742153167725 | KNN Loss: 3.706112861633301 | BCE Loss: 1.0156292915344238\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.712360858917236 | KNN Loss: 3.666696071624756 | BCE Loss: 1.045664668083191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.692770481109619 | KNN Loss: 3.685917615890503 | BCE Loss: 1.0068527460098267\n",
      "Epoch   449: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.7409772872924805 | KNN Loss: 3.6936752796173096 | BCE Loss: 1.04730224609375\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.7634100914001465 | KNN Loss: 3.726328134536743 | BCE Loss: 1.0370819568634033\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.75009822845459 | KNN Loss: 3.7112393379211426 | BCE Loss: 1.0388586521148682\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.666780471801758 | KNN Loss: 3.674314498901367 | BCE Loss: 0.9924658536911011\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.720156192779541 | KNN Loss: 3.6891753673553467 | BCE Loss: 1.0309808254241943\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.714337348937988 | KNN Loss: 3.6602962017059326 | BCE Loss: 1.0540409088134766\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.704272270202637 | KNN Loss: 3.692359685897827 | BCE Loss: 1.0119125843048096\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.734598636627197 | KNN Loss: 3.68157696723938 | BCE Loss: 1.0530216693878174\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.7413434982299805 | KNN Loss: 3.6977994441986084 | BCE Loss: 1.0435441732406616\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.693957328796387 | KNN Loss: 3.6802918910980225 | BCE Loss: 1.0136654376983643\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.6921234130859375 | KNN Loss: 3.6652398109436035 | BCE Loss: 1.0268833637237549\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.680673599243164 | KNN Loss: 3.690023183822632 | BCE Loss: 0.9906506538391113\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.757287502288818 | KNN Loss: 3.7128143310546875 | BCE Loss: 1.0444731712341309\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.69251012802124 | KNN Loss: 3.659119129180908 | BCE Loss: 1.033390998840332\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.724666595458984 | KNN Loss: 3.6805787086486816 | BCE Loss: 1.0440876483917236\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.732817649841309 | KNN Loss: 3.6878604888916016 | BCE Loss: 1.044956922531128\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.726531028747559 | KNN Loss: 3.7108819484710693 | BCE Loss: 1.0156490802764893\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.687500476837158 | KNN Loss: 3.6948068141937256 | BCE Loss: 0.9926935434341431\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.715862274169922 | KNN Loss: 3.6944000720977783 | BCE Loss: 1.0214624404907227\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.693348407745361 | KNN Loss: 3.69083833694458 | BCE Loss: 1.0025100708007812\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.7475905418396 | KNN Loss: 3.7009873390197754 | BCE Loss: 1.0466030836105347\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.73979377746582 | KNN Loss: 3.710036277770996 | BCE Loss: 1.0297577381134033\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.703698635101318 | KNN Loss: 3.665241241455078 | BCE Loss: 1.0384575128555298\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.776108741760254 | KNN Loss: 3.7461705207824707 | BCE Loss: 1.0299384593963623\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.684987545013428 | KNN Loss: 3.676766872406006 | BCE Loss: 1.0082206726074219\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.763481140136719 | KNN Loss: 3.7370078563690186 | BCE Loss: 1.026473045349121\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.65031623840332 | KNN Loss: 3.6607484817504883 | BCE Loss: 0.989567756652832\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.753250598907471 | KNN Loss: 3.7294769287109375 | BCE Loss: 1.0237736701965332\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.7294816970825195 | KNN Loss: 3.6835238933563232 | BCE Loss: 1.0459579229354858\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.742056846618652 | KNN Loss: 3.7173843383789062 | BCE Loss: 1.0246727466583252\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.723392009735107 | KNN Loss: 3.700469493865967 | BCE Loss: 1.0229225158691406\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.759922027587891 | KNN Loss: 3.7206101417541504 | BCE Loss: 1.0393118858337402\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.737741470336914 | KNN Loss: 3.7002322673797607 | BCE Loss: 1.0375093221664429\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.753692150115967 | KNN Loss: 3.727821111679077 | BCE Loss: 1.0258710384368896\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.705471515655518 | KNN Loss: 3.702518939971924 | BCE Loss: 1.0029526948928833\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.702279090881348 | KNN Loss: 3.671782970428467 | BCE Loss: 1.0304958820343018\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.706418514251709 | KNN Loss: 3.677013397216797 | BCE Loss: 1.0294052362442017\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.7373223304748535 | KNN Loss: 3.708981990814209 | BCE Loss: 1.0283403396606445\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.808195114135742 | KNN Loss: 3.767441987991333 | BCE Loss: 1.0407533645629883\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.725934028625488 | KNN Loss: 3.6859772205352783 | BCE Loss: 1.039957046508789\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.776715278625488 | KNN Loss: 3.746793746948242 | BCE Loss: 1.029921531677246\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.710042476654053 | KNN Loss: 3.6925973892211914 | BCE Loss: 1.0174449682235718\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.704591751098633 | KNN Loss: 3.673361301422119 | BCE Loss: 1.0312306880950928\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.748166084289551 | KNN Loss: 3.70833158493042 | BCE Loss: 1.0398346185684204\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.6935529708862305 | KNN Loss: 3.6642889976501465 | BCE Loss: 1.0292640924453735\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.715190410614014 | KNN Loss: 3.703068971633911 | BCE Loss: 1.0121214389801025\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.705648899078369 | KNN Loss: 3.666900873184204 | BCE Loss: 1.0387481451034546\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.71000862121582 | KNN Loss: 3.6966757774353027 | BCE Loss: 1.0133326053619385\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.686685085296631 | KNN Loss: 3.692539691925049 | BCE Loss: 0.9941455125808716\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.708696365356445 | KNN Loss: 3.684943199157715 | BCE Loss: 1.0237531661987305\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.662993431091309 | KNN Loss: 3.643367290496826 | BCE Loss: 1.0196260213851929\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.731611251831055 | KNN Loss: 3.6661577224731445 | BCE Loss: 1.0654535293579102\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.726833343505859 | KNN Loss: 3.698042869567871 | BCE Loss: 1.0287907123565674\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.684107780456543 | KNN Loss: 3.6849794387817383 | BCE Loss: 0.9991285800933838\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.748712539672852 | KNN Loss: 3.7130343914031982 | BCE Loss: 1.0356783866882324\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.760775089263916 | KNN Loss: 3.7175002098083496 | BCE Loss: 1.0432748794555664\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.684528350830078 | KNN Loss: 3.665541648864746 | BCE Loss: 1.0189869403839111\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.660165786743164 | KNN Loss: 3.65544056892395 | BCE Loss: 1.004725456237793\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.7255988121032715 | KNN Loss: 3.6914453506469727 | BCE Loss: 1.0341534614562988\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.707146644592285 | KNN Loss: 3.671023368835449 | BCE Loss: 1.036123275756836\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.7292256355285645 | KNN Loss: 3.694127082824707 | BCE Loss: 1.0350985527038574\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.698212146759033 | KNN Loss: 3.70038104057312 | BCE Loss: 0.9978309273719788\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.684928894042969 | KNN Loss: 3.680023431777954 | BCE Loss: 1.0049054622650146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.792212963104248 | KNN Loss: 3.7266476154327393 | BCE Loss: 1.0655654668807983\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.684537410736084 | KNN Loss: 3.6626217365264893 | BCE Loss: 1.0219155550003052\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.727397441864014 | KNN Loss: 3.6909310817718506 | BCE Loss: 1.036466360092163\n",
      "Epoch   460: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.748035430908203 | KNN Loss: 3.686722993850708 | BCE Loss: 1.0613125562667847\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.705294609069824 | KNN Loss: 3.6800954341888428 | BCE Loss: 1.0251991748809814\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.7114105224609375 | KNN Loss: 3.686742067337036 | BCE Loss: 1.0246682167053223\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.734474182128906 | KNN Loss: 3.681114912033081 | BCE Loss: 1.0533595085144043\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.76833438873291 | KNN Loss: 3.7519357204437256 | BCE Loss: 1.0163986682891846\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.774051666259766 | KNN Loss: 3.7083370685577393 | BCE Loss: 1.0657148361206055\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.745721817016602 | KNN Loss: 3.6927905082702637 | BCE Loss: 1.0529310703277588\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.682065963745117 | KNN Loss: 3.658470392227173 | BCE Loss: 1.0235955715179443\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.757991313934326 | KNN Loss: 3.7146472930908203 | BCE Loss: 1.0433439016342163\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.696566581726074 | KNN Loss: 3.689202070236206 | BCE Loss: 1.0073646306991577\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.672947883605957 | KNN Loss: 3.657601833343506 | BCE Loss: 1.0153462886810303\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.742177486419678 | KNN Loss: 3.7239086627960205 | BCE Loss: 1.0182687044143677\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.6732988357543945 | KNN Loss: 3.6486003398895264 | BCE Loss: 1.0246987342834473\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.663321018218994 | KNN Loss: 3.6576554775238037 | BCE Loss: 1.0056654214859009\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.663652420043945 | KNN Loss: 3.663278341293335 | BCE Loss: 1.0003743171691895\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.691188335418701 | KNN Loss: 3.6882364749908447 | BCE Loss: 1.002951979637146\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.71358585357666 | KNN Loss: 3.679927349090576 | BCE Loss: 1.033658742904663\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.706544399261475 | KNN Loss: 3.6769914627075195 | BCE Loss: 1.0295528173446655\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.685484409332275 | KNN Loss: 3.6742453575134277 | BCE Loss: 1.0112391710281372\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.704624176025391 | KNN Loss: 3.667614221572876 | BCE Loss: 1.037009835243225\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.711998462677002 | KNN Loss: 3.6882402896881104 | BCE Loss: 1.0237582921981812\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.706428527832031 | KNN Loss: 3.6830313205718994 | BCE Loss: 1.0233969688415527\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.707429885864258 | KNN Loss: 3.676133871078491 | BCE Loss: 1.0312961339950562\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.685125350952148 | KNN Loss: 3.685394525527954 | BCE Loss: 0.9997308254241943\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.679937839508057 | KNN Loss: 3.6724319458007812 | BCE Loss: 1.007506012916565\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.674863338470459 | KNN Loss: 3.677722454071045 | BCE Loss: 0.9971407055854797\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.702004432678223 | KNN Loss: 3.666475534439087 | BCE Loss: 1.0355288982391357\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.668877601623535 | KNN Loss: 3.6665682792663574 | BCE Loss: 1.0023093223571777\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.713776111602783 | KNN Loss: 3.66756010055542 | BCE Loss: 1.0462158918380737\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.7454633712768555 | KNN Loss: 3.7119295597076416 | BCE Loss: 1.0335335731506348\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.743515491485596 | KNN Loss: 3.741758346557617 | BCE Loss: 1.0017571449279785\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.69350528717041 | KNN Loss: 3.66634464263916 | BCE Loss: 1.027160882949829\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.746994972229004 | KNN Loss: 3.709153890609741 | BCE Loss: 1.0378413200378418\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.806514739990234 | KNN Loss: 3.71085524559021 | BCE Loss: 1.0956594944000244\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.692159652709961 | KNN Loss: 3.6549766063690186 | BCE Loss: 1.0371832847595215\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.724748134613037 | KNN Loss: 3.6693897247314453 | BCE Loss: 1.0553584098815918\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.7777838706970215 | KNN Loss: 3.7324330806732178 | BCE Loss: 1.0453506708145142\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.73796272277832 | KNN Loss: 3.7125093936920166 | BCE Loss: 1.0254533290863037\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.733752250671387 | KNN Loss: 3.7304956912994385 | BCE Loss: 1.0032563209533691\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.684532165527344 | KNN Loss: 3.6539134979248047 | BCE Loss: 1.030618667602539\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.750200271606445 | KNN Loss: 3.7095110416412354 | BCE Loss: 1.04068922996521\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.725115776062012 | KNN Loss: 3.713474988937378 | BCE Loss: 1.011641025543213\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.675405979156494 | KNN Loss: 3.702254295349121 | BCE Loss: 0.973151683807373\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.697481155395508 | KNN Loss: 3.6720824241638184 | BCE Loss: 1.0253984928131104\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.7224602699279785 | KNN Loss: 3.698406934738159 | BCE Loss: 1.0240533351898193\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.728717803955078 | KNN Loss: 3.691218137741089 | BCE Loss: 1.0374996662139893\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.754243850708008 | KNN Loss: 3.699927568435669 | BCE Loss: 1.0543160438537598\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.711200714111328 | KNN Loss: 3.6626031398773193 | BCE Loss: 1.0485975742340088\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.731606483459473 | KNN Loss: 3.682128667831421 | BCE Loss: 1.0494776964187622\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.722097396850586 | KNN Loss: 3.674722671508789 | BCE Loss: 1.047374963760376\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.692869186401367 | KNN Loss: 3.668869733810425 | BCE Loss: 1.0239992141723633\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.67581033706665 | KNN Loss: 3.6591689586639404 | BCE Loss: 1.0166412591934204\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.770041465759277 | KNN Loss: 3.6994829177856445 | BCE Loss: 1.0705585479736328\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.73126220703125 | KNN Loss: 3.6836636066436768 | BCE Loss: 1.0475986003875732\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.749917507171631 | KNN Loss: 3.7152817249298096 | BCE Loss: 1.0346356630325317\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.692446708679199 | KNN Loss: 3.645435333251953 | BCE Loss: 1.0470116138458252\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.709796905517578 | KNN Loss: 3.675577163696289 | BCE Loss: 1.0342199802398682\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.743644714355469 | KNN Loss: 3.702587127685547 | BCE Loss: 1.041057825088501\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.693952560424805 | KNN Loss: 3.6800174713134766 | BCE Loss: 1.0139353275299072\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.763706207275391 | KNN Loss: 3.7224137783050537 | BCE Loss: 1.041292667388916\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.7769365310668945 | KNN Loss: 3.7174694538116455 | BCE Loss: 1.059467077255249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.760499000549316 | KNN Loss: 3.741896629333496 | BCE Loss: 1.0186023712158203\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.7106242179870605 | KNN Loss: 3.6862051486968994 | BCE Loss: 1.0244190692901611\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.741048812866211 | KNN Loss: 3.709479808807373 | BCE Loss: 1.031569004058838\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.735234260559082 | KNN Loss: 3.6780471801757812 | BCE Loss: 1.0571868419647217\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.744417190551758 | KNN Loss: 3.7214269638061523 | BCE Loss: 1.0229899883270264\n",
      "Epoch   471: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.764540672302246 | KNN Loss: 3.7196900844573975 | BCE Loss: 1.0448505878448486\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.7561492919921875 | KNN Loss: 3.7200989723205566 | BCE Loss: 1.0360503196716309\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.737384796142578 | KNN Loss: 3.703965902328491 | BCE Loss: 1.0334187746047974\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.739398002624512 | KNN Loss: 3.7317466735839844 | BCE Loss: 1.007651448249817\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.752758979797363 | KNN Loss: 3.693493366241455 | BCE Loss: 1.0592656135559082\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.7410688400268555 | KNN Loss: 3.732644557952881 | BCE Loss: 1.0084240436553955\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.679426193237305 | KNN Loss: 3.682185649871826 | BCE Loss: 0.9972405433654785\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.759125232696533 | KNN Loss: 3.75095272064209 | BCE Loss: 1.0081725120544434\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.743124008178711 | KNN Loss: 3.710552930831909 | BCE Loss: 1.0325709581375122\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.72147274017334 | KNN Loss: 3.7018399238586426 | BCE Loss: 1.0196326971054077\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.710822105407715 | KNN Loss: 3.68609881401062 | BCE Loss: 1.0247230529785156\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.740813255310059 | KNN Loss: 3.690727949142456 | BCE Loss: 1.0500853061676025\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.734605312347412 | KNN Loss: 3.6696715354919434 | BCE Loss: 1.0649337768554688\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.7654242515563965 | KNN Loss: 3.7201309204101562 | BCE Loss: 1.0452932119369507\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.700492858886719 | KNN Loss: 3.68192195892334 | BCE Loss: 1.0185706615447998\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.716027736663818 | KNN Loss: 3.6944382190704346 | BCE Loss: 1.0215893983840942\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.736332893371582 | KNN Loss: 3.6805763244628906 | BCE Loss: 1.0557563304901123\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.743923187255859 | KNN Loss: 3.7146780490875244 | BCE Loss: 1.029245376586914\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.768006801605225 | KNN Loss: 3.721851348876953 | BCE Loss: 1.0461554527282715\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.723056793212891 | KNN Loss: 3.7024176120758057 | BCE Loss: 1.0206393003463745\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.689897537231445 | KNN Loss: 3.6723785400390625 | BCE Loss: 1.0175188779830933\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.709191799163818 | KNN Loss: 3.6865475177764893 | BCE Loss: 1.0226441621780396\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.76972770690918 | KNN Loss: 3.7593088150024414 | BCE Loss: 1.0104187726974487\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.676729679107666 | KNN Loss: 3.6612727642059326 | BCE Loss: 1.0154569149017334\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.748213291168213 | KNN Loss: 3.6998207569122314 | BCE Loss: 1.048392415046692\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.703423500061035 | KNN Loss: 3.652435541152954 | BCE Loss: 1.0509881973266602\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.696890354156494 | KNN Loss: 3.6830432415008545 | BCE Loss: 1.0138472318649292\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.726621627807617 | KNN Loss: 3.6992781162261963 | BCE Loss: 1.0273433923721313\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.72566556930542 | KNN Loss: 3.699005365371704 | BCE Loss: 1.0266602039337158\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.72786808013916 | KNN Loss: 3.697725296020508 | BCE Loss: 1.0301425457000732\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.787103652954102 | KNN Loss: 3.737717866897583 | BCE Loss: 1.0493855476379395\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.740633487701416 | KNN Loss: 3.701172113418579 | BCE Loss: 1.0394612550735474\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.746828079223633 | KNN Loss: 3.7127630710601807 | BCE Loss: 1.0340652465820312\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.707756996154785 | KNN Loss: 3.691448926925659 | BCE Loss: 1.016308307647705\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.715715408325195 | KNN Loss: 3.705871820449829 | BCE Loss: 1.0098435878753662\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.7334513664245605 | KNN Loss: 3.698620557785034 | BCE Loss: 1.0348308086395264\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.703062057495117 | KNN Loss: 3.700019598007202 | BCE Loss: 1.0030423402786255\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.690603733062744 | KNN Loss: 3.6687636375427246 | BCE Loss: 1.0218400955200195\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.720900535583496 | KNN Loss: 3.7105658054351807 | BCE Loss: 1.0103344917297363\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.701461315155029 | KNN Loss: 3.685704469680786 | BCE Loss: 1.0157568454742432\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.719476699829102 | KNN Loss: 3.6856555938720703 | BCE Loss: 1.0338213443756104\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.748931407928467 | KNN Loss: 3.7064120769500732 | BCE Loss: 1.0425193309783936\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.688383102416992 | KNN Loss: 3.6895995140075684 | BCE Loss: 0.9987833499908447\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.730020046234131 | KNN Loss: 3.68393874168396 | BCE Loss: 1.046081304550171\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.728188514709473 | KNN Loss: 3.720250129699707 | BCE Loss: 1.0079386234283447\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.687924861907959 | KNN Loss: 3.6961684226989746 | BCE Loss: 0.9917564988136292\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.6881561279296875 | KNN Loss: 3.64579176902771 | BCE Loss: 1.0423643589019775\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.731978416442871 | KNN Loss: 3.6980836391448975 | BCE Loss: 1.0338945388793945\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.713901996612549 | KNN Loss: 3.717008352279663 | BCE Loss: 0.9968938231468201\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.681581497192383 | KNN Loss: 3.684340238571167 | BCE Loss: 0.9972414970397949\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.756974220275879 | KNN Loss: 3.707625150680542 | BCE Loss: 1.049349308013916\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.686404705047607 | KNN Loss: 3.667013168334961 | BCE Loss: 1.019391655921936\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.740588188171387 | KNN Loss: 3.6860358715057373 | BCE Loss: 1.0545521974563599\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.717497825622559 | KNN Loss: 3.711855411529541 | BCE Loss: 1.0056421756744385\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.7395806312561035 | KNN Loss: 3.6991348266601562 | BCE Loss: 1.0404459238052368\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.697002410888672 | KNN Loss: 3.6788554191589355 | BCE Loss: 1.0181469917297363\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.754664421081543 | KNN Loss: 3.7345495223999023 | BCE Loss: 1.020114779472351\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.73112154006958 | KNN Loss: 3.709165334701538 | BCE Loss: 1.0219560861587524\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.7761406898498535 | KNN Loss: 3.723101854324341 | BCE Loss: 1.0530388355255127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.724907875061035 | KNN Loss: 3.7121329307556152 | BCE Loss: 1.0127747058868408\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.679256439208984 | KNN Loss: 3.6544671058654785 | BCE Loss: 1.0247893333435059\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.726641654968262 | KNN Loss: 3.69728946685791 | BCE Loss: 1.0293521881103516\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.739554405212402 | KNN Loss: 3.729313373565674 | BCE Loss: 1.0102410316467285\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.686199188232422 | KNN Loss: 3.6696643829345703 | BCE Loss: 1.0165349245071411\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.730989933013916 | KNN Loss: 3.6781208515167236 | BCE Loss: 1.0528690814971924\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.7508225440979 | KNN Loss: 3.6973724365234375 | BCE Loss: 1.053450107574463\n",
      "Epoch   482: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.713065147399902 | KNN Loss: 3.7063686847686768 | BCE Loss: 1.0066962242126465\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.7102508544921875 | KNN Loss: 3.6769473552703857 | BCE Loss: 1.0333032608032227\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.723089218139648 | KNN Loss: 3.6852803230285645 | BCE Loss: 1.0378086566925049\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.7029924392700195 | KNN Loss: 3.683119773864746 | BCE Loss: 1.019872784614563\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.7128448486328125 | KNN Loss: 3.700040578842163 | BCE Loss: 1.0128045082092285\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.717803955078125 | KNN Loss: 3.711566686630249 | BCE Loss: 1.0062371492385864\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.7448883056640625 | KNN Loss: 3.7062556743621826 | BCE Loss: 1.0386326313018799\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.722756385803223 | KNN Loss: 3.679553985595703 | BCE Loss: 1.0432021617889404\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.714228630065918 | KNN Loss: 3.6792490482330322 | BCE Loss: 1.0349794626235962\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.720942974090576 | KNN Loss: 3.700287103652954 | BCE Loss: 1.020655870437622\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.720420837402344 | KNN Loss: 3.697361707687378 | BCE Loss: 1.0230590105056763\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.738821983337402 | KNN Loss: 3.7249362468719482 | BCE Loss: 1.0138859748840332\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.71004056930542 | KNN Loss: 3.6827685832977295 | BCE Loss: 1.0272719860076904\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.688873291015625 | KNN Loss: 3.675434112548828 | BCE Loss: 1.013439416885376\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.693094253540039 | KNN Loss: 3.6852807998657227 | BCE Loss: 1.0078133344650269\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.708864212036133 | KNN Loss: 3.6812713146209717 | BCE Loss: 1.0275928974151611\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.698857307434082 | KNN Loss: 3.6747424602508545 | BCE Loss: 1.0241148471832275\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.685166358947754 | KNN Loss: 3.6480422019958496 | BCE Loss: 1.0371239185333252\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.727493762969971 | KNN Loss: 3.7113022804260254 | BCE Loss: 1.0161913633346558\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.724989891052246 | KNN Loss: 3.6805927753448486 | BCE Loss: 1.044397234916687\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.707066535949707 | KNN Loss: 3.6796674728393555 | BCE Loss: 1.0273993015289307\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.69494104385376 | KNN Loss: 3.686401605606079 | BCE Loss: 1.0085394382476807\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.737157821655273 | KNN Loss: 3.675853967666626 | BCE Loss: 1.061303973197937\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.691080093383789 | KNN Loss: 3.6895503997802734 | BCE Loss: 1.0015296936035156\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.762194633483887 | KNN Loss: 3.7106034755706787 | BCE Loss: 1.0515912771224976\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.708081245422363 | KNN Loss: 3.6578285694122314 | BCE Loss: 1.0502527952194214\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.7226362228393555 | KNN Loss: 3.6917755603790283 | BCE Loss: 1.030860424041748\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.737475872039795 | KNN Loss: 3.7120699882507324 | BCE Loss: 1.025406002998352\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.700015068054199 | KNN Loss: 3.6864936351776123 | BCE Loss: 1.0135211944580078\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.675837516784668 | KNN Loss: 3.671412229537964 | BCE Loss: 1.004425048828125\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.6984405517578125 | KNN Loss: 3.6709883213043213 | BCE Loss: 1.0274523496627808\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.717724800109863 | KNN Loss: 3.6799685955047607 | BCE Loss: 1.0377564430236816\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.699756622314453 | KNN Loss: 3.669430732727051 | BCE Loss: 1.0303261280059814\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.76210880279541 | KNN Loss: 3.7195911407470703 | BCE Loss: 1.0425175428390503\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.720672130584717 | KNN Loss: 3.721792697906494 | BCE Loss: 0.9988792538642883\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.73794412612915 | KNN Loss: 3.734748125076294 | BCE Loss: 1.0031960010528564\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.702112674713135 | KNN Loss: 3.6633198261260986 | BCE Loss: 1.0387928485870361\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.663812637329102 | KNN Loss: 3.658949613571167 | BCE Loss: 1.0048627853393555\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.754606246948242 | KNN Loss: 3.7361207008361816 | BCE Loss: 1.0184855461120605\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.677821636199951 | KNN Loss: 3.6515424251556396 | BCE Loss: 1.026279091835022\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.743999481201172 | KNN Loss: 3.7037172317504883 | BCE Loss: 1.0402820110321045\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.703223705291748 | KNN Loss: 3.682096481323242 | BCE Loss: 1.0211272239685059\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.720800876617432 | KNN Loss: 3.6927952766418457 | BCE Loss: 1.028005599975586\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.761983871459961 | KNN Loss: 3.7635321617126465 | BCE Loss: 0.9984514713287354\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.721746444702148 | KNN Loss: 3.682124376296997 | BCE Loss: 1.0396220684051514\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.715877532958984 | KNN Loss: 3.6961207389831543 | BCE Loss: 1.019756555557251\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.719075679779053 | KNN Loss: 3.712068557739258 | BCE Loss: 1.007007122039795\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.703588485717773 | KNN Loss: 3.684077739715576 | BCE Loss: 1.0195107460021973\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.747178077697754 | KNN Loss: 3.721877098083496 | BCE Loss: 1.0253009796142578\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.706404685974121 | KNN Loss: 3.7000463008880615 | BCE Loss: 1.0063586235046387\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.673528671264648 | KNN Loss: 3.6689438819885254 | BCE Loss: 1.0045850276947021\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.678126811981201 | KNN Loss: 3.680028200149536 | BCE Loss: 0.9980987310409546\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.7133612632751465 | KNN Loss: 3.6764049530029297 | BCE Loss: 1.0369564294815063\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.710287094116211 | KNN Loss: 3.6996910572052 | BCE Loss: 1.0105960369110107\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.66984748840332 | KNN Loss: 3.662810802459717 | BCE Loss: 1.0070369243621826\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.732119083404541 | KNN Loss: 3.6851539611816406 | BCE Loss: 1.0469651222229004\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.710300445556641 | KNN Loss: 3.6860013008117676 | BCE Loss: 1.024298906326294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.682605266571045 | KNN Loss: 3.6712288856506348 | BCE Loss: 1.0113763809204102\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.703763008117676 | KNN Loss: 3.685659408569336 | BCE Loss: 1.0181033611297607\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.735307693481445 | KNN Loss: 3.7150192260742188 | BCE Loss: 1.0202887058258057\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.648627758026123 | KNN Loss: 3.6306040287017822 | BCE Loss: 1.0180238485336304\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.706360816955566 | KNN Loss: 3.6899285316467285 | BCE Loss: 1.016432523727417\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.772653102874756 | KNN Loss: 3.7352843284606934 | BCE Loss: 1.0373687744140625\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.7292914390563965 | KNN Loss: 3.6734671592712402 | BCE Loss: 1.0558241605758667\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.723285675048828 | KNN Loss: 3.701040267944336 | BCE Loss: 1.022245168685913\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.71047830581665 | KNN Loss: 3.703606128692627 | BCE Loss: 1.006872296333313\n",
      "Epoch   493: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.73336124420166 | KNN Loss: 3.684051752090454 | BCE Loss: 1.0493097305297852\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.73201322555542 | KNN Loss: 3.6854195594787598 | BCE Loss: 1.0465936660766602\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.734037399291992 | KNN Loss: 3.705839157104492 | BCE Loss: 1.028198003768921\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.748527526855469 | KNN Loss: 3.707165479660034 | BCE Loss: 1.041361927986145\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.731552600860596 | KNN Loss: 3.7218704223632812 | BCE Loss: 1.009682297706604\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.7821245193481445 | KNN Loss: 3.7388179302215576 | BCE Loss: 1.043306589126587\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.689398765563965 | KNN Loss: 3.6558916568756104 | BCE Loss: 1.0335071086883545\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.748076915740967 | KNN Loss: 3.7482635974884033 | BCE Loss: 0.9998131394386292\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.730314254760742 | KNN Loss: 3.7107691764831543 | BCE Loss: 1.0195449590682983\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.733954906463623 | KNN Loss: 3.692941427230835 | BCE Loss: 1.0410135984420776\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.673260688781738 | KNN Loss: 3.6711556911468506 | BCE Loss: 1.0021047592163086\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.753771781921387 | KNN Loss: 3.7284178733825684 | BCE Loss: 1.0253536701202393\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.6897053718566895 | KNN Loss: 3.685189723968506 | BCE Loss: 1.0045157670974731\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.695342540740967 | KNN Loss: 3.678006887435913 | BCE Loss: 1.0173355340957642\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.74575138092041 | KNN Loss: 3.7061901092529297 | BCE Loss: 1.0395610332489014\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.701865196228027 | KNN Loss: 3.6817288398742676 | BCE Loss: 1.0201363563537598\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.718466281890869 | KNN Loss: 3.6803715229034424 | BCE Loss: 1.0380947589874268\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.72196102142334 | KNN Loss: 3.710562229156494 | BCE Loss: 1.0113987922668457\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.743007659912109 | KNN Loss: 3.7070510387420654 | BCE Loss: 1.0359563827514648\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.732144355773926 | KNN Loss: 3.7054996490478516 | BCE Loss: 1.0266449451446533\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.71040153503418 | KNN Loss: 3.6844277381896973 | BCE Loss: 1.0259735584259033\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.656641960144043 | KNN Loss: 3.665755033493042 | BCE Loss: 0.990886926651001\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.738516330718994 | KNN Loss: 3.6922664642333984 | BCE Loss: 1.0462498664855957\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.712428092956543 | KNN Loss: 3.695561170578003 | BCE Loss: 1.0168671607971191\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.740962028503418 | KNN Loss: 3.712110757827759 | BCE Loss: 1.0288512706756592\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.721685409545898 | KNN Loss: 3.712127685546875 | BCE Loss: 1.0095574855804443\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.719819068908691 | KNN Loss: 3.685375690460205 | BCE Loss: 1.0344431400299072\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.725241661071777 | KNN Loss: 3.688927412033081 | BCE Loss: 1.0363142490386963\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.7271928787231445 | KNN Loss: 3.6778712272644043 | BCE Loss: 1.0493216514587402\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.76076602935791 | KNN Loss: 3.699451446533203 | BCE Loss: 1.0613144636154175\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.704841136932373 | KNN Loss: 3.6741886138916016 | BCE Loss: 1.0306525230407715\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.679123401641846 | KNN Loss: 3.666754722595215 | BCE Loss: 1.0123686790466309\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.701310634613037 | KNN Loss: 3.6613352298736572 | BCE Loss: 1.0399755239486694\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.704067707061768 | KNN Loss: 3.679354190826416 | BCE Loss: 1.0247135162353516\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.745271682739258 | KNN Loss: 3.7383337020874023 | BCE Loss: 1.0069379806518555\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.749481201171875 | KNN Loss: 3.715418577194214 | BCE Loss: 1.034062385559082\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.729916572570801 | KNN Loss: 3.7233705520629883 | BCE Loss: 1.0065462589263916\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.683018684387207 | KNN Loss: 3.683534860610962 | BCE Loss: 0.9994840621948242\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.715856552124023 | KNN Loss: 3.6987173557281494 | BCE Loss: 1.017139196395874\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.695939540863037 | KNN Loss: 3.669078826904297 | BCE Loss: 1.0268607139587402\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.713281154632568 | KNN Loss: 3.674494981765747 | BCE Loss: 1.0387860536575317\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.750343322753906 | KNN Loss: 3.699354887008667 | BCE Loss: 1.0509883165359497\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8700,  3.2148,  2.5888,  3.2199,  2.9580,  0.7435,  2.5452,  1.7208,\n",
      "          2.3905,  2.0455,  2.3058,  1.8545,  0.7572,  1.9303,  1.3485,  1.4854,\n",
      "          2.4915,  2.9553,  2.8168,  1.8230,  1.8092,  2.7377,  2.3335,  2.2920,\n",
      "          2.6009,  1.8221,  2.0219,  1.3945,  1.5464,  0.3820, -0.2454,  0.9553,\n",
      "          0.2409,  1.0272,  1.3222,  1.4977,  1.0836,  2.8201,  0.8823,  1.3845,\n",
      "          0.9177, -0.6213, -0.2419,  2.4166,  2.2063,  0.6436, -0.1784, -0.0311,\n",
      "          1.5638,  2.1203,  1.9193,  0.2048,  1.4929,  0.5505, -0.6681,  1.1283,\n",
      "          1.5035,  0.8375,  1.4346,  1.8754,  0.6843,  0.9019,  0.2014,  1.7707,\n",
      "          1.2772,  1.5920, -2.0957,  0.3177,  2.0157,  1.9995,  2.4371,  0.5219,\n",
      "          1.3739,  2.4774,  2.0058,  1.2663,  0.2416,  0.7286,  0.2631,  1.6594,\n",
      "          0.0389,  0.4202,  1.9244, -0.2867,  0.2105, -1.0360, -2.3141, -0.4303,\n",
      "          0.5728, -1.8165,  0.4920, -0.1380, -0.5948, -0.8402,  0.5739,  1.3905,\n",
      "         -0.8045, -0.6958,  0.3266,  1.2115,  0.6505, -1.1720,  0.9374,  0.9932,\n",
      "         -1.2691, -1.0461, -0.0805,  0.0717, -1.0359, -1.5882, -0.5029, -3.2110,\n",
      "         -0.3207,  1.6042,  1.6449, -0.3926, -0.5681,  0.0302,  1.5541, -2.5300,\n",
      "          0.2356, -0.1115,  0.4058, -0.7327,  0.0327, -0.8474, -0.9020,  1.0158,\n",
      "          0.3790, -0.6137,  0.3411, -0.6735, -1.3256, -0.3211, -0.4911,  0.8159,\n",
      "         -0.3742,  0.1905, -1.9856, -0.9431, -1.2661,  0.5641, -1.8637, -0.9322,\n",
      "         -1.1333, -0.5682, -1.5296, -1.0255, -2.7561, -1.0150, -1.3682, -0.3808,\n",
      "         -1.7841,  0.5715, -1.4361, -0.6033, -4.0957,  0.2111, -0.1392, -0.6854,\n",
      "         -2.2010, -1.5484, -1.2884, -1.3442, -2.3957, -2.3662, -3.6261]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-4.0957, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.2199, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4185042f7c634ec3a64df3acc20316cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 78.14it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7101ac76d54c46518ba3e79706390b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee59be3b5a044123bae3bd712082182b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1567cb66bf7748adbfad607c163ab0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "layer 9: 0.0\n",
      "layer 10: 0.0\n",
      "Epoch: 00 | Batch: 000 / 026 | Total loss: 9.626 | Reg loss: 0.014 | Tree loss: 9.626 | Accuracy: 0.000000 | 6.821 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 026 | Total loss: 9.624 | Reg loss: 0.013 | Tree loss: 9.624 | Accuracy: 0.000000 | 6.117 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 026 | Total loss: 9.623 | Reg loss: 0.012 | Tree loss: 9.623 | Accuracy: 0.000000 | 5.831 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 026 | Total loss: 9.621 | Reg loss: 0.011 | Tree loss: 9.621 | Accuracy: 0.000000 | 5.674 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 026 | Total loss: 9.619 | Reg loss: 0.010 | Tree loss: 9.619 | Accuracy: 0.000000 | 5.561 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 026 | Total loss: 9.617 | Reg loss: 0.009 | Tree loss: 9.617 | Accuracy: 0.000000 | 5.479 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 026 | Total loss: 9.616 | Reg loss: 0.008 | Tree loss: 9.616 | Accuracy: 0.000000 | 5.411 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 026 | Total loss: 9.614 | Reg loss: 0.007 | Tree loss: 9.614 | Accuracy: 0.000000 | 5.355 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 026 | Total loss: 9.613 | Reg loss: 0.007 | Tree loss: 9.613 | Accuracy: 0.000000 | 5.244 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 026 | Total loss: 9.610 | Reg loss: 0.006 | Tree loss: 9.610 | Accuracy: 0.000000 | 5.129 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 026 | Total loss: 9.609 | Reg loss: 0.006 | Tree loss: 9.609 | Accuracy: 0.000000 | 5.175 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 026 | Total loss: 9.607 | Reg loss: 0.005 | Tree loss: 9.607 | Accuracy: 0.000000 | 5.191 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 026 | Total loss: 9.605 | Reg loss: 0.005 | Tree loss: 9.605 | Accuracy: 0.000000 | 5.19 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 026 | Total loss: 9.605 | Reg loss: 0.005 | Tree loss: 9.605 | Accuracy: 0.000000 | 5.177 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 026 | Total loss: 9.601 | Reg loss: 0.005 | Tree loss: 9.601 | Accuracy: 0.001953 | 5.156 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 026 | Total loss: 9.601 | Reg loss: 0.005 | Tree loss: 9.601 | Accuracy: 0.001953 | 5.137 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 026 | Total loss: 9.599 | Reg loss: 0.005 | Tree loss: 9.599 | Accuracy: 0.001953 | 5.117 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 026 | Total loss: 9.597 | Reg loss: 0.005 | Tree loss: 9.597 | Accuracy: 0.001953 | 5.099 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 026 | Total loss: 9.597 | Reg loss: 0.005 | Tree loss: 9.597 | Accuracy: 0.001953 | 5.084 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 026 | Total loss: 9.595 | Reg loss: 0.006 | Tree loss: 9.595 | Accuracy: 0.013672 | 5.069 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 026 | Total loss: 9.595 | Reg loss: 0.006 | Tree loss: 9.595 | Accuracy: 0.021484 | 5.053 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 026 | Total loss: 9.593 | Reg loss: 0.006 | Tree loss: 9.593 | Accuracy: 0.031250 | 5.04 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 026 | Total loss: 9.593 | Reg loss: 0.006 | Tree loss: 9.593 | Accuracy: 0.027344 | 5.027 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 026 | Total loss: 9.592 | Reg loss: 0.006 | Tree loss: 9.592 | Accuracy: 0.058594 | 5.016 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 026 | Total loss: 9.589 | Reg loss: 0.006 | Tree loss: 9.589 | Accuracy: 0.076172 | 5.005 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 026 | Total loss: 9.588 | Reg loss: 0.006 | Tree loss: 9.588 | Accuracy: 0.083871 | 4.952 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 026 | Total loss: 9.598 | Reg loss: 0.003 | Tree loss: 9.598 | Accuracy: 0.050781 | 5.041 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 026 | Total loss: 9.596 | Reg loss: 0.004 | Tree loss: 9.596 | Accuracy: 0.091797 | 5.049 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 026 | Total loss: 9.595 | Reg loss: 0.004 | Tree loss: 9.595 | Accuracy: 0.091797 | 5.056 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 026 | Total loss: 9.594 | Reg loss: 0.004 | Tree loss: 9.594 | Accuracy: 0.074219 | 5.064 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 026 | Total loss: 9.593 | Reg loss: 0.004 | Tree loss: 9.593 | Accuracy: 0.103516 | 5.071 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 026 | Total loss: 9.591 | Reg loss: 0.004 | Tree loss: 9.591 | Accuracy: 0.080078 | 5.077 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 026 | Total loss: 9.590 | Reg loss: 0.004 | Tree loss: 9.590 | Accuracy: 0.093750 | 5.081 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 026 | Total loss: 9.590 | Reg loss: 0.004 | Tree loss: 9.590 | Accuracy: 0.103516 | 5.084 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 026 | Total loss: 9.589 | Reg loss: 0.005 | Tree loss: 9.589 | Accuracy: 0.101562 | 5.086 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 026 | Total loss: 9.587 | Reg loss: 0.005 | Tree loss: 9.587 | Accuracy: 0.099609 | 5.087 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 026 | Total loss: 9.585 | Reg loss: 0.005 | Tree loss: 9.585 | Accuracy: 0.126953 | 5.086 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 026 | Total loss: 9.586 | Reg loss: 0.005 | Tree loss: 9.586 | Accuracy: 0.107422 | 5.084 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 026 | Total loss: 9.585 | Reg loss: 0.005 | Tree loss: 9.585 | Accuracy: 0.089844 | 5.081 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 026 | Total loss: 9.583 | Reg loss: 0.005 | Tree loss: 9.583 | Accuracy: 0.095703 | 5.074 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 026 | Total loss: 9.583 | Reg loss: 0.006 | Tree loss: 9.583 | Accuracy: 0.117188 | 5.065 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 026 | Total loss: 9.581 | Reg loss: 0.006 | Tree loss: 9.581 | Accuracy: 0.107422 | 5.058 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 026 | Total loss: 9.581 | Reg loss: 0.006 | Tree loss: 9.581 | Accuracy: 0.128906 | 5.05 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 026 | Total loss: 9.581 | Reg loss: 0.006 | Tree loss: 9.581 | Accuracy: 0.142578 | 5.043 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 026 | Total loss: 9.580 | Reg loss: 0.006 | Tree loss: 9.580 | Accuracy: 0.125000 | 5.03 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 026 | Total loss: 9.578 | Reg loss: 0.006 | Tree loss: 9.578 | Accuracy: 0.125000 | 5.01 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 026 | Total loss: 9.579 | Reg loss: 0.006 | Tree loss: 9.579 | Accuracy: 0.125000 | 5.028 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 026 | Total loss: 9.578 | Reg loss: 0.007 | Tree loss: 9.578 | Accuracy: 0.130859 | 5.047 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 026 | Total loss: 9.576 | Reg loss: 0.007 | Tree loss: 9.576 | Accuracy: 0.132812 | 5.06 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 026 | Total loss: 9.575 | Reg loss: 0.007 | Tree loss: 9.575 | Accuracy: 0.119141 | 5.068 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 026 | Total loss: 9.577 | Reg loss: 0.007 | Tree loss: 9.577 | Accuracy: 0.107422 | 5.073 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 026 | Total loss: 9.578 | Reg loss: 0.007 | Tree loss: 9.578 | Accuracy: 0.109677 | 5.044 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 026 | Total loss: 9.585 | Reg loss: 0.005 | Tree loss: 9.585 | Accuracy: 0.089844 | 5.066 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 026 | Total loss: 9.583 | Reg loss: 0.005 | Tree loss: 9.583 | Accuracy: 0.121094 | 5.064 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 026 | Total loss: 9.584 | Reg loss: 0.005 | Tree loss: 9.584 | Accuracy: 0.109375 | 5.063 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 026 | Total loss: 9.581 | Reg loss: 0.005 | Tree loss: 9.581 | Accuracy: 0.121094 | 5.06 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 026 | Total loss: 9.582 | Reg loss: 0.005 | Tree loss: 9.582 | Accuracy: 0.128906 | 5.057 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 026 | Total loss: 9.581 | Reg loss: 0.006 | Tree loss: 9.581 | Accuracy: 0.119141 | 5.053 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 026 | Total loss: 9.581 | Reg loss: 0.006 | Tree loss: 9.581 | Accuracy: 0.107422 | 5.049 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 026 | Total loss: 9.578 | Reg loss: 0.006 | Tree loss: 9.578 | Accuracy: 0.134766 | 5.045 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 008 / 026 | Total loss: 9.578 | Reg loss: 0.006 | Tree loss: 9.578 | Accuracy: 0.111328 | 5.041 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 026 | Total loss: 9.579 | Reg loss: 0.006 | Tree loss: 9.579 | Accuracy: 0.115234 | 5.037 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 026 | Total loss: 9.576 | Reg loss: 0.007 | Tree loss: 9.576 | Accuracy: 0.138672 | 5.03 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 026 | Total loss: 9.575 | Reg loss: 0.007 | Tree loss: 9.575 | Accuracy: 0.132812 | 5.015 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 026 | Total loss: 9.573 | Reg loss: 0.007 | Tree loss: 9.573 | Accuracy: 0.132812 | 5.026 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 026 | Total loss: 9.573 | Reg loss: 0.007 | Tree loss: 9.573 | Accuracy: 0.134766 | 5.038 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 026 | Total loss: 9.572 | Reg loss: 0.007 | Tree loss: 9.572 | Accuracy: 0.130859 | 5.051 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 026 | Total loss: 9.570 | Reg loss: 0.008 | Tree loss: 9.570 | Accuracy: 0.138672 | 5.064 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 026 | Total loss: 9.570 | Reg loss: 0.008 | Tree loss: 9.570 | Accuracy: 0.142578 | 5.074 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 026 | Total loss: 9.569 | Reg loss: 0.008 | Tree loss: 9.569 | Accuracy: 0.125000 | 5.081 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 026 | Total loss: 9.566 | Reg loss: 0.008 | Tree loss: 9.566 | Accuracy: 0.117188 | 5.086 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 026 | Total loss: 9.568 | Reg loss: 0.008 | Tree loss: 9.568 | Accuracy: 0.128906 | 5.089 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 026 | Total loss: 9.567 | Reg loss: 0.009 | Tree loss: 9.567 | Accuracy: 0.136719 | 5.09 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 026 | Total loss: 9.568 | Reg loss: 0.009 | Tree loss: 9.568 | Accuracy: 0.097656 | 5.09 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 026 | Total loss: 9.564 | Reg loss: 0.009 | Tree loss: 9.564 | Accuracy: 0.126953 | 5.087 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 026 | Total loss: 9.564 | Reg loss: 0.009 | Tree loss: 9.564 | Accuracy: 0.109375 | 5.084 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 026 | Total loss: 9.565 | Reg loss: 0.010 | Tree loss: 9.565 | Accuracy: 0.103516 | 5.081 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 026 | Total loss: 9.564 | Reg loss: 0.010 | Tree loss: 9.564 | Accuracy: 0.083871 | 5.062 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 026 | Total loss: 9.575 | Reg loss: 0.007 | Tree loss: 9.575 | Accuracy: 0.138672 | 5.11 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 026 | Total loss: 9.574 | Reg loss: 0.007 | Tree loss: 9.574 | Accuracy: 0.134766 | 5.116 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 026 | Total loss: 9.574 | Reg loss: 0.007 | Tree loss: 9.574 | Accuracy: 0.115234 | 5.105 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 026 | Total loss: 9.571 | Reg loss: 0.007 | Tree loss: 9.571 | Accuracy: 0.115234 | 5.109 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 026 | Total loss: 9.569 | Reg loss: 0.007 | Tree loss: 9.569 | Accuracy: 0.130859 | 5.113 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 026 | Total loss: 9.568 | Reg loss: 0.008 | Tree loss: 9.568 | Accuracy: 0.140625 | 5.119 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 026 | Total loss: 9.567 | Reg loss: 0.008 | Tree loss: 9.567 | Accuracy: 0.107422 | 5.123 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 026 | Total loss: 9.566 | Reg loss: 0.008 | Tree loss: 9.566 | Accuracy: 0.109375 | 5.127 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 026 | Total loss: 9.563 | Reg loss: 0.008 | Tree loss: 9.563 | Accuracy: 0.134766 | 5.131 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 026 | Total loss: 9.562 | Reg loss: 0.009 | Tree loss: 9.562 | Accuracy: 0.130859 | 5.134 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 026 | Total loss: 9.563 | Reg loss: 0.009 | Tree loss: 9.563 | Accuracy: 0.113281 | 5.136 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 026 | Total loss: 9.560 | Reg loss: 0.009 | Tree loss: 9.560 | Accuracy: 0.121094 | 5.138 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 026 | Total loss: 9.557 | Reg loss: 0.009 | Tree loss: 9.557 | Accuracy: 0.115234 | 5.138 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 026 | Total loss: 9.555 | Reg loss: 0.010 | Tree loss: 9.555 | Accuracy: 0.115234 | 5.139 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 026 | Total loss: 9.555 | Reg loss: 0.010 | Tree loss: 9.555 | Accuracy: 0.080078 | 5.138 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 026 | Total loss: 9.552 | Reg loss: 0.010 | Tree loss: 9.552 | Accuracy: 0.126953 | 5.137 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 026 | Total loss: 9.551 | Reg loss: 0.011 | Tree loss: 9.551 | Accuracy: 0.113281 | 5.135 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 026 | Total loss: 9.546 | Reg loss: 0.011 | Tree loss: 9.546 | Accuracy: 0.146484 | 5.133 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 026 | Total loss: 9.547 | Reg loss: 0.011 | Tree loss: 9.547 | Accuracy: 0.128906 | 5.129 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 026 | Total loss: 9.544 | Reg loss: 0.012 | Tree loss: 9.544 | Accuracy: 0.115234 | 5.125 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 026 | Total loss: 9.539 | Reg loss: 0.012 | Tree loss: 9.539 | Accuracy: 0.136719 | 5.118 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 026 | Total loss: 9.539 | Reg loss: 0.012 | Tree loss: 9.539 | Accuracy: 0.111328 | 5.107 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 026 | Total loss: 9.536 | Reg loss: 0.012 | Tree loss: 9.536 | Accuracy: 0.138672 | 5.115 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 026 | Total loss: 9.534 | Reg loss: 0.013 | Tree loss: 9.534 | Accuracy: 0.113281 | 5.121 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 026 | Total loss: 9.526 | Reg loss: 0.013 | Tree loss: 9.526 | Accuracy: 0.126953 | 5.123 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 026 | Total loss: 9.522 | Reg loss: 0.013 | Tree loss: 9.522 | Accuracy: 0.148387 | 5.109 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 026 | Total loss: 9.555 | Reg loss: 0.009 | Tree loss: 9.555 | Accuracy: 0.128906 | 5.119 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 026 | Total loss: 9.554 | Reg loss: 0.009 | Tree loss: 9.554 | Accuracy: 0.144531 | 5.119 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 026 | Total loss: 9.550 | Reg loss: 0.010 | Tree loss: 9.550 | Accuracy: 0.107422 | 5.119 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 026 | Total loss: 9.550 | Reg loss: 0.010 | Tree loss: 9.550 | Accuracy: 0.103516 | 5.119 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 026 | Total loss: 9.545 | Reg loss: 0.010 | Tree loss: 9.545 | Accuracy: 0.140625 | 5.119 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 026 | Total loss: 9.542 | Reg loss: 0.010 | Tree loss: 9.542 | Accuracy: 0.125000 | 5.118 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 026 | Total loss: 9.539 | Reg loss: 0.010 | Tree loss: 9.539 | Accuracy: 0.130859 | 5.116 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 026 | Total loss: 9.536 | Reg loss: 0.011 | Tree loss: 9.536 | Accuracy: 0.121094 | 5.114 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 026 | Total loss: 9.533 | Reg loss: 0.011 | Tree loss: 9.533 | Accuracy: 0.121094 | 5.111 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 026 | Total loss: 9.527 | Reg loss: 0.011 | Tree loss: 9.527 | Accuracy: 0.091797 | 5.109 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 026 | Total loss: 9.521 | Reg loss: 0.012 | Tree loss: 9.521 | Accuracy: 0.115234 | 5.106 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 026 | Total loss: 9.519 | Reg loss: 0.012 | Tree loss: 9.519 | Accuracy: 0.115234 | 5.103 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 026 | Total loss: 9.510 | Reg loss: 0.012 | Tree loss: 9.510 | Accuracy: 0.128906 | 5.098 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 026 | Total loss: 9.505 | Reg loss: 0.013 | Tree loss: 9.505 | Accuracy: 0.144531 | 5.09 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 026 | Total loss: 9.501 | Reg loss: 0.013 | Tree loss: 9.501 | Accuracy: 0.101562 | 5.096 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 026 | Total loss: 9.498 | Reg loss: 0.013 | Tree loss: 9.498 | Accuracy: 0.123047 | 5.103 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 026 | Total loss: 9.488 | Reg loss: 0.014 | Tree loss: 9.488 | Accuracy: 0.132812 | 5.109 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 017 / 026 | Total loss: 9.478 | Reg loss: 0.014 | Tree loss: 9.478 | Accuracy: 0.130859 | 5.113 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 026 | Total loss: 9.483 | Reg loss: 0.015 | Tree loss: 9.483 | Accuracy: 0.111328 | 5.115 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 026 | Total loss: 9.474 | Reg loss: 0.015 | Tree loss: 9.474 | Accuracy: 0.099609 | 5.117 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 026 | Total loss: 9.463 | Reg loss: 0.016 | Tree loss: 9.463 | Accuracy: 0.105469 | 5.117 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 026 | Total loss: 9.454 | Reg loss: 0.016 | Tree loss: 9.454 | Accuracy: 0.105469 | 5.116 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 026 | Total loss: 9.450 | Reg loss: 0.016 | Tree loss: 9.450 | Accuracy: 0.111328 | 5.114 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 026 | Total loss: 9.445 | Reg loss: 0.017 | Tree loss: 9.445 | Accuracy: 0.095703 | 5.111 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 026 | Total loss: 9.430 | Reg loss: 0.017 | Tree loss: 9.430 | Accuracy: 0.058594 | 5.109 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 026 | Total loss: 9.419 | Reg loss: 0.018 | Tree loss: 9.419 | Accuracy: 0.096774 | 5.097 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 026 | Total loss: 9.510 | Reg loss: 0.012 | Tree loss: 9.510 | Accuracy: 0.126953 | 5.126 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 026 | Total loss: 9.501 | Reg loss: 0.012 | Tree loss: 9.501 | Accuracy: 0.134766 | 5.129 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 026 | Total loss: 9.497 | Reg loss: 0.012 | Tree loss: 9.497 | Accuracy: 0.125000 | 5.132 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 026 | Total loss: 9.492 | Reg loss: 0.012 | Tree loss: 9.492 | Accuracy: 0.136719 | 5.136 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 026 | Total loss: 9.486 | Reg loss: 0.012 | Tree loss: 9.486 | Accuracy: 0.097656 | 5.129 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 026 | Total loss: 9.480 | Reg loss: 0.013 | Tree loss: 9.480 | Accuracy: 0.132812 | 5.132 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 026 | Total loss: 9.467 | Reg loss: 0.013 | Tree loss: 9.467 | Accuracy: 0.097656 | 5.135 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 026 | Total loss: 9.452 | Reg loss: 0.013 | Tree loss: 9.452 | Accuracy: 0.117188 | 5.138 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 026 | Total loss: 9.456 | Reg loss: 0.014 | Tree loss: 9.456 | Accuracy: 0.089844 | 5.14 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 026 | Total loss: 9.447 | Reg loss: 0.014 | Tree loss: 9.447 | Accuracy: 0.085938 | 5.143 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 026 | Total loss: 9.437 | Reg loss: 0.014 | Tree loss: 9.437 | Accuracy: 0.095703 | 5.145 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 026 | Total loss: 9.425 | Reg loss: 0.015 | Tree loss: 9.425 | Accuracy: 0.121094 | 5.146 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 026 | Total loss: 9.418 | Reg loss: 0.015 | Tree loss: 9.418 | Accuracy: 0.101562 | 5.148 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 026 | Total loss: 9.406 | Reg loss: 0.016 | Tree loss: 9.406 | Accuracy: 0.083984 | 5.148 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 026 | Total loss: 9.398 | Reg loss: 0.016 | Tree loss: 9.398 | Accuracy: 0.095703 | 5.149 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 026 | Total loss: 9.382 | Reg loss: 0.016 | Tree loss: 9.382 | Accuracy: 0.126953 | 5.148 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 026 | Total loss: 9.369 | Reg loss: 0.017 | Tree loss: 9.369 | Accuracy: 0.095703 | 5.148 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 026 | Total loss: 9.353 | Reg loss: 0.017 | Tree loss: 9.353 | Accuracy: 0.072266 | 5.147 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 026 | Total loss: 9.346 | Reg loss: 0.018 | Tree loss: 9.346 | Accuracy: 0.070312 | 5.145 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 026 | Total loss: 9.332 | Reg loss: 0.018 | Tree loss: 9.332 | Accuracy: 0.099609 | 5.143 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 026 | Total loss: 9.315 | Reg loss: 0.019 | Tree loss: 9.315 | Accuracy: 0.066406 | 5.14 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 026 | Total loss: 9.304 | Reg loss: 0.019 | Tree loss: 9.304 | Accuracy: 0.068359 | 5.137 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 026 | Total loss: 9.289 | Reg loss: 0.020 | Tree loss: 9.289 | Accuracy: 0.085938 | 5.133 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 026 | Total loss: 9.271 | Reg loss: 0.020 | Tree loss: 9.271 | Accuracy: 0.066406 | 5.126 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 026 | Total loss: 9.263 | Reg loss: 0.020 | Tree loss: 9.263 | Accuracy: 0.083984 | 5.131 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 026 | Total loss: 9.251 | Reg loss: 0.021 | Tree loss: 9.251 | Accuracy: 0.058065 | 5.121 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 026 | Total loss: 9.415 | Reg loss: 0.014 | Tree loss: 9.415 | Accuracy: 0.113281 | 5.132 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 026 | Total loss: 9.397 | Reg loss: 0.014 | Tree loss: 9.397 | Accuracy: 0.115234 | 5.133 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 026 | Total loss: 9.395 | Reg loss: 0.015 | Tree loss: 9.395 | Accuracy: 0.087891 | 5.133 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 026 | Total loss: 9.379 | Reg loss: 0.015 | Tree loss: 9.379 | Accuracy: 0.091797 | 5.134 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 026 | Total loss: 9.373 | Reg loss: 0.015 | Tree loss: 9.373 | Accuracy: 0.103516 | 5.134 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 026 | Total loss: 9.352 | Reg loss: 0.015 | Tree loss: 9.352 | Accuracy: 0.085938 | 5.134 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 026 | Total loss: 9.349 | Reg loss: 0.015 | Tree loss: 9.349 | Accuracy: 0.101562 | 5.133 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 026 | Total loss: 9.329 | Reg loss: 0.016 | Tree loss: 9.329 | Accuracy: 0.097656 | 5.131 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 026 | Total loss: 9.325 | Reg loss: 0.016 | Tree loss: 9.325 | Accuracy: 0.107422 | 5.129 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 026 | Total loss: 9.292 | Reg loss: 0.016 | Tree loss: 9.292 | Accuracy: 0.101562 | 5.127 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 026 | Total loss: 9.287 | Reg loss: 0.017 | Tree loss: 9.287 | Accuracy: 0.082031 | 5.125 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 026 | Total loss: 9.268 | Reg loss: 0.017 | Tree loss: 9.268 | Accuracy: 0.099609 | 5.123 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 026 | Total loss: 9.261 | Reg loss: 0.017 | Tree loss: 9.261 | Accuracy: 0.101562 | 5.121 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 026 | Total loss: 9.245 | Reg loss: 0.018 | Tree loss: 9.245 | Accuracy: 0.103516 | 5.119 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 026 | Total loss: 9.225 | Reg loss: 0.018 | Tree loss: 9.225 | Accuracy: 0.095703 | 5.116 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 026 | Total loss: 9.207 | Reg loss: 0.019 | Tree loss: 9.207 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 026 | Total loss: 9.201 | Reg loss: 0.019 | Tree loss: 9.201 | Accuracy: 0.070312 | 5.113 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 026 | Total loss: 9.173 | Reg loss: 0.020 | Tree loss: 9.173 | Accuracy: 0.066406 | 5.118 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 026 | Total loss: 9.158 | Reg loss: 0.020 | Tree loss: 9.158 | Accuracy: 0.078125 | 5.122 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 026 | Total loss: 9.122 | Reg loss: 0.020 | Tree loss: 9.122 | Accuracy: 0.070312 | 5.125 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 026 | Total loss: 9.103 | Reg loss: 0.021 | Tree loss: 9.103 | Accuracy: 0.050781 | 5.127 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 026 | Total loss: 9.106 | Reg loss: 0.021 | Tree loss: 9.106 | Accuracy: 0.060547 | 5.127 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 026 | Total loss: 9.080 | Reg loss: 0.022 | Tree loss: 9.080 | Accuracy: 0.050781 | 5.127 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 026 | Total loss: 9.076 | Reg loss: 0.022 | Tree loss: 9.076 | Accuracy: 0.048828 | 5.127 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 026 | Total loss: 9.063 | Reg loss: 0.023 | Tree loss: 9.063 | Accuracy: 0.046875 | 5.125 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 026 | Total loss: 8.991 | Reg loss: 0.023 | Tree loss: 8.991 | Accuracy: 0.045161 | 5.117 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 026 | Total loss: 9.285 | Reg loss: 0.017 | Tree loss: 9.285 | Accuracy: 0.128906 | 5.137 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 026 | Total loss: 9.248 | Reg loss: 0.017 | Tree loss: 9.248 | Accuracy: 0.109375 | 5.139 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 026 | Total loss: 9.236 | Reg loss: 0.017 | Tree loss: 9.236 | Accuracy: 0.107422 | 5.14 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 026 | Total loss: 9.213 | Reg loss: 0.017 | Tree loss: 9.213 | Accuracy: 0.097656 | 5.14 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 026 | Total loss: 9.208 | Reg loss: 0.017 | Tree loss: 9.208 | Accuracy: 0.103516 | 5.141 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 026 | Total loss: 9.182 | Reg loss: 0.017 | Tree loss: 9.182 | Accuracy: 0.107422 | 5.14 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 026 | Total loss: 9.159 | Reg loss: 0.018 | Tree loss: 9.159 | Accuracy: 0.111328 | 5.136 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 026 | Total loss: 9.150 | Reg loss: 0.018 | Tree loss: 9.150 | Accuracy: 0.074219 | 5.131 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 026 | Total loss: 9.125 | Reg loss: 0.018 | Tree loss: 9.125 | Accuracy: 0.076172 | 5.132 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 026 | Total loss: 9.119 | Reg loss: 0.018 | Tree loss: 9.119 | Accuracy: 0.072266 | 5.132 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 026 | Total loss: 9.101 | Reg loss: 0.019 | Tree loss: 9.101 | Accuracy: 0.062500 | 5.131 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 026 | Total loss: 9.059 | Reg loss: 0.019 | Tree loss: 9.059 | Accuracy: 0.070312 | 5.13 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 026 | Total loss: 9.024 | Reg loss: 0.019 | Tree loss: 9.024 | Accuracy: 0.083984 | 5.128 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 026 | Total loss: 9.029 | Reg loss: 0.020 | Tree loss: 9.029 | Accuracy: 0.066406 | 5.127 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 026 | Total loss: 8.999 | Reg loss: 0.020 | Tree loss: 8.999 | Accuracy: 0.064453 | 5.125 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 026 | Total loss: 8.983 | Reg loss: 0.021 | Tree loss: 8.983 | Accuracy: 0.044922 | 5.123 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 026 | Total loss: 8.973 | Reg loss: 0.021 | Tree loss: 8.973 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 026 | Total loss: 8.950 | Reg loss: 0.021 | Tree loss: 8.950 | Accuracy: 0.064453 | 5.118 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 026 | Total loss: 8.922 | Reg loss: 0.022 | Tree loss: 8.922 | Accuracy: 0.064453 | 5.117 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 026 | Total loss: 8.891 | Reg loss: 0.022 | Tree loss: 8.891 | Accuracy: 0.066406 | 5.115 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 026 | Total loss: 8.875 | Reg loss: 0.023 | Tree loss: 8.875 | Accuracy: 0.060547 | 5.113 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 026 | Total loss: 8.825 | Reg loss: 0.023 | Tree loss: 8.825 | Accuracy: 0.076172 | 5.111 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 026 | Total loss: 8.842 | Reg loss: 0.023 | Tree loss: 8.842 | Accuracy: 0.050781 | 5.109 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 026 | Total loss: 8.814 | Reg loss: 0.024 | Tree loss: 8.814 | Accuracy: 0.050781 | 5.107 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 026 | Total loss: 8.798 | Reg loss: 0.024 | Tree loss: 8.798 | Accuracy: 0.054688 | 5.104 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 026 | Total loss: 8.780 | Reg loss: 0.025 | Tree loss: 8.780 | Accuracy: 0.058065 | 5.097 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 026 | Total loss: 9.076 | Reg loss: 0.019 | Tree loss: 9.076 | Accuracy: 0.115234 | 5.116 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 026 | Total loss: 9.055 | Reg loss: 0.019 | Tree loss: 9.055 | Accuracy: 0.080078 | 5.118 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 026 | Total loss: 9.031 | Reg loss: 0.019 | Tree loss: 9.031 | Accuracy: 0.097656 | 5.119 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 026 | Total loss: 9.001 | Reg loss: 0.019 | Tree loss: 9.001 | Accuracy: 0.097656 | 5.121 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 026 | Total loss: 8.985 | Reg loss: 0.019 | Tree loss: 8.985 | Accuracy: 0.103516 | 5.122 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 026 | Total loss: 8.964 | Reg loss: 0.019 | Tree loss: 8.964 | Accuracy: 0.068359 | 5.123 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 026 | Total loss: 8.954 | Reg loss: 0.020 | Tree loss: 8.954 | Accuracy: 0.064453 | 5.123 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 026 | Total loss: 8.920 | Reg loss: 0.020 | Tree loss: 8.920 | Accuracy: 0.087891 | 5.123 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 026 | Total loss: 8.881 | Reg loss: 0.020 | Tree loss: 8.881 | Accuracy: 0.060547 | 5.122 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 026 | Total loss: 8.859 | Reg loss: 0.020 | Tree loss: 8.859 | Accuracy: 0.066406 | 5.121 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 026 | Total loss: 8.842 | Reg loss: 0.021 | Tree loss: 8.842 | Accuracy: 0.070312 | 5.119 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 026 | Total loss: 8.814 | Reg loss: 0.021 | Tree loss: 8.814 | Accuracy: 0.042969 | 5.118 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 026 | Total loss: 8.829 | Reg loss: 0.021 | Tree loss: 8.829 | Accuracy: 0.060547 | 5.117 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 026 | Total loss: 8.782 | Reg loss: 0.022 | Tree loss: 8.782 | Accuracy: 0.066406 | 5.115 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 026 | Total loss: 8.729 | Reg loss: 0.022 | Tree loss: 8.729 | Accuracy: 0.056641 | 5.114 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 026 | Total loss: 8.720 | Reg loss: 0.022 | Tree loss: 8.720 | Accuracy: 0.056641 | 5.111 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 026 | Total loss: 8.711 | Reg loss: 0.023 | Tree loss: 8.711 | Accuracy: 0.062500 | 5.107 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 026 | Total loss: 8.667 | Reg loss: 0.023 | Tree loss: 8.667 | Accuracy: 0.062500 | 5.109 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 026 | Total loss: 8.660 | Reg loss: 0.023 | Tree loss: 8.660 | Accuracy: 0.064453 | 5.112 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 026 | Total loss: 8.643 | Reg loss: 0.024 | Tree loss: 8.643 | Accuracy: 0.058594 | 5.115 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 026 | Total loss: 8.618 | Reg loss: 0.024 | Tree loss: 8.618 | Accuracy: 0.042969 | 5.119 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 026 | Total loss: 8.580 | Reg loss: 0.024 | Tree loss: 8.580 | Accuracy: 0.054688 | 5.122 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 026 | Total loss: 8.551 | Reg loss: 0.025 | Tree loss: 8.551 | Accuracy: 0.056641 | 5.125 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 026 | Total loss: 8.524 | Reg loss: 0.025 | Tree loss: 8.524 | Accuracy: 0.058594 | 5.127 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 026 | Total loss: 8.527 | Reg loss: 0.025 | Tree loss: 8.527 | Accuracy: 0.050781 | 5.128 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 026 | Total loss: 8.500 | Reg loss: 0.026 | Tree loss: 8.500 | Accuracy: 0.032258 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 026 | Total loss: 8.845 | Reg loss: 0.020 | Tree loss: 8.845 | Accuracy: 0.105469 | 5.127 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 026 | Total loss: 8.819 | Reg loss: 0.021 | Tree loss: 8.819 | Accuracy: 0.087891 | 5.127 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 026 | Total loss: 8.818 | Reg loss: 0.021 | Tree loss: 8.818 | Accuracy: 0.072266 | 5.128 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 026 | Total loss: 8.737 | Reg loss: 0.021 | Tree loss: 8.737 | Accuracy: 0.060547 | 5.128 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 026 | Total loss: 8.705 | Reg loss: 0.021 | Tree loss: 8.705 | Accuracy: 0.064453 | 5.129 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 026 | Total loss: 8.744 | Reg loss: 0.021 | Tree loss: 8.744 | Accuracy: 0.048828 | 5.129 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Batch: 006 / 026 | Total loss: 8.710 | Reg loss: 0.021 | Tree loss: 8.710 | Accuracy: 0.056641 | 5.129 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 026 | Total loss: 8.681 | Reg loss: 0.021 | Tree loss: 8.681 | Accuracy: 0.058594 | 5.128 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 026 | Total loss: 8.633 | Reg loss: 0.022 | Tree loss: 8.633 | Accuracy: 0.066406 | 5.124 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 026 | Total loss: 8.592 | Reg loss: 0.022 | Tree loss: 8.592 | Accuracy: 0.064453 | 5.126 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 026 | Total loss: 8.551 | Reg loss: 0.022 | Tree loss: 8.551 | Accuracy: 0.062500 | 5.128 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 026 | Total loss: 8.539 | Reg loss: 0.022 | Tree loss: 8.539 | Accuracy: 0.062500 | 5.13 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 026 | Total loss: 8.552 | Reg loss: 0.023 | Tree loss: 8.552 | Accuracy: 0.066406 | 5.132 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 026 | Total loss: 8.505 | Reg loss: 0.023 | Tree loss: 8.505 | Accuracy: 0.070312 | 5.135 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 026 | Total loss: 8.466 | Reg loss: 0.023 | Tree loss: 8.466 | Accuracy: 0.050781 | 5.136 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 026 | Total loss: 8.451 | Reg loss: 0.023 | Tree loss: 8.451 | Accuracy: 0.068359 | 5.138 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 026 | Total loss: 8.417 | Reg loss: 0.024 | Tree loss: 8.417 | Accuracy: 0.050781 | 5.139 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 026 | Total loss: 8.398 | Reg loss: 0.024 | Tree loss: 8.398 | Accuracy: 0.046875 | 5.14 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 026 | Total loss: 8.390 | Reg loss: 0.024 | Tree loss: 8.390 | Accuracy: 0.070312 | 5.141 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 026 | Total loss: 8.344 | Reg loss: 0.025 | Tree loss: 8.344 | Accuracy: 0.052734 | 5.141 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 026 | Total loss: 8.334 | Reg loss: 0.025 | Tree loss: 8.334 | Accuracy: 0.046875 | 5.141 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 026 | Total loss: 8.302 | Reg loss: 0.025 | Tree loss: 8.302 | Accuracy: 0.058594 | 5.141 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 026 | Total loss: 8.267 | Reg loss: 0.026 | Tree loss: 8.267 | Accuracy: 0.083984 | 5.141 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 026 | Total loss: 8.216 | Reg loss: 0.026 | Tree loss: 8.216 | Accuracy: 0.052734 | 5.14 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 026 | Total loss: 8.254 | Reg loss: 0.026 | Tree loss: 8.254 | Accuracy: 0.048828 | 5.139 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 026 | Total loss: 8.222 | Reg loss: 0.027 | Tree loss: 8.222 | Accuracy: 0.045161 | 5.133 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 026 | Total loss: 8.585 | Reg loss: 0.022 | Tree loss: 8.585 | Accuracy: 0.095703 | 5.14 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 026 | Total loss: 8.530 | Reg loss: 0.022 | Tree loss: 8.530 | Accuracy: 0.072266 | 5.14 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 026 | Total loss: 8.517 | Reg loss: 0.022 | Tree loss: 8.517 | Accuracy: 0.042969 | 5.141 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 026 | Total loss: 8.479 | Reg loss: 0.022 | Tree loss: 8.479 | Accuracy: 0.068359 | 5.141 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 026 | Total loss: 8.455 | Reg loss: 0.022 | Tree loss: 8.455 | Accuracy: 0.044922 | 5.142 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 026 | Total loss: 8.416 | Reg loss: 0.022 | Tree loss: 8.416 | Accuracy: 0.039062 | 5.142 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 026 | Total loss: 8.420 | Reg loss: 0.023 | Tree loss: 8.420 | Accuracy: 0.052734 | 5.142 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 026 | Total loss: 8.380 | Reg loss: 0.023 | Tree loss: 8.380 | Accuracy: 0.066406 | 5.142 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 026 | Total loss: 8.344 | Reg loss: 0.023 | Tree loss: 8.344 | Accuracy: 0.078125 | 5.142 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 026 | Total loss: 8.334 | Reg loss: 0.023 | Tree loss: 8.334 | Accuracy: 0.037109 | 5.142 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 026 | Total loss: 8.321 | Reg loss: 0.023 | Tree loss: 8.321 | Accuracy: 0.060547 | 5.141 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 026 | Total loss: 8.269 | Reg loss: 0.024 | Tree loss: 8.269 | Accuracy: 0.054688 | 5.141 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 026 | Total loss: 8.234 | Reg loss: 0.024 | Tree loss: 8.234 | Accuracy: 0.068359 | 5.139 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 026 | Total loss: 8.227 | Reg loss: 0.024 | Tree loss: 8.227 | Accuracy: 0.074219 | 5.138 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 026 | Total loss: 8.194 | Reg loss: 0.024 | Tree loss: 8.194 | Accuracy: 0.044922 | 5.137 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 026 | Total loss: 8.152 | Reg loss: 0.025 | Tree loss: 8.152 | Accuracy: 0.078125 | 5.135 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 026 | Total loss: 8.143 | Reg loss: 0.025 | Tree loss: 8.143 | Accuracy: 0.056641 | 5.133 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 026 | Total loss: 8.118 | Reg loss: 0.025 | Tree loss: 8.118 | Accuracy: 0.064453 | 5.132 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 026 | Total loss: 8.101 | Reg loss: 0.025 | Tree loss: 8.101 | Accuracy: 0.062500 | 5.13 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 026 | Total loss: 8.069 | Reg loss: 0.026 | Tree loss: 8.069 | Accuracy: 0.062500 | 5.126 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 026 | Total loss: 8.072 | Reg loss: 0.026 | Tree loss: 8.072 | Accuracy: 0.033203 | 5.129 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 026 | Total loss: 8.037 | Reg loss: 0.026 | Tree loss: 8.037 | Accuracy: 0.066406 | 5.131 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 026 | Total loss: 8.012 | Reg loss: 0.026 | Tree loss: 8.012 | Accuracy: 0.064453 | 5.133 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 026 | Total loss: 7.989 | Reg loss: 0.027 | Tree loss: 7.989 | Accuracy: 0.068359 | 5.134 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 026 | Total loss: 7.944 | Reg loss: 0.027 | Tree loss: 7.944 | Accuracy: 0.066406 | 5.134 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 026 | Total loss: 7.944 | Reg loss: 0.027 | Tree loss: 7.944 | Accuracy: 0.064516 | 5.129 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 026 | Total loss: 8.266 | Reg loss: 0.023 | Tree loss: 8.266 | Accuracy: 0.070312 | 5.139 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 026 | Total loss: 8.253 | Reg loss: 0.023 | Tree loss: 8.253 | Accuracy: 0.031250 | 5.138 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 026 | Total loss: 8.225 | Reg loss: 0.023 | Tree loss: 8.225 | Accuracy: 0.068359 | 5.137 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 026 | Total loss: 8.203 | Reg loss: 0.023 | Tree loss: 8.203 | Accuracy: 0.054688 | 5.135 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 026 | Total loss: 8.170 | Reg loss: 0.024 | Tree loss: 8.170 | Accuracy: 0.052734 | 5.134 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 026 | Total loss: 8.152 | Reg loss: 0.024 | Tree loss: 8.152 | Accuracy: 0.062500 | 5.132 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 026 | Total loss: 8.154 | Reg loss: 0.024 | Tree loss: 8.154 | Accuracy: 0.041016 | 5.131 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 026 | Total loss: 8.119 | Reg loss: 0.024 | Tree loss: 8.119 | Accuracy: 0.056641 | 5.129 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 026 | Total loss: 8.076 | Reg loss: 0.024 | Tree loss: 8.076 | Accuracy: 0.060547 | 5.128 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 026 | Total loss: 8.049 | Reg loss: 0.024 | Tree loss: 8.049 | Accuracy: 0.058594 | 5.127 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 026 | Total loss: 8.043 | Reg loss: 0.024 | Tree loss: 8.043 | Accuracy: 0.039062 | 5.124 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 026 | Total loss: 8.039 | Reg loss: 0.025 | Tree loss: 8.039 | Accuracy: 0.062500 | 5.121 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 026 | Total loss: 7.980 | Reg loss: 0.025 | Tree loss: 7.980 | Accuracy: 0.042969 | 5.124 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 026 | Total loss: 7.925 | Reg loss: 0.025 | Tree loss: 7.925 | Accuracy: 0.072266 | 5.126 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 026 | Total loss: 7.941 | Reg loss: 0.025 | Tree loss: 7.941 | Accuracy: 0.068359 | 5.127 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batch: 015 / 026 | Total loss: 7.885 | Reg loss: 0.025 | Tree loss: 7.885 | Accuracy: 0.062500 | 5.128 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 026 | Total loss: 7.870 | Reg loss: 0.026 | Tree loss: 7.870 | Accuracy: 0.064453 | 5.128 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 026 | Total loss: 7.841 | Reg loss: 0.026 | Tree loss: 7.841 | Accuracy: 0.056641 | 5.128 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 026 | Total loss: 7.794 | Reg loss: 0.026 | Tree loss: 7.794 | Accuracy: 0.066406 | 5.127 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 026 | Total loss: 7.786 | Reg loss: 0.026 | Tree loss: 7.786 | Accuracy: 0.054688 | 5.126 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 026 | Total loss: 7.769 | Reg loss: 0.026 | Tree loss: 7.769 | Accuracy: 0.068359 | 5.125 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 026 | Total loss: 7.702 | Reg loss: 0.027 | Tree loss: 7.702 | Accuracy: 0.072266 | 5.124 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 026 | Total loss: 7.736 | Reg loss: 0.027 | Tree loss: 7.736 | Accuracy: 0.056641 | 5.123 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 026 | Total loss: 7.722 | Reg loss: 0.027 | Tree loss: 7.722 | Accuracy: 0.054688 | 5.122 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 026 | Total loss: 7.679 | Reg loss: 0.027 | Tree loss: 7.679 | Accuracy: 0.050781 | 5.121 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 026 | Total loss: 7.695 | Reg loss: 0.028 | Tree loss: 7.695 | Accuracy: 0.064516 | 5.116 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 026 | Total loss: 8.019 | Reg loss: 0.024 | Tree loss: 8.019 | Accuracy: 0.048828 | 5.127 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 026 | Total loss: 7.956 | Reg loss: 0.024 | Tree loss: 7.956 | Accuracy: 0.056641 | 5.129 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 026 | Total loss: 7.931 | Reg loss: 0.024 | Tree loss: 7.931 | Accuracy: 0.054688 | 5.126 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 026 | Total loss: 7.926 | Reg loss: 0.024 | Tree loss: 7.926 | Accuracy: 0.068359 | 5.127 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 026 | Total loss: 7.876 | Reg loss: 0.025 | Tree loss: 7.876 | Accuracy: 0.070312 | 5.128 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 026 | Total loss: 7.868 | Reg loss: 0.025 | Tree loss: 7.868 | Accuracy: 0.060547 | 5.129 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 026 | Total loss: 7.888 | Reg loss: 0.025 | Tree loss: 7.888 | Accuracy: 0.042969 | 5.13 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 026 | Total loss: 7.835 | Reg loss: 0.025 | Tree loss: 7.835 | Accuracy: 0.052734 | 5.131 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 026 | Total loss: 7.821 | Reg loss: 0.025 | Tree loss: 7.821 | Accuracy: 0.062500 | 5.132 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 026 | Total loss: 7.791 | Reg loss: 0.025 | Tree loss: 7.791 | Accuracy: 0.070312 | 5.133 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 026 | Total loss: 7.765 | Reg loss: 0.025 | Tree loss: 7.765 | Accuracy: 0.062500 | 5.133 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 026 | Total loss: 7.725 | Reg loss: 0.025 | Tree loss: 7.725 | Accuracy: 0.052734 | 5.134 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 026 | Total loss: 7.708 | Reg loss: 0.026 | Tree loss: 7.708 | Accuracy: 0.060547 | 5.134 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 026 | Total loss: 7.668 | Reg loss: 0.026 | Tree loss: 7.668 | Accuracy: 0.064453 | 5.134 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 026 | Total loss: 7.654 | Reg loss: 0.026 | Tree loss: 7.654 | Accuracy: 0.046875 | 5.134 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 026 | Total loss: 7.608 | Reg loss: 0.026 | Tree loss: 7.608 | Accuracy: 0.052734 | 5.133 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 026 | Total loss: 7.607 | Reg loss: 0.026 | Tree loss: 7.607 | Accuracy: 0.044922 | 5.133 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 026 | Total loss: 7.571 | Reg loss: 0.026 | Tree loss: 7.571 | Accuracy: 0.066406 | 5.132 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 026 | Total loss: 7.546 | Reg loss: 0.027 | Tree loss: 7.546 | Accuracy: 0.054688 | 5.131 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 026 | Total loss: 7.530 | Reg loss: 0.027 | Tree loss: 7.530 | Accuracy: 0.054688 | 5.13 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 026 | Total loss: 7.523 | Reg loss: 0.027 | Tree loss: 7.523 | Accuracy: 0.050781 | 5.127 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 026 | Total loss: 7.462 | Reg loss: 0.027 | Tree loss: 7.462 | Accuracy: 0.066406 | 5.124 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 026 | Total loss: 7.437 | Reg loss: 0.027 | Tree loss: 7.437 | Accuracy: 0.062500 | 5.127 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 026 | Total loss: 7.420 | Reg loss: 0.028 | Tree loss: 7.420 | Accuracy: 0.060547 | 5.128 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 026 | Total loss: 7.459 | Reg loss: 0.028 | Tree loss: 7.459 | Accuracy: 0.060547 | 5.129 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 026 | Total loss: 7.413 | Reg loss: 0.028 | Tree loss: 7.413 | Accuracy: 0.051613 | 5.124 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 026 | Total loss: 7.702 | Reg loss: 0.025 | Tree loss: 7.702 | Accuracy: 0.062500 | 5.128 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 026 | Total loss: 7.675 | Reg loss: 0.025 | Tree loss: 7.675 | Accuracy: 0.074219 | 5.128 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 026 | Total loss: 7.674 | Reg loss: 0.025 | Tree loss: 7.674 | Accuracy: 0.068359 | 5.128 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 026 | Total loss: 7.622 | Reg loss: 0.025 | Tree loss: 7.622 | Accuracy: 0.060547 | 5.128 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 026 | Total loss: 7.623 | Reg loss: 0.025 | Tree loss: 7.623 | Accuracy: 0.064453 | 5.128 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 026 | Total loss: 7.592 | Reg loss: 0.025 | Tree loss: 7.592 | Accuracy: 0.048828 | 5.128 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 026 | Total loss: 7.608 | Reg loss: 0.026 | Tree loss: 7.608 | Accuracy: 0.050781 | 5.127 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 026 | Total loss: 7.561 | Reg loss: 0.026 | Tree loss: 7.561 | Accuracy: 0.052734 | 5.126 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 026 | Total loss: 7.553 | Reg loss: 0.026 | Tree loss: 7.553 | Accuracy: 0.060547 | 5.125 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 026 | Total loss: 7.527 | Reg loss: 0.026 | Tree loss: 7.527 | Accuracy: 0.050781 | 5.125 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 026 | Total loss: 7.481 | Reg loss: 0.026 | Tree loss: 7.481 | Accuracy: 0.060547 | 5.124 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 026 | Total loss: 7.446 | Reg loss: 0.026 | Tree loss: 7.446 | Accuracy: 0.062500 | 5.123 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 026 | Total loss: 7.405 | Reg loss: 0.026 | Tree loss: 7.405 | Accuracy: 0.052734 | 5.121 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 026 | Total loss: 7.436 | Reg loss: 0.026 | Tree loss: 7.436 | Accuracy: 0.048828 | 5.118 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 026 | Total loss: 7.389 | Reg loss: 0.026 | Tree loss: 7.389 | Accuracy: 0.048828 | 5.12 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 026 | Total loss: 7.364 | Reg loss: 0.027 | Tree loss: 7.364 | Accuracy: 0.046875 | 5.121 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 026 | Total loss: 7.333 | Reg loss: 0.027 | Tree loss: 7.333 | Accuracy: 0.052734 | 5.123 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 026 | Total loss: 7.308 | Reg loss: 0.027 | Tree loss: 7.308 | Accuracy: 0.072266 | 5.125 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 026 | Total loss: 7.299 | Reg loss: 0.027 | Tree loss: 7.299 | Accuracy: 0.054688 | 5.126 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 026 | Total loss: 7.303 | Reg loss: 0.027 | Tree loss: 7.303 | Accuracy: 0.058594 | 5.128 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 026 | Total loss: 7.256 | Reg loss: 0.027 | Tree loss: 7.256 | Accuracy: 0.060547 | 5.13 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 026 | Total loss: 7.225 | Reg loss: 0.028 | Tree loss: 7.225 | Accuracy: 0.056641 | 5.131 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 026 | Total loss: 7.223 | Reg loss: 0.028 | Tree loss: 7.223 | Accuracy: 0.056641 | 5.132 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 026 | Total loss: 7.208 | Reg loss: 0.028 | Tree loss: 7.208 | Accuracy: 0.058594 | 5.133 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 024 / 026 | Total loss: 7.177 | Reg loss: 0.028 | Tree loss: 7.177 | Accuracy: 0.054688 | 5.133 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 026 | Total loss: 7.139 | Reg loss: 0.028 | Tree loss: 7.139 | Accuracy: 0.083871 | 5.129 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 026 | Total loss: 7.455 | Reg loss: 0.026 | Tree loss: 7.455 | Accuracy: 0.060547 | 5.136 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 026 | Total loss: 7.448 | Reg loss: 0.026 | Tree loss: 7.448 | Accuracy: 0.054688 | 5.137 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 026 | Total loss: 7.385 | Reg loss: 0.026 | Tree loss: 7.385 | Accuracy: 0.046875 | 5.138 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 026 | Total loss: 7.378 | Reg loss: 0.026 | Tree loss: 7.378 | Accuracy: 0.058594 | 5.139 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 026 | Total loss: 7.345 | Reg loss: 0.026 | Tree loss: 7.345 | Accuracy: 0.052734 | 5.14 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 026 | Total loss: 7.304 | Reg loss: 0.026 | Tree loss: 7.304 | Accuracy: 0.039062 | 5.138 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 026 | Total loss: 7.325 | Reg loss: 0.026 | Tree loss: 7.325 | Accuracy: 0.062500 | 5.139 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 026 | Total loss: 7.271 | Reg loss: 0.026 | Tree loss: 7.271 | Accuracy: 0.070312 | 5.14 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 026 | Total loss: 7.257 | Reg loss: 0.026 | Tree loss: 7.257 | Accuracy: 0.074219 | 5.141 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 026 | Total loss: 7.240 | Reg loss: 0.026 | Tree loss: 7.240 | Accuracy: 0.060547 | 5.143 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 026 | Total loss: 7.239 | Reg loss: 0.027 | Tree loss: 7.239 | Accuracy: 0.041016 | 5.144 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 026 | Total loss: 7.195 | Reg loss: 0.027 | Tree loss: 7.195 | Accuracy: 0.056641 | 5.145 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 026 | Total loss: 7.203 | Reg loss: 0.027 | Tree loss: 7.203 | Accuracy: 0.046875 | 5.146 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 026 | Total loss: 7.140 | Reg loss: 0.027 | Tree loss: 7.140 | Accuracy: 0.074219 | 5.146 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 026 | Total loss: 7.144 | Reg loss: 0.027 | Tree loss: 7.144 | Accuracy: 0.062500 | 5.147 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 026 | Total loss: 7.110 | Reg loss: 0.027 | Tree loss: 7.110 | Accuracy: 0.046875 | 5.147 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 026 | Total loss: 7.085 | Reg loss: 0.027 | Tree loss: 7.085 | Accuracy: 0.062500 | 5.147 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 026 | Total loss: 7.076 | Reg loss: 0.027 | Tree loss: 7.076 | Accuracy: 0.052734 | 5.147 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 026 | Total loss: 7.048 | Reg loss: 0.028 | Tree loss: 7.048 | Accuracy: 0.058594 | 5.147 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 026 | Total loss: 7.026 | Reg loss: 0.028 | Tree loss: 7.026 | Accuracy: 0.056641 | 5.146 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 026 | Total loss: 7.015 | Reg loss: 0.028 | Tree loss: 7.015 | Accuracy: 0.070312 | 5.145 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 026 | Total loss: 7.008 | Reg loss: 0.028 | Tree loss: 7.008 | Accuracy: 0.044922 | 5.144 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 026 | Total loss: 6.982 | Reg loss: 0.028 | Tree loss: 6.982 | Accuracy: 0.074219 | 5.143 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 026 | Total loss: 6.949 | Reg loss: 0.028 | Tree loss: 6.949 | Accuracy: 0.056641 | 5.141 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 026 | Total loss: 6.949 | Reg loss: 0.028 | Tree loss: 6.949 | Accuracy: 0.064453 | 5.138 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 026 | Total loss: 6.939 | Reg loss: 0.029 | Tree loss: 6.939 | Accuracy: 0.051613 | 5.134 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 026 | Total loss: 7.182 | Reg loss: 0.027 | Tree loss: 7.182 | Accuracy: 0.066406 | 5.139 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 026 | Total loss: 7.180 | Reg loss: 0.027 | Tree loss: 7.180 | Accuracy: 0.056641 | 5.14 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 026 | Total loss: 7.147 | Reg loss: 0.027 | Tree loss: 7.147 | Accuracy: 0.072266 | 5.14 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 026 | Total loss: 7.113 | Reg loss: 0.027 | Tree loss: 7.113 | Accuracy: 0.044922 | 5.14 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 026 | Total loss: 7.077 | Reg loss: 0.027 | Tree loss: 7.077 | Accuracy: 0.068359 | 5.139 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 026 | Total loss: 7.082 | Reg loss: 0.027 | Tree loss: 7.082 | Accuracy: 0.058594 | 5.139 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 026 | Total loss: 7.054 | Reg loss: 0.027 | Tree loss: 7.054 | Accuracy: 0.046875 | 5.138 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 026 | Total loss: 7.024 | Reg loss: 0.027 | Tree loss: 7.024 | Accuracy: 0.062500 | 5.137 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 026 | Total loss: 7.002 | Reg loss: 0.027 | Tree loss: 7.002 | Accuracy: 0.070312 | 5.136 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 026 | Total loss: 7.001 | Reg loss: 0.027 | Tree loss: 7.001 | Accuracy: 0.058594 | 5.135 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 026 | Total loss: 6.938 | Reg loss: 0.027 | Tree loss: 6.938 | Accuracy: 0.054688 | 5.135 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 026 | Total loss: 6.944 | Reg loss: 0.027 | Tree loss: 6.944 | Accuracy: 0.064453 | 5.134 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 026 | Total loss: 6.911 | Reg loss: 0.027 | Tree loss: 6.911 | Accuracy: 0.046875 | 5.133 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 026 | Total loss: 6.903 | Reg loss: 0.027 | Tree loss: 6.903 | Accuracy: 0.050781 | 5.132 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 026 | Total loss: 6.908 | Reg loss: 0.028 | Tree loss: 6.908 | Accuracy: 0.050781 | 5.131 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 026 | Total loss: 6.869 | Reg loss: 0.028 | Tree loss: 6.869 | Accuracy: 0.078125 | 5.129 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 026 | Total loss: 6.860 | Reg loss: 0.028 | Tree loss: 6.860 | Accuracy: 0.058594 | 5.127 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 026 | Total loss: 6.855 | Reg loss: 0.028 | Tree loss: 6.855 | Accuracy: 0.068359 | 5.129 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 026 | Total loss: 6.835 | Reg loss: 0.028 | Tree loss: 6.835 | Accuracy: 0.054688 | 5.131 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 026 | Total loss: 6.805 | Reg loss: 0.028 | Tree loss: 6.805 | Accuracy: 0.064453 | 5.132 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 026 | Total loss: 6.771 | Reg loss: 0.028 | Tree loss: 6.771 | Accuracy: 0.066406 | 5.133 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 026 | Total loss: 6.753 | Reg loss: 0.028 | Tree loss: 6.753 | Accuracy: 0.039062 | 5.134 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 026 | Total loss: 6.744 | Reg loss: 0.028 | Tree loss: 6.744 | Accuracy: 0.048828 | 5.134 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 026 | Total loss: 6.736 | Reg loss: 0.029 | Tree loss: 6.736 | Accuracy: 0.064453 | 5.134 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 026 | Total loss: 6.736 | Reg loss: 0.029 | Tree loss: 6.736 | Accuracy: 0.041016 | 5.134 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 026 | Total loss: 6.706 | Reg loss: 0.029 | Tree loss: 6.706 | Accuracy: 0.025806 | 5.13 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 026 | Total loss: 6.934 | Reg loss: 0.027 | Tree loss: 6.934 | Accuracy: 0.044922 | 5.138 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 026 | Total loss: 6.937 | Reg loss: 0.027 | Tree loss: 6.937 | Accuracy: 0.042969 | 5.138 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 026 | Total loss: 6.868 | Reg loss: 0.027 | Tree loss: 6.868 | Accuracy: 0.046875 | 5.138 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 026 | Total loss: 6.877 | Reg loss: 0.027 | Tree loss: 6.877 | Accuracy: 0.078125 | 5.138 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 004 / 026 | Total loss: 6.857 | Reg loss: 0.027 | Tree loss: 6.857 | Accuracy: 0.054688 | 5.138 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 026 | Total loss: 6.829 | Reg loss: 0.027 | Tree loss: 6.829 | Accuracy: 0.054688 | 5.137 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 026 | Total loss: 6.799 | Reg loss: 0.027 | Tree loss: 6.799 | Accuracy: 0.076172 | 5.137 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 026 | Total loss: 6.791 | Reg loss: 0.027 | Tree loss: 6.791 | Accuracy: 0.046875 | 5.135 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 026 | Total loss: 6.756 | Reg loss: 0.028 | Tree loss: 6.756 | Accuracy: 0.062500 | 5.132 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 026 | Total loss: 6.788 | Reg loss: 0.028 | Tree loss: 6.788 | Accuracy: 0.066406 | 5.134 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 026 | Total loss: 6.704 | Reg loss: 0.028 | Tree loss: 6.704 | Accuracy: 0.058594 | 5.135 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 026 | Total loss: 6.713 | Reg loss: 0.028 | Tree loss: 6.713 | Accuracy: 0.054688 | 5.135 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 026 | Total loss: 6.709 | Reg loss: 0.028 | Tree loss: 6.709 | Accuracy: 0.056641 | 5.135 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 026 | Total loss: 6.717 | Reg loss: 0.028 | Tree loss: 6.717 | Accuracy: 0.039062 | 5.134 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 026 | Total loss: 6.618 | Reg loss: 0.028 | Tree loss: 6.618 | Accuracy: 0.066406 | 5.133 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 026 | Total loss: 6.625 | Reg loss: 0.028 | Tree loss: 6.625 | Accuracy: 0.076172 | 5.133 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 026 | Total loss: 6.632 | Reg loss: 0.028 | Tree loss: 6.632 | Accuracy: 0.060547 | 5.132 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 026 | Total loss: 6.605 | Reg loss: 0.028 | Tree loss: 6.605 | Accuracy: 0.052734 | 5.131 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 026 | Total loss: 6.588 | Reg loss: 0.028 | Tree loss: 6.588 | Accuracy: 0.042969 | 5.13 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 026 | Total loss: 6.561 | Reg loss: 0.028 | Tree loss: 6.561 | Accuracy: 0.062500 | 5.129 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 026 | Total loss: 6.540 | Reg loss: 0.029 | Tree loss: 6.540 | Accuracy: 0.066406 | 5.128 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 026 | Total loss: 6.536 | Reg loss: 0.029 | Tree loss: 6.536 | Accuracy: 0.052734 | 5.127 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 026 | Total loss: 6.520 | Reg loss: 0.029 | Tree loss: 6.520 | Accuracy: 0.078125 | 5.127 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 026 | Total loss: 6.533 | Reg loss: 0.029 | Tree loss: 6.533 | Accuracy: 0.050781 | 5.126 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 026 | Total loss: 6.462 | Reg loss: 0.029 | Tree loss: 6.462 | Accuracy: 0.060547 | 5.125 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 026 | Total loss: 6.492 | Reg loss: 0.029 | Tree loss: 6.492 | Accuracy: 0.038710 | 5.121 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 026 | Total loss: 6.652 | Reg loss: 0.028 | Tree loss: 6.652 | Accuracy: 0.070312 | 5.122 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 026 | Total loss: 6.669 | Reg loss: 0.028 | Tree loss: 6.669 | Accuracy: 0.042969 | 5.123 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 026 | Total loss: 6.618 | Reg loss: 0.028 | Tree loss: 6.618 | Accuracy: 0.058594 | 5.124 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 026 | Total loss: 6.626 | Reg loss: 0.028 | Tree loss: 6.626 | Accuracy: 0.074219 | 5.126 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 026 | Total loss: 6.614 | Reg loss: 0.028 | Tree loss: 6.614 | Accuracy: 0.056641 | 5.126 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 026 | Total loss: 6.578 | Reg loss: 0.028 | Tree loss: 6.578 | Accuracy: 0.052734 | 5.127 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 026 | Total loss: 6.563 | Reg loss: 0.028 | Tree loss: 6.563 | Accuracy: 0.044922 | 5.127 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 026 | Total loss: 6.570 | Reg loss: 0.028 | Tree loss: 6.570 | Accuracy: 0.056641 | 5.127 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 026 | Total loss: 6.546 | Reg loss: 0.028 | Tree loss: 6.546 | Accuracy: 0.050781 | 5.127 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 026 | Total loss: 6.525 | Reg loss: 0.028 | Tree loss: 6.525 | Accuracy: 0.048828 | 5.126 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 026 | Total loss: 6.529 | Reg loss: 0.028 | Tree loss: 6.529 | Accuracy: 0.056641 | 5.125 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 026 | Total loss: 6.482 | Reg loss: 0.028 | Tree loss: 6.482 | Accuracy: 0.052734 | 5.125 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 026 | Total loss: 6.463 | Reg loss: 0.028 | Tree loss: 6.463 | Accuracy: 0.068359 | 5.124 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 026 | Total loss: 6.454 | Reg loss: 0.028 | Tree loss: 6.454 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 026 | Total loss: 6.406 | Reg loss: 0.028 | Tree loss: 6.406 | Accuracy: 0.052734 | 5.122 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 026 | Total loss: 6.416 | Reg loss: 0.028 | Tree loss: 6.416 | Accuracy: 0.062500 | 5.122 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 026 | Total loss: 6.410 | Reg loss: 0.029 | Tree loss: 6.410 | Accuracy: 0.066406 | 5.121 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 026 | Total loss: 6.360 | Reg loss: 0.029 | Tree loss: 6.360 | Accuracy: 0.078125 | 5.12 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 026 | Total loss: 6.376 | Reg loss: 0.029 | Tree loss: 6.376 | Accuracy: 0.046875 | 5.117 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 026 | Total loss: 6.357 | Reg loss: 0.029 | Tree loss: 6.357 | Accuracy: 0.054688 | 5.119 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 026 | Total loss: 6.364 | Reg loss: 0.029 | Tree loss: 6.364 | Accuracy: 0.060547 | 5.121 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 026 | Total loss: 6.331 | Reg loss: 0.029 | Tree loss: 6.331 | Accuracy: 0.058594 | 5.122 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 026 | Total loss: 6.315 | Reg loss: 0.029 | Tree loss: 6.315 | Accuracy: 0.044922 | 5.123 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 026 | Total loss: 6.300 | Reg loss: 0.029 | Tree loss: 6.300 | Accuracy: 0.052734 | 5.123 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 026 | Total loss: 6.255 | Reg loss: 0.029 | Tree loss: 6.255 | Accuracy: 0.058594 | 5.123 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 026 | Total loss: 6.297 | Reg loss: 0.029 | Tree loss: 6.297 | Accuracy: 0.051613 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 026 | Total loss: 6.436 | Reg loss: 0.028 | Tree loss: 6.436 | Accuracy: 0.074219 | 5.127 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 026 | Total loss: 6.426 | Reg loss: 0.028 | Tree loss: 6.426 | Accuracy: 0.060547 | 5.126 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 026 | Total loss: 6.414 | Reg loss: 0.028 | Tree loss: 6.414 | Accuracy: 0.060547 | 5.126 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 026 | Total loss: 6.385 | Reg loss: 0.028 | Tree loss: 6.385 | Accuracy: 0.050781 | 5.125 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 026 | Total loss: 6.390 | Reg loss: 0.028 | Tree loss: 6.390 | Accuracy: 0.044922 | 5.125 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 026 | Total loss: 6.325 | Reg loss: 0.028 | Tree loss: 6.325 | Accuracy: 0.062500 | 5.124 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 026 | Total loss: 6.368 | Reg loss: 0.028 | Tree loss: 6.368 | Accuracy: 0.052734 | 5.123 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 026 | Total loss: 6.341 | Reg loss: 0.028 | Tree loss: 6.341 | Accuracy: 0.054688 | 5.122 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 026 | Total loss: 6.334 | Reg loss: 0.028 | Tree loss: 6.334 | Accuracy: 0.044922 | 5.121 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 026 | Total loss: 6.282 | Reg loss: 0.028 | Tree loss: 6.282 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 026 | Total loss: 6.278 | Reg loss: 0.028 | Tree loss: 6.278 | Accuracy: 0.054688 | 5.117 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 026 | Total loss: 6.270 | Reg loss: 0.029 | Tree loss: 6.270 | Accuracy: 0.064453 | 5.119 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 026 | Total loss: 6.249 | Reg loss: 0.029 | Tree loss: 6.249 | Accuracy: 0.064453 | 5.12 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 013 / 026 | Total loss: 6.216 | Reg loss: 0.029 | Tree loss: 6.216 | Accuracy: 0.062500 | 5.121 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 026 | Total loss: 6.201 | Reg loss: 0.029 | Tree loss: 6.201 | Accuracy: 0.072266 | 5.122 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 026 | Total loss: 6.231 | Reg loss: 0.029 | Tree loss: 6.231 | Accuracy: 0.068359 | 5.122 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 026 | Total loss: 6.177 | Reg loss: 0.029 | Tree loss: 6.177 | Accuracy: 0.062500 | 5.122 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 026 | Total loss: 6.181 | Reg loss: 0.029 | Tree loss: 6.181 | Accuracy: 0.062500 | 5.121 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 026 | Total loss: 6.167 | Reg loss: 0.029 | Tree loss: 6.167 | Accuracy: 0.048828 | 5.121 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 026 | Total loss: 6.141 | Reg loss: 0.029 | Tree loss: 6.141 | Accuracy: 0.050781 | 5.12 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 026 | Total loss: 6.124 | Reg loss: 0.029 | Tree loss: 6.124 | Accuracy: 0.044922 | 5.12 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 026 | Total loss: 6.150 | Reg loss: 0.029 | Tree loss: 6.150 | Accuracy: 0.052734 | 5.119 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 026 | Total loss: 6.090 | Reg loss: 0.029 | Tree loss: 6.090 | Accuracy: 0.052734 | 5.118 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 026 | Total loss: 6.073 | Reg loss: 0.029 | Tree loss: 6.073 | Accuracy: 0.064453 | 5.118 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 026 | Total loss: 6.067 | Reg loss: 0.029 | Tree loss: 6.067 | Accuracy: 0.039062 | 5.117 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 026 | Total loss: 6.022 | Reg loss: 0.030 | Tree loss: 6.022 | Accuracy: 0.064516 | 5.114 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 026 | Total loss: 6.198 | Reg loss: 0.028 | Tree loss: 6.198 | Accuracy: 0.054688 | 5.121 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 026 | Total loss: 6.209 | Reg loss: 0.029 | Tree loss: 6.209 | Accuracy: 0.042969 | 5.119 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 026 | Total loss: 6.202 | Reg loss: 0.029 | Tree loss: 6.202 | Accuracy: 0.050781 | 5.12 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 026 | Total loss: 6.167 | Reg loss: 0.029 | Tree loss: 6.167 | Accuracy: 0.054688 | 5.12 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 026 | Total loss: 6.160 | Reg loss: 0.029 | Tree loss: 6.160 | Accuracy: 0.056641 | 5.121 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 026 | Total loss: 6.176 | Reg loss: 0.029 | Tree loss: 6.176 | Accuracy: 0.048828 | 5.121 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 026 | Total loss: 6.113 | Reg loss: 0.029 | Tree loss: 6.113 | Accuracy: 0.080078 | 5.122 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 026 | Total loss: 6.110 | Reg loss: 0.029 | Tree loss: 6.110 | Accuracy: 0.054688 | 5.122 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 026 | Total loss: 6.087 | Reg loss: 0.029 | Tree loss: 6.087 | Accuracy: 0.035156 | 5.123 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 026 | Total loss: 6.105 | Reg loss: 0.029 | Tree loss: 6.105 | Accuracy: 0.052734 | 5.123 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 026 | Total loss: 6.044 | Reg loss: 0.029 | Tree loss: 6.044 | Accuracy: 0.074219 | 5.123 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 026 | Total loss: 6.083 | Reg loss: 0.029 | Tree loss: 6.083 | Accuracy: 0.042969 | 5.123 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 026 | Total loss: 6.036 | Reg loss: 0.029 | Tree loss: 6.036 | Accuracy: 0.078125 | 5.123 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 026 | Total loss: 6.045 | Reg loss: 0.029 | Tree loss: 6.045 | Accuracy: 0.062500 | 5.123 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 026 | Total loss: 6.009 | Reg loss: 0.029 | Tree loss: 6.009 | Accuracy: 0.058594 | 5.122 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 026 | Total loss: 6.011 | Reg loss: 0.029 | Tree loss: 6.011 | Accuracy: 0.041016 | 5.122 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 026 | Total loss: 5.949 | Reg loss: 0.029 | Tree loss: 5.949 | Accuracy: 0.076172 | 5.121 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 026 | Total loss: 5.964 | Reg loss: 0.029 | Tree loss: 5.964 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 026 | Total loss: 5.958 | Reg loss: 0.029 | Tree loss: 5.958 | Accuracy: 0.070312 | 5.119 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 026 | Total loss: 5.934 | Reg loss: 0.029 | Tree loss: 5.934 | Accuracy: 0.080078 | 5.118 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 026 | Total loss: 5.914 | Reg loss: 0.029 | Tree loss: 5.914 | Accuracy: 0.060547 | 5.116 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 026 | Total loss: 5.919 | Reg loss: 0.029 | Tree loss: 5.919 | Accuracy: 0.039062 | 5.118 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 026 | Total loss: 5.885 | Reg loss: 0.030 | Tree loss: 5.885 | Accuracy: 0.054688 | 5.119 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 026 | Total loss: 5.873 | Reg loss: 0.030 | Tree loss: 5.873 | Accuracy: 0.064453 | 5.12 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 026 | Total loss: 5.881 | Reg loss: 0.030 | Tree loss: 5.881 | Accuracy: 0.044922 | 5.12 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 026 | Total loss: 5.865 | Reg loss: 0.030 | Tree loss: 5.865 | Accuracy: 0.038710 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 026 | Total loss: 6.005 | Reg loss: 0.029 | Tree loss: 6.005 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 026 | Total loss: 5.981 | Reg loss: 0.029 | Tree loss: 5.981 | Accuracy: 0.066406 | 5.119 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 026 | Total loss: 5.954 | Reg loss: 0.029 | Tree loss: 5.954 | Accuracy: 0.056641 | 5.118 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 026 | Total loss: 5.956 | Reg loss: 0.029 | Tree loss: 5.956 | Accuracy: 0.066406 | 5.118 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 026 | Total loss: 5.930 | Reg loss: 0.029 | Tree loss: 5.930 | Accuracy: 0.052734 | 5.117 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 026 | Total loss: 5.932 | Reg loss: 0.029 | Tree loss: 5.932 | Accuracy: 0.052734 | 5.116 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 026 | Total loss: 5.921 | Reg loss: 0.029 | Tree loss: 5.921 | Accuracy: 0.048828 | 5.116 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 026 | Total loss: 5.915 | Reg loss: 0.029 | Tree loss: 5.915 | Accuracy: 0.062500 | 5.115 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 026 | Total loss: 5.890 | Reg loss: 0.029 | Tree loss: 5.890 | Accuracy: 0.058594 | 5.114 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 026 | Total loss: 5.888 | Reg loss: 0.029 | Tree loss: 5.888 | Accuracy: 0.050781 | 5.114 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 026 | Total loss: 5.863 | Reg loss: 0.029 | Tree loss: 5.863 | Accuracy: 0.060547 | 5.113 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 026 | Total loss: 5.848 | Reg loss: 0.029 | Tree loss: 5.848 | Accuracy: 0.058594 | 5.112 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 026 | Total loss: 5.848 | Reg loss: 0.029 | Tree loss: 5.848 | Accuracy: 0.048828 | 5.11 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 026 | Total loss: 5.818 | Reg loss: 0.029 | Tree loss: 5.818 | Accuracy: 0.068359 | 5.111 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 026 | Total loss: 5.828 | Reg loss: 0.029 | Tree loss: 5.828 | Accuracy: 0.056641 | 5.113 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 026 | Total loss: 5.818 | Reg loss: 0.029 | Tree loss: 5.818 | Accuracy: 0.058594 | 5.114 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 026 | Total loss: 5.782 | Reg loss: 0.029 | Tree loss: 5.782 | Accuracy: 0.062500 | 5.115 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 026 | Total loss: 5.802 | Reg loss: 0.029 | Tree loss: 5.802 | Accuracy: 0.050781 | 5.115 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 026 | Total loss: 5.736 | Reg loss: 0.030 | Tree loss: 5.736 | Accuracy: 0.062500 | 5.115 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 026 | Total loss: 5.750 | Reg loss: 0.030 | Tree loss: 5.750 | Accuracy: 0.058594 | 5.115 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 026 | Total loss: 5.722 | Reg loss: 0.030 | Tree loss: 5.722 | Accuracy: 0.058594 | 5.114 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 026 | Total loss: 5.742 | Reg loss: 0.030 | Tree loss: 5.742 | Accuracy: 0.041016 | 5.114 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 022 / 026 | Total loss: 5.704 | Reg loss: 0.030 | Tree loss: 5.704 | Accuracy: 0.050781 | 5.113 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 026 | Total loss: 5.688 | Reg loss: 0.030 | Tree loss: 5.688 | Accuracy: 0.052734 | 5.113 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 026 | Total loss: 5.684 | Reg loss: 0.030 | Tree loss: 5.684 | Accuracy: 0.060547 | 5.112 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 026 | Total loss: 5.653 | Reg loss: 0.030 | Tree loss: 5.653 | Accuracy: 0.083871 | 5.109 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 026 | Total loss: 5.825 | Reg loss: 0.029 | Tree loss: 5.825 | Accuracy: 0.068359 | 5.116 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 026 | Total loss: 5.789 | Reg loss: 0.029 | Tree loss: 5.789 | Accuracy: 0.058594 | 5.117 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 026 | Total loss: 5.767 | Reg loss: 0.029 | Tree loss: 5.767 | Accuracy: 0.050781 | 5.118 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 026 | Total loss: 5.756 | Reg loss: 0.029 | Tree loss: 5.756 | Accuracy: 0.046875 | 5.116 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 026 | Total loss: 5.759 | Reg loss: 0.029 | Tree loss: 5.759 | Accuracy: 0.050781 | 5.117 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 026 | Total loss: 5.739 | Reg loss: 0.029 | Tree loss: 5.739 | Accuracy: 0.068359 | 5.117 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 026 | Total loss: 5.725 | Reg loss: 0.029 | Tree loss: 5.725 | Accuracy: 0.062500 | 5.118 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 026 | Total loss: 5.710 | Reg loss: 0.029 | Tree loss: 5.710 | Accuracy: 0.050781 | 5.119 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 026 | Total loss: 5.693 | Reg loss: 0.029 | Tree loss: 5.693 | Accuracy: 0.056641 | 5.12 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 026 | Total loss: 5.693 | Reg loss: 0.029 | Tree loss: 5.693 | Accuracy: 0.074219 | 5.12 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 026 | Total loss: 5.655 | Reg loss: 0.029 | Tree loss: 5.655 | Accuracy: 0.044922 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 026 | Total loss: 5.639 | Reg loss: 0.029 | Tree loss: 5.639 | Accuracy: 0.046875 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 026 | Total loss: 5.641 | Reg loss: 0.029 | Tree loss: 5.641 | Accuracy: 0.078125 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 026 | Total loss: 5.618 | Reg loss: 0.030 | Tree loss: 5.618 | Accuracy: 0.060547 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 026 | Total loss: 5.629 | Reg loss: 0.030 | Tree loss: 5.629 | Accuracy: 0.066406 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 026 | Total loss: 5.611 | Reg loss: 0.030 | Tree loss: 5.611 | Accuracy: 0.058594 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 026 | Total loss: 5.600 | Reg loss: 0.030 | Tree loss: 5.600 | Accuracy: 0.041016 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 026 | Total loss: 5.581 | Reg loss: 0.030 | Tree loss: 5.581 | Accuracy: 0.062500 | 5.121 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 026 | Total loss: 5.592 | Reg loss: 0.030 | Tree loss: 5.592 | Accuracy: 0.050781 | 5.12 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 026 | Total loss: 5.573 | Reg loss: 0.030 | Tree loss: 5.573 | Accuracy: 0.068359 | 5.12 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 026 | Total loss: 5.564 | Reg loss: 0.030 | Tree loss: 5.564 | Accuracy: 0.060547 | 5.119 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 026 | Total loss: 5.532 | Reg loss: 0.030 | Tree loss: 5.532 | Accuracy: 0.039062 | 5.118 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 026 | Total loss: 5.510 | Reg loss: 0.030 | Tree loss: 5.510 | Accuracy: 0.056641 | 5.116 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 026 | Total loss: 5.462 | Reg loss: 0.030 | Tree loss: 5.462 | Accuracy: 0.064453 | 5.117 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 026 | Total loss: 5.498 | Reg loss: 0.030 | Tree loss: 5.498 | Accuracy: 0.064453 | 5.118 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 026 | Total loss: 5.523 | Reg loss: 0.030 | Tree loss: 5.523 | Accuracy: 0.045161 | 5.116 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 026 | Total loss: 5.609 | Reg loss: 0.029 | Tree loss: 5.609 | Accuracy: 0.048828 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 026 | Total loss: 5.575 | Reg loss: 0.029 | Tree loss: 5.575 | Accuracy: 0.080078 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 026 | Total loss: 5.589 | Reg loss: 0.029 | Tree loss: 5.589 | Accuracy: 0.062500 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 026 | Total loss: 5.561 | Reg loss: 0.029 | Tree loss: 5.561 | Accuracy: 0.072266 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 026 | Total loss: 5.564 | Reg loss: 0.029 | Tree loss: 5.564 | Accuracy: 0.056641 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 026 | Total loss: 5.516 | Reg loss: 0.029 | Tree loss: 5.516 | Accuracy: 0.058594 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 026 | Total loss: 5.524 | Reg loss: 0.030 | Tree loss: 5.524 | Accuracy: 0.048828 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 026 | Total loss: 5.533 | Reg loss: 0.030 | Tree loss: 5.533 | Accuracy: 0.060547 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 026 | Total loss: 5.537 | Reg loss: 0.030 | Tree loss: 5.537 | Accuracy: 0.044922 | 5.117 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 026 | Total loss: 5.498 | Reg loss: 0.030 | Tree loss: 5.498 | Accuracy: 0.082031 | 5.117 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 026 | Total loss: 5.469 | Reg loss: 0.030 | Tree loss: 5.469 | Accuracy: 0.058594 | 5.116 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 026 | Total loss: 5.458 | Reg loss: 0.030 | Tree loss: 5.458 | Accuracy: 0.052734 | 5.116 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 026 | Total loss: 5.459 | Reg loss: 0.030 | Tree loss: 5.459 | Accuracy: 0.046875 | 5.115 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 026 | Total loss: 5.456 | Reg loss: 0.030 | Tree loss: 5.456 | Accuracy: 0.060547 | 5.114 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 026 | Total loss: 5.442 | Reg loss: 0.030 | Tree loss: 5.442 | Accuracy: 0.041016 | 5.112 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 026 | Total loss: 5.419 | Reg loss: 0.030 | Tree loss: 5.419 | Accuracy: 0.042969 | 5.113 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 026 | Total loss: 5.409 | Reg loss: 0.030 | Tree loss: 5.409 | Accuracy: 0.060547 | 5.115 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 026 | Total loss: 5.392 | Reg loss: 0.030 | Tree loss: 5.392 | Accuracy: 0.072266 | 5.116 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 026 | Total loss: 5.388 | Reg loss: 0.030 | Tree loss: 5.388 | Accuracy: 0.058594 | 5.117 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 026 | Total loss: 5.411 | Reg loss: 0.030 | Tree loss: 5.411 | Accuracy: 0.056641 | 5.118 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 026 | Total loss: 5.364 | Reg loss: 0.030 | Tree loss: 5.364 | Accuracy: 0.050781 | 5.119 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 026 | Total loss: 5.376 | Reg loss: 0.030 | Tree loss: 5.376 | Accuracy: 0.064453 | 5.119 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 026 | Total loss: 5.368 | Reg loss: 0.030 | Tree loss: 5.368 | Accuracy: 0.041016 | 5.12 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 026 | Total loss: 5.342 | Reg loss: 0.030 | Tree loss: 5.342 | Accuracy: 0.062500 | 5.12 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 026 | Total loss: 5.304 | Reg loss: 0.030 | Tree loss: 5.304 | Accuracy: 0.068359 | 5.119 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 026 | Total loss: 5.329 | Reg loss: 0.030 | Tree loss: 5.329 | Accuracy: 0.038710 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 026 | Total loss: 5.403 | Reg loss: 0.030 | Tree loss: 5.403 | Accuracy: 0.062500 | 5.122 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 026 | Total loss: 5.418 | Reg loss: 0.030 | Tree loss: 5.418 | Accuracy: 0.064453 | 5.123 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 002 / 026 | Total loss: 5.382 | Reg loss: 0.030 | Tree loss: 5.382 | Accuracy: 0.062500 | 5.123 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 026 | Total loss: 5.397 | Reg loss: 0.030 | Tree loss: 5.397 | Accuracy: 0.046875 | 5.122 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 026 | Total loss: 5.358 | Reg loss: 0.030 | Tree loss: 5.358 | Accuracy: 0.056641 | 5.122 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 026 | Total loss: 5.359 | Reg loss: 0.030 | Tree loss: 5.359 | Accuracy: 0.054688 | 5.121 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 026 | Total loss: 5.381 | Reg loss: 0.030 | Tree loss: 5.381 | Accuracy: 0.056641 | 5.119 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 026 | Total loss: 5.349 | Reg loss: 0.030 | Tree loss: 5.349 | Accuracy: 0.060547 | 5.12 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 026 | Total loss: 5.358 | Reg loss: 0.030 | Tree loss: 5.358 | Accuracy: 0.044922 | 5.12 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 026 | Total loss: 5.294 | Reg loss: 0.030 | Tree loss: 5.294 | Accuracy: 0.058594 | 5.12 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 026 | Total loss: 5.302 | Reg loss: 0.030 | Tree loss: 5.302 | Accuracy: 0.056641 | 5.12 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 026 | Total loss: 5.273 | Reg loss: 0.030 | Tree loss: 5.273 | Accuracy: 0.056641 | 5.12 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 026 | Total loss: 5.282 | Reg loss: 0.030 | Tree loss: 5.282 | Accuracy: 0.046875 | 5.119 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 026 | Total loss: 5.287 | Reg loss: 0.030 | Tree loss: 5.287 | Accuracy: 0.054688 | 5.119 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 026 | Total loss: 5.264 | Reg loss: 0.030 | Tree loss: 5.264 | Accuracy: 0.068359 | 5.118 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 026 | Total loss: 5.219 | Reg loss: 0.030 | Tree loss: 5.219 | Accuracy: 0.070312 | 5.117 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 026 | Total loss: 5.234 | Reg loss: 0.030 | Tree loss: 5.234 | Accuracy: 0.072266 | 5.117 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 026 | Total loss: 5.203 | Reg loss: 0.030 | Tree loss: 5.203 | Accuracy: 0.046875 | 5.116 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 026 | Total loss: 5.256 | Reg loss: 0.030 | Tree loss: 5.256 | Accuracy: 0.042969 | 5.116 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 026 | Total loss: 5.216 | Reg loss: 0.030 | Tree loss: 5.216 | Accuracy: 0.052734 | 5.115 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 026 | Total loss: 5.184 | Reg loss: 0.030 | Tree loss: 5.184 | Accuracy: 0.080078 | 5.114 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 026 | Total loss: 5.213 | Reg loss: 0.030 | Tree loss: 5.213 | Accuracy: 0.062500 | 5.114 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 026 | Total loss: 5.179 | Reg loss: 0.030 | Tree loss: 5.179 | Accuracy: 0.056641 | 5.113 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 026 | Total loss: 5.176 | Reg loss: 0.030 | Tree loss: 5.176 | Accuracy: 0.064453 | 5.112 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 026 | Total loss: 5.183 | Reg loss: 0.030 | Tree loss: 5.183 | Accuracy: 0.052734 | 5.11 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 026 | Total loss: 5.193 | Reg loss: 0.030 | Tree loss: 5.193 | Accuracy: 0.038710 | 5.108 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 026 | Total loss: 5.248 | Reg loss: 0.030 | Tree loss: 5.248 | Accuracy: 0.058594 | 5.111 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 026 | Total loss: 5.249 | Reg loss: 0.030 | Tree loss: 5.249 | Accuracy: 0.046875 | 5.112 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 026 | Total loss: 5.208 | Reg loss: 0.030 | Tree loss: 5.208 | Accuracy: 0.046875 | 5.112 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 026 | Total loss: 5.226 | Reg loss: 0.030 | Tree loss: 5.226 | Accuracy: 0.066406 | 5.112 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 026 | Total loss: 5.222 | Reg loss: 0.030 | Tree loss: 5.222 | Accuracy: 0.050781 | 5.111 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 026 | Total loss: 5.217 | Reg loss: 0.030 | Tree loss: 5.217 | Accuracy: 0.068359 | 5.111 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 026 | Total loss: 5.193 | Reg loss: 0.030 | Tree loss: 5.193 | Accuracy: 0.052734 | 5.111 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 026 | Total loss: 5.168 | Reg loss: 0.030 | Tree loss: 5.168 | Accuracy: 0.066406 | 5.11 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 026 | Total loss: 5.189 | Reg loss: 0.030 | Tree loss: 5.189 | Accuracy: 0.066406 | 5.11 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 026 | Total loss: 5.145 | Reg loss: 0.030 | Tree loss: 5.145 | Accuracy: 0.058594 | 5.109 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 026 | Total loss: 5.166 | Reg loss: 0.030 | Tree loss: 5.166 | Accuracy: 0.050781 | 5.109 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 026 | Total loss: 5.069 | Reg loss: 0.030 | Tree loss: 5.069 | Accuracy: 0.064453 | 5.108 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 026 | Total loss: 5.131 | Reg loss: 0.030 | Tree loss: 5.131 | Accuracy: 0.060547 | 5.108 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 026 | Total loss: 5.098 | Reg loss: 0.030 | Tree loss: 5.098 | Accuracy: 0.060547 | 5.107 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 026 | Total loss: 5.076 | Reg loss: 0.030 | Tree loss: 5.076 | Accuracy: 0.060547 | 5.107 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 026 | Total loss: 5.095 | Reg loss: 0.030 | Tree loss: 5.095 | Accuracy: 0.060547 | 5.106 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 026 | Total loss: 5.115 | Reg loss: 0.030 | Tree loss: 5.115 | Accuracy: 0.041016 | 5.104 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 026 | Total loss: 5.066 | Reg loss: 0.030 | Tree loss: 5.066 | Accuracy: 0.062500 | 5.105 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 026 | Total loss: 5.056 | Reg loss: 0.030 | Tree loss: 5.056 | Accuracy: 0.058594 | 5.106 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 026 | Total loss: 5.044 | Reg loss: 0.030 | Tree loss: 5.044 | Accuracy: 0.060547 | 5.107 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 026 | Total loss: 5.015 | Reg loss: 0.030 | Tree loss: 5.015 | Accuracy: 0.054688 | 5.107 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 026 | Total loss: 5.016 | Reg loss: 0.030 | Tree loss: 5.016 | Accuracy: 0.052734 | 5.107 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 026 | Total loss: 5.024 | Reg loss: 0.030 | Tree loss: 5.024 | Accuracy: 0.056641 | 5.106 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 026 | Total loss: 5.001 | Reg loss: 0.031 | Tree loss: 5.001 | Accuracy: 0.066406 | 5.106 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 026 | Total loss: 4.978 | Reg loss: 0.031 | Tree loss: 4.978 | Accuracy: 0.058594 | 5.105 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 026 | Total loss: 5.011 | Reg loss: 0.031 | Tree loss: 5.011 | Accuracy: 0.045161 | 5.103 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 026 | Total loss: 5.069 | Reg loss: 0.030 | Tree loss: 5.069 | Accuracy: 0.048828 | 5.109 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 026 | Total loss: 5.080 | Reg loss: 0.030 | Tree loss: 5.080 | Accuracy: 0.044922 | 5.11 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 026 | Total loss: 5.070 | Reg loss: 0.030 | Tree loss: 5.070 | Accuracy: 0.060547 | 5.11 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 026 | Total loss: 5.036 | Reg loss: 0.030 | Tree loss: 5.036 | Accuracy: 0.085938 | 5.111 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 026 | Total loss: 5.003 | Reg loss: 0.030 | Tree loss: 5.003 | Accuracy: 0.068359 | 5.111 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 026 | Total loss: 4.998 | Reg loss: 0.030 | Tree loss: 4.998 | Accuracy: 0.060547 | 5.112 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 026 | Total loss: 5.022 | Reg loss: 0.030 | Tree loss: 5.022 | Accuracy: 0.042969 | 5.112 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 026 | Total loss: 5.038 | Reg loss: 0.030 | Tree loss: 5.038 | Accuracy: 0.041016 | 5.111 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 026 | Total loss: 5.001 | Reg loss: 0.030 | Tree loss: 5.001 | Accuracy: 0.066406 | 5.112 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 026 | Total loss: 4.995 | Reg loss: 0.030 | Tree loss: 4.995 | Accuracy: 0.033203 | 5.112 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 026 | Total loss: 4.990 | Reg loss: 0.030 | Tree loss: 4.990 | Accuracy: 0.062500 | 5.113 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 011 / 026 | Total loss: 5.033 | Reg loss: 0.030 | Tree loss: 5.033 | Accuracy: 0.050781 | 5.114 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 026 | Total loss: 4.968 | Reg loss: 0.030 | Tree loss: 4.968 | Accuracy: 0.060547 | 5.115 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 026 | Total loss: 4.936 | Reg loss: 0.030 | Tree loss: 4.936 | Accuracy: 0.068359 | 5.116 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 026 | Total loss: 4.951 | Reg loss: 0.030 | Tree loss: 4.951 | Accuracy: 0.052734 | 5.117 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 026 | Total loss: 4.927 | Reg loss: 0.030 | Tree loss: 4.927 | Accuracy: 0.062500 | 5.117 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 026 | Total loss: 4.918 | Reg loss: 0.030 | Tree loss: 4.918 | Accuracy: 0.068359 | 5.118 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 026 | Total loss: 4.909 | Reg loss: 0.030 | Tree loss: 4.909 | Accuracy: 0.066406 | 5.118 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 026 | Total loss: 4.905 | Reg loss: 0.030 | Tree loss: 4.905 | Accuracy: 0.058594 | 5.119 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 026 | Total loss: 4.896 | Reg loss: 0.031 | Tree loss: 4.896 | Accuracy: 0.058594 | 5.119 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 026 | Total loss: 4.901 | Reg loss: 0.031 | Tree loss: 4.901 | Accuracy: 0.070312 | 5.119 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 026 | Total loss: 4.865 | Reg loss: 0.031 | Tree loss: 4.865 | Accuracy: 0.050781 | 5.119 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 026 | Total loss: 4.867 | Reg loss: 0.031 | Tree loss: 4.867 | Accuracy: 0.058594 | 5.119 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 026 | Total loss: 4.871 | Reg loss: 0.031 | Tree loss: 4.871 | Accuracy: 0.064453 | 5.118 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 026 | Total loss: 4.831 | Reg loss: 0.031 | Tree loss: 4.831 | Accuracy: 0.048828 | 5.118 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 026 | Total loss: 4.849 | Reg loss: 0.031 | Tree loss: 4.849 | Accuracy: 0.032258 | 5.116 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 026 | Total loss: 4.921 | Reg loss: 0.030 | Tree loss: 4.921 | Accuracy: 0.066406 | 5.116 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 026 | Total loss: 4.925 | Reg loss: 0.030 | Tree loss: 4.925 | Accuracy: 0.050781 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 026 | Total loss: 4.883 | Reg loss: 0.030 | Tree loss: 4.883 | Accuracy: 0.054688 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 026 | Total loss: 4.885 | Reg loss: 0.030 | Tree loss: 4.885 | Accuracy: 0.060547 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 026 | Total loss: 4.911 | Reg loss: 0.030 | Tree loss: 4.911 | Accuracy: 0.054688 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 026 | Total loss: 4.868 | Reg loss: 0.030 | Tree loss: 4.868 | Accuracy: 0.062500 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 026 | Total loss: 4.886 | Reg loss: 0.030 | Tree loss: 4.886 | Accuracy: 0.037109 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 026 | Total loss: 4.845 | Reg loss: 0.030 | Tree loss: 4.845 | Accuracy: 0.078125 | 5.117 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 026 | Total loss: 4.872 | Reg loss: 0.030 | Tree loss: 4.872 | Accuracy: 0.037109 | 5.116 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 026 | Total loss: 4.847 | Reg loss: 0.030 | Tree loss: 4.847 | Accuracy: 0.066406 | 5.116 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 026 | Total loss: 4.804 | Reg loss: 0.030 | Tree loss: 4.804 | Accuracy: 0.070312 | 5.115 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 026 | Total loss: 4.819 | Reg loss: 0.030 | Tree loss: 4.819 | Accuracy: 0.052734 | 5.115 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 026 | Total loss: 4.805 | Reg loss: 0.030 | Tree loss: 4.805 | Accuracy: 0.042969 | 5.114 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 026 | Total loss: 4.796 | Reg loss: 0.030 | Tree loss: 4.796 | Accuracy: 0.056641 | 5.114 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 026 | Total loss: 4.787 | Reg loss: 0.031 | Tree loss: 4.787 | Accuracy: 0.066406 | 5.113 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 026 | Total loss: 4.788 | Reg loss: 0.031 | Tree loss: 4.788 | Accuracy: 0.048828 | 5.113 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 026 | Total loss: 4.769 | Reg loss: 0.031 | Tree loss: 4.769 | Accuracy: 0.062500 | 5.112 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 026 | Total loss: 4.743 | Reg loss: 0.031 | Tree loss: 4.743 | Accuracy: 0.048828 | 5.111 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 026 | Total loss: 4.747 | Reg loss: 0.031 | Tree loss: 4.747 | Accuracy: 0.064453 | 5.11 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 026 | Total loss: 4.765 | Reg loss: 0.031 | Tree loss: 4.765 | Accuracy: 0.054688 | 5.111 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 026 | Total loss: 4.770 | Reg loss: 0.031 | Tree loss: 4.770 | Accuracy: 0.044922 | 5.112 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 026 | Total loss: 4.725 | Reg loss: 0.031 | Tree loss: 4.725 | Accuracy: 0.058594 | 5.113 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 026 | Total loss: 4.719 | Reg loss: 0.031 | Tree loss: 4.719 | Accuracy: 0.064453 | 5.113 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 026 | Total loss: 4.715 | Reg loss: 0.031 | Tree loss: 4.715 | Accuracy: 0.070312 | 5.113 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 026 | Total loss: 4.746 | Reg loss: 0.031 | Tree loss: 4.746 | Accuracy: 0.068359 | 5.113 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 026 | Total loss: 4.760 | Reg loss: 0.031 | Tree loss: 4.760 | Accuracy: 0.070968 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 026 | Total loss: 4.794 | Reg loss: 0.030 | Tree loss: 4.794 | Accuracy: 0.078125 | 5.116 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 026 | Total loss: 4.783 | Reg loss: 0.030 | Tree loss: 4.783 | Accuracy: 0.070312 | 5.116 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 026 | Total loss: 4.760 | Reg loss: 0.030 | Tree loss: 4.760 | Accuracy: 0.056641 | 5.116 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 026 | Total loss: 4.772 | Reg loss: 0.030 | Tree loss: 4.772 | Accuracy: 0.074219 | 5.115 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 026 | Total loss: 4.748 | Reg loss: 0.030 | Tree loss: 4.748 | Accuracy: 0.062500 | 5.115 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 026 | Total loss: 4.710 | Reg loss: 0.030 | Tree loss: 4.710 | Accuracy: 0.039062 | 5.114 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 026 | Total loss: 4.714 | Reg loss: 0.030 | Tree loss: 4.714 | Accuracy: 0.074219 | 5.114 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 026 | Total loss: 4.702 | Reg loss: 0.031 | Tree loss: 4.702 | Accuracy: 0.058594 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 026 | Total loss: 4.693 | Reg loss: 0.031 | Tree loss: 4.693 | Accuracy: 0.054688 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 026 | Total loss: 4.689 | Reg loss: 0.031 | Tree loss: 4.689 | Accuracy: 0.056641 | 5.112 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 026 | Total loss: 4.682 | Reg loss: 0.031 | Tree loss: 4.682 | Accuracy: 0.052734 | 5.11 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 026 | Total loss: 4.666 | Reg loss: 0.031 | Tree loss: 4.666 | Accuracy: 0.062500 | 5.111 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 026 | Total loss: 4.688 | Reg loss: 0.031 | Tree loss: 4.688 | Accuracy: 0.054688 | 5.112 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 026 | Total loss: 4.664 | Reg loss: 0.031 | Tree loss: 4.664 | Accuracy: 0.050781 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 026 | Total loss: 4.633 | Reg loss: 0.031 | Tree loss: 4.633 | Accuracy: 0.042969 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 026 | Total loss: 4.652 | Reg loss: 0.031 | Tree loss: 4.652 | Accuracy: 0.066406 | 5.114 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 026 | Total loss: 4.668 | Reg loss: 0.031 | Tree loss: 4.668 | Accuracy: 0.050781 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 026 | Total loss: 4.635 | Reg loss: 0.031 | Tree loss: 4.635 | Accuracy: 0.050781 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 026 | Total loss: 4.633 | Reg loss: 0.031 | Tree loss: 4.633 | Accuracy: 0.042969 | 5.113 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 026 | Total loss: 4.611 | Reg loss: 0.031 | Tree loss: 4.611 | Accuracy: 0.060547 | 5.112 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 020 / 026 | Total loss: 4.583 | Reg loss: 0.031 | Tree loss: 4.583 | Accuracy: 0.052734 | 5.112 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 026 | Total loss: 4.608 | Reg loss: 0.031 | Tree loss: 4.608 | Accuracy: 0.078125 | 5.111 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 026 | Total loss: 4.588 | Reg loss: 0.031 | Tree loss: 4.588 | Accuracy: 0.066406 | 5.111 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 026 | Total loss: 4.578 | Reg loss: 0.031 | Tree loss: 4.578 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 026 | Total loss: 4.588 | Reg loss: 0.031 | Tree loss: 4.588 | Accuracy: 0.068359 | 5.11 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 026 | Total loss: 4.584 | Reg loss: 0.031 | Tree loss: 4.584 | Accuracy: 0.070968 | 5.108 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 026 | Total loss: 4.664 | Reg loss: 0.031 | Tree loss: 4.664 | Accuracy: 0.109375 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 026 | Total loss: 4.660 | Reg loss: 0.031 | Tree loss: 4.660 | Accuracy: 0.082031 | 5.111 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 026 | Total loss: 4.621 | Reg loss: 0.031 | Tree loss: 4.621 | Accuracy: 0.109375 | 5.112 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 026 | Total loss: 4.622 | Reg loss: 0.031 | Tree loss: 4.622 | Accuracy: 0.097656 | 5.112 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 026 | Total loss: 4.619 | Reg loss: 0.031 | Tree loss: 4.619 | Accuracy: 0.121094 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 026 | Total loss: 4.566 | Reg loss: 0.031 | Tree loss: 4.566 | Accuracy: 0.111328 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 026 | Total loss: 4.619 | Reg loss: 0.031 | Tree loss: 4.619 | Accuracy: 0.095703 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 026 | Total loss: 4.606 | Reg loss: 0.031 | Tree loss: 4.606 | Accuracy: 0.085938 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 026 | Total loss: 4.545 | Reg loss: 0.031 | Tree loss: 4.545 | Accuracy: 0.097656 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 026 | Total loss: 4.564 | Reg loss: 0.031 | Tree loss: 4.564 | Accuracy: 0.095703 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 026 | Total loss: 4.563 | Reg loss: 0.031 | Tree loss: 4.563 | Accuracy: 0.109375 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 026 | Total loss: 4.566 | Reg loss: 0.031 | Tree loss: 4.566 | Accuracy: 0.099609 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 026 | Total loss: 4.530 | Reg loss: 0.031 | Tree loss: 4.530 | Accuracy: 0.083984 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 026 | Total loss: 4.559 | Reg loss: 0.031 | Tree loss: 4.559 | Accuracy: 0.093750 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 026 | Total loss: 4.515 | Reg loss: 0.031 | Tree loss: 4.515 | Accuracy: 0.111328 | 5.115 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 026 | Total loss: 4.488 | Reg loss: 0.031 | Tree loss: 4.488 | Accuracy: 0.060547 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 026 | Total loss: 4.510 | Reg loss: 0.031 | Tree loss: 4.510 | Accuracy: 0.076172 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 026 | Total loss: 4.520 | Reg loss: 0.031 | Tree loss: 4.520 | Accuracy: 0.095703 | 5.114 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 026 | Total loss: 4.466 | Reg loss: 0.031 | Tree loss: 4.466 | Accuracy: 0.074219 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 026 | Total loss: 4.477 | Reg loss: 0.031 | Tree loss: 4.477 | Accuracy: 0.060547 | 5.112 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 026 | Total loss: 4.471 | Reg loss: 0.031 | Tree loss: 4.471 | Accuracy: 0.056641 | 5.111 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 026 | Total loss: 4.467 | Reg loss: 0.031 | Tree loss: 4.467 | Accuracy: 0.060547 | 5.112 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 026 | Total loss: 4.475 | Reg loss: 0.031 | Tree loss: 4.475 | Accuracy: 0.074219 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 026 | Total loss: 4.437 | Reg loss: 0.031 | Tree loss: 4.437 | Accuracy: 0.078125 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 026 | Total loss: 4.429 | Reg loss: 0.031 | Tree loss: 4.429 | Accuracy: 0.076172 | 5.113 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 026 | Total loss: 4.433 | Reg loss: 0.031 | Tree loss: 4.433 | Accuracy: 0.051613 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 026 | Total loss: 4.494 | Reg loss: 0.031 | Tree loss: 4.494 | Accuracy: 0.136719 | 5.115 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 026 | Total loss: 4.509 | Reg loss: 0.031 | Tree loss: 4.509 | Accuracy: 0.091797 | 5.114 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 026 | Total loss: 4.500 | Reg loss: 0.031 | Tree loss: 4.500 | Accuracy: 0.093750 | 5.113 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 026 | Total loss: 4.491 | Reg loss: 0.031 | Tree loss: 4.491 | Accuracy: 0.132812 | 5.113 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 026 | Total loss: 4.485 | Reg loss: 0.031 | Tree loss: 4.485 | Accuracy: 0.107422 | 5.112 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 026 | Total loss: 4.454 | Reg loss: 0.031 | Tree loss: 4.454 | Accuracy: 0.119141 | 5.112 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 026 | Total loss: 4.470 | Reg loss: 0.031 | Tree loss: 4.470 | Accuracy: 0.083984 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 026 | Total loss: 4.428 | Reg loss: 0.031 | Tree loss: 4.428 | Accuracy: 0.107422 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 026 | Total loss: 4.451 | Reg loss: 0.031 | Tree loss: 4.451 | Accuracy: 0.105469 | 5.11 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 026 | Total loss: 4.433 | Reg loss: 0.031 | Tree loss: 4.433 | Accuracy: 0.097656 | 5.11 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 026 | Total loss: 4.465 | Reg loss: 0.031 | Tree loss: 4.465 | Accuracy: 0.078125 | 5.109 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 026 | Total loss: 4.442 | Reg loss: 0.031 | Tree loss: 4.442 | Accuracy: 0.089844 | 5.109 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 026 | Total loss: 4.425 | Reg loss: 0.031 | Tree loss: 4.425 | Accuracy: 0.093750 | 5.107 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 026 | Total loss: 4.411 | Reg loss: 0.031 | Tree loss: 4.411 | Accuracy: 0.107422 | 5.108 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 026 | Total loss: 4.404 | Reg loss: 0.031 | Tree loss: 4.404 | Accuracy: 0.103516 | 5.109 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 026 | Total loss: 4.396 | Reg loss: 0.031 | Tree loss: 4.396 | Accuracy: 0.091797 | 5.11 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 026 | Total loss: 4.359 | Reg loss: 0.031 | Tree loss: 4.359 | Accuracy: 0.109375 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 026 | Total loss: 4.377 | Reg loss: 0.031 | Tree loss: 4.377 | Accuracy: 0.097656 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 026 | Total loss: 4.344 | Reg loss: 0.031 | Tree loss: 4.344 | Accuracy: 0.113281 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 026 | Total loss: 4.356 | Reg loss: 0.031 | Tree loss: 4.356 | Accuracy: 0.089844 | 5.112 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 026 | Total loss: 4.378 | Reg loss: 0.031 | Tree loss: 4.378 | Accuracy: 0.091797 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 026 | Total loss: 4.375 | Reg loss: 0.031 | Tree loss: 4.375 | Accuracy: 0.083984 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 026 | Total loss: 4.356 | Reg loss: 0.031 | Tree loss: 4.356 | Accuracy: 0.082031 | 5.111 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 026 | Total loss: 4.354 | Reg loss: 0.031 | Tree loss: 4.354 | Accuracy: 0.078125 | 5.11 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 026 | Total loss: 4.318 | Reg loss: 0.031 | Tree loss: 4.318 | Accuracy: 0.101562 | 5.11 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 026 | Total loss: 4.304 | Reg loss: 0.031 | Tree loss: 4.304 | Accuracy: 0.096774 | 5.108 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 000 / 026 | Total loss: 4.403 | Reg loss: 0.031 | Tree loss: 4.403 | Accuracy: 0.087891 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 026 | Total loss: 4.361 | Reg loss: 0.031 | Tree loss: 4.361 | Accuracy: 0.099609 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 026 | Total loss: 4.358 | Reg loss: 0.031 | Tree loss: 4.358 | Accuracy: 0.093750 | 5.114 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 026 | Total loss: 4.356 | Reg loss: 0.031 | Tree loss: 4.356 | Accuracy: 0.099609 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 026 | Total loss: 4.362 | Reg loss: 0.031 | Tree loss: 4.362 | Accuracy: 0.080078 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 026 | Total loss: 4.337 | Reg loss: 0.031 | Tree loss: 4.337 | Accuracy: 0.111328 | 5.114 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 026 | Total loss: 4.346 | Reg loss: 0.031 | Tree loss: 4.346 | Accuracy: 0.105469 | 5.114 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 026 | Total loss: 4.344 | Reg loss: 0.031 | Tree loss: 4.344 | Accuracy: 0.089844 | 5.115 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 026 | Total loss: 4.361 | Reg loss: 0.031 | Tree loss: 4.361 | Accuracy: 0.109375 | 5.115 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 026 | Total loss: 4.366 | Reg loss: 0.031 | Tree loss: 4.366 | Accuracy: 0.091797 | 5.116 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 026 | Total loss: 4.302 | Reg loss: 0.031 | Tree loss: 4.302 | Accuracy: 0.107422 | 5.116 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 026 | Total loss: 4.325 | Reg loss: 0.031 | Tree loss: 4.325 | Accuracy: 0.109375 | 5.117 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 026 | Total loss: 4.289 | Reg loss: 0.031 | Tree loss: 4.289 | Accuracy: 0.103516 | 5.117 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 026 | Total loss: 4.282 | Reg loss: 0.031 | Tree loss: 4.282 | Accuracy: 0.085938 | 5.117 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 026 | Total loss: 4.286 | Reg loss: 0.031 | Tree loss: 4.286 | Accuracy: 0.078125 | 5.117 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 026 | Total loss: 4.280 | Reg loss: 0.031 | Tree loss: 4.280 | Accuracy: 0.111328 | 5.117 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 026 | Total loss: 4.259 | Reg loss: 0.031 | Tree loss: 4.259 | Accuracy: 0.123047 | 5.117 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 026 | Total loss: 4.285 | Reg loss: 0.031 | Tree loss: 4.285 | Accuracy: 0.095703 | 5.117 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 026 | Total loss: 4.277 | Reg loss: 0.031 | Tree loss: 4.277 | Accuracy: 0.087891 | 5.116 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 026 | Total loss: 4.239 | Reg loss: 0.031 | Tree loss: 4.239 | Accuracy: 0.117188 | 5.116 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 026 | Total loss: 4.238 | Reg loss: 0.031 | Tree loss: 4.238 | Accuracy: 0.126953 | 5.116 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 026 | Total loss: 4.230 | Reg loss: 0.031 | Tree loss: 4.230 | Accuracy: 0.087891 | 5.115 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 026 | Total loss: 4.237 | Reg loss: 0.031 | Tree loss: 4.237 | Accuracy: 0.093750 | 5.113 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 026 | Total loss: 4.236 | Reg loss: 0.031 | Tree loss: 4.236 | Accuracy: 0.091797 | 5.114 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 026 | Total loss: 4.203 | Reg loss: 0.031 | Tree loss: 4.203 | Accuracy: 0.121094 | 5.115 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 026 | Total loss: 4.259 | Reg loss: 0.031 | Tree loss: 4.259 | Accuracy: 0.096774 | 5.113 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 026 | Total loss: 4.279 | Reg loss: 0.031 | Tree loss: 4.279 | Accuracy: 0.121094 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 026 | Total loss: 4.278 | Reg loss: 0.031 | Tree loss: 4.278 | Accuracy: 0.103516 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 026 | Total loss: 4.273 | Reg loss: 0.031 | Tree loss: 4.273 | Accuracy: 0.076172 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 026 | Total loss: 4.250 | Reg loss: 0.031 | Tree loss: 4.250 | Accuracy: 0.105469 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 026 | Total loss: 4.228 | Reg loss: 0.031 | Tree loss: 4.228 | Accuracy: 0.103516 | 5.115 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 026 | Total loss: 4.261 | Reg loss: 0.031 | Tree loss: 4.261 | Accuracy: 0.093750 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 026 | Total loss: 4.224 | Reg loss: 0.031 | Tree loss: 4.224 | Accuracy: 0.111328 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 026 | Total loss: 4.228 | Reg loss: 0.031 | Tree loss: 4.228 | Accuracy: 0.107422 | 5.114 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 026 | Total loss: 4.201 | Reg loss: 0.031 | Tree loss: 4.201 | Accuracy: 0.101562 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 026 | Total loss: 4.237 | Reg loss: 0.031 | Tree loss: 4.237 | Accuracy: 0.083984 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 026 | Total loss: 4.221 | Reg loss: 0.031 | Tree loss: 4.221 | Accuracy: 0.103516 | 5.112 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 026 | Total loss: 4.195 | Reg loss: 0.031 | Tree loss: 4.195 | Accuracy: 0.107422 | 5.112 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 026 | Total loss: 4.187 | Reg loss: 0.031 | Tree loss: 4.187 | Accuracy: 0.107422 | 5.112 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 026 | Total loss: 4.187 | Reg loss: 0.031 | Tree loss: 4.187 | Accuracy: 0.109375 | 5.111 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 026 | Total loss: 4.227 | Reg loss: 0.031 | Tree loss: 4.227 | Accuracy: 0.093750 | 5.11 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 026 | Total loss: 4.176 | Reg loss: 0.031 | Tree loss: 4.176 | Accuracy: 0.087891 | 5.111 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 026 | Total loss: 4.178 | Reg loss: 0.031 | Tree loss: 4.178 | Accuracy: 0.093750 | 5.111 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 026 | Total loss: 4.162 | Reg loss: 0.031 | Tree loss: 4.162 | Accuracy: 0.099609 | 5.112 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 026 | Total loss: 4.156 | Reg loss: 0.031 | Tree loss: 4.156 | Accuracy: 0.115234 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 026 | Total loss: 4.127 | Reg loss: 0.031 | Tree loss: 4.127 | Accuracy: 0.113281 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 026 | Total loss: 4.131 | Reg loss: 0.031 | Tree loss: 4.131 | Accuracy: 0.107422 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 026 | Total loss: 4.105 | Reg loss: 0.031 | Tree loss: 4.105 | Accuracy: 0.093750 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 026 | Total loss: 4.137 | Reg loss: 0.031 | Tree loss: 4.137 | Accuracy: 0.082031 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 026 | Total loss: 4.107 | Reg loss: 0.031 | Tree loss: 4.107 | Accuracy: 0.115234 | 5.113 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 026 | Total loss: 4.132 | Reg loss: 0.031 | Tree loss: 4.132 | Accuracy: 0.080078 | 5.112 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 026 | Total loss: 4.106 | Reg loss: 0.031 | Tree loss: 4.106 | Accuracy: 0.096774 | 5.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 026 | Total loss: 4.145 | Reg loss: 0.031 | Tree loss: 4.145 | Accuracy: 0.097656 | 5.115 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 026 | Total loss: 4.162 | Reg loss: 0.031 | Tree loss: 4.162 | Accuracy: 0.080078 | 5.116 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 026 | Total loss: 4.147 | Reg loss: 0.031 | Tree loss: 4.147 | Accuracy: 0.097656 | 5.117 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 026 | Total loss: 4.193 | Reg loss: 0.031 | Tree loss: 4.193 | Accuracy: 0.089844 | 5.117 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 026 | Total loss: 4.135 | Reg loss: 0.031 | Tree loss: 4.135 | Accuracy: 0.083984 | 5.118 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 026 | Total loss: 4.135 | Reg loss: 0.031 | Tree loss: 4.135 | Accuracy: 0.109375 | 5.117 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 026 | Total loss: 4.126 | Reg loss: 0.031 | Tree loss: 4.126 | Accuracy: 0.091797 | 5.118 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 026 | Total loss: 4.167 | Reg loss: 0.031 | Tree loss: 4.167 | Accuracy: 0.068359 | 5.118 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 026 | Total loss: 4.121 | Reg loss: 0.031 | Tree loss: 4.121 | Accuracy: 0.113281 | 5.119 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 009 / 026 | Total loss: 4.114 | Reg loss: 0.031 | Tree loss: 4.114 | Accuracy: 0.091797 | 5.12 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 026 | Total loss: 4.106 | Reg loss: 0.031 | Tree loss: 4.106 | Accuracy: 0.083984 | 5.12 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 026 | Total loss: 4.051 | Reg loss: 0.031 | Tree loss: 4.051 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 026 | Total loss: 4.076 | Reg loss: 0.031 | Tree loss: 4.076 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 026 | Total loss: 4.111 | Reg loss: 0.031 | Tree loss: 4.111 | Accuracy: 0.091797 | 5.121 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 026 | Total loss: 4.100 | Reg loss: 0.031 | Tree loss: 4.100 | Accuracy: 0.089844 | 5.122 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 026 | Total loss: 4.064 | Reg loss: 0.031 | Tree loss: 4.064 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 026 | Total loss: 4.057 | Reg loss: 0.031 | Tree loss: 4.057 | Accuracy: 0.101562 | 5.122 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 026 | Total loss: 4.082 | Reg loss: 0.031 | Tree loss: 4.082 | Accuracy: 0.105469 | 5.122 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 026 | Total loss: 4.034 | Reg loss: 0.031 | Tree loss: 4.034 | Accuracy: 0.095703 | 5.122 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 026 | Total loss: 4.057 | Reg loss: 0.031 | Tree loss: 4.057 | Accuracy: 0.101562 | 5.122 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 026 | Total loss: 4.044 | Reg loss: 0.031 | Tree loss: 4.044 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 026 | Total loss: 4.027 | Reg loss: 0.031 | Tree loss: 4.027 | Accuracy: 0.109375 | 5.121 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 026 | Total loss: 4.055 | Reg loss: 0.031 | Tree loss: 4.055 | Accuracy: 0.119141 | 5.12 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 026 | Total loss: 4.033 | Reg loss: 0.031 | Tree loss: 4.033 | Accuracy: 0.097656 | 5.12 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 026 | Total loss: 4.039 | Reg loss: 0.031 | Tree loss: 4.039 | Accuracy: 0.093750 | 5.118 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 026 | Total loss: 4.050 | Reg loss: 0.031 | Tree loss: 4.050 | Accuracy: 0.109677 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 026 | Total loss: 4.073 | Reg loss: 0.031 | Tree loss: 4.073 | Accuracy: 0.097656 | 5.119 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 026 | Total loss: 4.104 | Reg loss: 0.031 | Tree loss: 4.104 | Accuracy: 0.087891 | 5.119 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 026 | Total loss: 4.068 | Reg loss: 0.031 | Tree loss: 4.068 | Accuracy: 0.099609 | 5.119 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 026 | Total loss: 4.039 | Reg loss: 0.031 | Tree loss: 4.039 | Accuracy: 0.093750 | 5.119 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 026 | Total loss: 4.059 | Reg loss: 0.031 | Tree loss: 4.059 | Accuracy: 0.093750 | 5.119 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 026 | Total loss: 4.014 | Reg loss: 0.031 | Tree loss: 4.014 | Accuracy: 0.099609 | 5.119 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 026 | Total loss: 4.012 | Reg loss: 0.031 | Tree loss: 4.012 | Accuracy: 0.107422 | 5.118 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 026 | Total loss: 4.030 | Reg loss: 0.031 | Tree loss: 4.030 | Accuracy: 0.107422 | 5.118 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 026 | Total loss: 4.019 | Reg loss: 0.031 | Tree loss: 4.019 | Accuracy: 0.132812 | 5.118 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 026 | Total loss: 4.051 | Reg loss: 0.031 | Tree loss: 4.051 | Accuracy: 0.080078 | 5.117 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 026 | Total loss: 4.032 | Reg loss: 0.031 | Tree loss: 4.032 | Accuracy: 0.103516 | 5.117 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 026 | Total loss: 4.014 | Reg loss: 0.031 | Tree loss: 4.014 | Accuracy: 0.085938 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 026 | Total loss: 3.999 | Reg loss: 0.031 | Tree loss: 3.999 | Accuracy: 0.097656 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 026 | Total loss: 4.009 | Reg loss: 0.031 | Tree loss: 4.009 | Accuracy: 0.093750 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 026 | Total loss: 4.014 | Reg loss: 0.031 | Tree loss: 4.014 | Accuracy: 0.105469 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 026 | Total loss: 3.959 | Reg loss: 0.031 | Tree loss: 3.959 | Accuracy: 0.119141 | 5.114 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 026 | Total loss: 3.979 | Reg loss: 0.031 | Tree loss: 3.979 | Accuracy: 0.103516 | 5.113 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 026 | Total loss: 3.977 | Reg loss: 0.031 | Tree loss: 3.977 | Accuracy: 0.111328 | 5.114 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 026 | Total loss: 3.957 | Reg loss: 0.031 | Tree loss: 3.957 | Accuracy: 0.085938 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 026 | Total loss: 3.953 | Reg loss: 0.031 | Tree loss: 3.953 | Accuracy: 0.099609 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 026 | Total loss: 3.937 | Reg loss: 0.031 | Tree loss: 3.937 | Accuracy: 0.095703 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 026 | Total loss: 3.940 | Reg loss: 0.031 | Tree loss: 3.940 | Accuracy: 0.103516 | 5.116 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 026 | Total loss: 3.945 | Reg loss: 0.031 | Tree loss: 3.945 | Accuracy: 0.103516 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 026 | Total loss: 3.961 | Reg loss: 0.032 | Tree loss: 3.961 | Accuracy: 0.105469 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 026 | Total loss: 3.951 | Reg loss: 0.032 | Tree loss: 3.951 | Accuracy: 0.109375 | 5.115 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 026 | Total loss: 3.864 | Reg loss: 0.032 | Tree loss: 3.864 | Accuracy: 0.083871 | 5.113 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 026 | Total loss: 3.995 | Reg loss: 0.031 | Tree loss: 3.995 | Accuracy: 0.091797 | 5.117 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 026 | Total loss: 3.983 | Reg loss: 0.031 | Tree loss: 3.983 | Accuracy: 0.101562 | 5.118 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 026 | Total loss: 3.934 | Reg loss: 0.031 | Tree loss: 3.934 | Accuracy: 0.099609 | 5.118 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 026 | Total loss: 3.969 | Reg loss: 0.031 | Tree loss: 3.969 | Accuracy: 0.080078 | 5.119 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 026 | Total loss: 3.952 | Reg loss: 0.031 | Tree loss: 3.952 | Accuracy: 0.109375 | 5.119 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 026 | Total loss: 3.983 | Reg loss: 0.031 | Tree loss: 3.983 | Accuracy: 0.099609 | 5.12 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 026 | Total loss: 3.916 | Reg loss: 0.031 | Tree loss: 3.916 | Accuracy: 0.107422 | 5.12 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 026 | Total loss: 3.950 | Reg loss: 0.031 | Tree loss: 3.950 | Accuracy: 0.078125 | 5.119 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 026 | Total loss: 3.951 | Reg loss: 0.031 | Tree loss: 3.951 | Accuracy: 0.111328 | 5.119 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 026 | Total loss: 3.935 | Reg loss: 0.031 | Tree loss: 3.935 | Accuracy: 0.103516 | 5.12 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 026 | Total loss: 3.924 | Reg loss: 0.031 | Tree loss: 3.924 | Accuracy: 0.072266 | 5.12 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 026 | Total loss: 3.907 | Reg loss: 0.031 | Tree loss: 3.907 | Accuracy: 0.091797 | 5.121 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 026 | Total loss: 3.941 | Reg loss: 0.031 | Tree loss: 3.941 | Accuracy: 0.097656 | 5.121 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 026 | Total loss: 3.899 | Reg loss: 0.031 | Tree loss: 3.899 | Accuracy: 0.117188 | 5.122 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 026 | Total loss: 3.879 | Reg loss: 0.031 | Tree loss: 3.879 | Accuracy: 0.111328 | 5.122 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 026 | Total loss: 3.910 | Reg loss: 0.031 | Tree loss: 3.910 | Accuracy: 0.083984 | 5.122 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 026 | Total loss: 3.914 | Reg loss: 0.031 | Tree loss: 3.914 | Accuracy: 0.082031 | 5.123 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 026 | Total loss: 3.895 | Reg loss: 0.031 | Tree loss: 3.895 | Accuracy: 0.089844 | 5.123 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 018 / 026 | Total loss: 3.894 | Reg loss: 0.031 | Tree loss: 3.894 | Accuracy: 0.119141 | 5.123 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 026 | Total loss: 3.869 | Reg loss: 0.031 | Tree loss: 3.869 | Accuracy: 0.125000 | 5.123 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 026 | Total loss: 3.872 | Reg loss: 0.032 | Tree loss: 3.872 | Accuracy: 0.091797 | 5.123 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 026 | Total loss: 3.866 | Reg loss: 0.032 | Tree loss: 3.866 | Accuracy: 0.123047 | 5.123 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 026 | Total loss: 3.865 | Reg loss: 0.032 | Tree loss: 3.865 | Accuracy: 0.097656 | 5.122 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 026 | Total loss: 3.854 | Reg loss: 0.032 | Tree loss: 3.854 | Accuracy: 0.105469 | 5.122 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 026 | Total loss: 3.888 | Reg loss: 0.032 | Tree loss: 3.888 | Accuracy: 0.121094 | 5.122 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 026 | Total loss: 3.804 | Reg loss: 0.032 | Tree loss: 3.804 | Accuracy: 0.122581 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 026 | Total loss: 3.926 | Reg loss: 0.031 | Tree loss: 3.926 | Accuracy: 0.091797 | 5.12 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 026 | Total loss: 3.890 | Reg loss: 0.031 | Tree loss: 3.890 | Accuracy: 0.105469 | 5.121 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 026 | Total loss: 3.876 | Reg loss: 0.031 | Tree loss: 3.876 | Accuracy: 0.111328 | 5.121 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 026 | Total loss: 3.866 | Reg loss: 0.031 | Tree loss: 3.866 | Accuracy: 0.103516 | 5.121 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 026 | Total loss: 3.885 | Reg loss: 0.031 | Tree loss: 3.885 | Accuracy: 0.109375 | 5.122 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 026 | Total loss: 3.892 | Reg loss: 0.031 | Tree loss: 3.892 | Accuracy: 0.064453 | 5.122 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 026 | Total loss: 3.855 | Reg loss: 0.031 | Tree loss: 3.855 | Accuracy: 0.107422 | 5.122 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 026 | Total loss: 3.870 | Reg loss: 0.031 | Tree loss: 3.870 | Accuracy: 0.117188 | 5.122 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 026 | Total loss: 3.848 | Reg loss: 0.031 | Tree loss: 3.848 | Accuracy: 0.105469 | 5.121 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 026 | Total loss: 3.876 | Reg loss: 0.031 | Tree loss: 3.876 | Accuracy: 0.074219 | 5.121 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 026 | Total loss: 3.835 | Reg loss: 0.031 | Tree loss: 3.835 | Accuracy: 0.111328 | 5.121 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 026 | Total loss: 3.864 | Reg loss: 0.031 | Tree loss: 3.864 | Accuracy: 0.101562 | 5.12 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 026 | Total loss: 3.812 | Reg loss: 0.031 | Tree loss: 3.812 | Accuracy: 0.099609 | 5.12 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 026 | Total loss: 3.793 | Reg loss: 0.031 | Tree loss: 3.793 | Accuracy: 0.093750 | 5.119 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 026 | Total loss: 3.823 | Reg loss: 0.031 | Tree loss: 3.823 | Accuracy: 0.109375 | 5.119 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 026 | Total loss: 3.817 | Reg loss: 0.031 | Tree loss: 3.817 | Accuracy: 0.097656 | 5.119 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 026 | Total loss: 3.836 | Reg loss: 0.031 | Tree loss: 3.836 | Accuracy: 0.097656 | 5.118 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 026 | Total loss: 3.809 | Reg loss: 0.031 | Tree loss: 3.809 | Accuracy: 0.101562 | 5.118 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 026 | Total loss: 3.793 | Reg loss: 0.032 | Tree loss: 3.793 | Accuracy: 0.099609 | 5.117 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 026 | Total loss: 3.797 | Reg loss: 0.032 | Tree loss: 3.797 | Accuracy: 0.109375 | 5.117 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 026 | Total loss: 3.836 | Reg loss: 0.032 | Tree loss: 3.836 | Accuracy: 0.101562 | 5.118 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 026 | Total loss: 3.799 | Reg loss: 0.032 | Tree loss: 3.799 | Accuracy: 0.103516 | 5.119 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 026 | Total loss: 3.774 | Reg loss: 0.032 | Tree loss: 3.774 | Accuracy: 0.091797 | 5.119 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 026 | Total loss: 3.783 | Reg loss: 0.032 | Tree loss: 3.783 | Accuracy: 0.109375 | 5.119 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 026 | Total loss: 3.787 | Reg loss: 0.032 | Tree loss: 3.787 | Accuracy: 0.101562 | 5.119 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 026 | Total loss: 3.821 | Reg loss: 0.032 | Tree loss: 3.821 | Accuracy: 0.096774 | 5.118 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 026 | Total loss: 3.795 | Reg loss: 0.031 | Tree loss: 3.795 | Accuracy: 0.083984 | 5.121 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 026 | Total loss: 3.816 | Reg loss: 0.031 | Tree loss: 3.816 | Accuracy: 0.101562 | 5.121 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 026 | Total loss: 3.803 | Reg loss: 0.031 | Tree loss: 3.803 | Accuracy: 0.105469 | 5.121 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 026 | Total loss: 3.813 | Reg loss: 0.031 | Tree loss: 3.813 | Accuracy: 0.097656 | 5.121 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 026 | Total loss: 3.779 | Reg loss: 0.031 | Tree loss: 3.779 | Accuracy: 0.107422 | 5.12 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 026 | Total loss: 3.820 | Reg loss: 0.031 | Tree loss: 3.820 | Accuracy: 0.107422 | 5.12 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 026 | Total loss: 3.777 | Reg loss: 0.031 | Tree loss: 3.777 | Accuracy: 0.097656 | 5.119 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 026 | Total loss: 3.799 | Reg loss: 0.031 | Tree loss: 3.799 | Accuracy: 0.095703 | 5.119 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 026 | Total loss: 3.800 | Reg loss: 0.031 | Tree loss: 3.800 | Accuracy: 0.089844 | 5.118 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 026 | Total loss: 3.757 | Reg loss: 0.031 | Tree loss: 3.757 | Accuracy: 0.099609 | 5.118 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 026 | Total loss: 3.788 | Reg loss: 0.031 | Tree loss: 3.788 | Accuracy: 0.087891 | 5.117 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 026 | Total loss: 3.787 | Reg loss: 0.031 | Tree loss: 3.787 | Accuracy: 0.105469 | 5.117 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 026 | Total loss: 3.773 | Reg loss: 0.031 | Tree loss: 3.773 | Accuracy: 0.101562 | 5.118 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 026 | Total loss: 3.752 | Reg loss: 0.031 | Tree loss: 3.752 | Accuracy: 0.105469 | 5.119 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 026 | Total loss: 3.763 | Reg loss: 0.032 | Tree loss: 3.763 | Accuracy: 0.109375 | 5.119 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 026 | Total loss: 3.762 | Reg loss: 0.032 | Tree loss: 3.762 | Accuracy: 0.103516 | 5.12 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 026 | Total loss: 3.716 | Reg loss: 0.032 | Tree loss: 3.716 | Accuracy: 0.115234 | 5.12 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 026 | Total loss: 3.771 | Reg loss: 0.032 | Tree loss: 3.771 | Accuracy: 0.109375 | 5.12 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 026 | Total loss: 3.716 | Reg loss: 0.032 | Tree loss: 3.716 | Accuracy: 0.097656 | 5.119 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 026 | Total loss: 3.763 | Reg loss: 0.032 | Tree loss: 3.763 | Accuracy: 0.105469 | 5.119 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 026 | Total loss: 3.769 | Reg loss: 0.032 | Tree loss: 3.769 | Accuracy: 0.105469 | 5.119 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 026 | Total loss: 3.675 | Reg loss: 0.032 | Tree loss: 3.675 | Accuracy: 0.113281 | 5.118 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 026 | Total loss: 3.687 | Reg loss: 0.032 | Tree loss: 3.687 | Accuracy: 0.097656 | 5.118 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 026 | Total loss: 3.716 | Reg loss: 0.032 | Tree loss: 3.716 | Accuracy: 0.099609 | 5.118 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 026 | Total loss: 3.736 | Reg loss: 0.032 | Tree loss: 3.736 | Accuracy: 0.085938 | 5.117 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 026 | Total loss: 3.694 | Reg loss: 0.032 | Tree loss: 3.694 | Accuracy: 0.064516 | 5.116 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 000 / 026 | Total loss: 3.758 | Reg loss: 0.031 | Tree loss: 3.758 | Accuracy: 0.103516 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 026 | Total loss: 3.710 | Reg loss: 0.031 | Tree loss: 3.710 | Accuracy: 0.121094 | 5.118 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 026 | Total loss: 3.759 | Reg loss: 0.031 | Tree loss: 3.759 | Accuracy: 0.103516 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 026 | Total loss: 3.755 | Reg loss: 0.031 | Tree loss: 3.755 | Accuracy: 0.093750 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 026 | Total loss: 3.731 | Reg loss: 0.031 | Tree loss: 3.731 | Accuracy: 0.105469 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 026 | Total loss: 3.775 | Reg loss: 0.031 | Tree loss: 3.775 | Accuracy: 0.072266 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 026 | Total loss: 3.734 | Reg loss: 0.031 | Tree loss: 3.734 | Accuracy: 0.101562 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 026 | Total loss: 3.711 | Reg loss: 0.031 | Tree loss: 3.711 | Accuracy: 0.105469 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 026 | Total loss: 3.684 | Reg loss: 0.031 | Tree loss: 3.684 | Accuracy: 0.105469 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 026 | Total loss: 3.728 | Reg loss: 0.032 | Tree loss: 3.728 | Accuracy: 0.091797 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 026 | Total loss: 3.732 | Reg loss: 0.032 | Tree loss: 3.732 | Accuracy: 0.095703 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 026 | Total loss: 3.733 | Reg loss: 0.032 | Tree loss: 3.733 | Accuracy: 0.093750 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 026 | Total loss: 3.709 | Reg loss: 0.032 | Tree loss: 3.709 | Accuracy: 0.103516 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 026 | Total loss: 3.709 | Reg loss: 0.032 | Tree loss: 3.709 | Accuracy: 0.101562 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 026 | Total loss: 3.676 | Reg loss: 0.032 | Tree loss: 3.676 | Accuracy: 0.099609 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 026 | Total loss: 3.687 | Reg loss: 0.032 | Tree loss: 3.687 | Accuracy: 0.099609 | 5.12 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 026 | Total loss: 3.666 | Reg loss: 0.032 | Tree loss: 3.666 | Accuracy: 0.111328 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 026 | Total loss: 3.703 | Reg loss: 0.032 | Tree loss: 3.703 | Accuracy: 0.099609 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 026 | Total loss: 3.638 | Reg loss: 0.032 | Tree loss: 3.638 | Accuracy: 0.103516 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 026 | Total loss: 3.646 | Reg loss: 0.032 | Tree loss: 3.646 | Accuracy: 0.093750 | 5.118 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 026 | Total loss: 3.658 | Reg loss: 0.032 | Tree loss: 3.658 | Accuracy: 0.121094 | 5.117 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 026 | Total loss: 3.674 | Reg loss: 0.032 | Tree loss: 3.674 | Accuracy: 0.099609 | 5.118 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 026 | Total loss: 3.615 | Reg loss: 0.032 | Tree loss: 3.615 | Accuracy: 0.119141 | 5.118 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 026 | Total loss: 3.629 | Reg loss: 0.032 | Tree loss: 3.629 | Accuracy: 0.117188 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 026 | Total loss: 3.635 | Reg loss: 0.032 | Tree loss: 3.635 | Accuracy: 0.101562 | 5.119 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 026 | Total loss: 3.646 | Reg loss: 0.032 | Tree loss: 3.646 | Accuracy: 0.103226 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 026 | Total loss: 3.679 | Reg loss: 0.031 | Tree loss: 3.679 | Accuracy: 0.119141 | 5.118 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 026 | Total loss: 3.686 | Reg loss: 0.031 | Tree loss: 3.686 | Accuracy: 0.091797 | 5.118 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 026 | Total loss: 3.647 | Reg loss: 0.031 | Tree loss: 3.647 | Accuracy: 0.115234 | 5.118 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 026 | Total loss: 3.678 | Reg loss: 0.031 | Tree loss: 3.678 | Accuracy: 0.093750 | 5.117 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 026 | Total loss: 3.677 | Reg loss: 0.031 | Tree loss: 3.677 | Accuracy: 0.097656 | 5.117 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 026 | Total loss: 3.671 | Reg loss: 0.031 | Tree loss: 3.671 | Accuracy: 0.101562 | 5.117 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 026 | Total loss: 3.673 | Reg loss: 0.032 | Tree loss: 3.673 | Accuracy: 0.111328 | 5.117 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 026 | Total loss: 3.627 | Reg loss: 0.032 | Tree loss: 3.627 | Accuracy: 0.119141 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 026 | Total loss: 3.647 | Reg loss: 0.032 | Tree loss: 3.647 | Accuracy: 0.107422 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 026 | Total loss: 3.642 | Reg loss: 0.032 | Tree loss: 3.642 | Accuracy: 0.121094 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 026 | Total loss: 3.635 | Reg loss: 0.032 | Tree loss: 3.635 | Accuracy: 0.117188 | 5.115 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 026 | Total loss: 3.661 | Reg loss: 0.032 | Tree loss: 3.661 | Accuracy: 0.103516 | 5.115 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 026 | Total loss: 3.636 | Reg loss: 0.032 | Tree loss: 3.636 | Accuracy: 0.103516 | 5.114 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 026 | Total loss: 3.653 | Reg loss: 0.032 | Tree loss: 3.653 | Accuracy: 0.083984 | 5.114 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 026 | Total loss: 3.632 | Reg loss: 0.032 | Tree loss: 3.632 | Accuracy: 0.103516 | 5.115 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 026 | Total loss: 3.611 | Reg loss: 0.032 | Tree loss: 3.611 | Accuracy: 0.105469 | 5.115 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 026 | Total loss: 3.624 | Reg loss: 0.032 | Tree loss: 3.624 | Accuracy: 0.121094 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 026 | Total loss: 3.608 | Reg loss: 0.032 | Tree loss: 3.608 | Accuracy: 0.119141 | 5.116 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 026 | Total loss: 3.626 | Reg loss: 0.032 | Tree loss: 3.626 | Accuracy: 0.119141 | 5.117 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 026 | Total loss: 3.595 | Reg loss: 0.032 | Tree loss: 3.595 | Accuracy: 0.134766 | 5.117 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 026 | Total loss: 3.602 | Reg loss: 0.032 | Tree loss: 3.602 | Accuracy: 0.146484 | 5.117 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 026 | Total loss: 3.614 | Reg loss: 0.032 | Tree loss: 3.614 | Accuracy: 0.121094 | 5.118 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 026 | Total loss: 3.640 | Reg loss: 0.032 | Tree loss: 3.640 | Accuracy: 0.091797 | 5.118 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 026 | Total loss: 3.566 | Reg loss: 0.032 | Tree loss: 3.566 | Accuracy: 0.128906 | 5.118 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 026 | Total loss: 3.576 | Reg loss: 0.032 | Tree loss: 3.576 | Accuracy: 0.140625 | 5.118 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 026 | Total loss: 3.636 | Reg loss: 0.032 | Tree loss: 3.636 | Accuracy: 0.103226 | 5.116 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 026 | Total loss: 3.635 | Reg loss: 0.032 | Tree loss: 3.635 | Accuracy: 0.111328 | 5.119 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 026 | Total loss: 3.612 | Reg loss: 0.032 | Tree loss: 3.612 | Accuracy: 0.109375 | 5.119 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 026 | Total loss: 3.639 | Reg loss: 0.032 | Tree loss: 3.639 | Accuracy: 0.101562 | 5.12 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 026 | Total loss: 3.573 | Reg loss: 0.032 | Tree loss: 3.573 | Accuracy: 0.121094 | 5.12 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 026 | Total loss: 3.566 | Reg loss: 0.032 | Tree loss: 3.566 | Accuracy: 0.125000 | 5.119 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 026 | Total loss: 3.604 | Reg loss: 0.032 | Tree loss: 3.604 | Accuracy: 0.126953 | 5.12 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 026 | Total loss: 3.628 | Reg loss: 0.032 | Tree loss: 3.628 | Accuracy: 0.134766 | 5.12 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 026 | Total loss: 3.632 | Reg loss: 0.032 | Tree loss: 3.632 | Accuracy: 0.111328 | 5.121 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 026 | Total loss: 3.580 | Reg loss: 0.032 | Tree loss: 3.580 | Accuracy: 0.148438 | 5.121 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 009 / 026 | Total loss: 3.559 | Reg loss: 0.032 | Tree loss: 3.559 | Accuracy: 0.123047 | 5.122 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 026 | Total loss: 3.584 | Reg loss: 0.032 | Tree loss: 3.584 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 026 | Total loss: 3.592 | Reg loss: 0.032 | Tree loss: 3.592 | Accuracy: 0.119141 | 5.122 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 026 | Total loss: 3.588 | Reg loss: 0.032 | Tree loss: 3.588 | Accuracy: 0.105469 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 026 | Total loss: 3.590 | Reg loss: 0.032 | Tree loss: 3.590 | Accuracy: 0.111328 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 026 | Total loss: 3.582 | Reg loss: 0.032 | Tree loss: 3.582 | Accuracy: 0.130859 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 026 | Total loss: 3.583 | Reg loss: 0.032 | Tree loss: 3.583 | Accuracy: 0.138672 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 026 | Total loss: 3.579 | Reg loss: 0.032 | Tree loss: 3.579 | Accuracy: 0.132812 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 026 | Total loss: 3.569 | Reg loss: 0.032 | Tree loss: 3.569 | Accuracy: 0.103516 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 026 | Total loss: 3.519 | Reg loss: 0.032 | Tree loss: 3.519 | Accuracy: 0.136719 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 026 | Total loss: 3.536 | Reg loss: 0.032 | Tree loss: 3.536 | Accuracy: 0.125000 | 5.123 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 026 | Total loss: 3.554 | Reg loss: 0.032 | Tree loss: 3.554 | Accuracy: 0.134766 | 5.122 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 026 | Total loss: 3.534 | Reg loss: 0.032 | Tree loss: 3.534 | Accuracy: 0.144531 | 5.122 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 026 | Total loss: 3.535 | Reg loss: 0.032 | Tree loss: 3.535 | Accuracy: 0.142578 | 5.121 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 026 | Total loss: 3.589 | Reg loss: 0.032 | Tree loss: 3.589 | Accuracy: 0.093750 | 5.12 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 026 | Total loss: 3.547 | Reg loss: 0.032 | Tree loss: 3.547 | Accuracy: 0.113281 | 5.121 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 026 | Total loss: 3.543 | Reg loss: 0.032 | Tree loss: 3.543 | Accuracy: 0.129032 | 5.119 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 026 | Total loss: 3.560 | Reg loss: 0.032 | Tree loss: 3.560 | Accuracy: 0.123047 | 5.121 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 026 | Total loss: 3.597 | Reg loss: 0.032 | Tree loss: 3.597 | Accuracy: 0.130859 | 5.121 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 026 | Total loss: 3.561 | Reg loss: 0.032 | Tree loss: 3.561 | Accuracy: 0.109375 | 5.121 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 026 | Total loss: 3.568 | Reg loss: 0.032 | Tree loss: 3.568 | Accuracy: 0.121094 | 5.121 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 026 | Total loss: 3.560 | Reg loss: 0.032 | Tree loss: 3.560 | Accuracy: 0.121094 | 5.121 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 026 | Total loss: 3.564 | Reg loss: 0.032 | Tree loss: 3.564 | Accuracy: 0.132812 | 5.121 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 026 | Total loss: 3.553 | Reg loss: 0.032 | Tree loss: 3.553 | Accuracy: 0.142578 | 5.121 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 026 | Total loss: 3.538 | Reg loss: 0.032 | Tree loss: 3.538 | Accuracy: 0.126953 | 5.12 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 026 | Total loss: 3.506 | Reg loss: 0.032 | Tree loss: 3.506 | Accuracy: 0.138672 | 5.12 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 026 | Total loss: 3.546 | Reg loss: 0.032 | Tree loss: 3.546 | Accuracy: 0.109375 | 5.12 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 026 | Total loss: 3.528 | Reg loss: 0.032 | Tree loss: 3.528 | Accuracy: 0.115234 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 026 | Total loss: 3.519 | Reg loss: 0.032 | Tree loss: 3.519 | Accuracy: 0.138672 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 026 | Total loss: 3.536 | Reg loss: 0.032 | Tree loss: 3.536 | Accuracy: 0.117188 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 026 | Total loss: 3.507 | Reg loss: 0.032 | Tree loss: 3.507 | Accuracy: 0.111328 | 5.118 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 026 | Total loss: 3.488 | Reg loss: 0.032 | Tree loss: 3.488 | Accuracy: 0.140625 | 5.118 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 026 | Total loss: 3.488 | Reg loss: 0.032 | Tree loss: 3.488 | Accuracy: 0.113281 | 5.117 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 026 | Total loss: 3.518 | Reg loss: 0.032 | Tree loss: 3.518 | Accuracy: 0.107422 | 5.118 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 026 | Total loss: 3.519 | Reg loss: 0.032 | Tree loss: 3.519 | Accuracy: 0.125000 | 5.118 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 026 | Total loss: 3.513 | Reg loss: 0.032 | Tree loss: 3.513 | Accuracy: 0.138672 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 026 | Total loss: 3.486 | Reg loss: 0.032 | Tree loss: 3.486 | Accuracy: 0.134766 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 026 | Total loss: 3.508 | Reg loss: 0.032 | Tree loss: 3.508 | Accuracy: 0.132812 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 026 | Total loss: 3.498 | Reg loss: 0.032 | Tree loss: 3.498 | Accuracy: 0.117188 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 026 | Total loss: 3.535 | Reg loss: 0.032 | Tree loss: 3.535 | Accuracy: 0.111328 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 026 | Total loss: 3.507 | Reg loss: 0.032 | Tree loss: 3.507 | Accuracy: 0.126953 | 5.119 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 026 | Total loss: 3.489 | Reg loss: 0.032 | Tree loss: 3.489 | Accuracy: 0.117188 | 5.118 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 026 | Total loss: 3.560 | Reg loss: 0.032 | Tree loss: 3.560 | Accuracy: 0.083871 | 5.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 026 | Total loss: 3.518 | Reg loss: 0.032 | Tree loss: 3.518 | Accuracy: 0.128906 | 5.121 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 026 | Total loss: 3.513 | Reg loss: 0.032 | Tree loss: 3.513 | Accuracy: 0.117188 | 5.121 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 026 | Total loss: 3.502 | Reg loss: 0.032 | Tree loss: 3.502 | Accuracy: 0.142578 | 5.121 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 026 | Total loss: 3.482 | Reg loss: 0.032 | Tree loss: 3.482 | Accuracy: 0.119141 | 5.122 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 026 | Total loss: 3.539 | Reg loss: 0.032 | Tree loss: 3.539 | Accuracy: 0.115234 | 5.122 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 026 | Total loss: 3.477 | Reg loss: 0.032 | Tree loss: 3.477 | Accuracy: 0.113281 | 5.122 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 026 | Total loss: 3.495 | Reg loss: 0.032 | Tree loss: 3.495 | Accuracy: 0.136719 | 5.122 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 026 | Total loss: 3.522 | Reg loss: 0.032 | Tree loss: 3.522 | Accuracy: 0.093750 | 5.122 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 026 | Total loss: 3.513 | Reg loss: 0.032 | Tree loss: 3.513 | Accuracy: 0.138672 | 5.123 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 026 | Total loss: 3.505 | Reg loss: 0.032 | Tree loss: 3.505 | Accuracy: 0.125000 | 5.123 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 026 | Total loss: 3.477 | Reg loss: 0.032 | Tree loss: 3.477 | Accuracy: 0.113281 | 5.124 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 026 | Total loss: 3.504 | Reg loss: 0.032 | Tree loss: 3.504 | Accuracy: 0.130859 | 5.124 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 026 | Total loss: 3.488 | Reg loss: 0.032 | Tree loss: 3.488 | Accuracy: 0.126953 | 5.124 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 026 | Total loss: 3.459 | Reg loss: 0.032 | Tree loss: 3.459 | Accuracy: 0.132812 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 026 | Total loss: 3.454 | Reg loss: 0.032 | Tree loss: 3.454 | Accuracy: 0.138672 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 026 | Total loss: 3.438 | Reg loss: 0.032 | Tree loss: 3.438 | Accuracy: 0.111328 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 026 | Total loss: 3.483 | Reg loss: 0.032 | Tree loss: 3.483 | Accuracy: 0.128906 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 026 | Total loss: 3.456 | Reg loss: 0.032 | Tree loss: 3.456 | Accuracy: 0.115234 | 5.125 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 018 / 026 | Total loss: 3.534 | Reg loss: 0.032 | Tree loss: 3.534 | Accuracy: 0.097656 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 026 | Total loss: 3.462 | Reg loss: 0.032 | Tree loss: 3.462 | Accuracy: 0.111328 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 026 | Total loss: 3.422 | Reg loss: 0.032 | Tree loss: 3.422 | Accuracy: 0.158203 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 026 | Total loss: 3.474 | Reg loss: 0.032 | Tree loss: 3.474 | Accuracy: 0.132812 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 026 | Total loss: 3.457 | Reg loss: 0.032 | Tree loss: 3.457 | Accuracy: 0.105469 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 026 | Total loss: 3.463 | Reg loss: 0.032 | Tree loss: 3.463 | Accuracy: 0.105469 | 5.125 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 026 | Total loss: 3.407 | Reg loss: 0.032 | Tree loss: 3.407 | Accuracy: 0.144531 | 5.124 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 026 | Total loss: 3.390 | Reg loss: 0.032 | Tree loss: 3.390 | Accuracy: 0.148387 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 026 | Total loss: 3.465 | Reg loss: 0.032 | Tree loss: 3.465 | Accuracy: 0.123047 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 026 | Total loss: 3.475 | Reg loss: 0.032 | Tree loss: 3.475 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 026 | Total loss: 3.443 | Reg loss: 0.032 | Tree loss: 3.443 | Accuracy: 0.132812 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 026 | Total loss: 3.510 | Reg loss: 0.032 | Tree loss: 3.510 | Accuracy: 0.099609 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 026 | Total loss: 3.478 | Reg loss: 0.032 | Tree loss: 3.478 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 026 | Total loss: 3.451 | Reg loss: 0.032 | Tree loss: 3.451 | Accuracy: 0.109375 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 026 | Total loss: 3.448 | Reg loss: 0.032 | Tree loss: 3.448 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 026 | Total loss: 3.465 | Reg loss: 0.032 | Tree loss: 3.465 | Accuracy: 0.119141 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 026 | Total loss: 3.457 | Reg loss: 0.032 | Tree loss: 3.457 | Accuracy: 0.132812 | 5.126 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 026 | Total loss: 3.428 | Reg loss: 0.032 | Tree loss: 3.428 | Accuracy: 0.138672 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 026 | Total loss: 3.437 | Reg loss: 0.032 | Tree loss: 3.437 | Accuracy: 0.126953 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 026 | Total loss: 3.478 | Reg loss: 0.032 | Tree loss: 3.478 | Accuracy: 0.109375 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 026 | Total loss: 3.419 | Reg loss: 0.032 | Tree loss: 3.419 | Accuracy: 0.154297 | 5.124 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 026 | Total loss: 3.433 | Reg loss: 0.032 | Tree loss: 3.433 | Accuracy: 0.126953 | 5.124 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 026 | Total loss: 3.404 | Reg loss: 0.032 | Tree loss: 3.404 | Accuracy: 0.140625 | 5.124 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 026 | Total loss: 3.434 | Reg loss: 0.032 | Tree loss: 3.434 | Accuracy: 0.123047 | 5.123 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 026 | Total loss: 3.468 | Reg loss: 0.032 | Tree loss: 3.468 | Accuracy: 0.125000 | 5.122 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 026 | Total loss: 3.443 | Reg loss: 0.032 | Tree loss: 3.443 | Accuracy: 0.107422 | 5.123 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 026 | Total loss: 3.408 | Reg loss: 0.032 | Tree loss: 3.408 | Accuracy: 0.140625 | 5.124 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 026 | Total loss: 3.401 | Reg loss: 0.032 | Tree loss: 3.401 | Accuracy: 0.113281 | 5.124 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 026 | Total loss: 3.383 | Reg loss: 0.032 | Tree loss: 3.383 | Accuracy: 0.126953 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 026 | Total loss: 3.420 | Reg loss: 0.032 | Tree loss: 3.420 | Accuracy: 0.117188 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 026 | Total loss: 3.377 | Reg loss: 0.032 | Tree loss: 3.377 | Accuracy: 0.119141 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 026 | Total loss: 3.401 | Reg loss: 0.032 | Tree loss: 3.401 | Accuracy: 0.119141 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 026 | Total loss: 3.431 | Reg loss: 0.032 | Tree loss: 3.431 | Accuracy: 0.119141 | 5.125 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 026 | Total loss: 3.366 | Reg loss: 0.032 | Tree loss: 3.366 | Accuracy: 0.135484 | 5.124 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 026 | Total loss: 3.438 | Reg loss: 0.032 | Tree loss: 3.438 | Accuracy: 0.144531 | 5.127 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 026 | Total loss: 3.423 | Reg loss: 0.032 | Tree loss: 3.423 | Accuracy: 0.125000 | 5.127 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 026 | Total loss: 3.412 | Reg loss: 0.032 | Tree loss: 3.412 | Accuracy: 0.126953 | 5.127 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 026 | Total loss: 3.429 | Reg loss: 0.032 | Tree loss: 3.429 | Accuracy: 0.097656 | 5.127 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 026 | Total loss: 3.485 | Reg loss: 0.032 | Tree loss: 3.485 | Accuracy: 0.111328 | 5.127 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 026 | Total loss: 3.451 | Reg loss: 0.032 | Tree loss: 3.451 | Accuracy: 0.136719 | 5.127 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 026 | Total loss: 3.404 | Reg loss: 0.032 | Tree loss: 3.404 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 026 | Total loss: 3.395 | Reg loss: 0.032 | Tree loss: 3.395 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 026 | Total loss: 3.399 | Reg loss: 0.032 | Tree loss: 3.399 | Accuracy: 0.136719 | 5.125 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 026 | Total loss: 3.419 | Reg loss: 0.032 | Tree loss: 3.419 | Accuracy: 0.130859 | 5.125 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 026 | Total loss: 3.374 | Reg loss: 0.032 | Tree loss: 3.374 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 026 | Total loss: 3.399 | Reg loss: 0.032 | Tree loss: 3.399 | Accuracy: 0.119141 | 5.126 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 026 | Total loss: 3.428 | Reg loss: 0.032 | Tree loss: 3.428 | Accuracy: 0.126953 | 5.126 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 026 | Total loss: 3.401 | Reg loss: 0.032 | Tree loss: 3.401 | Accuracy: 0.130859 | 5.126 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 026 | Total loss: 3.392 | Reg loss: 0.032 | Tree loss: 3.392 | Accuracy: 0.126953 | 5.126 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 026 | Total loss: 3.369 | Reg loss: 0.032 | Tree loss: 3.369 | Accuracy: 0.150391 | 5.125 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 026 | Total loss: 3.383 | Reg loss: 0.032 | Tree loss: 3.383 | Accuracy: 0.119141 | 5.125 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 026 | Total loss: 3.378 | Reg loss: 0.032 | Tree loss: 3.378 | Accuracy: 0.117188 | 5.125 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 026 | Total loss: 3.365 | Reg loss: 0.032 | Tree loss: 3.365 | Accuracy: 0.138672 | 5.124 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 026 | Total loss: 3.387 | Reg loss: 0.032 | Tree loss: 3.387 | Accuracy: 0.117188 | 5.124 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 026 | Total loss: 3.386 | Reg loss: 0.032 | Tree loss: 3.386 | Accuracy: 0.125000 | 5.124 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 026 | Total loss: 3.386 | Reg loss: 0.032 | Tree loss: 3.386 | Accuracy: 0.101562 | 5.123 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 026 | Total loss: 3.350 | Reg loss: 0.032 | Tree loss: 3.350 | Accuracy: 0.130859 | 5.123 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 026 | Total loss: 3.332 | Reg loss: 0.032 | Tree loss: 3.332 | Accuracy: 0.130859 | 5.123 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 026 | Total loss: 3.385 | Reg loss: 0.032 | Tree loss: 3.385 | Accuracy: 0.093750 | 5.122 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 026 | Total loss: 3.327 | Reg loss: 0.032 | Tree loss: 3.327 | Accuracy: 0.090323 | 5.121 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 000 / 026 | Total loss: 3.422 | Reg loss: 0.032 | Tree loss: 3.422 | Accuracy: 0.130859 | 5.121 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 026 | Total loss: 3.407 | Reg loss: 0.032 | Tree loss: 3.407 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 026 | Total loss: 3.374 | Reg loss: 0.032 | Tree loss: 3.374 | Accuracy: 0.125000 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 026 | Total loss: 3.388 | Reg loss: 0.032 | Tree loss: 3.388 | Accuracy: 0.125000 | 5.123 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 026 | Total loss: 3.367 | Reg loss: 0.032 | Tree loss: 3.367 | Accuracy: 0.128906 | 5.123 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 026 | Total loss: 3.337 | Reg loss: 0.032 | Tree loss: 3.337 | Accuracy: 0.144531 | 5.124 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 026 | Total loss: 3.350 | Reg loss: 0.032 | Tree loss: 3.350 | Accuracy: 0.138672 | 5.124 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 026 | Total loss: 3.438 | Reg loss: 0.032 | Tree loss: 3.438 | Accuracy: 0.107422 | 5.124 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 026 | Total loss: 3.373 | Reg loss: 0.032 | Tree loss: 3.373 | Accuracy: 0.101562 | 5.124 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 026 | Total loss: 3.370 | Reg loss: 0.032 | Tree loss: 3.370 | Accuracy: 0.130859 | 5.123 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 026 | Total loss: 3.334 | Reg loss: 0.032 | Tree loss: 3.334 | Accuracy: 0.109375 | 5.123 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 026 | Total loss: 3.346 | Reg loss: 0.032 | Tree loss: 3.346 | Accuracy: 0.138672 | 5.123 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 026 | Total loss: 3.389 | Reg loss: 0.032 | Tree loss: 3.389 | Accuracy: 0.113281 | 5.123 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 026 | Total loss: 3.351 | Reg loss: 0.032 | Tree loss: 3.351 | Accuracy: 0.138672 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 026 | Total loss: 3.336 | Reg loss: 0.032 | Tree loss: 3.336 | Accuracy: 0.093750 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 026 | Total loss: 3.340 | Reg loss: 0.032 | Tree loss: 3.340 | Accuracy: 0.123047 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 026 | Total loss: 3.342 | Reg loss: 0.032 | Tree loss: 3.342 | Accuracy: 0.138672 | 5.121 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 026 | Total loss: 3.373 | Reg loss: 0.032 | Tree loss: 3.373 | Accuracy: 0.136719 | 5.121 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 026 | Total loss: 3.351 | Reg loss: 0.032 | Tree loss: 3.351 | Accuracy: 0.121094 | 5.12 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 026 | Total loss: 3.330 | Reg loss: 0.032 | Tree loss: 3.330 | Accuracy: 0.140625 | 5.121 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 026 | Total loss: 3.316 | Reg loss: 0.032 | Tree loss: 3.316 | Accuracy: 0.097656 | 5.121 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 026 | Total loss: 3.332 | Reg loss: 0.032 | Tree loss: 3.332 | Accuracy: 0.136719 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 026 | Total loss: 3.371 | Reg loss: 0.032 | Tree loss: 3.371 | Accuracy: 0.117188 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 026 | Total loss: 3.369 | Reg loss: 0.032 | Tree loss: 3.369 | Accuracy: 0.113281 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 026 | Total loss: 3.344 | Reg loss: 0.032 | Tree loss: 3.344 | Accuracy: 0.115234 | 5.122 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 026 | Total loss: 3.349 | Reg loss: 0.032 | Tree loss: 3.349 | Accuracy: 0.116129 | 5.121 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 026 | Total loss: 3.341 | Reg loss: 0.032 | Tree loss: 3.341 | Accuracy: 0.123047 | 5.124 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 026 | Total loss: 3.367 | Reg loss: 0.032 | Tree loss: 3.367 | Accuracy: 0.115234 | 5.123 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 026 | Total loss: 3.370 | Reg loss: 0.032 | Tree loss: 3.370 | Accuracy: 0.113281 | 5.123 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 026 | Total loss: 3.337 | Reg loss: 0.032 | Tree loss: 3.337 | Accuracy: 0.119141 | 5.123 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 026 | Total loss: 3.363 | Reg loss: 0.032 | Tree loss: 3.363 | Accuracy: 0.095703 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 026 | Total loss: 3.346 | Reg loss: 0.032 | Tree loss: 3.346 | Accuracy: 0.138672 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 026 | Total loss: 3.342 | Reg loss: 0.032 | Tree loss: 3.342 | Accuracy: 0.113281 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 026 | Total loss: 3.410 | Reg loss: 0.032 | Tree loss: 3.410 | Accuracy: 0.121094 | 5.121 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 026 | Total loss: 3.310 | Reg loss: 0.032 | Tree loss: 3.310 | Accuracy: 0.130859 | 5.121 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 026 | Total loss: 3.359 | Reg loss: 0.032 | Tree loss: 3.359 | Accuracy: 0.134766 | 5.12 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 026 | Total loss: 3.336 | Reg loss: 0.032 | Tree loss: 3.336 | Accuracy: 0.119141 | 5.12 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 026 | Total loss: 3.310 | Reg loss: 0.032 | Tree loss: 3.310 | Accuracy: 0.134766 | 5.12 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 026 | Total loss: 3.345 | Reg loss: 0.032 | Tree loss: 3.345 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 026 | Total loss: 3.293 | Reg loss: 0.032 | Tree loss: 3.293 | Accuracy: 0.138672 | 5.121 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 026 | Total loss: 3.321 | Reg loss: 0.032 | Tree loss: 3.321 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 026 | Total loss: 3.303 | Reg loss: 0.032 | Tree loss: 3.303 | Accuracy: 0.121094 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 026 | Total loss: 3.319 | Reg loss: 0.032 | Tree loss: 3.319 | Accuracy: 0.113281 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 026 | Total loss: 3.351 | Reg loss: 0.032 | Tree loss: 3.351 | Accuracy: 0.117188 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 026 | Total loss: 3.311 | Reg loss: 0.032 | Tree loss: 3.311 | Accuracy: 0.144531 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 026 | Total loss: 3.292 | Reg loss: 0.032 | Tree loss: 3.292 | Accuracy: 0.146484 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 026 | Total loss: 3.291 | Reg loss: 0.032 | Tree loss: 3.291 | Accuracy: 0.095703 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 026 | Total loss: 3.286 | Reg loss: 0.032 | Tree loss: 3.286 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 026 | Total loss: 3.300 | Reg loss: 0.032 | Tree loss: 3.300 | Accuracy: 0.130859 | 5.121 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 026 | Total loss: 3.307 | Reg loss: 0.032 | Tree loss: 3.307 | Accuracy: 0.123047 | 5.121 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 026 | Total loss: 3.304 | Reg loss: 0.032 | Tree loss: 3.304 | Accuracy: 0.128906 | 5.121 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 026 | Total loss: 3.320 | Reg loss: 0.032 | Tree loss: 3.320 | Accuracy: 0.103226 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 026 | Total loss: 3.330 | Reg loss: 0.032 | Tree loss: 3.330 | Accuracy: 0.119141 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 026 | Total loss: 3.321 | Reg loss: 0.032 | Tree loss: 3.321 | Accuracy: 0.115234 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 026 | Total loss: 3.291 | Reg loss: 0.032 | Tree loss: 3.291 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 026 | Total loss: 3.316 | Reg loss: 0.032 | Tree loss: 3.316 | Accuracy: 0.101562 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 026 | Total loss: 3.326 | Reg loss: 0.032 | Tree loss: 3.326 | Accuracy: 0.115234 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 026 | Total loss: 3.332 | Reg loss: 0.032 | Tree loss: 3.332 | Accuracy: 0.115234 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 026 | Total loss: 3.337 | Reg loss: 0.032 | Tree loss: 3.337 | Accuracy: 0.126953 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 026 | Total loss: 3.284 | Reg loss: 0.032 | Tree loss: 3.284 | Accuracy: 0.119141 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 026 | Total loss: 3.346 | Reg loss: 0.032 | Tree loss: 3.346 | Accuracy: 0.121094 | 5.124 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 009 / 026 | Total loss: 3.300 | Reg loss: 0.032 | Tree loss: 3.300 | Accuracy: 0.121094 | 5.124 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 026 | Total loss: 3.282 | Reg loss: 0.032 | Tree loss: 3.282 | Accuracy: 0.152344 | 5.124 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 026 | Total loss: 3.276 | Reg loss: 0.032 | Tree loss: 3.276 | Accuracy: 0.144531 | 5.124 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 026 | Total loss: 3.338 | Reg loss: 0.032 | Tree loss: 3.338 | Accuracy: 0.111328 | 5.124 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 026 | Total loss: 3.289 | Reg loss: 0.032 | Tree loss: 3.289 | Accuracy: 0.101562 | 5.124 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 026 | Total loss: 3.320 | Reg loss: 0.032 | Tree loss: 3.320 | Accuracy: 0.121094 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 026 | Total loss: 3.285 | Reg loss: 0.032 | Tree loss: 3.285 | Accuracy: 0.121094 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 026 | Total loss: 3.265 | Reg loss: 0.032 | Tree loss: 3.265 | Accuracy: 0.152344 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 026 | Total loss: 3.272 | Reg loss: 0.032 | Tree loss: 3.272 | Accuracy: 0.146484 | 5.123 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 026 | Total loss: 3.317 | Reg loss: 0.032 | Tree loss: 3.317 | Accuracy: 0.115234 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 026 | Total loss: 3.278 | Reg loss: 0.032 | Tree loss: 3.278 | Accuracy: 0.119141 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 026 | Total loss: 3.300 | Reg loss: 0.032 | Tree loss: 3.300 | Accuracy: 0.097656 | 5.121 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 026 | Total loss: 3.298 | Reg loss: 0.032 | Tree loss: 3.298 | Accuracy: 0.107422 | 5.121 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 026 | Total loss: 3.223 | Reg loss: 0.032 | Tree loss: 3.223 | Accuracy: 0.171875 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 026 | Total loss: 3.278 | Reg loss: 0.032 | Tree loss: 3.278 | Accuracy: 0.121094 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 026 | Total loss: 3.252 | Reg loss: 0.032 | Tree loss: 3.252 | Accuracy: 0.125000 | 5.122 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 026 | Total loss: 3.284 | Reg loss: 0.032 | Tree loss: 3.284 | Accuracy: 0.129032 | 5.121 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 026 | Total loss: 3.309 | Reg loss: 0.032 | Tree loss: 3.309 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 026 | Total loss: 3.284 | Reg loss: 0.032 | Tree loss: 3.284 | Accuracy: 0.136719 | 5.122 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 026 | Total loss: 3.284 | Reg loss: 0.032 | Tree loss: 3.284 | Accuracy: 0.117188 | 5.122 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 026 | Total loss: 3.275 | Reg loss: 0.032 | Tree loss: 3.275 | Accuracy: 0.123047 | 5.122 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 026 | Total loss: 3.288 | Reg loss: 0.032 | Tree loss: 3.288 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 026 | Total loss: 3.235 | Reg loss: 0.032 | Tree loss: 3.235 | Accuracy: 0.136719 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 026 | Total loss: 3.288 | Reg loss: 0.032 | Tree loss: 3.288 | Accuracy: 0.130859 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 026 | Total loss: 3.289 | Reg loss: 0.032 | Tree loss: 3.289 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 026 | Total loss: 3.276 | Reg loss: 0.032 | Tree loss: 3.276 | Accuracy: 0.148438 | 5.12 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 026 | Total loss: 3.290 | Reg loss: 0.032 | Tree loss: 3.290 | Accuracy: 0.138672 | 5.12 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 026 | Total loss: 3.251 | Reg loss: 0.032 | Tree loss: 3.251 | Accuracy: 0.142578 | 5.12 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 026 | Total loss: 3.316 | Reg loss: 0.032 | Tree loss: 3.316 | Accuracy: 0.097656 | 5.119 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 026 | Total loss: 3.242 | Reg loss: 0.032 | Tree loss: 3.242 | Accuracy: 0.126953 | 5.118 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 026 | Total loss: 3.285 | Reg loss: 0.032 | Tree loss: 3.285 | Accuracy: 0.115234 | 5.119 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 026 | Total loss: 3.286 | Reg loss: 0.032 | Tree loss: 3.286 | Accuracy: 0.142578 | 5.12 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 026 | Total loss: 3.334 | Reg loss: 0.032 | Tree loss: 3.334 | Accuracy: 0.089844 | 5.12 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 026 | Total loss: 3.273 | Reg loss: 0.032 | Tree loss: 3.273 | Accuracy: 0.128906 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 026 | Total loss: 3.263 | Reg loss: 0.032 | Tree loss: 3.263 | Accuracy: 0.123047 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 026 | Total loss: 3.262 | Reg loss: 0.032 | Tree loss: 3.262 | Accuracy: 0.099609 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 026 | Total loss: 3.241 | Reg loss: 0.032 | Tree loss: 3.241 | Accuracy: 0.126953 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 026 | Total loss: 3.225 | Reg loss: 0.032 | Tree loss: 3.225 | Accuracy: 0.113281 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 026 | Total loss: 3.261 | Reg loss: 0.032 | Tree loss: 3.261 | Accuracy: 0.113281 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 026 | Total loss: 3.225 | Reg loss: 0.032 | Tree loss: 3.225 | Accuracy: 0.130859 | 5.121 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 026 | Total loss: 3.229 | Reg loss: 0.032 | Tree loss: 3.229 | Accuracy: 0.109375 | 5.12 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 026 | Total loss: 3.262 | Reg loss: 0.032 | Tree loss: 3.262 | Accuracy: 0.117188 | 5.12 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 026 | Total loss: 3.214 | Reg loss: 0.032 | Tree loss: 3.214 | Accuracy: 0.141935 | 5.119 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 026 | Total loss: 3.295 | Reg loss: 0.032 | Tree loss: 3.295 | Accuracy: 0.121094 | 5.122 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 026 | Total loss: 3.291 | Reg loss: 0.032 | Tree loss: 3.291 | Accuracy: 0.107422 | 5.122 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 026 | Total loss: 3.273 | Reg loss: 0.032 | Tree loss: 3.273 | Accuracy: 0.128906 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 026 | Total loss: 3.297 | Reg loss: 0.032 | Tree loss: 3.297 | Accuracy: 0.109375 | 5.122 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 026 | Total loss: 3.256 | Reg loss: 0.032 | Tree loss: 3.256 | Accuracy: 0.125000 | 5.122 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 026 | Total loss: 3.245 | Reg loss: 0.032 | Tree loss: 3.245 | Accuracy: 0.132812 | 5.122 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 026 | Total loss: 3.268 | Reg loss: 0.032 | Tree loss: 3.268 | Accuracy: 0.128906 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 026 | Total loss: 3.237 | Reg loss: 0.032 | Tree loss: 3.237 | Accuracy: 0.119141 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 026 | Total loss: 3.230 | Reg loss: 0.032 | Tree loss: 3.230 | Accuracy: 0.138672 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 026 | Total loss: 3.241 | Reg loss: 0.032 | Tree loss: 3.241 | Accuracy: 0.115234 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 026 | Total loss: 3.243 | Reg loss: 0.032 | Tree loss: 3.243 | Accuracy: 0.126953 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 026 | Total loss: 3.217 | Reg loss: 0.032 | Tree loss: 3.217 | Accuracy: 0.134766 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 026 | Total loss: 3.245 | Reg loss: 0.032 | Tree loss: 3.245 | Accuracy: 0.128906 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 026 | Total loss: 3.209 | Reg loss: 0.032 | Tree loss: 3.209 | Accuracy: 0.125000 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 026 | Total loss: 3.276 | Reg loss: 0.032 | Tree loss: 3.276 | Accuracy: 0.111328 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 026 | Total loss: 3.241 | Reg loss: 0.032 | Tree loss: 3.241 | Accuracy: 0.132812 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 026 | Total loss: 3.277 | Reg loss: 0.032 | Tree loss: 3.277 | Accuracy: 0.136719 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 026 | Total loss: 3.254 | Reg loss: 0.032 | Tree loss: 3.254 | Accuracy: 0.123047 | 5.124 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 018 / 026 | Total loss: 3.251 | Reg loss: 0.032 | Tree loss: 3.251 | Accuracy: 0.105469 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 026 | Total loss: 3.198 | Reg loss: 0.032 | Tree loss: 3.198 | Accuracy: 0.125000 | 5.124 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 026 | Total loss: 3.241 | Reg loss: 0.032 | Tree loss: 3.241 | Accuracy: 0.123047 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 026 | Total loss: 3.192 | Reg loss: 0.032 | Tree loss: 3.192 | Accuracy: 0.126953 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 026 | Total loss: 3.200 | Reg loss: 0.032 | Tree loss: 3.200 | Accuracy: 0.132812 | 5.122 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 026 | Total loss: 3.188 | Reg loss: 0.032 | Tree loss: 3.188 | Accuracy: 0.144531 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 026 | Total loss: 3.269 | Reg loss: 0.032 | Tree loss: 3.269 | Accuracy: 0.093750 | 5.123 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 026 | Total loss: 3.188 | Reg loss: 0.032 | Tree loss: 3.188 | Accuracy: 0.103226 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 026 | Total loss: 3.222 | Reg loss: 0.032 | Tree loss: 3.222 | Accuracy: 0.109375 | 5.123 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 026 | Total loss: 3.251 | Reg loss: 0.032 | Tree loss: 3.251 | Accuracy: 0.121094 | 5.123 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 026 | Total loss: 3.243 | Reg loss: 0.032 | Tree loss: 3.243 | Accuracy: 0.134766 | 5.123 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 026 | Total loss: 3.284 | Reg loss: 0.032 | Tree loss: 3.284 | Accuracy: 0.132812 | 5.123 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 026 | Total loss: 3.267 | Reg loss: 0.032 | Tree loss: 3.267 | Accuracy: 0.111328 | 5.123 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 026 | Total loss: 3.216 | Reg loss: 0.032 | Tree loss: 3.216 | Accuracy: 0.142578 | 5.123 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 026 | Total loss: 3.235 | Reg loss: 0.032 | Tree loss: 3.235 | Accuracy: 0.138672 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 026 | Total loss: 3.250 | Reg loss: 0.032 | Tree loss: 3.250 | Accuracy: 0.123047 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 026 | Total loss: 3.215 | Reg loss: 0.032 | Tree loss: 3.215 | Accuracy: 0.132812 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 026 | Total loss: 3.202 | Reg loss: 0.032 | Tree loss: 3.202 | Accuracy: 0.130859 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 026 | Total loss: 3.204 | Reg loss: 0.032 | Tree loss: 3.204 | Accuracy: 0.130859 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 026 | Total loss: 3.264 | Reg loss: 0.032 | Tree loss: 3.264 | Accuracy: 0.119141 | 5.121 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 026 | Total loss: 3.195 | Reg loss: 0.032 | Tree loss: 3.195 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 026 | Total loss: 3.221 | Reg loss: 0.032 | Tree loss: 3.221 | Accuracy: 0.111328 | 5.12 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 026 | Total loss: 3.243 | Reg loss: 0.032 | Tree loss: 3.243 | Accuracy: 0.107422 | 5.12 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 026 | Total loss: 3.231 | Reg loss: 0.032 | Tree loss: 3.231 | Accuracy: 0.117188 | 5.12 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 026 | Total loss: 3.192 | Reg loss: 0.032 | Tree loss: 3.192 | Accuracy: 0.160156 | 5.121 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 026 | Total loss: 3.214 | Reg loss: 0.032 | Tree loss: 3.214 | Accuracy: 0.109375 | 5.121 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 026 | Total loss: 3.201 | Reg loss: 0.032 | Tree loss: 3.201 | Accuracy: 0.115234 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 026 | Total loss: 3.219 | Reg loss: 0.032 | Tree loss: 3.219 | Accuracy: 0.111328 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 026 | Total loss: 3.210 | Reg loss: 0.032 | Tree loss: 3.210 | Accuracy: 0.123047 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 026 | Total loss: 3.180 | Reg loss: 0.032 | Tree loss: 3.180 | Accuracy: 0.130859 | 5.122 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 026 | Total loss: 3.196 | Reg loss: 0.032 | Tree loss: 3.196 | Accuracy: 0.107422 | 5.121 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 026 | Total loss: 3.173 | Reg loss: 0.032 | Tree loss: 3.173 | Accuracy: 0.125000 | 5.121 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 026 | Total loss: 3.210 | Reg loss: 0.032 | Tree loss: 3.210 | Accuracy: 0.126953 | 5.121 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 026 | Total loss: 3.222 | Reg loss: 0.032 | Tree loss: 3.222 | Accuracy: 0.103226 | 5.12 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 026 | Total loss: 3.226 | Reg loss: 0.032 | Tree loss: 3.226 | Accuracy: 0.126953 | 5.123 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 026 | Total loss: 3.227 | Reg loss: 0.032 | Tree loss: 3.227 | Accuracy: 0.126953 | 5.123 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 026 | Total loss: 3.248 | Reg loss: 0.032 | Tree loss: 3.248 | Accuracy: 0.113281 | 5.123 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 026 | Total loss: 3.221 | Reg loss: 0.032 | Tree loss: 3.221 | Accuracy: 0.128906 | 5.123 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 026 | Total loss: 3.254 | Reg loss: 0.032 | Tree loss: 3.254 | Accuracy: 0.121094 | 5.124 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 026 | Total loss: 3.260 | Reg loss: 0.032 | Tree loss: 3.260 | Accuracy: 0.103516 | 5.123 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 026 | Total loss: 3.224 | Reg loss: 0.032 | Tree loss: 3.224 | Accuracy: 0.132812 | 5.123 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 026 | Total loss: 3.245 | Reg loss: 0.032 | Tree loss: 3.245 | Accuracy: 0.111328 | 5.124 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 026 | Total loss: 3.217 | Reg loss: 0.032 | Tree loss: 3.217 | Accuracy: 0.117188 | 5.124 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 026 | Total loss: 3.194 | Reg loss: 0.032 | Tree loss: 3.194 | Accuracy: 0.115234 | 5.125 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 026 | Total loss: 3.233 | Reg loss: 0.032 | Tree loss: 3.233 | Accuracy: 0.132812 | 5.125 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 026 | Total loss: 3.215 | Reg loss: 0.032 | Tree loss: 3.215 | Accuracy: 0.138672 | 5.125 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 026 | Total loss: 3.197 | Reg loss: 0.032 | Tree loss: 3.197 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 026 | Total loss: 3.200 | Reg loss: 0.032 | Tree loss: 3.200 | Accuracy: 0.128906 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 026 | Total loss: 3.163 | Reg loss: 0.032 | Tree loss: 3.163 | Accuracy: 0.128906 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 026 | Total loss: 3.153 | Reg loss: 0.032 | Tree loss: 3.153 | Accuracy: 0.144531 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 026 | Total loss: 3.169 | Reg loss: 0.032 | Tree loss: 3.169 | Accuracy: 0.121094 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 026 | Total loss: 3.198 | Reg loss: 0.032 | Tree loss: 3.198 | Accuracy: 0.115234 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 026 | Total loss: 3.187 | Reg loss: 0.032 | Tree loss: 3.187 | Accuracy: 0.109375 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 026 | Total loss: 3.164 | Reg loss: 0.032 | Tree loss: 3.164 | Accuracy: 0.150391 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 026 | Total loss: 3.159 | Reg loss: 0.032 | Tree loss: 3.159 | Accuracy: 0.132812 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 026 | Total loss: 3.166 | Reg loss: 0.032 | Tree loss: 3.166 | Accuracy: 0.113281 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 026 | Total loss: 3.156 | Reg loss: 0.032 | Tree loss: 3.156 | Accuracy: 0.142578 | 5.126 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 026 | Total loss: 3.168 | Reg loss: 0.032 | Tree loss: 3.168 | Accuracy: 0.113281 | 5.125 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 026 | Total loss: 3.171 | Reg loss: 0.032 | Tree loss: 3.171 | Accuracy: 0.095703 | 5.124 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 026 | Total loss: 3.154 | Reg loss: 0.032 | Tree loss: 3.154 | Accuracy: 0.141935 | 5.123 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 000 / 026 | Total loss: 3.169 | Reg loss: 0.032 | Tree loss: 3.169 | Accuracy: 0.148438 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 026 | Total loss: 3.223 | Reg loss: 0.032 | Tree loss: 3.223 | Accuracy: 0.130859 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 026 | Total loss: 3.169 | Reg loss: 0.032 | Tree loss: 3.169 | Accuracy: 0.140625 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 026 | Total loss: 3.226 | Reg loss: 0.032 | Tree loss: 3.226 | Accuracy: 0.111328 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 026 | Total loss: 3.164 | Reg loss: 0.032 | Tree loss: 3.164 | Accuracy: 0.128906 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 026 | Total loss: 3.208 | Reg loss: 0.032 | Tree loss: 3.208 | Accuracy: 0.107422 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 026 | Total loss: 3.187 | Reg loss: 0.032 | Tree loss: 3.187 | Accuracy: 0.115234 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 026 | Total loss: 3.171 | Reg loss: 0.032 | Tree loss: 3.171 | Accuracy: 0.132812 | 5.124 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 026 | Total loss: 3.174 | Reg loss: 0.032 | Tree loss: 3.174 | Accuracy: 0.119141 | 5.123 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 026 | Total loss: 3.225 | Reg loss: 0.032 | Tree loss: 3.225 | Accuracy: 0.136719 | 5.123 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 026 | Total loss: 3.174 | Reg loss: 0.032 | Tree loss: 3.174 | Accuracy: 0.128906 | 5.123 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 026 | Total loss: 3.159 | Reg loss: 0.032 | Tree loss: 3.159 | Accuracy: 0.130859 | 5.123 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 026 | Total loss: 3.207 | Reg loss: 0.032 | Tree loss: 3.207 | Accuracy: 0.113281 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 026 | Total loss: 3.162 | Reg loss: 0.032 | Tree loss: 3.162 | Accuracy: 0.140625 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 026 | Total loss: 3.207 | Reg loss: 0.032 | Tree loss: 3.207 | Accuracy: 0.138672 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 026 | Total loss: 3.204 | Reg loss: 0.032 | Tree loss: 3.204 | Accuracy: 0.134766 | 5.121 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 026 | Total loss: 3.158 | Reg loss: 0.032 | Tree loss: 3.158 | Accuracy: 0.101562 | 5.121 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 026 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.140625 | 5.121 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 026 | Total loss: 3.219 | Reg loss: 0.032 | Tree loss: 3.219 | Accuracy: 0.105469 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 026 | Total loss: 3.173 | Reg loss: 0.032 | Tree loss: 3.173 | Accuracy: 0.107422 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 026 | Total loss: 3.179 | Reg loss: 0.032 | Tree loss: 3.179 | Accuracy: 0.109375 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 026 | Total loss: 3.180 | Reg loss: 0.032 | Tree loss: 3.180 | Accuracy: 0.128906 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 026 | Total loss: 3.165 | Reg loss: 0.032 | Tree loss: 3.165 | Accuracy: 0.107422 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 026 | Total loss: 3.150 | Reg loss: 0.032 | Tree loss: 3.150 | Accuracy: 0.126953 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 026 | Total loss: 3.161 | Reg loss: 0.032 | Tree loss: 3.161 | Accuracy: 0.113281 | 5.122 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 026 | Total loss: 3.120 | Reg loss: 0.032 | Tree loss: 3.120 | Accuracy: 0.096774 | 5.121 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 026 | Total loss: 3.222 | Reg loss: 0.032 | Tree loss: 3.222 | Accuracy: 0.111328 | 5.123 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 026 | Total loss: 3.171 | Reg loss: 0.032 | Tree loss: 3.171 | Accuracy: 0.142578 | 5.124 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 026 | Total loss: 3.202 | Reg loss: 0.032 | Tree loss: 3.202 | Accuracy: 0.111328 | 5.124 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 026 | Total loss: 3.159 | Reg loss: 0.032 | Tree loss: 3.159 | Accuracy: 0.113281 | 5.125 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 026 | Total loss: 3.191 | Reg loss: 0.032 | Tree loss: 3.191 | Accuracy: 0.107422 | 5.125 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 026 | Total loss: 3.158 | Reg loss: 0.032 | Tree loss: 3.158 | Accuracy: 0.097656 | 5.126 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 026 | Total loss: 3.205 | Reg loss: 0.032 | Tree loss: 3.205 | Accuracy: 0.119141 | 5.126 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 026 | Total loss: 3.173 | Reg loss: 0.032 | Tree loss: 3.173 | Accuracy: 0.123047 | 5.125 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 026 | Total loss: 3.148 | Reg loss: 0.032 | Tree loss: 3.148 | Accuracy: 0.146484 | 5.126 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 026 | Total loss: 3.199 | Reg loss: 0.032 | Tree loss: 3.199 | Accuracy: 0.111328 | 5.126 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 026 | Total loss: 3.149 | Reg loss: 0.032 | Tree loss: 3.149 | Accuracy: 0.138672 | 5.126 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 026 | Total loss: 3.165 | Reg loss: 0.032 | Tree loss: 3.165 | Accuracy: 0.146484 | 5.127 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 026 | Total loss: 3.190 | Reg loss: 0.032 | Tree loss: 3.190 | Accuracy: 0.091797 | 5.127 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 026 | Total loss: 3.162 | Reg loss: 0.032 | Tree loss: 3.162 | Accuracy: 0.134766 | 5.128 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 026 | Total loss: 3.135 | Reg loss: 0.032 | Tree loss: 3.135 | Accuracy: 0.142578 | 5.128 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 026 | Total loss: 3.192 | Reg loss: 0.032 | Tree loss: 3.192 | Accuracy: 0.117188 | 5.128 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 026 | Total loss: 3.185 | Reg loss: 0.032 | Tree loss: 3.185 | Accuracy: 0.134766 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 026 | Total loss: 3.114 | Reg loss: 0.032 | Tree loss: 3.114 | Accuracy: 0.134766 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 026 | Total loss: 3.121 | Reg loss: 0.032 | Tree loss: 3.121 | Accuracy: 0.128906 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 026 | Total loss: 3.198 | Reg loss: 0.032 | Tree loss: 3.198 | Accuracy: 0.085938 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 026 | Total loss: 3.139 | Reg loss: 0.032 | Tree loss: 3.139 | Accuracy: 0.125000 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 026 | Total loss: 3.126 | Reg loss: 0.032 | Tree loss: 3.126 | Accuracy: 0.140625 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 026 | Total loss: 3.167 | Reg loss: 0.032 | Tree loss: 3.167 | Accuracy: 0.105469 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 026 | Total loss: 3.044 | Reg loss: 0.032 | Tree loss: 3.044 | Accuracy: 0.162109 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 026 | Total loss: 3.147 | Reg loss: 0.032 | Tree loss: 3.147 | Accuracy: 0.113281 | 5.129 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 026 | Total loss: 3.134 | Reg loss: 0.032 | Tree loss: 3.134 | Accuracy: 0.141935 | 5.128 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 026 | Total loss: 3.142 | Reg loss: 0.032 | Tree loss: 3.142 | Accuracy: 0.136719 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 026 | Total loss: 3.164 | Reg loss: 0.032 | Tree loss: 3.164 | Accuracy: 0.117188 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 026 | Total loss: 3.179 | Reg loss: 0.032 | Tree loss: 3.179 | Accuracy: 0.132812 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 026 | Total loss: 3.186 | Reg loss: 0.032 | Tree loss: 3.186 | Accuracy: 0.138672 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 026 | Total loss: 3.159 | Reg loss: 0.032 | Tree loss: 3.159 | Accuracy: 0.123047 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 026 | Total loss: 3.124 | Reg loss: 0.032 | Tree loss: 3.124 | Accuracy: 0.123047 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 026 | Total loss: 3.141 | Reg loss: 0.032 | Tree loss: 3.141 | Accuracy: 0.113281 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 026 | Total loss: 3.184 | Reg loss: 0.032 | Tree loss: 3.184 | Accuracy: 0.109375 | 5.128 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 026 | Total loss: 3.158 | Reg loss: 0.032 | Tree loss: 3.158 | Accuracy: 0.125000 | 5.127 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 009 / 026 | Total loss: 3.204 | Reg loss: 0.032 | Tree loss: 3.204 | Accuracy: 0.125000 | 5.127 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 026 | Total loss: 3.167 | Reg loss: 0.032 | Tree loss: 3.167 | Accuracy: 0.130859 | 5.127 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 026 | Total loss: 3.133 | Reg loss: 0.032 | Tree loss: 3.133 | Accuracy: 0.095703 | 5.127 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 026 | Total loss: 3.142 | Reg loss: 0.032 | Tree loss: 3.142 | Accuracy: 0.138672 | 5.126 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 026 | Total loss: 3.088 | Reg loss: 0.032 | Tree loss: 3.088 | Accuracy: 0.134766 | 5.126 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 026 | Total loss: 3.119 | Reg loss: 0.032 | Tree loss: 3.119 | Accuracy: 0.128906 | 5.126 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 026 | Total loss: 3.150 | Reg loss: 0.032 | Tree loss: 3.150 | Accuracy: 0.128906 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 026 | Total loss: 3.135 | Reg loss: 0.032 | Tree loss: 3.135 | Accuracy: 0.107422 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 026 | Total loss: 3.133 | Reg loss: 0.032 | Tree loss: 3.133 | Accuracy: 0.111328 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 026 | Total loss: 3.156 | Reg loss: 0.032 | Tree loss: 3.156 | Accuracy: 0.121094 | 5.124 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 026 | Total loss: 3.143 | Reg loss: 0.032 | Tree loss: 3.143 | Accuracy: 0.125000 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 026 | Total loss: 3.135 | Reg loss: 0.032 | Tree loss: 3.135 | Accuracy: 0.113281 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 026 | Total loss: 3.129 | Reg loss: 0.032 | Tree loss: 3.129 | Accuracy: 0.121094 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 026 | Total loss: 3.165 | Reg loss: 0.032 | Tree loss: 3.165 | Accuracy: 0.121094 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 026 | Total loss: 3.088 | Reg loss: 0.032 | Tree loss: 3.088 | Accuracy: 0.136719 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 026 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.126953 | 5.125 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 026 | Total loss: 3.120 | Reg loss: 0.032 | Tree loss: 3.120 | Accuracy: 0.141935 | 5.124 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 026 | Total loss: 3.150 | Reg loss: 0.032 | Tree loss: 3.150 | Accuracy: 0.126953 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 026 | Total loss: 3.166 | Reg loss: 0.032 | Tree loss: 3.166 | Accuracy: 0.097656 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 026 | Total loss: 3.185 | Reg loss: 0.032 | Tree loss: 3.185 | Accuracy: 0.125000 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 026 | Total loss: 3.103 | Reg loss: 0.032 | Tree loss: 3.103 | Accuracy: 0.136719 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 026 | Total loss: 3.148 | Reg loss: 0.032 | Tree loss: 3.148 | Accuracy: 0.128906 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 026 | Total loss: 3.176 | Reg loss: 0.032 | Tree loss: 3.176 | Accuracy: 0.119141 | 5.128 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 026 | Total loss: 3.191 | Reg loss: 0.032 | Tree loss: 3.191 | Accuracy: 0.125000 | 5.128 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 026 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.119141 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 026 | Total loss: 3.145 | Reg loss: 0.032 | Tree loss: 3.145 | Accuracy: 0.121094 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 026 | Total loss: 3.121 | Reg loss: 0.032 | Tree loss: 3.121 | Accuracy: 0.140625 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 026 | Total loss: 3.129 | Reg loss: 0.032 | Tree loss: 3.129 | Accuracy: 0.130859 | 5.126 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 026 | Total loss: 3.110 | Reg loss: 0.032 | Tree loss: 3.110 | Accuracy: 0.142578 | 5.126 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 026 | Total loss: 3.129 | Reg loss: 0.032 | Tree loss: 3.129 | Accuracy: 0.130859 | 5.127 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 026 | Total loss: 3.111 | Reg loss: 0.032 | Tree loss: 3.111 | Accuracy: 0.125000 | 5.126 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 026 | Total loss: 3.093 | Reg loss: 0.032 | Tree loss: 3.093 | Accuracy: 0.144531 | 5.126 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 026 | Total loss: 3.127 | Reg loss: 0.032 | Tree loss: 3.127 | Accuracy: 0.136719 | 5.126 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 026 | Total loss: 3.118 | Reg loss: 0.032 | Tree loss: 3.118 | Accuracy: 0.123047 | 5.126 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 026 | Total loss: 3.135 | Reg loss: 0.032 | Tree loss: 3.135 | Accuracy: 0.109375 | 5.126 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 026 | Total loss: 3.134 | Reg loss: 0.032 | Tree loss: 3.134 | Accuracy: 0.119141 | 5.125 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 026 | Total loss: 3.090 | Reg loss: 0.032 | Tree loss: 3.090 | Accuracy: 0.113281 | 5.125 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 026 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.136719 | 5.125 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 026 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.123047 | 5.125 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 026 | Total loss: 3.148 | Reg loss: 0.032 | Tree loss: 3.148 | Accuracy: 0.113281 | 5.124 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 026 | Total loss: 3.097 | Reg loss: 0.032 | Tree loss: 3.097 | Accuracy: 0.121094 | 5.124 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 026 | Total loss: 3.142 | Reg loss: 0.032 | Tree loss: 3.142 | Accuracy: 0.085938 | 5.124 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 026 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.109677 | 5.123 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 026 | Total loss: 3.123 | Reg loss: 0.032 | Tree loss: 3.123 | Accuracy: 0.117188 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 026 | Total loss: 3.164 | Reg loss: 0.032 | Tree loss: 3.164 | Accuracy: 0.117188 | 5.124 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 026 | Total loss: 3.136 | Reg loss: 0.032 | Tree loss: 3.136 | Accuracy: 0.125000 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 026 | Total loss: 3.171 | Reg loss: 0.032 | Tree loss: 3.171 | Accuracy: 0.126953 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 026 | Total loss: 3.116 | Reg loss: 0.032 | Tree loss: 3.116 | Accuracy: 0.123047 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 026 | Total loss: 3.145 | Reg loss: 0.032 | Tree loss: 3.145 | Accuracy: 0.103516 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 026 | Total loss: 3.109 | Reg loss: 0.032 | Tree loss: 3.109 | Accuracy: 0.162109 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 026 | Total loss: 3.105 | Reg loss: 0.032 | Tree loss: 3.105 | Accuracy: 0.130859 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 026 | Total loss: 3.149 | Reg loss: 0.032 | Tree loss: 3.149 | Accuracy: 0.132812 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 026 | Total loss: 3.162 | Reg loss: 0.032 | Tree loss: 3.162 | Accuracy: 0.101562 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 026 | Total loss: 3.153 | Reg loss: 0.032 | Tree loss: 3.153 | Accuracy: 0.138672 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 026 | Total loss: 3.113 | Reg loss: 0.032 | Tree loss: 3.113 | Accuracy: 0.111328 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 026 | Total loss: 3.118 | Reg loss: 0.032 | Tree loss: 3.118 | Accuracy: 0.111328 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 026 | Total loss: 3.090 | Reg loss: 0.032 | Tree loss: 3.090 | Accuracy: 0.109375 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 026 | Total loss: 3.096 | Reg loss: 0.032 | Tree loss: 3.096 | Accuracy: 0.103516 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 026 | Total loss: 3.106 | Reg loss: 0.032 | Tree loss: 3.106 | Accuracy: 0.134766 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 026 | Total loss: 3.110 | Reg loss: 0.032 | Tree loss: 3.110 | Accuracy: 0.115234 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 026 | Total loss: 3.078 | Reg loss: 0.032 | Tree loss: 3.078 | Accuracy: 0.134766 | 5.126 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 018 / 026 | Total loss: 3.114 | Reg loss: 0.032 | Tree loss: 3.114 | Accuracy: 0.128906 | 5.126 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 026 | Total loss: 3.123 | Reg loss: 0.032 | Tree loss: 3.123 | Accuracy: 0.109375 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 026 | Total loss: 3.090 | Reg loss: 0.032 | Tree loss: 3.090 | Accuracy: 0.113281 | 5.124 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 026 | Total loss: 3.089 | Reg loss: 0.032 | Tree loss: 3.089 | Accuracy: 0.115234 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 026 | Total loss: 3.108 | Reg loss: 0.032 | Tree loss: 3.108 | Accuracy: 0.128906 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.132812 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 026 | Total loss: 3.078 | Reg loss: 0.032 | Tree loss: 3.078 | Accuracy: 0.160156 | 5.125 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 026 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.135484 | 5.124 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 026 | Total loss: 3.100 | Reg loss: 0.032 | Tree loss: 3.100 | Accuracy: 0.148438 | 5.127 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 026 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.125000 | 5.127 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 026 | Total loss: 3.172 | Reg loss: 0.032 | Tree loss: 3.172 | Accuracy: 0.117188 | 5.127 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 026 | Total loss: 3.118 | Reg loss: 0.032 | Tree loss: 3.118 | Accuracy: 0.123047 | 5.128 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 026 | Total loss: 3.087 | Reg loss: 0.032 | Tree loss: 3.087 | Accuracy: 0.123047 | 5.128 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 026 | Total loss: 3.086 | Reg loss: 0.032 | Tree loss: 3.086 | Accuracy: 0.119141 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 026 | Total loss: 3.095 | Reg loss: 0.032 | Tree loss: 3.095 | Accuracy: 0.130859 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 026 | Total loss: 3.125 | Reg loss: 0.032 | Tree loss: 3.125 | Accuracy: 0.119141 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 026 | Total loss: 3.113 | Reg loss: 0.032 | Tree loss: 3.113 | Accuracy: 0.126953 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 026 | Total loss: 3.076 | Reg loss: 0.032 | Tree loss: 3.076 | Accuracy: 0.134766 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 026 | Total loss: 3.111 | Reg loss: 0.032 | Tree loss: 3.111 | Accuracy: 0.109375 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 026 | Total loss: 3.081 | Reg loss: 0.032 | Tree loss: 3.081 | Accuracy: 0.132812 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 026 | Total loss: 3.113 | Reg loss: 0.032 | Tree loss: 3.113 | Accuracy: 0.115234 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 026 | Total loss: 3.089 | Reg loss: 0.032 | Tree loss: 3.089 | Accuracy: 0.128906 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 026 | Total loss: 3.123 | Reg loss: 0.032 | Tree loss: 3.123 | Accuracy: 0.146484 | 5.129 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 026 | Total loss: 3.097 | Reg loss: 0.032 | Tree loss: 3.097 | Accuracy: 0.134766 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 026 | Total loss: 3.148 | Reg loss: 0.032 | Tree loss: 3.148 | Accuracy: 0.121094 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 026 | Total loss: 3.079 | Reg loss: 0.032 | Tree loss: 3.079 | Accuracy: 0.107422 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 026 | Total loss: 3.087 | Reg loss: 0.032 | Tree loss: 3.087 | Accuracy: 0.130859 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 026 | Total loss: 3.078 | Reg loss: 0.032 | Tree loss: 3.078 | Accuracy: 0.130859 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 026 | Total loss: 3.100 | Reg loss: 0.032 | Tree loss: 3.100 | Accuracy: 0.109375 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 026 | Total loss: 3.081 | Reg loss: 0.032 | Tree loss: 3.081 | Accuracy: 0.125000 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 026 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.121094 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 026 | Total loss: 3.135 | Reg loss: 0.032 | Tree loss: 3.135 | Accuracy: 0.107422 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 026 | Total loss: 3.110 | Reg loss: 0.032 | Tree loss: 3.110 | Accuracy: 0.107422 | 5.13 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 026 | Total loss: 3.051 | Reg loss: 0.032 | Tree loss: 3.051 | Accuracy: 0.109677 | 5.129 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 026 | Total loss: 3.121 | Reg loss: 0.032 | Tree loss: 3.121 | Accuracy: 0.117188 | 5.131 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 026 | Total loss: 3.170 | Reg loss: 0.032 | Tree loss: 3.170 | Accuracy: 0.109375 | 5.131 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 026 | Total loss: 3.083 | Reg loss: 0.032 | Tree loss: 3.083 | Accuracy: 0.125000 | 5.131 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 026 | Total loss: 3.177 | Reg loss: 0.032 | Tree loss: 3.177 | Accuracy: 0.109375 | 5.131 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 026 | Total loss: 3.162 | Reg loss: 0.032 | Tree loss: 3.162 | Accuracy: 0.095703 | 5.131 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 026 | Total loss: 3.099 | Reg loss: 0.032 | Tree loss: 3.099 | Accuracy: 0.113281 | 5.131 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 026 | Total loss: 3.089 | Reg loss: 0.032 | Tree loss: 3.089 | Accuracy: 0.132812 | 5.132 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 026 | Total loss: 3.097 | Reg loss: 0.032 | Tree loss: 3.097 | Accuracy: 0.130859 | 5.132 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 026 | Total loss: 3.105 | Reg loss: 0.032 | Tree loss: 3.105 | Accuracy: 0.126953 | 5.132 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 026 | Total loss: 3.078 | Reg loss: 0.032 | Tree loss: 3.078 | Accuracy: 0.152344 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 026 | Total loss: 3.094 | Reg loss: 0.032 | Tree loss: 3.094 | Accuracy: 0.144531 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 026 | Total loss: 3.067 | Reg loss: 0.032 | Tree loss: 3.067 | Accuracy: 0.128906 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 026 | Total loss: 3.104 | Reg loss: 0.032 | Tree loss: 3.104 | Accuracy: 0.142578 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 026 | Total loss: 3.043 | Reg loss: 0.032 | Tree loss: 3.043 | Accuracy: 0.125000 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 026 | Total loss: 3.069 | Reg loss: 0.032 | Tree loss: 3.069 | Accuracy: 0.146484 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 026 | Total loss: 3.072 | Reg loss: 0.032 | Tree loss: 3.072 | Accuracy: 0.121094 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 026 | Total loss: 3.095 | Reg loss: 0.032 | Tree loss: 3.095 | Accuracy: 0.105469 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 026 | Total loss: 3.067 | Reg loss: 0.032 | Tree loss: 3.067 | Accuracy: 0.117188 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 026 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.128906 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 026 | Total loss: 3.069 | Reg loss: 0.032 | Tree loss: 3.069 | Accuracy: 0.128906 | 5.133 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 026 | Total loss: 3.033 | Reg loss: 0.032 | Tree loss: 3.033 | Accuracy: 0.119141 | 5.132 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 026 | Total loss: 3.079 | Reg loss: 0.032 | Tree loss: 3.079 | Accuracy: 0.138672 | 5.132 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 026 | Total loss: 3.088 | Reg loss: 0.032 | Tree loss: 3.088 | Accuracy: 0.111328 | 5.131 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 026 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.117188 | 5.132 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 026 | Total loss: 3.074 | Reg loss: 0.032 | Tree loss: 3.074 | Accuracy: 0.105469 | 5.132 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 026 | Total loss: 3.140 | Reg loss: 0.032 | Tree loss: 3.140 | Accuracy: 0.116129 | 5.131 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 000 / 026 | Total loss: 3.092 | Reg loss: 0.032 | Tree loss: 3.092 | Accuracy: 0.115234 | 5.132 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 026 | Total loss: 3.098 | Reg loss: 0.032 | Tree loss: 3.098 | Accuracy: 0.140625 | 5.132 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 026 | Total loss: 3.083 | Reg loss: 0.032 | Tree loss: 3.083 | Accuracy: 0.126953 | 5.132 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 026 | Total loss: 3.114 | Reg loss: 0.032 | Tree loss: 3.114 | Accuracy: 0.136719 | 5.132 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 026 | Total loss: 3.138 | Reg loss: 0.032 | Tree loss: 3.138 | Accuracy: 0.132812 | 5.132 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 026 | Total loss: 3.103 | Reg loss: 0.032 | Tree loss: 3.103 | Accuracy: 0.142578 | 5.132 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 026 | Total loss: 3.098 | Reg loss: 0.032 | Tree loss: 3.098 | Accuracy: 0.123047 | 5.131 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 026 | Total loss: 3.109 | Reg loss: 0.032 | Tree loss: 3.109 | Accuracy: 0.103516 | 5.131 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 026 | Total loss: 3.086 | Reg loss: 0.032 | Tree loss: 3.086 | Accuracy: 0.125000 | 5.131 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.123047 | 5.131 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.134766 | 5.131 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 026 | Total loss: 3.086 | Reg loss: 0.032 | Tree loss: 3.086 | Accuracy: 0.130859 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 026 | Total loss: 3.124 | Reg loss: 0.032 | Tree loss: 3.124 | Accuracy: 0.107422 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 026 | Total loss: 3.110 | Reg loss: 0.032 | Tree loss: 3.110 | Accuracy: 0.089844 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 026 | Total loss: 3.093 | Reg loss: 0.032 | Tree loss: 3.093 | Accuracy: 0.117188 | 5.129 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 026 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.117188 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 026 | Total loss: 3.088 | Reg loss: 0.032 | Tree loss: 3.088 | Accuracy: 0.128906 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.117188 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 026 | Total loss: 3.054 | Reg loss: 0.032 | Tree loss: 3.054 | Accuracy: 0.136719 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.115234 | 5.131 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 026 | Total loss: 3.092 | Reg loss: 0.032 | Tree loss: 3.092 | Accuracy: 0.117188 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.132812 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 026 | Total loss: 3.025 | Reg loss: 0.032 | Tree loss: 3.025 | Accuracy: 0.138672 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 026 | Total loss: 3.091 | Reg loss: 0.032 | Tree loss: 3.091 | Accuracy: 0.134766 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 026 | Total loss: 3.044 | Reg loss: 0.032 | Tree loss: 3.044 | Accuracy: 0.101562 | 5.13 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.129032 | 5.129 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 026 | Total loss: 3.129 | Reg loss: 0.032 | Tree loss: 3.129 | Accuracy: 0.111328 | 5.131 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 026 | Total loss: 3.096 | Reg loss: 0.032 | Tree loss: 3.096 | Accuracy: 0.113281 | 5.131 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 026 | Total loss: 3.146 | Reg loss: 0.032 | Tree loss: 3.146 | Accuracy: 0.119141 | 5.132 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 026 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.126953 | 5.132 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 026 | Total loss: 3.092 | Reg loss: 0.032 | Tree loss: 3.092 | Accuracy: 0.113281 | 5.132 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 026 | Total loss: 3.089 | Reg loss: 0.032 | Tree loss: 3.089 | Accuracy: 0.117188 | 5.131 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 026 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.117188 | 5.132 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 026 | Total loss: 3.060 | Reg loss: 0.032 | Tree loss: 3.060 | Accuracy: 0.126953 | 5.132 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.107422 | 5.132 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 026 | Total loss: 3.082 | Reg loss: 0.032 | Tree loss: 3.082 | Accuracy: 0.119141 | 5.133 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 026 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.115234 | 5.133 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 026 | Total loss: 3.060 | Reg loss: 0.032 | Tree loss: 3.060 | Accuracy: 0.140625 | 5.133 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 026 | Total loss: 3.105 | Reg loss: 0.032 | Tree loss: 3.105 | Accuracy: 0.113281 | 5.134 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 026 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.132812 | 5.134 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 026 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.128906 | 5.134 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 026 | Total loss: 2.992 | Reg loss: 0.032 | Tree loss: 2.992 | Accuracy: 0.169922 | 5.134 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 026 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.126953 | 5.135 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 026 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.134766 | 5.135 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 026 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.105469 | 5.135 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 026 | Total loss: 3.073 | Reg loss: 0.032 | Tree loss: 3.073 | Accuracy: 0.109375 | 5.135 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.121094 | 5.135 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.121094 | 5.135 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.119141 | 5.134 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 026 | Total loss: 3.020 | Reg loss: 0.032 | Tree loss: 3.020 | Accuracy: 0.144531 | 5.134 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.134766 | 5.133 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 026 | Total loss: 3.026 | Reg loss: 0.032 | Tree loss: 3.026 | Accuracy: 0.129032 | 5.132 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 026 | Total loss: 3.137 | Reg loss: 0.032 | Tree loss: 3.137 | Accuracy: 0.125000 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 026 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.136719 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 026 | Total loss: 3.068 | Reg loss: 0.032 | Tree loss: 3.068 | Accuracy: 0.146484 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 026 | Total loss: 3.105 | Reg loss: 0.032 | Tree loss: 3.105 | Accuracy: 0.126953 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 026 | Total loss: 3.114 | Reg loss: 0.032 | Tree loss: 3.114 | Accuracy: 0.115234 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 026 | Total loss: 3.104 | Reg loss: 0.032 | Tree loss: 3.104 | Accuracy: 0.099609 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 026 | Total loss: 3.081 | Reg loss: 0.032 | Tree loss: 3.081 | Accuracy: 0.148438 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 026 | Total loss: 3.084 | Reg loss: 0.032 | Tree loss: 3.084 | Accuracy: 0.113281 | 5.133 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 026 | Total loss: 3.056 | Reg loss: 0.032 | Tree loss: 3.056 | Accuracy: 0.119141 | 5.132 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 009 / 026 | Total loss: 3.053 | Reg loss: 0.032 | Tree loss: 3.053 | Accuracy: 0.126953 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 026 | Total loss: 3.052 | Reg loss: 0.032 | Tree loss: 3.052 | Accuracy: 0.132812 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 026 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.109375 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 026 | Total loss: 3.060 | Reg loss: 0.032 | Tree loss: 3.060 | Accuracy: 0.113281 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.136719 | 5.131 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 026 | Total loss: 3.038 | Reg loss: 0.032 | Tree loss: 3.038 | Accuracy: 0.111328 | 5.131 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 026 | Total loss: 3.067 | Reg loss: 0.032 | Tree loss: 3.067 | Accuracy: 0.138672 | 5.131 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 026 | Total loss: 3.030 | Reg loss: 0.032 | Tree loss: 3.030 | Accuracy: 0.132812 | 5.13 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 026 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.115234 | 5.13 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 026 | Total loss: 3.049 | Reg loss: 0.032 | Tree loss: 3.049 | Accuracy: 0.134766 | 5.131 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 026 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.132812 | 5.131 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 026 | Total loss: 3.052 | Reg loss: 0.032 | Tree loss: 3.052 | Accuracy: 0.125000 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 026 | Total loss: 3.039 | Reg loss: 0.032 | Tree loss: 3.039 | Accuracy: 0.123047 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.101562 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.113281 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.109375 | 5.132 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 026 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.135484 | 5.131 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 026 | Total loss: 3.102 | Reg loss: 0.032 | Tree loss: 3.102 | Accuracy: 0.138672 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 026 | Total loss: 3.055 | Reg loss: 0.032 | Tree loss: 3.055 | Accuracy: 0.146484 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 026 | Total loss: 3.048 | Reg loss: 0.032 | Tree loss: 3.048 | Accuracy: 0.132812 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.140625 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 026 | Total loss: 3.088 | Reg loss: 0.032 | Tree loss: 3.088 | Accuracy: 0.117188 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 026 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.111328 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.136719 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 026 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.107422 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 026 | Total loss: 3.083 | Reg loss: 0.032 | Tree loss: 3.083 | Accuracy: 0.125000 | 5.131 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 026 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.123047 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.140625 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 026 | Total loss: 3.052 | Reg loss: 0.032 | Tree loss: 3.052 | Accuracy: 0.138672 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 026 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.128906 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 026 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.125000 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.121094 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 026 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.103516 | 5.133 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 026 | Total loss: 3.053 | Reg loss: 0.032 | Tree loss: 3.053 | Accuracy: 0.113281 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.125000 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 026 | Total loss: 3.059 | Reg loss: 0.032 | Tree loss: 3.059 | Accuracy: 0.117188 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 026 | Total loss: 3.039 | Reg loss: 0.032 | Tree loss: 3.039 | Accuracy: 0.091797 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 026 | Total loss: 3.019 | Reg loss: 0.032 | Tree loss: 3.019 | Accuracy: 0.126953 | 5.132 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 026 | Total loss: 3.018 | Reg loss: 0.032 | Tree loss: 3.018 | Accuracy: 0.123047 | 5.131 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 026 | Total loss: 3.038 | Reg loss: 0.032 | Tree loss: 3.038 | Accuracy: 0.107422 | 5.131 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 026 | Total loss: 3.029 | Reg loss: 0.032 | Tree loss: 3.029 | Accuracy: 0.125000 | 5.131 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 026 | Total loss: 3.055 | Reg loss: 0.032 | Tree loss: 3.055 | Accuracy: 0.113281 | 5.131 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 026 | Total loss: 3.033 | Reg loss: 0.032 | Tree loss: 3.033 | Accuracy: 0.161290 | 5.13 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.152344 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 026 | Total loss: 3.096 | Reg loss: 0.032 | Tree loss: 3.096 | Accuracy: 0.142578 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 026 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.125000 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 026 | Total loss: 3.039 | Reg loss: 0.032 | Tree loss: 3.039 | Accuracy: 0.123047 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 026 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.134766 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 026 | Total loss: 3.038 | Reg loss: 0.032 | Tree loss: 3.038 | Accuracy: 0.140625 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 026 | Total loss: 3.024 | Reg loss: 0.032 | Tree loss: 3.024 | Accuracy: 0.115234 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 026 | Total loss: 3.047 | Reg loss: 0.032 | Tree loss: 3.047 | Accuracy: 0.125000 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 026 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.136719 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 026 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.101562 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 026 | Total loss: 3.089 | Reg loss: 0.032 | Tree loss: 3.089 | Accuracy: 0.125000 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 026 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.138672 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 026 | Total loss: 3.019 | Reg loss: 0.032 | Tree loss: 3.019 | Accuracy: 0.130859 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.125000 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 026 | Total loss: 3.038 | Reg loss: 0.032 | Tree loss: 3.038 | Accuracy: 0.121094 | 5.131 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.095703 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 026 | Total loss: 3.065 | Reg loss: 0.032 | Tree loss: 3.065 | Accuracy: 0.083984 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 026 | Total loss: 3.055 | Reg loss: 0.032 | Tree loss: 3.055 | Accuracy: 0.121094 | 5.13 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 018 / 026 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.146484 | 5.129 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 026 | Total loss: 2.966 | Reg loss: 0.032 | Tree loss: 2.966 | Accuracy: 0.132812 | 5.129 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 026 | Total loss: 3.007 | Reg loss: 0.032 | Tree loss: 3.007 | Accuracy: 0.119141 | 5.129 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 026 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.121094 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.121094 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 026 | Total loss: 3.039 | Reg loss: 0.032 | Tree loss: 3.039 | Accuracy: 0.105469 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 026 | Total loss: 3.000 | Reg loss: 0.032 | Tree loss: 3.000 | Accuracy: 0.119141 | 5.13 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.083871 | 5.129 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 026 | Total loss: 3.109 | Reg loss: 0.032 | Tree loss: 3.109 | Accuracy: 0.095703 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.156250 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 026 | Total loss: 3.079 | Reg loss: 0.032 | Tree loss: 3.079 | Accuracy: 0.126953 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 026 | Total loss: 3.071 | Reg loss: 0.032 | Tree loss: 3.071 | Accuracy: 0.109375 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 026 | Total loss: 3.072 | Reg loss: 0.032 | Tree loss: 3.072 | Accuracy: 0.128906 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.125000 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 026 | Total loss: 3.074 | Reg loss: 0.032 | Tree loss: 3.074 | Accuracy: 0.123047 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 026 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.111328 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 026 | Total loss: 3.073 | Reg loss: 0.032 | Tree loss: 3.073 | Accuracy: 0.103516 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 026 | Total loss: 3.051 | Reg loss: 0.032 | Tree loss: 3.051 | Accuracy: 0.128906 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.130859 | 5.129 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.109375 | 5.129 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 026 | Total loss: 3.076 | Reg loss: 0.032 | Tree loss: 3.076 | Accuracy: 0.126953 | 5.129 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 026 | Total loss: 3.052 | Reg loss: 0.032 | Tree loss: 3.052 | Accuracy: 0.140625 | 5.129 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.130859 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 026 | Total loss: 3.013 | Reg loss: 0.032 | Tree loss: 3.013 | Accuracy: 0.107422 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 026 | Total loss: 3.084 | Reg loss: 0.032 | Tree loss: 3.084 | Accuracy: 0.113281 | 5.13 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 026 | Total loss: 3.013 | Reg loss: 0.032 | Tree loss: 3.013 | Accuracy: 0.146484 | 5.131 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.138672 | 5.131 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.132812 | 5.131 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 026 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.099609 | 5.131 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 026 | Total loss: 2.969 | Reg loss: 0.032 | Tree loss: 2.969 | Accuracy: 0.134766 | 5.132 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 026 | Total loss: 3.044 | Reg loss: 0.032 | Tree loss: 3.044 | Accuracy: 0.103516 | 5.132 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 026 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.130859 | 5.132 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 026 | Total loss: 2.976 | Reg loss: 0.032 | Tree loss: 2.976 | Accuracy: 0.130859 | 5.132 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.141935 | 5.131 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 026 | Total loss: 3.088 | Reg loss: 0.032 | Tree loss: 3.088 | Accuracy: 0.115234 | 5.132 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.115234 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 026 | Total loss: 3.069 | Reg loss: 0.032 | Tree loss: 3.069 | Accuracy: 0.097656 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 026 | Total loss: 3.097 | Reg loss: 0.032 | Tree loss: 3.097 | Accuracy: 0.111328 | 5.132 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 026 | Total loss: 3.084 | Reg loss: 0.032 | Tree loss: 3.084 | Accuracy: 0.123047 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 026 | Total loss: 3.013 | Reg loss: 0.032 | Tree loss: 3.013 | Accuracy: 0.144531 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 026 | Total loss: 3.068 | Reg loss: 0.032 | Tree loss: 3.068 | Accuracy: 0.117188 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 026 | Total loss: 3.055 | Reg loss: 0.032 | Tree loss: 3.055 | Accuracy: 0.119141 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 026 | Total loss: 3.028 | Reg loss: 0.032 | Tree loss: 3.028 | Accuracy: 0.123047 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 026 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.107422 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 026 | Total loss: 3.031 | Reg loss: 0.032 | Tree loss: 3.031 | Accuracy: 0.115234 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 026 | Total loss: 3.028 | Reg loss: 0.032 | Tree loss: 3.028 | Accuracy: 0.119141 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 026 | Total loss: 3.041 | Reg loss: 0.032 | Tree loss: 3.041 | Accuracy: 0.126953 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 026 | Total loss: 3.020 | Reg loss: 0.032 | Tree loss: 3.020 | Accuracy: 0.132812 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 026 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.126953 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 026 | Total loss: 3.007 | Reg loss: 0.032 | Tree loss: 3.007 | Accuracy: 0.126953 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 026 | Total loss: 2.998 | Reg loss: 0.032 | Tree loss: 2.998 | Accuracy: 0.111328 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 026 | Total loss: 2.992 | Reg loss: 0.032 | Tree loss: 2.992 | Accuracy: 0.130859 | 5.134 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 026 | Total loss: 3.012 | Reg loss: 0.032 | Tree loss: 3.012 | Accuracy: 0.140625 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.123047 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 026 | Total loss: 3.002 | Reg loss: 0.032 | Tree loss: 3.002 | Accuracy: 0.128906 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 026 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.121094 | 5.132 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 026 | Total loss: 2.966 | Reg loss: 0.032 | Tree loss: 2.966 | Accuracy: 0.140625 | 5.132 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 026 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.150391 | 5.132 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.123047 | 5.133 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 026 | Total loss: 3.011 | Reg loss: 0.032 | Tree loss: 3.011 | Accuracy: 0.122581 | 5.132 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Batch: 000 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.099609 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 026 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.117188 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.113281 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 026 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.128906 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 026 | Total loss: 3.015 | Reg loss: 0.032 | Tree loss: 3.015 | Accuracy: 0.138672 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 026 | Total loss: 3.049 | Reg loss: 0.032 | Tree loss: 3.049 | Accuracy: 0.101562 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.113281 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.152344 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 026 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.123047 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.117188 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 026 | Total loss: 2.989 | Reg loss: 0.032 | Tree loss: 2.989 | Accuracy: 0.154297 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 026 | Total loss: 3.018 | Reg loss: 0.032 | Tree loss: 3.018 | Accuracy: 0.109375 | 5.132 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 026 | Total loss: 3.017 | Reg loss: 0.032 | Tree loss: 3.017 | Accuracy: 0.115234 | 5.132 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 026 | Total loss: 3.020 | Reg loss: 0.032 | Tree loss: 3.020 | Accuracy: 0.144531 | 5.132 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 026 | Total loss: 3.029 | Reg loss: 0.032 | Tree loss: 3.029 | Accuracy: 0.130859 | 5.131 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 026 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.130859 | 5.132 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 026 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.117188 | 5.132 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.132812 | 5.132 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 026 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.130859 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 026 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.099609 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.140625 | 5.133 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 026 | Total loss: 3.015 | Reg loss: 0.032 | Tree loss: 3.015 | Accuracy: 0.128906 | 5.134 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 026 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.132812 | 5.134 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 026 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.109375 | 5.134 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 026 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.117188 | 5.134 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.096774 | 5.133 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 026 | Total loss: 3.037 | Reg loss: 0.032 | Tree loss: 3.037 | Accuracy: 0.140625 | 5.135 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 026 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.101562 | 5.135 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 026 | Total loss: 3.052 | Reg loss: 0.032 | Tree loss: 3.052 | Accuracy: 0.138672 | 5.135 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.111328 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 026 | Total loss: 3.076 | Reg loss: 0.032 | Tree loss: 3.076 | Accuracy: 0.095703 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.156250 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 026 | Total loss: 3.037 | Reg loss: 0.032 | Tree loss: 3.037 | Accuracy: 0.125000 | 5.135 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 026 | Total loss: 3.055 | Reg loss: 0.032 | Tree loss: 3.055 | Accuracy: 0.113281 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 026 | Total loss: 3.018 | Reg loss: 0.032 | Tree loss: 3.018 | Accuracy: 0.130859 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 026 | Total loss: 3.030 | Reg loss: 0.032 | Tree loss: 3.030 | Accuracy: 0.105469 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.128906 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.099609 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 026 | Total loss: 3.046 | Reg loss: 0.032 | Tree loss: 3.046 | Accuracy: 0.115234 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.142578 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 026 | Total loss: 3.077 | Reg loss: 0.032 | Tree loss: 3.077 | Accuracy: 0.105469 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.134766 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.169922 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.130859 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 026 | Total loss: 3.001 | Reg loss: 0.032 | Tree loss: 3.001 | Accuracy: 0.089844 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.136719 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 026 | Total loss: 2.993 | Reg loss: 0.032 | Tree loss: 2.993 | Accuracy: 0.103516 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 026 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.138672 | 5.137 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.138672 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 026 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.117188 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 026 | Total loss: 2.981 | Reg loss: 0.032 | Tree loss: 2.981 | Accuracy: 0.119141 | 5.136 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 026 | Total loss: 3.021 | Reg loss: 0.032 | Tree loss: 3.021 | Accuracy: 0.129032 | 5.135 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 026 | Total loss: 3.025 | Reg loss: 0.032 | Tree loss: 3.025 | Accuracy: 0.134766 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 026 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.119141 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 026 | Total loss: 3.024 | Reg loss: 0.032 | Tree loss: 3.024 | Accuracy: 0.125000 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.119141 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 026 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.142578 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 026 | Total loss: 3.061 | Reg loss: 0.032 | Tree loss: 3.061 | Accuracy: 0.128906 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 026 | Total loss: 3.024 | Reg loss: 0.032 | Tree loss: 3.024 | Accuracy: 0.111328 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 026 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.138672 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 026 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.115234 | 5.138 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 009 / 026 | Total loss: 3.054 | Reg loss: 0.032 | Tree loss: 3.054 | Accuracy: 0.105469 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 026 | Total loss: 3.002 | Reg loss: 0.032 | Tree loss: 3.002 | Accuracy: 0.119141 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 026 | Total loss: 2.998 | Reg loss: 0.032 | Tree loss: 2.998 | Accuracy: 0.105469 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 026 | Total loss: 3.025 | Reg loss: 0.032 | Tree loss: 3.025 | Accuracy: 0.119141 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 026 | Total loss: 3.019 | Reg loss: 0.032 | Tree loss: 3.019 | Accuracy: 0.109375 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.109375 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.125000 | 5.136 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 026 | Total loss: 2.983 | Reg loss: 0.032 | Tree loss: 2.983 | Accuracy: 0.123047 | 5.136 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.125000 | 5.136 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 026 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.140625 | 5.136 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.140625 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 026 | Total loss: 2.974 | Reg loss: 0.032 | Tree loss: 2.974 | Accuracy: 0.111328 | 5.137 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 026 | Total loss: 2.948 | Reg loss: 0.032 | Tree loss: 2.948 | Accuracy: 0.132812 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.134766 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 026 | Total loss: 2.986 | Reg loss: 0.032 | Tree loss: 2.986 | Accuracy: 0.140625 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.117188 | 5.138 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 026 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.116129 | 5.137 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.134766 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 026 | Total loss: 3.041 | Reg loss: 0.032 | Tree loss: 3.041 | Accuracy: 0.130859 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 026 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.117188 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 026 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.130859 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 026 | Total loss: 3.012 | Reg loss: 0.032 | Tree loss: 3.012 | Accuracy: 0.130859 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 026 | Total loss: 3.039 | Reg loss: 0.032 | Tree loss: 3.039 | Accuracy: 0.130859 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 026 | Total loss: 3.026 | Reg loss: 0.032 | Tree loss: 3.026 | Accuracy: 0.123047 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 026 | Total loss: 2.999 | Reg loss: 0.032 | Tree loss: 2.999 | Accuracy: 0.103516 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.097656 | 5.137 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 026 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.123047 | 5.137 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.123047 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.134766 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.138672 | 5.138 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 026 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.115234 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.123047 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 026 | Total loss: 3.000 | Reg loss: 0.032 | Tree loss: 3.000 | Accuracy: 0.125000 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.130859 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 026 | Total loss: 3.000 | Reg loss: 0.032 | Tree loss: 3.000 | Accuracy: 0.107422 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 026 | Total loss: 2.982 | Reg loss: 0.032 | Tree loss: 2.982 | Accuracy: 0.125000 | 5.14 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 026 | Total loss: 2.971 | Reg loss: 0.032 | Tree loss: 2.971 | Accuracy: 0.105469 | 5.14 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.125000 | 5.14 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 026 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.117188 | 5.14 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.150391 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 026 | Total loss: 2.955 | Reg loss: 0.032 | Tree loss: 2.955 | Accuracy: 0.142578 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 026 | Total loss: 3.032 | Reg loss: 0.032 | Tree loss: 3.032 | Accuracy: 0.105469 | 5.139 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 026 | Total loss: 2.963 | Reg loss: 0.032 | Tree loss: 2.963 | Accuracy: 0.122581 | 5.138 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.150391 | 5.139 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.138672 | 5.139 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 026 | Total loss: 3.033 | Reg loss: 0.032 | Tree loss: 3.033 | Accuracy: 0.103516 | 5.138 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.144531 | 5.137 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 026 | Total loss: 3.045 | Reg loss: 0.032 | Tree loss: 3.045 | Accuracy: 0.117188 | 5.136 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 026 | Total loss: 3.041 | Reg loss: 0.032 | Tree loss: 3.041 | Accuracy: 0.121094 | 5.136 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 026 | Total loss: 3.033 | Reg loss: 0.032 | Tree loss: 3.033 | Accuracy: 0.111328 | 5.135 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 026 | Total loss: 3.039 | Reg loss: 0.032 | Tree loss: 3.039 | Accuracy: 0.117188 | 5.134 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.097656 | 5.134 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.132812 | 5.133 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 026 | Total loss: 3.033 | Reg loss: 0.032 | Tree loss: 3.033 | Accuracy: 0.107422 | 5.132 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 026 | Total loss: 2.971 | Reg loss: 0.032 | Tree loss: 2.971 | Accuracy: 0.121094 | 5.132 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 026 | Total loss: 3.011 | Reg loss: 0.032 | Tree loss: 3.011 | Accuracy: 0.107422 | 5.131 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 026 | Total loss: 2.984 | Reg loss: 0.032 | Tree loss: 2.984 | Accuracy: 0.142578 | 5.13 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.109375 | 5.13 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 026 | Total loss: 2.992 | Reg loss: 0.032 | Tree loss: 2.992 | Accuracy: 0.113281 | 5.129 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 026 | Total loss: 2.974 | Reg loss: 0.032 | Tree loss: 2.974 | Accuracy: 0.138672 | 5.128 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.103516 | 5.128 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 018 / 026 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.107422 | 5.127 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.144531 | 5.126 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 026 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.107422 | 5.126 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 026 | Total loss: 2.976 | Reg loss: 0.032 | Tree loss: 2.976 | Accuracy: 0.142578 | 5.125 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.138672 | 5.124 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 026 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.136719 | 5.123 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 026 | Total loss: 2.948 | Reg loss: 0.032 | Tree loss: 2.948 | Accuracy: 0.125000 | 5.123 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 026 | Total loss: 2.895 | Reg loss: 0.032 | Tree loss: 2.895 | Accuracy: 0.161290 | 5.122 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.107422 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.128906 | 5.121 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.142578 | 5.12 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 026 | Total loss: 3.015 | Reg loss: 0.032 | Tree loss: 3.015 | Accuracy: 0.121094 | 5.119 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 026 | Total loss: 3.043 | Reg loss: 0.032 | Tree loss: 3.043 | Accuracy: 0.111328 | 5.118 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 026 | Total loss: 3.015 | Reg loss: 0.032 | Tree loss: 3.015 | Accuracy: 0.115234 | 5.118 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.132812 | 5.117 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 026 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.125000 | 5.116 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 026 | Total loss: 3.019 | Reg loss: 0.032 | Tree loss: 3.019 | Accuracy: 0.119141 | 5.116 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 026 | Total loss: 2.998 | Reg loss: 0.032 | Tree loss: 2.998 | Accuracy: 0.144531 | 5.115 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.134766 | 5.114 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.130859 | 5.114 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 026 | Total loss: 3.017 | Reg loss: 0.032 | Tree loss: 3.017 | Accuracy: 0.105469 | 5.113 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.144531 | 5.112 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.130859 | 5.112 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.126953 | 5.111 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 026 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.125000 | 5.11 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 026 | Total loss: 2.955 | Reg loss: 0.032 | Tree loss: 2.955 | Accuracy: 0.119141 | 5.11 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 026 | Total loss: 2.933 | Reg loss: 0.032 | Tree loss: 2.933 | Accuracy: 0.130859 | 5.109 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 026 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.140625 | 5.108 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.121094 | 5.108 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.095703 | 5.107 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 026 | Total loss: 2.941 | Reg loss: 0.032 | Tree loss: 2.941 | Accuracy: 0.109375 | 5.106 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 026 | Total loss: 2.989 | Reg loss: 0.032 | Tree loss: 2.989 | Accuracy: 0.119141 | 5.106 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 026 | Total loss: 3.002 | Reg loss: 0.032 | Tree loss: 3.002 | Accuracy: 0.109375 | 5.105 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.122581 | 5.104 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 026 | Total loss: 3.047 | Reg loss: 0.032 | Tree loss: 3.047 | Accuracy: 0.103516 | 5.104 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 026 | Total loss: 3.044 | Reg loss: 0.032 | Tree loss: 3.044 | Accuracy: 0.097656 | 5.103 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 026 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.115234 | 5.102 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.134766 | 5.102 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 026 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.132812 | 5.101 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.128906 | 5.1 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.111328 | 5.099 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.125000 | 5.099 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.113281 | 5.098 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.119141 | 5.098 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 026 | Total loss: 3.030 | Reg loss: 0.032 | Tree loss: 3.030 | Accuracy: 0.125000 | 5.097 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.152344 | 5.096 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 026 | Total loss: 3.018 | Reg loss: 0.032 | Tree loss: 3.018 | Accuracy: 0.117188 | 5.096 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 026 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.158203 | 5.095 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 026 | Total loss: 2.945 | Reg loss: 0.032 | Tree loss: 2.945 | Accuracy: 0.152344 | 5.094 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.103516 | 5.094 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 026 | Total loss: 3.019 | Reg loss: 0.032 | Tree loss: 3.019 | Accuracy: 0.103516 | 5.093 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 026 | Total loss: 2.948 | Reg loss: 0.032 | Tree loss: 2.948 | Accuracy: 0.113281 | 5.092 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.138672 | 5.092 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 026 | Total loss: 2.963 | Reg loss: 0.032 | Tree loss: 2.963 | Accuracy: 0.119141 | 5.091 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 026 | Total loss: 2.983 | Reg loss: 0.032 | Tree loss: 2.983 | Accuracy: 0.123047 | 5.09 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 026 | Total loss: 2.986 | Reg loss: 0.032 | Tree loss: 2.986 | Accuracy: 0.119141 | 5.09 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.121094 | 5.089 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.128906 | 5.089 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.121094 | 5.088 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 026 | Total loss: 2.905 | Reg loss: 0.032 | Tree loss: 2.905 | Accuracy: 0.167742 | 5.087 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 000 / 026 | Total loss: 3.059 | Reg loss: 0.032 | Tree loss: 3.059 | Accuracy: 0.109375 | 5.087 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 026 | Total loss: 3.001 | Reg loss: 0.032 | Tree loss: 3.001 | Accuracy: 0.123047 | 5.086 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 026 | Total loss: 3.037 | Reg loss: 0.032 | Tree loss: 3.037 | Accuracy: 0.111328 | 5.085 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 026 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.123047 | 5.084 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 026 | Total loss: 3.001 | Reg loss: 0.032 | Tree loss: 3.001 | Accuracy: 0.083984 | 5.084 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 026 | Total loss: 3.013 | Reg loss: 0.032 | Tree loss: 3.013 | Accuracy: 0.132812 | 5.083 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 026 | Total loss: 3.054 | Reg loss: 0.032 | Tree loss: 3.054 | Accuracy: 0.109375 | 5.082 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.132812 | 5.082 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 026 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.111328 | 5.081 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 026 | Total loss: 3.019 | Reg loss: 0.032 | Tree loss: 3.019 | Accuracy: 0.097656 | 5.081 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.132812 | 5.08 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.121094 | 5.079 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 026 | Total loss: 2.993 | Reg loss: 0.032 | Tree loss: 2.993 | Accuracy: 0.113281 | 5.079 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 026 | Total loss: 2.917 | Reg loss: 0.032 | Tree loss: 2.917 | Accuracy: 0.144531 | 5.078 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 026 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.152344 | 5.077 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 026 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.128906 | 5.077 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.113281 | 5.076 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 026 | Total loss: 2.960 | Reg loss: 0.032 | Tree loss: 2.960 | Accuracy: 0.119141 | 5.075 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 026 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.125000 | 5.075 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 026 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.128906 | 5.074 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.126953 | 5.074 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 026 | Total loss: 2.930 | Reg loss: 0.032 | Tree loss: 2.930 | Accuracy: 0.136719 | 5.073 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.150391 | 5.072 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 026 | Total loss: 2.950 | Reg loss: 0.032 | Tree loss: 2.950 | Accuracy: 0.138672 | 5.072 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 026 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.117188 | 5.071 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.148387 | 5.07 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 026 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.113281 | 5.07 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 026 | Total loss: 3.070 | Reg loss: 0.032 | Tree loss: 3.070 | Accuracy: 0.103516 | 5.069 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 026 | Total loss: 3.012 | Reg loss: 0.032 | Tree loss: 3.012 | Accuracy: 0.148438 | 5.068 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 026 | Total loss: 3.031 | Reg loss: 0.032 | Tree loss: 3.031 | Accuracy: 0.130859 | 5.068 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 026 | Total loss: 2.989 | Reg loss: 0.032 | Tree loss: 2.989 | Accuracy: 0.126953 | 5.067 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 026 | Total loss: 3.056 | Reg loss: 0.032 | Tree loss: 3.056 | Accuracy: 0.111328 | 5.066 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 026 | Total loss: 2.988 | Reg loss: 0.032 | Tree loss: 2.988 | Accuracy: 0.119141 | 5.066 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 026 | Total loss: 2.974 | Reg loss: 0.032 | Tree loss: 2.974 | Accuracy: 0.146484 | 5.065 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 026 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.107422 | 5.065 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.144531 | 5.064 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 026 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.132812 | 5.063 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 026 | Total loss: 3.002 | Reg loss: 0.032 | Tree loss: 3.002 | Accuracy: 0.121094 | 5.063 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 026 | Total loss: 2.998 | Reg loss: 0.032 | Tree loss: 2.998 | Accuracy: 0.101562 | 5.062 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 026 | Total loss: 2.989 | Reg loss: 0.032 | Tree loss: 2.989 | Accuracy: 0.119141 | 5.062 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 026 | Total loss: 2.985 | Reg loss: 0.032 | Tree loss: 2.985 | Accuracy: 0.126953 | 5.061 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.148438 | 5.06 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 026 | Total loss: 2.983 | Reg loss: 0.032 | Tree loss: 2.983 | Accuracy: 0.121094 | 5.06 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 026 | Total loss: 2.960 | Reg loss: 0.032 | Tree loss: 2.960 | Accuracy: 0.115234 | 5.059 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 026 | Total loss: 2.926 | Reg loss: 0.032 | Tree loss: 2.926 | Accuracy: 0.126953 | 5.058 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.132812 | 5.058 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 026 | Total loss: 2.977 | Reg loss: 0.032 | Tree loss: 2.977 | Accuracy: 0.126953 | 5.057 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 026 | Total loss: 2.932 | Reg loss: 0.032 | Tree loss: 2.932 | Accuracy: 0.119141 | 5.057 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.117188 | 5.056 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.121094 | 5.055 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.130859 | 5.055 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.051613 | 5.054 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 026 | Total loss: 3.029 | Reg loss: 0.032 | Tree loss: 3.029 | Accuracy: 0.123047 | 5.054 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.128906 | 5.053 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 026 | Total loss: 2.982 | Reg loss: 0.032 | Tree loss: 2.982 | Accuracy: 0.138672 | 5.052 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 026 | Total loss: 3.015 | Reg loss: 0.032 | Tree loss: 3.015 | Accuracy: 0.107422 | 5.052 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 026 | Total loss: 3.020 | Reg loss: 0.032 | Tree loss: 3.020 | Accuracy: 0.128906 | 5.051 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.119141 | 5.05 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 026 | Total loss: 3.018 | Reg loss: 0.032 | Tree loss: 3.018 | Accuracy: 0.117188 | 5.05 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.117188 | 5.049 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.132812 | 5.049 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 | Batch: 009 / 026 | Total loss: 3.013 | Reg loss: 0.032 | Tree loss: 3.013 | Accuracy: 0.132812 | 5.048 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.152344 | 5.047 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.101562 | 5.047 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 026 | Total loss: 2.918 | Reg loss: 0.032 | Tree loss: 2.918 | Accuracy: 0.132812 | 5.046 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 026 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.123047 | 5.046 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 026 | Total loss: 2.945 | Reg loss: 0.032 | Tree loss: 2.945 | Accuracy: 0.121094 | 5.045 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 026 | Total loss: 2.985 | Reg loss: 0.032 | Tree loss: 2.985 | Accuracy: 0.123047 | 5.044 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 026 | Total loss: 2.949 | Reg loss: 0.032 | Tree loss: 2.949 | Accuracy: 0.136719 | 5.044 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.113281 | 5.043 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 026 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.134766 | 5.043 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 026 | Total loss: 2.977 | Reg loss: 0.032 | Tree loss: 2.977 | Accuracy: 0.107422 | 5.042 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.138672 | 5.041 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.093750 | 5.041 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 026 | Total loss: 2.930 | Reg loss: 0.032 | Tree loss: 2.930 | Accuracy: 0.125000 | 5.04 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.130859 | 5.04 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.119141 | 5.039 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.096774 | 5.038 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.107422 | 5.038 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 026 | Total loss: 2.999 | Reg loss: 0.032 | Tree loss: 2.999 | Accuracy: 0.138672 | 5.037 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 026 | Total loss: 2.983 | Reg loss: 0.032 | Tree loss: 2.983 | Accuracy: 0.134766 | 5.037 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 026 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.130859 | 5.036 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.128906 | 5.035 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 026 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.128906 | 5.035 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.109375 | 5.034 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.123047 | 5.033 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 026 | Total loss: 2.957 | Reg loss: 0.032 | Tree loss: 2.957 | Accuracy: 0.121094 | 5.033 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 026 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.119141 | 5.032 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.105469 | 5.032 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 026 | Total loss: 3.012 | Reg loss: 0.032 | Tree loss: 3.012 | Accuracy: 0.126953 | 5.031 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.130859 | 5.031 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 026 | Total loss: 2.928 | Reg loss: 0.032 | Tree loss: 2.928 | Accuracy: 0.125000 | 5.03 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.128906 | 5.029 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 026 | Total loss: 2.932 | Reg loss: 0.032 | Tree loss: 2.932 | Accuracy: 0.132812 | 5.029 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 026 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.113281 | 5.028 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.113281 | 5.028 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 026 | Total loss: 2.992 | Reg loss: 0.032 | Tree loss: 2.992 | Accuracy: 0.111328 | 5.027 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 026 | Total loss: 2.951 | Reg loss: 0.032 | Tree loss: 2.951 | Accuracy: 0.138672 | 5.027 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.125000 | 5.026 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 026 | Total loss: 2.912 | Reg loss: 0.032 | Tree loss: 2.912 | Accuracy: 0.125000 | 5.025 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.111328 | 5.025 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 026 | Total loss: 2.933 | Reg loss: 0.032 | Tree loss: 2.933 | Accuracy: 0.128906 | 5.024 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 026 | Total loss: 2.907 | Reg loss: 0.032 | Tree loss: 2.907 | Accuracy: 0.130859 | 5.024 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 026 | Total loss: 2.917 | Reg loss: 0.032 | Tree loss: 2.917 | Accuracy: 0.129032 | 5.023 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.160156 | 5.023 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 026 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.121094 | 5.022 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.126953 | 5.021 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 026 | Total loss: 3.017 | Reg loss: 0.032 | Tree loss: 3.017 | Accuracy: 0.095703 | 5.021 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 026 | Total loss: 2.985 | Reg loss: 0.032 | Tree loss: 2.985 | Accuracy: 0.101562 | 5.02 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 026 | Total loss: 3.001 | Reg loss: 0.032 | Tree loss: 3.001 | Accuracy: 0.119141 | 5.019 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 026 | Total loss: 2.981 | Reg loss: 0.032 | Tree loss: 2.981 | Accuracy: 0.101562 | 5.019 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 026 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.126953 | 5.018 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 026 | Total loss: 2.963 | Reg loss: 0.032 | Tree loss: 2.963 | Accuracy: 0.121094 | 5.018 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.142578 | 5.017 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.134766 | 5.017 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 026 | Total loss: 2.974 | Reg loss: 0.032 | Tree loss: 2.974 | Accuracy: 0.138672 | 5.016 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 026 | Total loss: 2.977 | Reg loss: 0.032 | Tree loss: 2.977 | Accuracy: 0.148438 | 5.016 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 026 | Total loss: 2.957 | Reg loss: 0.032 | Tree loss: 2.957 | Accuracy: 0.146484 | 5.015 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 026 | Total loss: 3.007 | Reg loss: 0.032 | Tree loss: 3.007 | Accuracy: 0.085938 | 5.014 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.111328 | 5.014 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 026 | Total loss: 3.015 | Reg loss: 0.032 | Tree loss: 3.015 | Accuracy: 0.111328 | 5.013 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 026 | Total loss: 2.925 | Reg loss: 0.032 | Tree loss: 2.925 | Accuracy: 0.123047 | 5.013 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 018 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.144531 | 5.012 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 026 | Total loss: 2.941 | Reg loss: 0.032 | Tree loss: 2.941 | Accuracy: 0.148438 | 5.012 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.126953 | 5.011 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 026 | Total loss: 2.952 | Reg loss: 0.032 | Tree loss: 2.952 | Accuracy: 0.115234 | 5.011 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.103516 | 5.01 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.105469 | 5.01 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.130859 | 5.009 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 026 | Total loss: 2.905 | Reg loss: 0.032 | Tree loss: 2.905 | Accuracy: 0.122581 | 5.008 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 026 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.111328 | 5.008 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.105469 | 5.007 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 026 | Total loss: 2.992 | Reg loss: 0.032 | Tree loss: 2.992 | Accuracy: 0.111328 | 5.007 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.113281 | 5.006 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 026 | Total loss: 3.007 | Reg loss: 0.032 | Tree loss: 3.007 | Accuracy: 0.119141 | 5.005 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.125000 | 5.005 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.115234 | 5.004 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 026 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.154297 | 5.004 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.144531 | 5.003 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.117188 | 5.003 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 026 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.128906 | 5.002 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 026 | Total loss: 2.960 | Reg loss: 0.032 | Tree loss: 2.960 | Accuracy: 0.150391 | 5.002 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 026 | Total loss: 2.950 | Reg loss: 0.032 | Tree loss: 2.950 | Accuracy: 0.115234 | 5.001 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 026 | Total loss: 2.977 | Reg loss: 0.032 | Tree loss: 2.977 | Accuracy: 0.111328 | 5.001 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 026 | Total loss: 2.971 | Reg loss: 0.032 | Tree loss: 2.971 | Accuracy: 0.138672 | 5.0 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.105469 | 4.999 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 026 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.113281 | 4.999 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 026 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.109375 | 4.998 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 026 | Total loss: 2.921 | Reg loss: 0.032 | Tree loss: 2.921 | Accuracy: 0.111328 | 4.998 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.125000 | 4.997 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.136719 | 4.997 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 026 | Total loss: 2.925 | Reg loss: 0.032 | Tree loss: 2.925 | Accuracy: 0.136719 | 4.996 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.128906 | 4.996 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.130859 | 4.995 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 026 | Total loss: 2.906 | Reg loss: 0.032 | Tree loss: 2.906 | Accuracy: 0.130859 | 4.995 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 026 | Total loss: 2.877 | Reg loss: 0.032 | Tree loss: 2.877 | Accuracy: 0.129032 | 4.994 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 026 | Total loss: 2.999 | Reg loss: 0.032 | Tree loss: 2.999 | Accuracy: 0.140625 | 4.994 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 026 | Total loss: 2.992 | Reg loss: 0.032 | Tree loss: 2.992 | Accuracy: 0.130859 | 4.993 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.123047 | 4.992 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 026 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.121094 | 4.992 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.103516 | 4.991 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 026 | Total loss: 2.988 | Reg loss: 0.032 | Tree loss: 2.988 | Accuracy: 0.115234 | 4.991 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.111328 | 4.99 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 026 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.121094 | 4.99 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.126953 | 4.989 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.117188 | 4.989 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 026 | Total loss: 2.986 | Reg loss: 0.032 | Tree loss: 2.986 | Accuracy: 0.119141 | 4.988 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.123047 | 4.988 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 026 | Total loss: 2.951 | Reg loss: 0.032 | Tree loss: 2.951 | Accuracy: 0.132812 | 4.987 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 026 | Total loss: 2.984 | Reg loss: 0.032 | Tree loss: 2.984 | Accuracy: 0.117188 | 4.986 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 026 | Total loss: 2.931 | Reg loss: 0.032 | Tree loss: 2.931 | Accuracy: 0.140625 | 4.986 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.138672 | 4.985 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.121094 | 4.985 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 026 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.128906 | 4.984 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.128906 | 4.984 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 026 | Total loss: 2.906 | Reg loss: 0.032 | Tree loss: 2.906 | Accuracy: 0.121094 | 4.983 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 026 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.140625 | 4.983 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 026 | Total loss: 2.963 | Reg loss: 0.032 | Tree loss: 2.963 | Accuracy: 0.117188 | 4.982 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 026 | Total loss: 2.923 | Reg loss: 0.032 | Tree loss: 2.923 | Accuracy: 0.121094 | 4.982 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 026 | Total loss: 2.914 | Reg loss: 0.032 | Tree loss: 2.914 | Accuracy: 0.123047 | 4.981 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.107422 | 4.981 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 026 | Total loss: 2.981 | Reg loss: 0.032 | Tree loss: 2.981 | Accuracy: 0.122581 | 4.98 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 000 / 026 | Total loss: 2.988 | Reg loss: 0.032 | Tree loss: 2.988 | Accuracy: 0.115234 | 4.98 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 026 | Total loss: 2.993 | Reg loss: 0.032 | Tree loss: 2.993 | Accuracy: 0.126953 | 4.979 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 026 | Total loss: 3.021 | Reg loss: 0.032 | Tree loss: 3.021 | Accuracy: 0.105469 | 4.978 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 026 | Total loss: 3.059 | Reg loss: 0.032 | Tree loss: 3.059 | Accuracy: 0.111328 | 4.978 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.117188 | 4.977 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 026 | Total loss: 3.002 | Reg loss: 0.032 | Tree loss: 3.002 | Accuracy: 0.117188 | 4.977 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 026 | Total loss: 3.007 | Reg loss: 0.032 | Tree loss: 3.007 | Accuracy: 0.107422 | 4.976 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 026 | Total loss: 2.986 | Reg loss: 0.032 | Tree loss: 2.986 | Accuracy: 0.117188 | 4.976 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.126953 | 4.975 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.134766 | 4.975 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 026 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.144531 | 4.974 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 026 | Total loss: 2.981 | Reg loss: 0.032 | Tree loss: 2.981 | Accuracy: 0.123047 | 4.974 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 026 | Total loss: 2.976 | Reg loss: 0.032 | Tree loss: 2.976 | Accuracy: 0.101562 | 4.973 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.123047 | 4.973 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 026 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.121094 | 4.972 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 026 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.134766 | 4.972 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.132812 | 4.971 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 026 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.130859 | 4.971 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.130859 | 4.97 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 026 | Total loss: 2.926 | Reg loss: 0.032 | Tree loss: 2.926 | Accuracy: 0.136719 | 4.97 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.132812 | 4.969 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 026 | Total loss: 2.925 | Reg loss: 0.032 | Tree loss: 2.925 | Accuracy: 0.125000 | 4.969 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.119141 | 4.968 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.095703 | 4.968 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 026 | Total loss: 2.893 | Reg loss: 0.032 | Tree loss: 2.893 | Accuracy: 0.154297 | 4.967 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 026 | Total loss: 2.880 | Reg loss: 0.032 | Tree loss: 2.880 | Accuracy: 0.141935 | 4.966 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.136719 | 4.966 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 026 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.128906 | 4.966 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 026 | Total loss: 3.028 | Reg loss: 0.032 | Tree loss: 3.028 | Accuracy: 0.119141 | 4.965 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 026 | Total loss: 2.993 | Reg loss: 0.032 | Tree loss: 2.993 | Accuracy: 0.123047 | 4.964 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 026 | Total loss: 2.976 | Reg loss: 0.032 | Tree loss: 2.976 | Accuracy: 0.119141 | 4.964 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.128906 | 4.963 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 026 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.113281 | 4.963 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 026 | Total loss: 3.012 | Reg loss: 0.032 | Tree loss: 3.012 | Accuracy: 0.123047 | 4.962 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.123047 | 4.962 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.117188 | 4.961 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.128906 | 4.961 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 026 | Total loss: 2.952 | Reg loss: 0.032 | Tree loss: 2.952 | Accuracy: 0.115234 | 4.96 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 026 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.123047 | 4.96 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 026 | Total loss: 2.909 | Reg loss: 0.032 | Tree loss: 2.909 | Accuracy: 0.138672 | 4.959 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 026 | Total loss: 2.931 | Reg loss: 0.032 | Tree loss: 2.931 | Accuracy: 0.119141 | 4.959 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 026 | Total loss: 2.916 | Reg loss: 0.032 | Tree loss: 2.916 | Accuracy: 0.132812 | 4.958 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.115234 | 4.958 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 026 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.123047 | 4.957 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.128906 | 4.957 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 026 | Total loss: 2.917 | Reg loss: 0.032 | Tree loss: 2.917 | Accuracy: 0.132812 | 4.956 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 026 | Total loss: 2.921 | Reg loss: 0.032 | Tree loss: 2.921 | Accuracy: 0.105469 | 4.956 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.130859 | 4.955 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 026 | Total loss: 2.918 | Reg loss: 0.032 | Tree loss: 2.918 | Accuracy: 0.125000 | 4.955 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 026 | Total loss: 2.896 | Reg loss: 0.032 | Tree loss: 2.896 | Accuracy: 0.121094 | 4.954 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 026 | Total loss: 2.895 | Reg loss: 0.032 | Tree loss: 2.895 | Accuracy: 0.123047 | 4.954 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.109677 | 4.953 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 026 | Total loss: 2.998 | Reg loss: 0.032 | Tree loss: 2.998 | Accuracy: 0.132812 | 4.953 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 026 | Total loss: 3.021 | Reg loss: 0.032 | Tree loss: 3.021 | Accuracy: 0.115234 | 4.952 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.138672 | 4.952 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.142578 | 4.951 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.126953 | 4.951 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.125000 | 4.95 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.109375 | 4.95 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 026 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.123047 | 4.949 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.117188 | 4.949 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Batch: 009 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.140625 | 4.948 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 026 | Total loss: 2.925 | Reg loss: 0.032 | Tree loss: 2.925 | Accuracy: 0.144531 | 4.948 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.121094 | 4.947 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.115234 | 4.947 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 026 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.119141 | 4.946 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 026 | Total loss: 2.974 | Reg loss: 0.032 | Tree loss: 2.974 | Accuracy: 0.138672 | 4.946 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 026 | Total loss: 2.945 | Reg loss: 0.032 | Tree loss: 2.945 | Accuracy: 0.093750 | 4.945 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.142578 | 4.945 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 026 | Total loss: 2.960 | Reg loss: 0.032 | Tree loss: 2.960 | Accuracy: 0.101562 | 4.944 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.132812 | 4.944 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 026 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.107422 | 4.943 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.123047 | 4.943 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.123047 | 4.943 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 026 | Total loss: 2.900 | Reg loss: 0.032 | Tree loss: 2.900 | Accuracy: 0.117188 | 4.942 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 026 | Total loss: 2.905 | Reg loss: 0.032 | Tree loss: 2.905 | Accuracy: 0.115234 | 4.942 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.136719 | 4.941 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.083871 | 4.94 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 026 | Total loss: 2.988 | Reg loss: 0.032 | Tree loss: 2.988 | Accuracy: 0.130859 | 4.94 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 026 | Total loss: 2.985 | Reg loss: 0.032 | Tree loss: 2.985 | Accuracy: 0.128906 | 4.94 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 026 | Total loss: 2.951 | Reg loss: 0.032 | Tree loss: 2.951 | Accuracy: 0.136719 | 4.939 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 026 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.113281 | 4.938 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 026 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.121094 | 4.938 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.125000 | 4.937 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 026 | Total loss: 2.976 | Reg loss: 0.032 | Tree loss: 2.976 | Accuracy: 0.113281 | 4.937 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 026 | Total loss: 2.929 | Reg loss: 0.032 | Tree loss: 2.929 | Accuracy: 0.152344 | 4.937 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.099609 | 4.936 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 026 | Total loss: 2.982 | Reg loss: 0.032 | Tree loss: 2.982 | Accuracy: 0.134766 | 4.936 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 026 | Total loss: 2.932 | Reg loss: 0.032 | Tree loss: 2.932 | Accuracy: 0.130859 | 4.935 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.121094 | 4.935 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 026 | Total loss: 2.993 | Reg loss: 0.032 | Tree loss: 2.993 | Accuracy: 0.113281 | 4.934 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.117188 | 4.934 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 026 | Total loss: 2.928 | Reg loss: 0.032 | Tree loss: 2.928 | Accuracy: 0.121094 | 4.933 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 026 | Total loss: 2.905 | Reg loss: 0.032 | Tree loss: 2.905 | Accuracy: 0.132812 | 4.933 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 026 | Total loss: 2.940 | Reg loss: 0.032 | Tree loss: 2.940 | Accuracy: 0.121094 | 4.932 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 026 | Total loss: 2.933 | Reg loss: 0.032 | Tree loss: 2.933 | Accuracy: 0.134766 | 4.932 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.109375 | 4.931 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 026 | Total loss: 2.914 | Reg loss: 0.032 | Tree loss: 2.914 | Accuracy: 0.111328 | 4.931 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 026 | Total loss: 2.878 | Reg loss: 0.032 | Tree loss: 2.878 | Accuracy: 0.123047 | 4.93 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 026 | Total loss: 2.960 | Reg loss: 0.032 | Tree loss: 2.960 | Accuracy: 0.121094 | 4.93 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 026 | Total loss: 2.954 | Reg loss: 0.032 | Tree loss: 2.954 | Accuracy: 0.123047 | 4.929 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.117188 | 4.929 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.132812 | 4.929 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 026 | Total loss: 2.847 | Reg loss: 0.032 | Tree loss: 2.847 | Accuracy: 0.141935 | 4.928 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 026 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.115234 | 4.928 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 026 | Total loss: 3.017 | Reg loss: 0.032 | Tree loss: 3.017 | Accuracy: 0.115234 | 4.927 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 026 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.142578 | 4.927 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 026 | Total loss: 3.022 | Reg loss: 0.032 | Tree loss: 3.022 | Accuracy: 0.128906 | 4.926 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 026 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.128906 | 4.926 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 026 | Total loss: 2.941 | Reg loss: 0.032 | Tree loss: 2.941 | Accuracy: 0.113281 | 4.925 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 026 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.113281 | 4.925 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.119141 | 4.924 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.121094 | 4.924 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.142578 | 4.923 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 026 | Total loss: 2.951 | Reg loss: 0.032 | Tree loss: 2.951 | Accuracy: 0.125000 | 4.923 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 026 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.119141 | 4.922 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.146484 | 4.922 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 026 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.113281 | 4.921 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 026 | Total loss: 2.926 | Reg loss: 0.032 | Tree loss: 2.926 | Accuracy: 0.146484 | 4.921 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.125000 | 4.92 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 026 | Total loss: 2.929 | Reg loss: 0.032 | Tree loss: 2.929 | Accuracy: 0.119141 | 4.92 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.115234 | 4.92 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 018 / 026 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.111328 | 4.919 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 026 | Total loss: 2.909 | Reg loss: 0.032 | Tree loss: 2.909 | Accuracy: 0.140625 | 4.919 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 026 | Total loss: 2.951 | Reg loss: 0.032 | Tree loss: 2.951 | Accuracy: 0.119141 | 4.918 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 026 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.123047 | 4.918 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 026 | Total loss: 2.863 | Reg loss: 0.032 | Tree loss: 2.863 | Accuracy: 0.126953 | 4.917 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.107422 | 4.917 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 026 | Total loss: 2.867 | Reg loss: 0.032 | Tree loss: 2.867 | Accuracy: 0.126953 | 4.916 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.077419 | 4.916 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 026 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.144531 | 4.915 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 026 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.126953 | 4.915 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.111328 | 4.914 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.128906 | 4.914 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 026 | Total loss: 2.932 | Reg loss: 0.032 | Tree loss: 2.932 | Accuracy: 0.136719 | 4.913 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 026 | Total loss: 2.957 | Reg loss: 0.032 | Tree loss: 2.957 | Accuracy: 0.140625 | 4.913 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.132812 | 4.912 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 026 | Total loss: 2.974 | Reg loss: 0.032 | Tree loss: 2.974 | Accuracy: 0.105469 | 4.912 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.158203 | 4.911 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.134766 | 4.911 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.107422 | 4.911 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 026 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.083984 | 4.91 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 026 | Total loss: 2.926 | Reg loss: 0.032 | Tree loss: 2.926 | Accuracy: 0.128906 | 4.91 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 026 | Total loss: 2.993 | Reg loss: 0.032 | Tree loss: 2.993 | Accuracy: 0.109375 | 4.909 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 026 | Total loss: 2.942 | Reg loss: 0.032 | Tree loss: 2.942 | Accuracy: 0.136719 | 4.909 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 026 | Total loss: 2.951 | Reg loss: 0.032 | Tree loss: 2.951 | Accuracy: 0.130859 | 4.908 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 026 | Total loss: 2.931 | Reg loss: 0.032 | Tree loss: 2.931 | Accuracy: 0.121094 | 4.908 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.105469 | 4.907 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.121094 | 4.907 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.128906 | 4.906 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 026 | Total loss: 2.912 | Reg loss: 0.032 | Tree loss: 2.912 | Accuracy: 0.111328 | 4.906 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 026 | Total loss: 2.891 | Reg loss: 0.032 | Tree loss: 2.891 | Accuracy: 0.128906 | 4.906 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 026 | Total loss: 2.914 | Reg loss: 0.032 | Tree loss: 2.914 | Accuracy: 0.101562 | 4.905 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 026 | Total loss: 2.911 | Reg loss: 0.032 | Tree loss: 2.911 | Accuracy: 0.128906 | 4.905 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 026 | Total loss: 2.914 | Reg loss: 0.032 | Tree loss: 2.914 | Accuracy: 0.128906 | 4.904 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 026 | Total loss: 2.873 | Reg loss: 0.032 | Tree loss: 2.873 | Accuracy: 0.116129 | 4.904 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 026 | Total loss: 2.982 | Reg loss: 0.032 | Tree loss: 2.982 | Accuracy: 0.113281 | 4.903 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.136719 | 4.903 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 026 | Total loss: 2.971 | Reg loss: 0.032 | Tree loss: 2.971 | Accuracy: 0.121094 | 4.902 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 026 | Total loss: 2.984 | Reg loss: 0.032 | Tree loss: 2.984 | Accuracy: 0.128906 | 4.902 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 026 | Total loss: 2.989 | Reg loss: 0.032 | Tree loss: 2.989 | Accuracy: 0.117188 | 4.901 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 026 | Total loss: 2.952 | Reg loss: 0.032 | Tree loss: 2.952 | Accuracy: 0.142578 | 4.901 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 026 | Total loss: 2.982 | Reg loss: 0.032 | Tree loss: 2.982 | Accuracy: 0.111328 | 4.9 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 026 | Total loss: 2.950 | Reg loss: 0.032 | Tree loss: 2.950 | Accuracy: 0.109375 | 4.9 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.123047 | 4.9 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.121094 | 4.899 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.097656 | 4.899 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.144531 | 4.898 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 026 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.087891 | 4.898 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.132812 | 4.897 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.119141 | 4.897 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.154297 | 4.896 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.132812 | 4.896 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 026 | Total loss: 2.961 | Reg loss: 0.032 | Tree loss: 2.961 | Accuracy: 0.109375 | 4.896 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 026 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.117188 | 4.895 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 026 | Total loss: 2.949 | Reg loss: 0.032 | Tree loss: 2.949 | Accuracy: 0.107422 | 4.895 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 026 | Total loss: 2.897 | Reg loss: 0.032 | Tree loss: 2.897 | Accuracy: 0.128906 | 4.894 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 026 | Total loss: 2.906 | Reg loss: 0.032 | Tree loss: 2.906 | Accuracy: 0.144531 | 4.894 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.125000 | 4.893 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.123047 | 4.893 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 026 | Total loss: 2.839 | Reg loss: 0.032 | Tree loss: 2.839 | Accuracy: 0.138672 | 4.893 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 026 | Total loss: 2.918 | Reg loss: 0.032 | Tree loss: 2.918 | Accuracy: 0.135484 | 4.892 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 000 / 026 | Total loss: 3.032 | Reg loss: 0.032 | Tree loss: 3.032 | Accuracy: 0.119141 | 4.892 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 026 | Total loss: 2.982 | Reg loss: 0.032 | Tree loss: 2.982 | Accuracy: 0.128906 | 4.891 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 026 | Total loss: 3.000 | Reg loss: 0.032 | Tree loss: 3.000 | Accuracy: 0.117188 | 4.891 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.130859 | 4.89 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 026 | Total loss: 2.984 | Reg loss: 0.032 | Tree loss: 2.984 | Accuracy: 0.101562 | 4.89 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.130859 | 4.889 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 026 | Total loss: 2.949 | Reg loss: 0.032 | Tree loss: 2.949 | Accuracy: 0.125000 | 4.889 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 026 | Total loss: 2.969 | Reg loss: 0.032 | Tree loss: 2.969 | Accuracy: 0.126953 | 4.888 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 026 | Total loss: 2.916 | Reg loss: 0.032 | Tree loss: 2.916 | Accuracy: 0.128906 | 4.888 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 026 | Total loss: 3.017 | Reg loss: 0.032 | Tree loss: 3.017 | Accuracy: 0.099609 | 4.887 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 026 | Total loss: 2.948 | Reg loss: 0.032 | Tree loss: 2.948 | Accuracy: 0.115234 | 4.887 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.107422 | 4.887 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 026 | Total loss: 2.956 | Reg loss: 0.032 | Tree loss: 2.956 | Accuracy: 0.136719 | 4.886 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 026 | Total loss: 2.928 | Reg loss: 0.032 | Tree loss: 2.928 | Accuracy: 0.125000 | 4.886 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.115234 | 4.885 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 026 | Total loss: 2.899 | Reg loss: 0.032 | Tree loss: 2.899 | Accuracy: 0.126953 | 4.885 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.119141 | 4.884 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 026 | Total loss: 2.901 | Reg loss: 0.032 | Tree loss: 2.901 | Accuracy: 0.121094 | 4.884 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.126953 | 4.884 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 026 | Total loss: 2.922 | Reg loss: 0.032 | Tree loss: 2.922 | Accuracy: 0.132812 | 4.883 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 026 | Total loss: 2.886 | Reg loss: 0.032 | Tree loss: 2.886 | Accuracy: 0.150391 | 4.883 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 026 | Total loss: 2.886 | Reg loss: 0.032 | Tree loss: 2.886 | Accuracy: 0.136719 | 4.882 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 026 | Total loss: 2.928 | Reg loss: 0.032 | Tree loss: 2.928 | Accuracy: 0.105469 | 4.882 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 026 | Total loss: 2.880 | Reg loss: 0.032 | Tree loss: 2.880 | Accuracy: 0.121094 | 4.881 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 026 | Total loss: 2.868 | Reg loss: 0.032 | Tree loss: 2.868 | Accuracy: 0.146484 | 4.881 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.109677 | 4.88 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.134766 | 4.88 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.132812 | 4.88 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 026 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.097656 | 4.879 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 026 | Total loss: 2.974 | Reg loss: 0.032 | Tree loss: 2.974 | Accuracy: 0.125000 | 4.879 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 026 | Total loss: 3.020 | Reg loss: 0.032 | Tree loss: 3.020 | Accuracy: 0.093750 | 4.878 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 026 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.109375 | 4.878 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 026 | Total loss: 2.988 | Reg loss: 0.032 | Tree loss: 2.988 | Accuracy: 0.115234 | 4.877 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 026 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.130859 | 4.877 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.121094 | 4.877 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.109375 | 4.876 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.121094 | 4.876 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.103516 | 4.875 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 026 | Total loss: 2.932 | Reg loss: 0.032 | Tree loss: 2.932 | Accuracy: 0.119141 | 4.875 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 026 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.138672 | 4.874 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 026 | Total loss: 2.908 | Reg loss: 0.032 | Tree loss: 2.908 | Accuracy: 0.128906 | 4.874 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.138672 | 4.874 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 026 | Total loss: 2.907 | Reg loss: 0.032 | Tree loss: 2.907 | Accuracy: 0.140625 | 4.873 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 026 | Total loss: 2.921 | Reg loss: 0.032 | Tree loss: 2.921 | Accuracy: 0.140625 | 4.873 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 026 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.134766 | 4.872 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 026 | Total loss: 2.922 | Reg loss: 0.032 | Tree loss: 2.922 | Accuracy: 0.130859 | 4.872 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 026 | Total loss: 2.929 | Reg loss: 0.032 | Tree loss: 2.929 | Accuracy: 0.109375 | 4.871 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.115234 | 4.871 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 026 | Total loss: 2.883 | Reg loss: 0.032 | Tree loss: 2.883 | Accuracy: 0.136719 | 4.871 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 026 | Total loss: 2.873 | Reg loss: 0.032 | Tree loss: 2.873 | Accuracy: 0.136719 | 4.87 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 026 | Total loss: 2.908 | Reg loss: 0.032 | Tree loss: 2.908 | Accuracy: 0.117188 | 4.87 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.154839 | 4.869 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 026 | Total loss: 2.994 | Reg loss: 0.031 | Tree loss: 2.994 | Accuracy: 0.125000 | 4.869 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 026 | Total loss: 2.953 | Reg loss: 0.031 | Tree loss: 2.953 | Accuracy: 0.146484 | 4.868 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 026 | Total loss: 2.944 | Reg loss: 0.031 | Tree loss: 2.944 | Accuracy: 0.126953 | 4.868 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 026 | Total loss: 2.964 | Reg loss: 0.031 | Tree loss: 2.964 | Accuracy: 0.130859 | 4.868 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 026 | Total loss: 3.024 | Reg loss: 0.031 | Tree loss: 3.024 | Accuracy: 0.117188 | 4.867 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.113281 | 4.867 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 026 | Total loss: 2.971 | Reg loss: 0.032 | Tree loss: 2.971 | Accuracy: 0.134766 | 4.866 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.107422 | 4.866 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 026 | Total loss: 2.886 | Reg loss: 0.032 | Tree loss: 2.886 | Accuracy: 0.132812 | 4.865 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 | Batch: 009 / 026 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.113281 | 4.865 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.105469 | 4.865 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 026 | Total loss: 2.993 | Reg loss: 0.032 | Tree loss: 2.993 | Accuracy: 0.105469 | 4.864 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 026 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.119141 | 4.864 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 026 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.123047 | 4.863 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 026 | Total loss: 2.921 | Reg loss: 0.032 | Tree loss: 2.921 | Accuracy: 0.103516 | 4.863 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 026 | Total loss: 2.917 | Reg loss: 0.032 | Tree loss: 2.917 | Accuracy: 0.134766 | 4.862 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 026 | Total loss: 2.951 | Reg loss: 0.032 | Tree loss: 2.951 | Accuracy: 0.101562 | 4.862 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 026 | Total loss: 2.929 | Reg loss: 0.032 | Tree loss: 2.929 | Accuracy: 0.105469 | 4.862 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 026 | Total loss: 2.894 | Reg loss: 0.032 | Tree loss: 2.894 | Accuracy: 0.136719 | 4.861 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 026 | Total loss: 2.923 | Reg loss: 0.032 | Tree loss: 2.923 | Accuracy: 0.142578 | 4.861 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.142578 | 4.86 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 026 | Total loss: 2.899 | Reg loss: 0.032 | Tree loss: 2.899 | Accuracy: 0.119141 | 4.86 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 026 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.126953 | 4.86 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 026 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.144531 | 4.859 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 026 | Total loss: 2.893 | Reg loss: 0.032 | Tree loss: 2.893 | Accuracy: 0.132812 | 4.859 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 026 | Total loss: 2.917 | Reg loss: 0.032 | Tree loss: 2.917 | Accuracy: 0.122581 | 4.858 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 026 | Total loss: 2.988 | Reg loss: 0.031 | Tree loss: 2.988 | Accuracy: 0.109375 | 4.858 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 026 | Total loss: 3.013 | Reg loss: 0.031 | Tree loss: 3.013 | Accuracy: 0.123047 | 4.857 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 026 | Total loss: 3.024 | Reg loss: 0.031 | Tree loss: 3.024 | Accuracy: 0.091797 | 4.857 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 026 | Total loss: 2.982 | Reg loss: 0.031 | Tree loss: 2.982 | Accuracy: 0.121094 | 4.857 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 026 | Total loss: 2.945 | Reg loss: 0.031 | Tree loss: 2.945 | Accuracy: 0.150391 | 4.856 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 026 | Total loss: 3.009 | Reg loss: 0.031 | Tree loss: 3.009 | Accuracy: 0.130859 | 4.856 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 026 | Total loss: 2.995 | Reg loss: 0.031 | Tree loss: 2.995 | Accuracy: 0.140625 | 4.855 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 026 | Total loss: 2.948 | Reg loss: 0.032 | Tree loss: 2.948 | Accuracy: 0.109375 | 4.855 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 026 | Total loss: 2.931 | Reg loss: 0.032 | Tree loss: 2.931 | Accuracy: 0.125000 | 4.854 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.113281 | 4.854 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.117188 | 4.854 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.115234 | 4.853 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 026 | Total loss: 2.921 | Reg loss: 0.032 | Tree loss: 2.921 | Accuracy: 0.140625 | 4.853 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 026 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.156250 | 4.852 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 026 | Total loss: 2.907 | Reg loss: 0.032 | Tree loss: 2.907 | Accuracy: 0.125000 | 4.852 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 026 | Total loss: 2.957 | Reg loss: 0.032 | Tree loss: 2.957 | Accuracy: 0.119141 | 4.852 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 026 | Total loss: 2.826 | Reg loss: 0.032 | Tree loss: 2.826 | Accuracy: 0.132812 | 4.851 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 026 | Total loss: 2.903 | Reg loss: 0.032 | Tree loss: 2.903 | Accuracy: 0.115234 | 4.851 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 026 | Total loss: 2.902 | Reg loss: 0.032 | Tree loss: 2.902 | Accuracy: 0.130859 | 4.85 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 026 | Total loss: 2.875 | Reg loss: 0.032 | Tree loss: 2.875 | Accuracy: 0.105469 | 4.85 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 026 | Total loss: 2.907 | Reg loss: 0.032 | Tree loss: 2.907 | Accuracy: 0.105469 | 4.85 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.123047 | 4.849 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 026 | Total loss: 2.864 | Reg loss: 0.032 | Tree loss: 2.864 | Accuracy: 0.158203 | 4.849 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.113281 | 4.848 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 026 | Total loss: 2.901 | Reg loss: 0.032 | Tree loss: 2.901 | Accuracy: 0.123047 | 4.848 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 026 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.109677 | 4.847 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 026 | Total loss: 2.959 | Reg loss: 0.031 | Tree loss: 2.959 | Accuracy: 0.148438 | 4.847 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 026 | Total loss: 3.001 | Reg loss: 0.031 | Tree loss: 3.001 | Accuracy: 0.111328 | 4.847 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 026 | Total loss: 2.944 | Reg loss: 0.031 | Tree loss: 2.944 | Accuracy: 0.109375 | 4.846 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 026 | Total loss: 3.017 | Reg loss: 0.031 | Tree loss: 3.017 | Accuracy: 0.113281 | 4.846 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 026 | Total loss: 2.938 | Reg loss: 0.031 | Tree loss: 2.938 | Accuracy: 0.156250 | 4.845 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 026 | Total loss: 2.956 | Reg loss: 0.031 | Tree loss: 2.956 | Accuracy: 0.107422 | 4.845 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 026 | Total loss: 2.928 | Reg loss: 0.031 | Tree loss: 2.928 | Accuracy: 0.134766 | 4.845 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 026 | Total loss: 2.973 | Reg loss: 0.031 | Tree loss: 2.973 | Accuracy: 0.126953 | 4.844 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 026 | Total loss: 2.958 | Reg loss: 0.031 | Tree loss: 2.958 | Accuracy: 0.109375 | 4.844 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.134766 | 4.843 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 026 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.107422 | 4.843 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 026 | Total loss: 2.941 | Reg loss: 0.032 | Tree loss: 2.941 | Accuracy: 0.113281 | 4.843 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 026 | Total loss: 2.930 | Reg loss: 0.032 | Tree loss: 2.930 | Accuracy: 0.128906 | 4.842 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.107422 | 4.842 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.091797 | 4.841 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 026 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.144531 | 4.841 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 026 | Total loss: 2.922 | Reg loss: 0.032 | Tree loss: 2.922 | Accuracy: 0.119141 | 4.841 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.105469 | 4.84 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 018 / 026 | Total loss: 2.915 | Reg loss: 0.032 | Tree loss: 2.915 | Accuracy: 0.148438 | 4.84 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.111328 | 4.84 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 026 | Total loss: 2.905 | Reg loss: 0.032 | Tree loss: 2.905 | Accuracy: 0.121094 | 4.839 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 026 | Total loss: 2.882 | Reg loss: 0.032 | Tree loss: 2.882 | Accuracy: 0.144531 | 4.839 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 026 | Total loss: 2.862 | Reg loss: 0.032 | Tree loss: 2.862 | Accuracy: 0.152344 | 4.838 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 026 | Total loss: 2.926 | Reg loss: 0.032 | Tree loss: 2.926 | Accuracy: 0.111328 | 4.838 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 026 | Total loss: 2.883 | Reg loss: 0.032 | Tree loss: 2.883 | Accuracy: 0.123047 | 4.838 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 026 | Total loss: 2.868 | Reg loss: 0.032 | Tree loss: 2.868 | Accuracy: 0.154839 | 4.837 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 026 | Total loss: 2.986 | Reg loss: 0.031 | Tree loss: 2.986 | Accuracy: 0.123047 | 4.837 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 026 | Total loss: 2.975 | Reg loss: 0.031 | Tree loss: 2.975 | Accuracy: 0.140625 | 4.836 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 026 | Total loss: 2.972 | Reg loss: 0.031 | Tree loss: 2.972 | Accuracy: 0.125000 | 4.836 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 026 | Total loss: 2.971 | Reg loss: 0.031 | Tree loss: 2.971 | Accuracy: 0.150391 | 4.835 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 026 | Total loss: 3.026 | Reg loss: 0.031 | Tree loss: 3.026 | Accuracy: 0.083984 | 4.835 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 026 | Total loss: 2.963 | Reg loss: 0.031 | Tree loss: 2.963 | Accuracy: 0.117188 | 4.835 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 026 | Total loss: 2.962 | Reg loss: 0.031 | Tree loss: 2.962 | Accuracy: 0.125000 | 4.834 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 026 | Total loss: 2.941 | Reg loss: 0.031 | Tree loss: 2.941 | Accuracy: 0.126953 | 4.834 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 026 | Total loss: 2.950 | Reg loss: 0.031 | Tree loss: 2.950 | Accuracy: 0.130859 | 4.833 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 026 | Total loss: 2.947 | Reg loss: 0.031 | Tree loss: 2.947 | Accuracy: 0.117188 | 4.833 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.117188 | 4.833 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.113281 | 4.832 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.117188 | 4.832 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 026 | Total loss: 2.922 | Reg loss: 0.032 | Tree loss: 2.922 | Accuracy: 0.123047 | 4.831 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 026 | Total loss: 2.895 | Reg loss: 0.032 | Tree loss: 2.895 | Accuracy: 0.119141 | 4.831 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 026 | Total loss: 2.871 | Reg loss: 0.032 | Tree loss: 2.871 | Accuracy: 0.126953 | 4.831 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 026 | Total loss: 2.931 | Reg loss: 0.032 | Tree loss: 2.931 | Accuracy: 0.123047 | 4.83 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.103516 | 4.83 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 026 | Total loss: 2.904 | Reg loss: 0.032 | Tree loss: 2.904 | Accuracy: 0.117188 | 4.83 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 026 | Total loss: 2.906 | Reg loss: 0.032 | Tree loss: 2.906 | Accuracy: 0.130859 | 4.829 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 026 | Total loss: 2.883 | Reg loss: 0.032 | Tree loss: 2.883 | Accuracy: 0.136719 | 4.829 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 026 | Total loss: 2.874 | Reg loss: 0.032 | Tree loss: 2.874 | Accuracy: 0.142578 | 4.828 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.144531 | 4.828 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 026 | Total loss: 2.902 | Reg loss: 0.032 | Tree loss: 2.902 | Accuracy: 0.109375 | 4.828 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 026 | Total loss: 2.880 | Reg loss: 0.032 | Tree loss: 2.880 | Accuracy: 0.111328 | 4.827 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 026 | Total loss: 2.872 | Reg loss: 0.032 | Tree loss: 2.872 | Accuracy: 0.174194 | 4.827 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 026 | Total loss: 2.976 | Reg loss: 0.031 | Tree loss: 2.976 | Accuracy: 0.126953 | 4.826 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 026 | Total loss: 2.958 | Reg loss: 0.031 | Tree loss: 2.958 | Accuracy: 0.130859 | 4.826 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 026 | Total loss: 3.005 | Reg loss: 0.031 | Tree loss: 3.005 | Accuracy: 0.109375 | 4.826 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 026 | Total loss: 2.968 | Reg loss: 0.031 | Tree loss: 2.968 | Accuracy: 0.123047 | 4.825 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 026 | Total loss: 2.958 | Reg loss: 0.031 | Tree loss: 2.958 | Accuracy: 0.123047 | 4.825 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 026 | Total loss: 2.992 | Reg loss: 0.031 | Tree loss: 2.992 | Accuracy: 0.134766 | 4.824 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 026 | Total loss: 2.999 | Reg loss: 0.031 | Tree loss: 2.999 | Accuracy: 0.097656 | 4.824 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 026 | Total loss: 2.942 | Reg loss: 0.031 | Tree loss: 2.942 | Accuracy: 0.119141 | 4.824 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 026 | Total loss: 2.924 | Reg loss: 0.031 | Tree loss: 2.924 | Accuracy: 0.152344 | 4.823 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 026 | Total loss: 2.904 | Reg loss: 0.031 | Tree loss: 2.904 | Accuracy: 0.138672 | 4.823 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 026 | Total loss: 2.937 | Reg loss: 0.031 | Tree loss: 2.937 | Accuracy: 0.119141 | 4.822 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 026 | Total loss: 2.921 | Reg loss: 0.032 | Tree loss: 2.921 | Accuracy: 0.101562 | 4.822 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.109375 | 4.822 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 026 | Total loss: 2.900 | Reg loss: 0.032 | Tree loss: 2.900 | Accuracy: 0.119141 | 4.821 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.117188 | 4.821 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 026 | Total loss: 2.901 | Reg loss: 0.032 | Tree loss: 2.901 | Accuracy: 0.144531 | 4.821 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.130859 | 4.82 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 026 | Total loss: 2.893 | Reg loss: 0.032 | Tree loss: 2.893 | Accuracy: 0.105469 | 4.82 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 026 | Total loss: 2.874 | Reg loss: 0.032 | Tree loss: 2.874 | Accuracy: 0.134766 | 4.819 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 026 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.113281 | 4.819 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 026 | Total loss: 2.955 | Reg loss: 0.032 | Tree loss: 2.955 | Accuracy: 0.115234 | 4.819 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 026 | Total loss: 2.894 | Reg loss: 0.032 | Tree loss: 2.894 | Accuracy: 0.132812 | 4.818 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 026 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.121094 | 4.818 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 026 | Total loss: 2.876 | Reg loss: 0.032 | Tree loss: 2.876 | Accuracy: 0.142578 | 4.818 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 026 | Total loss: 2.890 | Reg loss: 0.032 | Tree loss: 2.890 | Accuracy: 0.103516 | 4.817 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 026 | Total loss: 2.828 | Reg loss: 0.032 | Tree loss: 2.828 | Accuracy: 0.206452 | 4.817 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 000 / 026 | Total loss: 3.043 | Reg loss: 0.031 | Tree loss: 3.043 | Accuracy: 0.115234 | 4.816 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 026 | Total loss: 3.011 | Reg loss: 0.031 | Tree loss: 3.011 | Accuracy: 0.101562 | 4.816 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 026 | Total loss: 2.975 | Reg loss: 0.031 | Tree loss: 2.975 | Accuracy: 0.111328 | 4.816 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 026 | Total loss: 3.018 | Reg loss: 0.031 | Tree loss: 3.018 | Accuracy: 0.123047 | 4.815 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 026 | Total loss: 2.980 | Reg loss: 0.031 | Tree loss: 2.980 | Accuracy: 0.140625 | 4.815 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 026 | Total loss: 2.980 | Reg loss: 0.031 | Tree loss: 2.980 | Accuracy: 0.117188 | 4.814 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 026 | Total loss: 2.947 | Reg loss: 0.031 | Tree loss: 2.947 | Accuracy: 0.126953 | 4.814 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 026 | Total loss: 2.935 | Reg loss: 0.031 | Tree loss: 2.935 | Accuracy: 0.128906 | 4.814 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 026 | Total loss: 2.928 | Reg loss: 0.031 | Tree loss: 2.928 | Accuracy: 0.119141 | 4.813 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 026 | Total loss: 2.963 | Reg loss: 0.031 | Tree loss: 2.963 | Accuracy: 0.117188 | 4.813 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 026 | Total loss: 2.946 | Reg loss: 0.031 | Tree loss: 2.946 | Accuracy: 0.130859 | 4.812 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 026 | Total loss: 2.868 | Reg loss: 0.031 | Tree loss: 2.868 | Accuracy: 0.144531 | 4.812 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 026 | Total loss: 2.948 | Reg loss: 0.031 | Tree loss: 2.948 | Accuracy: 0.119141 | 4.812 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.117188 | 4.811 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.109375 | 4.811 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 026 | Total loss: 2.911 | Reg loss: 0.032 | Tree loss: 2.911 | Accuracy: 0.146484 | 4.811 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 026 | Total loss: 2.859 | Reg loss: 0.032 | Tree loss: 2.859 | Accuracy: 0.148438 | 4.81 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 026 | Total loss: 2.915 | Reg loss: 0.032 | Tree loss: 2.915 | Accuracy: 0.113281 | 4.81 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 026 | Total loss: 2.904 | Reg loss: 0.032 | Tree loss: 2.904 | Accuracy: 0.109375 | 4.81 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 026 | Total loss: 2.897 | Reg loss: 0.032 | Tree loss: 2.897 | Accuracy: 0.115234 | 4.809 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 026 | Total loss: 2.866 | Reg loss: 0.032 | Tree loss: 2.866 | Accuracy: 0.138672 | 4.809 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.121094 | 4.808 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 026 | Total loss: 2.893 | Reg loss: 0.032 | Tree loss: 2.893 | Accuracy: 0.109375 | 4.808 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 026 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.123047 | 4.808 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 026 | Total loss: 2.862 | Reg loss: 0.032 | Tree loss: 2.862 | Accuracy: 0.142578 | 4.807 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 026 | Total loss: 2.896 | Reg loss: 0.032 | Tree loss: 2.896 | Accuracy: 0.129032 | 4.807 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 026 | Total loss: 2.996 | Reg loss: 0.031 | Tree loss: 2.996 | Accuracy: 0.117188 | 4.807 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 026 | Total loss: 2.972 | Reg loss: 0.031 | Tree loss: 2.972 | Accuracy: 0.109375 | 4.806 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 026 | Total loss: 2.961 | Reg loss: 0.031 | Tree loss: 2.961 | Accuracy: 0.140625 | 4.806 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 026 | Total loss: 2.963 | Reg loss: 0.031 | Tree loss: 2.963 | Accuracy: 0.132812 | 4.805 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 026 | Total loss: 3.009 | Reg loss: 0.031 | Tree loss: 3.009 | Accuracy: 0.113281 | 4.805 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 026 | Total loss: 2.947 | Reg loss: 0.031 | Tree loss: 2.947 | Accuracy: 0.128906 | 4.805 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 026 | Total loss: 2.940 | Reg loss: 0.031 | Tree loss: 2.940 | Accuracy: 0.123047 | 4.804 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 026 | Total loss: 2.967 | Reg loss: 0.031 | Tree loss: 2.967 | Accuracy: 0.119141 | 4.804 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 026 | Total loss: 2.926 | Reg loss: 0.031 | Tree loss: 2.926 | Accuracy: 0.138672 | 4.803 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 026 | Total loss: 2.940 | Reg loss: 0.031 | Tree loss: 2.940 | Accuracy: 0.134766 | 4.803 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 026 | Total loss: 2.907 | Reg loss: 0.031 | Tree loss: 2.907 | Accuracy: 0.115234 | 4.803 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 026 | Total loss: 2.961 | Reg loss: 0.031 | Tree loss: 2.961 | Accuracy: 0.117188 | 4.802 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 026 | Total loss: 2.932 | Reg loss: 0.031 | Tree loss: 2.932 | Accuracy: 0.095703 | 4.802 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 026 | Total loss: 2.960 | Reg loss: 0.031 | Tree loss: 2.960 | Accuracy: 0.125000 | 4.802 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 026 | Total loss: 2.911 | Reg loss: 0.032 | Tree loss: 2.911 | Accuracy: 0.123047 | 4.801 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 026 | Total loss: 2.883 | Reg loss: 0.032 | Tree loss: 2.883 | Accuracy: 0.146484 | 4.801 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 026 | Total loss: 2.908 | Reg loss: 0.032 | Tree loss: 2.908 | Accuracy: 0.132812 | 4.801 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 026 | Total loss: 2.922 | Reg loss: 0.032 | Tree loss: 2.922 | Accuracy: 0.107422 | 4.8 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 026 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.115234 | 4.8 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 026 | Total loss: 2.916 | Reg loss: 0.032 | Tree loss: 2.916 | Accuracy: 0.105469 | 4.8 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 026 | Total loss: 2.909 | Reg loss: 0.032 | Tree loss: 2.909 | Accuracy: 0.117188 | 4.799 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 026 | Total loss: 2.893 | Reg loss: 0.032 | Tree loss: 2.893 | Accuracy: 0.115234 | 4.799 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 026 | Total loss: 2.857 | Reg loss: 0.032 | Tree loss: 2.857 | Accuracy: 0.128906 | 4.799 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 026 | Total loss: 2.916 | Reg loss: 0.032 | Tree loss: 2.916 | Accuracy: 0.144531 | 4.798 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 026 | Total loss: 2.875 | Reg loss: 0.032 | Tree loss: 2.875 | Accuracy: 0.138672 | 4.798 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 026 | Total loss: 2.842 | Reg loss: 0.032 | Tree loss: 2.842 | Accuracy: 0.141935 | 4.797 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 026 | Total loss: 3.017 | Reg loss: 0.031 | Tree loss: 3.017 | Accuracy: 0.123047 | 4.797 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 026 | Total loss: 2.966 | Reg loss: 0.031 | Tree loss: 2.966 | Accuracy: 0.119141 | 4.797 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 026 | Total loss: 2.965 | Reg loss: 0.031 | Tree loss: 2.965 | Accuracy: 0.136719 | 4.796 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 026 | Total loss: 2.967 | Reg loss: 0.031 | Tree loss: 2.967 | Accuracy: 0.123047 | 4.796 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 026 | Total loss: 3.007 | Reg loss: 0.031 | Tree loss: 3.007 | Accuracy: 0.105469 | 4.795 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 026 | Total loss: 2.931 | Reg loss: 0.031 | Tree loss: 2.931 | Accuracy: 0.148438 | 4.795 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 026 | Total loss: 2.957 | Reg loss: 0.031 | Tree loss: 2.957 | Accuracy: 0.125000 | 4.795 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 026 | Total loss: 2.946 | Reg loss: 0.031 | Tree loss: 2.946 | Accuracy: 0.142578 | 4.794 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 026 | Total loss: 2.974 | Reg loss: 0.031 | Tree loss: 2.974 | Accuracy: 0.123047 | 4.794 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 009 / 026 | Total loss: 2.974 | Reg loss: 0.031 | Tree loss: 2.974 | Accuracy: 0.119141 | 4.794 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 026 | Total loss: 2.923 | Reg loss: 0.031 | Tree loss: 2.923 | Accuracy: 0.128906 | 4.793 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 026 | Total loss: 2.936 | Reg loss: 0.031 | Tree loss: 2.936 | Accuracy: 0.107422 | 4.793 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 026 | Total loss: 2.918 | Reg loss: 0.031 | Tree loss: 2.918 | Accuracy: 0.107422 | 4.793 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 026 | Total loss: 2.911 | Reg loss: 0.031 | Tree loss: 2.911 | Accuracy: 0.101562 | 4.792 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 026 | Total loss: 2.921 | Reg loss: 0.031 | Tree loss: 2.921 | Accuracy: 0.109375 | 4.792 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 026 | Total loss: 2.879 | Reg loss: 0.032 | Tree loss: 2.879 | Accuracy: 0.132812 | 4.792 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 026 | Total loss: 2.891 | Reg loss: 0.032 | Tree loss: 2.891 | Accuracy: 0.119141 | 4.791 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 026 | Total loss: 2.914 | Reg loss: 0.032 | Tree loss: 2.914 | Accuracy: 0.119141 | 4.791 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.121094 | 4.791 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.126953 | 4.79 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 026 | Total loss: 2.863 | Reg loss: 0.032 | Tree loss: 2.863 | Accuracy: 0.142578 | 4.79 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 026 | Total loss: 2.875 | Reg loss: 0.032 | Tree loss: 2.875 | Accuracy: 0.130859 | 4.79 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 026 | Total loss: 2.854 | Reg loss: 0.032 | Tree loss: 2.854 | Accuracy: 0.130859 | 4.789 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 026 | Total loss: 2.899 | Reg loss: 0.032 | Tree loss: 2.899 | Accuracy: 0.128906 | 4.789 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 026 | Total loss: 2.916 | Reg loss: 0.032 | Tree loss: 2.916 | Accuracy: 0.128906 | 4.788 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 026 | Total loss: 2.889 | Reg loss: 0.032 | Tree loss: 2.889 | Accuracy: 0.090323 | 4.788 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 026 | Total loss: 2.973 | Reg loss: 0.031 | Tree loss: 2.973 | Accuracy: 0.134766 | 4.788 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 026 | Total loss: 2.980 | Reg loss: 0.031 | Tree loss: 2.980 | Accuracy: 0.142578 | 4.787 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 026 | Total loss: 2.928 | Reg loss: 0.031 | Tree loss: 2.928 | Accuracy: 0.148438 | 4.787 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 026 | Total loss: 2.952 | Reg loss: 0.031 | Tree loss: 2.952 | Accuracy: 0.123047 | 4.787 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 026 | Total loss: 2.956 | Reg loss: 0.031 | Tree loss: 2.956 | Accuracy: 0.107422 | 4.786 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 026 | Total loss: 2.928 | Reg loss: 0.031 | Tree loss: 2.928 | Accuracy: 0.121094 | 4.786 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.099609 | 4.785 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 026 | Total loss: 2.989 | Reg loss: 0.031 | Tree loss: 2.989 | Accuracy: 0.103516 | 4.785 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 026 | Total loss: 2.922 | Reg loss: 0.031 | Tree loss: 2.922 | Accuracy: 0.134766 | 4.785 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 026 | Total loss: 2.952 | Reg loss: 0.031 | Tree loss: 2.952 | Accuracy: 0.132812 | 4.784 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 026 | Total loss: 2.944 | Reg loss: 0.031 | Tree loss: 2.944 | Accuracy: 0.128906 | 4.784 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 026 | Total loss: 2.935 | Reg loss: 0.031 | Tree loss: 2.935 | Accuracy: 0.125000 | 4.784 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 026 | Total loss: 2.974 | Reg loss: 0.031 | Tree loss: 2.974 | Accuracy: 0.128906 | 4.783 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.144531 | 4.783 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.111328 | 4.783 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 026 | Total loss: 2.930 | Reg loss: 0.032 | Tree loss: 2.930 | Accuracy: 0.121094 | 4.782 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 026 | Total loss: 2.862 | Reg loss: 0.032 | Tree loss: 2.862 | Accuracy: 0.126953 | 4.782 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 026 | Total loss: 2.911 | Reg loss: 0.032 | Tree loss: 2.911 | Accuracy: 0.105469 | 4.782 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 026 | Total loss: 2.894 | Reg loss: 0.032 | Tree loss: 2.894 | Accuracy: 0.093750 | 4.781 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.119141 | 4.781 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 026 | Total loss: 2.912 | Reg loss: 0.032 | Tree loss: 2.912 | Accuracy: 0.115234 | 4.781 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 026 | Total loss: 2.860 | Reg loss: 0.032 | Tree loss: 2.860 | Accuracy: 0.134766 | 4.78 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 026 | Total loss: 2.856 | Reg loss: 0.032 | Tree loss: 2.856 | Accuracy: 0.140625 | 4.78 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 026 | Total loss: 2.901 | Reg loss: 0.032 | Tree loss: 2.901 | Accuracy: 0.138672 | 4.78 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 026 | Total loss: 2.920 | Reg loss: 0.032 | Tree loss: 2.920 | Accuracy: 0.113281 | 4.779 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 026 | Total loss: 2.878 | Reg loss: 0.032 | Tree loss: 2.878 | Accuracy: 0.109677 | 4.779 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 026 | Total loss: 2.978 | Reg loss: 0.031 | Tree loss: 2.978 | Accuracy: 0.130859 | 4.779 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 026 | Total loss: 3.003 | Reg loss: 0.031 | Tree loss: 3.003 | Accuracy: 0.091797 | 4.778 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 026 | Total loss: 3.005 | Reg loss: 0.031 | Tree loss: 3.005 | Accuracy: 0.130859 | 4.778 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 026 | Total loss: 2.988 | Reg loss: 0.031 | Tree loss: 2.988 | Accuracy: 0.126953 | 4.777 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 026 | Total loss: 2.934 | Reg loss: 0.031 | Tree loss: 2.934 | Accuracy: 0.134766 | 4.777 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 026 | Total loss: 2.944 | Reg loss: 0.031 | Tree loss: 2.944 | Accuracy: 0.140625 | 4.777 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 026 | Total loss: 2.983 | Reg loss: 0.031 | Tree loss: 2.983 | Accuracy: 0.117188 | 4.776 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 026 | Total loss: 2.918 | Reg loss: 0.031 | Tree loss: 2.918 | Accuracy: 0.134766 | 4.776 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 026 | Total loss: 2.912 | Reg loss: 0.031 | Tree loss: 2.912 | Accuracy: 0.130859 | 4.776 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.140625 | 4.775 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 026 | Total loss: 2.925 | Reg loss: 0.031 | Tree loss: 2.925 | Accuracy: 0.132812 | 4.775 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.128906 | 4.775 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 026 | Total loss: 2.936 | Reg loss: 0.031 | Tree loss: 2.936 | Accuracy: 0.103516 | 4.774 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 026 | Total loss: 2.932 | Reg loss: 0.031 | Tree loss: 2.932 | Accuracy: 0.134766 | 4.774 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 026 | Total loss: 2.926 | Reg loss: 0.031 | Tree loss: 2.926 | Accuracy: 0.123047 | 4.774 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 026 | Total loss: 2.933 | Reg loss: 0.031 | Tree loss: 2.933 | Accuracy: 0.119141 | 4.773 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.134766 | 4.773 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 026 | Total loss: 2.874 | Reg loss: 0.032 | Tree loss: 2.874 | Accuracy: 0.105469 | 4.773 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 018 / 026 | Total loss: 2.894 | Reg loss: 0.032 | Tree loss: 2.894 | Accuracy: 0.111328 | 4.772 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 026 | Total loss: 2.879 | Reg loss: 0.032 | Tree loss: 2.879 | Accuracy: 0.144531 | 4.772 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 026 | Total loss: 2.880 | Reg loss: 0.032 | Tree loss: 2.880 | Accuracy: 0.119141 | 4.772 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.115234 | 4.771 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 026 | Total loss: 2.853 | Reg loss: 0.032 | Tree loss: 2.853 | Accuracy: 0.128906 | 4.771 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 026 | Total loss: 2.922 | Reg loss: 0.032 | Tree loss: 2.922 | Accuracy: 0.101562 | 4.771 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 026 | Total loss: 2.864 | Reg loss: 0.032 | Tree loss: 2.864 | Accuracy: 0.117188 | 4.77 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 026 | Total loss: 2.883 | Reg loss: 0.032 | Tree loss: 2.883 | Accuracy: 0.096774 | 4.77 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 026 | Total loss: 2.972 | Reg loss: 0.031 | Tree loss: 2.972 | Accuracy: 0.115234 | 4.77 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 026 | Total loss: 2.986 | Reg loss: 0.031 | Tree loss: 2.986 | Accuracy: 0.111328 | 4.769 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 026 | Total loss: 2.955 | Reg loss: 0.031 | Tree loss: 2.955 | Accuracy: 0.134766 | 4.769 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 026 | Total loss: 2.958 | Reg loss: 0.031 | Tree loss: 2.958 | Accuracy: 0.134766 | 4.769 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 026 | Total loss: 2.992 | Reg loss: 0.031 | Tree loss: 2.992 | Accuracy: 0.074219 | 4.768 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 026 | Total loss: 2.974 | Reg loss: 0.031 | Tree loss: 2.974 | Accuracy: 0.126953 | 4.768 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 026 | Total loss: 2.935 | Reg loss: 0.031 | Tree loss: 2.935 | Accuracy: 0.119141 | 4.767 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 026 | Total loss: 2.972 | Reg loss: 0.031 | Tree loss: 2.972 | Accuracy: 0.103516 | 4.767 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 026 | Total loss: 2.917 | Reg loss: 0.031 | Tree loss: 2.917 | Accuracy: 0.136719 | 4.767 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 026 | Total loss: 2.961 | Reg loss: 0.031 | Tree loss: 2.961 | Accuracy: 0.117188 | 4.767 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 026 | Total loss: 2.928 | Reg loss: 0.031 | Tree loss: 2.928 | Accuracy: 0.134766 | 4.766 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 026 | Total loss: 2.903 | Reg loss: 0.031 | Tree loss: 2.903 | Accuracy: 0.123047 | 4.766 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 026 | Total loss: 2.938 | Reg loss: 0.031 | Tree loss: 2.938 | Accuracy: 0.119141 | 4.766 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 026 | Total loss: 2.893 | Reg loss: 0.031 | Tree loss: 2.893 | Accuracy: 0.123047 | 4.765 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 026 | Total loss: 2.924 | Reg loss: 0.031 | Tree loss: 2.924 | Accuracy: 0.117188 | 4.765 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 026 | Total loss: 2.903 | Reg loss: 0.031 | Tree loss: 2.903 | Accuracy: 0.107422 | 4.765 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 026 | Total loss: 2.901 | Reg loss: 0.031 | Tree loss: 2.901 | Accuracy: 0.134766 | 4.764 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.123047 | 4.764 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 026 | Total loss: 2.900 | Reg loss: 0.032 | Tree loss: 2.900 | Accuracy: 0.140625 | 4.764 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 026 | Total loss: 2.877 | Reg loss: 0.032 | Tree loss: 2.877 | Accuracy: 0.144531 | 4.763 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 026 | Total loss: 2.879 | Reg loss: 0.032 | Tree loss: 2.879 | Accuracy: 0.119141 | 4.763 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 026 | Total loss: 2.911 | Reg loss: 0.032 | Tree loss: 2.911 | Accuracy: 0.140625 | 4.763 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 026 | Total loss: 2.873 | Reg loss: 0.032 | Tree loss: 2.873 | Accuracy: 0.136719 | 4.762 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 026 | Total loss: 2.866 | Reg loss: 0.032 | Tree loss: 2.866 | Accuracy: 0.134766 | 4.762 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.123047 | 4.762 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 026 | Total loss: 2.883 | Reg loss: 0.032 | Tree loss: 2.883 | Accuracy: 0.109677 | 4.761 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 026 | Total loss: 2.964 | Reg loss: 0.031 | Tree loss: 2.964 | Accuracy: 0.123047 | 4.761 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 026 | Total loss: 2.962 | Reg loss: 0.031 | Tree loss: 2.962 | Accuracy: 0.146484 | 4.761 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 026 | Total loss: 2.952 | Reg loss: 0.031 | Tree loss: 2.952 | Accuracy: 0.119141 | 4.76 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 026 | Total loss: 2.947 | Reg loss: 0.031 | Tree loss: 2.947 | Accuracy: 0.128906 | 4.76 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 026 | Total loss: 2.974 | Reg loss: 0.031 | Tree loss: 2.974 | Accuracy: 0.128906 | 4.759 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 026 | Total loss: 2.967 | Reg loss: 0.031 | Tree loss: 2.967 | Accuracy: 0.132812 | 4.759 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 026 | Total loss: 2.915 | Reg loss: 0.031 | Tree loss: 2.915 | Accuracy: 0.148438 | 4.759 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.136719 | 4.759 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 026 | Total loss: 2.957 | Reg loss: 0.031 | Tree loss: 2.957 | Accuracy: 0.107422 | 4.758 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 026 | Total loss: 2.907 | Reg loss: 0.031 | Tree loss: 2.907 | Accuracy: 0.109375 | 4.758 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 026 | Total loss: 2.918 | Reg loss: 0.031 | Tree loss: 2.918 | Accuracy: 0.121094 | 4.758 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 026 | Total loss: 2.938 | Reg loss: 0.031 | Tree loss: 2.938 | Accuracy: 0.146484 | 4.757 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 026 | Total loss: 2.941 | Reg loss: 0.031 | Tree loss: 2.941 | Accuracy: 0.134766 | 4.757 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 026 | Total loss: 2.865 | Reg loss: 0.031 | Tree loss: 2.865 | Accuracy: 0.136719 | 4.757 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 026 | Total loss: 2.932 | Reg loss: 0.031 | Tree loss: 2.932 | Accuracy: 0.126953 | 4.756 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 026 | Total loss: 2.931 | Reg loss: 0.031 | Tree loss: 2.931 | Accuracy: 0.115234 | 4.756 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 026 | Total loss: 2.996 | Reg loss: 0.031 | Tree loss: 2.996 | Accuracy: 0.113281 | 4.756 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 026 | Total loss: 2.894 | Reg loss: 0.031 | Tree loss: 2.894 | Accuracy: 0.099609 | 4.755 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 026 | Total loss: 2.862 | Reg loss: 0.032 | Tree loss: 2.862 | Accuracy: 0.113281 | 4.755 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 026 | Total loss: 2.886 | Reg loss: 0.032 | Tree loss: 2.886 | Accuracy: 0.134766 | 4.755 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 026 | Total loss: 2.886 | Reg loss: 0.032 | Tree loss: 2.886 | Accuracy: 0.125000 | 4.754 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 026 | Total loss: 2.906 | Reg loss: 0.032 | Tree loss: 2.906 | Accuracy: 0.105469 | 4.754 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 026 | Total loss: 2.873 | Reg loss: 0.032 | Tree loss: 2.873 | Accuracy: 0.111328 | 4.754 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 026 | Total loss: 2.889 | Reg loss: 0.032 | Tree loss: 2.889 | Accuracy: 0.126953 | 4.753 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 026 | Total loss: 2.940 | Reg loss: 0.032 | Tree loss: 2.940 | Accuracy: 0.101562 | 4.753 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 026 | Total loss: 2.834 | Reg loss: 0.032 | Tree loss: 2.834 | Accuracy: 0.116129 | 4.753 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6ff0da6aa74951a35016f44b1314e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eca4e1ec6604f209a926eb18adb2118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed8987746ae46e1823173c8a964bf40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06157ee98107455597e978de6eadad80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 11.991871921182266\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 4060\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 =============="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "12955\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "============== Pattern 1025 ==============\n",
      "============== Pattern 1026 ==============\n",
      "============== Pattern 1027 ==============\n",
      "============== Pattern 1028 ==============\n",
      "============== Pattern 1029 ==============\n",
      "============== Pattern 1030 ==============\n",
      "============== Pattern 1031 ==============\n",
      "============== Pattern 1032 ==============\n",
      "============== Pattern 1033 ==============\n",
      "============== Pattern 1034 ==============\n",
      "============== Pattern 1035 ==============\n",
      "============== Pattern 1036 ==============\n",
      "============== Pattern 1037 ==============\n",
      "============== Pattern 1038 ==============\n",
      "============== Pattern 1039 ==============\n",
      "============== Pattern 1040 ==============\n",
      "============== Pattern 1041 ==============\n",
      "============== Pattern 1042 ==============\n",
      "============== Pattern 1043 ==============\n",
      "============== Pattern 1044 ==============\n",
      "============== Pattern 1045 ==============\n",
      "============== Pattern 1046 ==============\n",
      "============== Pattern 1047 ==============\n",
      "============== Pattern 1048 ==============\n",
      "============== Pattern 1049 ==============\n",
      "============== Pattern 1050 ==============\n",
      "============== Pattern 1051 ==============\n",
      "============== Pattern 1052 ==============\n",
      "============== Pattern 1053 ==============\n",
      "============== Pattern 1054 ==============\n",
      "============== Pattern 1055 ==============\n",
      "============== Pattern 1056 ==============\n",
      "============== Pattern 1057 ==============\n",
      "============== Pattern 1058 ==============\n",
      "============== Pattern 1059 ==============\n",
      "============== Pattern 1060 ==============\n",
      "============== Pattern 1061 ==============\n",
      "============== Pattern 1062 ==============\n",
      "============== Pattern 1063 ==============\n",
      "============== Pattern 1064 ==============\n",
      "============== Pattern 1065 ==============\n",
      "============== Pattern 1066 ==============\n",
      "============== Pattern 1067 ==============\n",
      "============== Pattern 1068 ==============\n",
      "============== Pattern 1069 ==============\n",
      "============== Pattern 1070 ==============\n",
      "============== Pattern 1071 ==============\n",
      "============== Pattern 1072 ==============\n",
      "============== Pattern 1073 ==============\n",
      "============== Pattern 1074 ==============\n",
      "============== Pattern 1075 ==============\n",
      "============== Pattern 1076 ==============\n",
      "============== Pattern 1077 ==============\n",
      "============== Pattern 1078 ==============\n",
      "============== Pattern 1079 ==============\n",
      "============== Pattern 1080 ==============\n",
      "============== Pattern 1081 ==============\n",
      "============== Pattern 1082 ==============\n",
      "============== Pattern 1083 ==============\n",
      "============== Pattern 1084 ==============\n",
      "============== Pattern 1085 ==============\n",
      "============== Pattern 1086 ==============\n",
      "============== Pattern 1087 ==============\n",
      "============== Pattern 1088 ==============\n",
      "============== Pattern 1089 ==============\n",
      "============== Pattern 1090 ==============\n",
      "============== Pattern 1091 ==============\n",
      "============== Pattern 1092 ==============\n",
      "============== Pattern 1093 ==============\n",
      "============== Pattern 1094 ==============\n",
      "============== Pattern 1095 ==============\n",
      "============== Pattern 1096 ==============\n",
      "============== Pattern 1097 ==============\n",
      "============== Pattern 1098 ==============\n",
      "============== Pattern 1099 ==============\n",
      "============== Pattern 1100 ==============\n",
      "============== Pattern 1101 ==============\n",
      "============== Pattern 1102 ==============\n",
      "============== Pattern 1103 ==============\n",
      "============== Pattern 1104 ==============\n",
      "============== Pattern 1105 ==============\n",
      "============== Pattern 1106 ==============\n",
      "============== Pattern 1107 ==============\n",
      "============== Pattern 1108 ==============\n",
      "============== Pattern 1109 ==============\n",
      "============== Pattern 1110 ==============\n",
      "============== Pattern 1111 ==============\n",
      "============== Pattern 1112 ==============\n",
      "============== Pattern 1113 ==============\n",
      "============== Pattern 1114 ==============\n",
      "============== Pattern 1115 ==============\n",
      "============== Pattern 1116 ==============\n",
      "============== Pattern 1117 ==============\n",
      "============== Pattern 1118 ==============\n",
      "============== Pattern 1119 ==============\n",
      "============== Pattern 1120 ==============\n",
      "============== Pattern 1121 ==============\n",
      "============== Pattern 1122 ==============\n",
      "============== Pattern 1123 ==============\n",
      "============== Pattern 1124 ==============\n",
      "============== Pattern 1125 ==============\n",
      "============== Pattern 1126 ==============\n",
      "============== Pattern 1127 ==============\n",
      "============== Pattern 1128 ==============\n",
      "============== Pattern 1129 ==============\n",
      "============== Pattern 1130 ==============\n",
      "============== Pattern 1131 ==============\n",
      "============== Pattern 1132 ==============\n",
      "============== Pattern 1133 ==============\n",
      "============== Pattern 1134 ==============\n",
      "============== Pattern 1135 ==============\n",
      "============== Pattern 1136 ==============\n",
      "============== Pattern 1137 ==============\n",
      "============== Pattern 1138 ==============\n",
      "============== Pattern 1139 ==============\n",
      "============== Pattern 1140 ==============\n",
      "============== Pattern 1141 ==============\n",
      "============== Pattern 1142 ==============\n",
      "============== Pattern 1143 ==============\n",
      "============== Pattern 1144 ==============\n",
      "============== Pattern 1145 ==============\n",
      "============== Pattern 1146 ==============\n",
      "============== Pattern 1147 ==============\n",
      "============== Pattern 1148 ==============\n",
      "============== Pattern 1149 ==============\n",
      "============== Pattern 1150 ==============\n",
      "============== Pattern 1151 ==============\n",
      "============== Pattern 1152 ==============\n",
      "============== Pattern 1153 ==============\n",
      "============== Pattern 1154 ==============\n",
      "============== Pattern 1155 ==============\n",
      "============== Pattern 1156 ==============\n",
      "============== Pattern 1157 ==============\n",
      "============== Pattern 1158 ==============\n",
      "============== Pattern 1159 ==============\n",
      "============== Pattern 1160 ==============\n",
      "============== Pattern 1161 ==============\n",
      "============== Pattern 1162 ==============\n",
      "============== Pattern 1163 ==============\n",
      "============== Pattern 1164 ==============\n",
      "============== Pattern 1165 ==============\n",
      "============== Pattern 1166 ==============\n",
      "============== Pattern 1167 ==============\n",
      "============== Pattern 1168 ==============\n",
      "============== Pattern 1169 ==============\n",
      "============== Pattern 1170 ==============\n",
      "============== Pattern 1171 ==============\n",
      "============== Pattern 1172 ==============\n",
      "============== Pattern 1173 ==============\n",
      "============== Pattern 1174 ==============\n",
      "============== Pattern 1175 ==============\n",
      "============== Pattern 1176 ==============\n",
      "============== Pattern 1177 ==============\n",
      "============== Pattern 1178 ==============\n",
      "============== Pattern 1179 ==============\n",
      "============== Pattern 1180 ==============\n",
      "============== Pattern 1181 ==============\n",
      "============== Pattern 1182 ==============\n",
      "============== Pattern 1183 ==============\n",
      "============== Pattern 1184 ==============\n",
      "============== Pattern 1185 ==============\n",
      "============== Pattern 1186 ==============\n",
      "============== Pattern 1187 ==============\n",
      "============== Pattern 1188 ==============\n",
      "============== Pattern 1189 ==============\n",
      "============== Pattern 1190 ==============\n",
      "============== Pattern 1191 ==============\n",
      "============== Pattern 1192 ==============\n",
      "============== Pattern 1193 ==============\n",
      "============== Pattern 1194 ==============\n",
      "============== Pattern 1195 ==============\n",
      "============== Pattern 1196 ==============\n",
      "============== Pattern 1197 ==============\n",
      "============== Pattern 1198 ==============\n",
      "============== Pattern 1199 ==============\n",
      "============== Pattern 1200 ==============\n",
      "============== Pattern 1201 ==============\n",
      "============== Pattern 1202 ==============\n",
      "============== Pattern 1203 ==============\n",
      "============== Pattern 1204 ==============\n",
      "============== Pattern 1205 ==============\n",
      "============== Pattern 1206 ==============\n",
      "============== Pattern 1207 ==============\n",
      "============== Pattern 1208 ==============\n",
      "============== Pattern 1209 ==============\n",
      "============== Pattern 1210 ==============\n",
      "============== Pattern 1211 ==============\n",
      "============== Pattern 1212 ==============\n",
      "============== Pattern 1213 ==============\n",
      "============== Pattern 1214 ==============\n",
      "============== Pattern 1215 ==============\n",
      "============== Pattern 1216 ==============\n",
      "============== Pattern 1217 ==============\n",
      "============== Pattern 1218 ==============\n",
      "============== Pattern 1219 ==============\n",
      "============== Pattern 1220 ==============\n",
      "============== Pattern 1221 ==============\n",
      "============== Pattern 1222 ==============\n",
      "============== Pattern 1223 ==============\n",
      "============== Pattern 1224 ==============\n",
      "============== Pattern 1225 ==============\n",
      "============== Pattern 1226 ==============\n",
      "============== Pattern 1227 ==============\n",
      "============== Pattern 1228 ==============\n",
      "============== Pattern 1229 ==============\n",
      "============== Pattern 1230 ==============\n",
      "============== Pattern 1231 ==============\n",
      "============== Pattern 1232 ==============\n",
      "============== Pattern 1233 ==============\n",
      "============== Pattern 1234 ==============\n",
      "============== Pattern 1235 ==============\n",
      "============== Pattern 1236 ==============\n",
      "============== Pattern 1237 ==============\n",
      "============== Pattern 1238 ==============\n",
      "============== Pattern 1239 ==============\n",
      "============== Pattern 1240 ==============\n",
      "============== Pattern 1241 ==============\n",
      "============== Pattern 1242 ==============\n",
      "============== Pattern 1243 ==============\n",
      "============== Pattern 1244 ==============\n",
      "============== Pattern 1245 ==============\n",
      "============== Pattern 1246 ==============\n",
      "============== Pattern 1247 ==============\n",
      "============== Pattern 1248 ==============\n",
      "============== Pattern 1249 ==============\n",
      "============== Pattern 1250 ==============\n",
      "============== Pattern 1251 ==============\n",
      "============== Pattern 1252 ==============\n",
      "============== Pattern 1253 ==============\n",
      "============== Pattern 1254 ==============\n",
      "============== Pattern 1255 ==============\n",
      "============== Pattern 1256 ==============\n",
      "============== Pattern 1257 ==============\n",
      "============== Pattern 1258 ==============\n",
      "============== Pattern 1259 ==============\n",
      "============== Pattern 1260 ==============\n",
      "============== Pattern 1261 ==============\n",
      "============== Pattern 1262 ==============\n",
      "============== Pattern 1263 ==============\n",
      "============== Pattern 1264 ==============\n",
      "============== Pattern 1265 ==============\n",
      "============== Pattern 1266 ==============\n",
      "============== Pattern 1267 ==============\n",
      "============== Pattern 1268 ==============\n",
      "============== Pattern 1269 ==============\n",
      "============== Pattern 1270 ==============\n",
      "============== Pattern 1271 ==============\n",
      "============== Pattern 1272 ==============\n",
      "============== Pattern 1273 ==============\n",
      "============== Pattern 1274 ==============\n",
      "============== Pattern 1275 ==============\n",
      "============== Pattern 1276 ==============\n",
      "============== Pattern 1277 ==============\n",
      "============== Pattern 1278 ==============\n",
      "============== Pattern 1279 ==============\n",
      "============== Pattern 1280 ==============\n",
      "============== Pattern 1281 ==============\n",
      "============== Pattern 1282 ==============\n",
      "============== Pattern 1283 ==============\n",
      "============== Pattern 1284 ==============\n",
      "============== Pattern 1285 ==============\n",
      "============== Pattern 1286 ==============\n",
      "============== Pattern 1287 ==============\n",
      "============== Pattern 1288 ==============\n",
      "============== Pattern 1289 ==============\n",
      "============== Pattern 1290 ==============\n",
      "============== Pattern 1291 ==============\n",
      "============== Pattern 1292 ==============\n",
      "============== Pattern 1293 ==============\n",
      "============== Pattern 1294 ==============\n",
      "============== Pattern 1295 ==============\n",
      "============== Pattern 1296 ==============\n",
      "============== Pattern 1297 ==============\n",
      "============== Pattern 1298 ==============\n",
      "============== Pattern 1299 ==============\n",
      "============== Pattern 1300 ==============\n",
      "============== Pattern 1301 ==============\n",
      "============== Pattern 1302 ==============\n",
      "============== Pattern 1303 ==============\n",
      "============== Pattern 1304 ==============\n",
      "============== Pattern 1305 ==============\n",
      "============== Pattern 1306 ==============\n",
      "============== Pattern 1307 ==============\n",
      "============== Pattern 1308 ==============\n",
      "============== Pattern 1309 ==============\n",
      "============== Pattern 1310 ==============\n",
      "============== Pattern 1311 ==============\n",
      "============== Pattern 1312 ==============\n",
      "============== Pattern 1313 ==============\n",
      "============== Pattern 1314 ==============\n",
      "============== Pattern 1315 ==============\n",
      "============== Pattern 1316 ==============\n",
      "============== Pattern 1317 ==============\n",
      "============== Pattern 1318 ==============\n",
      "============== Pattern 1319 ==============\n",
      "============== Pattern 1320 ==============\n",
      "============== Pattern 1321 ==============\n",
      "============== Pattern 1322 ==============\n",
      "============== Pattern 1323 ==============\n",
      "============== Pattern 1324 ==============\n",
      "============== Pattern 1325 ==============\n",
      "============== Pattern 1326 ==============\n",
      "============== Pattern 1327 ==============\n",
      "============== Pattern 1328 ==============\n",
      "============== Pattern 1329 ==============\n",
      "============== Pattern 1330 ==============\n",
      "============== Pattern 1331 ==============\n",
      "============== Pattern 1332 ==============\n",
      "============== Pattern 1333 ==============\n",
      "============== Pattern 1334 ==============\n",
      "============== Pattern 1335 ==============\n",
      "============== Pattern 1336 ==============\n",
      "============== Pattern 1337 ==============\n",
      "============== Pattern 1338 ==============\n",
      "============== Pattern 1339 ==============\n",
      "============== Pattern 1340 ==============\n",
      "============== Pattern 1341 ==============\n",
      "============== Pattern 1342 ==============\n",
      "============== Pattern 1343 ==============\n",
      "============== Pattern 1344 ==============\n",
      "============== Pattern 1345 ==============\n",
      "============== Pattern 1346 ==============\n",
      "============== Pattern 1347 ==============\n",
      "============== Pattern 1348 ==============\n",
      "============== Pattern 1349 ==============\n",
      "============== Pattern 1350 ==============\n",
      "============== Pattern 1351 ==============\n",
      "============== Pattern 1352 ==============\n",
      "============== Pattern 1353 ==============\n",
      "============== Pattern 1354 ==============\n",
      "============== Pattern 1355 ==============\n",
      "============== Pattern 1356 ==============\n",
      "============== Pattern 1357 ==============\n",
      "============== Pattern 1358 ==============\n",
      "============== Pattern 1359 ==============\n",
      "============== Pattern 1360 ==============\n",
      "============== Pattern 1361 ==============\n",
      "============== Pattern 1362 ==============\n",
      "============== Pattern 1363 ==============\n",
      "============== Pattern 1364 ==============\n",
      "============== Pattern 1365 ==============\n",
      "============== Pattern 1366 ==============\n",
      "============== Pattern 1367 ==============\n",
      "============== Pattern 1368 ==============\n",
      "============== Pattern 1369 ==============\n",
      "============== Pattern 1370 ==============\n",
      "============== Pattern 1371 ==============\n",
      "============== Pattern 1372 ==============\n",
      "============== Pattern 1373 ==============\n",
      "============== Pattern 1374 ==============\n",
      "============== Pattern 1375 ==============\n",
      "============== Pattern 1376 ==============\n",
      "============== Pattern 1377 ==============\n",
      "============== Pattern 1378 ==============\n",
      "============== Pattern 1379 ==============\n",
      "============== Pattern 1380 ==============\n",
      "============== Pattern 1381 ==============\n",
      "============== Pattern 1382 ==============\n",
      "============== Pattern 1383 ==============\n",
      "============== Pattern 1384 ==============\n",
      "============== Pattern 1385 ==============\n",
      "============== Pattern 1386 ==============\n",
      "============== Pattern 1387 ==============\n",
      "============== Pattern 1388 ==============\n",
      "============== Pattern 1389 ==============\n",
      "============== Pattern 1390 ==============\n",
      "============== Pattern 1391 ==============\n",
      "============== Pattern 1392 ==============\n",
      "============== Pattern 1393 ==============\n",
      "============== Pattern 1394 ==============\n",
      "============== Pattern 1395 ==============\n",
      "============== Pattern 1396 ==============\n",
      "============== Pattern 1397 ==============\n",
      "============== Pattern 1398 ==============\n",
      "============== Pattern 1399 ==============\n",
      "============== Pattern 1400 ==============\n",
      "============== Pattern 1401 ==============\n",
      "============== Pattern 1402 ==============\n",
      "============== Pattern 1403 ==============\n",
      "============== Pattern 1404 ==============\n",
      "============== Pattern 1405 ==============\n",
      "============== Pattern 1406 ==============\n",
      "============== Pattern 1407 ==============\n",
      "============== Pattern 1408 ==============\n",
      "============== Pattern 1409 ==============\n",
      "============== Pattern 1410 ==============\n",
      "============== Pattern 1411 ==============\n",
      "============== Pattern 1412 ==============\n",
      "============== Pattern 1413 ==============\n",
      "============== Pattern 1414 ==============\n",
      "============== Pattern 1415 ==============\n",
      "============== Pattern 1416 ==============\n",
      "============== Pattern 1417 ==============\n",
      "============== Pattern 1418 ==============\n",
      "============== Pattern 1419 ==============\n",
      "============== Pattern 1420 ==============\n",
      "============== Pattern 1421 ==============\n",
      "============== Pattern 1422 ==============\n",
      "============== Pattern 1423 ==============\n",
      "============== Pattern 1424 ==============\n",
      "============== Pattern 1425 ==============\n",
      "============== Pattern 1426 ==============\n",
      "============== Pattern 1427 ==============\n",
      "============== Pattern 1428 ==============\n",
      "============== Pattern 1429 ==============\n",
      "============== Pattern 1430 ==============\n",
      "============== Pattern 1431 ==============\n",
      "============== Pattern 1432 ==============\n",
      "============== Pattern 1433 ==============\n",
      "============== Pattern 1434 ==============\n",
      "============== Pattern 1435 ==============\n",
      "============== Pattern 1436 ==============\n",
      "============== Pattern 1437 ==============\n",
      "============== Pattern 1438 ==============\n",
      "============== Pattern 1439 ==============\n",
      "============== Pattern 1440 ==============\n",
      "============== Pattern 1441 ==============\n",
      "============== Pattern 1442 ==============\n",
      "============== Pattern 1443 ==============\n",
      "============== Pattern 1444 ==============\n",
      "============== Pattern 1445 ==============\n",
      "============== Pattern 1446 ==============\n",
      "============== Pattern 1447 ==============\n",
      "============== Pattern 1448 ==============\n",
      "============== Pattern 1449 ==============\n",
      "============== Pattern 1450 ==============\n",
      "============== Pattern 1451 ==============\n",
      "============== Pattern 1452 ==============\n",
      "============== Pattern 1453 ==============\n",
      "============== Pattern 1454 ==============\n",
      "============== Pattern 1455 ==============\n",
      "============== Pattern 1456 ==============\n",
      "============== Pattern 1457 ==============\n",
      "============== Pattern 1458 ==============\n",
      "============== Pattern 1459 ==============\n",
      "============== Pattern 1460 ==============\n",
      "============== Pattern 1461 ==============\n",
      "============== Pattern 1462 ==============\n",
      "============== Pattern 1463 ==============\n",
      "============== Pattern 1464 ==============\n",
      "============== Pattern 1465 ==============\n",
      "============== Pattern 1466 ==============\n",
      "============== Pattern 1467 ==============\n",
      "============== Pattern 1468 ==============\n",
      "============== Pattern 1469 ==============\n",
      "============== Pattern 1470 ==============\n",
      "============== Pattern 1471 ==============\n",
      "============== Pattern 1472 ==============\n",
      "============== Pattern 1473 ==============\n",
      "============== Pattern 1474 ==============\n",
      "============== Pattern 1475 ==============\n",
      "============== Pattern 1476 ==============\n",
      "============== Pattern 1477 ==============\n",
      "============== Pattern 1478 ==============\n",
      "============== Pattern 1479 ==============\n",
      "============== Pattern 1480 ==============\n",
      "============== Pattern 1481 ==============\n",
      "============== Pattern 1482 ==============\n",
      "============== Pattern 1483 ==============\n",
      "============== Pattern 1484 ==============\n",
      "============== Pattern 1485 ==============\n",
      "============== Pattern 1486 ==============\n",
      "============== Pattern 1487 ==============\n",
      "============== Pattern 1488 ==============\n",
      "============== Pattern 1489 ==============\n",
      "============== Pattern 1490 ==============\n",
      "============== Pattern 1491 ==============\n",
      "============== Pattern 1492 ==============\n",
      "============== Pattern 1493 ==============\n",
      "============== Pattern 1494 ==============\n",
      "============== Pattern 1495 ==============\n",
      "============== Pattern 1496 ==============\n",
      "============== Pattern 1497 ==============\n",
      "============== Pattern 1498 ==============\n",
      "============== Pattern 1499 ==============\n",
      "============== Pattern 1500 ==============\n",
      "============== Pattern 1501 ==============\n",
      "============== Pattern 1502 ==============\n",
      "============== Pattern 1503 ==============\n",
      "============== Pattern 1504 ==============\n",
      "============== Pattern 1505 ==============\n",
      "============== Pattern 1506 ==============\n",
      "============== Pattern 1507 ==============\n",
      "============== Pattern 1508 ==============\n",
      "============== Pattern 1509 ==============\n",
      "============== Pattern 1510 ==============\n",
      "============== Pattern 1511 ==============\n",
      "============== Pattern 1512 ==============\n",
      "============== Pattern 1513 ==============\n",
      "============== Pattern 1514 ==============\n",
      "============== Pattern 1515 ==============\n",
      "============== Pattern 1516 ==============\n",
      "============== Pattern 1517 ==============\n",
      "============== Pattern 1518 ==============\n",
      "============== Pattern 1519 ==============\n",
      "============== Pattern 1520 ==============\n",
      "============== Pattern 1521 ==============\n",
      "============== Pattern 1522 ==============\n",
      "============== Pattern 1523 ==============\n",
      "============== Pattern 1524 ==============\n",
      "============== Pattern 1525 ==============\n",
      "============== Pattern 1526 ==============\n",
      "============== Pattern 1527 ==============\n",
      "============== Pattern 1528 ==============\n",
      "============== Pattern 1529 ==============\n",
      "============== Pattern 1530 ==============\n",
      "============== Pattern 1531 ==============\n",
      "============== Pattern 1532 ==============\n",
      "============== Pattern 1533 ==============\n",
      "============== Pattern 1534 ==============\n",
      "============== Pattern 1535 ==============\n",
      "============== Pattern 1536 ==============\n",
      "============== Pattern 1537 ==============\n",
      "============== Pattern 1538 ==============\n",
      "============== Pattern 1539 ==============\n",
      "============== Pattern 1540 ==============\n",
      "============== Pattern 1541 ==============\n",
      "============== Pattern 1542 ==============\n",
      "============== Pattern 1543 ==============\n",
      "============== Pattern 1544 ==============\n",
      "============== Pattern 1545 ==============\n",
      "============== Pattern 1546 ==============\n",
      "============== Pattern 1547 ==============\n",
      "============== Pattern 1548 ==============\n",
      "============== Pattern 1549 ==============\n",
      "============== Pattern 1550 ==============\n",
      "============== Pattern 1551 ==============\n",
      "============== Pattern 1552 ==============\n",
      "============== Pattern 1553 ==============\n",
      "============== Pattern 1554 ==============\n",
      "============== Pattern 1555 ==============\n",
      "============== Pattern 1556 ==============\n",
      "============== Pattern 1557 ==============\n",
      "============== Pattern 1558 ==============\n",
      "============== Pattern 1559 ==============\n",
      "============== Pattern 1560 ==============\n",
      "============== Pattern 1561 ==============\n",
      "============== Pattern 1562 ==============\n",
      "============== Pattern 1563 ==============\n",
      "============== Pattern 1564 ==============\n",
      "============== Pattern 1565 ==============\n",
      "============== Pattern 1566 ==============\n",
      "============== Pattern 1567 ==============\n",
      "============== Pattern 1568 ==============\n",
      "============== Pattern 1569 ==============\n",
      "============== Pattern 1570 ==============\n",
      "============== Pattern 1571 ==============\n",
      "============== Pattern 1572 ==============\n",
      "============== Pattern 1573 ==============\n",
      "============== Pattern 1574 ==============\n",
      "============== Pattern 1575 ==============\n",
      "============== Pattern 1576 ==============\n",
      "============== Pattern 1577 ==============\n",
      "============== Pattern 1578 ==============\n",
      "============== Pattern 1579 ==============\n",
      "============== Pattern 1580 ==============\n",
      "============== Pattern 1581 ==============\n",
      "============== Pattern 1582 ==============\n",
      "============== Pattern 1583 ==============\n",
      "============== Pattern 1584 ==============\n",
      "============== Pattern 1585 ==============\n",
      "============== Pattern 1586 ==============\n",
      "============== Pattern 1587 ==============\n",
      "============== Pattern 1588 ==============\n",
      "============== Pattern 1589 ==============\n",
      "============== Pattern 1590 ==============\n",
      "============== Pattern 1591 ==============\n",
      "============== Pattern 1592 ==============\n",
      "============== Pattern 1593 ==============\n",
      "============== Pattern 1594 ==============\n",
      "============== Pattern 1595 ==============\n",
      "============== Pattern 1596 ==============\n",
      "============== Pattern 1597 ==============\n",
      "============== Pattern 1598 ==============\n",
      "============== Pattern 1599 ==============\n",
      "============== Pattern 1600 ==============\n",
      "============== Pattern 1601 ==============\n",
      "============== Pattern 1602 ==============\n",
      "============== Pattern 1603 ==============\n",
      "============== Pattern 1604 ==============\n",
      "============== Pattern 1605 ==============\n",
      "============== Pattern 1606 ==============\n",
      "============== Pattern 1607 ==============\n",
      "============== Pattern 1608 ==============\n",
      "============== Pattern 1609 ==============\n",
      "============== Pattern 1610 ==============\n",
      "============== Pattern 1611 ==============\n",
      "============== Pattern 1612 ==============\n",
      "============== Pattern 1613 ==============\n",
      "============== Pattern 1614 ==============\n",
      "============== Pattern 1615 ==============\n",
      "============== Pattern 1616 ==============\n",
      "============== Pattern 1617 ==============\n",
      "============== Pattern 1618 ==============\n",
      "============== Pattern 1619 ==============\n",
      "============== Pattern 1620 ==============\n",
      "============== Pattern 1621 ==============\n",
      "============== Pattern 1622 ==============\n",
      "============== Pattern 1623 ==============\n",
      "============== Pattern 1624 ==============\n",
      "============== Pattern 1625 ==============\n",
      "============== Pattern 1626 ==============\n",
      "============== Pattern 1627 ==============\n",
      "============== Pattern 1628 ==============\n",
      "============== Pattern 1629 ==============\n",
      "============== Pattern 1630 ==============\n",
      "============== Pattern 1631 ==============\n",
      "============== Pattern 1632 ==============\n",
      "============== Pattern 1633 ==============\n",
      "============== Pattern 1634 ==============\n",
      "============== Pattern 1635 ==============\n",
      "============== Pattern 1636 ==============\n",
      "============== Pattern 1637 ==============\n",
      "============== Pattern 1638 ==============\n",
      "============== Pattern 1639 ==============\n",
      "============== Pattern 1640 ==============\n",
      "============== Pattern 1641 ==============\n",
      "============== Pattern 1642 ==============\n",
      "============== Pattern 1643 ==============\n",
      "============== Pattern 1644 ==============\n",
      "============== Pattern 1645 ==============\n",
      "============== Pattern 1646 ==============\n",
      "============== Pattern 1647 ==============\n",
      "============== Pattern 1648 ==============\n",
      "============== Pattern 1649 ==============\n",
      "============== Pattern 1650 ==============\n",
      "============== Pattern 1651 ==============\n",
      "============== Pattern 1652 ==============\n",
      "============== Pattern 1653 ==============\n",
      "============== Pattern 1654 ==============\n",
      "============== Pattern 1655 ==============\n",
      "============== Pattern 1656 ==============\n",
      "============== Pattern 1657 ==============\n",
      "============== Pattern 1658 ==============\n",
      "============== Pattern 1659 ==============\n",
      "============== Pattern 1660 ==============\n",
      "============== Pattern 1661 ==============\n",
      "============== Pattern 1662 ==============\n",
      "============== Pattern 1663 ==============\n",
      "============== Pattern 1664 ==============\n",
      "============== Pattern 1665 ==============\n",
      "============== Pattern 1666 ==============\n",
      "============== Pattern 1667 ==============\n",
      "============== Pattern 1668 ==============\n",
      "============== Pattern 1669 ==============\n",
      "============== Pattern 1670 ==============\n",
      "============== Pattern 1671 ==============\n",
      "============== Pattern 1672 ==============\n",
      "============== Pattern 1673 ==============\n",
      "============== Pattern 1674 ==============\n",
      "============== Pattern 1675 ==============\n",
      "============== Pattern 1676 ==============\n",
      "============== Pattern 1677 ==============\n",
      "============== Pattern 1678 ==============\n",
      "============== Pattern 1679 ==============\n",
      "============== Pattern 1680 ==============\n",
      "============== Pattern 1681 ==============\n",
      "============== Pattern 1682 ==============\n",
      "============== Pattern 1683 ==============\n",
      "============== Pattern 1684 ==============\n",
      "============== Pattern 1685 ==============\n",
      "============== Pattern 1686 ==============\n",
      "============== Pattern 1687 ==============\n",
      "============== Pattern 1688 ==============\n",
      "============== Pattern 1689 ==============\n",
      "============== Pattern 1690 ==============\n",
      "============== Pattern 1691 ==============\n",
      "============== Pattern 1692 ==============\n",
      "============== Pattern 1693 ==============\n",
      "============== Pattern 1694 ==============\n",
      "============== Pattern 1695 ==============\n",
      "============== Pattern 1696 ==============\n",
      "============== Pattern 1697 ==============\n",
      "============== Pattern 1698 ==============\n",
      "============== Pattern 1699 ==============\n",
      "============== Pattern 1700 ==============\n",
      "============== Pattern 1701 ==============\n",
      "============== Pattern 1702 ==============\n",
      "============== Pattern 1703 ==============\n",
      "============== Pattern 1704 ==============\n",
      "============== Pattern 1705 ==============\n",
      "============== Pattern 1706 ==============\n",
      "============== Pattern 1707 ==============\n",
      "============== Pattern 1708 ==============\n",
      "============== Pattern 1709 ==============\n",
      "============== Pattern 1710 ==============\n",
      "============== Pattern 1711 ==============\n",
      "============== Pattern 1712 ==============\n",
      "============== Pattern 1713 ==============\n",
      "============== Pattern 1714 ==============\n",
      "============== Pattern 1715 ==============\n",
      "============== Pattern 1716 ==============\n",
      "============== Pattern 1717 ==============\n",
      "============== Pattern 1718 ==============\n",
      "============== Pattern 1719 ==============\n",
      "============== Pattern 1720 ==============\n",
      "============== Pattern 1721 ==============\n",
      "============== Pattern 1722 ==============\n",
      "============== Pattern 1723 ==============\n",
      "============== Pattern 1724 ==============\n",
      "============== Pattern 1725 ==============\n",
      "============== Pattern 1726 ==============\n",
      "============== Pattern 1727 ==============\n",
      "============== Pattern 1728 ==============\n",
      "============== Pattern 1729 ==============\n",
      "============== Pattern 1730 ==============\n",
      "============== Pattern 1731 ==============\n",
      "============== Pattern 1732 ==============\n",
      "============== Pattern 1733 ==============\n",
      "============== Pattern 1734 ==============\n",
      "============== Pattern 1735 ==============\n",
      "============== Pattern 1736 ==============\n",
      "============== Pattern 1737 ==============\n",
      "============== Pattern 1738 ==============\n",
      "============== Pattern 1739 ==============\n",
      "============== Pattern 1740 ==============\n",
      "============== Pattern 1741 ==============\n",
      "============== Pattern 1742 ==============\n",
      "============== Pattern 1743 ==============\n",
      "============== Pattern 1744 ==============\n",
      "============== Pattern 1745 ==============\n",
      "============== Pattern 1746 ==============\n",
      "============== Pattern 1747 ==============\n",
      "============== Pattern 1748 ==============\n",
      "============== Pattern 1749 ==============\n",
      "============== Pattern 1750 ==============\n",
      "============== Pattern 1751 ==============\n",
      "============== Pattern 1752 ==============\n",
      "============== Pattern 1753 ==============\n",
      "============== Pattern 1754 ==============\n",
      "============== Pattern 1755 ==============\n",
      "============== Pattern 1756 ==============\n",
      "============== Pattern 1757 ==============\n",
      "============== Pattern 1758 ==============\n",
      "============== Pattern 1759 ==============\n",
      "============== Pattern 1760 ==============\n",
      "============== Pattern 1761 ==============\n",
      "============== Pattern 1762 ==============\n",
      "============== Pattern 1763 ==============\n",
      "============== Pattern 1764 ==============\n",
      "============== Pattern 1765 ==============\n",
      "============== Pattern 1766 ==============\n",
      "============== Pattern 1767 ==============\n",
      "============== Pattern 1768 ==============\n",
      "============== Pattern 1769 ==============\n",
      "============== Pattern 1770 ==============\n",
      "============== Pattern 1771 ==============\n",
      "============== Pattern 1772 ==============\n",
      "============== Pattern 1773 ==============\n",
      "============== Pattern 1774 ==============\n",
      "============== Pattern 1775 ==============\n",
      "============== Pattern 1776 ==============\n",
      "============== Pattern 1777 ==============\n",
      "============== Pattern 1778 ==============\n",
      "============== Pattern 1779 ==============\n",
      "============== Pattern 1780 ==============\n",
      "============== Pattern 1781 ==============\n",
      "============== Pattern 1782 ==============\n",
      "============== Pattern 1783 ==============\n",
      "============== Pattern 1784 ==============\n",
      "============== Pattern 1785 ==============\n",
      "============== Pattern 1786 ==============\n",
      "============== Pattern 1787 ==============\n",
      "============== Pattern 1788 ==============\n",
      "============== Pattern 1789 ==============\n",
      "============== Pattern 1790 ==============\n",
      "============== Pattern 1791 ==============\n",
      "============== Pattern 1792 ==============\n",
      "============== Pattern 1793 ==============\n",
      "============== Pattern 1794 ==============\n",
      "============== Pattern 1795 ==============\n",
      "============== Pattern 1796 ==============\n",
      "============== Pattern 1797 ==============\n",
      "============== Pattern 1798 ==============\n",
      "============== Pattern 1799 ==============\n",
      "============== Pattern 1800 ==============\n",
      "============== Pattern 1801 ==============\n",
      "============== Pattern 1802 ==============\n",
      "============== Pattern 1803 ==============\n",
      "============== Pattern 1804 ==============\n",
      "============== Pattern 1805 ==============\n",
      "============== Pattern 1806 ==============\n",
      "============== Pattern 1807 ==============\n",
      "============== Pattern 1808 ==============\n",
      "============== Pattern 1809 ==============\n",
      "============== Pattern 1810 ==============\n",
      "============== Pattern 1811 ==============\n",
      "============== Pattern 1812 ==============\n",
      "============== Pattern 1813 ==============\n",
      "============== Pattern 1814 ==============\n",
      "============== Pattern 1815 ==============\n",
      "============== Pattern 1816 ==============\n",
      "============== Pattern 1817 ==============\n",
      "============== Pattern 1818 ==============\n",
      "============== Pattern 1819 ==============\n",
      "============== Pattern 1820 ==============\n",
      "============== Pattern 1821 ==============\n",
      "============== Pattern 1822 ==============\n",
      "============== Pattern 1823 ==============\n",
      "============== Pattern 1824 ==============\n",
      "============== Pattern 1825 ==============\n",
      "============== Pattern 1826 ==============\n",
      "============== Pattern 1827 ==============\n",
      "============== Pattern 1828 ==============\n",
      "============== Pattern 1829 ==============\n",
      "============== Pattern 1830 ==============\n",
      "============== Pattern 1831 ==============\n",
      "============== Pattern 1832 ==============\n",
      "============== Pattern 1833 ==============\n",
      "============== Pattern 1834 ==============\n",
      "============== Pattern 1835 ==============\n",
      "============== Pattern 1836 ==============\n",
      "============== Pattern 1837 ==============\n",
      "============== Pattern 1838 ==============\n",
      "============== Pattern 1839 ==============\n",
      "============== Pattern 1840 ==============\n",
      "============== Pattern 1841 ==============\n",
      "============== Pattern 1842 ==============\n",
      "============== Pattern 1843 ==============\n",
      "============== Pattern 1844 ==============\n",
      "============== Pattern 1845 ==============\n",
      "============== Pattern 1846 ==============\n",
      "============== Pattern 1847 ==============\n",
      "============== Pattern 1848 ==============\n",
      "============== Pattern 1849 ==============\n",
      "============== Pattern 1850 ==============\n",
      "============== Pattern 1851 ==============\n",
      "============== Pattern 1852 ==============\n",
      "============== Pattern 1853 ==============\n",
      "============== Pattern 1854 ==============\n",
      "============== Pattern 1855 ==============\n",
      "============== Pattern 1856 ==============\n",
      "============== Pattern 1857 ==============\n",
      "============== Pattern 1858 ==============\n",
      "============== Pattern 1859 ==============\n",
      "============== Pattern 1860 ==============\n",
      "============== Pattern 1861 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1862 ==============\n",
      "============== Pattern 1863 ==============\n",
      "============== Pattern 1864 ==============\n",
      "============== Pattern 1865 ==============\n",
      "============== Pattern 1866 ==============\n",
      "============== Pattern 1867 ==============\n",
      "============== Pattern 1868 ==============\n",
      "============== Pattern 1869 ==============\n",
      "============== Pattern 1870 ==============\n",
      "============== Pattern 1871 ==============\n",
      "============== Pattern 1872 ==============\n",
      "============== Pattern 1873 ==============\n",
      "============== Pattern 1874 ==============\n",
      "============== Pattern 1875 ==============\n",
      "============== Pattern 1876 ==============\n",
      "============== Pattern 1877 ==============\n",
      "============== Pattern 1878 ==============\n",
      "============== Pattern 1879 ==============\n",
      "============== Pattern 1880 ==============\n",
      "============== Pattern 1881 ==============\n",
      "============== Pattern 1882 ==============\n",
      "============== Pattern 1883 ==============\n",
      "============== Pattern 1884 ==============\n",
      "============== Pattern 1885 ==============\n",
      "============== Pattern 1886 ==============\n",
      "============== Pattern 1887 ==============\n",
      "============== Pattern 1888 ==============\n",
      "============== Pattern 1889 ==============\n",
      "============== Pattern 1890 ==============\n",
      "============== Pattern 1891 ==============\n",
      "============== Pattern 1892 ==============\n",
      "============== Pattern 1893 ==============\n",
      "============== Pattern 1894 ==============\n",
      "============== Pattern 1895 ==============\n",
      "============== Pattern 1896 ==============\n",
      "============== Pattern 1897 ==============\n",
      "============== Pattern 1898 ==============\n",
      "============== Pattern 1899 ==============\n",
      "============== Pattern 1900 ==============\n",
      "============== Pattern 1901 ==============\n",
      "============== Pattern 1902 ==============\n",
      "============== Pattern 1903 ==============\n",
      "============== Pattern 1904 ==============\n",
      "============== Pattern 1905 ==============\n",
      "============== Pattern 1906 ==============\n",
      "============== Pattern 1907 ==============\n",
      "============== Pattern 1908 ==============\n",
      "============== Pattern 1909 ==============\n",
      "============== Pattern 1910 ==============\n",
      "============== Pattern 1911 ==============\n",
      "============== Pattern 1912 ==============\n",
      "============== Pattern 1913 ==============\n",
      "============== Pattern 1914 ==============\n",
      "============== Pattern 1915 ==============\n",
      "============== Pattern 1916 ==============\n",
      "============== Pattern 1917 ==============\n",
      "============== Pattern 1918 ==============\n",
      "============== Pattern 1919 ==============\n",
      "============== Pattern 1920 ==============\n",
      "============== Pattern 1921 ==============\n",
      "============== Pattern 1922 ==============\n",
      "============== Pattern 1923 ==============\n",
      "============== Pattern 1924 ==============\n",
      "============== Pattern 1925 ==============\n",
      "============== Pattern 1926 ==============\n",
      "============== Pattern 1927 ==============\n",
      "============== Pattern 1928 ==============\n",
      "============== Pattern 1929 ==============\n",
      "============== Pattern 1930 ==============\n",
      "============== Pattern 1931 ==============\n",
      "============== Pattern 1932 ==============\n",
      "============== Pattern 1933 ==============\n",
      "============== Pattern 1934 ==============\n",
      "============== Pattern 1935 ==============\n",
      "============== Pattern 1936 ==============\n",
      "============== Pattern 1937 ==============\n",
      "============== Pattern 1938 ==============\n",
      "============== Pattern 1939 ==============\n",
      "============== Pattern 1940 ==============\n",
      "============== Pattern 1941 ==============\n",
      "============== Pattern 1942 ==============\n",
      "============== Pattern 1943 ==============\n",
      "============== Pattern 1944 ==============\n",
      "============== Pattern 1945 ==============\n",
      "============== Pattern 1946 ==============\n",
      "============== Pattern 1947 ==============\n",
      "============== Pattern 1948 ==============\n",
      "============== Pattern 1949 ==============\n",
      "============== Pattern 1950 ==============\n",
      "============== Pattern 1951 ==============\n",
      "============== Pattern 1952 ==============\n",
      "============== Pattern 1953 ==============\n",
      "============== Pattern 1954 ==============\n",
      "============== Pattern 1955 ==============\n",
      "============== Pattern 1956 ==============\n",
      "============== Pattern 1957 ==============\n",
      "============== Pattern 1958 ==============\n",
      "============== Pattern 1959 ==============\n",
      "============== Pattern 1960 ==============\n",
      "============== Pattern 1961 ==============\n",
      "============== Pattern 1962 ==============\n",
      "============== Pattern 1963 ==============\n",
      "============== Pattern 1964 ==============\n",
      "============== Pattern 1965 ==============\n",
      "============== Pattern 1966 ==============\n",
      "============== Pattern 1967 ==============\n",
      "============== Pattern 1968 ==============\n",
      "============== Pattern 1969 ==============\n",
      "============== Pattern 1970 ==============\n",
      "============== Pattern 1971 ==============\n",
      "============== Pattern 1972 ==============\n",
      "============== Pattern 1973 ==============\n",
      "============== Pattern 1974 ==============\n",
      "============== Pattern 1975 ==============\n",
      "============== Pattern 1976 ==============\n",
      "============== Pattern 1977 ==============\n",
      "============== Pattern 1978 ==============\n",
      "============== Pattern 1979 ==============\n",
      "============== Pattern 1980 ==============\n",
      "============== Pattern 1981 ==============\n",
      "============== Pattern 1982 ==============\n",
      "============== Pattern 1983 ==============\n",
      "============== Pattern 1984 ==============\n",
      "============== Pattern 1985 ==============\n",
      "============== Pattern 1986 ==============\n",
      "============== Pattern 1987 ==============\n",
      "============== Pattern 1988 ==============\n",
      "============== Pattern 1989 ==============\n",
      "============== Pattern 1990 ==============\n",
      "============== Pattern 1991 ==============\n",
      "============== Pattern 1992 ==============\n",
      "============== Pattern 1993 ==============\n",
      "============== Pattern 1994 ==============\n",
      "============== Pattern 1995 ==============\n",
      "============== Pattern 1996 ==============\n",
      "============== Pattern 1997 ==============\n",
      "============== Pattern 1998 ==============\n",
      "============== Pattern 1999 ==============\n",
      "============== Pattern 2000 ==============\n",
      "============== Pattern 2001 ==============\n",
      "============== Pattern 2002 ==============\n",
      "============== Pattern 2003 ==============\n",
      "============== Pattern 2004 ==============\n",
      "============== Pattern 2005 ==============\n",
      "============== Pattern 2006 ==============\n",
      "============== Pattern 2007 ==============\n",
      "============== Pattern 2008 ==============\n",
      "============== Pattern 2009 ==============\n",
      "============== Pattern 2010 ==============\n",
      "============== Pattern 2011 ==============\n",
      "============== Pattern 2012 ==============\n",
      "============== Pattern 2013 ==============\n",
      "============== Pattern 2014 ==============\n",
      "============== Pattern 2015 ==============\n",
      "============== Pattern 2016 ==============\n",
      "============== Pattern 2017 ==============\n",
      "============== Pattern 2018 ==============\n",
      "============== Pattern 2019 ==============\n",
      "============== Pattern 2020 ==============\n",
      "============== Pattern 2021 ==============\n",
      "============== Pattern 2022 ==============\n",
      "============== Pattern 2023 ==============\n",
      "============== Pattern 2024 ==============\n",
      "============== Pattern 2025 ==============\n",
      "============== Pattern 2026 ==============\n",
      "============== Pattern 2027 ==============\n",
      "============== Pattern 2028 ==============\n",
      "============== Pattern 2029 ==============\n",
      "============== Pattern 2030 ==============\n",
      "============== Pattern 2031 ==============\n",
      "============== Pattern 2032 ==============\n",
      "============== Pattern 2033 ==============\n",
      "============== Pattern 2034 ==============\n",
      "============== Pattern 2035 ==============\n",
      "============== Pattern 2036 ==============\n",
      "============== Pattern 2037 ==============\n",
      "============== Pattern 2038 ==============\n",
      "============== Pattern 2039 ==============\n",
      "============== Pattern 2040 ==============\n",
      "============== Pattern 2041 ==============\n",
      "============== Pattern 2042 ==============\n",
      "============== Pattern 2043 ==============\n",
      "============== Pattern 2044 ==============\n",
      "============== Pattern 2045 ==============\n",
      "============== Pattern 2046 ==============\n",
      "============== Pattern 2047 ==============\n",
      "============== Pattern 2048 ==============\n",
      "============== Pattern 2049 ==============\n",
      "============== Pattern 2050 ==============\n",
      "============== Pattern 2051 ==============\n",
      "============== Pattern 2052 ==============\n",
      "============== Pattern 2053 ==============\n",
      "============== Pattern 2054 ==============\n",
      "============== Pattern 2055 ==============\n",
      "============== Pattern 2056 ==============\n",
      "============== Pattern 2057 ==============\n",
      "============== Pattern 2058 ==============\n",
      "============== Pattern 2059 ==============\n",
      "============== Pattern 2060 ==============\n",
      "============== Pattern 2061 ==============\n",
      "============== Pattern 2062 ==============\n",
      "============== Pattern 2063 ==============\n",
      "============== Pattern 2064 ==============\n",
      "============== Pattern 2065 ==============\n",
      "============== Pattern 2066 ==============\n",
      "============== Pattern 2067 ==============\n",
      "============== Pattern 2068 ==============\n",
      "============== Pattern 2069 ==============\n",
      "============== Pattern 2070 ==============\n",
      "============== Pattern 2071 ==============\n",
      "============== Pattern 2072 ==============\n",
      "============== Pattern 2073 ==============\n",
      "============== Pattern 2074 ==============\n",
      "============== Pattern 2075 ==============\n",
      "============== Pattern 2076 ==============\n",
      "============== Pattern 2077 ==============\n",
      "============== Pattern 2078 ==============\n",
      "============== Pattern 2079 ==============\n",
      "============== Pattern 2080 ==============\n",
      "============== Pattern 2081 ==============\n",
      "============== Pattern 2082 ==============\n",
      "============== Pattern 2083 ==============\n",
      "============== Pattern 2084 ==============\n",
      "============== Pattern 2085 ==============\n",
      "============== Pattern 2086 ==============\n",
      "============== Pattern 2087 ==============\n",
      "============== Pattern 2088 ==============\n",
      "============== Pattern 2089 ==============\n",
      "============== Pattern 2090 ==============\n",
      "============== Pattern 2091 ==============\n",
      "============== Pattern 2092 ==============\n",
      "============== Pattern 2093 ==============\n",
      "============== Pattern 2094 ==============\n",
      "============== Pattern 2095 ==============\n",
      "============== Pattern 2096 ==============\n",
      "============== Pattern 2097 ==============\n",
      "============== Pattern 2098 ==============\n",
      "============== Pattern 2099 ==============\n",
      "============== Pattern 2100 ==============\n",
      "============== Pattern 2101 ==============\n",
      "============== Pattern 2102 ==============\n",
      "============== Pattern 2103 ==============\n",
      "============== Pattern 2104 ==============\n",
      "============== Pattern 2105 ==============\n",
      "============== Pattern 2106 ==============\n",
      "============== Pattern 2107 ==============\n",
      "============== Pattern 2108 ==============\n",
      "============== Pattern 2109 ==============\n",
      "============== Pattern 2110 ==============\n",
      "============== Pattern 2111 ==============\n",
      "============== Pattern 2112 ==============\n",
      "============== Pattern 2113 ==============\n",
      "============== Pattern 2114 ==============\n",
      "============== Pattern 2115 ==============\n",
      "============== Pattern 2116 ==============\n",
      "============== Pattern 2117 ==============\n",
      "============== Pattern 2118 ==============\n",
      "============== Pattern 2119 ==============\n",
      "============== Pattern 2120 ==============\n",
      "============== Pattern 2121 ==============\n",
      "============== Pattern 2122 ==============\n",
      "============== Pattern 2123 ==============\n",
      "============== Pattern 2124 ==============\n",
      "============== Pattern 2125 ==============\n",
      "============== Pattern 2126 ==============\n",
      "============== Pattern 2127 ==============\n",
      "============== Pattern 2128 ==============\n",
      "============== Pattern 2129 ==============\n",
      "============== Pattern 2130 ==============\n",
      "============== Pattern 2131 ==============\n",
      "============== Pattern 2132 ==============\n",
      "============== Pattern 2133 ==============\n",
      "============== Pattern 2134 ==============\n",
      "============== Pattern 2135 ==============\n",
      "============== Pattern 2136 ==============\n",
      "============== Pattern 2137 ==============\n",
      "============== Pattern 2138 ==============\n",
      "============== Pattern 2139 ==============\n",
      "============== Pattern 2140 ==============\n",
      "============== Pattern 2141 ==============\n",
      "============== Pattern 2142 ==============\n",
      "============== Pattern 2143 ==============\n",
      "============== Pattern 2144 ==============\n",
      "============== Pattern 2145 ==============\n",
      "============== Pattern 2146 ==============\n",
      "============== Pattern 2147 ==============\n",
      "============== Pattern 2148 ==============\n",
      "============== Pattern 2149 ==============\n",
      "============== Pattern 2150 ==============\n",
      "============== Pattern 2151 ==============\n",
      "============== Pattern 2152 ==============\n",
      "============== Pattern 2153 ==============\n",
      "============== Pattern 2154 ==============\n",
      "============== Pattern 2155 ==============\n",
      "============== Pattern 2156 ==============\n",
      "============== Pattern 2157 ==============\n",
      "============== Pattern 2158 ==============\n",
      "============== Pattern 2159 ==============\n",
      "============== Pattern 2160 ==============\n",
      "============== Pattern 2161 ==============\n",
      "============== Pattern 2162 ==============\n",
      "============== Pattern 2163 ==============\n",
      "============== Pattern 2164 ==============\n",
      "============== Pattern 2165 ==============\n",
      "============== Pattern 2166 ==============\n",
      "============== Pattern 2167 ==============\n",
      "============== Pattern 2168 ==============\n",
      "============== Pattern 2169 ==============\n",
      "============== Pattern 2170 ==============\n",
      "============== Pattern 2171 ==============\n",
      "============== Pattern 2172 ==============\n",
      "============== Pattern 2173 ==============\n",
      "============== Pattern 2174 ==============\n",
      "============== Pattern 2175 ==============\n",
      "============== Pattern 2176 ==============\n",
      "============== Pattern 2177 ==============\n",
      "============== Pattern 2178 ==============\n",
      "============== Pattern 2179 ==============\n",
      "============== Pattern 2180 ==============\n",
      "============== Pattern 2181 ==============\n",
      "============== Pattern 2182 ==============\n",
      "============== Pattern 2183 ==============\n",
      "============== Pattern 2184 ==============\n",
      "============== Pattern 2185 ==============\n",
      "============== Pattern 2186 ==============\n",
      "============== Pattern 2187 ==============\n",
      "============== Pattern 2188 ==============\n",
      "============== Pattern 2189 ==============\n",
      "============== Pattern 2190 ==============\n",
      "============== Pattern 2191 ==============\n",
      "============== Pattern 2192 ==============\n",
      "============== Pattern 2193 ==============\n",
      "============== Pattern 2194 ==============\n",
      "============== Pattern 2195 ==============\n",
      "============== Pattern 2196 ==============\n",
      "============== Pattern 2197 ==============\n",
      "============== Pattern 2198 ==============\n",
      "============== Pattern 2199 ==============\n",
      "============== Pattern 2200 ==============\n",
      "============== Pattern 2201 ==============\n",
      "============== Pattern 2202 ==============\n",
      "============== Pattern 2203 ==============\n",
      "============== Pattern 2204 ==============\n",
      "============== Pattern 2205 ==============\n",
      "============== Pattern 2206 ==============\n",
      "============== Pattern 2207 ==============\n",
      "============== Pattern 2208 ==============\n",
      "============== Pattern 2209 ==============\n",
      "============== Pattern 2210 ==============\n",
      "============== Pattern 2211 ==============\n",
      "============== Pattern 2212 ==============\n",
      "============== Pattern 2213 ==============\n",
      "============== Pattern 2214 ==============\n",
      "============== Pattern 2215 ==============\n",
      "============== Pattern 2216 ==============\n",
      "============== Pattern 2217 ==============\n",
      "============== Pattern 2218 ==============\n",
      "============== Pattern 2219 ==============\n",
      "============== Pattern 2220 ==============\n",
      "============== Pattern 2221 ==============\n",
      "============== Pattern 2222 ==============\n",
      "============== Pattern 2223 ==============\n",
      "============== Pattern 2224 ==============\n",
      "============== Pattern 2225 ==============\n",
      "============== Pattern 2226 ==============\n",
      "============== Pattern 2227 ==============\n",
      "============== Pattern 2228 ==============\n",
      "============== Pattern 2229 ==============\n",
      "============== Pattern 2230 ==============\n",
      "============== Pattern 2231 ==============\n",
      "============== Pattern 2232 ==============\n",
      "============== Pattern 2233 ==============\n",
      "============== Pattern 2234 ==============\n",
      "============== Pattern 2235 ==============\n",
      "============== Pattern 2236 ==============\n",
      "============== Pattern 2237 ==============\n",
      "============== Pattern 2238 ==============\n",
      "============== Pattern 2239 ==============\n",
      "============== Pattern 2240 ==============\n",
      "============== Pattern 2241 ==============\n",
      "============== Pattern 2242 ==============\n",
      "============== Pattern 2243 ==============\n",
      "============== Pattern 2244 ==============\n",
      "============== Pattern 2245 ==============\n",
      "============== Pattern 2246 ==============\n",
      "============== Pattern 2247 ==============\n",
      "============== Pattern 2248 ==============\n",
      "============== Pattern 2249 ==============\n",
      "============== Pattern 2250 ==============\n",
      "============== Pattern 2251 ==============\n",
      "============== Pattern 2252 ==============\n",
      "============== Pattern 2253 ==============\n",
      "============== Pattern 2254 ==============\n",
      "============== Pattern 2255 ==============\n",
      "============== Pattern 2256 ==============\n",
      "============== Pattern 2257 ==============\n",
      "============== Pattern 2258 ==============\n",
      "============== Pattern 2259 ==============\n",
      "============== Pattern 2260 ==============\n",
      "============== Pattern 2261 ==============\n",
      "============== Pattern 2262 ==============\n",
      "============== Pattern 2263 ==============\n",
      "============== Pattern 2264 ==============\n",
      "============== Pattern 2265 ==============\n",
      "============== Pattern 2266 ==============\n",
      "============== Pattern 2267 ==============\n",
      "============== Pattern 2268 ==============\n",
      "============== Pattern 2269 ==============\n",
      "============== Pattern 2270 ==============\n",
      "============== Pattern 2271 ==============\n",
      "============== Pattern 2272 ==============\n",
      "============== Pattern 2273 ==============\n",
      "============== Pattern 2274 ==============\n",
      "============== Pattern 2275 ==============\n",
      "============== Pattern 2276 ==============\n",
      "============== Pattern 2277 ==============\n",
      "============== Pattern 2278 ==============\n",
      "============== Pattern 2279 ==============\n",
      "============== Pattern 2280 ==============\n",
      "============== Pattern 2281 ==============\n",
      "============== Pattern 2282 ==============\n",
      "============== Pattern 2283 ==============\n",
      "============== Pattern 2284 ==============\n",
      "============== Pattern 2285 ==============\n",
      "============== Pattern 2286 ==============\n",
      "============== Pattern 2287 ==============\n",
      "============== Pattern 2288 ==============\n",
      "============== Pattern 2289 ==============\n",
      "============== Pattern 2290 ==============\n",
      "============== Pattern 2291 ==============\n",
      "============== Pattern 2292 ==============\n",
      "============== Pattern 2293 ==============\n",
      "============== Pattern 2294 ==============\n",
      "============== Pattern 2295 ==============\n",
      "============== Pattern 2296 ==============\n",
      "============== Pattern 2297 ==============\n",
      "============== Pattern 2298 ==============\n",
      "============== Pattern 2299 ==============\n",
      "============== Pattern 2300 ==============\n",
      "============== Pattern 2301 ==============\n",
      "============== Pattern 2302 ==============\n",
      "============== Pattern 2303 ==============\n",
      "============== Pattern 2304 ==============\n",
      "============== Pattern 2305 ==============\n",
      "============== Pattern 2306 ==============\n",
      "============== Pattern 2307 ==============\n",
      "============== Pattern 2308 ==============\n",
      "============== Pattern 2309 ==============\n",
      "============== Pattern 2310 ==============\n",
      "============== Pattern 2311 ==============\n",
      "============== Pattern 2312 ==============\n",
      "============== Pattern 2313 ==============\n",
      "============== Pattern 2314 ==============\n",
      "============== Pattern 2315 ==============\n",
      "============== Pattern 2316 ==============\n",
      "============== Pattern 2317 ==============\n",
      "============== Pattern 2318 ==============\n",
      "============== Pattern 2319 ==============\n",
      "============== Pattern 2320 ==============\n",
      "============== Pattern 2321 ==============\n",
      "============== Pattern 2322 ==============\n",
      "============== Pattern 2323 ==============\n",
      "============== Pattern 2324 ==============\n",
      "============== Pattern 2325 ==============\n",
      "============== Pattern 2326 ==============\n",
      "============== Pattern 2327 ==============\n",
      "============== Pattern 2328 ==============\n",
      "============== Pattern 2329 ==============\n",
      "============== Pattern 2330 ==============\n",
      "============== Pattern 2331 ==============\n",
      "============== Pattern 2332 ==============\n",
      "============== Pattern 2333 ==============\n",
      "============== Pattern 2334 ==============\n",
      "============== Pattern 2335 ==============\n",
      "============== Pattern 2336 ==============\n",
      "============== Pattern 2337 ==============\n",
      "============== Pattern 2338 ==============\n",
      "============== Pattern 2339 ==============\n",
      "============== Pattern 2340 ==============\n",
      "============== Pattern 2341 ==============\n",
      "============== Pattern 2342 ==============\n",
      "============== Pattern 2343 ==============\n",
      "============== Pattern 2344 ==============\n",
      "============== Pattern 2345 ==============\n",
      "============== Pattern 2346 ==============\n",
      "============== Pattern 2347 ==============\n",
      "============== Pattern 2348 ==============\n",
      "============== Pattern 2349 ==============\n",
      "============== Pattern 2350 ==============\n",
      "============== Pattern 2351 ==============\n",
      "============== Pattern 2352 ==============\n",
      "============== Pattern 2353 ==============\n",
      "============== Pattern 2354 ==============\n",
      "============== Pattern 2355 ==============\n",
      "============== Pattern 2356 ==============\n",
      "============== Pattern 2357 ==============\n",
      "============== Pattern 2358 ==============\n",
      "============== Pattern 2359 ==============\n",
      "============== Pattern 2360 ==============\n",
      "============== Pattern 2361 ==============\n",
      "============== Pattern 2362 ==============\n",
      "============== Pattern 2363 ==============\n",
      "============== Pattern 2364 ==============\n",
      "============== Pattern 2365 ==============\n",
      "============== Pattern 2366 ==============\n",
      "============== Pattern 2367 ==============\n",
      "============== Pattern 2368 ==============\n",
      "============== Pattern 2369 ==============\n",
      "============== Pattern 2370 ==============\n",
      "============== Pattern 2371 ==============\n",
      "============== Pattern 2372 ==============\n",
      "============== Pattern 2373 ==============\n",
      "============== Pattern 2374 ==============\n",
      "============== Pattern 2375 ==============\n",
      "============== Pattern 2376 ==============\n",
      "============== Pattern 2377 ==============\n",
      "============== Pattern 2378 ==============\n",
      "============== Pattern 2379 ==============\n",
      "============== Pattern 2380 ==============\n",
      "============== Pattern 2381 ==============\n",
      "============== Pattern 2382 ==============\n",
      "============== Pattern 2383 ==============\n",
      "============== Pattern 2384 ==============\n",
      "============== Pattern 2385 ==============\n",
      "============== Pattern 2386 ==============\n",
      "============== Pattern 2387 ==============\n",
      "============== Pattern 2388 ==============\n",
      "============== Pattern 2389 ==============\n",
      "============== Pattern 2390 ==============\n",
      "============== Pattern 2391 ==============\n",
      "============== Pattern 2392 ==============\n",
      "============== Pattern 2393 ==============\n",
      "============== Pattern 2394 ==============\n",
      "============== Pattern 2395 ==============\n",
      "============== Pattern 2396 ==============\n",
      "============== Pattern 2397 ==============\n",
      "============== Pattern 2398 ==============\n",
      "============== Pattern 2399 ==============\n",
      "============== Pattern 2400 ==============\n",
      "============== Pattern 2401 ==============\n",
      "============== Pattern 2402 ==============\n",
      "============== Pattern 2403 ==============\n",
      "============== Pattern 2404 ==============\n",
      "============== Pattern 2405 ==============\n",
      "============== Pattern 2406 ==============\n",
      "============== Pattern 2407 ==============\n",
      "============== Pattern 2408 ==============\n",
      "============== Pattern 2409 ==============\n",
      "============== Pattern 2410 ==============\n",
      "============== Pattern 2411 ==============\n",
      "============== Pattern 2412 ==============\n",
      "============== Pattern 2413 ==============\n",
      "============== Pattern 2414 ==============\n",
      "============== Pattern 2415 ==============\n",
      "============== Pattern 2416 ==============\n",
      "============== Pattern 2417 ==============\n",
      "============== Pattern 2418 ==============\n",
      "============== Pattern 2419 ==============\n",
      "============== Pattern 2420 ==============\n",
      "============== Pattern 2421 ==============\n",
      "============== Pattern 2422 ==============\n",
      "============== Pattern 2423 ==============\n",
      "============== Pattern 2424 ==============\n",
      "============== Pattern 2425 ==============\n",
      "============== Pattern 2426 ==============\n",
      "============== Pattern 2427 ==============\n",
      "============== Pattern 2428 ==============\n",
      "============== Pattern 2429 ==============\n",
      "============== Pattern 2430 ==============\n",
      "============== Pattern 2431 ==============\n",
      "============== Pattern 2432 ==============\n",
      "============== Pattern 2433 ==============\n",
      "============== Pattern 2434 ==============\n",
      "============== Pattern 2435 ==============\n",
      "============== Pattern 2436 ==============\n",
      "============== Pattern 2437 ==============\n",
      "============== Pattern 2438 ==============\n",
      "============== Pattern 2439 ==============\n",
      "============== Pattern 2440 ==============\n",
      "============== Pattern 2441 ==============\n",
      "============== Pattern 2442 ==============\n",
      "============== Pattern 2443 ==============\n",
      "============== Pattern 2444 ==============\n",
      "============== Pattern 2445 ==============\n",
      "============== Pattern 2446 ==============\n",
      "============== Pattern 2447 ==============\n",
      "============== Pattern 2448 ==============\n",
      "============== Pattern 2449 ==============\n",
      "============== Pattern 2450 ==============\n",
      "============== Pattern 2451 ==============\n",
      "============== Pattern 2452 ==============\n",
      "============== Pattern 2453 ==============\n",
      "============== Pattern 2454 ==============\n",
      "============== Pattern 2455 ==============\n",
      "============== Pattern 2456 ==============\n",
      "============== Pattern 2457 ==============\n",
      "============== Pattern 2458 ==============\n",
      "============== Pattern 2459 ==============\n",
      "============== Pattern 2460 ==============\n",
      "============== Pattern 2461 ==============\n",
      "============== Pattern 2462 ==============\n",
      "============== Pattern 2463 ==============\n",
      "============== Pattern 2464 ==============\n",
      "============== Pattern 2465 ==============\n",
      "============== Pattern 2466 ==============\n",
      "============== Pattern 2467 ==============\n",
      "============== Pattern 2468 ==============\n",
      "============== Pattern 2469 ==============\n",
      "============== Pattern 2470 ==============\n",
      "============== Pattern 2471 ==============\n",
      "============== Pattern 2472 ==============\n",
      "============== Pattern 2473 ==============\n",
      "============== Pattern 2474 ==============\n",
      "============== Pattern 2475 ==============\n",
      "============== Pattern 2476 ==============\n",
      "============== Pattern 2477 ==============\n",
      "============== Pattern 2478 ==============\n",
      "============== Pattern 2479 ==============\n",
      "============== Pattern 2480 ==============\n",
      "============== Pattern 2481 ==============\n",
      "============== Pattern 2482 ==============\n",
      "============== Pattern 2483 ==============\n",
      "============== Pattern 2484 ==============\n",
      "============== Pattern 2485 ==============\n",
      "============== Pattern 2486 ==============\n",
      "============== Pattern 2487 ==============\n",
      "============== Pattern 2488 ==============\n",
      "============== Pattern 2489 ==============\n",
      "============== Pattern 2490 ==============\n",
      "============== Pattern 2491 ==============\n",
      "============== Pattern 2492 ==============\n",
      "============== Pattern 2493 ==============\n",
      "============== Pattern 2494 ==============\n",
      "============== Pattern 2495 ==============\n",
      "============== Pattern 2496 ==============\n",
      "============== Pattern 2497 ==============\n",
      "============== Pattern 2498 ==============\n",
      "============== Pattern 2499 ==============\n",
      "============== Pattern 2500 ==============\n",
      "============== Pattern 2501 ==============\n",
      "============== Pattern 2502 ==============\n",
      "============== Pattern 2503 ==============\n",
      "============== Pattern 2504 ==============\n",
      "============== Pattern 2505 ==============\n",
      "============== Pattern 2506 ==============\n",
      "============== Pattern 2507 ==============\n",
      "============== Pattern 2508 ==============\n",
      "============== Pattern 2509 ==============\n",
      "============== Pattern 2510 ==============\n",
      "============== Pattern 2511 ==============\n",
      "============== Pattern 2512 ==============\n",
      "============== Pattern 2513 ==============\n",
      "============== Pattern 2514 ==============\n",
      "============== Pattern 2515 ==============\n",
      "============== Pattern 2516 ==============\n",
      "============== Pattern 2517 ==============\n",
      "============== Pattern 2518 ==============\n",
      "============== Pattern 2519 ==============\n",
      "============== Pattern 2520 ==============\n",
      "============== Pattern 2521 ==============\n",
      "============== Pattern 2522 ==============\n",
      "============== Pattern 2523 ==============\n",
      "============== Pattern 2524 ==============\n",
      "============== Pattern 2525 ==============\n",
      "============== Pattern 2526 ==============\n",
      "============== Pattern 2527 ==============\n",
      "============== Pattern 2528 ==============\n",
      "============== Pattern 2529 ==============\n",
      "============== Pattern 2530 ==============\n",
      "============== Pattern 2531 ==============\n",
      "============== Pattern 2532 ==============\n",
      "============== Pattern 2533 ==============\n",
      "============== Pattern 2534 ==============\n",
      "============== Pattern 2535 ==============\n",
      "============== Pattern 2536 ==============\n",
      "============== Pattern 2537 ==============\n",
      "============== Pattern 2538 ==============\n",
      "============== Pattern 2539 ==============\n",
      "============== Pattern 2540 ==============\n",
      "============== Pattern 2541 ==============\n",
      "============== Pattern 2542 ==============\n",
      "============== Pattern 2543 ==============\n",
      "============== Pattern 2544 ==============\n",
      "============== Pattern 2545 ==============\n",
      "============== Pattern 2546 ==============\n",
      "============== Pattern 2547 ==============\n",
      "============== Pattern 2548 ==============\n",
      "============== Pattern 2549 ==============\n",
      "============== Pattern 2550 ==============\n",
      "============== Pattern 2551 ==============\n",
      "============== Pattern 2552 ==============\n",
      "============== Pattern 2553 ==============\n",
      "============== Pattern 2554 ==============\n",
      "============== Pattern 2555 ==============\n",
      "============== Pattern 2556 ==============\n",
      "============== Pattern 2557 ==============\n",
      "============== Pattern 2558 ==============\n",
      "============== Pattern 2559 ==============\n",
      "============== Pattern 2560 ==============\n",
      "============== Pattern 2561 ==============\n",
      "============== Pattern 2562 ==============\n",
      "============== Pattern 2563 ==============\n",
      "============== Pattern 2564 ==============\n",
      "============== Pattern 2565 ==============\n",
      "============== Pattern 2566 ==============\n",
      "============== Pattern 2567 ==============\n",
      "============== Pattern 2568 ==============\n",
      "============== Pattern 2569 ==============\n",
      "============== Pattern 2570 ==============\n",
      "============== Pattern 2571 ==============\n",
      "============== Pattern 2572 ==============\n",
      "============== Pattern 2573 ==============\n",
      "============== Pattern 2574 ==============\n",
      "============== Pattern 2575 ==============\n",
      "============== Pattern 2576 ==============\n",
      "============== Pattern 2577 ==============\n",
      "============== Pattern 2578 ==============\n",
      "============== Pattern 2579 ==============\n",
      "============== Pattern 2580 ==============\n",
      "============== Pattern 2581 ==============\n",
      "============== Pattern 2582 ==============\n",
      "============== Pattern 2583 ==============\n",
      "============== Pattern 2584 ==============\n",
      "============== Pattern 2585 ==============\n",
      "============== Pattern 2586 ==============\n",
      "============== Pattern 2587 ==============\n",
      "============== Pattern 2588 ==============\n",
      "============== Pattern 2589 ==============\n",
      "============== Pattern 2590 ==============\n",
      "============== Pattern 2591 ==============\n",
      "============== Pattern 2592 ==============\n",
      "============== Pattern 2593 ==============\n",
      "============== Pattern 2594 ==============\n",
      "============== Pattern 2595 ==============\n",
      "============== Pattern 2596 ==============\n",
      "============== Pattern 2597 ==============\n",
      "============== Pattern 2598 ==============\n",
      "============== Pattern 2599 ==============\n",
      "============== Pattern 2600 ==============\n",
      "============== Pattern 2601 ==============\n",
      "============== Pattern 2602 ==============\n",
      "============== Pattern 2603 ==============\n",
      "============== Pattern 2604 ==============\n",
      "============== Pattern 2605 ==============\n",
      "============== Pattern 2606 ==============\n",
      "============== Pattern 2607 ==============\n",
      "============== Pattern 2608 ==============\n",
      "============== Pattern 2609 ==============\n",
      "============== Pattern 2610 ==============\n",
      "============== Pattern 2611 ==============\n",
      "============== Pattern 2612 ==============\n",
      "============== Pattern 2613 ==============\n",
      "============== Pattern 2614 ==============\n",
      "============== Pattern 2615 ==============\n",
      "============== Pattern 2616 ==============\n",
      "============== Pattern 2617 ==============\n",
      "============== Pattern 2618 ==============\n",
      "============== Pattern 2619 ==============\n",
      "============== Pattern 2620 ==============\n",
      "============== Pattern 2621 ==============\n",
      "============== Pattern 2622 ==============\n",
      "============== Pattern 2623 ==============\n",
      "============== Pattern 2624 ==============\n",
      "============== Pattern 2625 ==============\n",
      "============== Pattern 2626 ==============\n",
      "============== Pattern 2627 ==============\n",
      "============== Pattern 2628 ==============\n",
      "============== Pattern 2629 ==============\n",
      "============== Pattern 2630 ==============\n",
      "============== Pattern 2631 ==============\n",
      "============== Pattern 2632 ==============\n",
      "============== Pattern 2633 ==============\n",
      "============== Pattern 2634 ==============\n",
      "============== Pattern 2635 ==============\n",
      "============== Pattern 2636 ==============\n",
      "============== Pattern 2637 ==============\n",
      "============== Pattern 2638 ==============\n",
      "============== Pattern 2639 ==============\n",
      "============== Pattern 2640 ==============\n",
      "============== Pattern 2641 ==============\n",
      "============== Pattern 2642 ==============\n",
      "============== Pattern 2643 ==============\n",
      "============== Pattern 2644 ==============\n",
      "============== Pattern 2645 ==============\n",
      "============== Pattern 2646 ==============\n",
      "============== Pattern 2647 ==============\n",
      "============== Pattern 2648 ==============\n",
      "============== Pattern 2649 ==============\n",
      "============== Pattern 2650 ==============\n",
      "============== Pattern 2651 ==============\n",
      "============== Pattern 2652 ==============\n",
      "============== Pattern 2653 ==============\n",
      "============== Pattern 2654 ==============\n",
      "============== Pattern 2655 ==============\n",
      "============== Pattern 2656 ==============\n",
      "============== Pattern 2657 ==============\n",
      "============== Pattern 2658 ==============\n",
      "============== Pattern 2659 ==============\n",
      "============== Pattern 2660 ==============\n",
      "============== Pattern 2661 ==============\n",
      "============== Pattern 2662 ==============\n",
      "============== Pattern 2663 ==============\n",
      "============== Pattern 2664 ==============\n",
      "============== Pattern 2665 ==============\n",
      "============== Pattern 2666 ==============\n",
      "============== Pattern 2667 ==============\n",
      "============== Pattern 2668 ==============\n",
      "============== Pattern 2669 ==============\n",
      "============== Pattern 2670 ==============\n",
      "============== Pattern 2671 ==============\n",
      "============== Pattern 2672 ==============\n",
      "============== Pattern 2673 ==============\n",
      "============== Pattern 2674 ==============\n",
      "============== Pattern 2675 ==============\n",
      "============== Pattern 2676 ==============\n",
      "============== Pattern 2677 ==============\n",
      "============== Pattern 2678 ==============\n",
      "============== Pattern 2679 ==============\n",
      "============== Pattern 2680 ==============\n",
      "============== Pattern 2681 ==============\n",
      "============== Pattern 2682 ==============\n",
      "============== Pattern 2683 ==============\n",
      "============== Pattern 2684 ==============\n",
      "============== Pattern 2685 ==============\n",
      "============== Pattern 2686 ==============\n",
      "============== Pattern 2687 ==============\n",
      "============== Pattern 2688 ==============\n",
      "============== Pattern 2689 ==============\n",
      "============== Pattern 2690 ==============\n",
      "============== Pattern 2691 ==============\n",
      "============== Pattern 2692 ==============\n",
      "============== Pattern 2693 ==============\n",
      "============== Pattern 2694 ==============\n",
      "============== Pattern 2695 ==============\n",
      "============== Pattern 2696 ==============\n",
      "============== Pattern 2697 ==============\n",
      "============== Pattern 2698 ==============\n",
      "============== Pattern 2699 ==============\n",
      "============== Pattern 2700 ==============\n",
      "============== Pattern 2701 ==============\n",
      "============== Pattern 2702 ==============\n",
      "============== Pattern 2703 ==============\n",
      "============== Pattern 2704 ==============\n",
      "============== Pattern 2705 ==============\n",
      "============== Pattern 2706 ==============\n",
      "============== Pattern 2707 ==============\n",
      "============== Pattern 2708 ==============\n",
      "============== Pattern 2709 ==============\n",
      "============== Pattern 2710 ==============\n",
      "============== Pattern 2711 ==============\n",
      "============== Pattern 2712 ==============\n",
      "============== Pattern 2713 ==============\n",
      "============== Pattern 2714 ==============\n",
      "============== Pattern 2715 ==============\n",
      "============== Pattern 2716 ==============\n",
      "============== Pattern 2717 ==============\n",
      "============== Pattern 2718 ==============\n",
      "============== Pattern 2719 ==============\n",
      "============== Pattern 2720 ==============\n",
      "============== Pattern 2721 ==============\n",
      "============== Pattern 2722 ==============\n",
      "============== Pattern 2723 ==============\n",
      "============== Pattern 2724 ==============\n",
      "============== Pattern 2725 ==============\n",
      "============== Pattern 2726 ==============\n",
      "============== Pattern 2727 ==============\n",
      "============== Pattern 2728 ==============\n",
      "============== Pattern 2729 ==============\n",
      "============== Pattern 2730 ==============\n",
      "============== Pattern 2731 ==============\n",
      "============== Pattern 2732 ==============\n",
      "============== Pattern 2733 ==============\n",
      "============== Pattern 2734 ==============\n",
      "============== Pattern 2735 ==============\n",
      "============== Pattern 2736 ==============\n",
      "============== Pattern 2737 ==============\n",
      "============== Pattern 2738 ==============\n",
      "============== Pattern 2739 ==============\n",
      "============== Pattern 2740 ==============\n",
      "============== Pattern 2741 ==============\n",
      "============== Pattern 2742 ==============\n",
      "============== Pattern 2743 ==============\n",
      "============== Pattern 2744 ==============\n",
      "============== Pattern 2745 ==============\n",
      "============== Pattern 2746 ==============\n",
      "============== Pattern 2747 ==============\n",
      "============== Pattern 2748 ==============\n",
      "============== Pattern 2749 ==============\n",
      "============== Pattern 2750 ==============\n",
      "============== Pattern 2751 ==============\n",
      "============== Pattern 2752 ==============\n",
      "============== Pattern 2753 ==============\n",
      "============== Pattern 2754 ==============\n",
      "============== Pattern 2755 ==============\n",
      "============== Pattern 2756 ==============\n",
      "============== Pattern 2757 ==============\n",
      "============== Pattern 2758 ==============\n",
      "============== Pattern 2759 ==============\n",
      "============== Pattern 2760 ==============\n",
      "============== Pattern 2761 ==============\n",
      "============== Pattern 2762 ==============\n",
      "============== Pattern 2763 ==============\n",
      "============== Pattern 2764 ==============\n",
      "============== Pattern 2765 ==============\n",
      "============== Pattern 2766 ==============\n",
      "============== Pattern 2767 ==============\n",
      "============== Pattern 2768 ==============\n",
      "============== Pattern 2769 ==============\n",
      "============== Pattern 2770 ==============\n",
      "============== Pattern 2771 ==============\n",
      "============== Pattern 2772 ==============\n",
      "============== Pattern 2773 ==============\n",
      "============== Pattern 2774 ==============\n",
      "============== Pattern 2775 ==============\n",
      "============== Pattern 2776 ==============\n",
      "============== Pattern 2777 ==============\n",
      "============== Pattern 2778 ==============\n",
      "============== Pattern 2779 ==============\n",
      "============== Pattern 2780 ==============\n",
      "============== Pattern 2781 ==============\n",
      "============== Pattern 2782 ==============\n",
      "============== Pattern 2783 ==============\n",
      "============== Pattern 2784 ==============\n",
      "============== Pattern 2785 ==============\n",
      "============== Pattern 2786 ==============\n",
      "============== Pattern 2787 ==============\n",
      "============== Pattern 2788 ==============\n",
      "============== Pattern 2789 ==============\n",
      "============== Pattern 2790 ==============\n",
      "============== Pattern 2791 ==============\n",
      "============== Pattern 2792 ==============\n",
      "============== Pattern 2793 ==============\n",
      "============== Pattern 2794 ==============\n",
      "============== Pattern 2795 ==============\n",
      "============== Pattern 2796 ==============\n",
      "============== Pattern 2797 ==============\n",
      "============== Pattern 2798 ==============\n",
      "============== Pattern 2799 ==============\n",
      "============== Pattern 2800 ==============\n",
      "============== Pattern 2801 ==============\n",
      "============== Pattern 2802 ==============\n",
      "============== Pattern 2803 ==============\n",
      "============== Pattern 2804 ==============\n",
      "============== Pattern 2805 ==============\n",
      "============== Pattern 2806 ==============\n",
      "============== Pattern 2807 ==============\n",
      "============== Pattern 2808 ==============\n",
      "============== Pattern 2809 ==============\n",
      "============== Pattern 2810 ==============\n",
      "============== Pattern 2811 ==============\n",
      "============== Pattern 2812 ==============\n",
      "============== Pattern 2813 ==============\n",
      "============== Pattern 2814 ==============\n",
      "============== Pattern 2815 ==============\n",
      "============== Pattern 2816 ==============\n",
      "============== Pattern 2817 ==============\n",
      "============== Pattern 2818 ==============\n",
      "============== Pattern 2819 ==============\n",
      "============== Pattern 2820 ==============\n",
      "============== Pattern 2821 ==============\n",
      "============== Pattern 2822 ==============\n",
      "============== Pattern 2823 ==============\n",
      "============== Pattern 2824 ==============\n",
      "============== Pattern 2825 ==============\n",
      "============== Pattern 2826 ==============\n",
      "============== Pattern 2827 ==============\n",
      "============== Pattern 2828 ==============\n",
      "============== Pattern 2829 ==============\n",
      "============== Pattern 2830 ==============\n",
      "============== Pattern 2831 ==============\n",
      "============== Pattern 2832 ==============\n",
      "============== Pattern 2833 ==============\n",
      "============== Pattern 2834 ==============\n",
      "============== Pattern 2835 ==============\n",
      "============== Pattern 2836 ==============\n",
      "============== Pattern 2837 ==============\n",
      "============== Pattern 2838 ==============\n",
      "============== Pattern 2839 ==============\n",
      "============== Pattern 2840 ==============\n",
      "============== Pattern 2841 ==============\n",
      "============== Pattern 2842 ==============\n",
      "============== Pattern 2843 ==============\n",
      "============== Pattern 2844 ==============\n",
      "============== Pattern 2845 ==============\n",
      "============== Pattern 2846 ==============\n",
      "============== Pattern 2847 ==============\n",
      "============== Pattern 2848 ==============\n",
      "============== Pattern 2849 ==============\n",
      "============== Pattern 2850 ==============\n",
      "============== Pattern 2851 ==============\n",
      "============== Pattern 2852 ==============\n",
      "============== Pattern 2853 ==============\n",
      "============== Pattern 2854 ==============\n",
      "============== Pattern 2855 ==============\n",
      "============== Pattern 2856 ==============\n",
      "============== Pattern 2857 ==============\n",
      "============== Pattern 2858 ==============\n",
      "============== Pattern 2859 ==============\n",
      "============== Pattern 2860 ==============\n",
      "============== Pattern 2861 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2862 ==============\n",
      "============== Pattern 2863 ==============\n",
      "============== Pattern 2864 ==============\n",
      "============== Pattern 2865 ==============\n",
      "============== Pattern 2866 ==============\n",
      "============== Pattern 2867 ==============\n",
      "============== Pattern 2868 ==============\n",
      "============== Pattern 2869 ==============\n",
      "============== Pattern 2870 ==============\n",
      "============== Pattern 2871 ==============\n",
      "============== Pattern 2872 ==============\n",
      "============== Pattern 2873 ==============\n",
      "============== Pattern 2874 ==============\n",
      "============== Pattern 2875 ==============\n",
      "============== Pattern 2876 ==============\n",
      "============== Pattern 2877 ==============\n",
      "============== Pattern 2878 ==============\n",
      "============== Pattern 2879 ==============\n",
      "============== Pattern 2880 ==============\n",
      "============== Pattern 2881 ==============\n",
      "============== Pattern 2882 ==============\n",
      "============== Pattern 2883 ==============\n",
      "============== Pattern 2884 ==============\n",
      "============== Pattern 2885 ==============\n",
      "============== Pattern 2886 ==============\n",
      "============== Pattern 2887 ==============\n",
      "============== Pattern 2888 ==============\n",
      "============== Pattern 2889 ==============\n",
      "============== Pattern 2890 ==============\n",
      "============== Pattern 2891 ==============\n",
      "============== Pattern 2892 ==============\n",
      "============== Pattern 2893 ==============\n",
      "============== Pattern 2894 ==============\n",
      "============== Pattern 2895 ==============\n",
      "============== Pattern 2896 ==============\n",
      "============== Pattern 2897 ==============\n",
      "============== Pattern 2898 ==============\n",
      "============== Pattern 2899 ==============\n",
      "============== Pattern 2900 ==============\n",
      "============== Pattern 2901 ==============\n",
      "============== Pattern 2902 ==============\n",
      "============== Pattern 2903 ==============\n",
      "============== Pattern 2904 ==============\n",
      "============== Pattern 2905 ==============\n",
      "============== Pattern 2906 ==============\n",
      "============== Pattern 2907 ==============\n",
      "============== Pattern 2908 ==============\n",
      "============== Pattern 2909 ==============\n",
      "============== Pattern 2910 ==============\n",
      "============== Pattern 2911 ==============\n",
      "============== Pattern 2912 ==============\n",
      "============== Pattern 2913 ==============\n",
      "============== Pattern 2914 ==============\n",
      "============== Pattern 2915 ==============\n",
      "============== Pattern 2916 ==============\n",
      "============== Pattern 2917 ==============\n",
      "============== Pattern 2918 ==============\n",
      "============== Pattern 2919 ==============\n",
      "============== Pattern 2920 ==============\n",
      "============== Pattern 2921 ==============\n",
      "============== Pattern 2922 ==============\n",
      "============== Pattern 2923 ==============\n",
      "============== Pattern 2924 ==============\n",
      "============== Pattern 2925 ==============\n",
      "============== Pattern 2926 ==============\n",
      "============== Pattern 2927 ==============\n",
      "============== Pattern 2928 ==============\n",
      "============== Pattern 2929 ==============\n",
      "============== Pattern 2930 ==============\n",
      "============== Pattern 2931 ==============\n",
      "============== Pattern 2932 ==============\n",
      "============== Pattern 2933 ==============\n",
      "============== Pattern 2934 ==============\n",
      "============== Pattern 2935 ==============\n",
      "============== Pattern 2936 ==============\n",
      "============== Pattern 2937 ==============\n",
      "============== Pattern 2938 ==============\n",
      "============== Pattern 2939 ==============\n",
      "============== Pattern 2940 ==============\n",
      "============== Pattern 2941 ==============\n",
      "============== Pattern 2942 ==============\n",
      "============== Pattern 2943 ==============\n",
      "============== Pattern 2944 ==============\n",
      "============== Pattern 2945 ==============\n",
      "============== Pattern 2946 ==============\n",
      "============== Pattern 2947 ==============\n",
      "============== Pattern 2948 ==============\n",
      "============== Pattern 2949 ==============\n",
      "============== Pattern 2950 ==============\n",
      "============== Pattern 2951 ==============\n",
      "============== Pattern 2952 ==============\n",
      "============== Pattern 2953 ==============\n",
      "============== Pattern 2954 ==============\n",
      "============== Pattern 2955 ==============\n",
      "============== Pattern 2956 ==============\n",
      "============== Pattern 2957 ==============\n",
      "============== Pattern 2958 ==============\n",
      "============== Pattern 2959 ==============\n",
      "============== Pattern 2960 ==============\n",
      "============== Pattern 2961 ==============\n",
      "============== Pattern 2962 ==============\n",
      "============== Pattern 2963 ==============\n",
      "============== Pattern 2964 ==============\n",
      "============== Pattern 2965 ==============\n",
      "============== Pattern 2966 ==============\n",
      "============== Pattern 2967 ==============\n",
      "============== Pattern 2968 ==============\n",
      "============== Pattern 2969 ==============\n",
      "============== Pattern 2970 ==============\n",
      "============== Pattern 2971 ==============\n",
      "============== Pattern 2972 ==============\n",
      "============== Pattern 2973 ==============\n",
      "============== Pattern 2974 ==============\n",
      "============== Pattern 2975 ==============\n",
      "============== Pattern 2976 ==============\n",
      "============== Pattern 2977 ==============\n",
      "============== Pattern 2978 ==============\n",
      "============== Pattern 2979 ==============\n",
      "============== Pattern 2980 ==============\n",
      "============== Pattern 2981 ==============\n",
      "============== Pattern 2982 ==============\n",
      "============== Pattern 2983 ==============\n",
      "============== Pattern 2984 ==============\n",
      "============== Pattern 2985 ==============\n",
      "============== Pattern 2986 ==============\n",
      "============== Pattern 2987 ==============\n",
      "============== Pattern 2988 ==============\n",
      "============== Pattern 2989 ==============\n",
      "============== Pattern 2990 ==============\n",
      "============== Pattern 2991 ==============\n",
      "============== Pattern 2992 ==============\n",
      "============== Pattern 2993 ==============\n",
      "============== Pattern 2994 ==============\n",
      "============== Pattern 2995 ==============\n",
      "============== Pattern 2996 ==============\n",
      "============== Pattern 2997 ==============\n",
      "============== Pattern 2998 ==============\n",
      "============== Pattern 2999 ==============\n",
      "============== Pattern 3000 ==============\n",
      "============== Pattern 3001 ==============\n",
      "============== Pattern 3002 ==============\n",
      "============== Pattern 3003 ==============\n",
      "============== Pattern 3004 ==============\n",
      "============== Pattern 3005 ==============\n",
      "============== Pattern 3006 ==============\n",
      "============== Pattern 3007 ==============\n",
      "============== Pattern 3008 ==============\n",
      "============== Pattern 3009 ==============\n",
      "============== Pattern 3010 ==============\n",
      "============== Pattern 3011 ==============\n",
      "============== Pattern 3012 ==============\n",
      "============== Pattern 3013 ==============\n",
      "============== Pattern 3014 ==============\n",
      "============== Pattern 3015 ==============\n",
      "============== Pattern 3016 ==============\n",
      "============== Pattern 3017 ==============\n",
      "============== Pattern 3018 ==============\n",
      "============== Pattern 3019 ==============\n",
      "============== Pattern 3020 ==============\n",
      "============== Pattern 3021 ==============\n",
      "============== Pattern 3022 ==============\n",
      "============== Pattern 3023 ==============\n",
      "============== Pattern 3024 ==============\n",
      "============== Pattern 3025 ==============\n",
      "============== Pattern 3026 ==============\n",
      "============== Pattern 3027 ==============\n",
      "============== Pattern 3028 ==============\n",
      "============== Pattern 3029 ==============\n",
      "============== Pattern 3030 ==============\n",
      "============== Pattern 3031 ==============\n",
      "============== Pattern 3032 ==============\n",
      "============== Pattern 3033 ==============\n",
      "============== Pattern 3034 ==============\n",
      "============== Pattern 3035 ==============\n",
      "============== Pattern 3036 ==============\n",
      "============== Pattern 3037 ==============\n",
      "============== Pattern 3038 ==============\n",
      "============== Pattern 3039 ==============\n",
      "============== Pattern 3040 ==============\n",
      "============== Pattern 3041 ==============\n",
      "============== Pattern 3042 ==============\n",
      "============== Pattern 3043 ==============\n",
      "============== Pattern 3044 ==============\n",
      "============== Pattern 3045 ==============\n",
      "============== Pattern 3046 ==============\n",
      "============== Pattern 3047 ==============\n",
      "============== Pattern 3048 ==============\n",
      "============== Pattern 3049 ==============\n",
      "============== Pattern 3050 ==============\n",
      "============== Pattern 3051 ==============\n",
      "============== Pattern 3052 ==============\n",
      "============== Pattern 3053 ==============\n",
      "============== Pattern 3054 ==============\n",
      "============== Pattern 3055 ==============\n",
      "============== Pattern 3056 ==============\n",
      "============== Pattern 3057 ==============\n",
      "============== Pattern 3058 ==============\n",
      "============== Pattern 3059 ==============\n",
      "============== Pattern 3060 ==============\n",
      "============== Pattern 3061 ==============\n",
      "============== Pattern 3062 ==============\n",
      "============== Pattern 3063 ==============\n",
      "============== Pattern 3064 ==============\n",
      "============== Pattern 3065 ==============\n",
      "============== Pattern 3066 ==============\n",
      "============== Pattern 3067 ==============\n",
      "============== Pattern 3068 ==============\n",
      "============== Pattern 3069 ==============\n",
      "============== Pattern 3070 ==============\n",
      "============== Pattern 3071 ==============\n",
      "============== Pattern 3072 ==============\n",
      "============== Pattern 3073 ==============\n",
      "============== Pattern 3074 ==============\n",
      "============== Pattern 3075 ==============\n",
      "============== Pattern 3076 ==============\n",
      "============== Pattern 3077 ==============\n",
      "============== Pattern 3078 ==============\n",
      "============== Pattern 3079 ==============\n",
      "============== Pattern 3080 ==============\n",
      "============== Pattern 3081 ==============\n",
      "============== Pattern 3082 ==============\n",
      "============== Pattern 3083 ==============\n",
      "============== Pattern 3084 ==============\n",
      "============== Pattern 3085 ==============\n",
      "============== Pattern 3086 ==============\n",
      "============== Pattern 3087 ==============\n",
      "============== Pattern 3088 ==============\n",
      "============== Pattern 3089 ==============\n",
      "============== Pattern 3090 ==============\n",
      "============== Pattern 3091 ==============\n",
      "============== Pattern 3092 ==============\n",
      "============== Pattern 3093 ==============\n",
      "============== Pattern 3094 ==============\n",
      "============== Pattern 3095 ==============\n",
      "============== Pattern 3096 ==============\n",
      "============== Pattern 3097 ==============\n",
      "============== Pattern 3098 ==============\n",
      "============== Pattern 3099 ==============\n",
      "============== Pattern 3100 ==============\n",
      "============== Pattern 3101 ==============\n",
      "============== Pattern 3102 ==============\n",
      "============== Pattern 3103 ==============\n",
      "============== Pattern 3104 ==============\n",
      "============== Pattern 3105 ==============\n",
      "============== Pattern 3106 ==============\n",
      "============== Pattern 3107 ==============\n",
      "============== Pattern 3108 ==============\n",
      "============== Pattern 3109 ==============\n",
      "============== Pattern 3110 ==============\n",
      "============== Pattern 3111 ==============\n",
      "============== Pattern 3112 ==============\n",
      "============== Pattern 3113 ==============\n",
      "============== Pattern 3114 ==============\n",
      "============== Pattern 3115 ==============\n",
      "============== Pattern 3116 ==============\n",
      "============== Pattern 3117 ==============\n",
      "============== Pattern 3118 ==============\n",
      "============== Pattern 3119 ==============\n",
      "============== Pattern 3120 ==============\n",
      "============== Pattern 3121 ==============\n",
      "============== Pattern 3122 ==============\n",
      "============== Pattern 3123 ==============\n",
      "============== Pattern 3124 ==============\n",
      "============== Pattern 3125 ==============\n",
      "============== Pattern 3126 ==============\n",
      "============== Pattern 3127 ==============\n",
      "============== Pattern 3128 ==============\n",
      "============== Pattern 3129 ==============\n",
      "============== Pattern 3130 ==============\n",
      "============== Pattern 3131 ==============\n",
      "============== Pattern 3132 ==============\n",
      "============== Pattern 3133 ==============\n",
      "============== Pattern 3134 ==============\n",
      "============== Pattern 3135 ==============\n",
      "============== Pattern 3136 ==============\n",
      "============== Pattern 3137 ==============\n",
      "============== Pattern 3138 ==============\n",
      "============== Pattern 3139 ==============\n",
      "============== Pattern 3140 ==============\n",
      "============== Pattern 3141 ==============\n",
      "============== Pattern 3142 ==============\n",
      "============== Pattern 3143 ==============\n",
      "============== Pattern 3144 ==============\n",
      "============== Pattern 3145 ==============\n",
      "============== Pattern 3146 ==============\n",
      "============== Pattern 3147 ==============\n",
      "============== Pattern 3148 ==============\n",
      "============== Pattern 3149 ==============\n",
      "============== Pattern 3150 ==============\n",
      "============== Pattern 3151 ==============\n",
      "============== Pattern 3152 ==============\n",
      "============== Pattern 3153 ==============\n",
      "============== Pattern 3154 ==============\n",
      "============== Pattern 3155 ==============\n",
      "============== Pattern 3156 ==============\n",
      "============== Pattern 3157 ==============\n",
      "============== Pattern 3158 ==============\n",
      "============== Pattern 3159 ==============\n",
      "============== Pattern 3160 ==============\n",
      "============== Pattern 3161 ==============\n",
      "============== Pattern 3162 ==============\n",
      "============== Pattern 3163 ==============\n",
      "============== Pattern 3164 ==============\n",
      "============== Pattern 3165 ==============\n",
      "============== Pattern 3166 ==============\n",
      "============== Pattern 3167 ==============\n",
      "============== Pattern 3168 ==============\n",
      "============== Pattern 3169 ==============\n",
      "============== Pattern 3170 ==============\n",
      "============== Pattern 3171 ==============\n",
      "============== Pattern 3172 ==============\n",
      "============== Pattern 3173 ==============\n",
      "============== Pattern 3174 ==============\n",
      "============== Pattern 3175 ==============\n",
      "============== Pattern 3176 ==============\n",
      "============== Pattern 3177 ==============\n",
      "============== Pattern 3178 ==============\n",
      "============== Pattern 3179 ==============\n",
      "============== Pattern 3180 ==============\n",
      "============== Pattern 3181 ==============\n",
      "============== Pattern 3182 ==============\n",
      "============== Pattern 3183 ==============\n",
      "============== Pattern 3184 ==============\n",
      "============== Pattern 3185 ==============\n",
      "============== Pattern 3186 ==============\n",
      "============== Pattern 3187 ==============\n",
      "============== Pattern 3188 ==============\n",
      "============== Pattern 3189 ==============\n",
      "============== Pattern 3190 ==============\n",
      "============== Pattern 3191 ==============\n",
      "============== Pattern 3192 ==============\n",
      "============== Pattern 3193 ==============\n",
      "============== Pattern 3194 ==============\n",
      "============== Pattern 3195 ==============\n",
      "============== Pattern 3196 ==============\n",
      "============== Pattern 3197 ==============\n",
      "============== Pattern 3198 ==============\n",
      "============== Pattern 3199 ==============\n",
      "============== Pattern 3200 ==============\n",
      "============== Pattern 3201 ==============\n",
      "============== Pattern 3202 ==============\n",
      "============== Pattern 3203 ==============\n",
      "============== Pattern 3204 ==============\n",
      "============== Pattern 3205 ==============\n",
      "============== Pattern 3206 ==============\n",
      "============== Pattern 3207 ==============\n",
      "============== Pattern 3208 ==============\n",
      "============== Pattern 3209 ==============\n",
      "============== Pattern 3210 ==============\n",
      "============== Pattern 3211 ==============\n",
      "============== Pattern 3212 ==============\n",
      "============== Pattern 3213 ==============\n",
      "============== Pattern 3214 ==============\n",
      "============== Pattern 3215 ==============\n",
      "============== Pattern 3216 ==============\n",
      "============== Pattern 3217 ==============\n",
      "============== Pattern 3218 ==============\n",
      "============== Pattern 3219 ==============\n",
      "============== Pattern 3220 ==============\n",
      "============== Pattern 3221 ==============\n",
      "============== Pattern 3222 ==============\n",
      "============== Pattern 3223 ==============\n",
      "============== Pattern 3224 ==============\n",
      "============== Pattern 3225 ==============\n",
      "============== Pattern 3226 ==============\n",
      "============== Pattern 3227 ==============\n",
      "============== Pattern 3228 ==============\n",
      "============== Pattern 3229 ==============\n",
      "============== Pattern 3230 ==============\n",
      "============== Pattern 3231 ==============\n",
      "============== Pattern 3232 ==============\n",
      "============== Pattern 3233 ==============\n",
      "============== Pattern 3234 ==============\n",
      "============== Pattern 3235 ==============\n",
      "============== Pattern 3236 ==============\n",
      "============== Pattern 3237 ==============\n",
      "============== Pattern 3238 ==============\n",
      "============== Pattern 3239 ==============\n",
      "============== Pattern 3240 ==============\n",
      "============== Pattern 3241 ==============\n",
      "============== Pattern 3242 ==============\n",
      "============== Pattern 3243 ==============\n",
      "============== Pattern 3244 ==============\n",
      "============== Pattern 3245 ==============\n",
      "============== Pattern 3246 ==============\n",
      "============== Pattern 3247 ==============\n",
      "============== Pattern 3248 ==============\n",
      "============== Pattern 3249 ==============\n",
      "============== Pattern 3250 ==============\n",
      "============== Pattern 3251 ==============\n",
      "============== Pattern 3252 ==============\n",
      "============== Pattern 3253 ==============\n",
      "============== Pattern 3254 ==============\n",
      "============== Pattern 3255 ==============\n",
      "============== Pattern 3256 ==============\n",
      "============== Pattern 3257 ==============\n",
      "============== Pattern 3258 ==============\n",
      "============== Pattern 3259 ==============\n",
      "============== Pattern 3260 ==============\n",
      "============== Pattern 3261 ==============\n",
      "============== Pattern 3262 ==============\n",
      "============== Pattern 3263 ==============\n",
      "============== Pattern 3264 ==============\n",
      "============== Pattern 3265 ==============\n",
      "============== Pattern 3266 ==============\n",
      "============== Pattern 3267 ==============\n",
      "============== Pattern 3268 ==============\n",
      "============== Pattern 3269 ==============\n",
      "============== Pattern 3270 ==============\n",
      "============== Pattern 3271 ==============\n",
      "============== Pattern 3272 ==============\n",
      "============== Pattern 3273 ==============\n",
      "============== Pattern 3274 ==============\n",
      "============== Pattern 3275 ==============\n",
      "============== Pattern 3276 ==============\n",
      "============== Pattern 3277 ==============\n",
      "============== Pattern 3278 ==============\n",
      "============== Pattern 3279 ==============\n",
      "============== Pattern 3280 ==============\n",
      "============== Pattern 3281 ==============\n",
      "============== Pattern 3282 ==============\n",
      "============== Pattern 3283 ==============\n",
      "============== Pattern 3284 ==============\n",
      "============== Pattern 3285 ==============\n",
      "============== Pattern 3286 ==============\n",
      "============== Pattern 3287 ==============\n",
      "============== Pattern 3288 ==============\n",
      "============== Pattern 3289 ==============\n",
      "============== Pattern 3290 ==============\n",
      "============== Pattern 3291 ==============\n",
      "============== Pattern 3292 ==============\n",
      "============== Pattern 3293 ==============\n",
      "============== Pattern 3294 ==============\n",
      "============== Pattern 3295 ==============\n",
      "============== Pattern 3296 ==============\n",
      "============== Pattern 3297 ==============\n",
      "============== Pattern 3298 ==============\n",
      "============== Pattern 3299 ==============\n",
      "============== Pattern 3300 ==============\n",
      "============== Pattern 3301 ==============\n",
      "============== Pattern 3302 ==============\n",
      "============== Pattern 3303 ==============\n",
      "============== Pattern 3304 ==============\n",
      "============== Pattern 3305 ==============\n",
      "============== Pattern 3306 ==============\n",
      "============== Pattern 3307 ==============\n",
      "============== Pattern 3308 ==============\n",
      "============== Pattern 3309 ==============\n",
      "============== Pattern 3310 ==============\n",
      "============== Pattern 3311 ==============\n",
      "============== Pattern 3312 ==============\n",
      "============== Pattern 3313 ==============\n",
      "============== Pattern 3314 ==============\n",
      "============== Pattern 3315 ==============\n",
      "============== Pattern 3316 ==============\n",
      "============== Pattern 3317 ==============\n",
      "============== Pattern 3318 ==============\n",
      "============== Pattern 3319 ==============\n",
      "============== Pattern 3320 ==============\n",
      "============== Pattern 3321 ==============\n",
      "============== Pattern 3322 ==============\n",
      "============== Pattern 3323 ==============\n",
      "============== Pattern 3324 ==============\n",
      "============== Pattern 3325 ==============\n",
      "============== Pattern 3326 ==============\n",
      "============== Pattern 3327 ==============\n",
      "============== Pattern 3328 ==============\n",
      "============== Pattern 3329 ==============\n",
      "============== Pattern 3330 ==============\n",
      "============== Pattern 3331 ==============\n",
      "============== Pattern 3332 ==============\n",
      "============== Pattern 3333 ==============\n",
      "============== Pattern 3334 ==============\n",
      "============== Pattern 3335 ==============\n",
      "============== Pattern 3336 ==============\n",
      "============== Pattern 3337 ==============\n",
      "============== Pattern 3338 ==============\n",
      "============== Pattern 3339 ==============\n",
      "============== Pattern 3340 ==============\n",
      "============== Pattern 3341 ==============\n",
      "============== Pattern 3342 ==============\n",
      "============== Pattern 3343 ==============\n",
      "============== Pattern 3344 ==============\n",
      "============== Pattern 3345 ==============\n",
      "============== Pattern 3346 ==============\n",
      "============== Pattern 3347 ==============\n",
      "============== Pattern 3348 ==============\n",
      "============== Pattern 3349 ==============\n",
      "============== Pattern 3350 ==============\n",
      "============== Pattern 3351 ==============\n",
      "============== Pattern 3352 ==============\n",
      "============== Pattern 3353 ==============\n",
      "============== Pattern 3354 ==============\n",
      "============== Pattern 3355 ==============\n",
      "============== Pattern 3356 ==============\n",
      "============== Pattern 3357 ==============\n",
      "============== Pattern 3358 ==============\n",
      "============== Pattern 3359 ==============\n",
      "============== Pattern 3360 ==============\n",
      "============== Pattern 3361 ==============\n",
      "============== Pattern 3362 ==============\n",
      "============== Pattern 3363 ==============\n",
      "============== Pattern 3364 ==============\n",
      "============== Pattern 3365 ==============\n",
      "============== Pattern 3366 ==============\n",
      "============== Pattern 3367 ==============\n",
      "============== Pattern 3368 ==============\n",
      "============== Pattern 3369 ==============\n",
      "============== Pattern 3370 ==============\n",
      "============== Pattern 3371 ==============\n",
      "============== Pattern 3372 ==============\n",
      "============== Pattern 3373 ==============\n",
      "============== Pattern 3374 ==============\n",
      "============== Pattern 3375 ==============\n",
      "============== Pattern 3376 ==============\n",
      "============== Pattern 3377 ==============\n",
      "============== Pattern 3378 ==============\n",
      "============== Pattern 3379 ==============\n",
      "============== Pattern 3380 ==============\n",
      "============== Pattern 3381 ==============\n",
      "============== Pattern 3382 ==============\n",
      "============== Pattern 3383 ==============\n",
      "============== Pattern 3384 ==============\n",
      "============== Pattern 3385 ==============\n",
      "============== Pattern 3386 ==============\n",
      "============== Pattern 3387 ==============\n",
      "============== Pattern 3388 ==============\n",
      "============== Pattern 3389 ==============\n",
      "============== Pattern 3390 ==============\n",
      "============== Pattern 3391 ==============\n",
      "============== Pattern 3392 ==============\n",
      "============== Pattern 3393 ==============\n",
      "============== Pattern 3394 ==============\n",
      "============== Pattern 3395 ==============\n",
      "============== Pattern 3396 ==============\n",
      "============== Pattern 3397 ==============\n",
      "============== Pattern 3398 ==============\n",
      "============== Pattern 3399 ==============\n",
      "============== Pattern 3400 ==============\n",
      "============== Pattern 3401 ==============\n",
      "============== Pattern 3402 ==============\n",
      "============== Pattern 3403 ==============\n",
      "============== Pattern 3404 ==============\n",
      "============== Pattern 3405 ==============\n",
      "============== Pattern 3406 ==============\n",
      "============== Pattern 3407 ==============\n",
      "============== Pattern 3408 ==============\n",
      "============== Pattern 3409 ==============\n",
      "============== Pattern 3410 ==============\n",
      "============== Pattern 3411 ==============\n",
      "============== Pattern 3412 ==============\n",
      "============== Pattern 3413 ==============\n",
      "============== Pattern 3414 ==============\n",
      "============== Pattern 3415 ==============\n",
      "============== Pattern 3416 ==============\n",
      "============== Pattern 3417 ==============\n",
      "============== Pattern 3418 ==============\n",
      "============== Pattern 3419 ==============\n",
      "============== Pattern 3420 ==============\n",
      "============== Pattern 3421 ==============\n",
      "============== Pattern 3422 ==============\n",
      "============== Pattern 3423 ==============\n",
      "============== Pattern 3424 ==============\n",
      "============== Pattern 3425 ==============\n",
      "============== Pattern 3426 ==============\n",
      "============== Pattern 3427 ==============\n",
      "============== Pattern 3428 ==============\n",
      "============== Pattern 3429 ==============\n",
      "============== Pattern 3430 ==============\n",
      "============== Pattern 3431 ==============\n",
      "============== Pattern 3432 ==============\n",
      "============== Pattern 3433 ==============\n",
      "============== Pattern 3434 ==============\n",
      "============== Pattern 3435 ==============\n",
      "============== Pattern 3436 ==============\n",
      "============== Pattern 3437 ==============\n",
      "============== Pattern 3438 ==============\n",
      "============== Pattern 3439 ==============\n",
      "============== Pattern 3440 ==============\n",
      "============== Pattern 3441 ==============\n",
      "============== Pattern 3442 ==============\n",
      "============== Pattern 3443 ==============\n",
      "============== Pattern 3444 ==============\n",
      "============== Pattern 3445 ==============\n",
      "============== Pattern 3446 ==============\n",
      "============== Pattern 3447 ==============\n",
      "============== Pattern 3448 ==============\n",
      "============== Pattern 3449 ==============\n",
      "============== Pattern 3450 ==============\n",
      "============== Pattern 3451 ==============\n",
      "============== Pattern 3452 ==============\n",
      "============== Pattern 3453 ==============\n",
      "============== Pattern 3454 ==============\n",
      "============== Pattern 3455 ==============\n",
      "============== Pattern 3456 ==============\n",
      "============== Pattern 3457 ==============\n",
      "============== Pattern 3458 ==============\n",
      "============== Pattern 3459 ==============\n",
      "============== Pattern 3460 ==============\n",
      "============== Pattern 3461 ==============\n",
      "============== Pattern 3462 ==============\n",
      "============== Pattern 3463 ==============\n",
      "============== Pattern 3464 ==============\n",
      "============== Pattern 3465 ==============\n",
      "============== Pattern 3466 ==============\n",
      "============== Pattern 3467 ==============\n",
      "============== Pattern 3468 ==============\n",
      "============== Pattern 3469 ==============\n",
      "============== Pattern 3470 ==============\n",
      "============== Pattern 3471 ==============\n",
      "============== Pattern 3472 ==============\n",
      "============== Pattern 3473 ==============\n",
      "============== Pattern 3474 ==============\n",
      "============== Pattern 3475 ==============\n",
      "============== Pattern 3476 ==============\n",
      "============== Pattern 3477 ==============\n",
      "============== Pattern 3478 ==============\n",
      "============== Pattern 3479 ==============\n",
      "============== Pattern 3480 ==============\n",
      "============== Pattern 3481 ==============\n",
      "============== Pattern 3482 ==============\n",
      "============== Pattern 3483 ==============\n",
      "============== Pattern 3484 ==============\n",
      "============== Pattern 3485 ==============\n",
      "============== Pattern 3486 ==============\n",
      "============== Pattern 3487 ==============\n",
      "============== Pattern 3488 ==============\n",
      "============== Pattern 3489 ==============\n",
      "============== Pattern 3490 ==============\n",
      "============== Pattern 3491 ==============\n",
      "============== Pattern 3492 ==============\n",
      "============== Pattern 3493 ==============\n",
      "============== Pattern 3494 ==============\n",
      "============== Pattern 3495 ==============\n",
      "============== Pattern 3496 ==============\n",
      "============== Pattern 3497 ==============\n",
      "============== Pattern 3498 ==============\n",
      "============== Pattern 3499 ==============\n",
      "============== Pattern 3500 ==============\n",
      "============== Pattern 3501 ==============\n",
      "============== Pattern 3502 ==============\n",
      "============== Pattern 3503 ==============\n",
      "============== Pattern 3504 ==============\n",
      "============== Pattern 3505 ==============\n",
      "============== Pattern 3506 ==============\n",
      "============== Pattern 3507 ==============\n",
      "============== Pattern 3508 ==============\n",
      "============== Pattern 3509 ==============\n",
      "============== Pattern 3510 ==============\n",
      "============== Pattern 3511 ==============\n",
      "============== Pattern 3512 ==============\n",
      "============== Pattern 3513 ==============\n",
      "============== Pattern 3514 ==============\n",
      "============== Pattern 3515 ==============\n",
      "============== Pattern 3516 ==============\n",
      "============== Pattern 3517 ==============\n",
      "============== Pattern 3518 ==============\n",
      "============== Pattern 3519 ==============\n",
      "============== Pattern 3520 ==============\n",
      "============== Pattern 3521 ==============\n",
      "============== Pattern 3522 ==============\n",
      "============== Pattern 3523 ==============\n",
      "============== Pattern 3524 ==============\n",
      "============== Pattern 3525 ==============\n",
      "============== Pattern 3526 ==============\n",
      "============== Pattern 3527 ==============\n",
      "============== Pattern 3528 ==============\n",
      "============== Pattern 3529 ==============\n",
      "============== Pattern 3530 ==============\n",
      "============== Pattern 3531 ==============\n",
      "============== Pattern 3532 ==============\n",
      "============== Pattern 3533 ==============\n",
      "============== Pattern 3534 ==============\n",
      "============== Pattern 3535 ==============\n",
      "============== Pattern 3536 ==============\n",
      "============== Pattern 3537 ==============\n",
      "============== Pattern 3538 ==============\n",
      "============== Pattern 3539 ==============\n",
      "============== Pattern 3540 ==============\n",
      "============== Pattern 3541 ==============\n",
      "============== Pattern 3542 ==============\n",
      "============== Pattern 3543 ==============\n",
      "============== Pattern 3544 ==============\n",
      "============== Pattern 3545 ==============\n",
      "============== Pattern 3546 ==============\n",
      "============== Pattern 3547 ==============\n",
      "============== Pattern 3548 ==============\n",
      "============== Pattern 3549 ==============\n",
      "============== Pattern 3550 ==============\n",
      "============== Pattern 3551 ==============\n",
      "============== Pattern 3552 ==============\n",
      "============== Pattern 3553 ==============\n",
      "============== Pattern 3554 ==============\n",
      "============== Pattern 3555 ==============\n",
      "============== Pattern 3556 ==============\n",
      "============== Pattern 3557 ==============\n",
      "============== Pattern 3558 ==============\n",
      "============== Pattern 3559 ==============\n",
      "============== Pattern 3560 ==============\n",
      "============== Pattern 3561 ==============\n",
      "============== Pattern 3562 ==============\n",
      "============== Pattern 3563 ==============\n",
      "============== Pattern 3564 ==============\n",
      "============== Pattern 3565 ==============\n",
      "============== Pattern 3566 ==============\n",
      "============== Pattern 3567 ==============\n",
      "============== Pattern 3568 ==============\n",
      "============== Pattern 3569 ==============\n",
      "============== Pattern 3570 ==============\n",
      "============== Pattern 3571 ==============\n",
      "============== Pattern 3572 ==============\n",
      "============== Pattern 3573 ==============\n",
      "============== Pattern 3574 ==============\n",
      "============== Pattern 3575 ==============\n",
      "============== Pattern 3576 ==============\n",
      "============== Pattern 3577 ==============\n",
      "============== Pattern 3578 ==============\n",
      "============== Pattern 3579 ==============\n",
      "============== Pattern 3580 ==============\n",
      "============== Pattern 3581 ==============\n",
      "============== Pattern 3582 ==============\n",
      "============== Pattern 3583 ==============\n",
      "============== Pattern 3584 ==============\n",
      "============== Pattern 3585 ==============\n",
      "============== Pattern 3586 ==============\n",
      "============== Pattern 3587 ==============\n",
      "============== Pattern 3588 ==============\n",
      "============== Pattern 3589 ==============\n",
      "============== Pattern 3590 ==============\n",
      "============== Pattern 3591 ==============\n",
      "============== Pattern 3592 ==============\n",
      "============== Pattern 3593 ==============\n",
      "============== Pattern 3594 ==============\n",
      "============== Pattern 3595 ==============\n",
      "============== Pattern 3596 ==============\n",
      "============== Pattern 3597 ==============\n",
      "============== Pattern 3598 ==============\n",
      "============== Pattern 3599 ==============\n",
      "============== Pattern 3600 ==============\n",
      "============== Pattern 3601 ==============\n",
      "============== Pattern 3602 ==============\n",
      "============== Pattern 3603 ==============\n",
      "============== Pattern 3604 ==============\n",
      "============== Pattern 3605 ==============\n",
      "============== Pattern 3606 ==============\n",
      "============== Pattern 3607 ==============\n",
      "============== Pattern 3608 ==============\n",
      "============== Pattern 3609 ==============\n",
      "============== Pattern 3610 ==============\n",
      "============== Pattern 3611 ==============\n",
      "============== Pattern 3612 ==============\n",
      "============== Pattern 3613 ==============\n",
      "============== Pattern 3614 ==============\n",
      "============== Pattern 3615 ==============\n",
      "============== Pattern 3616 ==============\n",
      "============== Pattern 3617 ==============\n",
      "============== Pattern 3618 ==============\n",
      "============== Pattern 3619 ==============\n",
      "============== Pattern 3620 ==============\n",
      "============== Pattern 3621 ==============\n",
      "============== Pattern 3622 ==============\n",
      "============== Pattern 3623 ==============\n",
      "============== Pattern 3624 ==============\n",
      "============== Pattern 3625 ==============\n",
      "============== Pattern 3626 ==============\n",
      "============== Pattern 3627 ==============\n",
      "============== Pattern 3628 ==============\n",
      "============== Pattern 3629 ==============\n",
      "============== Pattern 3630 ==============\n",
      "============== Pattern 3631 ==============\n",
      "============== Pattern 3632 ==============\n",
      "============== Pattern 3633 ==============\n",
      "============== Pattern 3634 ==============\n",
      "============== Pattern 3635 ==============\n",
      "============== Pattern 3636 ==============\n",
      "============== Pattern 3637 ==============\n",
      "============== Pattern 3638 ==============\n",
      "============== Pattern 3639 ==============\n",
      "============== Pattern 3640 ==============\n",
      "============== Pattern 3641 ==============\n",
      "============== Pattern 3642 ==============\n",
      "============== Pattern 3643 ==============\n",
      "============== Pattern 3644 ==============\n",
      "============== Pattern 3645 ==============\n",
      "============== Pattern 3646 ==============\n",
      "============== Pattern 3647 ==============\n",
      "============== Pattern 3648 ==============\n",
      "============== Pattern 3649 ==============\n",
      "============== Pattern 3650 ==============\n",
      "============== Pattern 3651 ==============\n",
      "============== Pattern 3652 ==============\n",
      "============== Pattern 3653 ==============\n",
      "============== Pattern 3654 ==============\n",
      "============== Pattern 3655 ==============\n",
      "============== Pattern 3656 ==============\n",
      "============== Pattern 3657 ==============\n",
      "============== Pattern 3658 ==============\n",
      "============== Pattern 3659 ==============\n",
      "============== Pattern 3660 ==============\n",
      "============== Pattern 3661 ==============\n",
      "============== Pattern 3662 ==============\n",
      "============== Pattern 3663 ==============\n",
      "============== Pattern 3664 ==============\n",
      "============== Pattern 3665 ==============\n",
      "============== Pattern 3666 ==============\n",
      "============== Pattern 3667 ==============\n",
      "============== Pattern 3668 ==============\n",
      "============== Pattern 3669 ==============\n",
      "============== Pattern 3670 ==============\n",
      "============== Pattern 3671 ==============\n",
      "============== Pattern 3672 ==============\n",
      "============== Pattern 3673 ==============\n",
      "============== Pattern 3674 ==============\n",
      "============== Pattern 3675 ==============\n",
      "============== Pattern 3676 ==============\n",
      "============== Pattern 3677 ==============\n",
      "============== Pattern 3678 ==============\n",
      "============== Pattern 3679 ==============\n",
      "============== Pattern 3680 ==============\n",
      "============== Pattern 3681 ==============\n",
      "============== Pattern 3682 ==============\n",
      "============== Pattern 3683 ==============\n",
      "============== Pattern 3684 ==============\n",
      "============== Pattern 3685 ==============\n",
      "============== Pattern 3686 ==============\n",
      "============== Pattern 3687 ==============\n",
      "============== Pattern 3688 ==============\n",
      "============== Pattern 3689 ==============\n",
      "============== Pattern 3690 ==============\n",
      "============== Pattern 3691 ==============\n",
      "============== Pattern 3692 ==============\n",
      "============== Pattern 3693 ==============\n",
      "============== Pattern 3694 ==============\n",
      "============== Pattern 3695 ==============\n",
      "============== Pattern 3696 ==============\n",
      "============== Pattern 3697 ==============\n",
      "============== Pattern 3698 ==============\n",
      "============== Pattern 3699 ==============\n",
      "============== Pattern 3700 ==============\n",
      "============== Pattern 3701 ==============\n",
      "============== Pattern 3702 ==============\n",
      "============== Pattern 3703 ==============\n",
      "============== Pattern 3704 ==============\n",
      "============== Pattern 3705 ==============\n",
      "============== Pattern 3706 ==============\n",
      "============== Pattern 3707 ==============\n",
      "============== Pattern 3708 ==============\n",
      "============== Pattern 3709 ==============\n",
      "============== Pattern 3710 ==============\n",
      "============== Pattern 3711 ==============\n",
      "============== Pattern 3712 ==============\n",
      "============== Pattern 3713 ==============\n",
      "============== Pattern 3714 ==============\n",
      "============== Pattern 3715 ==============\n",
      "============== Pattern 3716 ==============\n",
      "============== Pattern 3717 ==============\n",
      "============== Pattern 3718 ==============\n",
      "============== Pattern 3719 ==============\n",
      "============== Pattern 3720 ==============\n",
      "============== Pattern 3721 ==============\n",
      "============== Pattern 3722 ==============\n",
      "============== Pattern 3723 ==============\n",
      "============== Pattern 3724 ==============\n",
      "============== Pattern 3725 ==============\n",
      "============== Pattern 3726 ==============\n",
      "============== Pattern 3727 ==============\n",
      "============== Pattern 3728 ==============\n",
      "============== Pattern 3729 ==============\n",
      "============== Pattern 3730 ==============\n",
      "============== Pattern 3731 ==============\n",
      "============== Pattern 3732 ==============\n",
      "============== Pattern 3733 ==============\n",
      "============== Pattern 3734 ==============\n",
      "============== Pattern 3735 ==============\n",
      "============== Pattern 3736 ==============\n",
      "============== Pattern 3737 ==============\n",
      "============== Pattern 3738 ==============\n",
      "============== Pattern 3739 ==============\n",
      "============== Pattern 3740 ==============\n",
      "============== Pattern 3741 ==============\n",
      "============== Pattern 3742 ==============\n",
      "============== Pattern 3743 ==============\n",
      "============== Pattern 3744 ==============\n",
      "============== Pattern 3745 ==============\n",
      "============== Pattern 3746 ==============\n",
      "============== Pattern 3747 ==============\n",
      "============== Pattern 3748 ==============\n",
      "============== Pattern 3749 ==============\n",
      "============== Pattern 3750 ==============\n",
      "============== Pattern 3751 ==============\n",
      "============== Pattern 3752 ==============\n",
      "============== Pattern 3753 ==============\n",
      "============== Pattern 3754 ==============\n",
      "============== Pattern 3755 ==============\n",
      "============== Pattern 3756 ==============\n",
      "============== Pattern 3757 ==============\n",
      "============== Pattern 3758 ==============\n",
      "============== Pattern 3759 ==============\n",
      "============== Pattern 3760 ==============\n",
      "============== Pattern 3761 ==============\n",
      "============== Pattern 3762 ==============\n",
      "============== Pattern 3763 ==============\n",
      "============== Pattern 3764 ==============\n",
      "============== Pattern 3765 ==============\n",
      "============== Pattern 3766 ==============\n",
      "============== Pattern 3767 ==============\n",
      "============== Pattern 3768 ==============\n",
      "============== Pattern 3769 ==============\n",
      "============== Pattern 3770 ==============\n",
      "============== Pattern 3771 ==============\n",
      "============== Pattern 3772 ==============\n",
      "============== Pattern 3773 ==============\n",
      "============== Pattern 3774 ==============\n",
      "============== Pattern 3775 ==============\n",
      "============== Pattern 3776 ==============\n",
      "============== Pattern 3777 ==============\n",
      "============== Pattern 3778 ==============\n",
      "============== Pattern 3779 ==============\n",
      "============== Pattern 3780 ==============\n",
      "============== Pattern 3781 ==============\n",
      "============== Pattern 3782 ==============\n",
      "============== Pattern 3783 ==============\n",
      "============== Pattern 3784 ==============\n",
      "============== Pattern 3785 ==============\n",
      "============== Pattern 3786 ==============\n",
      "============== Pattern 3787 ==============\n",
      "============== Pattern 3788 ==============\n",
      "============== Pattern 3789 ==============\n",
      "============== Pattern 3790 ==============\n",
      "============== Pattern 3791 ==============\n",
      "============== Pattern 3792 ==============\n",
      "============== Pattern 3793 ==============\n",
      "============== Pattern 3794 ==============\n",
      "============== Pattern 3795 ==============\n",
      "============== Pattern 3796 ==============\n",
      "============== Pattern 3797 ==============\n",
      "============== Pattern 3798 ==============\n",
      "============== Pattern 3799 ==============\n",
      "============== Pattern 3800 ==============\n",
      "============== Pattern 3801 ==============\n",
      "============== Pattern 3802 ==============\n",
      "============== Pattern 3803 ==============\n",
      "============== Pattern 3804 ==============\n",
      "============== Pattern 3805 ==============\n",
      "============== Pattern 3806 ==============\n",
      "============== Pattern 3807 ==============\n",
      "============== Pattern 3808 ==============\n",
      "============== Pattern 3809 ==============\n",
      "============== Pattern 3810 ==============\n",
      "============== Pattern 3811 ==============\n",
      "============== Pattern 3812 ==============\n",
      "============== Pattern 3813 ==============\n",
      "============== Pattern 3814 ==============\n",
      "============== Pattern 3815 ==============\n",
      "============== Pattern 3816 ==============\n",
      "============== Pattern 3817 ==============\n",
      "============== Pattern 3818 ==============\n",
      "============== Pattern 3819 ==============\n",
      "============== Pattern 3820 ==============\n",
      "============== Pattern 3821 ==============\n",
      "============== Pattern 3822 ==============\n",
      "============== Pattern 3823 ==============\n",
      "============== Pattern 3824 ==============\n",
      "============== Pattern 3825 ==============\n",
      "============== Pattern 3826 ==============\n",
      "============== Pattern 3827 ==============\n",
      "============== Pattern 3828 ==============\n",
      "============== Pattern 3829 ==============\n",
      "============== Pattern 3830 ==============\n",
      "============== Pattern 3831 ==============\n",
      "============== Pattern 3832 ==============\n",
      "============== Pattern 3833 ==============\n",
      "============== Pattern 3834 ==============\n",
      "============== Pattern 3835 ==============\n",
      "============== Pattern 3836 ==============\n",
      "============== Pattern 3837 ==============\n",
      "============== Pattern 3838 ==============\n",
      "============== Pattern 3839 ==============\n",
      "============== Pattern 3840 ==============\n",
      "============== Pattern 3841 ==============\n",
      "============== Pattern 3842 ==============\n",
      "============== Pattern 3843 ==============\n",
      "============== Pattern 3844 ==============\n",
      "============== Pattern 3845 ==============\n",
      "============== Pattern 3846 ==============\n",
      "============== Pattern 3847 ==============\n",
      "============== Pattern 3848 ==============\n",
      "============== Pattern 3849 ==============\n",
      "============== Pattern 3850 ==============\n",
      "============== Pattern 3851 ==============\n",
      "============== Pattern 3852 ==============\n",
      "============== Pattern 3853 ==============\n",
      "============== Pattern 3854 ==============\n",
      "============== Pattern 3855 ==============\n",
      "============== Pattern 3856 ==============\n",
      "============== Pattern 3857 ==============\n",
      "============== Pattern 3858 ==============\n",
      "============== Pattern 3859 ==============\n",
      "============== Pattern 3860 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 3861 ==============\n",
      "============== Pattern 3862 ==============\n",
      "============== Pattern 3863 ==============\n",
      "============== Pattern 3864 ==============\n",
      "============== Pattern 3865 ==============\n",
      "============== Pattern 3866 ==============\n",
      "============== Pattern 3867 ==============\n",
      "============== Pattern 3868 ==============\n",
      "============== Pattern 3869 ==============\n",
      "============== Pattern 3870 ==============\n",
      "============== Pattern 3871 ==============\n",
      "============== Pattern 3872 ==============\n",
      "============== Pattern 3873 ==============\n",
      "============== Pattern 3874 ==============\n",
      "============== Pattern 3875 ==============\n",
      "============== Pattern 3876 ==============\n",
      "============== Pattern 3877 ==============\n",
      "============== Pattern 3878 ==============\n",
      "============== Pattern 3879 ==============\n",
      "============== Pattern 3880 ==============\n",
      "============== Pattern 3881 ==============\n",
      "============== Pattern 3882 ==============\n",
      "============== Pattern 3883 ==============\n",
      "============== Pattern 3884 ==============\n",
      "============== Pattern 3885 ==============\n",
      "============== Pattern 3886 ==============\n",
      "============== Pattern 3887 ==============\n",
      "============== Pattern 3888 ==============\n",
      "============== Pattern 3889 ==============\n",
      "============== Pattern 3890 ==============\n",
      "============== Pattern 3891 ==============\n",
      "============== Pattern 3892 ==============\n",
      "============== Pattern 3893 ==============\n",
      "============== Pattern 3894 ==============\n",
      "============== Pattern 3895 ==============\n",
      "============== Pattern 3896 ==============\n",
      "============== Pattern 3897 ==============\n",
      "============== Pattern 3898 ==============\n",
      "============== Pattern 3899 ==============\n",
      "============== Pattern 3900 ==============\n",
      "============== Pattern 3901 ==============\n",
      "============== Pattern 3902 ==============\n",
      "============== Pattern 3903 ==============\n",
      "============== Pattern 3904 ==============\n",
      "============== Pattern 3905 ==============\n",
      "============== Pattern 3906 ==============\n",
      "============== Pattern 3907 ==============\n",
      "============== Pattern 3908 ==============\n",
      "============== Pattern 3909 ==============\n",
      "============== Pattern 3910 ==============\n",
      "============== Pattern 3911 ==============\n",
      "============== Pattern 3912 ==============\n",
      "============== Pattern 3913 ==============\n",
      "============== Pattern 3914 ==============\n",
      "============== Pattern 3915 ==============\n",
      "============== Pattern 3916 ==============\n",
      "============== Pattern 3917 ==============\n",
      "============== Pattern 3918 ==============\n",
      "============== Pattern 3919 ==============\n",
      "============== Pattern 3920 ==============\n",
      "============== Pattern 3921 ==============\n",
      "============== Pattern 3922 ==============\n",
      "============== Pattern 3923 ==============\n",
      "============== Pattern 3924 ==============\n",
      "============== Pattern 3925 ==============\n",
      "============== Pattern 3926 ==============\n",
      "============== Pattern 3927 ==============\n",
      "============== Pattern 3928 ==============\n",
      "============== Pattern 3929 ==============\n",
      "============== Pattern 3930 ==============\n",
      "============== Pattern 3931 ==============\n",
      "============== Pattern 3932 ==============\n",
      "============== Pattern 3933 ==============\n",
      "============== Pattern 3934 ==============\n",
      "============== Pattern 3935 ==============\n",
      "============== Pattern 3936 ==============\n",
      "============== Pattern 3937 ==============\n",
      "============== Pattern 3938 ==============\n",
      "============== Pattern 3939 ==============\n",
      "============== Pattern 3940 ==============\n",
      "============== Pattern 3941 ==============\n",
      "============== Pattern 3942 ==============\n",
      "============== Pattern 3943 ==============\n",
      "============== Pattern 3944 ==============\n",
      "============== Pattern 3945 ==============\n",
      "============== Pattern 3946 ==============\n",
      "============== Pattern 3947 ==============\n",
      "============== Pattern 3948 ==============\n",
      "============== Pattern 3949 ==============\n",
      "============== Pattern 3950 ==============\n",
      "============== Pattern 3951 ==============\n",
      "============== Pattern 3952 ==============\n",
      "============== Pattern 3953 ==============\n",
      "============== Pattern 3954 ==============\n",
      "============== Pattern 3955 ==============\n",
      "============== Pattern 3956 ==============\n",
      "============== Pattern 3957 ==============\n",
      "============== Pattern 3958 ==============\n",
      "============== Pattern 3959 ==============\n",
      "============== Pattern 3960 ==============\n",
      "============== Pattern 3961 ==============\n",
      "============== Pattern 3962 ==============\n",
      "============== Pattern 3963 ==============\n",
      "============== Pattern 3964 ==============\n",
      "============== Pattern 3965 ==============\n",
      "============== Pattern 3966 ==============\n",
      "============== Pattern 3967 ==============\n",
      "============== Pattern 3968 ==============\n",
      "============== Pattern 3969 ==============\n",
      "============== Pattern 3970 ==============\n",
      "============== Pattern 3971 ==============\n",
      "============== Pattern 3972 ==============\n",
      "============== Pattern 3973 ==============\n",
      "============== Pattern 3974 ==============\n",
      "============== Pattern 3975 ==============\n",
      "============== Pattern 3976 ==============\n",
      "============== Pattern 3977 ==============\n",
      "============== Pattern 3978 ==============\n",
      "============== Pattern 3979 ==============\n",
      "============== Pattern 3980 ==============\n",
      "============== Pattern 3981 ==============\n",
      "============== Pattern 3982 ==============\n",
      "============== Pattern 3983 ==============\n",
      "============== Pattern 3984 ==============\n",
      "============== Pattern 3985 ==============\n",
      "============== Pattern 3986 ==============\n",
      "============== Pattern 3987 ==============\n",
      "============== Pattern 3988 ==============\n",
      "============== Pattern 3989 ==============\n",
      "============== Pattern 3990 ==============\n",
      "============== Pattern 3991 ==============\n",
      "============== Pattern 3992 ==============\n",
      "============== Pattern 3993 ==============\n",
      "============== Pattern 3994 ==============\n",
      "============== Pattern 3995 ==============\n",
      "============== Pattern 3996 ==============\n",
      "============== Pattern 3997 ==============\n",
      "============== Pattern 3998 ==============\n",
      "============== Pattern 3999 ==============\n",
      "============== Pattern 4000 ==============\n",
      "============== Pattern 4001 ==============\n",
      "============== Pattern 4002 ==============\n",
      "============== Pattern 4003 ==============\n",
      "============== Pattern 4004 ==============\n",
      "============== Pattern 4005 ==============\n",
      "============== Pattern 4006 ==============\n",
      "============== Pattern 4007 ==============\n",
      "============== Pattern 4008 ==============\n",
      "============== Pattern 4009 ==============\n",
      "============== Pattern 4010 ==============\n",
      "============== Pattern 4011 ==============\n",
      "============== Pattern 4012 ==============\n",
      "============== Pattern 4013 ==============\n",
      "============== Pattern 4014 ==============\n",
      "============== Pattern 4015 ==============\n",
      "============== Pattern 4016 ==============\n",
      "============== Pattern 4017 ==============\n",
      "============== Pattern 4018 ==============\n",
      "============== Pattern 4019 ==============\n",
      "============== Pattern 4020 ==============\n",
      "============== Pattern 4021 ==============\n",
      "============== Pattern 4022 ==============\n",
      "============== Pattern 4023 ==============\n",
      "============== Pattern 4024 ==============\n",
      "============== Pattern 4025 ==============\n",
      "============== Pattern 4026 ==============\n",
      "============== Pattern 4027 ==============\n",
      "============== Pattern 4028 ==============\n",
      "============== Pattern 4029 ==============\n",
      "============== Pattern 4030 ==============\n",
      "============== Pattern 4031 ==============\n",
      "============== Pattern 4032 ==============\n",
      "============== Pattern 4033 ==============\n",
      "============== Pattern 4034 ==============\n",
      "============== Pattern 4035 ==============\n",
      "============== Pattern 4036 ==============\n",
      "============== Pattern 4037 ==============\n",
      "============== Pattern 4038 ==============\n",
      "============== Pattern 4039 ==============\n",
      "============== Pattern 4040 ==============\n",
      "============== Pattern 4041 ==============\n",
      "============== Pattern 4042 ==============\n",
      "============== Pattern 4043 ==============\n",
      "============== Pattern 4044 ==============\n",
      "============== Pattern 4045 ==============\n",
      "============== Pattern 4046 ==============\n",
      "============== Pattern 4047 ==============\n",
      "============== Pattern 4048 ==============\n",
      "============== Pattern 4049 ==============\n",
      "============== Pattern 4050 ==============\n",
      "============== Pattern 4051 ==============\n",
      "============== Pattern 4052 ==============\n",
      "============== Pattern 4053 ==============\n",
      "============== Pattern 4054 ==============\n",
      "============== Pattern 4055 ==============\n",
      "============== Pattern 4056 ==============\n",
      "============== Pattern 4057 ==============\n",
      "============== Pattern 4058 ==============\n",
      "============== Pattern 4059 ==============\n",
      "============== Pattern 4060 ==============\n",
      "Average comprehensibility: 60.83793103448276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std comprehensibility: 4.078338748915884\n",
      "var comprehensibility: 16.63284695090878\n",
      "minimum comprehensibility: 42\n",
      "maximum comprehensibility: 70\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
