{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "tree_depth = 12\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.198968887329102 | KNN Loss: 6.225536346435547 | BCE Loss: 1.9734323024749756\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.135589599609375 | KNN Loss: 6.225613117218018 | BCE Loss: 1.9099769592285156\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.183895111083984 | KNN Loss: 6.225164413452148 | BCE Loss: 1.9587311744689941\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.114386558532715 | KNN Loss: 6.22507905960083 | BCE Loss: 1.8893071413040161\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.159957885742188 | KNN Loss: 6.224706172943115 | BCE Loss: 1.9352517127990723\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.136695861816406 | KNN Loss: 6.224417686462402 | BCE Loss: 1.912278652191162\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.143320083618164 | KNN Loss: 6.2238640785217285 | BCE Loss: 1.9194557666778564\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.132482528686523 | KNN Loss: 6.223404407501221 | BCE Loss: 1.9090784788131714\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.114105224609375 | KNN Loss: 6.222763538360596 | BCE Loss: 1.8913413286209106\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.153192520141602 | KNN Loss: 6.221945762634277 | BCE Loss: 1.9312469959259033\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.099300384521484 | KNN Loss: 6.2213335037231445 | BCE Loss: 1.8779664039611816\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.12055492401123 | KNN Loss: 6.220554351806641 | BCE Loss: 1.9000003337860107\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.083200454711914 | KNN Loss: 6.219651699066162 | BCE Loss: 1.863548755645752\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.053701400756836 | KNN Loss: 6.218647003173828 | BCE Loss: 1.8350539207458496\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.069229125976562 | KNN Loss: 6.217989921569824 | BCE Loss: 1.8512393236160278\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.030778884887695 | KNN Loss: 6.217182636260986 | BCE Loss: 1.8135957717895508\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.022584915161133 | KNN Loss: 6.217214107513428 | BCE Loss: 1.8053709268569946\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.059027671813965 | KNN Loss: 6.215147972106934 | BCE Loss: 1.8438800573349\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.017549514770508 | KNN Loss: 6.2144060134887695 | BCE Loss: 1.80314302444458\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.048784255981445 | KNN Loss: 6.212793827056885 | BCE Loss: 1.8359907865524292\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.016236305236816 | KNN Loss: 6.212398052215576 | BCE Loss: 1.8038382530212402\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 7.9861907958984375 | KNN Loss: 6.2102789878845215 | BCE Loss: 1.7759116888046265\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.007341384887695 | KNN Loss: 6.208273887634277 | BCE Loss: 1.7990670204162598\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.999517917633057 | KNN Loss: 6.206359386444092 | BCE Loss: 1.7931585311889648\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.950681686401367 | KNN Loss: 6.204629421234131 | BCE Loss: 1.7460521459579468\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.966765880584717 | KNN Loss: 6.201870918273926 | BCE Loss: 1.7648948431015015\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.938894271850586 | KNN Loss: 6.198941230773926 | BCE Loss: 1.7399530410766602\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.919280052185059 | KNN Loss: 6.195672035217285 | BCE Loss: 1.7236082553863525\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.95775842666626 | KNN Loss: 6.1934990882873535 | BCE Loss: 1.7642592191696167\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.9189252853393555 | KNN Loss: 6.18981409072876 | BCE Loss: 1.7291111946105957\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.908750057220459 | KNN Loss: 6.185203552246094 | BCE Loss: 1.7235466241836548\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.857428550720215 | KNN Loss: 6.1841583251953125 | BCE Loss: 1.6732699871063232\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.8780341148376465 | KNN Loss: 6.17529296875 | BCE Loss: 1.7027411460876465\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.837172508239746 | KNN Loss: 6.165554046630859 | BCE Loss: 1.6716185808181763\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.807738304138184 | KNN Loss: 6.166284084320068 | BCE Loss: 1.6414544582366943\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.770726203918457 | KNN Loss: 6.158243656158447 | BCE Loss: 1.6124827861785889\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.793177127838135 | KNN Loss: 6.141658782958984 | BCE Loss: 1.6515182256698608\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.737636566162109 | KNN Loss: 6.136190414428711 | BCE Loss: 1.601446270942688\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.744671821594238 | KNN Loss: 6.128030300140381 | BCE Loss: 1.6166414022445679\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.704354286193848 | KNN Loss: 6.116796016693115 | BCE Loss: 1.587558388710022\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.691051483154297 | KNN Loss: 6.103086948394775 | BCE Loss: 1.5879642963409424\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.638751029968262 | KNN Loss: 6.087118148803711 | BCE Loss: 1.5516328811645508\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.568620681762695 | KNN Loss: 6.067776203155518 | BCE Loss: 1.5008442401885986\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.5301079750061035 | KNN Loss: 6.042931079864502 | BCE Loss: 1.4871770143508911\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.505641937255859 | KNN Loss: 6.0155348777771 | BCE Loss: 1.4901068210601807\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.539292335510254 | KNN Loss: 6.00989294052124 | BCE Loss: 1.5293993949890137\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.446617126464844 | KNN Loss: 5.9806060791015625 | BCE Loss: 1.4660111665725708\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.397039413452148 | KNN Loss: 5.928584575653076 | BCE Loss: 1.4684548377990723\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.340020179748535 | KNN Loss: 5.897227764129639 | BCE Loss: 1.442792296409607\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.245639324188232 | KNN Loss: 5.84482479095459 | BCE Loss: 1.4008145332336426\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.169347763061523 | KNN Loss: 5.789454936981201 | BCE Loss: 1.3798930644989014\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.1076812744140625 | KNN Loss: 5.756680011749268 | BCE Loss: 1.3510010242462158\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 6.987465858459473 | KNN Loss: 5.670913219451904 | BCE Loss: 1.3165526390075684\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 6.894669532775879 | KNN Loss: 5.619997024536133 | BCE Loss: 1.274672269821167\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 6.818183898925781 | KNN Loss: 5.549234390258789 | BCE Loss: 1.2689497470855713\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 6.73622989654541 | KNN Loss: 5.455451011657715 | BCE Loss: 1.2807787656784058\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.603320121765137 | KNN Loss: 5.368850231170654 | BCE Loss: 1.2344698905944824\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.472398281097412 | KNN Loss: 5.2724432945251465 | BCE Loss: 1.1999549865722656\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.365781784057617 | KNN Loss: 5.157776832580566 | BCE Loss: 1.2080048322677612\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.293875217437744 | KNN Loss: 5.09997034072876 | BCE Loss: 1.1939048767089844\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.102066993713379 | KNN Loss: 4.960762977600098 | BCE Loss: 1.1413040161132812\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 5.970390319824219 | KNN Loss: 4.829520225524902 | BCE Loss: 1.1408700942993164\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 5.89615535736084 | KNN Loss: 4.737115859985352 | BCE Loss: 1.1590394973754883\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 5.728987693786621 | KNN Loss: 4.601353645324707 | BCE Loss: 1.1276341676712036\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 5.590305328369141 | KNN Loss: 4.463218688964844 | BCE Loss: 1.1270867586135864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 5.453589916229248 | KNN Loss: 4.331330299377441 | BCE Loss: 1.122259497642517\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.370828628540039 | KNN Loss: 4.244254112243652 | BCE Loss: 1.1265745162963867\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.28737735748291 | KNN Loss: 4.171576499938965 | BCE Loss: 1.1158009767532349\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.168726921081543 | KNN Loss: 4.064449787139893 | BCE Loss: 1.1042771339416504\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.064929008483887 | KNN Loss: 3.9546024799346924 | BCE Loss: 1.1103267669677734\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 4.961371421813965 | KNN Loss: 3.8593170642852783 | BCE Loss: 1.1020543575286865\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 4.923951148986816 | KNN Loss: 3.8265035152435303 | BCE Loss: 1.0974478721618652\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 4.831162452697754 | KNN Loss: 3.720099925994873 | BCE Loss: 1.11106276512146\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 4.715553283691406 | KNN Loss: 3.630492687225342 | BCE Loss: 1.0850608348846436\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 4.602646827697754 | KNN Loss: 3.537299871444702 | BCE Loss: 1.0653471946716309\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 4.622596740722656 | KNN Loss: 3.5267832279205322 | BCE Loss: 1.0958137512207031\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 4.575448513031006 | KNN Loss: 3.4910857677459717 | BCE Loss: 1.0843627452850342\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 4.4636430740356445 | KNN Loss: 3.3920533657073975 | BCE Loss: 1.0715899467468262\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 4.395510673522949 | KNN Loss: 3.3097174167633057 | BCE Loss: 1.0857930183410645\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 4.348389625549316 | KNN Loss: 3.284360647201538 | BCE Loss: 1.0640292167663574\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 4.32593297958374 | KNN Loss: 3.228900671005249 | BCE Loss: 1.0970323085784912\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 4.296324729919434 | KNN Loss: 3.191852331161499 | BCE Loss: 1.1044723987579346\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 4.223711967468262 | KNN Loss: 3.1541876792907715 | BCE Loss: 1.0695242881774902\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 4.161526203155518 | KNN Loss: 3.0864617824554443 | BCE Loss: 1.0750643014907837\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 4.1490373611450195 | KNN Loss: 3.057126760482788 | BCE Loss: 1.0919103622436523\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 4.180394172668457 | KNN Loss: 3.0982234477996826 | BCE Loss: 1.0821709632873535\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.078426837921143 | KNN Loss: 2.9882612228393555 | BCE Loss: 1.090165615081787\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.068357944488525 | KNN Loss: 2.9860246181488037 | BCE Loss: 1.0823332071304321\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.050460338592529 | KNN Loss: 2.9735360145568848 | BCE Loss: 1.0769243240356445\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 3.984647512435913 | KNN Loss: 2.9078407287597656 | BCE Loss: 1.0768067836761475\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 4.01548957824707 | KNN Loss: 2.936781406402588 | BCE Loss: 1.0787081718444824\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 3.9662342071533203 | KNN Loss: 2.872840642929077 | BCE Loss: 1.0933935642242432\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 3.920130968093872 | KNN Loss: 2.8686981201171875 | BCE Loss: 1.0514328479766846\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 3.9213979244232178 | KNN Loss: 2.8523495197296143 | BCE Loss: 1.0690484046936035\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 3.8397202491760254 | KNN Loss: 2.7806742191314697 | BCE Loss: 1.0590461492538452\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 3.8900113105773926 | KNN Loss: 2.821808338165283 | BCE Loss: 1.0682029724121094\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 3.9581918716430664 | KNN Loss: 2.896066904067993 | BCE Loss: 1.0621248483657837\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 3.818202018737793 | KNN Loss: 2.7576723098754883 | BCE Loss: 1.0605297088623047\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 3.817068099975586 | KNN Loss: 2.7551491260528564 | BCE Loss: 1.06191885471344\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 3.8657569885253906 | KNN Loss: 2.8039636611938477 | BCE Loss: 1.061793327331543\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 3.778857469558716 | KNN Loss: 2.735123872756958 | BCE Loss: 1.0437335968017578\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 3.8127212524414062 | KNN Loss: 2.762794256210327 | BCE Loss: 1.0499268770217896\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 3.825371265411377 | KNN Loss: 2.784771203994751 | BCE Loss: 1.040600061416626\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 3.7442023754119873 | KNN Loss: 2.7037322521209717 | BCE Loss: 1.0404701232910156\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 3.809476852416992 | KNN Loss: 2.769463062286377 | BCE Loss: 1.0400137901306152\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 3.794436454772949 | KNN Loss: 2.7236502170562744 | BCE Loss: 1.0707862377166748\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 3.786769390106201 | KNN Loss: 2.735281467437744 | BCE Loss: 1.051487922668457\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 3.80794095993042 | KNN Loss: 2.715707302093506 | BCE Loss: 1.0922335386276245\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 3.8020377159118652 | KNN Loss: 2.749779462814331 | BCE Loss: 1.0522581338882446\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 3.77343487739563 | KNN Loss: 2.734882116317749 | BCE Loss: 1.0385527610778809\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 3.7167277336120605 | KNN Loss: 2.6709251403808594 | BCE Loss: 1.0458025932312012\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 3.7107865810394287 | KNN Loss: 2.6432242393493652 | BCE Loss: 1.0675623416900635\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 3.7244253158569336 | KNN Loss: 2.684664249420166 | BCE Loss: 1.0397611856460571\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 3.721113681793213 | KNN Loss: 2.666245937347412 | BCE Loss: 1.0548676252365112\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 3.717900514602661 | KNN Loss: 2.6630897521972656 | BCE Loss: 1.0548107624053955\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 3.7117347717285156 | KNN Loss: 2.688762664794922 | BCE Loss: 1.0229719877243042\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 3.7160632610321045 | KNN Loss: 2.6461429595947266 | BCE Loss: 1.069920301437378\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 3.742666006088257 | KNN Loss: 2.693727731704712 | BCE Loss: 1.048938274383545\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 3.6563453674316406 | KNN Loss: 2.621415376663208 | BCE Loss: 1.034929871559143\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 3.6970911026000977 | KNN Loss: 2.650723695755005 | BCE Loss: 1.0463675260543823\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 3.6331005096435547 | KNN Loss: 2.5681450366973877 | BCE Loss: 1.064955472946167\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 3.7191085815429688 | KNN Loss: 2.6510725021362305 | BCE Loss: 1.0680360794067383\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 3.6688177585601807 | KNN Loss: 2.6133482456207275 | BCE Loss: 1.0554695129394531\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 3.6553404331207275 | KNN Loss: 2.6051416397094727 | BCE Loss: 1.0501987934112549\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 3.634065866470337 | KNN Loss: 2.5848097801208496 | BCE Loss: 1.0492560863494873\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 3.682370662689209 | KNN Loss: 2.647606372833252 | BCE Loss: 1.034764289855957\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 3.6791951656341553 | KNN Loss: 2.586013078689575 | BCE Loss: 1.09318208694458\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 3.677475929260254 | KNN Loss: 2.6189589500427246 | BCE Loss: 1.0585169792175293\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 3.6290600299835205 | KNN Loss: 2.5884432792663574 | BCE Loss: 1.040616750717163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 3.6977899074554443 | KNN Loss: 2.6339762210845947 | BCE Loss: 1.0638136863708496\n",
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 3.7353272438049316 | KNN Loss: 2.6816258430480957 | BCE Loss: 1.0537012815475464\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 3.670689105987549 | KNN Loss: 2.639291763305664 | BCE Loss: 1.0313974618911743\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 3.6490042209625244 | KNN Loss: 2.616891860961914 | BCE Loss: 1.0321123600006104\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 3.6471664905548096 | KNN Loss: 2.5883514881134033 | BCE Loss: 1.0588150024414062\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 3.649770498275757 | KNN Loss: 2.6246516704559326 | BCE Loss: 1.0251188278198242\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 3.6736395359039307 | KNN Loss: 2.6033623218536377 | BCE Loss: 1.070277214050293\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 3.6453888416290283 | KNN Loss: 2.597358226776123 | BCE Loss: 1.0480306148529053\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 3.642819404602051 | KNN Loss: 2.593799591064453 | BCE Loss: 1.049019694328308\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 3.635784149169922 | KNN Loss: 2.6030173301696777 | BCE Loss: 1.0327668190002441\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 3.601633071899414 | KNN Loss: 2.5749099254608154 | BCE Loss: 1.026723027229309\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 3.644804000854492 | KNN Loss: 2.6003687381744385 | BCE Loss: 1.0444352626800537\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 3.660526752471924 | KNN Loss: 2.6179351806640625 | BCE Loss: 1.0425915718078613\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 3.6909706592559814 | KNN Loss: 2.633950710296631 | BCE Loss: 1.0570199489593506\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 3.62570858001709 | KNN Loss: 2.5663654804229736 | BCE Loss: 1.0593430995941162\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 3.6806344985961914 | KNN Loss: 2.6563003063201904 | BCE Loss: 1.024334192276001\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 3.6404240131378174 | KNN Loss: 2.597343683242798 | BCE Loss: 1.0430803298950195\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 3.6180784702301025 | KNN Loss: 2.5776257514953613 | BCE Loss: 1.0404527187347412\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 3.6974871158599854 | KNN Loss: 2.6383142471313477 | BCE Loss: 1.0591728687286377\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 3.6490893363952637 | KNN Loss: 2.5937087535858154 | BCE Loss: 1.0553805828094482\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 3.6017088890075684 | KNN Loss: 2.5732672214508057 | BCE Loss: 1.0284417867660522\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 3.5823726654052734 | KNN Loss: 2.5451338291168213 | BCE Loss: 1.0372389554977417\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 3.6775994300842285 | KNN Loss: 2.6270689964294434 | BCE Loss: 1.0505304336547852\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 3.619356870651245 | KNN Loss: 2.5858256816864014 | BCE Loss: 1.0335311889648438\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 3.6141157150268555 | KNN Loss: 2.563133955001831 | BCE Loss: 1.0509816408157349\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 3.6102089881896973 | KNN Loss: 2.549441337585449 | BCE Loss: 1.060767650604248\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 3.605278968811035 | KNN Loss: 2.5771937370300293 | BCE Loss: 1.0280851125717163\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 3.6610348224639893 | KNN Loss: 2.6328697204589844 | BCE Loss: 1.0281651020050049\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 3.6569812297821045 | KNN Loss: 2.598320722579956 | BCE Loss: 1.0586605072021484\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 3.5859427452087402 | KNN Loss: 2.5297038555145264 | BCE Loss: 1.0562388896942139\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 3.6641433238983154 | KNN Loss: 2.6279850006103516 | BCE Loss: 1.0361583232879639\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 3.5819458961486816 | KNN Loss: 2.542236566543579 | BCE Loss: 1.039709210395813\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 3.5895583629608154 | KNN Loss: 2.5542232990264893 | BCE Loss: 1.0353350639343262\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 3.5763845443725586 | KNN Loss: 2.5569629669189453 | BCE Loss: 1.0194215774536133\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 3.699709415435791 | KNN Loss: 2.635237455368042 | BCE Loss: 1.0644720792770386\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 3.6065030097961426 | KNN Loss: 2.5690789222717285 | BCE Loss: 1.037424087524414\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 3.7047109603881836 | KNN Loss: 2.6277735233306885 | BCE Loss: 1.0769374370574951\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 3.5980536937713623 | KNN Loss: 2.56522536277771 | BCE Loss: 1.0328283309936523\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 3.613227367401123 | KNN Loss: 2.5977072715759277 | BCE Loss: 1.0155202150344849\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 3.6021993160247803 | KNN Loss: 2.547297239303589 | BCE Loss: 1.0549020767211914\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 3.6137335300445557 | KNN Loss: 2.5364558696746826 | BCE Loss: 1.077277660369873\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 3.6417653560638428 | KNN Loss: 2.5972492694854736 | BCE Loss: 1.0445160865783691\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 3.5915420055389404 | KNN Loss: 2.5735833644866943 | BCE Loss: 1.017958641052246\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 3.589740753173828 | KNN Loss: 2.551726818084717 | BCE Loss: 1.0380138158798218\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 3.572795867919922 | KNN Loss: 2.5549097061157227 | BCE Loss: 1.0178861618041992\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 3.6447317600250244 | KNN Loss: 2.579786539077759 | BCE Loss: 1.0649452209472656\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 3.5503759384155273 | KNN Loss: 2.534771203994751 | BCE Loss: 1.015604853630066\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 3.548875331878662 | KNN Loss: 2.5544683933258057 | BCE Loss: 0.9944069385528564\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 3.6504313945770264 | KNN Loss: 2.6057379245758057 | BCE Loss: 1.0446934700012207\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 3.6406280994415283 | KNN Loss: 2.5870680809020996 | BCE Loss: 1.0535600185394287\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 3.638413429260254 | KNN Loss: 2.568565607070923 | BCE Loss: 1.0698479413986206\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 3.6185860633850098 | KNN Loss: 2.5921103954315186 | BCE Loss: 1.0264755487442017\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 3.5470404624938965 | KNN Loss: 2.523076295852661 | BCE Loss: 1.0239641666412354\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 3.598576307296753 | KNN Loss: 2.524318218231201 | BCE Loss: 1.0742580890655518\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 3.6281697750091553 | KNN Loss: 2.584709882736206 | BCE Loss: 1.0434598922729492\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 3.6069250106811523 | KNN Loss: 2.5647528171539307 | BCE Loss: 1.0421720743179321\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 3.605804443359375 | KNN Loss: 2.5403361320495605 | BCE Loss: 1.065468430519104\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 3.6471939086914062 | KNN Loss: 2.604987859725952 | BCE Loss: 1.042206048965454\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 3.6318299770355225 | KNN Loss: 2.5792593955993652 | BCE Loss: 1.0525705814361572\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 3.6263203620910645 | KNN Loss: 2.5798017978668213 | BCE Loss: 1.0465185642242432\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 3.594949722290039 | KNN Loss: 2.559356212615967 | BCE Loss: 1.0355935096740723\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 3.555939197540283 | KNN Loss: 2.5341179370880127 | BCE Loss: 1.0218212604522705\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 3.6366772651672363 | KNN Loss: 2.6045379638671875 | BCE Loss: 1.0321393013000488\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 3.5607433319091797 | KNN Loss: 2.538670063018799 | BCE Loss: 1.0220732688903809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 3.618807077407837 | KNN Loss: 2.5717806816101074 | BCE Loss: 1.0470263957977295\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 3.569949150085449 | KNN Loss: 2.5322418212890625 | BCE Loss: 1.0377073287963867\n",
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 3.577805995941162 | KNN Loss: 2.5482277870178223 | BCE Loss: 1.0295783281326294\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 3.6160964965820312 | KNN Loss: 2.5657830238342285 | BCE Loss: 1.0503135919570923\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 3.622591495513916 | KNN Loss: 2.5976924896240234 | BCE Loss: 1.0248990058898926\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 3.5767431259155273 | KNN Loss: 2.555893898010254 | BCE Loss: 1.0208492279052734\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 3.610532760620117 | KNN Loss: 2.5846874713897705 | BCE Loss: 1.0258452892303467\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 3.701009511947632 | KNN Loss: 2.645540952682495 | BCE Loss: 1.0554685592651367\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 3.603384256362915 | KNN Loss: 2.552731513977051 | BCE Loss: 1.0506527423858643\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 3.6645565032958984 | KNN Loss: 2.6275010108947754 | BCE Loss: 1.0370556116104126\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 3.618581771850586 | KNN Loss: 2.576108455657959 | BCE Loss: 1.0424734354019165\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 3.5419249534606934 | KNN Loss: 2.532054901123047 | BCE Loss: 1.0098700523376465\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 3.649029016494751 | KNN Loss: 2.582242727279663 | BCE Loss: 1.066786289215088\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 3.630450487136841 | KNN Loss: 2.5704197883605957 | BCE Loss: 1.0600306987762451\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 3.5581564903259277 | KNN Loss: 2.5466866493225098 | BCE Loss: 1.0114699602127075\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 3.6420936584472656 | KNN Loss: 2.58693790435791 | BCE Loss: 1.055155873298645\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 3.612666130065918 | KNN Loss: 2.5565621852874756 | BCE Loss: 1.056104063987732\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 3.629638671875 | KNN Loss: 2.583031177520752 | BCE Loss: 1.0466073751449585\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 3.600661277770996 | KNN Loss: 2.5827841758728027 | BCE Loss: 1.0178769826889038\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 3.5173189640045166 | KNN Loss: 2.4845306873321533 | BCE Loss: 1.0327882766723633\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 3.574901580810547 | KNN Loss: 2.543943405151367 | BCE Loss: 1.0309580564498901\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 3.6157071590423584 | KNN Loss: 2.563751459121704 | BCE Loss: 1.0519556999206543\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 3.5558695793151855 | KNN Loss: 2.5342423915863037 | BCE Loss: 1.0216270685195923\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 3.575566530227661 | KNN Loss: 2.546213388442993 | BCE Loss: 1.029353141784668\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 3.5766310691833496 | KNN Loss: 2.5364484786987305 | BCE Loss: 1.0401824712753296\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 3.5872716903686523 | KNN Loss: 2.5288002490997314 | BCE Loss: 1.0584713220596313\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 3.537177801132202 | KNN Loss: 2.523851156234741 | BCE Loss: 1.013326644897461\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 3.577327251434326 | KNN Loss: 2.5681052207946777 | BCE Loss: 1.009222149848938\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 3.5762698650360107 | KNN Loss: 2.5640158653259277 | BCE Loss: 1.012253999710083\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 3.570827007293701 | KNN Loss: 2.538982629776001 | BCE Loss: 1.0318443775177002\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 3.5504515171051025 | KNN Loss: 2.490968704223633 | BCE Loss: 1.0594828128814697\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 3.5828402042388916 | KNN Loss: 2.550903558731079 | BCE Loss: 1.0319366455078125\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 3.6277101039886475 | KNN Loss: 2.5777955055236816 | BCE Loss: 1.0499145984649658\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 3.6160902976989746 | KNN Loss: 2.615694046020508 | BCE Loss: 1.0003962516784668\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 3.590902328491211 | KNN Loss: 2.549652576446533 | BCE Loss: 1.0412497520446777\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 3.56313157081604 | KNN Loss: 2.524376153945923 | BCE Loss: 1.0387554168701172\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 3.4910197257995605 | KNN Loss: 2.4925341606140137 | BCE Loss: 0.9984856247901917\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 3.6256308555603027 | KNN Loss: 2.5596837997436523 | BCE Loss: 1.0659470558166504\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 3.5719261169433594 | KNN Loss: 2.522304058074951 | BCE Loss: 1.0496219396591187\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 3.5470218658447266 | KNN Loss: 2.532331943511963 | BCE Loss: 1.0146899223327637\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 3.592700242996216 | KNN Loss: 2.56345534324646 | BCE Loss: 1.0292448997497559\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 3.5151233673095703 | KNN Loss: 2.4852449893951416 | BCE Loss: 1.0298783779144287\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 3.6216421127319336 | KNN Loss: 2.560041666030884 | BCE Loss: 1.0616004467010498\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 3.5804686546325684 | KNN Loss: 2.5454487800598145 | BCE Loss: 1.035019874572754\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 3.5258090496063232 | KNN Loss: 2.5179810523986816 | BCE Loss: 1.0078279972076416\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 3.6266090869903564 | KNN Loss: 2.5632147789001465 | BCE Loss: 1.06339430809021\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 3.580815315246582 | KNN Loss: 2.549739122390747 | BCE Loss: 1.0310763120651245\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 3.5555076599121094 | KNN Loss: 2.5152907371520996 | BCE Loss: 1.0402169227600098\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 3.6229610443115234 | KNN Loss: 2.598357915878296 | BCE Loss: 1.0246031284332275\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 3.56889009475708 | KNN Loss: 2.5504515171051025 | BCE Loss: 1.0184385776519775\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 3.5794758796691895 | KNN Loss: 2.5237343311309814 | BCE Loss: 1.0557414293289185\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 3.6073405742645264 | KNN Loss: 2.565117835998535 | BCE Loss: 1.0422227382659912\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 3.607722759246826 | KNN Loss: 2.5526444911956787 | BCE Loss: 1.055078387260437\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 3.6412105560302734 | KNN Loss: 2.5819599628448486 | BCE Loss: 1.0592505931854248\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 3.532419443130493 | KNN Loss: 2.523082971572876 | BCE Loss: 1.0093364715576172\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 3.565101385116577 | KNN Loss: 2.521047353744507 | BCE Loss: 1.0440540313720703\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 3.576303482055664 | KNN Loss: 2.544802665710449 | BCE Loss: 1.0315006971359253\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 3.5882620811462402 | KNN Loss: 2.537144660949707 | BCE Loss: 1.0511173009872437\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 3.583627700805664 | KNN Loss: 2.5508012771606445 | BCE Loss: 1.03282630443573\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 3.58024263381958 | KNN Loss: 2.5318121910095215 | BCE Loss: 1.0484305620193481\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 3.5609664916992188 | KNN Loss: 2.5372540950775146 | BCE Loss: 1.023712396621704\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 3.5307564735412598 | KNN Loss: 2.527003049850464 | BCE Loss: 1.003753423690796\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 3.5806617736816406 | KNN Loss: 2.540818452835083 | BCE Loss: 1.0398433208465576\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 3.5891318321228027 | KNN Loss: 2.5202372074127197 | BCE Loss: 1.0688945055007935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 3.599447250366211 | KNN Loss: 2.5760529041290283 | BCE Loss: 1.0233944654464722\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 3.576629161834717 | KNN Loss: 2.5417699813842773 | BCE Loss: 1.0348591804504395\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 3.612381935119629 | KNN Loss: 2.572984457015991 | BCE Loss: 1.0393973588943481\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 3.5771279335021973 | KNN Loss: 2.5571000576019287 | BCE Loss: 1.0200278759002686\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 3.6161727905273438 | KNN Loss: 2.5712759494781494 | BCE Loss: 1.0448968410491943\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 3.5584521293640137 | KNN Loss: 2.5300369262695312 | BCE Loss: 1.0284152030944824\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 3.539503335952759 | KNN Loss: 2.492119789123535 | BCE Loss: 1.0473835468292236\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 3.569394588470459 | KNN Loss: 2.5309765338897705 | BCE Loss: 1.0384180545806885\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 3.65505313873291 | KNN Loss: 2.6292693614959717 | BCE Loss: 1.0257837772369385\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 3.605403423309326 | KNN Loss: 2.5545196533203125 | BCE Loss: 1.0508838891983032\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 3.5665063858032227 | KNN Loss: 2.5443994998931885 | BCE Loss: 1.0221068859100342\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 3.59151029586792 | KNN Loss: 2.524353265762329 | BCE Loss: 1.0671570301055908\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 3.5365731716156006 | KNN Loss: 2.4922142028808594 | BCE Loss: 1.0443589687347412\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 3.5984933376312256 | KNN Loss: 2.574056625366211 | BCE Loss: 1.0244367122650146\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 3.6038036346435547 | KNN Loss: 2.5932247638702393 | BCE Loss: 1.0105787515640259\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 3.5306236743927 | KNN Loss: 2.522066354751587 | BCE Loss: 1.0085573196411133\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 3.6090617179870605 | KNN Loss: 2.5842838287353516 | BCE Loss: 1.024777889251709\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 3.5547447204589844 | KNN Loss: 2.5394954681396484 | BCE Loss: 1.0152491331100464\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 3.646965503692627 | KNN Loss: 2.585155725479126 | BCE Loss: 1.0618098974227905\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 3.5673398971557617 | KNN Loss: 2.5604333877563477 | BCE Loss: 1.006906509399414\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 3.5189123153686523 | KNN Loss: 2.4913971424102783 | BCE Loss: 1.027515172958374\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 3.5507678985595703 | KNN Loss: 2.534396171569824 | BCE Loss: 1.0163716077804565\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 3.508737564086914 | KNN Loss: 2.498964309692383 | BCE Loss: 1.0097731351852417\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 3.5800909996032715 | KNN Loss: 2.538543939590454 | BCE Loss: 1.0415470600128174\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 3.5603318214416504 | KNN Loss: 2.527078866958618 | BCE Loss: 1.0332529544830322\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 3.553636074066162 | KNN Loss: 2.521718740463257 | BCE Loss: 1.0319173336029053\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 3.5653433799743652 | KNN Loss: 2.531360387802124 | BCE Loss: 1.0339831113815308\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 3.557110071182251 | KNN Loss: 2.5039079189300537 | BCE Loss: 1.0532021522521973\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 3.550675630569458 | KNN Loss: 2.543478488922119 | BCE Loss: 1.0071971416473389\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 3.5537543296813965 | KNN Loss: 2.5238702297210693 | BCE Loss: 1.0298840999603271\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 3.5928497314453125 | KNN Loss: 2.563864231109619 | BCE Loss: 1.0289855003356934\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 3.5055642127990723 | KNN Loss: 2.510871410369873 | BCE Loss: 0.9946929216384888\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 3.6184635162353516 | KNN Loss: 2.590244770050049 | BCE Loss: 1.0282186269760132\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 3.584782600402832 | KNN Loss: 2.5629024505615234 | BCE Loss: 1.0218801498413086\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 3.5343194007873535 | KNN Loss: 2.517436981201172 | BCE Loss: 1.016882300376892\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 3.577991485595703 | KNN Loss: 2.561453104019165 | BCE Loss: 1.016538381576538\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 3.5568528175354004 | KNN Loss: 2.5320231914520264 | BCE Loss: 1.024829626083374\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 3.5488200187683105 | KNN Loss: 2.5279541015625 | BCE Loss: 1.0208660364151\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 3.6014199256896973 | KNN Loss: 2.583042860031128 | BCE Loss: 1.0183771848678589\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 3.55802059173584 | KNN Loss: 2.545064687728882 | BCE Loss: 1.0129560232162476\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 3.5632314682006836 | KNN Loss: 2.540938138961792 | BCE Loss: 1.0222934484481812\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 3.5319552421569824 | KNN Loss: 2.4937021732330322 | BCE Loss: 1.0382529497146606\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 3.5614638328552246 | KNN Loss: 2.536635398864746 | BCE Loss: 1.0248284339904785\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 3.501204490661621 | KNN Loss: 2.512269973754883 | BCE Loss: 0.9889344573020935\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 3.6158387660980225 | KNN Loss: 2.589228391647339 | BCE Loss: 1.0266103744506836\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 3.5797457695007324 | KNN Loss: 2.5430169105529785 | BCE Loss: 1.0367289781570435\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 3.60837459564209 | KNN Loss: 2.590785264968872 | BCE Loss: 1.0175893306732178\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 3.5574750900268555 | KNN Loss: 2.54142165184021 | BCE Loss: 1.0160534381866455\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 3.558467388153076 | KNN Loss: 2.5011398792266846 | BCE Loss: 1.0573275089263916\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 3.5406081676483154 | KNN Loss: 2.5122408866882324 | BCE Loss: 1.028367280960083\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 3.564131498336792 | KNN Loss: 2.5312142372131348 | BCE Loss: 1.0329172611236572\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 3.596004009246826 | KNN Loss: 2.5549428462982178 | BCE Loss: 1.0410610437393188\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 3.581085681915283 | KNN Loss: 2.561034917831421 | BCE Loss: 1.0200507640838623\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 3.5614960193634033 | KNN Loss: 2.517930269241333 | BCE Loss: 1.0435657501220703\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 3.5382723808288574 | KNN Loss: 2.528836965560913 | BCE Loss: 1.0094354152679443\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 3.519294261932373 | KNN Loss: 2.491974353790283 | BCE Loss: 1.0273199081420898\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 3.5116119384765625 | KNN Loss: 2.4779887199401855 | BCE Loss: 1.0336230993270874\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 3.582797050476074 | KNN Loss: 2.552755832672119 | BCE Loss: 1.0300413370132446\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 3.5509157180786133 | KNN Loss: 2.51267147064209 | BCE Loss: 1.0382442474365234\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 3.527320384979248 | KNN Loss: 2.4989805221557617 | BCE Loss: 1.0283398628234863\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 3.515010118484497 | KNN Loss: 2.4760239124298096 | BCE Loss: 1.0389862060546875\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 3.544400691986084 | KNN Loss: 2.5134096145629883 | BCE Loss: 1.0309909582138062\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 3.542579174041748 | KNN Loss: 2.5231471061706543 | BCE Loss: 1.0194320678710938\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 3.567243814468384 | KNN Loss: 2.5208921432495117 | BCE Loss: 1.046351671218872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 3.511972665786743 | KNN Loss: 2.4963977336883545 | BCE Loss: 1.0155749320983887\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 3.5446720123291016 | KNN Loss: 2.494915008544922 | BCE Loss: 1.0497570037841797\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 3.548696756362915 | KNN Loss: 2.5070040225982666 | BCE Loss: 1.0416927337646484\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 3.548344850540161 | KNN Loss: 2.5258564949035645 | BCE Loss: 1.0224883556365967\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 3.5514042377471924 | KNN Loss: 2.5236122608184814 | BCE Loss: 1.027791976928711\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 3.547266960144043 | KNN Loss: 2.526203155517578 | BCE Loss: 1.0210636854171753\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 3.556274890899658 | KNN Loss: 2.538482904434204 | BCE Loss: 1.0177921056747437\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 3.562323570251465 | KNN Loss: 2.5182886123657227 | BCE Loss: 1.0440349578857422\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 3.5590226650238037 | KNN Loss: 2.5428783893585205 | BCE Loss: 1.0161442756652832\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 3.4987411499023438 | KNN Loss: 2.517036199569702 | BCE Loss: 0.9817050695419312\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 3.5751824378967285 | KNN Loss: 2.545522451400757 | BCE Loss: 1.0296601057052612\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 3.5619421005249023 | KNN Loss: 2.5453808307647705 | BCE Loss: 1.0165612697601318\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 3.593651533126831 | KNN Loss: 2.5543153285980225 | BCE Loss: 1.0393362045288086\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 3.5186784267425537 | KNN Loss: 2.477861166000366 | BCE Loss: 1.0408172607421875\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 3.538360834121704 | KNN Loss: 2.5045382976531982 | BCE Loss: 1.0338225364685059\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 3.5146327018737793 | KNN Loss: 2.4809064865112305 | BCE Loss: 1.0337263345718384\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 3.581120014190674 | KNN Loss: 2.5667243003845215 | BCE Loss: 1.0143957138061523\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 3.58837628364563 | KNN Loss: 2.541562080383301 | BCE Loss: 1.046814203262329\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 3.5412099361419678 | KNN Loss: 2.500229835510254 | BCE Loss: 1.0409801006317139\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 3.5242323875427246 | KNN Loss: 2.4972596168518066 | BCE Loss: 1.026972770690918\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 3.494523286819458 | KNN Loss: 2.4832069873809814 | BCE Loss: 1.0113162994384766\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 3.5797505378723145 | KNN Loss: 2.5764408111572266 | BCE Loss: 1.003309726715088\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 3.5299301147460938 | KNN Loss: 2.5257911682128906 | BCE Loss: 1.0041388273239136\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 3.5578274726867676 | KNN Loss: 2.5237109661102295 | BCE Loss: 1.0341163873672485\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 3.550595760345459 | KNN Loss: 2.539177417755127 | BCE Loss: 1.011418342590332\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 3.5208818912506104 | KNN Loss: 2.5210957527160645 | BCE Loss: 0.9997860789299011\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 3.5986294746398926 | KNN Loss: 2.567551851272583 | BCE Loss: 1.03107750415802\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 3.5311226844787598 | KNN Loss: 2.5268971920013428 | BCE Loss: 1.004225492477417\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 3.5431811809539795 | KNN Loss: 2.5078015327453613 | BCE Loss: 1.0353796482086182\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 3.5530264377593994 | KNN Loss: 2.53752064704895 | BCE Loss: 1.0155057907104492\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 3.541118621826172 | KNN Loss: 2.5417587757110596 | BCE Loss: 0.9993597865104675\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 3.5179991722106934 | KNN Loss: 2.498438596725464 | BCE Loss: 1.0195605754852295\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 3.5294811725616455 | KNN Loss: 2.5279767513275146 | BCE Loss: 1.0015044212341309\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 3.556882381439209 | KNN Loss: 2.5601646900177 | BCE Loss: 0.9967176914215088\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 3.511073589324951 | KNN Loss: 2.4907877445220947 | BCE Loss: 1.0202858448028564\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 3.552781105041504 | KNN Loss: 2.521888494491577 | BCE Loss: 1.0308926105499268\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 3.5127997398376465 | KNN Loss: 2.5133137702941895 | BCE Loss: 0.9994858503341675\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 3.5882184505462646 | KNN Loss: 2.5466153621673584 | BCE Loss: 1.0416030883789062\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 3.542358875274658 | KNN Loss: 2.518918752670288 | BCE Loss: 1.0234402418136597\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 3.5995941162109375 | KNN Loss: 2.561030149459839 | BCE Loss: 1.0385639667510986\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 3.518490791320801 | KNN Loss: 2.505769729614258 | BCE Loss: 1.012721061706543\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 3.5428879261016846 | KNN Loss: 2.522500991821289 | BCE Loss: 1.0203869342803955\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 3.5703330039978027 | KNN Loss: 2.5366876125335693 | BCE Loss: 1.033645510673523\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 3.5772194862365723 | KNN Loss: 2.514608860015869 | BCE Loss: 1.0626106262207031\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 3.506206512451172 | KNN Loss: 2.4933676719665527 | BCE Loss: 1.0128389596939087\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 3.529329776763916 | KNN Loss: 2.509373903274536 | BCE Loss: 1.0199559926986694\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 3.567784309387207 | KNN Loss: 2.540691614151001 | BCE Loss: 1.027092695236206\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 3.5237975120544434 | KNN Loss: 2.5491085052490234 | BCE Loss: 0.9746888875961304\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 3.5443577766418457 | KNN Loss: 2.4739866256713867 | BCE Loss: 1.070371150970459\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 3.568028211593628 | KNN Loss: 2.5621931552886963 | BCE Loss: 1.0058350563049316\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 3.557509422302246 | KNN Loss: 2.534285306930542 | BCE Loss: 1.023224115371704\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 3.5650596618652344 | KNN Loss: 2.5362441539764404 | BCE Loss: 1.0288153886795044\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 3.6078972816467285 | KNN Loss: 2.5383880138397217 | BCE Loss: 1.0695092678070068\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 3.588036060333252 | KNN Loss: 2.5769848823547363 | BCE Loss: 1.0110512971878052\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 3.58603835105896 | KNN Loss: 2.5582969188690186 | BCE Loss: 1.0277414321899414\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 3.555311679840088 | KNN Loss: 2.517799139022827 | BCE Loss: 1.0375124216079712\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 3.575099468231201 | KNN Loss: 2.5342600345611572 | BCE Loss: 1.040839433670044\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 3.549449920654297 | KNN Loss: 2.5085289478302 | BCE Loss: 1.0409208536148071\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 3.5621395111083984 | KNN Loss: 2.5246541500091553 | BCE Loss: 1.0374854803085327\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 3.5729293823242188 | KNN Loss: 2.546896457672119 | BCE Loss: 1.02603280544281\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 3.552682638168335 | KNN Loss: 2.541656255722046 | BCE Loss: 1.011026382446289\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 3.538908004760742 | KNN Loss: 2.4948441982269287 | BCE Loss: 1.0440638065338135\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 3.4932117462158203 | KNN Loss: 2.481968879699707 | BCE Loss: 1.0112427473068237\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 3.5668559074401855 | KNN Loss: 2.5204503536224365 | BCE Loss: 1.046405553817749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 3.5428080558776855 | KNN Loss: 2.51823353767395 | BCE Loss: 1.0245743989944458\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 3.54805850982666 | KNN Loss: 2.524513006210327 | BCE Loss: 1.0235453844070435\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 3.565990447998047 | KNN Loss: 2.51900053024292 | BCE Loss: 1.0469897985458374\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 3.50648832321167 | KNN Loss: 2.4750001430511475 | BCE Loss: 1.031488060951233\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 3.54034686088562 | KNN Loss: 2.5017125606536865 | BCE Loss: 1.0386343002319336\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 3.5117130279541016 | KNN Loss: 2.4871654510498047 | BCE Loss: 1.0245476961135864\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 3.6115224361419678 | KNN Loss: 2.575932502746582 | BCE Loss: 1.0355899333953857\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 3.5500192642211914 | KNN Loss: 2.523078441619873 | BCE Loss: 1.0269408226013184\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 3.5287647247314453 | KNN Loss: 2.511021375656128 | BCE Loss: 1.017743468284607\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 3.5629944801330566 | KNN Loss: 2.5242042541503906 | BCE Loss: 1.038790225982666\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 3.5397844314575195 | KNN Loss: 2.505392074584961 | BCE Loss: 1.0343923568725586\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 3.5551657676696777 | KNN Loss: 2.533695697784424 | BCE Loss: 1.0214701890945435\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 3.565189838409424 | KNN Loss: 2.5683608055114746 | BCE Loss: 0.9968289136886597\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 3.5507993698120117 | KNN Loss: 2.5389931201934814 | BCE Loss: 1.0118061304092407\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 3.563142776489258 | KNN Loss: 2.531043529510498 | BCE Loss: 1.0320993661880493\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 3.5335023403167725 | KNN Loss: 2.5306875705718994 | BCE Loss: 1.002814769744873\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 3.551744222640991 | KNN Loss: 2.522461414337158 | BCE Loss: 1.029282808303833\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 3.5579729080200195 | KNN Loss: 2.531663656234741 | BCE Loss: 1.0263092517852783\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 3.554440498352051 | KNN Loss: 2.53524112701416 | BCE Loss: 1.0191993713378906\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 3.535334587097168 | KNN Loss: 2.5086863040924072 | BCE Loss: 1.0266482830047607\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 3.607110023498535 | KNN Loss: 2.5622799396514893 | BCE Loss: 1.0448299646377563\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 3.5235884189605713 | KNN Loss: 2.509284496307373 | BCE Loss: 1.0143039226531982\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 3.601459503173828 | KNN Loss: 2.552497386932373 | BCE Loss: 1.0489622354507446\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 3.51932692527771 | KNN Loss: 2.4853992462158203 | BCE Loss: 1.0339276790618896\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 3.4932360649108887 | KNN Loss: 2.497647523880005 | BCE Loss: 0.9955886006355286\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 3.5796022415161133 | KNN Loss: 2.5388853549957275 | BCE Loss: 1.0407168865203857\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 3.573727607727051 | KNN Loss: 2.534137010574341 | BCE Loss: 1.0395904779434204\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 3.507951259613037 | KNN Loss: 2.5123417377471924 | BCE Loss: 0.9956095218658447\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 3.5570061206817627 | KNN Loss: 2.518404006958008 | BCE Loss: 1.0386021137237549\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 3.5085105895996094 | KNN Loss: 2.4968972206115723 | BCE Loss: 1.0116134881973267\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 3.544924259185791 | KNN Loss: 2.5233535766601562 | BCE Loss: 1.0215708017349243\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 3.5199732780456543 | KNN Loss: 2.5101094245910645 | BCE Loss: 1.0098638534545898\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 3.5130515098571777 | KNN Loss: 2.4851622581481934 | BCE Loss: 1.0278892517089844\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 3.5001792907714844 | KNN Loss: 2.464780330657959 | BCE Loss: 1.035399079322815\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 3.555112361907959 | KNN Loss: 2.4998769760131836 | BCE Loss: 1.0552353858947754\n",
      "Epoch    70: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 3.539175271987915 | KNN Loss: 2.5290610790252686 | BCE Loss: 1.0101141929626465\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 3.5536727905273438 | KNN Loss: 2.527946949005127 | BCE Loss: 1.0257257223129272\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 3.520786762237549 | KNN Loss: 2.5053930282592773 | BCE Loss: 1.015393614768982\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 3.5372180938720703 | KNN Loss: 2.4989609718322754 | BCE Loss: 1.0382570028305054\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 3.5443036556243896 | KNN Loss: 2.5479931831359863 | BCE Loss: 0.9963105320930481\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 3.5371506214141846 | KNN Loss: 2.5166549682617188 | BCE Loss: 1.0204956531524658\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 3.5920605659484863 | KNN Loss: 2.5521488189697266 | BCE Loss: 1.0399118661880493\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 3.553030252456665 | KNN Loss: 2.541475296020508 | BCE Loss: 1.0115549564361572\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 3.571621894836426 | KNN Loss: 2.5120527744293213 | BCE Loss: 1.059569239616394\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 3.5549778938293457 | KNN Loss: 2.5244386196136475 | BCE Loss: 1.0305392742156982\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 3.50883412361145 | KNN Loss: 2.5022737979888916 | BCE Loss: 1.0065603256225586\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 3.4930362701416016 | KNN Loss: 2.495379686355591 | BCE Loss: 0.9976564645767212\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 3.5146167278289795 | KNN Loss: 2.4978487491607666 | BCE Loss: 1.016767978668213\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 3.596529483795166 | KNN Loss: 2.555123805999756 | BCE Loss: 1.0414056777954102\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 3.5589406490325928 | KNN Loss: 2.5171923637390137 | BCE Loss: 1.041748285293579\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 3.571723222732544 | KNN Loss: 2.520705223083496 | BCE Loss: 1.0510179996490479\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 3.5271339416503906 | KNN Loss: 2.508786916732788 | BCE Loss: 1.0183470249176025\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 3.5287842750549316 | KNN Loss: 2.510352373123169 | BCE Loss: 1.0184317827224731\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 3.552053928375244 | KNN Loss: 2.5488622188568115 | BCE Loss: 1.003191590309143\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 3.5517876148223877 | KNN Loss: 2.5202620029449463 | BCE Loss: 1.0315256118774414\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 3.5526609420776367 | KNN Loss: 2.5101823806762695 | BCE Loss: 1.0424785614013672\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 3.5363340377807617 | KNN Loss: 2.515770673751831 | BCE Loss: 1.0205633640289307\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 3.5856850147247314 | KNN Loss: 2.5624139308929443 | BCE Loss: 1.023271083831787\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 3.531454563140869 | KNN Loss: 2.5047554969787598 | BCE Loss: 1.0266990661621094\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 3.5047783851623535 | KNN Loss: 2.50142765045166 | BCE Loss: 1.0033507347106934\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 3.5100724697113037 | KNN Loss: 2.513340473175049 | BCE Loss: 0.9967319965362549\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 3.555856704711914 | KNN Loss: 2.5317375659942627 | BCE Loss: 1.0241190195083618\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 3.5220746994018555 | KNN Loss: 2.496047258377075 | BCE Loss: 1.0260274410247803\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 3.4748482704162598 | KNN Loss: 2.475243091583252 | BCE Loss: 0.9996050596237183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 3.516369342803955 | KNN Loss: 2.4834420680999756 | BCE Loss: 1.0329272747039795\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 3.5317420959472656 | KNN Loss: 2.501016139984131 | BCE Loss: 1.0307259559631348\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 3.5574615001678467 | KNN Loss: 2.533857583999634 | BCE Loss: 1.023603916168213\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 3.511458396911621 | KNN Loss: 2.511655569076538 | BCE Loss: 0.9998027086257935\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 3.4992496967315674 | KNN Loss: 2.4767322540283203 | BCE Loss: 1.022517442703247\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 3.52164626121521 | KNN Loss: 2.4742960929870605 | BCE Loss: 1.0473501682281494\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 3.556767225265503 | KNN Loss: 2.51163387298584 | BCE Loss: 1.045133352279663\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 3.5352797508239746 | KNN Loss: 2.519198417663574 | BCE Loss: 1.0160813331604004\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 3.522392511367798 | KNN Loss: 2.4980568885803223 | BCE Loss: 1.0243356227874756\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 3.5464131832122803 | KNN Loss: 2.503485918045044 | BCE Loss: 1.0429272651672363\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 3.562350273132324 | KNN Loss: 2.5112087726593018 | BCE Loss: 1.0511415004730225\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 3.536987543106079 | KNN Loss: 2.534147024154663 | BCE Loss: 1.002840518951416\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 3.554151773452759 | KNN Loss: 2.5470926761627197 | BCE Loss: 1.007059097290039\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 3.493196964263916 | KNN Loss: 2.5022058486938477 | BCE Loss: 0.9909911155700684\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 3.4860801696777344 | KNN Loss: 2.4871628284454346 | BCE Loss: 0.9989173412322998\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 3.513842821121216 | KNN Loss: 2.4968302249908447 | BCE Loss: 1.017012596130371\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 3.566875457763672 | KNN Loss: 2.526066541671753 | BCE Loss: 1.0408087968826294\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 3.5057878494262695 | KNN Loss: 2.512047290802002 | BCE Loss: 0.993740439414978\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 3.5633296966552734 | KNN Loss: 2.5015714168548584 | BCE Loss: 1.0617583990097046\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 3.5215415954589844 | KNN Loss: 2.484940767288208 | BCE Loss: 1.0366007089614868\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 3.5776546001434326 | KNN Loss: 2.525846481323242 | BCE Loss: 1.0518081188201904\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 3.6060612201690674 | KNN Loss: 2.5449695587158203 | BCE Loss: 1.061091661453247\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 3.610011577606201 | KNN Loss: 2.5676472187042236 | BCE Loss: 1.0423643589019775\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 3.539680004119873 | KNN Loss: 2.526374578475952 | BCE Loss: 1.013305425643921\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 3.5598201751708984 | KNN Loss: 2.5209522247314453 | BCE Loss: 1.0388679504394531\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 3.5290541648864746 | KNN Loss: 2.521347999572754 | BCE Loss: 1.0077062845230103\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 3.5687482357025146 | KNN Loss: 2.533398389816284 | BCE Loss: 1.0353498458862305\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 3.5541415214538574 | KNN Loss: 2.5165231227874756 | BCE Loss: 1.0376182794570923\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 3.5577616691589355 | KNN Loss: 2.5045969486236572 | BCE Loss: 1.0531648397445679\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 3.4964065551757812 | KNN Loss: 2.474560022354126 | BCE Loss: 1.0218466520309448\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 3.526359796524048 | KNN Loss: 2.507861375808716 | BCE Loss: 1.018498420715332\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 3.5463345050811768 | KNN Loss: 2.528480291366577 | BCE Loss: 1.0178542137145996\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 3.513859748840332 | KNN Loss: 2.523695230484009 | BCE Loss: 0.9901644587516785\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 3.4970741271972656 | KNN Loss: 2.4805259704589844 | BCE Loss: 1.0165480375289917\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 3.5171091556549072 | KNN Loss: 2.5243048667907715 | BCE Loss: 0.9928042888641357\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 3.4866209030151367 | KNN Loss: 2.49599027633667 | BCE Loss: 0.9906305074691772\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 3.543405532836914 | KNN Loss: 2.5038371086120605 | BCE Loss: 1.039568543434143\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 3.502415895462036 | KNN Loss: 2.5039632320404053 | BCE Loss: 0.9984526634216309\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 3.5671768188476562 | KNN Loss: 2.5277063846588135 | BCE Loss: 1.0394704341888428\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 3.5118818283081055 | KNN Loss: 2.492358922958374 | BCE Loss: 1.0195229053497314\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 3.515249252319336 | KNN Loss: 2.5000011920928955 | BCE Loss: 1.0152480602264404\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 3.506938934326172 | KNN Loss: 2.474531888961792 | BCE Loss: 1.0324070453643799\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 3.469679594039917 | KNN Loss: 2.4642984867095947 | BCE Loss: 1.0053811073303223\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 3.517627716064453 | KNN Loss: 2.507659435272217 | BCE Loss: 1.0099682807922363\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 3.5310311317443848 | KNN Loss: 2.5135433673858643 | BCE Loss: 1.0174877643585205\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 3.5394227504730225 | KNN Loss: 2.4964282512664795 | BCE Loss: 1.042994499206543\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 3.5440454483032227 | KNN Loss: 2.5073704719543457 | BCE Loss: 1.0366748571395874\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 3.528536319732666 | KNN Loss: 2.49406099319458 | BCE Loss: 1.0344752073287964\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 3.4864068031311035 | KNN Loss: 2.4867873191833496 | BCE Loss: 0.9996194839477539\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 3.5434248447418213 | KNN Loss: 2.500631093978882 | BCE Loss: 1.0427937507629395\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 3.528264284133911 | KNN Loss: 2.52044415473938 | BCE Loss: 1.0078201293945312\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 3.499098300933838 | KNN Loss: 2.5107064247131348 | BCE Loss: 0.9883919358253479\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 3.5251517295837402 | KNN Loss: 2.5156233310699463 | BCE Loss: 1.0095282793045044\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 3.511861562728882 | KNN Loss: 2.4905154705047607 | BCE Loss: 1.021346092224121\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 3.507122278213501 | KNN Loss: 2.4878294467926025 | BCE Loss: 1.0192928314208984\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 3.510202407836914 | KNN Loss: 2.479072093963623 | BCE Loss: 1.031130313873291\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 3.5094146728515625 | KNN Loss: 2.4999618530273438 | BCE Loss: 1.0094528198242188\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 3.527587413787842 | KNN Loss: 2.513017177581787 | BCE Loss: 1.0145702362060547\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 3.5103163719177246 | KNN Loss: 2.503945827484131 | BCE Loss: 1.0063705444335938\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 3.5365748405456543 | KNN Loss: 2.505181312561035 | BCE Loss: 1.0313936471939087\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 3.564465284347534 | KNN Loss: 2.552762269973755 | BCE Loss: 1.0117030143737793\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 3.534134864807129 | KNN Loss: 2.501958131790161 | BCE Loss: 1.0321767330169678\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 3.459144353866577 | KNN Loss: 2.4638028144836426 | BCE Loss: 0.9953415393829346\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 3.554311752319336 | KNN Loss: 2.540005922317505 | BCE Loss: 1.014305830001831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 3.48433780670166 | KNN Loss: 2.487887144088745 | BCE Loss: 0.9964507818222046\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 3.518157720565796 | KNN Loss: 2.4706478118896484 | BCE Loss: 1.0475099086761475\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 3.581376314163208 | KNN Loss: 2.5050547122955322 | BCE Loss: 1.0763216018676758\n",
      "Epoch    86: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 3.4986252784729004 | KNN Loss: 2.470700740814209 | BCE Loss: 1.027924656867981\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 3.5552327632904053 | KNN Loss: 2.530477285385132 | BCE Loss: 1.0247554779052734\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 3.4889910221099854 | KNN Loss: 2.502777576446533 | BCE Loss: 0.9862135052680969\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 3.523919105529785 | KNN Loss: 2.4937007427215576 | BCE Loss: 1.030218243598938\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 3.50435209274292 | KNN Loss: 2.4904232025146484 | BCE Loss: 1.0139288902282715\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 3.5513854026794434 | KNN Loss: 2.5138251781463623 | BCE Loss: 1.0375601053237915\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 3.506169557571411 | KNN Loss: 2.4975879192352295 | BCE Loss: 1.0085816383361816\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 3.500108480453491 | KNN Loss: 2.4819881916046143 | BCE Loss: 1.018120288848877\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 3.5212135314941406 | KNN Loss: 2.5160512924194336 | BCE Loss: 1.0051623582839966\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 3.4946141242980957 | KNN Loss: 2.4855329990386963 | BCE Loss: 1.0090811252593994\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 3.5467090606689453 | KNN Loss: 2.5315067768096924 | BCE Loss: 1.0152021646499634\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 3.5079431533813477 | KNN Loss: 2.48757004737854 | BCE Loss: 1.0203732252120972\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 3.530865430831909 | KNN Loss: 2.491258144378662 | BCE Loss: 1.039607286453247\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 3.532510280609131 | KNN Loss: 2.5010011196136475 | BCE Loss: 1.0315090417861938\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 3.4807212352752686 | KNN Loss: 2.472216844558716 | BCE Loss: 1.0085043907165527\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 3.5794811248779297 | KNN Loss: 2.5505917072296143 | BCE Loss: 1.0288894176483154\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 3.504070520401001 | KNN Loss: 2.4935789108276367 | BCE Loss: 1.0104916095733643\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 3.5658950805664062 | KNN Loss: 2.4826197624206543 | BCE Loss: 1.0832754373550415\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 3.4672653675079346 | KNN Loss: 2.4748446941375732 | BCE Loss: 0.9924206733703613\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 3.5058138370513916 | KNN Loss: 2.447549819946289 | BCE Loss: 1.0582640171051025\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 3.488063335418701 | KNN Loss: 2.485872268676758 | BCE Loss: 1.0021910667419434\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 3.5772266387939453 | KNN Loss: 2.515040874481201 | BCE Loss: 1.0621858835220337\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 3.5400843620300293 | KNN Loss: 2.4929885864257812 | BCE Loss: 1.0470958948135376\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 3.554969072341919 | KNN Loss: 2.5165328979492188 | BCE Loss: 1.0384361743927002\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 3.5593557357788086 | KNN Loss: 2.5279057025909424 | BCE Loss: 1.0314500331878662\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 3.491145133972168 | KNN Loss: 2.509321451187134 | BCE Loss: 0.981823742389679\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 3.524735450744629 | KNN Loss: 2.5116400718688965 | BCE Loss: 1.0130952596664429\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 3.515402317047119 | KNN Loss: 2.5036139488220215 | BCE Loss: 1.0117884874343872\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 3.491116523742676 | KNN Loss: 2.4994165897369385 | BCE Loss: 0.9917000532150269\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 3.5255651473999023 | KNN Loss: 2.5154600143432617 | BCE Loss: 1.0101051330566406\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 3.5608890056610107 | KNN Loss: 2.5047011375427246 | BCE Loss: 1.0561878681182861\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 3.495114326477051 | KNN Loss: 2.4824469089508057 | BCE Loss: 1.0126674175262451\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 3.5256640911102295 | KNN Loss: 2.5193190574645996 | BCE Loss: 1.0063450336456299\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 3.5309205055236816 | KNN Loss: 2.5026161670684814 | BCE Loss: 1.0283043384552002\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 3.5560455322265625 | KNN Loss: 2.5346615314483643 | BCE Loss: 1.0213840007781982\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 3.549325942993164 | KNN Loss: 2.5365958213806152 | BCE Loss: 1.0127302408218384\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 3.514024496078491 | KNN Loss: 2.511303186416626 | BCE Loss: 1.0027213096618652\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 3.548490524291992 | KNN Loss: 2.49306583404541 | BCE Loss: 1.055424690246582\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 3.5171377658843994 | KNN Loss: 2.486468553543091 | BCE Loss: 1.0306692123413086\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 3.521583080291748 | KNN Loss: 2.509554386138916 | BCE Loss: 1.012028694152832\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 3.5472583770751953 | KNN Loss: 2.50193190574646 | BCE Loss: 1.045326590538025\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 3.4798624515533447 | KNN Loss: 2.4966797828674316 | BCE Loss: 0.9831826686859131\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 3.5211968421936035 | KNN Loss: 2.496403932571411 | BCE Loss: 1.0247927904129028\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 3.4888644218444824 | KNN Loss: 2.476569652557373 | BCE Loss: 1.0122946500778198\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 3.522674322128296 | KNN Loss: 2.5132853984832764 | BCE Loss: 1.0093889236450195\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 3.513249397277832 | KNN Loss: 2.488506555557251 | BCE Loss: 1.024742841720581\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 3.508220911026001 | KNN Loss: 2.5045993328094482 | BCE Loss: 1.0036215782165527\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 3.4927470684051514 | KNN Loss: 2.4943294525146484 | BCE Loss: 0.9984176158905029\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 3.5201973915100098 | KNN Loss: 2.4745752811431885 | BCE Loss: 1.0456221103668213\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 3.53953218460083 | KNN Loss: 2.5241408348083496 | BCE Loss: 1.01539146900177\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 3.50791597366333 | KNN Loss: 2.484541416168213 | BCE Loss: 1.0233744382858276\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 3.47715163230896 | KNN Loss: 2.477691888809204 | BCE Loss: 0.9994598031044006\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 3.54013729095459 | KNN Loss: 2.5213961601257324 | BCE Loss: 1.0187411308288574\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 3.4870033264160156 | KNN Loss: 2.4848642349243164 | BCE Loss: 1.0021392107009888\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 3.5338644981384277 | KNN Loss: 2.504575729370117 | BCE Loss: 1.029288649559021\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 3.4890546798706055 | KNN Loss: 2.4634766578674316 | BCE Loss: 1.0255780220031738\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 3.6100568771362305 | KNN Loss: 2.566673755645752 | BCE Loss: 1.043383002281189\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 3.4838953018188477 | KNN Loss: 2.4828929901123047 | BCE Loss: 1.0010024309158325\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 3.51151967048645 | KNN Loss: 2.4897115230560303 | BCE Loss: 1.02180814743042\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 3.534799814224243 | KNN Loss: 2.4975979328155518 | BCE Loss: 1.0372018814086914\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 3.5015709400177 | KNN Loss: 2.487511396408081 | BCE Loss: 1.0140595436096191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 3.5028865337371826 | KNN Loss: 2.476839303970337 | BCE Loss: 1.0260472297668457\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 3.5230791568756104 | KNN Loss: 2.511836290359497 | BCE Loss: 1.0112428665161133\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 3.5682125091552734 | KNN Loss: 2.499727725982666 | BCE Loss: 1.0684847831726074\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 3.4807815551757812 | KNN Loss: 2.4838593006134033 | BCE Loss: 0.9969223737716675\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 3.513824939727783 | KNN Loss: 2.514911651611328 | BCE Loss: 0.9989132881164551\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 3.5171098709106445 | KNN Loss: 2.4811134338378906 | BCE Loss: 1.0359965562820435\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 3.5042455196380615 | KNN Loss: 2.498847007751465 | BCE Loss: 1.0053985118865967\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 3.4921188354492188 | KNN Loss: 2.48435378074646 | BCE Loss: 1.0077650547027588\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 3.5246198177337646 | KNN Loss: 2.4859211444854736 | BCE Loss: 1.038698673248291\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 3.5146634578704834 | KNN Loss: 2.4778881072998047 | BCE Loss: 1.0367753505706787\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 3.5087318420410156 | KNN Loss: 2.513277292251587 | BCE Loss: 0.9954546689987183\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 3.4994680881500244 | KNN Loss: 2.4807708263397217 | BCE Loss: 1.0186972618103027\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 3.5425636768341064 | KNN Loss: 2.5029308795928955 | BCE Loss: 1.039632797241211\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 3.4783143997192383 | KNN Loss: 2.462709665298462 | BCE Loss: 1.015604853630066\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 3.505598545074463 | KNN Loss: 2.492553472518921 | BCE Loss: 1.0130449533462524\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 3.503950595855713 | KNN Loss: 2.4609835147857666 | BCE Loss: 1.0429672002792358\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 3.5295891761779785 | KNN Loss: 2.4798946380615234 | BCE Loss: 1.049694538116455\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 3.4555747509002686 | KNN Loss: 2.4621191024780273 | BCE Loss: 0.9934556484222412\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 3.5158302783966064 | KNN Loss: 2.487391710281372 | BCE Loss: 1.0284385681152344\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 3.5197410583496094 | KNN Loss: 2.5020699501037598 | BCE Loss: 1.0176711082458496\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 3.5066843032836914 | KNN Loss: 2.5001220703125 | BCE Loss: 1.006562352180481\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 3.5175766944885254 | KNN Loss: 2.5275750160217285 | BCE Loss: 0.9900017976760864\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 3.5225205421447754 | KNN Loss: 2.5084457397460938 | BCE Loss: 1.014074683189392\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 3.5151257514953613 | KNN Loss: 2.515364170074463 | BCE Loss: 0.9997614622116089\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 3.512369155883789 | KNN Loss: 2.4910480976104736 | BCE Loss: 1.0213210582733154\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 3.5080556869506836 | KNN Loss: 2.5003960132598877 | BCE Loss: 1.0076595544815063\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 3.508741855621338 | KNN Loss: 2.4932944774627686 | BCE Loss: 1.0154472589492798\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 3.528923511505127 | KNN Loss: 2.4988303184509277 | BCE Loss: 1.0300931930541992\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 3.5212860107421875 | KNN Loss: 2.4792582988739014 | BCE Loss: 1.0420277118682861\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 3.5106301307678223 | KNN Loss: 2.515787363052368 | BCE Loss: 0.9948427677154541\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 3.5206174850463867 | KNN Loss: 2.488495349884033 | BCE Loss: 1.032122015953064\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 3.51448130607605 | KNN Loss: 2.5457253456115723 | BCE Loss: 0.9687559604644775\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 3.5101754665374756 | KNN Loss: 2.4889047145843506 | BCE Loss: 1.021270751953125\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 3.526329278945923 | KNN Loss: 2.488637924194336 | BCE Loss: 1.037691354751587\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 3.5098068714141846 | KNN Loss: 2.503178596496582 | BCE Loss: 1.0066282749176025\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 3.5593764781951904 | KNN Loss: 2.5066637992858887 | BCE Loss: 1.0527126789093018\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 3.539365768432617 | KNN Loss: 2.4945995807647705 | BCE Loss: 1.0447660684585571\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 3.5008368492126465 | KNN Loss: 2.489140272140503 | BCE Loss: 1.011696457862854\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 3.543247938156128 | KNN Loss: 2.5151212215423584 | BCE Loss: 1.0281267166137695\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 3.5026421546936035 | KNN Loss: 2.471872568130493 | BCE Loss: 1.0307695865631104\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 3.551069974899292 | KNN Loss: 2.5145628452301025 | BCE Loss: 1.0365071296691895\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 3.5064103603363037 | KNN Loss: 2.491424560546875 | BCE Loss: 1.0149857997894287\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 3.5141615867614746 | KNN Loss: 2.495635986328125 | BCE Loss: 1.01852548122406\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 3.5151450634002686 | KNN Loss: 2.487150192260742 | BCE Loss: 1.0279948711395264\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 3.534301519393921 | KNN Loss: 2.503978729248047 | BCE Loss: 1.030322790145874\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 3.5108890533447266 | KNN Loss: 2.4706079959869385 | BCE Loss: 1.0402809381484985\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 3.5145740509033203 | KNN Loss: 2.5035560131073 | BCE Loss: 1.01101815700531\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 3.519923210144043 | KNN Loss: 2.5111308097839355 | BCE Loss: 1.0087922811508179\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 3.510167121887207 | KNN Loss: 2.4695749282836914 | BCE Loss: 1.040592074394226\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 3.516728639602661 | KNN Loss: 2.5085654258728027 | BCE Loss: 1.0081632137298584\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 3.5048463344573975 | KNN Loss: 2.48026967048645 | BCE Loss: 1.0245766639709473\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 3.5089545249938965 | KNN Loss: 2.4807751178741455 | BCE Loss: 1.0281795263290405\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 3.502391815185547 | KNN Loss: 2.481461524963379 | BCE Loss: 1.0209301710128784\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 3.5292067527770996 | KNN Loss: 2.5068106651306152 | BCE Loss: 1.0223959684371948\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 3.5247740745544434 | KNN Loss: 2.496016025543213 | BCE Loss: 1.0287580490112305\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 3.516557216644287 | KNN Loss: 2.4942681789398193 | BCE Loss: 1.0222890377044678\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 3.5342462062835693 | KNN Loss: 2.526089906692505 | BCE Loss: 1.0081562995910645\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 3.5519227981567383 | KNN Loss: 2.505000114440918 | BCE Loss: 1.0469226837158203\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 3.502129554748535 | KNN Loss: 2.496377944946289 | BCE Loss: 1.0057514905929565\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 3.474909782409668 | KNN Loss: 2.454946517944336 | BCE Loss: 1.019963264465332\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 3.534031391143799 | KNN Loss: 2.5069656372070312 | BCE Loss: 1.0270658731460571\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 3.5563302040100098 | KNN Loss: 2.5063838958740234 | BCE Loss: 1.0499463081359863\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 3.493411064147949 | KNN Loss: 2.4739127159118652 | BCE Loss: 1.019498348236084\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 3.5286691188812256 | KNN Loss: 2.4978041648864746 | BCE Loss: 1.030864953994751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 3.5105643272399902 | KNN Loss: 2.4997990131378174 | BCE Loss: 1.0107653141021729\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 3.5715267658233643 | KNN Loss: 2.5322704315185547 | BCE Loss: 1.0392563343048096\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 3.523423194885254 | KNN Loss: 2.495837450027466 | BCE Loss: 1.0275858640670776\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 3.507741928100586 | KNN Loss: 2.4925918579101562 | BCE Loss: 1.0151499509811401\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 3.497525691986084 | KNN Loss: 2.4803102016448975 | BCE Loss: 1.017215371131897\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 3.494323492050171 | KNN Loss: 2.477142572402954 | BCE Loss: 1.0171809196472168\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 3.4929041862487793 | KNN Loss: 2.4785025119781494 | BCE Loss: 1.0144016742706299\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 3.5089058876037598 | KNN Loss: 2.495849847793579 | BCE Loss: 1.0130560398101807\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 3.4735107421875 | KNN Loss: 2.4726033210754395 | BCE Loss: 1.0009074211120605\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 3.5184342861175537 | KNN Loss: 2.4784722328186035 | BCE Loss: 1.0399620532989502\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 3.5049057006835938 | KNN Loss: 2.504952907562256 | BCE Loss: 0.9999527335166931\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 3.4833102226257324 | KNN Loss: 2.4900972843170166 | BCE Loss: 0.9932128190994263\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 3.499664068222046 | KNN Loss: 2.4677042961120605 | BCE Loss: 1.0319597721099854\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 3.5419836044311523 | KNN Loss: 2.526195526123047 | BCE Loss: 1.015788197517395\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 3.508256435394287 | KNN Loss: 2.4922611713409424 | BCE Loss: 1.0159952640533447\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 3.4741482734680176 | KNN Loss: 2.4845693111419678 | BCE Loss: 0.9895789623260498\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 3.520857334136963 | KNN Loss: 2.4823355674743652 | BCE Loss: 1.0385217666625977\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 3.476864814758301 | KNN Loss: 2.4521799087524414 | BCE Loss: 1.024685025215149\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 3.5081262588500977 | KNN Loss: 2.484426736831665 | BCE Loss: 1.0236996412277222\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 3.4976859092712402 | KNN Loss: 2.4891912937164307 | BCE Loss: 1.00849449634552\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 3.530683994293213 | KNN Loss: 2.5050530433654785 | BCE Loss: 1.025631070137024\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 3.5132532119750977 | KNN Loss: 2.493976593017578 | BCE Loss: 1.0192766189575195\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 3.5360169410705566 | KNN Loss: 2.5192737579345703 | BCE Loss: 1.0167431831359863\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 3.490294933319092 | KNN Loss: 2.462773323059082 | BCE Loss: 1.0275214910507202\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 3.527489423751831 | KNN Loss: 2.482146978378296 | BCE Loss: 1.0453424453735352\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 3.5711982250213623 | KNN Loss: 2.5283310413360596 | BCE Loss: 1.0428671836853027\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 3.5160927772521973 | KNN Loss: 2.5136642456054688 | BCE Loss: 1.002428412437439\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 3.498161554336548 | KNN Loss: 2.4901413917541504 | BCE Loss: 1.0080201625823975\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 3.5038976669311523 | KNN Loss: 2.504103422164917 | BCE Loss: 0.9997943043708801\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 3.5176494121551514 | KNN Loss: 2.4851291179656982 | BCE Loss: 1.0325202941894531\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 3.508298873901367 | KNN Loss: 2.5062472820281982 | BCE Loss: 1.0020517110824585\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 3.5189290046691895 | KNN Loss: 2.47453236579895 | BCE Loss: 1.0443966388702393\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 3.5641732215881348 | KNN Loss: 2.516085386276245 | BCE Loss: 1.0480878353118896\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 3.5055012702941895 | KNN Loss: 2.474761962890625 | BCE Loss: 1.030739426612854\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 3.5140368938446045 | KNN Loss: 2.5041491985321045 | BCE Loss: 1.0098876953125\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 3.514970302581787 | KNN Loss: 2.4813649654388428 | BCE Loss: 1.0336053371429443\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 3.5229690074920654 | KNN Loss: 2.5027356147766113 | BCE Loss: 1.020233392715454\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 3.512416124343872 | KNN Loss: 2.5030593872070312 | BCE Loss: 1.0093567371368408\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 3.5148279666900635 | KNN Loss: 2.4902586936950684 | BCE Loss: 1.0245692729949951\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 3.5730338096618652 | KNN Loss: 2.5370469093322754 | BCE Loss: 1.0359870195388794\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 3.4991018772125244 | KNN Loss: 2.4848763942718506 | BCE Loss: 1.0142254829406738\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 3.5283145904541016 | KNN Loss: 2.511024236679077 | BCE Loss: 1.017290472984314\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 3.525475025177002 | KNN Loss: 2.5033748149871826 | BCE Loss: 1.0221002101898193\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 3.510988712310791 | KNN Loss: 2.484983444213867 | BCE Loss: 1.0260053873062134\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 3.526912212371826 | KNN Loss: 2.512192964553833 | BCE Loss: 1.0147192478179932\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 3.5246224403381348 | KNN Loss: 2.5053586959838867 | BCE Loss: 1.019263744354248\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 3.535738468170166 | KNN Loss: 2.504465341567993 | BCE Loss: 1.0312730073928833\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 3.5086517333984375 | KNN Loss: 2.4737017154693604 | BCE Loss: 1.0349500179290771\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 3.5065834522247314 | KNN Loss: 2.482419967651367 | BCE Loss: 1.0241634845733643\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 3.5024664402008057 | KNN Loss: 2.499706506729126 | BCE Loss: 1.0027599334716797\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 3.4878532886505127 | KNN Loss: 2.4917731285095215 | BCE Loss: 0.9960801601409912\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 3.487354040145874 | KNN Loss: 2.4506309032440186 | BCE Loss: 1.0367231369018555\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 3.529836416244507 | KNN Loss: 2.528364896774292 | BCE Loss: 1.0014715194702148\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 3.5271286964416504 | KNN Loss: 2.498433828353882 | BCE Loss: 1.0286948680877686\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 3.540491819381714 | KNN Loss: 2.5140068531036377 | BCE Loss: 1.0264849662780762\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 3.4649312496185303 | KNN Loss: 2.4629738330841064 | BCE Loss: 1.0019574165344238\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 3.515113115310669 | KNN Loss: 2.494234800338745 | BCE Loss: 1.0208783149719238\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 3.4779131412506104 | KNN Loss: 2.470759630203247 | BCE Loss: 1.0071535110473633\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 3.493680477142334 | KNN Loss: 2.489300012588501 | BCE Loss: 1.004380464553833\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 3.4740829467773438 | KNN Loss: 2.487105131149292 | BCE Loss: 0.9869778156280518\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 3.468665838241577 | KNN Loss: 2.4677786827087402 | BCE Loss: 1.000887155532837\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 3.496338367462158 | KNN Loss: 2.472214460372925 | BCE Loss: 1.0241237878799438\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 3.4882380962371826 | KNN Loss: 2.478776454925537 | BCE Loss: 1.0094616413116455\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 3.505087375640869 | KNN Loss: 2.48710036277771 | BCE Loss: 1.0179871320724487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 3.527435779571533 | KNN Loss: 2.4842774868011475 | BCE Loss: 1.0431582927703857\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 3.508188009262085 | KNN Loss: 2.5029351711273193 | BCE Loss: 1.0052528381347656\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 3.4684646129608154 | KNN Loss: 2.4497814178466797 | BCE Loss: 1.0186831951141357\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 3.5249102115631104 | KNN Loss: 2.504594564437866 | BCE Loss: 1.0203156471252441\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 3.494647741317749 | KNN Loss: 2.49564790725708 | BCE Loss: 0.9989997744560242\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 3.508951187133789 | KNN Loss: 2.506401777267456 | BCE Loss: 1.002549409866333\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 3.524812698364258 | KNN Loss: 2.4995064735412598 | BCE Loss: 1.025306224822998\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 3.5236291885375977 | KNN Loss: 2.510352611541748 | BCE Loss: 1.0132765769958496\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 3.5055863857269287 | KNN Loss: 2.4701087474823 | BCE Loss: 1.035477638244629\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 3.522095203399658 | KNN Loss: 2.4936962127685547 | BCE Loss: 1.0283989906311035\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 3.509838104248047 | KNN Loss: 2.4938466548919678 | BCE Loss: 1.015991449356079\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 3.487626552581787 | KNN Loss: 2.469231367111206 | BCE Loss: 1.018395185470581\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 3.485180377960205 | KNN Loss: 2.463806390762329 | BCE Loss: 1.021373987197876\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 3.5124399662017822 | KNN Loss: 2.4903745651245117 | BCE Loss: 1.0220654010772705\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 3.4857616424560547 | KNN Loss: 2.476253032684326 | BCE Loss: 1.009508728981018\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 3.524193525314331 | KNN Loss: 2.513913154602051 | BCE Loss: 1.0102803707122803\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 3.55405592918396 | KNN Loss: 2.495866537094116 | BCE Loss: 1.0581893920898438\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 3.4808831214904785 | KNN Loss: 2.476738452911377 | BCE Loss: 1.004144549369812\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 3.4760842323303223 | KNN Loss: 2.4810726642608643 | BCE Loss: 0.9950116872787476\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 3.4989449977874756 | KNN Loss: 2.480848789215088 | BCE Loss: 1.0180962085723877\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 3.5330755710601807 | KNN Loss: 2.4885456562042236 | BCE Loss: 1.044529914855957\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 3.455388307571411 | KNN Loss: 2.4505083560943604 | BCE Loss: 1.0048799514770508\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 3.5357460975646973 | KNN Loss: 2.496387243270874 | BCE Loss: 1.0393588542938232\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 3.524423837661743 | KNN Loss: 2.4994139671325684 | BCE Loss: 1.0250098705291748\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 3.498703956604004 | KNN Loss: 2.4902138710021973 | BCE Loss: 1.0084902048110962\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 3.498605251312256 | KNN Loss: 2.466374397277832 | BCE Loss: 1.0322308540344238\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 3.522176504135132 | KNN Loss: 2.4916892051696777 | BCE Loss: 1.030487298965454\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 3.5491318702697754 | KNN Loss: 2.519306182861328 | BCE Loss: 1.0298256874084473\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 3.5526866912841797 | KNN Loss: 2.5191850662231445 | BCE Loss: 1.0335015058517456\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 3.5093469619750977 | KNN Loss: 2.489015817642212 | BCE Loss: 1.0203312635421753\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 3.5248193740844727 | KNN Loss: 2.5009236335754395 | BCE Loss: 1.0238956212997437\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 3.504948139190674 | KNN Loss: 2.484718084335327 | BCE Loss: 1.0202301740646362\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 3.472797393798828 | KNN Loss: 2.4739773273468018 | BCE Loss: 0.9988200664520264\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 3.479241371154785 | KNN Loss: 2.460527181625366 | BCE Loss: 1.0187140703201294\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 3.492710828781128 | KNN Loss: 2.486668348312378 | BCE Loss: 1.00604248046875\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 3.53100323677063 | KNN Loss: 2.5064356327056885 | BCE Loss: 1.0245676040649414\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 3.5012621879577637 | KNN Loss: 2.476900339126587 | BCE Loss: 1.0243617296218872\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 3.5262386798858643 | KNN Loss: 2.4884939193725586 | BCE Loss: 1.0377447605133057\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 3.511514186859131 | KNN Loss: 2.4970946311950684 | BCE Loss: 1.014419674873352\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 3.502622127532959 | KNN Loss: 2.4717588424682617 | BCE Loss: 1.0308632850646973\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 3.483797073364258 | KNN Loss: 2.4753801822662354 | BCE Loss: 1.0084168910980225\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 3.517306089401245 | KNN Loss: 2.4825222492218018 | BCE Loss: 1.0347838401794434\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 3.4702041149139404 | KNN Loss: 2.460068464279175 | BCE Loss: 1.0101356506347656\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 3.505199909210205 | KNN Loss: 2.488623857498169 | BCE Loss: 1.0165760517120361\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 3.4583306312561035 | KNN Loss: 2.4840264320373535 | BCE Loss: 0.9743042588233948\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 3.4752798080444336 | KNN Loss: 2.4692678451538086 | BCE Loss: 1.0060120820999146\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 3.5514028072357178 | KNN Loss: 2.494891881942749 | BCE Loss: 1.0565109252929688\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 3.53887939453125 | KNN Loss: 2.510690927505493 | BCE Loss: 1.0281884670257568\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 3.53639817237854 | KNN Loss: 2.508343458175659 | BCE Loss: 1.0280547142028809\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 3.5005838871002197 | KNN Loss: 2.492222547531128 | BCE Loss: 1.0083613395690918\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 3.519737482070923 | KNN Loss: 2.4945600032806396 | BCE Loss: 1.0251774787902832\n",
      "Epoch   126: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 3.544055461883545 | KNN Loss: 2.4976747035980225 | BCE Loss: 1.046380639076233\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 3.512068748474121 | KNN Loss: 2.4885449409484863 | BCE Loss: 1.0235236883163452\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 3.5208139419555664 | KNN Loss: 2.499368667602539 | BCE Loss: 1.0214452743530273\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 3.502711296081543 | KNN Loss: 2.4774718284606934 | BCE Loss: 1.0252394676208496\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 3.5151166915893555 | KNN Loss: 2.4958040714263916 | BCE Loss: 1.0193126201629639\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 3.5319161415100098 | KNN Loss: 2.5188398361206055 | BCE Loss: 1.0130761861801147\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 3.4772937297821045 | KNN Loss: 2.4737746715545654 | BCE Loss: 1.003519058227539\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 3.515045404434204 | KNN Loss: 2.494361162185669 | BCE Loss: 1.0206842422485352\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 3.549509286880493 | KNN Loss: 2.5250136852264404 | BCE Loss: 1.0244956016540527\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 3.5212340354919434 | KNN Loss: 2.5088648796081543 | BCE Loss: 1.0123692750930786\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 3.5128040313720703 | KNN Loss: 2.485440731048584 | BCE Loss: 1.0273633003234863\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 3.506516695022583 | KNN Loss: 2.4760241508483887 | BCE Loss: 1.0304925441741943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 3.492321491241455 | KNN Loss: 2.478208303451538 | BCE Loss: 1.014113187789917\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 3.4962241649627686 | KNN Loss: 2.4720571041107178 | BCE Loss: 1.0241670608520508\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 3.4817395210266113 | KNN Loss: 2.474567413330078 | BCE Loss: 1.0071719884872437\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 3.4710018634796143 | KNN Loss: 2.479167938232422 | BCE Loss: 0.9918339252471924\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 3.5352823734283447 | KNN Loss: 2.501796245574951 | BCE Loss: 1.0334861278533936\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 3.4706225395202637 | KNN Loss: 2.4541523456573486 | BCE Loss: 1.0164700746536255\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 3.484872341156006 | KNN Loss: 2.4847629070281982 | BCE Loss: 1.0001094341278076\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 3.499220371246338 | KNN Loss: 2.491783857345581 | BCE Loss: 1.0074366331100464\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 3.4976940155029297 | KNN Loss: 2.4934046268463135 | BCE Loss: 1.0042895078659058\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 3.5026755332946777 | KNN Loss: 2.484614610671997 | BCE Loss: 1.0180609226226807\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 3.5114893913269043 | KNN Loss: 2.5028467178344727 | BCE Loss: 1.008642554283142\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 3.538546323776245 | KNN Loss: 2.507035970687866 | BCE Loss: 1.031510353088379\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 3.5192837715148926 | KNN Loss: 2.4789817333221436 | BCE Loss: 1.040302038192749\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 3.488844156265259 | KNN Loss: 2.4806292057037354 | BCE Loss: 1.0082149505615234\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 3.5392889976501465 | KNN Loss: 2.519261598587036 | BCE Loss: 1.0200275182724\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 3.5308496952056885 | KNN Loss: 2.498532772064209 | BCE Loss: 1.0323169231414795\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 3.4846487045288086 | KNN Loss: 2.4715590476989746 | BCE Loss: 1.013089656829834\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 3.519402265548706 | KNN Loss: 2.4861226081848145 | BCE Loss: 1.0332796573638916\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 3.519636631011963 | KNN Loss: 2.495425224304199 | BCE Loss: 1.0242114067077637\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 3.477766990661621 | KNN Loss: 2.4775965213775635 | BCE Loss: 1.0001705884933472\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 3.5407023429870605 | KNN Loss: 2.496964693069458 | BCE Loss: 1.0437376499176025\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 3.5319430828094482 | KNN Loss: 2.500312328338623 | BCE Loss: 1.0316307544708252\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 3.499208688735962 | KNN Loss: 2.507363796234131 | BCE Loss: 0.991844892501831\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 3.4934334754943848 | KNN Loss: 2.51208233833313 | BCE Loss: 0.9813512563705444\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 3.524745225906372 | KNN Loss: 2.4901580810546875 | BCE Loss: 1.0345871448516846\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 3.4837288856506348 | KNN Loss: 2.4748339653015137 | BCE Loss: 1.008894920349121\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 3.4996442794799805 | KNN Loss: 2.488292932510376 | BCE Loss: 1.011351466178894\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 3.5011377334594727 | KNN Loss: 2.4971375465393066 | BCE Loss: 1.0040000677108765\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 3.5137226581573486 | KNN Loss: 2.4906821250915527 | BCE Loss: 1.023040533065796\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 3.506171941757202 | KNN Loss: 2.506770372390747 | BCE Loss: 0.9994015097618103\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 3.464585304260254 | KNN Loss: 2.462909460067749 | BCE Loss: 1.0016758441925049\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 3.491537570953369 | KNN Loss: 2.4884111881256104 | BCE Loss: 1.0031262636184692\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 3.525827407836914 | KNN Loss: 2.5060431957244873 | BCE Loss: 1.0197840929031372\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 3.4745068550109863 | KNN Loss: 2.4467151165008545 | BCE Loss: 1.0277918577194214\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 3.472656011581421 | KNN Loss: 2.4500937461853027 | BCE Loss: 1.0225622653961182\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 3.500761032104492 | KNN Loss: 2.4833552837371826 | BCE Loss: 1.0174057483673096\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 3.427187919616699 | KNN Loss: 2.450580358505249 | BCE Loss: 0.9766076803207397\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 3.533322334289551 | KNN Loss: 2.4865169525146484 | BCE Loss: 1.046805500984192\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 3.5174784660339355 | KNN Loss: 2.4760284423828125 | BCE Loss: 1.0414501428604126\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 3.556297540664673 | KNN Loss: 2.5259957313537598 | BCE Loss: 1.030301809310913\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 3.5075442790985107 | KNN Loss: 2.4908814430236816 | BCE Loss: 1.016662836074829\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 3.46402645111084 | KNN Loss: 2.471646785736084 | BCE Loss: 0.9923795461654663\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 3.4835665225982666 | KNN Loss: 2.476759433746338 | BCE Loss: 1.0068070888519287\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 3.5064620971679688 | KNN Loss: 2.4831018447875977 | BCE Loss: 1.023360252380371\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 3.5470786094665527 | KNN Loss: 2.5057716369628906 | BCE Loss: 1.0413070917129517\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 3.5025482177734375 | KNN Loss: 2.468848943710327 | BCE Loss: 1.0336991548538208\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 3.523789644241333 | KNN Loss: 2.502119779586792 | BCE Loss: 1.021669864654541\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 3.5133838653564453 | KNN Loss: 2.5056099891662598 | BCE Loss: 1.007773756980896\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 3.470170497894287 | KNN Loss: 2.4505302906036377 | BCE Loss: 1.019640326499939\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 3.5305399894714355 | KNN Loss: 2.5016772747039795 | BCE Loss: 1.0288625955581665\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 3.4555509090423584 | KNN Loss: 2.448577642440796 | BCE Loss: 1.0069732666015625\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 3.4956464767456055 | KNN Loss: 2.484271764755249 | BCE Loss: 1.0113747119903564\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 3.488107204437256 | KNN Loss: 2.4552016258239746 | BCE Loss: 1.0329055786132812\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 3.4754858016967773 | KNN Loss: 2.4624791145324707 | BCE Loss: 1.0130066871643066\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 3.5176825523376465 | KNN Loss: 2.4889516830444336 | BCE Loss: 1.0287307500839233\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 3.552082061767578 | KNN Loss: 2.4933838844299316 | BCE Loss: 1.0586981773376465\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 3.5110881328582764 | KNN Loss: 2.487377882003784 | BCE Loss: 1.0237102508544922\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 3.488314628601074 | KNN Loss: 2.4731695652008057 | BCE Loss: 1.0151450634002686\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 3.4939310550689697 | KNN Loss: 2.4931752681732178 | BCE Loss: 1.000755786895752\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 3.5173561573028564 | KNN Loss: 2.4832561016082764 | BCE Loss: 1.03410005569458\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 3.463313102722168 | KNN Loss: 2.4908292293548584 | BCE Loss: 0.9724838137626648\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 3.4688029289245605 | KNN Loss: 2.469219446182251 | BCE Loss: 0.99958336353302\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 3.490318775177002 | KNN Loss: 2.4851863384246826 | BCE Loss: 1.0051324367523193\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 3.514611005783081 | KNN Loss: 2.4801371097564697 | BCE Loss: 1.0344738960266113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 3.4935483932495117 | KNN Loss: 2.495151996612549 | BCE Loss: 0.9983965158462524\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 3.4936907291412354 | KNN Loss: 2.4574263095855713 | BCE Loss: 1.036264419555664\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 3.500603675842285 | KNN Loss: 2.487312078475952 | BCE Loss: 1.013291597366333\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 3.4797768592834473 | KNN Loss: 2.466181993484497 | BCE Loss: 1.0135948657989502\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 3.5796473026275635 | KNN Loss: 2.538252592086792 | BCE Loss: 1.0413947105407715\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 3.505741596221924 | KNN Loss: 2.4781224727630615 | BCE Loss: 1.0276191234588623\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 3.5333735942840576 | KNN Loss: 2.504798412322998 | BCE Loss: 1.0285751819610596\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 3.4414145946502686 | KNN Loss: 2.4544451236724854 | BCE Loss: 0.9869694113731384\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 3.4890828132629395 | KNN Loss: 2.4754927158355713 | BCE Loss: 1.0135900974273682\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 3.4912142753601074 | KNN Loss: 2.474395990371704 | BCE Loss: 1.0168182849884033\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 3.4996707439422607 | KNN Loss: 2.451443672180176 | BCE Loss: 1.048227071762085\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 3.5031847953796387 | KNN Loss: 2.4999489784240723 | BCE Loss: 1.0032356977462769\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 3.468857765197754 | KNN Loss: 2.469282627105713 | BCE Loss: 0.9995752573013306\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 3.529954433441162 | KNN Loss: 2.4844348430633545 | BCE Loss: 1.0455195903778076\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 3.498584032058716 | KNN Loss: 2.503291130065918 | BCE Loss: 0.9952929615974426\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 3.4813828468322754 | KNN Loss: 2.451906204223633 | BCE Loss: 1.0294767618179321\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 3.5233945846557617 | KNN Loss: 2.4949512481689453 | BCE Loss: 1.028443455696106\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 3.507493019104004 | KNN Loss: 2.4730069637298584 | BCE Loss: 1.0344860553741455\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 3.526776075363159 | KNN Loss: 2.4910786151885986 | BCE Loss: 1.0356974601745605\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 3.5618114471435547 | KNN Loss: 2.483860969543457 | BCE Loss: 1.0779504776000977\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 3.5439558029174805 | KNN Loss: 2.506667375564575 | BCE Loss: 1.0372884273529053\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 3.510014533996582 | KNN Loss: 2.5034260749816895 | BCE Loss: 1.0065885782241821\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 3.5014216899871826 | KNN Loss: 2.4802119731903076 | BCE Loss: 1.021209716796875\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 3.558225154876709 | KNN Loss: 2.515960216522217 | BCE Loss: 1.0422649383544922\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 3.542266845703125 | KNN Loss: 2.530345916748047 | BCE Loss: 1.0119210481643677\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 3.523679256439209 | KNN Loss: 2.4853408336639404 | BCE Loss: 1.0383384227752686\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 3.4849162101745605 | KNN Loss: 2.4776837825775146 | BCE Loss: 1.0072323083877563\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 3.5143613815307617 | KNN Loss: 2.4892420768737793 | BCE Loss: 1.025119423866272\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 3.53914737701416 | KNN Loss: 2.4959022998809814 | BCE Loss: 1.0432450771331787\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 3.474440336227417 | KNN Loss: 2.4617841243743896 | BCE Loss: 1.0126562118530273\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 3.519611120223999 | KNN Loss: 2.500580310821533 | BCE Loss: 1.0190308094024658\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 3.5101757049560547 | KNN Loss: 2.441033124923706 | BCE Loss: 1.069142460823059\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 3.5176331996917725 | KNN Loss: 2.4924604892730713 | BCE Loss: 1.0251727104187012\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 3.519780158996582 | KNN Loss: 2.4844939708709717 | BCE Loss: 1.0352861881256104\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 3.564032793045044 | KNN Loss: 2.5058083534240723 | BCE Loss: 1.0582244396209717\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 3.5362532138824463 | KNN Loss: 2.502974271774292 | BCE Loss: 1.0332789421081543\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 3.4735891819000244 | KNN Loss: 2.4575891494750977 | BCE Loss: 1.0160000324249268\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 3.460160732269287 | KNN Loss: 2.442861795425415 | BCE Loss: 1.017298936843872\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 3.533705949783325 | KNN Loss: 2.486668586730957 | BCE Loss: 1.0470373630523682\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 3.4907541275024414 | KNN Loss: 2.4965832233428955 | BCE Loss: 0.9941707849502563\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 3.502190113067627 | KNN Loss: 2.484912633895874 | BCE Loss: 1.0172775983810425\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 3.5361790657043457 | KNN Loss: 2.49454402923584 | BCE Loss: 1.0416351556777954\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 3.4940099716186523 | KNN Loss: 2.502262830734253 | BCE Loss: 0.9917470216751099\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 3.481224536895752 | KNN Loss: 2.474838972091675 | BCE Loss: 1.0063854455947876\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 3.5073230266571045 | KNN Loss: 2.4655532836914062 | BCE Loss: 1.0417697429656982\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 3.528703451156616 | KNN Loss: 2.5016379356384277 | BCE Loss: 1.0270655155181885\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 3.472322463989258 | KNN Loss: 2.467461109161377 | BCE Loss: 1.0048612356185913\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 3.4959802627563477 | KNN Loss: 2.4732279777526855 | BCE Loss: 1.0227521657943726\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 3.5007779598236084 | KNN Loss: 2.4674620628356934 | BCE Loss: 1.033315896987915\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 3.4909114837646484 | KNN Loss: 2.4786736965179443 | BCE Loss: 1.0122376680374146\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 3.509939432144165 | KNN Loss: 2.4854791164398193 | BCE Loss: 1.0244603157043457\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 3.5544273853302 | KNN Loss: 2.5137107372283936 | BCE Loss: 1.0407166481018066\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 3.5526440143585205 | KNN Loss: 2.498957395553589 | BCE Loss: 1.0536866188049316\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 3.505434989929199 | KNN Loss: 2.487694263458252 | BCE Loss: 1.0177408456802368\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 3.5242156982421875 | KNN Loss: 2.5029966831207275 | BCE Loss: 1.02121901512146\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 3.5204529762268066 | KNN Loss: 2.482661247253418 | BCE Loss: 1.0377916097640991\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 3.506479501724243 | KNN Loss: 2.469036340713501 | BCE Loss: 1.0374431610107422\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 3.5057430267333984 | KNN Loss: 2.466115713119507 | BCE Loss: 1.0396273136138916\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 3.4600865840911865 | KNN Loss: 2.461347818374634 | BCE Loss: 0.9987387657165527\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 3.534785032272339 | KNN Loss: 2.514254570007324 | BCE Loss: 1.0205304622650146\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 3.4730238914489746 | KNN Loss: 2.466587781906128 | BCE Loss: 1.0064361095428467\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 3.488694190979004 | KNN Loss: 2.474543333053589 | BCE Loss: 1.014150857925415\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 3.5113303661346436 | KNN Loss: 2.4937257766723633 | BCE Loss: 1.0176045894622803\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 3.501549243927002 | KNN Loss: 2.4944875240325928 | BCE Loss: 1.0070616006851196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 3.517822265625 | KNN Loss: 2.4781246185302734 | BCE Loss: 1.0396976470947266\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 3.5461506843566895 | KNN Loss: 2.516878366470337 | BCE Loss: 1.0292723178863525\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 3.486924648284912 | KNN Loss: 2.4669923782348633 | BCE Loss: 1.0199322700500488\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 3.4714860916137695 | KNN Loss: 2.476755380630493 | BCE Loss: 0.9947308301925659\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 3.4871912002563477 | KNN Loss: 2.475466012954712 | BCE Loss: 1.0117251873016357\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 3.4891839027404785 | KNN Loss: 2.4775004386901855 | BCE Loss: 1.0116833448410034\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 3.5118227005004883 | KNN Loss: 2.477031707763672 | BCE Loss: 1.0347909927368164\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 3.4957919120788574 | KNN Loss: 2.4792494773864746 | BCE Loss: 1.0165423154830933\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 3.502686023712158 | KNN Loss: 2.4932539463043213 | BCE Loss: 1.009432077407837\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 3.517059087753296 | KNN Loss: 2.4820728302001953 | BCE Loss: 1.0349862575531006\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 3.481247901916504 | KNN Loss: 2.458061695098877 | BCE Loss: 1.0231863260269165\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 3.5208349227905273 | KNN Loss: 2.4864461421966553 | BCE Loss: 1.0343888998031616\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 3.4994394779205322 | KNN Loss: 2.4701130390167236 | BCE Loss: 1.0293264389038086\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 3.4920146465301514 | KNN Loss: 2.4944863319396973 | BCE Loss: 0.9975283145904541\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 3.4836907386779785 | KNN Loss: 2.475782632827759 | BCE Loss: 1.0079079866409302\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 3.5365450382232666 | KNN Loss: 2.496734619140625 | BCE Loss: 1.0398104190826416\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 3.4837441444396973 | KNN Loss: 2.4728243350982666 | BCE Loss: 1.0109198093414307\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 3.528921127319336 | KNN Loss: 2.4782333374023438 | BCE Loss: 1.0506876707077026\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 3.4660959243774414 | KNN Loss: 2.437830686569214 | BCE Loss: 1.0282652378082275\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 3.473539113998413 | KNN Loss: 2.460977554321289 | BCE Loss: 1.012561559677124\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 3.533109664916992 | KNN Loss: 2.4935386180877686 | BCE Loss: 1.0395710468292236\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 3.479114532470703 | KNN Loss: 2.4807002544403076 | BCE Loss: 0.9984142780303955\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 3.5317702293395996 | KNN Loss: 2.504075527191162 | BCE Loss: 1.0276947021484375\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 3.5620241165161133 | KNN Loss: 2.501040458679199 | BCE Loss: 1.060983657836914\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 3.4627785682678223 | KNN Loss: 2.4565324783325195 | BCE Loss: 1.0062460899353027\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 3.4889438152313232 | KNN Loss: 2.4573991298675537 | BCE Loss: 1.0315446853637695\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 3.48714017868042 | KNN Loss: 2.4716875553131104 | BCE Loss: 1.0154526233673096\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 3.4725899696350098 | KNN Loss: 2.4706003665924072 | BCE Loss: 1.0019896030426025\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 3.5262651443481445 | KNN Loss: 2.489861488342285 | BCE Loss: 1.0364036560058594\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 3.5150327682495117 | KNN Loss: 2.508241653442383 | BCE Loss: 1.0067909955978394\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 3.4591317176818848 | KNN Loss: 2.45873761177063 | BCE Loss: 1.0003942251205444\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 3.5434813499450684 | KNN Loss: 2.504565715789795 | BCE Loss: 1.0389155149459839\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 3.4573984146118164 | KNN Loss: 2.4647934436798096 | BCE Loss: 0.9926050305366516\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 3.4780964851379395 | KNN Loss: 2.463627576828003 | BCE Loss: 1.0144689083099365\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 3.490582227706909 | KNN Loss: 2.4724481105804443 | BCE Loss: 1.0181341171264648\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 3.492359161376953 | KNN Loss: 2.4607579708099365 | BCE Loss: 1.0316013097763062\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 3.506361484527588 | KNN Loss: 2.504826068878174 | BCE Loss: 1.0015355348587036\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 3.4946482181549072 | KNN Loss: 2.4785783290863037 | BCE Loss: 1.0160698890686035\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 3.5250792503356934 | KNN Loss: 2.4912173748016357 | BCE Loss: 1.0338618755340576\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 3.484221935272217 | KNN Loss: 2.46280574798584 | BCE Loss: 1.021416187286377\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 3.5253145694732666 | KNN Loss: 2.487579345703125 | BCE Loss: 1.0377352237701416\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 3.48270320892334 | KNN Loss: 2.4732580184936523 | BCE Loss: 1.009445309638977\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 3.5085959434509277 | KNN Loss: 2.5137951374053955 | BCE Loss: 0.9948008060455322\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 3.5238304138183594 | KNN Loss: 2.4983415603637695 | BCE Loss: 1.0254887342453003\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 3.4950528144836426 | KNN Loss: 2.490358352661133 | BCE Loss: 1.0046944618225098\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 3.4785475730895996 | KNN Loss: 2.461575984954834 | BCE Loss: 1.0169717073440552\n",
      "Epoch   157: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 3.572903871536255 | KNN Loss: 2.5347163677215576 | BCE Loss: 1.0381875038146973\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 3.52877140045166 | KNN Loss: 2.4867653846740723 | BCE Loss: 1.042006015777588\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 3.529218912124634 | KNN Loss: 2.5340046882629395 | BCE Loss: 0.9952142238616943\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 3.5186336040496826 | KNN Loss: 2.500664472579956 | BCE Loss: 1.0179691314697266\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 3.5340933799743652 | KNN Loss: 2.480210065841675 | BCE Loss: 1.05388343334198\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 3.483102321624756 | KNN Loss: 2.456369400024414 | BCE Loss: 1.0267328023910522\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 3.4818313121795654 | KNN Loss: 2.470309019088745 | BCE Loss: 1.0115222930908203\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 3.504579544067383 | KNN Loss: 2.4652044773101807 | BCE Loss: 1.0393749475479126\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 3.5506420135498047 | KNN Loss: 2.517777442932129 | BCE Loss: 1.0328644514083862\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 3.493192195892334 | KNN Loss: 2.464705228805542 | BCE Loss: 1.028486967086792\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 3.464646816253662 | KNN Loss: 2.468421459197998 | BCE Loss: 0.9962252378463745\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 3.518129348754883 | KNN Loss: 2.4855613708496094 | BCE Loss: 1.0325679779052734\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 3.4652163982391357 | KNN Loss: 2.4551873207092285 | BCE Loss: 1.0100290775299072\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 3.5030901432037354 | KNN Loss: 2.5121359825134277 | BCE Loss: 0.9909541606903076\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 3.495077133178711 | KNN Loss: 2.482196807861328 | BCE Loss: 1.0128802061080933\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 3.499213457107544 | KNN Loss: 2.466622829437256 | BCE Loss: 1.032590627670288\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 3.5536766052246094 | KNN Loss: 2.523831605911255 | BCE Loss: 1.029845118522644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 3.5687875747680664 | KNN Loss: 2.5259628295898438 | BCE Loss: 1.0428248643875122\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 3.4703948497772217 | KNN Loss: 2.4795725345611572 | BCE Loss: 0.9908223748207092\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 3.502793073654175 | KNN Loss: 2.482849597930908 | BCE Loss: 1.0199434757232666\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 3.480257034301758 | KNN Loss: 2.479675531387329 | BCE Loss: 1.0005815029144287\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 3.538160562515259 | KNN Loss: 2.5098910331726074 | BCE Loss: 1.0282695293426514\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 3.4917283058166504 | KNN Loss: 2.489123821258545 | BCE Loss: 1.002604603767395\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 3.4330670833587646 | KNN Loss: 2.451557159423828 | BCE Loss: 0.9815099239349365\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 3.5235633850097656 | KNN Loss: 2.495378255844116 | BCE Loss: 1.0281851291656494\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 3.4929862022399902 | KNN Loss: 2.4724552631378174 | BCE Loss: 1.0205309391021729\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 3.4882116317749023 | KNN Loss: 2.4686028957366943 | BCE Loss: 1.0196086168289185\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 3.4901609420776367 | KNN Loss: 2.4804749488830566 | BCE Loss: 1.00968599319458\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 3.4952268600463867 | KNN Loss: 2.4733457565307617 | BCE Loss: 1.021881103515625\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 3.475248336791992 | KNN Loss: 2.4680092334747314 | BCE Loss: 1.0072392225265503\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 3.477151870727539 | KNN Loss: 2.4540069103240967 | BCE Loss: 1.0231449604034424\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 3.509084939956665 | KNN Loss: 2.4665112495422363 | BCE Loss: 1.0425736904144287\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 3.4923014640808105 | KNN Loss: 2.473325729370117 | BCE Loss: 1.0189757347106934\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 3.485344171524048 | KNN Loss: 2.478085517883301 | BCE Loss: 1.007258653640747\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 3.5093350410461426 | KNN Loss: 2.5070455074310303 | BCE Loss: 1.0022895336151123\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 3.517367362976074 | KNN Loss: 2.476059913635254 | BCE Loss: 1.0413074493408203\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 3.553290367126465 | KNN Loss: 2.497781753540039 | BCE Loss: 1.0555086135864258\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 3.513817548751831 | KNN Loss: 2.498582124710083 | BCE Loss: 1.015235424041748\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 3.5109386444091797 | KNN Loss: 2.481379985809326 | BCE Loss: 1.029558777809143\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 3.5392088890075684 | KNN Loss: 2.479701280593872 | BCE Loss: 1.0595074892044067\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 3.511444568634033 | KNN Loss: 2.4832687377929688 | BCE Loss: 1.0281758308410645\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 3.490935802459717 | KNN Loss: 2.4804158210754395 | BCE Loss: 1.010520100593567\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 3.507110118865967 | KNN Loss: 2.4857561588287354 | BCE Loss: 1.021353840827942\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 3.4900999069213867 | KNN Loss: 2.476856231689453 | BCE Loss: 1.013243556022644\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 3.514467239379883 | KNN Loss: 2.5048840045928955 | BCE Loss: 1.0095832347869873\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 3.4920217990875244 | KNN Loss: 2.4662246704101562 | BCE Loss: 1.0257971286773682\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 3.4883198738098145 | KNN Loss: 2.460357427597046 | BCE Loss: 1.027962565422058\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 3.52496337890625 | KNN Loss: 2.498319625854492 | BCE Loss: 1.0266437530517578\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 3.5046892166137695 | KNN Loss: 2.509598970413208 | BCE Loss: 0.9950903654098511\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 3.4734132289886475 | KNN Loss: 2.4435598850250244 | BCE Loss: 1.029853343963623\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 3.5181543827056885 | KNN Loss: 2.5158677101135254 | BCE Loss: 1.002286672592163\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 3.5034232139587402 | KNN Loss: 2.4851696491241455 | BCE Loss: 1.0182535648345947\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 3.573770761489868 | KNN Loss: 2.520463705062866 | BCE Loss: 1.053307056427002\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 3.4649014472961426 | KNN Loss: 2.457937240600586 | BCE Loss: 1.0069642066955566\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 3.463148832321167 | KNN Loss: 2.4454140663146973 | BCE Loss: 1.0177347660064697\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 3.568307638168335 | KNN Loss: 2.5123636722564697 | BCE Loss: 1.0559439659118652\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 3.485936403274536 | KNN Loss: 2.4902918338775635 | BCE Loss: 0.9956446290016174\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 3.4960930347442627 | KNN Loss: 2.480290174484253 | BCE Loss: 1.0158028602600098\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 3.520962953567505 | KNN Loss: 2.4937851428985596 | BCE Loss: 1.0271778106689453\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 3.5249292850494385 | KNN Loss: 2.488121271133423 | BCE Loss: 1.0368080139160156\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 3.503232479095459 | KNN Loss: 2.4788522720336914 | BCE Loss: 1.0243802070617676\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 3.5014970302581787 | KNN Loss: 2.482592821121216 | BCE Loss: 1.018904209136963\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 3.46865177154541 | KNN Loss: 2.467803716659546 | BCE Loss: 1.0008480548858643\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 3.5083963871002197 | KNN Loss: 2.4858651161193848 | BCE Loss: 1.022531270980835\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 3.495990753173828 | KNN Loss: 2.471250534057617 | BCE Loss: 1.0247403383255005\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 3.4785752296447754 | KNN Loss: 2.4822394847869873 | BCE Loss: 0.9963357448577881\n",
      "Epoch   168: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 3.46770977973938 | KNN Loss: 2.4628708362579346 | BCE Loss: 1.0048389434814453\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 3.499164581298828 | KNN Loss: 2.4768059253692627 | BCE Loss: 1.0223586559295654\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 3.481182336807251 | KNN Loss: 2.493215560913086 | BCE Loss: 0.9879668354988098\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 3.485374927520752 | KNN Loss: 2.4547953605651855 | BCE Loss: 1.0305795669555664\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 3.4756929874420166 | KNN Loss: 2.463573932647705 | BCE Loss: 1.0121190547943115\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 3.5227677822113037 | KNN Loss: 2.473979949951172 | BCE Loss: 1.0487878322601318\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 3.4981002807617188 | KNN Loss: 2.4521913528442383 | BCE Loss: 1.0459089279174805\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 3.4853549003601074 | KNN Loss: 2.476760149002075 | BCE Loss: 1.0085947513580322\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 3.5234293937683105 | KNN Loss: 2.4948673248291016 | BCE Loss: 1.028562068939209\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 3.4746909141540527 | KNN Loss: 2.461747407913208 | BCE Loss: 1.0129436254501343\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 3.4475345611572266 | KNN Loss: 2.4500343799591064 | BCE Loss: 0.9975002408027649\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 3.4905972480773926 | KNN Loss: 2.479931592941284 | BCE Loss: 1.0106655359268188\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 3.5297956466674805 | KNN Loss: 2.5135674476623535 | BCE Loss: 1.016228199005127\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 3.540431261062622 | KNN Loss: 2.5071864128112793 | BCE Loss: 1.0332448482513428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 3.492882251739502 | KNN Loss: 2.4751317501068115 | BCE Loss: 1.0177505016326904\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 3.510105609893799 | KNN Loss: 2.4771065711975098 | BCE Loss: 1.032999038696289\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 3.464184522628784 | KNN Loss: 2.48067045211792 | BCE Loss: 0.9835140705108643\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 3.5360872745513916 | KNN Loss: 2.5044565200805664 | BCE Loss: 1.0316307544708252\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 3.4814095497131348 | KNN Loss: 2.4773924350738525 | BCE Loss: 1.0040171146392822\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 3.5272955894470215 | KNN Loss: 2.501317262649536 | BCE Loss: 1.0259783267974854\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 3.5181872844696045 | KNN Loss: 2.4573137760162354 | BCE Loss: 1.0608735084533691\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 3.4752697944641113 | KNN Loss: 2.467348098754883 | BCE Loss: 1.007921576499939\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 3.477029323577881 | KNN Loss: 2.471937656402588 | BCE Loss: 1.0050917863845825\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 3.5082931518554688 | KNN Loss: 2.4812543392181396 | BCE Loss: 1.027038812637329\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 3.4818930625915527 | KNN Loss: 2.4856646060943604 | BCE Loss: 0.9962285757064819\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 3.481393814086914 | KNN Loss: 2.474769115447998 | BCE Loss: 1.006624698638916\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 3.5111148357391357 | KNN Loss: 2.4875335693359375 | BCE Loss: 1.0235812664031982\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 3.479252338409424 | KNN Loss: 2.462752103805542 | BCE Loss: 1.0165002346038818\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 3.50461745262146 | KNN Loss: 2.483759641647339 | BCE Loss: 1.020857810974121\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 3.469913959503174 | KNN Loss: 2.4702634811401367 | BCE Loss: 0.9996503591537476\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 3.501772880554199 | KNN Loss: 2.498689889907837 | BCE Loss: 1.0030831098556519\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 3.51686692237854 | KNN Loss: 2.484828472137451 | BCE Loss: 1.0320384502410889\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 3.4688475131988525 | KNN Loss: 2.4513328075408936 | BCE Loss: 1.017514705657959\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 3.4598379135131836 | KNN Loss: 2.455799102783203 | BCE Loss: 1.00403892993927\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 3.487551689147949 | KNN Loss: 2.4470374584198 | BCE Loss: 1.0405142307281494\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 3.472468852996826 | KNN Loss: 2.4624063968658447 | BCE Loss: 1.010062575340271\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 3.5390560626983643 | KNN Loss: 2.465914487838745 | BCE Loss: 1.0731415748596191\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 3.552288055419922 | KNN Loss: 2.4933478832244873 | BCE Loss: 1.058940052986145\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 3.4979236125946045 | KNN Loss: 2.4716808795928955 | BCE Loss: 1.026242733001709\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 3.4922614097595215 | KNN Loss: 2.4952125549316406 | BCE Loss: 0.9970489740371704\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 3.499124050140381 | KNN Loss: 2.479468822479248 | BCE Loss: 1.0196552276611328\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 3.4685869216918945 | KNN Loss: 2.4676902294158936 | BCE Loss: 1.0008968114852905\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 3.514444351196289 | KNN Loss: 2.484072685241699 | BCE Loss: 1.0303716659545898\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 3.5055770874023438 | KNN Loss: 2.4817028045654297 | BCE Loss: 1.023874282836914\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 3.4709041118621826 | KNN Loss: 2.473816394805908 | BCE Loss: 0.9970877170562744\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 3.5309925079345703 | KNN Loss: 2.510120391845703 | BCE Loss: 1.0208722352981567\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 3.488844394683838 | KNN Loss: 2.4837591648101807 | BCE Loss: 1.0050852298736572\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 3.4988222122192383 | KNN Loss: 2.473412036895752 | BCE Loss: 1.0254101753234863\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 3.5187907218933105 | KNN Loss: 2.478550672531128 | BCE Loss: 1.0402400493621826\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 3.501798629760742 | KNN Loss: 2.480077028274536 | BCE Loss: 1.021721601486206\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 3.4926416873931885 | KNN Loss: 2.481401205062866 | BCE Loss: 1.0112404823303223\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 3.494351863861084 | KNN Loss: 2.4704344272613525 | BCE Loss: 1.023917555809021\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 3.4615960121154785 | KNN Loss: 2.455878973007202 | BCE Loss: 1.0057170391082764\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 3.4758782386779785 | KNN Loss: 2.4833202362060547 | BCE Loss: 0.9925580024719238\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 3.4528493881225586 | KNN Loss: 2.4671239852905273 | BCE Loss: 0.985725462436676\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 3.471393585205078 | KNN Loss: 2.434659719467163 | BCE Loss: 1.0367337465286255\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 3.4893908500671387 | KNN Loss: 2.4789156913757324 | BCE Loss: 1.0104752779006958\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 3.5214734077453613 | KNN Loss: 2.4774415493011475 | BCE Loss: 1.0440318584442139\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 3.5545127391815186 | KNN Loss: 2.516101360321045 | BCE Loss: 1.0384113788604736\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 3.5075299739837646 | KNN Loss: 2.457580804824829 | BCE Loss: 1.0499491691589355\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 3.491093635559082 | KNN Loss: 2.4768810272216797 | BCE Loss: 1.0142126083374023\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 3.4644176959991455 | KNN Loss: 2.4583444595336914 | BCE Loss: 1.006073236465454\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 3.4659457206726074 | KNN Loss: 2.4641129970550537 | BCE Loss: 1.0018328428268433\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 3.4910359382629395 | KNN Loss: 2.4774038791656494 | BCE Loss: 1.01363205909729\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 3.49226713180542 | KNN Loss: 2.485088348388672 | BCE Loss: 1.007178783416748\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 3.5090901851654053 | KNN Loss: 2.504420518875122 | BCE Loss: 1.0046696662902832\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 3.5211024284362793 | KNN Loss: 2.4957573413848877 | BCE Loss: 1.0253452062606812\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 3.4772207736968994 | KNN Loss: 2.472626209259033 | BCE Loss: 1.0045945644378662\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 3.4893836975097656 | KNN Loss: 2.4751875400543213 | BCE Loss: 1.0141961574554443\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 3.4642279148101807 | KNN Loss: 2.470968246459961 | BCE Loss: 0.9932596683502197\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 3.498777389526367 | KNN Loss: 2.472608804702759 | BCE Loss: 1.0261684656143188\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 3.5301315784454346 | KNN Loss: 2.4988207817077637 | BCE Loss: 1.031310796737671\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 3.519951820373535 | KNN Loss: 2.4720897674560547 | BCE Loss: 1.0478620529174805\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 3.5662670135498047 | KNN Loss: 2.5212855339050293 | BCE Loss: 1.0449813604354858\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 3.4871108531951904 | KNN Loss: 2.4662275314331055 | BCE Loss: 1.020883321762085\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 3.5058748722076416 | KNN Loss: 2.5189597606658936 | BCE Loss: 0.9869150519371033\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 3.49300479888916 | KNN Loss: 2.4655184745788574 | BCE Loss: 1.0274862051010132\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 3.4917383193969727 | KNN Loss: 2.4817535877227783 | BCE Loss: 1.0099846124649048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 3.477966785430908 | KNN Loss: 2.4622085094451904 | BCE Loss: 1.0157583951950073\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 3.507354259490967 | KNN Loss: 2.4803056716918945 | BCE Loss: 1.0270485877990723\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 3.494075298309326 | KNN Loss: 2.50473952293396 | BCE Loss: 0.9893357157707214\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 3.4678003787994385 | KNN Loss: 2.4632842540740967 | BCE Loss: 1.0045161247253418\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 3.461928367614746 | KNN Loss: 2.43854022026062 | BCE Loss: 1.0233880281448364\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 3.5018322467803955 | KNN Loss: 2.491797924041748 | BCE Loss: 1.0100343227386475\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 3.5308685302734375 | KNN Loss: 2.4839329719543457 | BCE Loss: 1.0469355583190918\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 3.4266469478607178 | KNN Loss: 2.4288527965545654 | BCE Loss: 0.9977940917015076\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 3.5364465713500977 | KNN Loss: 2.480576753616333 | BCE Loss: 1.0558699369430542\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 3.5148491859436035 | KNN Loss: 2.4832465648651123 | BCE Loss: 1.0316027402877808\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 3.4773709774017334 | KNN Loss: 2.452028512954712 | BCE Loss: 1.0253424644470215\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 3.489259719848633 | KNN Loss: 2.4658095836639404 | BCE Loss: 1.0234501361846924\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 3.42207670211792 | KNN Loss: 2.444096803665161 | BCE Loss: 0.9779798984527588\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 3.4764363765716553 | KNN Loss: 2.446604013442993 | BCE Loss: 1.029832363128662\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 3.4559855461120605 | KNN Loss: 2.4528372287750244 | BCE Loss: 1.0031483173370361\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 3.446058511734009 | KNN Loss: 2.4606683254241943 | BCE Loss: 0.9853901863098145\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 3.528799295425415 | KNN Loss: 2.506941556930542 | BCE Loss: 1.021857738494873\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 3.4480011463165283 | KNN Loss: 2.4666390419006348 | BCE Loss: 0.9813621044158936\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 3.511958360671997 | KNN Loss: 2.503049850463867 | BCE Loss: 1.0089085102081299\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 3.5250072479248047 | KNN Loss: 2.511941432952881 | BCE Loss: 1.0130658149719238\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 3.5257270336151123 | KNN Loss: 2.462836265563965 | BCE Loss: 1.0628907680511475\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 3.5195884704589844 | KNN Loss: 2.481576681137085 | BCE Loss: 1.0380117893218994\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 3.44769287109375 | KNN Loss: 2.45340895652771 | BCE Loss: 0.9942840337753296\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 3.5063467025756836 | KNN Loss: 2.483626365661621 | BCE Loss: 1.022720217704773\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 3.4717559814453125 | KNN Loss: 2.4524412155151367 | BCE Loss: 1.0193147659301758\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 3.483302116394043 | KNN Loss: 2.4771227836608887 | BCE Loss: 1.0061794519424438\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 3.5108983516693115 | KNN Loss: 2.5069336891174316 | BCE Loss: 1.0039646625518799\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 3.4535410404205322 | KNN Loss: 2.4551327228546143 | BCE Loss: 0.998408317565918\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 3.483829975128174 | KNN Loss: 2.473088502883911 | BCE Loss: 1.0107414722442627\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 3.4716670513153076 | KNN Loss: 2.475024700164795 | BCE Loss: 0.9966423511505127\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 3.4767775535583496 | KNN Loss: 2.4763779640197754 | BCE Loss: 1.0003995895385742\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 3.4982619285583496 | KNN Loss: 2.481593608856201 | BCE Loss: 1.0166683197021484\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 3.500033378601074 | KNN Loss: 2.4713234901428223 | BCE Loss: 1.028709888458252\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 3.4866645336151123 | KNN Loss: 2.4622466564178467 | BCE Loss: 1.0244178771972656\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 3.493044376373291 | KNN Loss: 2.4972031116485596 | BCE Loss: 0.995841383934021\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 3.449608325958252 | KNN Loss: 2.436378240585327 | BCE Loss: 1.0132299661636353\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 3.449721336364746 | KNN Loss: 2.438918113708496 | BCE Loss: 1.01080322265625\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 3.483203887939453 | KNN Loss: 2.4805407524108887 | BCE Loss: 1.002663254737854\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 3.496079206466675 | KNN Loss: 2.4737374782562256 | BCE Loss: 1.0223417282104492\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 3.518220901489258 | KNN Loss: 2.5070226192474365 | BCE Loss: 1.0111984014511108\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 3.463754177093506 | KNN Loss: 2.4626352787017822 | BCE Loss: 1.0011188983917236\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 3.5001556873321533 | KNN Loss: 2.469804048538208 | BCE Loss: 1.0303516387939453\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 3.504105806350708 | KNN Loss: 2.4834036827087402 | BCE Loss: 1.0207021236419678\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 3.498575210571289 | KNN Loss: 2.4649713039398193 | BCE Loss: 1.0336039066314697\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 3.5437440872192383 | KNN Loss: 2.5036659240722656 | BCE Loss: 1.040078043937683\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 3.4721343517303467 | KNN Loss: 2.4774749279022217 | BCE Loss: 0.9946594834327698\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 3.4703783988952637 | KNN Loss: 2.4680733680725098 | BCE Loss: 1.002305030822754\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 3.4838075637817383 | KNN Loss: 2.480455160140991 | BCE Loss: 1.0033525228500366\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 3.471085548400879 | KNN Loss: 2.472503423690796 | BCE Loss: 0.9985822439193726\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 3.479020118713379 | KNN Loss: 2.454859495162964 | BCE Loss: 1.024160623550415\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 3.511775493621826 | KNN Loss: 2.4676578044891357 | BCE Loss: 1.0441175699234009\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 3.500467300415039 | KNN Loss: 2.485623598098755 | BCE Loss: 1.0148437023162842\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 3.508488416671753 | KNN Loss: 2.486640453338623 | BCE Loss: 1.0218479633331299\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 3.4698715209960938 | KNN Loss: 2.4704689979553223 | BCE Loss: 0.999402642250061\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 3.4945807456970215 | KNN Loss: 2.494311809539795 | BCE Loss: 1.0002689361572266\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 3.500054359436035 | KNN Loss: 2.472508192062378 | BCE Loss: 1.0275461673736572\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 3.4987640380859375 | KNN Loss: 2.4534099102020264 | BCE Loss: 1.0453541278839111\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 3.507913827896118 | KNN Loss: 2.4716203212738037 | BCE Loss: 1.0362935066223145\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 3.4530577659606934 | KNN Loss: 2.470438003540039 | BCE Loss: 0.9826198816299438\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 3.491910934448242 | KNN Loss: 2.4763505458831787 | BCE Loss: 1.0155603885650635\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 3.5065174102783203 | KNN Loss: 2.472088575363159 | BCE Loss: 1.0344288349151611\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 3.5424718856811523 | KNN Loss: 2.502443552017212 | BCE Loss: 1.0400282144546509\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 3.495370864868164 | KNN Loss: 2.4693093299865723 | BCE Loss: 1.0260616540908813\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 3.474834680557251 | KNN Loss: 2.4513769149780273 | BCE Loss: 1.0234577655792236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 3.5003325939178467 | KNN Loss: 2.4927446842193604 | BCE Loss: 1.0075879096984863\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 3.500047206878662 | KNN Loss: 2.4765772819519043 | BCE Loss: 1.0234699249267578\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 3.47090482711792 | KNN Loss: 2.4573416709899902 | BCE Loss: 1.0135632753372192\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 3.471834182739258 | KNN Loss: 2.464101791381836 | BCE Loss: 1.0077323913574219\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 3.5221688747406006 | KNN Loss: 2.4798760414123535 | BCE Loss: 1.042292833328247\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 3.4889001846313477 | KNN Loss: 2.452802896499634 | BCE Loss: 1.0360972881317139\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 3.4980108737945557 | KNN Loss: 2.470329761505127 | BCE Loss: 1.0276811122894287\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 3.499168634414673 | KNN Loss: 2.4761974811553955 | BCE Loss: 1.0229711532592773\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 3.493264675140381 | KNN Loss: 2.4933629035949707 | BCE Loss: 0.9999016523361206\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 3.5344889163970947 | KNN Loss: 2.5156025886535645 | BCE Loss: 1.0188863277435303\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 3.493619441986084 | KNN Loss: 2.4978103637695312 | BCE Loss: 0.995809018611908\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 3.469827651977539 | KNN Loss: 2.456716537475586 | BCE Loss: 1.0131112337112427\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 3.5042409896850586 | KNN Loss: 2.45670747756958 | BCE Loss: 1.0475335121154785\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 3.4846112728118896 | KNN Loss: 2.469528913497925 | BCE Loss: 1.0150823593139648\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 3.4388270378112793 | KNN Loss: 2.444418430328369 | BCE Loss: 0.9944084882736206\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 3.527863025665283 | KNN Loss: 2.4799060821533203 | BCE Loss: 1.0479570627212524\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 3.5079052448272705 | KNN Loss: 2.4839706420898438 | BCE Loss: 1.0239346027374268\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 3.507338762283325 | KNN Loss: 2.478325605392456 | BCE Loss: 1.0290131568908691\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 3.4761414527893066 | KNN Loss: 2.4691531658172607 | BCE Loss: 1.0069884061813354\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 3.502885341644287 | KNN Loss: 2.477019786834717 | BCE Loss: 1.0258655548095703\n",
      "Epoch   195: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 3.4235267639160156 | KNN Loss: 2.445385217666626 | BCE Loss: 0.9781414866447449\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 3.4997079372406006 | KNN Loss: 2.489323616027832 | BCE Loss: 1.0103843212127686\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 3.4920105934143066 | KNN Loss: 2.4522135257720947 | BCE Loss: 1.039797067642212\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 3.4699981212615967 | KNN Loss: 2.447903633117676 | BCE Loss: 1.022094488143921\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 3.486314058303833 | KNN Loss: 2.4591922760009766 | BCE Loss: 1.0271217823028564\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 3.529568672180176 | KNN Loss: 2.5024709701538086 | BCE Loss: 1.0270977020263672\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 3.504674196243286 | KNN Loss: 2.474271059036255 | BCE Loss: 1.0304031372070312\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 3.4251558780670166 | KNN Loss: 2.457300901412964 | BCE Loss: 0.967854917049408\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 3.480287551879883 | KNN Loss: 2.4502627849578857 | BCE Loss: 1.030024766921997\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 3.485832691192627 | KNN Loss: 2.4766478538513184 | BCE Loss: 1.009184718132019\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 3.533155918121338 | KNN Loss: 2.498250722885132 | BCE Loss: 1.034905195236206\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 3.474943161010742 | KNN Loss: 2.46810245513916 | BCE Loss: 1.006840705871582\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 3.483427047729492 | KNN Loss: 2.465756416320801 | BCE Loss: 1.017670750617981\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 3.4593114852905273 | KNN Loss: 2.460768699645996 | BCE Loss: 0.9985426664352417\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 3.5141987800598145 | KNN Loss: 2.492325782775879 | BCE Loss: 1.021872878074646\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 3.4855666160583496 | KNN Loss: 2.490553617477417 | BCE Loss: 0.9950128793716431\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 3.4880409240722656 | KNN Loss: 2.4551339149475098 | BCE Loss: 1.0329070091247559\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 3.5101847648620605 | KNN Loss: 2.491208791732788 | BCE Loss: 1.018976092338562\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 3.4936234951019287 | KNN Loss: 2.4678494930267334 | BCE Loss: 1.0257740020751953\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 3.500156879425049 | KNN Loss: 2.4750070571899414 | BCE Loss: 1.025149941444397\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 3.4809305667877197 | KNN Loss: 2.4591622352600098 | BCE Loss: 1.02176833152771\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 3.497917652130127 | KNN Loss: 2.4802587032318115 | BCE Loss: 1.0176589488983154\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 3.477799415588379 | KNN Loss: 2.4861583709716797 | BCE Loss: 0.9916409850120544\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 3.48073673248291 | KNN Loss: 2.4629037380218506 | BCE Loss: 1.0178329944610596\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 3.512097120285034 | KNN Loss: 2.4935550689697266 | BCE Loss: 1.0185420513153076\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 3.456453800201416 | KNN Loss: 2.457681655883789 | BCE Loss: 0.9987722635269165\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 3.4913864135742188 | KNN Loss: 2.4952938556671143 | BCE Loss: 0.996092677116394\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 3.437384843826294 | KNN Loss: 2.4341013431549072 | BCE Loss: 1.0032835006713867\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 3.4793484210968018 | KNN Loss: 2.459451675415039 | BCE Loss: 1.0198967456817627\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 3.474520206451416 | KNN Loss: 2.482062816619873 | BCE Loss: 0.992457389831543\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 3.46159029006958 | KNN Loss: 2.4458022117614746 | BCE Loss: 1.0157880783081055\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 3.4593262672424316 | KNN Loss: 2.4494924545288086 | BCE Loss: 1.009833812713623\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 3.4698550701141357 | KNN Loss: 2.4500107765197754 | BCE Loss: 1.0198442935943604\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 3.496089458465576 | KNN Loss: 2.474593162536621 | BCE Loss: 1.0214961767196655\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 3.452059745788574 | KNN Loss: 2.4502508640289307 | BCE Loss: 1.0018088817596436\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 3.483466863632202 | KNN Loss: 2.467341661453247 | BCE Loss: 1.016125202178955\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 3.5015506744384766 | KNN Loss: 2.4622983932495117 | BCE Loss: 1.0392522811889648\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 3.476506233215332 | KNN Loss: 2.459386110305786 | BCE Loss: 1.017120122909546\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 3.4825100898742676 | KNN Loss: 2.468587875366211 | BCE Loss: 1.0139223337173462\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 3.4875855445861816 | KNN Loss: 2.4610366821289062 | BCE Loss: 1.0265488624572754\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 3.5033223628997803 | KNN Loss: 2.4788153171539307 | BCE Loss: 1.0245070457458496\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 3.496856212615967 | KNN Loss: 2.460832357406616 | BCE Loss: 1.0360239744186401\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 3.5269088745117188 | KNN Loss: 2.4667232036590576 | BCE Loss: 1.0601857900619507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 3.5068349838256836 | KNN Loss: 2.46734881401062 | BCE Loss: 1.0394861698150635\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 3.499793291091919 | KNN Loss: 2.462345600128174 | BCE Loss: 1.0374476909637451\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 3.481081962585449 | KNN Loss: 2.454221725463867 | BCE Loss: 1.0268603563308716\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 3.496720314025879 | KNN Loss: 2.5101540088653564 | BCE Loss: 0.9865663051605225\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 3.471802234649658 | KNN Loss: 2.462562084197998 | BCE Loss: 1.0092402696609497\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 3.5006136894226074 | KNN Loss: 2.489772081375122 | BCE Loss: 1.0108414888381958\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 3.467963933944702 | KNN Loss: 2.4658584594726562 | BCE Loss: 1.002105474472046\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 3.4860987663269043 | KNN Loss: 2.479560136795044 | BCE Loss: 1.00653874874115\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 3.490436553955078 | KNN Loss: 2.485436201095581 | BCE Loss: 1.005000352859497\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 3.499791145324707 | KNN Loss: 2.474048376083374 | BCE Loss: 1.025742769241333\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 3.5171563625335693 | KNN Loss: 2.491698741912842 | BCE Loss: 1.0254576206207275\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 3.4784908294677734 | KNN Loss: 2.4724221229553223 | BCE Loss: 1.0060688257217407\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 3.474431037902832 | KNN Loss: 2.462914228439331 | BCE Loss: 1.0115166902542114\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 3.488680839538574 | KNN Loss: 2.445435047149658 | BCE Loss: 1.043245792388916\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 3.457735776901245 | KNN Loss: 2.4546315670013428 | BCE Loss: 1.0031042098999023\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 3.5212037563323975 | KNN Loss: 2.477970838546753 | BCE Loss: 1.0432329177856445\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 3.5060617923736572 | KNN Loss: 2.493290901184082 | BCE Loss: 1.0127708911895752\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 3.4923601150512695 | KNN Loss: 2.474538564682007 | BCE Loss: 1.0178214311599731\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 3.513604164123535 | KNN Loss: 2.473499298095703 | BCE Loss: 1.040104866027832\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 3.4463579654693604 | KNN Loss: 2.4422783851623535 | BCE Loss: 1.0040795803070068\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 3.540132999420166 | KNN Loss: 2.50484037399292 | BCE Loss: 1.0352927446365356\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 3.4727184772491455 | KNN Loss: 2.44960880279541 | BCE Loss: 1.0231096744537354\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 3.4596519470214844 | KNN Loss: 2.4605929851531982 | BCE Loss: 0.9990589618682861\n",
      "Epoch   206: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 3.5186500549316406 | KNN Loss: 2.4797298908233643 | BCE Loss: 1.0389201641082764\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 3.4880497455596924 | KNN Loss: 2.4411838054656982 | BCE Loss: 1.0468659400939941\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 3.5245370864868164 | KNN Loss: 2.484262704849243 | BCE Loss: 1.0402742624282837\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 3.4705145359039307 | KNN Loss: 2.4623396396636963 | BCE Loss: 1.0081748962402344\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 3.4695231914520264 | KNN Loss: 2.457704782485962 | BCE Loss: 1.0118184089660645\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 3.494199275970459 | KNN Loss: 2.448408365249634 | BCE Loss: 1.0457909107208252\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 3.5252718925476074 | KNN Loss: 2.4688374996185303 | BCE Loss: 1.0564343929290771\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 3.4511218070983887 | KNN Loss: 2.455573081970215 | BCE Loss: 0.9955486059188843\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 3.4647183418273926 | KNN Loss: 2.459364175796509 | BCE Loss: 1.0053542852401733\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 3.480630397796631 | KNN Loss: 2.470231294631958 | BCE Loss: 1.0103991031646729\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 3.5111148357391357 | KNN Loss: 2.4843122959136963 | BCE Loss: 1.0268025398254395\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 3.473163604736328 | KNN Loss: 2.4601902961730957 | BCE Loss: 1.0129733085632324\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 3.523933172225952 | KNN Loss: 2.489234447479248 | BCE Loss: 1.034698724746704\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 3.4375133514404297 | KNN Loss: 2.4678523540496826 | BCE Loss: 0.9696608781814575\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 3.4765868186950684 | KNN Loss: 2.4679434299468994 | BCE Loss: 1.0086432695388794\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 3.516988515853882 | KNN Loss: 2.500706672668457 | BCE Loss: 1.0162818431854248\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 3.497713565826416 | KNN Loss: 2.4666523933410645 | BCE Loss: 1.0310611724853516\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 3.495683193206787 | KNN Loss: 2.4631669521331787 | BCE Loss: 1.0325161218643188\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 3.464613914489746 | KNN Loss: 2.4450185298919678 | BCE Loss: 1.0195952653884888\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 3.4066100120544434 | KNN Loss: 2.4241936206817627 | BCE Loss: 0.9824164509773254\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 3.495997667312622 | KNN Loss: 2.482431411743164 | BCE Loss: 1.013566255569458\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 3.477530002593994 | KNN Loss: 2.462653160095215 | BCE Loss: 1.0148768424987793\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 3.4281113147735596 | KNN Loss: 2.416130304336548 | BCE Loss: 1.0119810104370117\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 3.4448623657226562 | KNN Loss: 2.429680585861206 | BCE Loss: 1.0151817798614502\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 3.495692729949951 | KNN Loss: 2.46639084815979 | BCE Loss: 1.0293018817901611\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 3.4949448108673096 | KNN Loss: 2.488250494003296 | BCE Loss: 1.0066943168640137\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 3.458691120147705 | KNN Loss: 2.463264226913452 | BCE Loss: 0.9954268336296082\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 3.471508502960205 | KNN Loss: 2.434065818786621 | BCE Loss: 1.037442684173584\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 3.506284475326538 | KNN Loss: 2.4721906185150146 | BCE Loss: 1.0340938568115234\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 3.467074155807495 | KNN Loss: 2.4488375186920166 | BCE Loss: 1.0182366371154785\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 3.4582998752593994 | KNN Loss: 2.459296703338623 | BCE Loss: 0.9990032315254211\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 3.487475633621216 | KNN Loss: 2.4717612266540527 | BCE Loss: 1.015714406967163\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 3.482055902481079 | KNN Loss: 2.4893980026245117 | BCE Loss: 0.9926578998565674\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 3.4834725856781006 | KNN Loss: 2.4540607929229736 | BCE Loss: 1.029411792755127\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 3.5066475868225098 | KNN Loss: 2.4632794857025146 | BCE Loss: 1.0433679819107056\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 3.517017364501953 | KNN Loss: 2.4779183864593506 | BCE Loss: 1.0390989780426025\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 3.452395439147949 | KNN Loss: 2.4627294540405273 | BCE Loss: 0.9896661043167114\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 3.5222911834716797 | KNN Loss: 2.465467929840088 | BCE Loss: 1.0568232536315918\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 3.439472198486328 | KNN Loss: 2.4603488445281982 | BCE Loss: 0.9791233539581299\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 3.4920573234558105 | KNN Loss: 2.4501495361328125 | BCE Loss: 1.0419079065322876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 3.462161064147949 | KNN Loss: 2.4326059818267822 | BCE Loss: 1.029555082321167\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 3.4633519649505615 | KNN Loss: 2.461988687515259 | BCE Loss: 1.0013632774353027\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 3.486776351928711 | KNN Loss: 2.466282367706299 | BCE Loss: 1.0204941034317017\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 3.528419256210327 | KNN Loss: 2.475679874420166 | BCE Loss: 1.0527393817901611\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 3.497161388397217 | KNN Loss: 2.490509271621704 | BCE Loss: 1.0066522359848022\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 3.4970736503601074 | KNN Loss: 2.4694621562957764 | BCE Loss: 1.027611494064331\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 3.500676155090332 | KNN Loss: 2.461939573287964 | BCE Loss: 1.0387364625930786\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 3.479408025741577 | KNN Loss: 2.464498996734619 | BCE Loss: 1.014909029006958\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 3.4712798595428467 | KNN Loss: 2.443941593170166 | BCE Loss: 1.0273382663726807\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 3.463318347930908 | KNN Loss: 2.477567434310913 | BCE Loss: 0.9857507944107056\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 3.4487102031707764 | KNN Loss: 2.453679323196411 | BCE Loss: 0.9950308799743652\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 3.5014007091522217 | KNN Loss: 2.4643125534057617 | BCE Loss: 1.03708815574646\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 3.5525293350219727 | KNN Loss: 2.4979164600372314 | BCE Loss: 1.0546128749847412\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 3.4776828289031982 | KNN Loss: 2.454819679260254 | BCE Loss: 1.0228631496429443\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 3.537614345550537 | KNN Loss: 2.4971628189086914 | BCE Loss: 1.0404515266418457\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 3.4671034812927246 | KNN Loss: 2.444596290588379 | BCE Loss: 1.0225070714950562\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 3.464557647705078 | KNN Loss: 2.4596235752105713 | BCE Loss: 1.0049340724945068\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 3.4935004711151123 | KNN Loss: 2.462599039077759 | BCE Loss: 1.0309014320373535\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 3.4789505004882812 | KNN Loss: 2.4671719074249268 | BCE Loss: 1.0117785930633545\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 3.4791259765625 | KNN Loss: 2.4815781116485596 | BCE Loss: 0.9975479245185852\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 3.5295281410217285 | KNN Loss: 2.5024971961975098 | BCE Loss: 1.0270309448242188\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 3.471245527267456 | KNN Loss: 2.4651739597320557 | BCE Loss: 1.0060715675354004\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 3.5066335201263428 | KNN Loss: 2.472132682800293 | BCE Loss: 1.0345008373260498\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 3.514680862426758 | KNN Loss: 2.5304477214813232 | BCE Loss: 0.984233021736145\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 3.479283571243286 | KNN Loss: 2.4716475009918213 | BCE Loss: 1.0076360702514648\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 3.491123676300049 | KNN Loss: 2.483863592147827 | BCE Loss: 1.0072599649429321\n",
      "Epoch   217: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 3.483694553375244 | KNN Loss: 2.4669337272644043 | BCE Loss: 1.0167608261108398\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 3.52956485748291 | KNN Loss: 2.4685277938842773 | BCE Loss: 1.0610370635986328\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 3.4349255561828613 | KNN Loss: 2.4261724948883057 | BCE Loss: 1.0087531805038452\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 3.4767208099365234 | KNN Loss: 2.4513392448425293 | BCE Loss: 1.0253815650939941\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 3.523817300796509 | KNN Loss: 2.486799955368042 | BCE Loss: 1.0370173454284668\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 3.4861581325531006 | KNN Loss: 2.4699318408966064 | BCE Loss: 1.0162262916564941\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 3.4966845512390137 | KNN Loss: 2.480865955352783 | BCE Loss: 1.0158185958862305\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 3.473493814468384 | KNN Loss: 2.4489033222198486 | BCE Loss: 1.0245904922485352\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 3.5067317485809326 | KNN Loss: 2.4574649333953857 | BCE Loss: 1.0492668151855469\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 3.4819228649139404 | KNN Loss: 2.4741179943084717 | BCE Loss: 1.0078048706054688\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 3.4171814918518066 | KNN Loss: 2.4294092655181885 | BCE Loss: 0.9877723455429077\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 3.5004119873046875 | KNN Loss: 2.443624496459961 | BCE Loss: 1.0567874908447266\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 3.4874916076660156 | KNN Loss: 2.446974277496338 | BCE Loss: 1.0405173301696777\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 3.4279651641845703 | KNN Loss: 2.4629299640655518 | BCE Loss: 0.9650353193283081\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 3.4821531772613525 | KNN Loss: 2.469799280166626 | BCE Loss: 1.0123538970947266\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 3.5205163955688477 | KNN Loss: 2.4847733974456787 | BCE Loss: 1.0357428789138794\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 3.4432406425476074 | KNN Loss: 2.466400623321533 | BCE Loss: 0.9768401384353638\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 3.430297613143921 | KNN Loss: 2.438659429550171 | BCE Loss: 0.9916381239891052\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 3.4919018745422363 | KNN Loss: 2.462411880493164 | BCE Loss: 1.0294899940490723\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 3.4928722381591797 | KNN Loss: 2.452770709991455 | BCE Loss: 1.0401015281677246\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 3.4792680740356445 | KNN Loss: 2.479616641998291 | BCE Loss: 0.9996514320373535\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 3.4932947158813477 | KNN Loss: 2.4685757160186768 | BCE Loss: 1.0247191190719604\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 3.4629933834075928 | KNN Loss: 2.45772647857666 | BCE Loss: 1.0052669048309326\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 3.5093483924865723 | KNN Loss: 2.450690746307373 | BCE Loss: 1.0586577653884888\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 3.5087194442749023 | KNN Loss: 2.4931585788726807 | BCE Loss: 1.0155607461929321\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 3.4687724113464355 | KNN Loss: 2.43776535987854 | BCE Loss: 1.0310070514678955\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 3.512284755706787 | KNN Loss: 2.4946112632751465 | BCE Loss: 1.0176734924316406\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 3.5055606365203857 | KNN Loss: 2.498544692993164 | BCE Loss: 1.0070159435272217\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 3.5027916431427 | KNN Loss: 2.474846601486206 | BCE Loss: 1.0279450416564941\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 3.5176658630371094 | KNN Loss: 2.4881844520568848 | BCE Loss: 1.029481291770935\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 3.441412925720215 | KNN Loss: 2.4505653381347656 | BCE Loss: 0.990847647190094\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 3.505679130554199 | KNN Loss: 2.480926990509033 | BCE Loss: 1.0247522592544556\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 3.475128173828125 | KNN Loss: 2.460960865020752 | BCE Loss: 1.014167308807373\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 3.4973833560943604 | KNN Loss: 2.466156244277954 | BCE Loss: 1.0312271118164062\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 3.4924492835998535 | KNN Loss: 2.479156970977783 | BCE Loss: 1.0132923126220703\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 3.473949432373047 | KNN Loss: 2.4568116664886475 | BCE Loss: 1.017137885093689\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 3.483839511871338 | KNN Loss: 2.467329740524292 | BCE Loss: 1.016509771347046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 3.4995951652526855 | KNN Loss: 2.4769914150238037 | BCE Loss: 1.0226038694381714\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 3.472158193588257 | KNN Loss: 2.4525303840637207 | BCE Loss: 1.0196278095245361\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 3.4825668334960938 | KNN Loss: 2.4722959995269775 | BCE Loss: 1.0102709531784058\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 3.499417304992676 | KNN Loss: 2.474975109100342 | BCE Loss: 1.024442195892334\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 3.504148006439209 | KNN Loss: 2.466987133026123 | BCE Loss: 1.037160873413086\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 3.5083110332489014 | KNN Loss: 2.481292486190796 | BCE Loss: 1.0270185470581055\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 3.5025506019592285 | KNN Loss: 2.465989589691162 | BCE Loss: 1.036561131477356\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 3.51839542388916 | KNN Loss: 2.4881367683410645 | BCE Loss: 1.0302586555480957\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 3.464759349822998 | KNN Loss: 2.44191837310791 | BCE Loss: 1.0228410959243774\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 3.4389688968658447 | KNN Loss: 2.435617446899414 | BCE Loss: 1.0033514499664307\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 3.499453544616699 | KNN Loss: 2.4685187339782715 | BCE Loss: 1.0309349298477173\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 3.49236798286438 | KNN Loss: 2.4467105865478516 | BCE Loss: 1.0456573963165283\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 3.4782662391662598 | KNN Loss: 2.474332571029663 | BCE Loss: 1.0039336681365967\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 3.4696147441864014 | KNN Loss: 2.4554622173309326 | BCE Loss: 1.0141525268554688\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 3.486574649810791 | KNN Loss: 2.4579551219940186 | BCE Loss: 1.028619408607483\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 3.482175827026367 | KNN Loss: 2.438596487045288 | BCE Loss: 1.043579339981079\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 3.491609811782837 | KNN Loss: 2.450246810913086 | BCE Loss: 1.041363000869751\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 3.459620237350464 | KNN Loss: 2.4508774280548096 | BCE Loss: 1.0087428092956543\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 3.5210933685302734 | KNN Loss: 2.491192579269409 | BCE Loss: 1.0299007892608643\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 3.5282418727874756 | KNN Loss: 2.4771764278411865 | BCE Loss: 1.051065444946289\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 3.45790958404541 | KNN Loss: 2.452852964401245 | BCE Loss: 1.0050567388534546\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 3.479738235473633 | KNN Loss: 2.4448885917663574 | BCE Loss: 1.034849762916565\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 3.446861982345581 | KNN Loss: 2.45699143409729 | BCE Loss: 0.9898706078529358\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 3.496969223022461 | KNN Loss: 2.4763782024383545 | BCE Loss: 1.020590901374817\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 3.4707179069519043 | KNN Loss: 2.4629008769989014 | BCE Loss: 1.0078171491622925\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 3.493730306625366 | KNN Loss: 2.4895882606506348 | BCE Loss: 1.0041420459747314\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 3.4584054946899414 | KNN Loss: 2.4486472606658936 | BCE Loss: 1.0097583532333374\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 3.4665355682373047 | KNN Loss: 2.465092420578003 | BCE Loss: 1.0014431476593018\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 3.476278781890869 | KNN Loss: 2.4630579948425293 | BCE Loss: 1.0132207870483398\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 3.5061757564544678 | KNN Loss: 2.4732136726379395 | BCE Loss: 1.0329620838165283\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 3.482694149017334 | KNN Loss: 2.437976837158203 | BCE Loss: 1.0447174310684204\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 3.437143325805664 | KNN Loss: 2.4652466773986816 | BCE Loss: 0.9718965291976929\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 3.4812488555908203 | KNN Loss: 2.4619481563568115 | BCE Loss: 1.0193005800247192\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 3.4952874183654785 | KNN Loss: 2.477344512939453 | BCE Loss: 1.0179429054260254\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 3.482100009918213 | KNN Loss: 2.452399492263794 | BCE Loss: 1.029700517654419\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 3.502533435821533 | KNN Loss: 2.467237949371338 | BCE Loss: 1.0352956056594849\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 3.467494010925293 | KNN Loss: 2.4626035690307617 | BCE Loss: 1.0048903226852417\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 3.5132946968078613 | KNN Loss: 2.4667205810546875 | BCE Loss: 1.0465739965438843\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 3.481933832168579 | KNN Loss: 2.4651143550872803 | BCE Loss: 1.0168194770812988\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 3.527272939682007 | KNN Loss: 2.499011993408203 | BCE Loss: 1.0282609462738037\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 3.4584622383117676 | KNN Loss: 2.4406518936157227 | BCE Loss: 1.017810344696045\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 3.4752919673919678 | KNN Loss: 2.466832399368286 | BCE Loss: 1.0084595680236816\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 3.507108688354492 | KNN Loss: 2.489832639694214 | BCE Loss: 1.0172759294509888\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 3.4583182334899902 | KNN Loss: 2.449388027191162 | BCE Loss: 1.0089302062988281\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 3.4673564434051514 | KNN Loss: 2.460446357727051 | BCE Loss: 1.0069100856781006\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 3.466895580291748 | KNN Loss: 2.4409310817718506 | BCE Loss: 1.0259644985198975\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 3.4540562629699707 | KNN Loss: 2.4209609031677246 | BCE Loss: 1.033095359802246\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 3.4973714351654053 | KNN Loss: 2.476288080215454 | BCE Loss: 1.0210833549499512\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 3.548860549926758 | KNN Loss: 2.507958173751831 | BCE Loss: 1.0409023761749268\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 3.5126585960388184 | KNN Loss: 2.481335401535034 | BCE Loss: 1.0313231945037842\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 3.496168613433838 | KNN Loss: 2.4677176475524902 | BCE Loss: 1.028450846672058\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 3.451345920562744 | KNN Loss: 2.459827184677124 | BCE Loss: 0.9915187358856201\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 3.478494882583618 | KNN Loss: 2.4446916580200195 | BCE Loss: 1.0338032245635986\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 3.4430735111236572 | KNN Loss: 2.4221439361572266 | BCE Loss: 1.0209295749664307\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 3.440699577331543 | KNN Loss: 2.434936285018921 | BCE Loss: 1.0057634115219116\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 3.4873242378234863 | KNN Loss: 2.463538885116577 | BCE Loss: 1.0237853527069092\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 3.543597459793091 | KNN Loss: 2.5062665939331055 | BCE Loss: 1.0373308658599854\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 3.502289056777954 | KNN Loss: 2.4634644985198975 | BCE Loss: 1.0388245582580566\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 3.5092692375183105 | KNN Loss: 2.4741735458374023 | BCE Loss: 1.0350958108901978\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 3.485229015350342 | KNN Loss: 2.4759364128112793 | BCE Loss: 1.0092926025390625\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 3.493710994720459 | KNN Loss: 2.4885637760162354 | BCE Loss: 1.0051472187042236\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 3.494749069213867 | KNN Loss: 2.478914260864258 | BCE Loss: 1.0158348083496094\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 3.4776151180267334 | KNN Loss: 2.426874876022339 | BCE Loss: 1.0507402420043945\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 3.429110288619995 | KNN Loss: 2.429732322692871 | BCE Loss: 0.9993779063224792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 3.434509754180908 | KNN Loss: 2.4173152446746826 | BCE Loss: 1.017194390296936\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 3.4717183113098145 | KNN Loss: 2.4419703483581543 | BCE Loss: 1.0297480821609497\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 3.4583611488342285 | KNN Loss: 2.4240524768829346 | BCE Loss: 1.034308671951294\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 3.5188021659851074 | KNN Loss: 2.4815423488616943 | BCE Loss: 1.0372599363327026\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 3.4368722438812256 | KNN Loss: 2.4405100345611572 | BCE Loss: 0.9963622689247131\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 3.4783968925476074 | KNN Loss: 2.475135087966919 | BCE Loss: 1.0032618045806885\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 3.539263963699341 | KNN Loss: 2.4771318435668945 | BCE Loss: 1.0621321201324463\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 3.5090279579162598 | KNN Loss: 2.4575414657592773 | BCE Loss: 1.0514864921569824\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 3.4700324535369873 | KNN Loss: 2.4493393898010254 | BCE Loss: 1.020693063735962\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 3.456986904144287 | KNN Loss: 2.443202495574951 | BCE Loss: 1.0137842893600464\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 3.4916558265686035 | KNN Loss: 2.47700572013855 | BCE Loss: 1.0146499872207642\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 3.46536922454834 | KNN Loss: 2.4590766429901123 | BCE Loss: 1.0062925815582275\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 3.5153565406799316 | KNN Loss: 2.4904680252075195 | BCE Loss: 1.024888515472412\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 3.4725658893585205 | KNN Loss: 2.4540576934814453 | BCE Loss: 1.0185081958770752\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 3.4747941493988037 | KNN Loss: 2.4619832038879395 | BCE Loss: 1.0128109455108643\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 3.539102554321289 | KNN Loss: 2.5051043033599854 | BCE Loss: 1.0339981317520142\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 3.480147123336792 | KNN Loss: 2.4543581008911133 | BCE Loss: 1.0257890224456787\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 3.4802803993225098 | KNN Loss: 2.469064235687256 | BCE Loss: 1.0112162828445435\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 3.523552894592285 | KNN Loss: 2.473564386367798 | BCE Loss: 1.0499885082244873\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 3.5094006061553955 | KNN Loss: 2.4738285541534424 | BCE Loss: 1.0355720520019531\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 3.4500732421875 | KNN Loss: 2.4584786891937256 | BCE Loss: 0.9915944337844849\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 3.4659223556518555 | KNN Loss: 2.4601640701293945 | BCE Loss: 1.005758285522461\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 3.509661912918091 | KNN Loss: 2.4784998893737793 | BCE Loss: 1.0311620235443115\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 3.458322525024414 | KNN Loss: 2.4555389881134033 | BCE Loss: 1.0027834177017212\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 3.452385425567627 | KNN Loss: 2.4293856620788574 | BCE Loss: 1.02299964427948\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 3.452613353729248 | KNN Loss: 2.4528815746307373 | BCE Loss: 0.9997318983078003\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 3.45029616355896 | KNN Loss: 2.447603225708008 | BCE Loss: 1.0026929378509521\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 3.476144790649414 | KNN Loss: 2.493729591369629 | BCE Loss: 0.9824152588844299\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 3.5154311656951904 | KNN Loss: 2.4932124614715576 | BCE Loss: 1.0222187042236328\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 3.50396990776062 | KNN Loss: 2.476367235183716 | BCE Loss: 1.0276026725769043\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 3.4710750579833984 | KNN Loss: 2.431718349456787 | BCE Loss: 1.0393565893173218\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 3.5257561206817627 | KNN Loss: 2.468325138092041 | BCE Loss: 1.0574309825897217\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 3.4542579650878906 | KNN Loss: 2.4467406272888184 | BCE Loss: 1.0075173377990723\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 3.480402946472168 | KNN Loss: 2.4565789699554443 | BCE Loss: 1.0238240957260132\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 3.486762285232544 | KNN Loss: 2.4728803634643555 | BCE Loss: 1.0138819217681885\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 3.4839162826538086 | KNN Loss: 2.464745044708252 | BCE Loss: 1.0191713571548462\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 3.492475986480713 | KNN Loss: 2.4573585987091064 | BCE Loss: 1.035117506980896\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 3.4925827980041504 | KNN Loss: 2.497403621673584 | BCE Loss: 0.995179295539856\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 3.476438045501709 | KNN Loss: 2.4846837520599365 | BCE Loss: 0.9917541742324829\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 3.4911952018737793 | KNN Loss: 2.4520130157470703 | BCE Loss: 1.0391823053359985\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 3.4908089637756348 | KNN Loss: 2.4530084133148193 | BCE Loss: 1.0378005504608154\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 3.4895482063293457 | KNN Loss: 2.462371587753296 | BCE Loss: 1.0271766185760498\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 3.4656596183776855 | KNN Loss: 2.4565844535827637 | BCE Loss: 1.0090752840042114\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 3.4899754524230957 | KNN Loss: 2.452801465988159 | BCE Loss: 1.037173867225647\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 3.458294630050659 | KNN Loss: 2.45489764213562 | BCE Loss: 1.003396987915039\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 3.491466760635376 | KNN Loss: 2.4897639751434326 | BCE Loss: 1.0017027854919434\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 3.467111587524414 | KNN Loss: 2.4512696266174316 | BCE Loss: 1.0158419609069824\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 3.4494895935058594 | KNN Loss: 2.4257071018218994 | BCE Loss: 1.0237823724746704\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 3.505413770675659 | KNN Loss: 2.4736528396606445 | BCE Loss: 1.0317609310150146\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 3.5075204372406006 | KNN Loss: 2.4725279808044434 | BCE Loss: 1.0349924564361572\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 3.496453046798706 | KNN Loss: 2.489093780517578 | BCE Loss: 1.007359266281128\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 3.524268865585327 | KNN Loss: 2.500661849975586 | BCE Loss: 1.0236070156097412\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 3.505397081375122 | KNN Loss: 2.4636929035186768 | BCE Loss: 1.0417041778564453\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 3.436248779296875 | KNN Loss: 2.4382994174957275 | BCE Loss: 0.9979492425918579\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 3.447997570037842 | KNN Loss: 2.4517557621002197 | BCE Loss: 0.9962416887283325\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 3.421814441680908 | KNN Loss: 2.4292407035827637 | BCE Loss: 0.992573618888855\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 3.4727413654327393 | KNN Loss: 2.461826801300049 | BCE Loss: 1.0109145641326904\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 3.470738172531128 | KNN Loss: 2.4490420818328857 | BCE Loss: 1.0216960906982422\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 3.4720427989959717 | KNN Loss: 2.4462766647338867 | BCE Loss: 1.025766134262085\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 3.4590916633605957 | KNN Loss: 2.428919792175293 | BCE Loss: 1.0301717519760132\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 3.4641551971435547 | KNN Loss: 2.4429516792297363 | BCE Loss: 1.0212035179138184\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 3.4605839252471924 | KNN Loss: 2.460529327392578 | BCE Loss: 1.0000545978546143\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 3.452988624572754 | KNN Loss: 2.4430603981018066 | BCE Loss: 1.0099283456802368\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 3.455871820449829 | KNN Loss: 2.4556922912597656 | BCE Loss: 1.0001795291900635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 3.453809976577759 | KNN Loss: 2.4478659629821777 | BCE Loss: 1.005944013595581\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 3.4986910820007324 | KNN Loss: 2.467963457107544 | BCE Loss: 1.030727744102478\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 3.490947961807251 | KNN Loss: 2.4728846549987793 | BCE Loss: 1.0180633068084717\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 3.5086991786956787 | KNN Loss: 2.481510639190674 | BCE Loss: 1.0271885395050049\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 3.4248032569885254 | KNN Loss: 2.431762456893921 | BCE Loss: 0.993040919303894\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 3.4481449127197266 | KNN Loss: 2.4581263065338135 | BCE Loss: 0.9900187253952026\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 3.4930777549743652 | KNN Loss: 2.4678890705108643 | BCE Loss: 1.025188684463501\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 3.491400718688965 | KNN Loss: 2.465294361114502 | BCE Loss: 1.0261064767837524\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 3.4937305450439453 | KNN Loss: 2.4797935485839844 | BCE Loss: 1.013936996459961\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 3.529219150543213 | KNN Loss: 2.4946017265319824 | BCE Loss: 1.0346174240112305\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 3.4665286540985107 | KNN Loss: 2.449538230895996 | BCE Loss: 1.0169904232025146\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 3.4407031536102295 | KNN Loss: 2.446913957595825 | BCE Loss: 0.9937891960144043\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 3.458453416824341 | KNN Loss: 2.4405291080474854 | BCE Loss: 1.0179243087768555\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 3.4571681022644043 | KNN Loss: 2.4502644538879395 | BCE Loss: 1.0069037675857544\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 3.463894844055176 | KNN Loss: 2.450133800506592 | BCE Loss: 1.013761043548584\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 3.4579966068267822 | KNN Loss: 2.461894989013672 | BCE Loss: 0.9961016178131104\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 3.4795775413513184 | KNN Loss: 2.42691969871521 | BCE Loss: 1.0526578426361084\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 3.4447944164276123 | KNN Loss: 2.4450929164886475 | BCE Loss: 0.9997014999389648\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 3.481926918029785 | KNN Loss: 2.4653663635253906 | BCE Loss: 1.016560435295105\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 3.4738731384277344 | KNN Loss: 2.4624788761138916 | BCE Loss: 1.0113943815231323\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 3.465843677520752 | KNN Loss: 2.459728717803955 | BCE Loss: 1.0061149597167969\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 3.5092339515686035 | KNN Loss: 2.459223747253418 | BCE Loss: 1.050010085105896\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 3.515110731124878 | KNN Loss: 2.464885711669922 | BCE Loss: 1.050225019454956\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 3.5223586559295654 | KNN Loss: 2.511396884918213 | BCE Loss: 1.0109617710113525\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 3.5442590713500977 | KNN Loss: 2.514749526977539 | BCE Loss: 1.029509425163269\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 3.4860215187072754 | KNN Loss: 2.4507052898406982 | BCE Loss: 1.0353161096572876\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 3.4794979095458984 | KNN Loss: 2.452054262161255 | BCE Loss: 1.0274436473846436\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 3.460585117340088 | KNN Loss: 2.448936939239502 | BCE Loss: 1.011648178100586\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 3.4567978382110596 | KNN Loss: 2.4499685764312744 | BCE Loss: 1.0068292617797852\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 3.5013017654418945 | KNN Loss: 2.497046947479248 | BCE Loss: 1.0042548179626465\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 3.4849066734313965 | KNN Loss: 2.4750113487243652 | BCE Loss: 1.0098953247070312\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 3.49219012260437 | KNN Loss: 2.460069417953491 | BCE Loss: 1.032120704650879\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 3.4540953636169434 | KNN Loss: 2.4299230575561523 | BCE Loss: 1.024172306060791\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 3.4581689834594727 | KNN Loss: 2.4449245929718018 | BCE Loss: 1.013244390487671\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 3.4809248447418213 | KNN Loss: 2.4874565601348877 | BCE Loss: 0.9934682250022888\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 3.476773977279663 | KNN Loss: 2.4743423461914062 | BCE Loss: 1.0024316310882568\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 3.4815046787261963 | KNN Loss: 2.4794039726257324 | BCE Loss: 1.0021007061004639\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 3.4724931716918945 | KNN Loss: 2.474709987640381 | BCE Loss: 0.9977831244468689\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 3.4901552200317383 | KNN Loss: 2.4714887142181396 | BCE Loss: 1.0186665058135986\n",
      "Epoch   251: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 3.4810566902160645 | KNN Loss: 2.464651584625244 | BCE Loss: 1.0164049863815308\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 3.4735827445983887 | KNN Loss: 2.4530534744262695 | BCE Loss: 1.0205292701721191\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 3.4758009910583496 | KNN Loss: 2.4525153636932373 | BCE Loss: 1.0232856273651123\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 3.5373194217681885 | KNN Loss: 2.4990291595458984 | BCE Loss: 1.03829026222229\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 3.486043930053711 | KNN Loss: 2.469531297683716 | BCE Loss: 1.0165126323699951\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 3.4940099716186523 | KNN Loss: 2.476304531097412 | BCE Loss: 1.0177054405212402\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 3.4317433834075928 | KNN Loss: 2.432145357131958 | BCE Loss: 0.9995980858802795\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 3.4501543045043945 | KNN Loss: 2.448228359222412 | BCE Loss: 1.0019258260726929\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 3.4808101654052734 | KNN Loss: 2.473824977874756 | BCE Loss: 1.0069853067398071\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 3.509364128112793 | KNN Loss: 2.4662439823150635 | BCE Loss: 1.0431201457977295\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 3.4725775718688965 | KNN Loss: 2.44581937789917 | BCE Loss: 1.0267583131790161\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 3.478336811065674 | KNN Loss: 2.4714138507843018 | BCE Loss: 1.006922960281372\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 3.4652061462402344 | KNN Loss: 2.46038818359375 | BCE Loss: 1.0048179626464844\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 3.4219319820404053 | KNN Loss: 2.4226107597351074 | BCE Loss: 0.9993211627006531\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 3.4676334857940674 | KNN Loss: 2.4369699954986572 | BCE Loss: 1.0306634902954102\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 3.51397705078125 | KNN Loss: 2.4825565814971924 | BCE Loss: 1.031420350074768\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 3.4368529319763184 | KNN Loss: 2.4477994441986084 | BCE Loss: 0.9890535473823547\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 3.477625846862793 | KNN Loss: 2.485910654067993 | BCE Loss: 0.9917153120040894\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 3.479979991912842 | KNN Loss: 2.4575090408325195 | BCE Loss: 1.0224710702896118\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 3.4710299968719482 | KNN Loss: 2.445662021636963 | BCE Loss: 1.0253679752349854\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 3.4806113243103027 | KNN Loss: 2.456484079360962 | BCE Loss: 1.0241273641586304\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 3.447439432144165 | KNN Loss: 2.4504218101501465 | BCE Loss: 0.9970176219940186\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 3.459596633911133 | KNN Loss: 2.4572701454162598 | BCE Loss: 1.002326488494873\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 3.492016315460205 | KNN Loss: 2.4677646160125732 | BCE Loss: 1.0242516994476318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 3.478372097015381 | KNN Loss: 2.4450743198394775 | BCE Loss: 1.0332977771759033\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 3.46812105178833 | KNN Loss: 2.446790933609009 | BCE Loss: 1.0213301181793213\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 3.5101044178009033 | KNN Loss: 2.510512351989746 | BCE Loss: 0.9995920062065125\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 3.4798073768615723 | KNN Loss: 2.4878506660461426 | BCE Loss: 0.9919568300247192\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 3.4724669456481934 | KNN Loss: 2.46370792388916 | BCE Loss: 1.0087590217590332\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 3.530513286590576 | KNN Loss: 2.478782892227173 | BCE Loss: 1.0517303943634033\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 3.498157024383545 | KNN Loss: 2.4786581993103027 | BCE Loss: 1.0194987058639526\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 3.4699110984802246 | KNN Loss: 2.4530200958251953 | BCE Loss: 1.0168908834457397\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 3.4772655963897705 | KNN Loss: 2.437901496887207 | BCE Loss: 1.0393640995025635\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 3.5010826587677 | KNN Loss: 2.4866061210632324 | BCE Loss: 1.0144765377044678\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 3.530827283859253 | KNN Loss: 2.4894485473632812 | BCE Loss: 1.0413787364959717\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 3.497506856918335 | KNN Loss: 2.482853412628174 | BCE Loss: 1.0146534442901611\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 3.439774513244629 | KNN Loss: 2.4308013916015625 | BCE Loss: 1.0089731216430664\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 3.471538543701172 | KNN Loss: 2.4517102241516113 | BCE Loss: 1.019828200340271\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 3.477994441986084 | KNN Loss: 2.4623847007751465 | BCE Loss: 1.0156097412109375\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 3.474796772003174 | KNN Loss: 2.4484152793884277 | BCE Loss: 1.026381492614746\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 3.4936745166778564 | KNN Loss: 2.4706971645355225 | BCE Loss: 1.022977352142334\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 3.5059993267059326 | KNN Loss: 2.492583751678467 | BCE Loss: 1.0134155750274658\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 3.5147621631622314 | KNN Loss: 2.4741880893707275 | BCE Loss: 1.040574073791504\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 3.4842934608459473 | KNN Loss: 2.497124433517456 | BCE Loss: 0.9871690273284912\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 3.482356309890747 | KNN Loss: 2.4726486206054688 | BCE Loss: 1.0097076892852783\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 3.4627633094787598 | KNN Loss: 2.447943925857544 | BCE Loss: 1.0148193836212158\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 3.4832866191864014 | KNN Loss: 2.4281814098358154 | BCE Loss: 1.055105209350586\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 3.4465160369873047 | KNN Loss: 2.4355030059814453 | BCE Loss: 1.0110130310058594\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 3.490212917327881 | KNN Loss: 2.4962382316589355 | BCE Loss: 0.9939748048782349\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 3.470792293548584 | KNN Loss: 2.4431421756744385 | BCE Loss: 1.027649998664856\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 3.4877066612243652 | KNN Loss: 2.4739372730255127 | BCE Loss: 1.0137693881988525\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 3.486013412475586 | KNN Loss: 2.4501922130584717 | BCE Loss: 1.0358210802078247\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 3.501513957977295 | KNN Loss: 2.474252939224243 | BCE Loss: 1.0272610187530518\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 3.502901554107666 | KNN Loss: 2.491687536239624 | BCE Loss: 1.0112138986587524\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 3.490144729614258 | KNN Loss: 2.4527084827423096 | BCE Loss: 1.0374361276626587\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 3.4652976989746094 | KNN Loss: 2.4782609939575195 | BCE Loss: 0.9870368242263794\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 3.4905147552490234 | KNN Loss: 2.4699692726135254 | BCE Loss: 1.0205456018447876\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 3.4738543033599854 | KNN Loss: 2.4518797397613525 | BCE Loss: 1.0219745635986328\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 3.4962399005889893 | KNN Loss: 2.493882656097412 | BCE Loss: 1.0023572444915771\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 3.5127978324890137 | KNN Loss: 2.5088932514190674 | BCE Loss: 1.0039047002792358\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 3.466832160949707 | KNN Loss: 2.4465105533599854 | BCE Loss: 1.0203217267990112\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 3.4797463417053223 | KNN Loss: 2.457326650619507 | BCE Loss: 1.0224196910858154\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 3.455784559249878 | KNN Loss: 2.442741632461548 | BCE Loss: 1.01304292678833\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 3.4493329524993896 | KNN Loss: 2.4378416538238525 | BCE Loss: 1.011491298675537\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 3.4791359901428223 | KNN Loss: 2.465522289276123 | BCE Loss: 1.0136137008666992\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 3.478623390197754 | KNN Loss: 2.471221685409546 | BCE Loss: 1.0074018239974976\n",
      "Epoch   262: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 3.431455135345459 | KNN Loss: 2.4282963275909424 | BCE Loss: 1.0031589269638062\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 3.467883348464966 | KNN Loss: 2.4435770511627197 | BCE Loss: 1.024306297302246\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 3.463003635406494 | KNN Loss: 2.453843355178833 | BCE Loss: 1.0091603994369507\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 3.4685616493225098 | KNN Loss: 2.462271213531494 | BCE Loss: 1.0062905550003052\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 3.4598793983459473 | KNN Loss: 2.4390056133270264 | BCE Loss: 1.0208739042282104\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 3.495232105255127 | KNN Loss: 2.468456268310547 | BCE Loss: 1.02677583694458\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 3.4346141815185547 | KNN Loss: 2.449857473373413 | BCE Loss: 0.9847568273544312\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 3.4459824562072754 | KNN Loss: 2.4304728507995605 | BCE Loss: 1.0155094861984253\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 3.447482109069824 | KNN Loss: 2.4373996257781982 | BCE Loss: 1.010082483291626\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 3.486254930496216 | KNN Loss: 2.4765326976776123 | BCE Loss: 1.0097222328186035\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 3.4658775329589844 | KNN Loss: 2.465498447418213 | BCE Loss: 1.000378966331482\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 3.50860857963562 | KNN Loss: 2.509598731994629 | BCE Loss: 0.9990097880363464\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 3.5268211364746094 | KNN Loss: 2.4856200218200684 | BCE Loss: 1.041201114654541\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 3.4295763969421387 | KNN Loss: 2.4411394596099854 | BCE Loss: 0.9884369373321533\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 3.442000389099121 | KNN Loss: 2.4383175373077393 | BCE Loss: 1.0036829710006714\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 3.445297956466675 | KNN Loss: 2.4348511695861816 | BCE Loss: 1.0104467868804932\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 3.5237581729888916 | KNN Loss: 2.4548721313476562 | BCE Loss: 1.0688860416412354\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 3.5440168380737305 | KNN Loss: 2.4985358715057373 | BCE Loss: 1.0454809665679932\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 3.4290404319763184 | KNN Loss: 2.4437904357910156 | BCE Loss: 0.9852500557899475\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 3.4385945796966553 | KNN Loss: 2.438359260559082 | BCE Loss: 1.0002353191375732\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 3.4782865047454834 | KNN Loss: 2.451413631439209 | BCE Loss: 1.0268728733062744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 3.426283359527588 | KNN Loss: 2.4487533569335938 | BCE Loss: 0.9775299429893494\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 3.4668264389038086 | KNN Loss: 2.457836627960205 | BCE Loss: 1.008989691734314\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 3.51741886138916 | KNN Loss: 2.4636075496673584 | BCE Loss: 1.0538113117218018\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 3.4980392456054688 | KNN Loss: 2.471996545791626 | BCE Loss: 1.0260426998138428\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 3.4218153953552246 | KNN Loss: 2.430177688598633 | BCE Loss: 0.9916378259658813\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 3.4970548152923584 | KNN Loss: 2.4707229137420654 | BCE Loss: 1.026331901550293\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 3.473012924194336 | KNN Loss: 2.476369619369507 | BCE Loss: 0.9966433048248291\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 3.475454330444336 | KNN Loss: 2.439985752105713 | BCE Loss: 1.0354686975479126\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 3.447157859802246 | KNN Loss: 2.424445867538452 | BCE Loss: 1.0227118730545044\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 3.4621315002441406 | KNN Loss: 2.423381805419922 | BCE Loss: 1.0387496948242188\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 3.4666731357574463 | KNN Loss: 2.4444222450256348 | BCE Loss: 1.0222508907318115\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 3.4896976947784424 | KNN Loss: 2.4482080936431885 | BCE Loss: 1.041489601135254\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 3.43634033203125 | KNN Loss: 2.4504764080047607 | BCE Loss: 0.9858639240264893\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 3.430762767791748 | KNN Loss: 2.43322491645813 | BCE Loss: 0.9975378513336182\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 3.455733299255371 | KNN Loss: 2.4658238887786865 | BCE Loss: 0.989909291267395\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 3.437913417816162 | KNN Loss: 2.43375825881958 | BCE Loss: 1.004155158996582\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 3.4931983947753906 | KNN Loss: 2.475950241088867 | BCE Loss: 1.0172481536865234\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 3.505819320678711 | KNN Loss: 2.470940113067627 | BCE Loss: 1.034879207611084\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 3.489098072052002 | KNN Loss: 2.468696117401123 | BCE Loss: 1.020401954650879\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 3.5101070404052734 | KNN Loss: 2.486722707748413 | BCE Loss: 1.0233843326568604\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 3.452270984649658 | KNN Loss: 2.44427227973938 | BCE Loss: 1.0079985857009888\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 3.469973564147949 | KNN Loss: 2.4470412731170654 | BCE Loss: 1.0229322910308838\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 3.4761195182800293 | KNN Loss: 2.4740190505981445 | BCE Loss: 1.0021004676818848\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 3.477964401245117 | KNN Loss: 2.4402525424957275 | BCE Loss: 1.0377118587493896\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 3.486461639404297 | KNN Loss: 2.471308708190918 | BCE Loss: 1.0151528120040894\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 3.481358766555786 | KNN Loss: 2.464148998260498 | BCE Loss: 1.017209768295288\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 3.4786319732666016 | KNN Loss: 2.465362787246704 | BCE Loss: 1.0132691860198975\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 3.5043320655822754 | KNN Loss: 2.4821834564208984 | BCE Loss: 1.0221484899520874\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 3.4722890853881836 | KNN Loss: 2.469529628753662 | BCE Loss: 1.0027594566345215\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 3.514918565750122 | KNN Loss: 2.4992308616638184 | BCE Loss: 1.0156877040863037\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 3.4769699573516846 | KNN Loss: 2.456475257873535 | BCE Loss: 1.0204946994781494\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 3.4722397327423096 | KNN Loss: 2.4682188034057617 | BCE Loss: 1.0040209293365479\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 3.482642412185669 | KNN Loss: 2.4725427627563477 | BCE Loss: 1.0100996494293213\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 3.510979652404785 | KNN Loss: 2.4984018802642822 | BCE Loss: 1.0125778913497925\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 3.417250394821167 | KNN Loss: 2.4560060501098633 | BCE Loss: 0.9612444043159485\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 3.4785871505737305 | KNN Loss: 2.439405679702759 | BCE Loss: 1.0391814708709717\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 3.490715503692627 | KNN Loss: 2.4685652256011963 | BCE Loss: 1.0221501588821411\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 3.4588708877563477 | KNN Loss: 2.446932554244995 | BCE Loss: 1.0119383335113525\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 3.4476139545440674 | KNN Loss: 2.4170517921447754 | BCE Loss: 1.030562162399292\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 3.4616501331329346 | KNN Loss: 2.470060348510742 | BCE Loss: 0.9915897250175476\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 3.479039430618286 | KNN Loss: 2.4636189937591553 | BCE Loss: 1.0154204368591309\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 3.5212676525115967 | KNN Loss: 2.491523504257202 | BCE Loss: 1.0297441482543945\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 3.4837536811828613 | KNN Loss: 2.4691755771636963 | BCE Loss: 1.0145779848098755\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 3.5080747604370117 | KNN Loss: 2.478898525238037 | BCE Loss: 1.0291762351989746\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 3.491121768951416 | KNN Loss: 2.474186897277832 | BCE Loss: 1.016934871673584\n",
      "Epoch   273: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 3.508701801300049 | KNN Loss: 2.4673614501953125 | BCE Loss: 1.0413403511047363\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 3.4581246376037598 | KNN Loss: 2.44311785697937 | BCE Loss: 1.0150067806243896\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 3.4793412685394287 | KNN Loss: 2.4478492736816406 | BCE Loss: 1.031491994857788\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 3.4618847370147705 | KNN Loss: 2.460843086242676 | BCE Loss: 1.0010416507720947\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 3.469119071960449 | KNN Loss: 2.4362192153930664 | BCE Loss: 1.0328998565673828\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 3.4224190711975098 | KNN Loss: 2.4138059616088867 | BCE Loss: 1.008613109588623\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 3.4760308265686035 | KNN Loss: 2.47686767578125 | BCE Loss: 0.9991632699966431\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 3.484104633331299 | KNN Loss: 2.484084367752075 | BCE Loss: 1.0000202655792236\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 3.486361265182495 | KNN Loss: 2.455866575241089 | BCE Loss: 1.0304946899414062\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 3.462132692337036 | KNN Loss: 2.4502336978912354 | BCE Loss: 1.0118989944458008\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 3.5074000358581543 | KNN Loss: 2.461188793182373 | BCE Loss: 1.0462112426757812\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 3.49643874168396 | KNN Loss: 2.484997272491455 | BCE Loss: 1.0114414691925049\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 3.461887836456299 | KNN Loss: 2.454193592071533 | BCE Loss: 1.0076942443847656\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 3.4851245880126953 | KNN Loss: 2.4618749618530273 | BCE Loss: 1.023249626159668\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 3.480104446411133 | KNN Loss: 2.4652347564697266 | BCE Loss: 1.0148696899414062\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 3.552408218383789 | KNN Loss: 2.518207550048828 | BCE Loss: 1.0342007875442505\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 3.5082454681396484 | KNN Loss: 2.4745101928710938 | BCE Loss: 1.0337352752685547\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 3.457972288131714 | KNN Loss: 2.4463083744049072 | BCE Loss: 1.0116639137268066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 3.478055953979492 | KNN Loss: 2.4417800903320312 | BCE Loss: 1.0362757444381714\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 3.4640164375305176 | KNN Loss: 2.451784133911133 | BCE Loss: 1.0122324228286743\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 3.47721266746521 | KNN Loss: 2.4417550563812256 | BCE Loss: 1.0354576110839844\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 3.4294252395629883 | KNN Loss: 2.4271347522735596 | BCE Loss: 1.0022904872894287\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 3.4510223865509033 | KNN Loss: 2.4492104053497314 | BCE Loss: 1.0018119812011719\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 3.499720573425293 | KNN Loss: 2.472156524658203 | BCE Loss: 1.0275640487670898\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 3.467503070831299 | KNN Loss: 2.4519171714782715 | BCE Loss: 1.0155858993530273\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 3.4813172817230225 | KNN Loss: 2.482120990753174 | BCE Loss: 0.9991963505744934\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 3.472759246826172 | KNN Loss: 2.442736864089966 | BCE Loss: 1.030022382736206\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 3.4659037590026855 | KNN Loss: 2.4598324298858643 | BCE Loss: 1.0060713291168213\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 3.430889129638672 | KNN Loss: 2.430590867996216 | BCE Loss: 1.0002981424331665\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 3.486574172973633 | KNN Loss: 2.458932876586914 | BCE Loss: 1.0276412963867188\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 3.4914321899414062 | KNN Loss: 2.4796926975250244 | BCE Loss: 1.0117394924163818\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 3.4643802642822266 | KNN Loss: 2.467161178588867 | BCE Loss: 0.9972190260887146\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 3.440181255340576 | KNN Loss: 2.4442532062530518 | BCE Loss: 0.9959279298782349\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 3.4555022716522217 | KNN Loss: 2.4484245777130127 | BCE Loss: 1.007077693939209\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 3.4671242237091064 | KNN Loss: 2.431673526763916 | BCE Loss: 1.0354506969451904\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 3.4439449310302734 | KNN Loss: 2.460780143737793 | BCE Loss: 0.9831648468971252\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 3.5352368354797363 | KNN Loss: 2.516697406768799 | BCE Loss: 1.0185394287109375\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 3.49413800239563 | KNN Loss: 2.4852209091186523 | BCE Loss: 1.0089170932769775\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 3.4475455284118652 | KNN Loss: 2.4441304206848145 | BCE Loss: 1.0034151077270508\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 3.464829444885254 | KNN Loss: 2.4419262409210205 | BCE Loss: 1.022903323173523\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 3.4500951766967773 | KNN Loss: 2.44840407371521 | BCE Loss: 1.0016909837722778\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 3.5456180572509766 | KNN Loss: 2.4949495792388916 | BCE Loss: 1.0506683588027954\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 3.469205141067505 | KNN Loss: 2.421827554702759 | BCE Loss: 1.047377586364746\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 3.4757609367370605 | KNN Loss: 2.4556422233581543 | BCE Loss: 1.0201188325881958\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 3.4550440311431885 | KNN Loss: 2.469021797180176 | BCE Loss: 0.9860222935676575\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 3.5420007705688477 | KNN Loss: 2.4970083236694336 | BCE Loss: 1.0449925661087036\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 3.498716115951538 | KNN Loss: 2.4766438007354736 | BCE Loss: 1.0220723152160645\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 3.4377307891845703 | KNN Loss: 2.4341137409210205 | BCE Loss: 1.0036169290542603\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 3.4608240127563477 | KNN Loss: 2.437927484512329 | BCE Loss: 1.022896647453308\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 3.4727816581726074 | KNN Loss: 2.436164617538452 | BCE Loss: 1.0366170406341553\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 3.4505093097686768 | KNN Loss: 2.412344455718994 | BCE Loss: 1.0381648540496826\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 3.4199342727661133 | KNN Loss: 2.413609743118286 | BCE Loss: 1.0063245296478271\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 3.4993975162506104 | KNN Loss: 2.502495765686035 | BCE Loss: 0.9969017505645752\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 3.461433172225952 | KNN Loss: 2.4118337631225586 | BCE Loss: 1.0495994091033936\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 3.5022168159484863 | KNN Loss: 2.4826314449310303 | BCE Loss: 1.0195852518081665\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 3.494403839111328 | KNN Loss: 2.453902244567871 | BCE Loss: 1.040501594543457\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 3.4263744354248047 | KNN Loss: 2.418290138244629 | BCE Loss: 1.0080842971801758\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 3.498861312866211 | KNN Loss: 2.4741873741149902 | BCE Loss: 1.0246738195419312\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 3.4543774127960205 | KNN Loss: 2.459322929382324 | BCE Loss: 0.9950545430183411\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 3.488065004348755 | KNN Loss: 2.4579105377197266 | BCE Loss: 1.0301544666290283\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 3.4792308807373047 | KNN Loss: 2.480344295501709 | BCE Loss: 0.9988865852355957\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 3.5321521759033203 | KNN Loss: 2.498652696609497 | BCE Loss: 1.0334994792938232\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 3.4737772941589355 | KNN Loss: 2.4640090465545654 | BCE Loss: 1.0097682476043701\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 3.5020246505737305 | KNN Loss: 2.469951629638672 | BCE Loss: 1.0320730209350586\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 3.476710796356201 | KNN Loss: 2.4401445388793945 | BCE Loss: 1.0365662574768066\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 3.4644129276275635 | KNN Loss: 2.4545488357543945 | BCE Loss: 1.009864091873169\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 3.4903690814971924 | KNN Loss: 2.4483485221862793 | BCE Loss: 1.042020559310913\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 3.5264410972595215 | KNN Loss: 2.485377550125122 | BCE Loss: 1.0410635471343994\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 3.4468977451324463 | KNN Loss: 2.4343349933624268 | BCE Loss: 1.0125627517700195\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 3.4596948623657227 | KNN Loss: 2.4355502128601074 | BCE Loss: 1.0241447687149048\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 3.5133450031280518 | KNN Loss: 2.4750924110412598 | BCE Loss: 1.038252592086792\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 3.4854586124420166 | KNN Loss: 2.4596147537231445 | BCE Loss: 1.025843858718872\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 3.5294575691223145 | KNN Loss: 2.4786782264709473 | BCE Loss: 1.0507793426513672\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 3.486555814743042 | KNN Loss: 2.4589357376098633 | BCE Loss: 1.0276200771331787\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 3.501767158508301 | KNN Loss: 2.4687588214874268 | BCE Loss: 1.033008337020874\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 3.4870529174804688 | KNN Loss: 2.501579523086548 | BCE Loss: 0.9854733943939209\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 3.461536407470703 | KNN Loss: 2.449374198913574 | BCE Loss: 1.012162208557129\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 3.492006301879883 | KNN Loss: 2.4732470512390137 | BCE Loss: 1.0187591314315796\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 3.473201274871826 | KNN Loss: 2.451763153076172 | BCE Loss: 1.0214381217956543\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 3.4674510955810547 | KNN Loss: 2.4480674266815186 | BCE Loss: 1.0193837881088257\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 3.4800965785980225 | KNN Loss: 2.4602365493774414 | BCE Loss: 1.019860029220581\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 3.478044033050537 | KNN Loss: 2.473057985305786 | BCE Loss: 1.0049861669540405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 3.4972965717315674 | KNN Loss: 2.476848602294922 | BCE Loss: 1.0204479694366455\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 3.4601330757141113 | KNN Loss: 2.4301607608795166 | BCE Loss: 1.0299721956253052\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 3.519158124923706 | KNN Loss: 2.491647958755493 | BCE Loss: 1.027510166168213\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 3.4610531330108643 | KNN Loss: 2.4582440853118896 | BCE Loss: 1.0028090476989746\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 3.4943246841430664 | KNN Loss: 2.492602586746216 | BCE Loss: 1.001721978187561\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 3.4334664344787598 | KNN Loss: 2.4535648822784424 | BCE Loss: 0.9799015522003174\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 3.4592132568359375 | KNN Loss: 2.4530551433563232 | BCE Loss: 1.0061581134796143\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 3.4757306575775146 | KNN Loss: 2.4403820037841797 | BCE Loss: 1.035348653793335\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 3.4726102352142334 | KNN Loss: 2.477766513824463 | BCE Loss: 0.9948436617851257\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 3.472172260284424 | KNN Loss: 2.4669110774993896 | BCE Loss: 1.0052610635757446\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 3.4706733226776123 | KNN Loss: 2.479309558868408 | BCE Loss: 0.9913637638092041\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 3.502697467803955 | KNN Loss: 2.461669683456421 | BCE Loss: 1.0410277843475342\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 3.4284276962280273 | KNN Loss: 2.4289252758026123 | BCE Loss: 0.9995023012161255\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 3.4528956413269043 | KNN Loss: 2.447000026702881 | BCE Loss: 1.0058954954147339\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 3.4793050289154053 | KNN Loss: 2.4466872215270996 | BCE Loss: 1.0326178073883057\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 3.4666168689727783 | KNN Loss: 2.433647394180298 | BCE Loss: 1.0329694747924805\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 3.4330697059631348 | KNN Loss: 2.4537723064422607 | BCE Loss: 0.9792972803115845\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 3.4644126892089844 | KNN Loss: 2.4560232162475586 | BCE Loss: 1.0083895921707153\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 3.490846633911133 | KNN Loss: 2.4454526901245117 | BCE Loss: 1.0453938245773315\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 3.478982448577881 | KNN Loss: 2.452366828918457 | BCE Loss: 1.0266155004501343\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 3.488720178604126 | KNN Loss: 2.4459068775177 | BCE Loss: 1.0428133010864258\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 3.4817557334899902 | KNN Loss: 2.4393558502197266 | BCE Loss: 1.0423998832702637\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 3.501446485519409 | KNN Loss: 2.469012498855591 | BCE Loss: 1.0324339866638184\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 3.4668424129486084 | KNN Loss: 2.4463958740234375 | BCE Loss: 1.020446538925171\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 3.48091983795166 | KNN Loss: 2.4485182762145996 | BCE Loss: 1.0324015617370605\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 3.5178608894348145 | KNN Loss: 2.479790210723877 | BCE Loss: 1.0380706787109375\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 3.484349250793457 | KNN Loss: 2.460721015930176 | BCE Loss: 1.0236281156539917\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 3.4874515533447266 | KNN Loss: 2.464284896850586 | BCE Loss: 1.0231666564941406\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 3.450108766555786 | KNN Loss: 2.4277544021606445 | BCE Loss: 1.0223543643951416\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 3.47560453414917 | KNN Loss: 2.4554154872894287 | BCE Loss: 1.0201891660690308\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 3.4612553119659424 | KNN Loss: 2.4377801418304443 | BCE Loss: 1.023475170135498\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 3.487222194671631 | KNN Loss: 2.4656903743743896 | BCE Loss: 1.0215318202972412\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 3.4934821128845215 | KNN Loss: 2.466435194015503 | BCE Loss: 1.0270469188690186\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 3.455164670944214 | KNN Loss: 2.4586358070373535 | BCE Loss: 0.9965288639068604\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 3.480905771255493 | KNN Loss: 2.4646313190460205 | BCE Loss: 1.0162744522094727\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 3.459818124771118 | KNN Loss: 2.450773239135742 | BCE Loss: 1.009044885635376\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 3.466944694519043 | KNN Loss: 2.4321327209472656 | BCE Loss: 1.0348119735717773\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 3.5494141578674316 | KNN Loss: 2.5037336349487305 | BCE Loss: 1.0456804037094116\n",
      "Epoch   293: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 3.457435131072998 | KNN Loss: 2.4517974853515625 | BCE Loss: 1.005637526512146\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 3.454702377319336 | KNN Loss: 2.45100474357605 | BCE Loss: 1.0036975145339966\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 3.506755828857422 | KNN Loss: 2.492011308670044 | BCE Loss: 1.014744520187378\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 3.467801094055176 | KNN Loss: 2.452556848526001 | BCE Loss: 1.0152442455291748\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 3.499494791030884 | KNN Loss: 2.4575164318084717 | BCE Loss: 1.041978359222412\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 3.4327821731567383 | KNN Loss: 2.4226014614105225 | BCE Loss: 1.0101807117462158\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 3.4884352684020996 | KNN Loss: 2.4639430046081543 | BCE Loss: 1.0244922637939453\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 3.427938461303711 | KNN Loss: 2.4442787170410156 | BCE Loss: 0.9836598634719849\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 3.4669108390808105 | KNN Loss: 2.434356212615967 | BCE Loss: 1.0325546264648438\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 3.4413931369781494 | KNN Loss: 2.4319403171539307 | BCE Loss: 1.0094528198242188\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 3.489219903945923 | KNN Loss: 2.479398727416992 | BCE Loss: 1.0098211765289307\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 3.49933123588562 | KNN Loss: 2.491520404815674 | BCE Loss: 1.0078108310699463\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 3.4813928604125977 | KNN Loss: 2.4527103900909424 | BCE Loss: 1.0286824703216553\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 3.458958864212036 | KNN Loss: 2.4732842445373535 | BCE Loss: 0.9856745600700378\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 3.4375386238098145 | KNN Loss: 2.4508917331695557 | BCE Loss: 0.9866467714309692\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 3.534756898880005 | KNN Loss: 2.4793450832366943 | BCE Loss: 1.0554118156433105\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 3.4653148651123047 | KNN Loss: 2.4602155685424805 | BCE Loss: 1.0050991773605347\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 3.524388074874878 | KNN Loss: 2.4722561836242676 | BCE Loss: 1.0521318912506104\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 3.465096950531006 | KNN Loss: 2.438572406768799 | BCE Loss: 1.0265246629714966\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 3.4505362510681152 | KNN Loss: 2.462552309036255 | BCE Loss: 0.9879840612411499\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 3.4426615238189697 | KNN Loss: 2.442418336868286 | BCE Loss: 1.0002431869506836\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 3.51499605178833 | KNN Loss: 2.4587671756744385 | BCE Loss: 1.0562288761138916\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 3.475397825241089 | KNN Loss: 2.4590816497802734 | BCE Loss: 1.0163161754608154\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 3.513669013977051 | KNN Loss: 2.4639134407043457 | BCE Loss: 1.0497554540634155\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 3.4809670448303223 | KNN Loss: 2.4918456077575684 | BCE Loss: 0.9891214966773987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 3.4732837677001953 | KNN Loss: 2.4351251125335693 | BCE Loss: 1.038158655166626\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 3.520873785018921 | KNN Loss: 2.508338212966919 | BCE Loss: 1.012535572052002\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 3.4514873027801514 | KNN Loss: 2.4574573040008545 | BCE Loss: 0.9940299987792969\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 3.513929843902588 | KNN Loss: 2.4694881439208984 | BCE Loss: 1.0444415807724\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 3.454925537109375 | KNN Loss: 2.4508113861083984 | BCE Loss: 1.0041142702102661\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 3.495342969894409 | KNN Loss: 2.4714572429656982 | BCE Loss: 1.023885726928711\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 3.5227975845336914 | KNN Loss: 2.4586291313171387 | BCE Loss: 1.0641684532165527\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 3.469710350036621 | KNN Loss: 2.453362464904785 | BCE Loss: 1.0163480043411255\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 3.499946117401123 | KNN Loss: 2.4482555389404297 | BCE Loss: 1.0516905784606934\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 3.4752161502838135 | KNN Loss: 2.45499324798584 | BCE Loss: 1.0202229022979736\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 3.4693610668182373 | KNN Loss: 2.464050531387329 | BCE Loss: 1.0053105354309082\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 3.469221353530884 | KNN Loss: 2.4621353149414062 | BCE Loss: 1.0070860385894775\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 3.4826056957244873 | KNN Loss: 2.4508914947509766 | BCE Loss: 1.0317142009735107\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 3.4702606201171875 | KNN Loss: 2.467622756958008 | BCE Loss: 1.0026378631591797\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 3.476576805114746 | KNN Loss: 2.448739767074585 | BCE Loss: 1.0278370380401611\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 3.4552907943725586 | KNN Loss: 2.4425761699676514 | BCE Loss: 1.0127147436141968\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 3.4607512950897217 | KNN Loss: 2.4567906856536865 | BCE Loss: 1.0039606094360352\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 3.4581642150878906 | KNN Loss: 2.440864086151123 | BCE Loss: 1.0173001289367676\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 3.449686288833618 | KNN Loss: 2.4259591102600098 | BCE Loss: 1.0237271785736084\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 3.465733051300049 | KNN Loss: 2.4431848526000977 | BCE Loss: 1.0225483179092407\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 3.5046026706695557 | KNN Loss: 2.4796087741851807 | BCE Loss: 1.024993896484375\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 3.4759299755096436 | KNN Loss: 2.4693312644958496 | BCE Loss: 1.006598711013794\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 3.50900936126709 | KNN Loss: 2.4774374961853027 | BCE Loss: 1.031571865081787\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 3.445864677429199 | KNN Loss: 2.444483995437622 | BCE Loss: 1.0013808012008667\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 3.488973617553711 | KNN Loss: 2.462425708770752 | BCE Loss: 1.026547908782959\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 3.4582273960113525 | KNN Loss: 2.4323885440826416 | BCE Loss: 1.025838851928711\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 3.5102286338806152 | KNN Loss: 2.482603073120117 | BCE Loss: 1.0276254415512085\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 3.489166259765625 | KNN Loss: 2.46929669380188 | BCE Loss: 1.0198696851730347\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 3.50606632232666 | KNN Loss: 2.4981894493103027 | BCE Loss: 1.0078767538070679\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 3.5066299438476562 | KNN Loss: 2.4665443897247314 | BCE Loss: 1.0400856733322144\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 3.4746546745300293 | KNN Loss: 2.4861881732940674 | BCE Loss: 0.9884665012359619\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 3.468541383743286 | KNN Loss: 2.450505495071411 | BCE Loss: 1.018035888671875\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 3.484175205230713 | KNN Loss: 2.45805287361145 | BCE Loss: 1.0261223316192627\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 3.5007052421569824 | KNN Loss: 2.4983012676239014 | BCE Loss: 1.002403974533081\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 3.4398109912872314 | KNN Loss: 2.446856737136841 | BCE Loss: 0.9929541945457458\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 3.472233533859253 | KNN Loss: 2.4555349349975586 | BCE Loss: 1.0166985988616943\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 3.5136003494262695 | KNN Loss: 2.5211212635040283 | BCE Loss: 0.9924789667129517\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 3.4857137203216553 | KNN Loss: 2.4866559505462646 | BCE Loss: 0.9990578293800354\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 3.483464241027832 | KNN Loss: 2.4578654766082764 | BCE Loss: 1.0255988836288452\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 3.484614849090576 | KNN Loss: 2.4681925773620605 | BCE Loss: 1.0164222717285156\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 3.499917507171631 | KNN Loss: 2.4924769401550293 | BCE Loss: 1.0074405670166016\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 3.4835891723632812 | KNN Loss: 2.459789276123047 | BCE Loss: 1.0237997770309448\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 3.4376120567321777 | KNN Loss: 2.4445230960845947 | BCE Loss: 0.9930890202522278\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 3.5019702911376953 | KNN Loss: 2.487978458404541 | BCE Loss: 1.0139918327331543\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 3.428539276123047 | KNN Loss: 2.430939197540283 | BCE Loss: 0.9976001977920532\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 3.45841121673584 | KNN Loss: 2.44389009475708 | BCE Loss: 1.0145212411880493\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 3.427778720855713 | KNN Loss: 2.4400198459625244 | BCE Loss: 0.9877587556838989\n",
      "Epoch   305: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 3.4821062088012695 | KNN Loss: 2.4793317317962646 | BCE Loss: 1.0027744770050049\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 3.4637789726257324 | KNN Loss: 2.4592700004577637 | BCE Loss: 1.0045089721679688\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 3.532057285308838 | KNN Loss: 2.5134456157684326 | BCE Loss: 1.0186117887496948\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 3.459843635559082 | KNN Loss: 2.4530723094940186 | BCE Loss: 1.0067713260650635\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 3.5366835594177246 | KNN Loss: 2.490201711654663 | BCE Loss: 1.0464818477630615\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 3.4489593505859375 | KNN Loss: 2.452045440673828 | BCE Loss: 0.9969139099121094\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 3.4901084899902344 | KNN Loss: 2.445651054382324 | BCE Loss: 1.0444574356079102\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 3.4507880210876465 | KNN Loss: 2.4209606647491455 | BCE Loss: 1.0298272371292114\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 3.4776461124420166 | KNN Loss: 2.458841323852539 | BCE Loss: 1.0188047885894775\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 3.4926352500915527 | KNN Loss: 2.4629151821136475 | BCE Loss: 1.0297201871871948\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 3.494401216506958 | KNN Loss: 2.464730978012085 | BCE Loss: 1.029670238494873\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 3.517612934112549 | KNN Loss: 2.498168706893921 | BCE Loss: 1.0194443464279175\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 3.483696460723877 | KNN Loss: 2.4527077674865723 | BCE Loss: 1.0309885740280151\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 3.474245309829712 | KNN Loss: 2.479154109954834 | BCE Loss: 0.9950911998748779\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 3.5130560398101807 | KNN Loss: 2.4697916507720947 | BCE Loss: 1.043264389038086\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 3.487497329711914 | KNN Loss: 2.454490900039673 | BCE Loss: 1.0330063104629517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 3.468215227127075 | KNN Loss: 2.475731372833252 | BCE Loss: 0.992483913898468\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 3.533130407333374 | KNN Loss: 2.4826128482818604 | BCE Loss: 1.0505175590515137\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 3.495448589324951 | KNN Loss: 2.447493314743042 | BCE Loss: 1.0479552745819092\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 3.508626699447632 | KNN Loss: 2.489301919937134 | BCE Loss: 1.019324779510498\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 3.5021634101867676 | KNN Loss: 2.4864394664764404 | BCE Loss: 1.0157240629196167\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 3.4730708599090576 | KNN Loss: 2.4594860076904297 | BCE Loss: 1.013584852218628\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 3.4839322566986084 | KNN Loss: 2.47733473777771 | BCE Loss: 1.0065975189208984\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 3.4828743934631348 | KNN Loss: 2.457120180130005 | BCE Loss: 1.0257542133331299\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 3.4897677898406982 | KNN Loss: 2.4613521099090576 | BCE Loss: 1.0284156799316406\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 3.501420021057129 | KNN Loss: 2.488062620162964 | BCE Loss: 1.013357400894165\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 3.4524950981140137 | KNN Loss: 2.4479897022247314 | BCE Loss: 1.0045053958892822\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 3.4656729698181152 | KNN Loss: 2.4457547664642334 | BCE Loss: 1.0199180841445923\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 3.466398000717163 | KNN Loss: 2.4547393321990967 | BCE Loss: 1.0116586685180664\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 3.457681894302368 | KNN Loss: 2.432771921157837 | BCE Loss: 1.0249099731445312\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 3.4257590770721436 | KNN Loss: 2.419848680496216 | BCE Loss: 1.0059103965759277\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 3.4269654750823975 | KNN Loss: 2.410951614379883 | BCE Loss: 1.0160138607025146\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 3.4694085121154785 | KNN Loss: 2.4540352821350098 | BCE Loss: 1.0153731107711792\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 3.5307846069335938 | KNN Loss: 2.5090837478637695 | BCE Loss: 1.0217008590698242\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 3.3989675045013428 | KNN Loss: 2.4147491455078125 | BCE Loss: 0.9842183589935303\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 3.500901937484741 | KNN Loss: 2.468380928039551 | BCE Loss: 1.0325210094451904\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 3.5005364418029785 | KNN Loss: 2.4828011989593506 | BCE Loss: 1.017735242843628\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 3.480494260787964 | KNN Loss: 2.4726598262786865 | BCE Loss: 1.0078344345092773\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 3.523106098175049 | KNN Loss: 2.479748487472534 | BCE Loss: 1.043357491493225\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 3.488429069519043 | KNN Loss: 2.4720911979675293 | BCE Loss: 1.0163379907608032\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 3.4442949295043945 | KNN Loss: 2.44547176361084 | BCE Loss: 0.9988232851028442\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 3.496110439300537 | KNN Loss: 2.457016706466675 | BCE Loss: 1.0390937328338623\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 3.4868812561035156 | KNN Loss: 2.45986008644104 | BCE Loss: 1.0270211696624756\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 3.4530632495880127 | KNN Loss: 2.4410488605499268 | BCE Loss: 1.012014389038086\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 3.438378095626831 | KNN Loss: 2.4322237968444824 | BCE Loss: 1.0061542987823486\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 3.4634547233581543 | KNN Loss: 2.4457788467407227 | BCE Loss: 1.017675757408142\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 3.4355669021606445 | KNN Loss: 2.431159734725952 | BCE Loss: 1.004407286643982\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 3.5106582641601562 | KNN Loss: 2.44549298286438 | BCE Loss: 1.065165400505066\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 3.4559807777404785 | KNN Loss: 2.455742597579956 | BCE Loss: 1.000238060951233\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 3.512573719024658 | KNN Loss: 2.4946365356445312 | BCE Loss: 1.0179370641708374\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 3.4687180519104004 | KNN Loss: 2.4813060760498047 | BCE Loss: 0.9874120950698853\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 3.434446334838867 | KNN Loss: 2.41532826423645 | BCE Loss: 1.0191181898117065\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 3.4953315258026123 | KNN Loss: 2.4651191234588623 | BCE Loss: 1.03021240234375\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 3.4908924102783203 | KNN Loss: 2.4781301021575928 | BCE Loss: 1.012762188911438\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 3.4679317474365234 | KNN Loss: 2.4548401832580566 | BCE Loss: 1.0130914449691772\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 3.4679930210113525 | KNN Loss: 2.449948310852051 | BCE Loss: 1.0180447101593018\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 3.4819719791412354 | KNN Loss: 2.4720404148101807 | BCE Loss: 1.0099315643310547\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 3.454836845397949 | KNN Loss: 2.453787088394165 | BCE Loss: 1.0010497570037842\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 3.4460346698760986 | KNN Loss: 2.4564759731292725 | BCE Loss: 0.9895586371421814\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 3.4830925464630127 | KNN Loss: 2.4737319946289062 | BCE Loss: 1.0093605518341064\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 3.489001512527466 | KNN Loss: 2.4809019565582275 | BCE Loss: 1.0080995559692383\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 3.493891716003418 | KNN Loss: 2.4484591484069824 | BCE Loss: 1.0454325675964355\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 3.4619884490966797 | KNN Loss: 2.4428586959838867 | BCE Loss: 1.019129753112793\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 3.453927755355835 | KNN Loss: 2.4323010444641113 | BCE Loss: 1.0216267108917236\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 3.4910104274749756 | KNN Loss: 2.4701285362243652 | BCE Loss: 1.0208818912506104\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 3.4755353927612305 | KNN Loss: 2.462036609649658 | BCE Loss: 1.0134986639022827\n",
      "Epoch   316: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 3.5140979290008545 | KNN Loss: 2.4955859184265137 | BCE Loss: 1.0185120105743408\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 3.4719953536987305 | KNN Loss: 2.453885078430176 | BCE Loss: 1.0181103944778442\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 3.503061294555664 | KNN Loss: 2.493837356567383 | BCE Loss: 1.0092239379882812\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 3.506438732147217 | KNN Loss: 2.4904162883758545 | BCE Loss: 1.0160224437713623\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 3.4483470916748047 | KNN Loss: 2.4390108585357666 | BCE Loss: 1.0093363523483276\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 3.4516916275024414 | KNN Loss: 2.455195903778076 | BCE Loss: 0.9964956045150757\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 3.4950575828552246 | KNN Loss: 2.4929521083831787 | BCE Loss: 1.002105474472046\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 3.4966912269592285 | KNN Loss: 2.438530683517456 | BCE Loss: 1.058160662651062\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 3.5396149158477783 | KNN Loss: 2.496570110321045 | BCE Loss: 1.0430448055267334\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 3.4796204566955566 | KNN Loss: 2.4568610191345215 | BCE Loss: 1.0227594375610352\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 3.483994960784912 | KNN Loss: 2.4638564586639404 | BCE Loss: 1.0201385021209717\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 3.4466617107391357 | KNN Loss: 2.4372189044952393 | BCE Loss: 1.0094428062438965\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 3.461966037750244 | KNN Loss: 2.4452433586120605 | BCE Loss: 1.0167226791381836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 3.4578561782836914 | KNN Loss: 2.4376814365386963 | BCE Loss: 1.0201748609542847\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 3.466712474822998 | KNN Loss: 2.4801025390625 | BCE Loss: 0.9866100549697876\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 3.475964307785034 | KNN Loss: 2.469437599182129 | BCE Loss: 1.0065267086029053\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 3.495105743408203 | KNN Loss: 2.4960546493530273 | BCE Loss: 0.999051034450531\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 3.4792048931121826 | KNN Loss: 2.440356492996216 | BCE Loss: 1.0388484001159668\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 3.4917168617248535 | KNN Loss: 2.4539999961853027 | BCE Loss: 1.0377168655395508\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 3.4845714569091797 | KNN Loss: 2.460747718811035 | BCE Loss: 1.023823857307434\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 3.4620461463928223 | KNN Loss: 2.4688944816589355 | BCE Loss: 0.9931516051292419\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 3.442744255065918 | KNN Loss: 2.4482369422912598 | BCE Loss: 0.9945071935653687\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 3.4255073070526123 | KNN Loss: 2.424098491668701 | BCE Loss: 1.0014088153839111\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 3.485443115234375 | KNN Loss: 2.4637093544006348 | BCE Loss: 1.0217337608337402\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 3.476562738418579 | KNN Loss: 2.4615612030029297 | BCE Loss: 1.0150015354156494\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 3.465475082397461 | KNN Loss: 2.4415271282196045 | BCE Loss: 1.0239479541778564\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 3.447265148162842 | KNN Loss: 2.436108350753784 | BCE Loss: 1.0111567974090576\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 3.455775260925293 | KNN Loss: 2.429145097732544 | BCE Loss: 1.0266302824020386\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 3.450976848602295 | KNN Loss: 2.4526541233062744 | BCE Loss: 0.998322606086731\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 3.463432550430298 | KNN Loss: 2.425736665725708 | BCE Loss: 1.0376958847045898\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 3.449646472930908 | KNN Loss: 2.4172143936157227 | BCE Loss: 1.032432198524475\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 3.431316375732422 | KNN Loss: 2.4265518188476562 | BCE Loss: 1.0047646760940552\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 3.4721903800964355 | KNN Loss: 2.449683666229248 | BCE Loss: 1.0225067138671875\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 3.4612016677856445 | KNN Loss: 2.4305381774902344 | BCE Loss: 1.0306636095046997\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 3.4709014892578125 | KNN Loss: 2.4748291969299316 | BCE Loss: 0.9960724115371704\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 3.481147289276123 | KNN Loss: 2.490884780883789 | BCE Loss: 0.9902625679969788\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 3.4708197116851807 | KNN Loss: 2.438485622406006 | BCE Loss: 1.0323340892791748\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 3.4910590648651123 | KNN Loss: 2.4479820728302 | BCE Loss: 1.043076992034912\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 3.48496150970459 | KNN Loss: 2.467360496520996 | BCE Loss: 1.0176010131835938\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 3.4242489337921143 | KNN Loss: 2.4265003204345703 | BCE Loss: 0.997748613357544\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 3.4558427333831787 | KNN Loss: 2.4619877338409424 | BCE Loss: 0.9938550591468811\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 3.4782845973968506 | KNN Loss: 2.4513256549835205 | BCE Loss: 1.02695894241333\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 3.452226161956787 | KNN Loss: 2.4298901557922363 | BCE Loss: 1.0223361253738403\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 3.485244035720825 | KNN Loss: 2.4509196281433105 | BCE Loss: 1.0343244075775146\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 3.4546775817871094 | KNN Loss: 2.442713975906372 | BCE Loss: 1.0119634866714478\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 3.4561450481414795 | KNN Loss: 2.4540750980377197 | BCE Loss: 1.0020699501037598\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 3.4794726371765137 | KNN Loss: 2.4352753162384033 | BCE Loss: 1.0441973209381104\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 3.465921640396118 | KNN Loss: 2.436342239379883 | BCE Loss: 1.0295794010162354\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 3.4856104850769043 | KNN Loss: 2.458176851272583 | BCE Loss: 1.0274336338043213\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 3.4798166751861572 | KNN Loss: 2.4929261207580566 | BCE Loss: 0.9868906140327454\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 3.506380558013916 | KNN Loss: 2.4858717918395996 | BCE Loss: 1.020508885383606\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 3.4480960369110107 | KNN Loss: 2.446195125579834 | BCE Loss: 1.0019009113311768\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 3.4995365142822266 | KNN Loss: 2.451448440551758 | BCE Loss: 1.0480880737304688\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 3.510486602783203 | KNN Loss: 2.4624648094177246 | BCE Loss: 1.0480217933654785\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 3.4543886184692383 | KNN Loss: 2.4286227226257324 | BCE Loss: 1.0257658958435059\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 3.4879202842712402 | KNN Loss: 2.4830446243286133 | BCE Loss: 1.0048755407333374\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 3.488326072692871 | KNN Loss: 2.4601728916168213 | BCE Loss: 1.0281531810760498\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 3.4501407146453857 | KNN Loss: 2.442415237426758 | BCE Loss: 1.007725477218628\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 3.4874744415283203 | KNN Loss: 2.4519150257110596 | BCE Loss: 1.0355594158172607\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 3.4245872497558594 | KNN Loss: 2.440953254699707 | BCE Loss: 0.9836340546607971\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 3.481823682785034 | KNN Loss: 2.461843252182007 | BCE Loss: 1.0199804306030273\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 3.4621994495391846 | KNN Loss: 2.451663017272949 | BCE Loss: 1.0105364322662354\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 3.455148696899414 | KNN Loss: 2.453970432281494 | BCE Loss: 1.0011781454086304\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 3.4716639518737793 | KNN Loss: 2.4632766246795654 | BCE Loss: 1.0083872079849243\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 3.458163022994995 | KNN Loss: 2.4273204803466797 | BCE Loss: 1.0308425426483154\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 3.466737747192383 | KNN Loss: 2.447012186050415 | BCE Loss: 1.0197255611419678\n",
      "Epoch   327: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 3.424609899520874 | KNN Loss: 2.4041788578033447 | BCE Loss: 1.0204310417175293\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 3.432110548019409 | KNN Loss: 2.4268686771392822 | BCE Loss: 1.005241870880127\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 3.485215902328491 | KNN Loss: 2.4821276664733887 | BCE Loss: 1.0030882358551025\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 3.4578728675842285 | KNN Loss: 2.4436638355255127 | BCE Loss: 1.0142090320587158\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 3.458887815475464 | KNN Loss: 2.4470179080963135 | BCE Loss: 1.0118699073791504\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 3.459280014038086 | KNN Loss: 2.4178860187530518 | BCE Loss: 1.0413939952850342\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 3.5132405757904053 | KNN Loss: 2.477229118347168 | BCE Loss: 1.0360114574432373\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 3.4979443550109863 | KNN Loss: 2.4408507347106934 | BCE Loss: 1.057093620300293\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 3.5343246459960938 | KNN Loss: 2.4948713779449463 | BCE Loss: 1.039453148841858\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 3.459824800491333 | KNN Loss: 2.442906618118286 | BCE Loss: 1.0169181823730469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 3.4735074043273926 | KNN Loss: 2.4690756797790527 | BCE Loss: 1.0044316053390503\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 3.4629359245300293 | KNN Loss: 2.483506679534912 | BCE Loss: 0.9794292449951172\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 3.472550392150879 | KNN Loss: 2.446959972381592 | BCE Loss: 1.025590419769287\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 3.5078933238983154 | KNN Loss: 2.470393419265747 | BCE Loss: 1.0374999046325684\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 3.4523143768310547 | KNN Loss: 2.447262763977051 | BCE Loss: 1.0050514936447144\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 3.508495569229126 | KNN Loss: 2.444810628890991 | BCE Loss: 1.0636849403381348\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 3.4900126457214355 | KNN Loss: 2.489133834838867 | BCE Loss: 1.0008786916732788\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 3.5150203704833984 | KNN Loss: 2.4893178939819336 | BCE Loss: 1.0257025957107544\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 3.4602015018463135 | KNN Loss: 2.4341542720794678 | BCE Loss: 1.0260472297668457\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 3.4405741691589355 | KNN Loss: 2.440364360809326 | BCE Loss: 1.000209927558899\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 3.537024974822998 | KNN Loss: 2.5077426433563232 | BCE Loss: 1.0292822122573853\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 3.4833335876464844 | KNN Loss: 2.4466922283172607 | BCE Loss: 1.0366414785385132\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 3.46817684173584 | KNN Loss: 2.443070888519287 | BCE Loss: 1.0251059532165527\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 3.444582462310791 | KNN Loss: 2.4760091304779053 | BCE Loss: 0.9685733318328857\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 3.492391347885132 | KNN Loss: 2.470582962036133 | BCE Loss: 1.021808385848999\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 3.479292392730713 | KNN Loss: 2.4557461738586426 | BCE Loss: 1.0235462188720703\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 3.528177261352539 | KNN Loss: 2.501337766647339 | BCE Loss: 1.0268396139144897\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 3.4519195556640625 | KNN Loss: 2.461202383041382 | BCE Loss: 0.9907172918319702\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 3.507817506790161 | KNN Loss: 2.472517251968384 | BCE Loss: 1.0353002548217773\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 3.470060110092163 | KNN Loss: 2.450976848602295 | BCE Loss: 1.0190832614898682\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 3.471055269241333 | KNN Loss: 2.4588799476623535 | BCE Loss: 1.0121753215789795\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 3.4675605297088623 | KNN Loss: 2.448413848876953 | BCE Loss: 1.0191466808319092\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 3.4758715629577637 | KNN Loss: 2.4526405334472656 | BCE Loss: 1.0232309103012085\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 3.497142791748047 | KNN Loss: 2.4686825275421143 | BCE Loss: 1.028460144996643\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 3.453023910522461 | KNN Loss: 2.434500217437744 | BCE Loss: 1.0185235738754272\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 3.4765048027038574 | KNN Loss: 2.4665348529815674 | BCE Loss: 1.00996994972229\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 3.4753122329711914 | KNN Loss: 2.484802722930908 | BCE Loss: 0.9905093908309937\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 3.5172805786132812 | KNN Loss: 2.4666335582733154 | BCE Loss: 1.0506471395492554\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 3.5090060234069824 | KNN Loss: 2.4579508304595947 | BCE Loss: 1.0510550737380981\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 3.502706289291382 | KNN Loss: 2.4771647453308105 | BCE Loss: 1.0255415439605713\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 3.496762990951538 | KNN Loss: 2.464146852493286 | BCE Loss: 1.032616138458252\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 3.485609531402588 | KNN Loss: 2.4414045810699463 | BCE Loss: 1.0442049503326416\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 3.4758036136627197 | KNN Loss: 2.4350929260253906 | BCE Loss: 1.040710687637329\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 3.484545946121216 | KNN Loss: 2.467510938644409 | BCE Loss: 1.0170350074768066\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 3.5042974948883057 | KNN Loss: 2.4678258895874023 | BCE Loss: 1.0364716053009033\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 3.462397575378418 | KNN Loss: 2.457287073135376 | BCE Loss: 1.005110502243042\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 3.509099006652832 | KNN Loss: 2.46035099029541 | BCE Loss: 1.0487481355667114\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 3.5327656269073486 | KNN Loss: 2.4682984352111816 | BCE Loss: 1.064467191696167\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 3.4951565265655518 | KNN Loss: 2.4763221740722656 | BCE Loss: 1.0188343524932861\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 3.4971749782562256 | KNN Loss: 2.48114275932312 | BCE Loss: 1.0160322189331055\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 3.453714609146118 | KNN Loss: 2.423621654510498 | BCE Loss: 1.0300929546356201\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 3.4899301528930664 | KNN Loss: 2.4655110836029053 | BCE Loss: 1.0244190692901611\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 3.433077573776245 | KNN Loss: 2.4245686531066895 | BCE Loss: 1.0085089206695557\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 3.478372573852539 | KNN Loss: 2.45396089553833 | BCE Loss: 1.024411678314209\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 3.545804977416992 | KNN Loss: 2.4823882579803467 | BCE Loss: 1.0634167194366455\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 3.5158703327178955 | KNN Loss: 2.4907772541046143 | BCE Loss: 1.0250930786132812\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 3.4742045402526855 | KNN Loss: 2.4379403591156006 | BCE Loss: 1.036264181137085\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 3.461308002471924 | KNN Loss: 2.448300361633301 | BCE Loss: 1.0130075216293335\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 3.513444662094116 | KNN Loss: 2.4770257472991943 | BCE Loss: 1.0364189147949219\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 3.4863839149475098 | KNN Loss: 2.4851181507110596 | BCE Loss: 1.0012658834457397\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 3.4800896644592285 | KNN Loss: 2.4614179134368896 | BCE Loss: 1.0186716318130493\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 3.4806294441223145 | KNN Loss: 2.461641311645508 | BCE Loss: 1.0189881324768066\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 3.506484270095825 | KNN Loss: 2.4853343963623047 | BCE Loss: 1.0211498737335205\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 3.496567726135254 | KNN Loss: 2.4515907764434814 | BCE Loss: 1.0449769496917725\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 3.508124351501465 | KNN Loss: 2.4897477626800537 | BCE Loss: 1.0183765888214111\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 3.474444627761841 | KNN Loss: 2.4560248851776123 | BCE Loss: 1.0184197425842285\n",
      "Epoch   338: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 3.4616785049438477 | KNN Loss: 2.4417624473571777 | BCE Loss: 1.01991605758667\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 3.537121295928955 | KNN Loss: 2.4680187702178955 | BCE Loss: 1.0691025257110596\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 3.47249698638916 | KNN Loss: 2.447641372680664 | BCE Loss: 1.024855613708496\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 3.464329957962036 | KNN Loss: 2.441087007522583 | BCE Loss: 1.0232429504394531\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 3.449038505554199 | KNN Loss: 2.441088914871216 | BCE Loss: 1.0079495906829834\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 3.4627437591552734 | KNN Loss: 2.4703776836395264 | BCE Loss: 0.9923659563064575\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 3.480750322341919 | KNN Loss: 2.4487318992614746 | BCE Loss: 1.0320184230804443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 3.4773764610290527 | KNN Loss: 2.453782320022583 | BCE Loss: 1.0235941410064697\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 3.446526050567627 | KNN Loss: 2.4267659187316895 | BCE Loss: 1.019760251045227\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 3.4576079845428467 | KNN Loss: 2.4474525451660156 | BCE Loss: 1.010155439376831\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 3.4715757369995117 | KNN Loss: 2.4687118530273438 | BCE Loss: 1.0028637647628784\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 3.4999752044677734 | KNN Loss: 2.4689481258392334 | BCE Loss: 1.0310271978378296\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 3.458094835281372 | KNN Loss: 2.43710994720459 | BCE Loss: 1.0209848880767822\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 3.4802255630493164 | KNN Loss: 2.4444146156311035 | BCE Loss: 1.035810947418213\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 3.482548713684082 | KNN Loss: 2.4590632915496826 | BCE Loss: 1.0234853029251099\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 3.4817824363708496 | KNN Loss: 2.4618122577667236 | BCE Loss: 1.019970178604126\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 3.4884490966796875 | KNN Loss: 2.4690983295440674 | BCE Loss: 1.0193506479263306\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 3.4049289226531982 | KNN Loss: 2.4224133491516113 | BCE Loss: 0.9825155138969421\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 3.4490485191345215 | KNN Loss: 2.4178974628448486 | BCE Loss: 1.0311510562896729\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 3.480320453643799 | KNN Loss: 2.4778964519500732 | BCE Loss: 1.0024241209030151\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 3.4119279384613037 | KNN Loss: 2.3975648880004883 | BCE Loss: 1.0143630504608154\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 3.4655613899230957 | KNN Loss: 2.45639967918396 | BCE Loss: 1.0091617107391357\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 3.477355718612671 | KNN Loss: 2.446045160293579 | BCE Loss: 1.0313105583190918\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 3.480705976486206 | KNN Loss: 2.474984645843506 | BCE Loss: 1.0057213306427002\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 3.4598774909973145 | KNN Loss: 2.428586959838867 | BCE Loss: 1.0312905311584473\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 3.4655282497406006 | KNN Loss: 2.443512439727783 | BCE Loss: 1.0220158100128174\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 3.458083391189575 | KNN Loss: 2.4477009773254395 | BCE Loss: 1.0103824138641357\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 3.4327709674835205 | KNN Loss: 2.4497463703155518 | BCE Loss: 0.9830245971679688\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 3.482802391052246 | KNN Loss: 2.4691412448883057 | BCE Loss: 1.01366126537323\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 3.479473829269409 | KNN Loss: 2.4571189880371094 | BCE Loss: 1.0223548412322998\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 3.4580483436584473 | KNN Loss: 2.449897527694702 | BCE Loss: 1.0081508159637451\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 3.4942681789398193 | KNN Loss: 2.4566171169281006 | BCE Loss: 1.0376510620117188\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 3.4834914207458496 | KNN Loss: 2.481595516204834 | BCE Loss: 1.001895785331726\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 3.4725399017333984 | KNN Loss: 2.45404052734375 | BCE Loss: 1.0184993743896484\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 3.4576618671417236 | KNN Loss: 2.434659004211426 | BCE Loss: 1.0230028629302979\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 3.4688100814819336 | KNN Loss: 2.4380722045898438 | BCE Loss: 1.0307379961013794\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 3.5170681476593018 | KNN Loss: 2.477062463760376 | BCE Loss: 1.0400056838989258\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 3.459076404571533 | KNN Loss: 2.459177255630493 | BCE Loss: 0.9998992085456848\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 3.502523899078369 | KNN Loss: 2.476898193359375 | BCE Loss: 1.0256258249282837\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 3.4730277061462402 | KNN Loss: 2.4459445476531982 | BCE Loss: 1.027083158493042\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 3.4710793495178223 | KNN Loss: 2.455157995223999 | BCE Loss: 1.0159214735031128\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 3.4476852416992188 | KNN Loss: 2.4480791091918945 | BCE Loss: 0.9996061325073242\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 3.4480390548706055 | KNN Loss: 2.446620464324951 | BCE Loss: 1.0014187097549438\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 3.5082623958587646 | KNN Loss: 2.481651544570923 | BCE Loss: 1.0266108512878418\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 3.456099033355713 | KNN Loss: 2.4838082790374756 | BCE Loss: 0.9722908735275269\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 3.4436748027801514 | KNN Loss: 2.447706460952759 | BCE Loss: 0.9959683418273926\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 3.4831206798553467 | KNN Loss: 2.4325153827667236 | BCE Loss: 1.050605297088623\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 3.485532760620117 | KNN Loss: 2.459632396697998 | BCE Loss: 1.0259002447128296\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 3.465421199798584 | KNN Loss: 2.4483776092529297 | BCE Loss: 1.0170434713363647\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 3.4806032180786133 | KNN Loss: 2.4835948944091797 | BCE Loss: 0.9970083236694336\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 3.44708514213562 | KNN Loss: 2.4279978275299072 | BCE Loss: 1.019087314605713\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 3.517993211746216 | KNN Loss: 2.4764487743377686 | BCE Loss: 1.0415444374084473\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 3.4853780269622803 | KNN Loss: 2.4590134620666504 | BCE Loss: 1.0263645648956299\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 3.4727962017059326 | KNN Loss: 2.454770803451538 | BCE Loss: 1.0180253982543945\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 3.4770522117614746 | KNN Loss: 2.462447166442871 | BCE Loss: 1.0146050453186035\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 3.414562702178955 | KNN Loss: 2.4198989868164062 | BCE Loss: 0.9946637153625488\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 3.4536612033843994 | KNN Loss: 2.451698064804077 | BCE Loss: 1.0019631385803223\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 3.476318120956421 | KNN Loss: 2.4643187522888184 | BCE Loss: 1.0119993686676025\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 3.431976079940796 | KNN Loss: 2.4178147315979004 | BCE Loss: 1.0141613483428955\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 3.494260787963867 | KNN Loss: 2.471015214920044 | BCE Loss: 1.0232455730438232\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 3.491318702697754 | KNN Loss: 2.462796449661255 | BCE Loss: 1.028522253036499\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 3.462883234024048 | KNN Loss: 2.4308788776397705 | BCE Loss: 1.0320043563842773\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 3.4981725215911865 | KNN Loss: 2.4649100303649902 | BCE Loss: 1.0332624912261963\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 3.480349540710449 | KNN Loss: 2.454479694366455 | BCE Loss: 1.0258697271347046\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 3.4325594902038574 | KNN Loss: 2.453775405883789 | BCE Loss: 0.9787840247154236\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 3.4827425479888916 | KNN Loss: 2.4604480266571045 | BCE Loss: 1.022294521331787\n",
      "Epoch   349: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 3.4874863624572754 | KNN Loss: 2.4714226722717285 | BCE Loss: 1.0160636901855469\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 3.488741159439087 | KNN Loss: 2.437378168106079 | BCE Loss: 1.0513629913330078\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 3.5107405185699463 | KNN Loss: 2.476855754852295 | BCE Loss: 1.0338847637176514\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 3.4826905727386475 | KNN Loss: 2.4462976455688477 | BCE Loss: 1.0363929271697998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 3.497955560684204 | KNN Loss: 2.4773476123809814 | BCE Loss: 1.0206079483032227\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 3.4847192764282227 | KNN Loss: 2.494239330291748 | BCE Loss: 0.9904800653457642\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 3.466337203979492 | KNN Loss: 2.4480340480804443 | BCE Loss: 1.0183031558990479\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 3.4647974967956543 | KNN Loss: 2.4471867084503174 | BCE Loss: 1.017610788345337\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 3.445967197418213 | KNN Loss: 2.442220449447632 | BCE Loss: 1.003746747970581\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 3.458871603012085 | KNN Loss: 2.441967248916626 | BCE Loss: 1.016904354095459\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 3.4091196060180664 | KNN Loss: 2.4076266288757324 | BCE Loss: 1.0014928579330444\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 3.4765677452087402 | KNN Loss: 2.4457225799560547 | BCE Loss: 1.0308451652526855\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 3.4751691818237305 | KNN Loss: 2.462843418121338 | BCE Loss: 1.0123257637023926\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 3.468831777572632 | KNN Loss: 2.4594485759735107 | BCE Loss: 1.009383201599121\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 3.523878574371338 | KNN Loss: 2.4789175987243652 | BCE Loss: 1.0449610948562622\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 3.427765130996704 | KNN Loss: 2.436800241470337 | BCE Loss: 0.9909648895263672\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 3.4720194339752197 | KNN Loss: 2.457434892654419 | BCE Loss: 1.0145845413208008\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 3.4371039867401123 | KNN Loss: 2.4367334842681885 | BCE Loss: 1.0003705024719238\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 3.4650821685791016 | KNN Loss: 2.4447858333587646 | BCE Loss: 1.0202962160110474\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 3.5254998207092285 | KNN Loss: 2.4922003746032715 | BCE Loss: 1.0332995653152466\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 3.4959325790405273 | KNN Loss: 2.492194890975952 | BCE Loss: 1.0037378072738647\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 3.4467265605926514 | KNN Loss: 2.4203481674194336 | BCE Loss: 1.0263783931732178\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 3.4380152225494385 | KNN Loss: 2.4338674545288086 | BCE Loss: 1.0041477680206299\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 3.517751932144165 | KNN Loss: 2.473461389541626 | BCE Loss: 1.044290542602539\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 3.4757347106933594 | KNN Loss: 2.4574241638183594 | BCE Loss: 1.0183104276657104\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 3.450887441635132 | KNN Loss: 2.4372098445892334 | BCE Loss: 1.0136775970458984\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 3.493271827697754 | KNN Loss: 2.469536542892456 | BCE Loss: 1.0237354040145874\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 3.4947900772094727 | KNN Loss: 2.453343629837036 | BCE Loss: 1.041446328163147\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 3.462186336517334 | KNN Loss: 2.4611923694610596 | BCE Loss: 1.000994086265564\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 3.4522781372070312 | KNN Loss: 2.4472227096557617 | BCE Loss: 1.00505530834198\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 3.434190273284912 | KNN Loss: 2.458134412765503 | BCE Loss: 0.9760557413101196\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 3.482208728790283 | KNN Loss: 2.440810441970825 | BCE Loss: 1.041398286819458\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 3.499727487564087 | KNN Loss: 2.467066764831543 | BCE Loss: 1.032660722732544\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 3.4748432636260986 | KNN Loss: 2.4518067836761475 | BCE Loss: 1.0230364799499512\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 3.528898000717163 | KNN Loss: 2.4662041664123535 | BCE Loss: 1.0626938343048096\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 3.466097593307495 | KNN Loss: 2.440554141998291 | BCE Loss: 1.025543451309204\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 3.480415105819702 | KNN Loss: 2.4206182956695557 | BCE Loss: 1.0597968101501465\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 3.511413097381592 | KNN Loss: 2.4872794151306152 | BCE Loss: 1.0241338014602661\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 3.5001840591430664 | KNN Loss: 2.478036642074585 | BCE Loss: 1.022147297859192\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 3.454468250274658 | KNN Loss: 2.4457921981811523 | BCE Loss: 1.0086760520935059\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 3.476968765258789 | KNN Loss: 2.451415538787842 | BCE Loss: 1.0255532264709473\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 3.449131488800049 | KNN Loss: 2.4525763988494873 | BCE Loss: 0.9965551495552063\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 3.473533868789673 | KNN Loss: 2.461618423461914 | BCE Loss: 1.0119154453277588\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 3.5164990425109863 | KNN Loss: 2.4762024879455566 | BCE Loss: 1.0402965545654297\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 3.4905078411102295 | KNN Loss: 2.446876049041748 | BCE Loss: 1.0436317920684814\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 3.4647209644317627 | KNN Loss: 2.4678165912628174 | BCE Loss: 0.9969043731689453\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 3.459022283554077 | KNN Loss: 2.447479248046875 | BCE Loss: 1.0115430355072021\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 3.5179243087768555 | KNN Loss: 2.4840011596679688 | BCE Loss: 1.0339231491088867\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 3.4393718242645264 | KNN Loss: 2.4384095668792725 | BCE Loss: 1.000962257385254\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 3.454071521759033 | KNN Loss: 2.441220760345459 | BCE Loss: 1.0128506422042847\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 3.455592155456543 | KNN Loss: 2.4406206607818604 | BCE Loss: 1.0149716138839722\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 3.480212450027466 | KNN Loss: 2.446195363998413 | BCE Loss: 1.0340170860290527\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 3.448627471923828 | KNN Loss: 2.442958354949951 | BCE Loss: 1.0056692361831665\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 3.4260952472686768 | KNN Loss: 2.4453461170196533 | BCE Loss: 0.9807491302490234\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 3.4661402702331543 | KNN Loss: 2.4328622817993164 | BCE Loss: 1.0332781076431274\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 3.462911605834961 | KNN Loss: 2.434868812561035 | BCE Loss: 1.0280427932739258\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 3.466634750366211 | KNN Loss: 2.44128680229187 | BCE Loss: 1.0253479480743408\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 3.4370250701904297 | KNN Loss: 2.4174911975860596 | BCE Loss: 1.0195337533950806\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 3.507204055786133 | KNN Loss: 2.4423906803131104 | BCE Loss: 1.064813494682312\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 3.5317623615264893 | KNN Loss: 2.523937702178955 | BCE Loss: 1.0078246593475342\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 3.4519102573394775 | KNN Loss: 2.450918674468994 | BCE Loss: 1.0009915828704834\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 3.4581146240234375 | KNN Loss: 2.4232749938964844 | BCE Loss: 1.0348396301269531\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 3.4996182918548584 | KNN Loss: 2.47114896774292 | BCE Loss: 1.0284693241119385\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 3.4697375297546387 | KNN Loss: 2.4479222297668457 | BCE Loss: 1.0218151807785034\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 3.4808945655822754 | KNN Loss: 2.4504427909851074 | BCE Loss: 1.030451774597168\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 3.5254604816436768 | KNN Loss: 2.4604427814483643 | BCE Loss: 1.0650177001953125\n",
      "Epoch   360: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 3.471665382385254 | KNN Loss: 2.4507546424865723 | BCE Loss: 1.020910620689392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 3.497386932373047 | KNN Loss: 2.449998140335083 | BCE Loss: 1.0473887920379639\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 3.4849693775177 | KNN Loss: 2.448499917984009 | BCE Loss: 1.0364694595336914\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 3.457946300506592 | KNN Loss: 2.4493463039398193 | BCE Loss: 1.008600115776062\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 3.478201389312744 | KNN Loss: 2.450784683227539 | BCE Loss: 1.0274165868759155\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 3.4787216186523438 | KNN Loss: 2.4399068355560303 | BCE Loss: 1.038814902305603\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 3.4922869205474854 | KNN Loss: 2.46174693107605 | BCE Loss: 1.0305399894714355\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 3.4928555488586426 | KNN Loss: 2.4699490070343018 | BCE Loss: 1.0229065418243408\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 3.477891445159912 | KNN Loss: 2.452258348464966 | BCE Loss: 1.0256332159042358\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 3.437204360961914 | KNN Loss: 2.4281811714172363 | BCE Loss: 1.0090231895446777\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 3.501508951187134 | KNN Loss: 2.4809200763702393 | BCE Loss: 1.0205888748168945\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 3.493222713470459 | KNN Loss: 2.4966330528259277 | BCE Loss: 0.9965897798538208\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 3.5110549926757812 | KNN Loss: 2.5078792572021484 | BCE Loss: 1.0031757354736328\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 3.4981443881988525 | KNN Loss: 2.4689455032348633 | BCE Loss: 1.0291988849639893\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 3.500579357147217 | KNN Loss: 2.4561655521392822 | BCE Loss: 1.0444138050079346\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 3.4618663787841797 | KNN Loss: 2.4935977458953857 | BCE Loss: 0.968268632888794\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 3.4606359004974365 | KNN Loss: 2.4482946395874023 | BCE Loss: 1.0123412609100342\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 3.4983067512512207 | KNN Loss: 2.483323574066162 | BCE Loss: 1.0149831771850586\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 3.450596570968628 | KNN Loss: 2.4380738735198975 | BCE Loss: 1.0125226974487305\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 3.4570538997650146 | KNN Loss: 2.4455978870391846 | BCE Loss: 1.01145601272583\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 3.5152227878570557 | KNN Loss: 2.4839377403259277 | BCE Loss: 1.031285047531128\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 3.466475009918213 | KNN Loss: 2.442949056625366 | BCE Loss: 1.0235258340835571\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 3.494074583053589 | KNN Loss: 2.481356382369995 | BCE Loss: 1.0127182006835938\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 3.465229034423828 | KNN Loss: 2.45300555229187 | BCE Loss: 1.012223482131958\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 3.4421305656433105 | KNN Loss: 2.4425766468048096 | BCE Loss: 0.9995540380477905\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 3.5098586082458496 | KNN Loss: 2.484004497528076 | BCE Loss: 1.0258541107177734\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 3.4592325687408447 | KNN Loss: 2.4539291858673096 | BCE Loss: 1.0053033828735352\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 3.4348363876342773 | KNN Loss: 2.4248507022857666 | BCE Loss: 1.0099855661392212\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 3.467041254043579 | KNN Loss: 2.463611125946045 | BCE Loss: 1.0034301280975342\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 3.4874401092529297 | KNN Loss: 2.4724347591400146 | BCE Loss: 1.0150054693222046\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 3.4855587482452393 | KNN Loss: 2.455315113067627 | BCE Loss: 1.0302436351776123\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 3.4943599700927734 | KNN Loss: 2.4764461517333984 | BCE Loss: 1.017913818359375\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 3.4734742641448975 | KNN Loss: 2.4641788005828857 | BCE Loss: 1.0092954635620117\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 3.4760074615478516 | KNN Loss: 2.4575157165527344 | BCE Loss: 1.0184917449951172\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 3.466336727142334 | KNN Loss: 2.4271485805511475 | BCE Loss: 1.039188265800476\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 3.451415777206421 | KNN Loss: 2.434112071990967 | BCE Loss: 1.017303705215454\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 3.510418653488159 | KNN Loss: 2.4803051948547363 | BCE Loss: 1.0301134586334229\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 3.453464984893799 | KNN Loss: 2.461756706237793 | BCE Loss: 0.9917082786560059\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 3.509439468383789 | KNN Loss: 2.4750282764434814 | BCE Loss: 1.0344111919403076\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 3.499227523803711 | KNN Loss: 2.47298002243042 | BCE Loss: 1.026247501373291\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 3.455962657928467 | KNN Loss: 2.4583091735839844 | BCE Loss: 0.9976534843444824\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 3.4507007598876953 | KNN Loss: 2.4765939712524414 | BCE Loss: 0.9741069078445435\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 3.416146755218506 | KNN Loss: 2.4387001991271973 | BCE Loss: 0.9774466753005981\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 3.4657464027404785 | KNN Loss: 2.438999891281128 | BCE Loss: 1.0267465114593506\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 3.4652559757232666 | KNN Loss: 2.4492886066436768 | BCE Loss: 1.0159673690795898\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 3.4497621059417725 | KNN Loss: 2.4520435333251953 | BCE Loss: 0.9977185726165771\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 3.4828031063079834 | KNN Loss: 2.463068723678589 | BCE Loss: 1.0197343826293945\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 3.4342832565307617 | KNN Loss: 2.4464399814605713 | BCE Loss: 0.9878432750701904\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 3.5212502479553223 | KNN Loss: 2.4931466579437256 | BCE Loss: 1.0281035900115967\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 3.4474475383758545 | KNN Loss: 2.4412460327148438 | BCE Loss: 1.0062015056610107\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 3.4908719062805176 | KNN Loss: 2.4502289295196533 | BCE Loss: 1.0406429767608643\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 3.4756784439086914 | KNN Loss: 2.4827277660369873 | BCE Loss: 0.9929506778717041\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 3.48423171043396 | KNN Loss: 2.4843389987945557 | BCE Loss: 0.9998927116394043\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 3.495358467102051 | KNN Loss: 2.485912561416626 | BCE Loss: 1.0094460248947144\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 3.4968018531799316 | KNN Loss: 2.4737346172332764 | BCE Loss: 1.0230673551559448\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 3.4491190910339355 | KNN Loss: 2.437521457672119 | BCE Loss: 1.0115976333618164\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 3.498774528503418 | KNN Loss: 2.4895248413085938 | BCE Loss: 1.0092498064041138\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 3.473780870437622 | KNN Loss: 2.448059558868408 | BCE Loss: 1.0257213115692139\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 3.5001821517944336 | KNN Loss: 2.4694838523864746 | BCE Loss: 1.030698299407959\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 3.514770984649658 | KNN Loss: 2.50753116607666 | BCE Loss: 1.0072399377822876\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 3.464886426925659 | KNN Loss: 2.455639600753784 | BCE Loss: 1.009246826171875\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 3.515183687210083 | KNN Loss: 2.4952235221862793 | BCE Loss: 1.0199601650238037\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 3.5052270889282227 | KNN Loss: 2.494635820388794 | BCE Loss: 1.0105912685394287\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 3.486912727355957 | KNN Loss: 2.478564739227295 | BCE Loss: 1.008347988128662\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 3.4630532264709473 | KNN Loss: 2.4296061992645264 | BCE Loss: 1.033447027206421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 3.437398672103882 | KNN Loss: 2.4345626831054688 | BCE Loss: 1.002835988998413\n",
      "Epoch   371: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 3.4330203533172607 | KNN Loss: 2.45805287361145 | BCE Loss: 0.9749674797058105\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 3.4354896545410156 | KNN Loss: 2.4290504455566406 | BCE Loss: 1.006439208984375\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 3.511507749557495 | KNN Loss: 2.493105173110962 | BCE Loss: 1.0184025764465332\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 3.4580392837524414 | KNN Loss: 2.4449856281280518 | BCE Loss: 1.0130537748336792\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 3.4615907669067383 | KNN Loss: 2.4442012310028076 | BCE Loss: 1.0173895359039307\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 3.4890990257263184 | KNN Loss: 2.4638450145721436 | BCE Loss: 1.0252541303634644\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 3.4668054580688477 | KNN Loss: 2.425010919570923 | BCE Loss: 1.0417946577072144\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 3.4632370471954346 | KNN Loss: 2.459429979324341 | BCE Loss: 1.0038070678710938\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 3.460287570953369 | KNN Loss: 2.4521408081054688 | BCE Loss: 1.00814688205719\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 3.460463762283325 | KNN Loss: 2.4714009761810303 | BCE Loss: 0.9890628457069397\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 3.491417169570923 | KNN Loss: 2.4562909603118896 | BCE Loss: 1.0351262092590332\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 3.4245052337646484 | KNN Loss: 2.4311068058013916 | BCE Loss: 0.9933984279632568\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 3.5127687454223633 | KNN Loss: 2.478060245513916 | BCE Loss: 1.0347086191177368\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 3.4572529792785645 | KNN Loss: 2.422470808029175 | BCE Loss: 1.0347821712493896\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 3.4619386196136475 | KNN Loss: 2.4513144493103027 | BCE Loss: 1.0106241703033447\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 3.5029373168945312 | KNN Loss: 2.4763240814208984 | BCE Loss: 1.0266133546829224\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 3.488389492034912 | KNN Loss: 2.4593727588653564 | BCE Loss: 1.0290167331695557\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 3.469054937362671 | KNN Loss: 2.469740390777588 | BCE Loss: 0.999314546585083\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 3.430675983428955 | KNN Loss: 2.437689781188965 | BCE Loss: 0.9929861426353455\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 3.453183650970459 | KNN Loss: 2.4353644847869873 | BCE Loss: 1.0178192853927612\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 3.4433958530426025 | KNN Loss: 2.443570375442505 | BCE Loss: 0.9998254179954529\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 3.4271841049194336 | KNN Loss: 2.429440975189209 | BCE Loss: 0.9977432489395142\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 3.4386768341064453 | KNN Loss: 2.4267749786376953 | BCE Loss: 1.0119019746780396\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 3.4670355319976807 | KNN Loss: 2.439922571182251 | BCE Loss: 1.0271129608154297\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 3.469900131225586 | KNN Loss: 2.443904161453247 | BCE Loss: 1.0259960889816284\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 3.521576166152954 | KNN Loss: 2.4738945960998535 | BCE Loss: 1.0476815700531006\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 3.4822168350219727 | KNN Loss: 2.4504482746124268 | BCE Loss: 1.031768560409546\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 3.4437289237976074 | KNN Loss: 2.444404125213623 | BCE Loss: 0.9993247389793396\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 3.44624662399292 | KNN Loss: 2.4417264461517334 | BCE Loss: 1.0045201778411865\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 3.445014238357544 | KNN Loss: 2.4206061363220215 | BCE Loss: 1.0244081020355225\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 3.4987621307373047 | KNN Loss: 2.4736826419830322 | BCE Loss: 1.025079369544983\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 3.4807333946228027 | KNN Loss: 2.458162546157837 | BCE Loss: 1.0225707292556763\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 3.4591150283813477 | KNN Loss: 2.4499094486236572 | BCE Loss: 1.0092054605484009\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 3.432215452194214 | KNN Loss: 2.437669038772583 | BCE Loss: 0.9945464134216309\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 3.4439315795898438 | KNN Loss: 2.42642879486084 | BCE Loss: 1.0175026655197144\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 3.481276273727417 | KNN Loss: 2.4694266319274902 | BCE Loss: 1.0118496417999268\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 3.4711403846740723 | KNN Loss: 2.4597623348236084 | BCE Loss: 1.0113779306411743\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 3.5112619400024414 | KNN Loss: 2.487967014312744 | BCE Loss: 1.0232950448989868\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 3.470903158187866 | KNN Loss: 2.448876142501831 | BCE Loss: 1.0220270156860352\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 3.435586452484131 | KNN Loss: 2.4430367946624756 | BCE Loss: 0.9925495982170105\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 3.475177049636841 | KNN Loss: 2.447201728820801 | BCE Loss: 1.02797532081604\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 3.4601171016693115 | KNN Loss: 2.440157651901245 | BCE Loss: 1.0199594497680664\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 3.4540719985961914 | KNN Loss: 2.4364326000213623 | BCE Loss: 1.017639398574829\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 3.457589864730835 | KNN Loss: 2.45703387260437 | BCE Loss: 1.0005559921264648\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 3.468116521835327 | KNN Loss: 2.4318926334381104 | BCE Loss: 1.0362238883972168\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 3.5029335021972656 | KNN Loss: 2.4696388244628906 | BCE Loss: 1.033294677734375\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 3.5098495483398438 | KNN Loss: 2.460893392562866 | BCE Loss: 1.048956274986267\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 3.4918909072875977 | KNN Loss: 2.449429750442505 | BCE Loss: 1.0424611568450928\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 3.4725003242492676 | KNN Loss: 2.455528736114502 | BCE Loss: 1.0169717073440552\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 3.5032310485839844 | KNN Loss: 2.4539577960968018 | BCE Loss: 1.0492733716964722\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 3.4690158367156982 | KNN Loss: 2.4400017261505127 | BCE Loss: 1.0290141105651855\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 3.478968620300293 | KNN Loss: 2.4371602535247803 | BCE Loss: 1.0418084859848022\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 3.4561691284179688 | KNN Loss: 2.498227119445801 | BCE Loss: 0.9579418897628784\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 3.455484628677368 | KNN Loss: 2.443223714828491 | BCE Loss: 1.012260913848877\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 3.4662861824035645 | KNN Loss: 2.44077467918396 | BCE Loss: 1.0255115032196045\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 3.500483274459839 | KNN Loss: 2.463780164718628 | BCE Loss: 1.036703109741211\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 3.4549684524536133 | KNN Loss: 2.460638999938965 | BCE Loss: 0.9943293929100037\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 3.52115535736084 | KNN Loss: 2.494295120239258 | BCE Loss: 1.026860237121582\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 3.449171543121338 | KNN Loss: 2.42056941986084 | BCE Loss: 1.028602123260498\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 3.43088960647583 | KNN Loss: 2.427309274673462 | BCE Loss: 1.0035803318023682\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 3.444756031036377 | KNN Loss: 2.461249351501465 | BCE Loss: 0.9835066199302673\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 3.468106746673584 | KNN Loss: 2.462832450866699 | BCE Loss: 1.0052742958068848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 3.473507881164551 | KNN Loss: 2.446810007095337 | BCE Loss: 1.0266978740692139\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 3.463573694229126 | KNN Loss: 2.440509796142578 | BCE Loss: 1.0230638980865479\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 3.500549077987671 | KNN Loss: 2.464442491531372 | BCE Loss: 1.0361065864562988\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 3.4594476222991943 | KNN Loss: 2.4439120292663574 | BCE Loss: 1.015535593032837\n",
      "Epoch   382: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 3.5113072395324707 | KNN Loss: 2.4863266944885254 | BCE Loss: 1.0249804258346558\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 3.4707465171813965 | KNN Loss: 2.4653351306915283 | BCE Loss: 1.0054112672805786\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 3.438887119293213 | KNN Loss: 2.4046576023101807 | BCE Loss: 1.0342293977737427\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 3.474118232727051 | KNN Loss: 2.454413652420044 | BCE Loss: 1.0197045803070068\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 3.4701390266418457 | KNN Loss: 2.442411422729492 | BCE Loss: 1.0277276039123535\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 3.4545724391937256 | KNN Loss: 2.4414377212524414 | BCE Loss: 1.0131347179412842\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 3.5253095626831055 | KNN Loss: 2.4703052043914795 | BCE Loss: 1.055004358291626\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 3.4816479682922363 | KNN Loss: 2.5015244483947754 | BCE Loss: 0.9801235198974609\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 3.473762035369873 | KNN Loss: 2.4704978466033936 | BCE Loss: 1.00326406955719\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 3.4648280143737793 | KNN Loss: 2.431764841079712 | BCE Loss: 1.0330631732940674\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 3.447545289993286 | KNN Loss: 2.442873001098633 | BCE Loss: 1.0046722888946533\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 3.455833911895752 | KNN Loss: 2.456883430480957 | BCE Loss: 0.9989504814147949\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 3.484940767288208 | KNN Loss: 2.477128744125366 | BCE Loss: 1.0078120231628418\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 3.4625391960144043 | KNN Loss: 2.45638370513916 | BCE Loss: 1.0061553716659546\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 3.5040249824523926 | KNN Loss: 2.4815361499786377 | BCE Loss: 1.0224888324737549\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 3.502919912338257 | KNN Loss: 2.459211826324463 | BCE Loss: 1.043708086013794\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 3.5036163330078125 | KNN Loss: 2.4718806743621826 | BCE Loss: 1.0317355394363403\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 3.4706830978393555 | KNN Loss: 2.46451997756958 | BCE Loss: 1.0061631202697754\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 3.453047275543213 | KNN Loss: 2.4572324752807617 | BCE Loss: 0.9958148002624512\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 3.504887104034424 | KNN Loss: 2.4595444202423096 | BCE Loss: 1.0453426837921143\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 3.4606692790985107 | KNN Loss: 2.4660069942474365 | BCE Loss: 0.9946622252464294\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 3.4522252082824707 | KNN Loss: 2.4299747943878174 | BCE Loss: 1.0222504138946533\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 3.442962408065796 | KNN Loss: 2.4356613159179688 | BCE Loss: 1.0073010921478271\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 3.4911813735961914 | KNN Loss: 2.493875741958618 | BCE Loss: 0.9973055124282837\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 3.4834375381469727 | KNN Loss: 2.4595706462860107 | BCE Loss: 1.0238667726516724\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 3.453829050064087 | KNN Loss: 2.4279274940490723 | BCE Loss: 1.0259015560150146\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 3.4920270442962646 | KNN Loss: 2.456160545349121 | BCE Loss: 1.0358664989471436\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 3.4965696334838867 | KNN Loss: 2.4667294025421143 | BCE Loss: 1.0298402309417725\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 3.496971368789673 | KNN Loss: 2.482936143875122 | BCE Loss: 1.0140352249145508\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 3.4773926734924316 | KNN Loss: 2.444638967514038 | BCE Loss: 1.0327537059783936\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 3.4542784690856934 | KNN Loss: 2.4578945636749268 | BCE Loss: 0.9963838458061218\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 3.4824275970458984 | KNN Loss: 2.472895860671997 | BCE Loss: 1.0095317363739014\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 3.474663019180298 | KNN Loss: 2.445108652114868 | BCE Loss: 1.0295543670654297\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 3.4369962215423584 | KNN Loss: 2.4589340686798096 | BCE Loss: 0.9780621528625488\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 3.42451548576355 | KNN Loss: 2.429954767227173 | BCE Loss: 0.994560718536377\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 3.484035015106201 | KNN Loss: 2.4719111919403076 | BCE Loss: 1.012123942375183\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 3.4479188919067383 | KNN Loss: 2.4328951835632324 | BCE Loss: 1.0150235891342163\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 3.441690444946289 | KNN Loss: 2.432025909423828 | BCE Loss: 1.0096644163131714\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 3.451859474182129 | KNN Loss: 2.4600701332092285 | BCE Loss: 0.9917894601821899\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 3.4731526374816895 | KNN Loss: 2.4491610527038574 | BCE Loss: 1.023991584777832\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 3.492079257965088 | KNN Loss: 2.4677352905273438 | BCE Loss: 1.0243439674377441\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 3.4420037269592285 | KNN Loss: 2.431835412979126 | BCE Loss: 1.0101683139801025\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 3.4652652740478516 | KNN Loss: 2.4556827545166016 | BCE Loss: 1.0095826387405396\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 3.4620065689086914 | KNN Loss: 2.4370388984680176 | BCE Loss: 1.0249676704406738\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 3.4538328647613525 | KNN Loss: 2.439164400100708 | BCE Loss: 1.0146684646606445\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 3.4519288539886475 | KNN Loss: 2.4434871673583984 | BCE Loss: 1.008441686630249\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 3.4865407943725586 | KNN Loss: 2.4706976413726807 | BCE Loss: 1.015843152999878\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 3.492743492126465 | KNN Loss: 2.4684808254241943 | BCE Loss: 1.0242626667022705\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 3.4748525619506836 | KNN Loss: 2.4695963859558105 | BCE Loss: 1.005256175994873\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 3.4503798484802246 | KNN Loss: 2.4280991554260254 | BCE Loss: 1.0222808122634888\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 3.4649763107299805 | KNN Loss: 2.4399592876434326 | BCE Loss: 1.0250170230865479\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 3.4593732357025146 | KNN Loss: 2.4519593715667725 | BCE Loss: 1.0074138641357422\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 3.4868266582489014 | KNN Loss: 2.4527902603149414 | BCE Loss: 1.03403639793396\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 3.4760231971740723 | KNN Loss: 2.4814271926879883 | BCE Loss: 0.9945958852767944\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 3.4689059257507324 | KNN Loss: 2.4644320011138916 | BCE Loss: 1.0044739246368408\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 3.4865002632141113 | KNN Loss: 2.4427924156188965 | BCE Loss: 1.0437078475952148\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 3.50136137008667 | KNN Loss: 2.4884817600250244 | BCE Loss: 1.012879490852356\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 3.4444284439086914 | KNN Loss: 2.415766716003418 | BCE Loss: 1.0286617279052734\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 3.4682648181915283 | KNN Loss: 2.47068190574646 | BCE Loss: 0.9975829720497131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 3.487957000732422 | KNN Loss: 2.487403154373169 | BCE Loss: 1.000553846359253\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 3.4655370712280273 | KNN Loss: 2.4671952724456787 | BCE Loss: 0.9983417987823486\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 3.490931510925293 | KNN Loss: 2.456716299057007 | BCE Loss: 1.0342150926589966\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 3.444540500640869 | KNN Loss: 2.4568886756896973 | BCE Loss: 0.9876517653465271\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 3.4635839462280273 | KNN Loss: 2.4381258487701416 | BCE Loss: 1.0254582166671753\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 3.4503414630889893 | KNN Loss: 2.431884527206421 | BCE Loss: 1.0184569358825684\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 3.4753336906433105 | KNN Loss: 2.430148124694824 | BCE Loss: 1.0451855659484863\n",
      "Epoch   393: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 3.4580166339874268 | KNN Loss: 2.4345953464508057 | BCE Loss: 1.023421287536621\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 3.4585464000701904 | KNN Loss: 2.4252278804779053 | BCE Loss: 1.0333185195922852\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 3.475128412246704 | KNN Loss: 2.454115390777588 | BCE Loss: 1.0210130214691162\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 3.4525678157806396 | KNN Loss: 2.4397177696228027 | BCE Loss: 1.012850046157837\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 3.47780179977417 | KNN Loss: 2.452497959136963 | BCE Loss: 1.0253037214279175\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 3.438983917236328 | KNN Loss: 2.4559364318847656 | BCE Loss: 0.9830474257469177\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 3.5000391006469727 | KNN Loss: 2.4563307762145996 | BCE Loss: 1.043708324432373\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 3.4589366912841797 | KNN Loss: 2.429896354675293 | BCE Loss: 1.0290402173995972\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 3.4798343181610107 | KNN Loss: 2.4594430923461914 | BCE Loss: 1.0203912258148193\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 3.470247268676758 | KNN Loss: 2.4462902545928955 | BCE Loss: 1.0239571332931519\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 3.467226982116699 | KNN Loss: 2.468883752822876 | BCE Loss: 0.9983431696891785\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 3.468334436416626 | KNN Loss: 2.4348604679107666 | BCE Loss: 1.0334739685058594\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 3.4805119037628174 | KNN Loss: 2.458671808242798 | BCE Loss: 1.0218400955200195\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 3.4959826469421387 | KNN Loss: 2.465395450592041 | BCE Loss: 1.0305871963500977\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 3.5192389488220215 | KNN Loss: 2.5088541507720947 | BCE Loss: 1.0103846788406372\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 3.4587533473968506 | KNN Loss: 2.423163890838623 | BCE Loss: 1.0355894565582275\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 3.4622955322265625 | KNN Loss: 2.4392027854919434 | BCE Loss: 1.0230928659439087\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 3.4577643871307373 | KNN Loss: 2.457157850265503 | BCE Loss: 1.0006065368652344\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 3.490746259689331 | KNN Loss: 2.4613230228424072 | BCE Loss: 1.0294232368469238\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 3.4676969051361084 | KNN Loss: 2.4552974700927734 | BCE Loss: 1.012399435043335\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 3.4752566814422607 | KNN Loss: 2.4616382122039795 | BCE Loss: 1.0136184692382812\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 3.4825658798217773 | KNN Loss: 2.439093589782715 | BCE Loss: 1.043472409248352\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 3.464218854904175 | KNN Loss: 2.441797971725464 | BCE Loss: 1.022420883178711\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 3.4989185333251953 | KNN Loss: 2.4726340770721436 | BCE Loss: 1.0262844562530518\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 3.479905843734741 | KNN Loss: 2.453876495361328 | BCE Loss: 1.026029348373413\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 3.4796433448791504 | KNN Loss: 2.4514951705932617 | BCE Loss: 1.0281482934951782\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 3.463794231414795 | KNN Loss: 2.453779935836792 | BCE Loss: 1.0100144147872925\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 3.483959197998047 | KNN Loss: 2.4472885131835938 | BCE Loss: 1.0366706848144531\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 3.51963210105896 | KNN Loss: 2.4816341400146484 | BCE Loss: 1.0379979610443115\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 3.4739747047424316 | KNN Loss: 2.443056583404541 | BCE Loss: 1.030918002128601\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 3.4513256549835205 | KNN Loss: 2.461045265197754 | BCE Loss: 0.9902803897857666\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 3.474905014038086 | KNN Loss: 2.4479353427886963 | BCE Loss: 1.0269695520401\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 3.470973491668701 | KNN Loss: 2.4548676013946533 | BCE Loss: 1.0161058902740479\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 3.4911093711853027 | KNN Loss: 2.453463315963745 | BCE Loss: 1.0376461744308472\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 3.4665510654449463 | KNN Loss: 2.456697463989258 | BCE Loss: 1.0098536014556885\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 3.4672515392303467 | KNN Loss: 2.457315683364868 | BCE Loss: 1.0099358558654785\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 3.4518215656280518 | KNN Loss: 2.4436349868774414 | BCE Loss: 1.0081865787506104\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 3.506186008453369 | KNN Loss: 2.4799845218658447 | BCE Loss: 1.0262013673782349\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 3.5056538581848145 | KNN Loss: 2.4744515419006348 | BCE Loss: 1.0312021970748901\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 3.444479465484619 | KNN Loss: 2.43029522895813 | BCE Loss: 1.0141842365264893\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 3.475080966949463 | KNN Loss: 2.4489638805389404 | BCE Loss: 1.0261170864105225\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 3.5080699920654297 | KNN Loss: 2.4827051162719727 | BCE Loss: 1.025364875793457\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 3.4595956802368164 | KNN Loss: 2.430516242980957 | BCE Loss: 1.0290794372558594\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 3.441999912261963 | KNN Loss: 2.4325075149536133 | BCE Loss: 1.0094923973083496\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 3.4533486366271973 | KNN Loss: 2.4326882362365723 | BCE Loss: 1.0206602811813354\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 3.446049451828003 | KNN Loss: 2.4438023567199707 | BCE Loss: 1.0022470951080322\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 3.5037598609924316 | KNN Loss: 2.4747257232666016 | BCE Loss: 1.0290340185165405\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 3.509969711303711 | KNN Loss: 2.4814624786376953 | BCE Loss: 1.0285072326660156\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 3.470038890838623 | KNN Loss: 2.447937488555908 | BCE Loss: 1.0221012830734253\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 3.5011448860168457 | KNN Loss: 2.4749529361724854 | BCE Loss: 1.0261919498443604\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 3.444628953933716 | KNN Loss: 2.4287638664245605 | BCE Loss: 1.0158650875091553\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 3.408399820327759 | KNN Loss: 2.417340040206909 | BCE Loss: 0.9910597801208496\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 3.4917006492614746 | KNN Loss: 2.474884510040283 | BCE Loss: 1.016816258430481\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 3.457448959350586 | KNN Loss: 2.435127019882202 | BCE Loss: 1.0223220586776733\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 3.506563425064087 | KNN Loss: 2.466108798980713 | BCE Loss: 1.040454626083374\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 3.5024876594543457 | KNN Loss: 2.514371871948242 | BCE Loss: 0.9881157875061035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 3.4767160415649414 | KNN Loss: 2.45137882232666 | BCE Loss: 1.0253373384475708\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 3.5057761669158936 | KNN Loss: 2.5011417865753174 | BCE Loss: 1.0046343803405762\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 3.4773662090301514 | KNN Loss: 2.4580719470977783 | BCE Loss: 1.019294261932373\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 3.490044593811035 | KNN Loss: 2.4456827640533447 | BCE Loss: 1.0443618297576904\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 3.489441156387329 | KNN Loss: 2.4473602771759033 | BCE Loss: 1.0420808792114258\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 3.4443180561065674 | KNN Loss: 2.4254701137542725 | BCE Loss: 1.018847942352295\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 3.438912868499756 | KNN Loss: 2.430589437484741 | BCE Loss: 1.008323311805725\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 3.4608047008514404 | KNN Loss: 2.4704086780548096 | BCE Loss: 0.9903960227966309\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 3.4290390014648438 | KNN Loss: 2.4143519401550293 | BCE Loss: 1.0146870613098145\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 3.475961446762085 | KNN Loss: 2.4807913303375244 | BCE Loss: 0.9951701164245605\n",
      "Epoch   404: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 3.503497838973999 | KNN Loss: 2.4924912452697754 | BCE Loss: 1.0110065937042236\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 3.4672389030456543 | KNN Loss: 2.4561479091644287 | BCE Loss: 1.0110911130905151\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 3.4386754035949707 | KNN Loss: 2.444542407989502 | BCE Loss: 0.9941330552101135\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 3.4647440910339355 | KNN Loss: 2.4623751640319824 | BCE Loss: 1.0023689270019531\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 3.4910409450531006 | KNN Loss: 2.455789566040039 | BCE Loss: 1.0352513790130615\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 3.4954307079315186 | KNN Loss: 2.465973377227783 | BCE Loss: 1.0294573307037354\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 3.465890407562256 | KNN Loss: 2.4703919887542725 | BCE Loss: 0.9954982995986938\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 3.4594483375549316 | KNN Loss: 2.4460604190826416 | BCE Loss: 1.0133880376815796\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 3.4609932899475098 | KNN Loss: 2.4517455101013184 | BCE Loss: 1.0092477798461914\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 3.5184621810913086 | KNN Loss: 2.4723236560821533 | BCE Loss: 1.0461385250091553\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 3.4795584678649902 | KNN Loss: 2.457484483718872 | BCE Loss: 1.0220738649368286\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 3.4392504692077637 | KNN Loss: 2.442072629928589 | BCE Loss: 0.9971779584884644\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 3.5214219093322754 | KNN Loss: 2.4735829830169678 | BCE Loss: 1.0478389263153076\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 3.431774616241455 | KNN Loss: 2.439544916152954 | BCE Loss: 0.9922298192977905\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 3.5161802768707275 | KNN Loss: 2.4670770168304443 | BCE Loss: 1.0491032600402832\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 3.4567580223083496 | KNN Loss: 2.4518520832061768 | BCE Loss: 1.0049059391021729\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 3.455131769180298 | KNN Loss: 2.4426896572113037 | BCE Loss: 1.0124421119689941\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 3.4593420028686523 | KNN Loss: 2.4349987506866455 | BCE Loss: 1.0243432521820068\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 3.4835619926452637 | KNN Loss: 2.469161033630371 | BCE Loss: 1.0144009590148926\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 3.4686670303344727 | KNN Loss: 2.452139377593994 | BCE Loss: 1.016527771949768\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 3.492420196533203 | KNN Loss: 2.4531962871551514 | BCE Loss: 1.0392239093780518\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 3.4316608905792236 | KNN Loss: 2.4467897415161133 | BCE Loss: 0.9848711490631104\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 3.4743242263793945 | KNN Loss: 2.4564855098724365 | BCE Loss: 1.0178385972976685\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 3.470226287841797 | KNN Loss: 2.462796449661255 | BCE Loss: 1.007429838180542\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 3.545279026031494 | KNN Loss: 2.5074498653411865 | BCE Loss: 1.037829041481018\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 3.414910316467285 | KNN Loss: 2.421875476837158 | BCE Loss: 0.9930347204208374\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 3.4774606227874756 | KNN Loss: 2.471888303756714 | BCE Loss: 1.0055723190307617\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 3.446978807449341 | KNN Loss: 2.4417340755462646 | BCE Loss: 1.0052447319030762\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 3.504417896270752 | KNN Loss: 2.4669971466064453 | BCE Loss: 1.0374207496643066\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 3.444612741470337 | KNN Loss: 2.434708833694458 | BCE Loss: 1.009903907775879\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 3.4465901851654053 | KNN Loss: 2.4466803073883057 | BCE Loss: 0.9999098181724548\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 3.4660189151763916 | KNN Loss: 2.452255964279175 | BCE Loss: 1.0137629508972168\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 3.509390354156494 | KNN Loss: 2.469773054122925 | BCE Loss: 1.0396174192428589\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 3.4777281284332275 | KNN Loss: 2.4501354694366455 | BCE Loss: 1.027592658996582\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 3.4342241287231445 | KNN Loss: 2.423738956451416 | BCE Loss: 1.010485053062439\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 3.4434752464294434 | KNN Loss: 2.4386909008026123 | BCE Loss: 1.004784345626831\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 3.492980718612671 | KNN Loss: 2.4763853549957275 | BCE Loss: 1.0165953636169434\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 3.4505720138549805 | KNN Loss: 2.4710841178894043 | BCE Loss: 0.9794877767562866\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 3.490684986114502 | KNN Loss: 2.4754040241241455 | BCE Loss: 1.015281081199646\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 3.481396436691284 | KNN Loss: 2.4441497325897217 | BCE Loss: 1.0372467041015625\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 3.483982563018799 | KNN Loss: 2.4621098041534424 | BCE Loss: 1.021872878074646\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 3.4852492809295654 | KNN Loss: 2.4432973861694336 | BCE Loss: 1.0419518947601318\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 3.4714574813842773 | KNN Loss: 2.4507195949554443 | BCE Loss: 1.0207380056381226\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 3.4864864349365234 | KNN Loss: 2.455021858215332 | BCE Loss: 1.0314644575119019\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 3.5083580017089844 | KNN Loss: 2.48006272315979 | BCE Loss: 1.0282952785491943\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 3.4497716426849365 | KNN Loss: 2.4527063369750977 | BCE Loss: 0.9970653653144836\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 3.5045995712280273 | KNN Loss: 2.456547260284424 | BCE Loss: 1.048052430152893\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 3.4980337619781494 | KNN Loss: 2.45443058013916 | BCE Loss: 1.0436031818389893\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 3.459406852722168 | KNN Loss: 2.45641827583313 | BCE Loss: 1.002988576889038\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 3.4909777641296387 | KNN Loss: 2.44343900680542 | BCE Loss: 1.0475388765335083\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 3.477639675140381 | KNN Loss: 2.4734814167022705 | BCE Loss: 1.0041581392288208\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 3.4670157432556152 | KNN Loss: 2.4536049365997314 | BCE Loss: 1.0134106874465942\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 3.4694814682006836 | KNN Loss: 2.449258804321289 | BCE Loss: 1.0202226638793945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 3.5131309032440186 | KNN Loss: 2.4810879230499268 | BCE Loss: 1.0320429801940918\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 3.466654062271118 | KNN Loss: 2.4532883167266846 | BCE Loss: 1.0133657455444336\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 3.483829975128174 | KNN Loss: 2.4658114910125732 | BCE Loss: 1.0180184841156006\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 3.467008113861084 | KNN Loss: 2.447864532470703 | BCE Loss: 1.0191435813903809\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 3.456756114959717 | KNN Loss: 2.4543142318725586 | BCE Loss: 1.0024418830871582\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 3.4565889835357666 | KNN Loss: 2.457531452178955 | BCE Loss: 0.9990575313568115\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 3.4298341274261475 | KNN Loss: 2.4092140197753906 | BCE Loss: 1.0206201076507568\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 3.480556011199951 | KNN Loss: 2.4865055084228516 | BCE Loss: 0.9940506219863892\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 3.496903657913208 | KNN Loss: 2.47063946723938 | BCE Loss: 1.0262641906738281\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 3.427628993988037 | KNN Loss: 2.4642066955566406 | BCE Loss: 0.963422417640686\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 3.4566855430603027 | KNN Loss: 2.4510366916656494 | BCE Loss: 1.0056489706039429\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 3.505415201187134 | KNN Loss: 2.46256947517395 | BCE Loss: 1.0428457260131836\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 3.505720615386963 | KNN Loss: 2.472978353500366 | BCE Loss: 1.0327422618865967\n",
      "Epoch   415: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 3.459190845489502 | KNN Loss: 2.4368844032287598 | BCE Loss: 1.0223063230514526\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 3.490349769592285 | KNN Loss: 2.4681570529937744 | BCE Loss: 1.0221928358078003\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 3.4875025749206543 | KNN Loss: 2.4654781818389893 | BCE Loss: 1.0220245122909546\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 3.461982488632202 | KNN Loss: 2.448406934738159 | BCE Loss: 1.013575553894043\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 3.5132646560668945 | KNN Loss: 2.484755039215088 | BCE Loss: 1.0285096168518066\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 3.4642040729522705 | KNN Loss: 2.4367587566375732 | BCE Loss: 1.0274453163146973\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 3.444756507873535 | KNN Loss: 2.4717772006988525 | BCE Loss: 0.9729793071746826\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 3.410465955734253 | KNN Loss: 2.3978004455566406 | BCE Loss: 1.0126655101776123\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 3.4784045219421387 | KNN Loss: 2.444175958633423 | BCE Loss: 1.0342285633087158\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 3.4803264141082764 | KNN Loss: 2.448843002319336 | BCE Loss: 1.0314834117889404\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 3.4530534744262695 | KNN Loss: 2.446908473968506 | BCE Loss: 1.0061448812484741\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 3.455134391784668 | KNN Loss: 2.4400198459625244 | BCE Loss: 1.015114426612854\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 3.4577255249023438 | KNN Loss: 2.4592010974884033 | BCE Loss: 0.9985243678092957\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 3.4938597679138184 | KNN Loss: 2.4730939865112305 | BCE Loss: 1.0207656621932983\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 3.5283892154693604 | KNN Loss: 2.49410080909729 | BCE Loss: 1.0342884063720703\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 3.458972692489624 | KNN Loss: 2.431669235229492 | BCE Loss: 1.0273034572601318\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 3.5085582733154297 | KNN Loss: 2.470355987548828 | BCE Loss: 1.0382022857666016\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 3.5115132331848145 | KNN Loss: 2.4582762718200684 | BCE Loss: 1.053236961364746\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 3.522956371307373 | KNN Loss: 2.498716115951538 | BCE Loss: 1.024240255355835\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 3.4685966968536377 | KNN Loss: 2.462388277053833 | BCE Loss: 1.0062084197998047\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 3.4865164756774902 | KNN Loss: 2.435330867767334 | BCE Loss: 1.0511856079101562\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 3.452523708343506 | KNN Loss: 2.4425597190856934 | BCE Loss: 1.009964108467102\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 3.469829559326172 | KNN Loss: 2.4538183212280273 | BCE Loss: 1.0160112380981445\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 3.5259151458740234 | KNN Loss: 2.5243098735809326 | BCE Loss: 1.0016053915023804\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 3.4779105186462402 | KNN Loss: 2.4401376247406006 | BCE Loss: 1.0377728939056396\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 3.4427528381347656 | KNN Loss: 2.4419307708740234 | BCE Loss: 1.0008220672607422\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 3.469801664352417 | KNN Loss: 2.4458131790161133 | BCE Loss: 1.0239884853363037\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 3.495793581008911 | KNN Loss: 2.4575283527374268 | BCE Loss: 1.0382652282714844\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 3.5036978721618652 | KNN Loss: 2.498168468475342 | BCE Loss: 1.0055294036865234\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 3.5021963119506836 | KNN Loss: 2.482440710067749 | BCE Loss: 1.0197556018829346\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 3.452878475189209 | KNN Loss: 2.457176923751831 | BCE Loss: 0.9957014322280884\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 3.4636268615722656 | KNN Loss: 2.4338865280151367 | BCE Loss: 1.0297404527664185\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 3.5246284008026123 | KNN Loss: 2.4804959297180176 | BCE Loss: 1.0441324710845947\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 3.4801034927368164 | KNN Loss: 2.460049629211426 | BCE Loss: 1.0200539827346802\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 3.47930645942688 | KNN Loss: 2.449545383453369 | BCE Loss: 1.0297610759735107\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 3.546614646911621 | KNN Loss: 2.4974751472473145 | BCE Loss: 1.0491396188735962\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 3.4809539318084717 | KNN Loss: 2.4316110610961914 | BCE Loss: 1.0493428707122803\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 3.466583728790283 | KNN Loss: 2.4713134765625 | BCE Loss: 0.9952702522277832\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 3.5274109840393066 | KNN Loss: 2.493999719619751 | BCE Loss: 1.0334112644195557\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 3.48671817779541 | KNN Loss: 2.4545631408691406 | BCE Loss: 1.0321550369262695\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 3.437771797180176 | KNN Loss: 2.4407854080200195 | BCE Loss: 0.9969865083694458\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 3.4782848358154297 | KNN Loss: 2.450474500656128 | BCE Loss: 1.0278102159500122\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 3.4492616653442383 | KNN Loss: 2.4363186359405518 | BCE Loss: 1.012943148612976\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 3.4723129272460938 | KNN Loss: 2.468963861465454 | BCE Loss: 1.00334894657135\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 3.472285032272339 | KNN Loss: 2.4641809463500977 | BCE Loss: 1.0081040859222412\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 3.5187644958496094 | KNN Loss: 2.4689550399780273 | BCE Loss: 1.0498095750808716\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 3.4711642265319824 | KNN Loss: 2.4441349506378174 | BCE Loss: 1.027029275894165\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 3.425828695297241 | KNN Loss: 2.425015687942505 | BCE Loss: 1.0008130073547363\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 3.5147531032562256 | KNN Loss: 2.4771370887756348 | BCE Loss: 1.0376160144805908\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 3.4653077125549316 | KNN Loss: 2.4610581398010254 | BCE Loss: 1.0042494535446167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 3.4923417568206787 | KNN Loss: 2.461247682571411 | BCE Loss: 1.0310940742492676\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 3.495669364929199 | KNN Loss: 2.46765398979187 | BCE Loss: 1.028015375137329\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 3.4793505668640137 | KNN Loss: 2.4492850303649902 | BCE Loss: 1.0300655364990234\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 3.4683918952941895 | KNN Loss: 2.461319923400879 | BCE Loss: 1.0070719718933105\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 3.4450132846832275 | KNN Loss: 2.4509239196777344 | BCE Loss: 0.9940893054008484\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 3.461169719696045 | KNN Loss: 2.4487059116363525 | BCE Loss: 1.0124636888504028\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 3.469176769256592 | KNN Loss: 2.469172954559326 | BCE Loss: 1.0000039339065552\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 3.443237781524658 | KNN Loss: 2.4356861114501953 | BCE Loss: 1.0075517892837524\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 3.4971323013305664 | KNN Loss: 2.441779136657715 | BCE Loss: 1.0553531646728516\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 3.494131088256836 | KNN Loss: 2.4657487869262695 | BCE Loss: 1.0283823013305664\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 3.4806976318359375 | KNN Loss: 2.4617536067962646 | BCE Loss: 1.0189440250396729\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 3.501713752746582 | KNN Loss: 2.4534661769866943 | BCE Loss: 1.0482474565505981\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 3.50141978263855 | KNN Loss: 2.476337194442749 | BCE Loss: 1.0250825881958008\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 3.4994664192199707 | KNN Loss: 2.477515935897827 | BCE Loss: 1.021950602531433\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 3.4535276889801025 | KNN Loss: 2.4660794734954834 | BCE Loss: 0.9874482750892639\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 3.504852771759033 | KNN Loss: 2.4789814949035645 | BCE Loss: 1.0258713960647583\n",
      "Epoch   426: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 3.469935894012451 | KNN Loss: 2.454077959060669 | BCE Loss: 1.0158580541610718\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 3.453191041946411 | KNN Loss: 2.4362282752990723 | BCE Loss: 1.0169627666473389\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 3.428053855895996 | KNN Loss: 2.4373481273651123 | BCE Loss: 0.9907056093215942\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 3.4785401821136475 | KNN Loss: 2.4633171558380127 | BCE Loss: 1.0152230262756348\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 3.5156707763671875 | KNN Loss: 2.4617769718170166 | BCE Loss: 1.053893804550171\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 3.4291977882385254 | KNN Loss: 2.437490701675415 | BCE Loss: 0.9917070269584656\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 3.513023614883423 | KNN Loss: 2.472139596939087 | BCE Loss: 1.040884017944336\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 3.4817957878112793 | KNN Loss: 2.475620985031128 | BCE Loss: 1.0061748027801514\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 3.4599595069885254 | KNN Loss: 2.4343512058258057 | BCE Loss: 1.0256083011627197\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 3.5185251235961914 | KNN Loss: 2.485042095184326 | BCE Loss: 1.0334831476211548\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 3.429220676422119 | KNN Loss: 2.441162347793579 | BCE Loss: 0.9880584478378296\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 3.504364252090454 | KNN Loss: 2.4630043506622314 | BCE Loss: 1.0413599014282227\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 3.501394271850586 | KNN Loss: 2.4735307693481445 | BCE Loss: 1.0278635025024414\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 3.428267478942871 | KNN Loss: 2.447249174118042 | BCE Loss: 0.9810183048248291\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 3.5100741386413574 | KNN Loss: 2.458469867706299 | BCE Loss: 1.0516043901443481\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 3.5051701068878174 | KNN Loss: 2.5010008811950684 | BCE Loss: 1.004169225692749\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 3.4745190143585205 | KNN Loss: 2.457820415496826 | BCE Loss: 1.0166985988616943\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 3.45371150970459 | KNN Loss: 2.461298704147339 | BCE Loss: 0.9924129247665405\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 3.4636850357055664 | KNN Loss: 2.443605422973633 | BCE Loss: 1.0200797319412231\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 3.4792566299438477 | KNN Loss: 2.4612984657287598 | BCE Loss: 1.017958164215088\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 3.4681406021118164 | KNN Loss: 2.443243980407715 | BCE Loss: 1.024896502494812\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 3.4749488830566406 | KNN Loss: 2.4384758472442627 | BCE Loss: 1.036473035812378\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 3.4474244117736816 | KNN Loss: 2.4381277561187744 | BCE Loss: 1.0092966556549072\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 3.496264934539795 | KNN Loss: 2.4670047760009766 | BCE Loss: 1.0292601585388184\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 3.4868650436401367 | KNN Loss: 2.450536012649536 | BCE Loss: 1.0363290309906006\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 3.46626615524292 | KNN Loss: 2.4496991634368896 | BCE Loss: 1.0165669918060303\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 3.4656660556793213 | KNN Loss: 2.4336090087890625 | BCE Loss: 1.0320570468902588\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 3.467164993286133 | KNN Loss: 2.4388134479522705 | BCE Loss: 1.0283515453338623\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 3.487436056137085 | KNN Loss: 2.459571599960327 | BCE Loss: 1.0278644561767578\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 3.494872570037842 | KNN Loss: 2.45369291305542 | BCE Loss: 1.0411796569824219\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 3.424659490585327 | KNN Loss: 2.4164342880249023 | BCE Loss: 1.0082252025604248\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 3.45212459564209 | KNN Loss: 2.4496452808380127 | BCE Loss: 1.0024791955947876\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 3.468259334564209 | KNN Loss: 2.431199312210083 | BCE Loss: 1.0370601415634155\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 3.4994895458221436 | KNN Loss: 2.4799232482910156 | BCE Loss: 1.019566297531128\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 3.4478743076324463 | KNN Loss: 2.4476001262664795 | BCE Loss: 1.0002741813659668\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 3.510422706604004 | KNN Loss: 2.4820027351379395 | BCE Loss: 1.028420090675354\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 3.48018217086792 | KNN Loss: 2.476510763168335 | BCE Loss: 1.0036712884902954\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 3.502005100250244 | KNN Loss: 2.480201244354248 | BCE Loss: 1.021803855895996\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 3.4740750789642334 | KNN Loss: 2.4606363773345947 | BCE Loss: 1.0134387016296387\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 3.5001959800720215 | KNN Loss: 2.473850727081299 | BCE Loss: 1.0263452529907227\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 3.4694671630859375 | KNN Loss: 2.4585623741149902 | BCE Loss: 1.0109049081802368\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 3.442054271697998 | KNN Loss: 2.4522488117218018 | BCE Loss: 0.9898053407669067\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 3.4599719047546387 | KNN Loss: 2.437121629714966 | BCE Loss: 1.0228501558303833\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 3.4849331378936768 | KNN Loss: 2.468282699584961 | BCE Loss: 1.0166504383087158\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 3.4716832637786865 | KNN Loss: 2.4519853591918945 | BCE Loss: 1.019697904586792\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 3.4897217750549316 | KNN Loss: 2.47278094291687 | BCE Loss: 1.0169408321380615\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 3.469268321990967 | KNN Loss: 2.4390275478363037 | BCE Loss: 1.0302408933639526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 3.4620742797851562 | KNN Loss: 2.4270942211151123 | BCE Loss: 1.034980058670044\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 3.4873569011688232 | KNN Loss: 2.4771249294281006 | BCE Loss: 1.0102319717407227\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 3.46212100982666 | KNN Loss: 2.431734085083008 | BCE Loss: 1.0303869247436523\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 3.48880672454834 | KNN Loss: 2.4545857906341553 | BCE Loss: 1.0342210531234741\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 3.4438915252685547 | KNN Loss: 2.4465861320495605 | BCE Loss: 0.9973052740097046\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 3.4529895782470703 | KNN Loss: 2.434837818145752 | BCE Loss: 1.0181517601013184\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 3.461735248565674 | KNN Loss: 2.4463207721710205 | BCE Loss: 1.0154144763946533\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 3.4560623168945312 | KNN Loss: 2.4573206901550293 | BCE Loss: 0.9987416863441467\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 3.462754249572754 | KNN Loss: 2.4417788982391357 | BCE Loss: 1.0209753513336182\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 3.4838602542877197 | KNN Loss: 2.4591469764709473 | BCE Loss: 1.0247132778167725\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 3.4197511672973633 | KNN Loss: 2.4256062507629395 | BCE Loss: 0.9941450357437134\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 3.415827989578247 | KNN Loss: 2.420933961868286 | BCE Loss: 0.9948940873146057\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 3.4867448806762695 | KNN Loss: 2.46885085105896 | BCE Loss: 1.01789391040802\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 3.45930814743042 | KNN Loss: 2.474766254425049 | BCE Loss: 0.9845420122146606\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 3.5117015838623047 | KNN Loss: 2.470066785812378 | BCE Loss: 1.0416347980499268\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 3.460131883621216 | KNN Loss: 2.4669580459594727 | BCE Loss: 0.9931738376617432\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 3.4745595455169678 | KNN Loss: 2.4483203887939453 | BCE Loss: 1.0262391567230225\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 3.4796500205993652 | KNN Loss: 2.468796491622925 | BCE Loss: 1.0108535289764404\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 3.4500155448913574 | KNN Loss: 2.4315481185913086 | BCE Loss: 1.0184674263000488\n",
      "Epoch   437: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 3.416759967803955 | KNN Loss: 2.4269566535949707 | BCE Loss: 0.9898033142089844\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 3.443748712539673 | KNN Loss: 2.441819906234741 | BCE Loss: 1.0019288063049316\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 3.4841151237487793 | KNN Loss: 2.4411845207214355 | BCE Loss: 1.0429307222366333\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 3.4391629695892334 | KNN Loss: 2.4337568283081055 | BCE Loss: 1.005406141281128\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 3.5225419998168945 | KNN Loss: 2.501833200454712 | BCE Loss: 1.0207087993621826\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 3.4479477405548096 | KNN Loss: 2.4329564571380615 | BCE Loss: 1.014991283416748\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 3.453428268432617 | KNN Loss: 2.4338953495025635 | BCE Loss: 1.0195330381393433\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 3.472923994064331 | KNN Loss: 2.4402501583099365 | BCE Loss: 1.0326738357543945\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 3.437983989715576 | KNN Loss: 2.4472625255584717 | BCE Loss: 0.9907213449478149\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 3.4863476753234863 | KNN Loss: 2.45206356048584 | BCE Loss: 1.034283995628357\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 3.4859111309051514 | KNN Loss: 2.470221757888794 | BCE Loss: 1.0156893730163574\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 3.4560422897338867 | KNN Loss: 2.435386896133423 | BCE Loss: 1.0206552743911743\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 3.488377094268799 | KNN Loss: 2.471745014190674 | BCE Loss: 1.016632080078125\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 3.5279481410980225 | KNN Loss: 2.498969316482544 | BCE Loss: 1.0289788246154785\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 3.4725327491760254 | KNN Loss: 2.4670960903167725 | BCE Loss: 1.005436658859253\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 3.4373557567596436 | KNN Loss: 2.4376473426818848 | BCE Loss: 0.9997084736824036\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 3.4703593254089355 | KNN Loss: 2.4501421451568604 | BCE Loss: 1.0202171802520752\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 3.509371757507324 | KNN Loss: 2.4731295108795166 | BCE Loss: 1.036242127418518\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 3.4423298835754395 | KNN Loss: 2.446246385574341 | BCE Loss: 0.9960836172103882\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 3.5196921825408936 | KNN Loss: 2.4824512004852295 | BCE Loss: 1.037240982055664\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 3.5011708736419678 | KNN Loss: 2.4673049449920654 | BCE Loss: 1.0338659286499023\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 3.475721836090088 | KNN Loss: 2.466647148132324 | BCE Loss: 1.0090748071670532\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 3.436732769012451 | KNN Loss: 2.42080020904541 | BCE Loss: 1.0159326791763306\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 3.4956579208374023 | KNN Loss: 2.4592535495758057 | BCE Loss: 1.0364043712615967\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 3.48197078704834 | KNN Loss: 2.4811360836029053 | BCE Loss: 1.0008348226547241\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 3.4734866619110107 | KNN Loss: 2.439547061920166 | BCE Loss: 1.0339395999908447\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 3.4410367012023926 | KNN Loss: 2.4178783893585205 | BCE Loss: 1.0231581926345825\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 3.493753433227539 | KNN Loss: 2.466187000274658 | BCE Loss: 1.0275664329528809\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 3.4723715782165527 | KNN Loss: 2.4514200687408447 | BCE Loss: 1.020951509475708\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 3.4505255222320557 | KNN Loss: 2.4276671409606934 | BCE Loss: 1.0228583812713623\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 3.4926772117614746 | KNN Loss: 2.49029278755188 | BCE Loss: 1.0023844242095947\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 3.4325006008148193 | KNN Loss: 2.4298737049102783 | BCE Loss: 1.002626895904541\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 3.4872183799743652 | KNN Loss: 2.438751459121704 | BCE Loss: 1.0484668016433716\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 3.4840846061706543 | KNN Loss: 2.464625120162964 | BCE Loss: 1.01945960521698\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 3.4574332237243652 | KNN Loss: 2.4545676708221436 | BCE Loss: 1.0028655529022217\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 3.5483803749084473 | KNN Loss: 2.5177791118621826 | BCE Loss: 1.0306012630462646\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 3.494363307952881 | KNN Loss: 2.4894049167633057 | BCE Loss: 1.0049585103988647\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 3.4839651584625244 | KNN Loss: 2.4515318870544434 | BCE Loss: 1.032433271408081\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 3.497727155685425 | KNN Loss: 2.4784398078918457 | BCE Loss: 1.019287347793579\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 3.4961512088775635 | KNN Loss: 2.4313149452209473 | BCE Loss: 1.0648362636566162\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 3.5078036785125732 | KNN Loss: 2.4562828540802 | BCE Loss: 1.051520824432373\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 3.479144811630249 | KNN Loss: 2.477290630340576 | BCE Loss: 1.0018541812896729\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 3.4645426273345947 | KNN Loss: 2.445199489593506 | BCE Loss: 1.0193431377410889\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 3.4635910987854004 | KNN Loss: 2.435436487197876 | BCE Loss: 1.0281546115875244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 3.449402093887329 | KNN Loss: 2.4508118629455566 | BCE Loss: 0.9985901713371277\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 3.4989774227142334 | KNN Loss: 2.4647326469421387 | BCE Loss: 1.0342447757720947\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 3.493281364440918 | KNN Loss: 2.472656011581421 | BCE Loss: 1.0206252336502075\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 3.5535709857940674 | KNN Loss: 2.5044548511505127 | BCE Loss: 1.0491161346435547\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 3.527285575866699 | KNN Loss: 2.5087225437164307 | BCE Loss: 1.0185630321502686\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 3.4583017826080322 | KNN Loss: 2.4532322883605957 | BCE Loss: 1.0050694942474365\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 3.4738872051239014 | KNN Loss: 2.452670097351074 | BCE Loss: 1.0212171077728271\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 3.4590539932250977 | KNN Loss: 2.436281204223633 | BCE Loss: 1.0227729082107544\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 3.444544792175293 | KNN Loss: 2.427255630493164 | BCE Loss: 1.0172892808914185\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 3.4617490768432617 | KNN Loss: 2.4628028869628906 | BCE Loss: 0.9989461898803711\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 3.465014696121216 | KNN Loss: 2.4459588527679443 | BCE Loss: 1.0190558433532715\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 3.475398302078247 | KNN Loss: 2.448216676712036 | BCE Loss: 1.027181625366211\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 3.464470863342285 | KNN Loss: 2.4573497772216797 | BCE Loss: 1.007121205329895\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 3.4604551792144775 | KNN Loss: 2.4557385444641113 | BCE Loss: 1.0047166347503662\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 3.4301233291625977 | KNN Loss: 2.429255723953247 | BCE Loss: 1.0008676052093506\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 3.539383888244629 | KNN Loss: 2.4913477897644043 | BCE Loss: 1.0480360984802246\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 3.445824384689331 | KNN Loss: 2.4189300537109375 | BCE Loss: 1.0268943309783936\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 3.5191543102264404 | KNN Loss: 2.4950129985809326 | BCE Loss: 1.0241413116455078\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 3.442117691040039 | KNN Loss: 2.425532341003418 | BCE Loss: 1.0165854692459106\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 3.4691615104675293 | KNN Loss: 2.4366161823272705 | BCE Loss: 1.0325454473495483\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 3.474151372909546 | KNN Loss: 2.4502463340759277 | BCE Loss: 1.0239050388336182\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 3.4900588989257812 | KNN Loss: 2.4878766536712646 | BCE Loss: 1.0021822452545166\n",
      "Epoch   448: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 3.5135645866394043 | KNN Loss: 2.464595079421997 | BCE Loss: 1.0489693880081177\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 3.5187318325042725 | KNN Loss: 2.502028703689575 | BCE Loss: 1.0167031288146973\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 3.5129101276397705 | KNN Loss: 2.4590094089508057 | BCE Loss: 1.0539007186889648\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 3.471322774887085 | KNN Loss: 2.4558281898498535 | BCE Loss: 1.0154945850372314\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 3.479095697402954 | KNN Loss: 2.464899778366089 | BCE Loss: 1.0141959190368652\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 3.4620938301086426 | KNN Loss: 2.442420721054077 | BCE Loss: 1.019673228263855\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 3.472076892852783 | KNN Loss: 2.449798107147217 | BCE Loss: 1.0222787857055664\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 3.511643409729004 | KNN Loss: 2.4781103134155273 | BCE Loss: 1.0335332155227661\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 3.4655375480651855 | KNN Loss: 2.4159226417541504 | BCE Loss: 1.0496149063110352\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 3.4454896450042725 | KNN Loss: 2.429990768432617 | BCE Loss: 1.0154988765716553\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 3.475839376449585 | KNN Loss: 2.4499258995056152 | BCE Loss: 1.0259134769439697\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 3.449296474456787 | KNN Loss: 2.4388458728790283 | BCE Loss: 1.0104506015777588\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 3.449835777282715 | KNN Loss: 2.4177989959716797 | BCE Loss: 1.0320367813110352\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 3.5022692680358887 | KNN Loss: 2.487732172012329 | BCE Loss: 1.0145372152328491\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 3.491994857788086 | KNN Loss: 2.4686834812164307 | BCE Loss: 1.0233114957809448\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 3.500761032104492 | KNN Loss: 2.4823191165924072 | BCE Loss: 1.0184420347213745\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 3.456747055053711 | KNN Loss: 2.4360191822052 | BCE Loss: 1.0207277536392212\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 3.485440254211426 | KNN Loss: 2.483893871307373 | BCE Loss: 1.0015463829040527\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 3.4775443077087402 | KNN Loss: 2.429326057434082 | BCE Loss: 1.0482183694839478\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 3.5016260147094727 | KNN Loss: 2.4450137615203857 | BCE Loss: 1.056612253189087\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 3.4418301582336426 | KNN Loss: 2.4279799461364746 | BCE Loss: 1.013850212097168\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 3.463041305541992 | KNN Loss: 2.4561424255371094 | BCE Loss: 1.0068988800048828\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 3.4800994396209717 | KNN Loss: 2.4544870853424072 | BCE Loss: 1.0256123542785645\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 3.4448280334472656 | KNN Loss: 2.4553637504577637 | BCE Loss: 0.989464282989502\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 3.4967284202575684 | KNN Loss: 2.461371421813965 | BCE Loss: 1.035356879234314\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 3.479749917984009 | KNN Loss: 2.477360248565674 | BCE Loss: 1.002389669418335\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 3.464662790298462 | KNN Loss: 2.483628034591675 | BCE Loss: 0.9810346961021423\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 3.4604332447052 | KNN Loss: 2.4446380138397217 | BCE Loss: 1.0157952308654785\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 3.4279918670654297 | KNN Loss: 2.4227306842803955 | BCE Loss: 1.0052611827850342\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 3.4368724822998047 | KNN Loss: 2.4331512451171875 | BCE Loss: 1.0037213563919067\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 3.478809356689453 | KNN Loss: 2.4595675468444824 | BCE Loss: 1.0192418098449707\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 3.442122459411621 | KNN Loss: 2.420893430709839 | BCE Loss: 1.0212291479110718\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 3.4565181732177734 | KNN Loss: 2.444272756576538 | BCE Loss: 1.012245535850525\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 3.4241433143615723 | KNN Loss: 2.425142526626587 | BCE Loss: 0.9990007877349854\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 3.505805253982544 | KNN Loss: 2.463435411453247 | BCE Loss: 1.0423698425292969\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 3.4718379974365234 | KNN Loss: 2.4852256774902344 | BCE Loss: 0.9866122007369995\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 3.5049493312835693 | KNN Loss: 2.487429141998291 | BCE Loss: 1.0175201892852783\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 3.525261878967285 | KNN Loss: 2.4659929275512695 | BCE Loss: 1.0592690706253052\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 3.4408135414123535 | KNN Loss: 2.4240455627441406 | BCE Loss: 1.0167680978775024\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 3.4414877891540527 | KNN Loss: 2.4179329872131348 | BCE Loss: 1.0235549211502075\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 3.4890477657318115 | KNN Loss: 2.497786521911621 | BCE Loss: 0.9912612438201904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 3.488745927810669 | KNN Loss: 2.463747024536133 | BCE Loss: 1.0249989032745361\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 3.4412522315979004 | KNN Loss: 2.463918924331665 | BCE Loss: 0.9773333072662354\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 3.467306613922119 | KNN Loss: 2.4757308959960938 | BCE Loss: 0.9915756583213806\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 3.512146234512329 | KNN Loss: 2.493724822998047 | BCE Loss: 1.0184214115142822\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 3.498267650604248 | KNN Loss: 2.504296064376831 | BCE Loss: 0.9939714670181274\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 3.4648666381835938 | KNN Loss: 2.4375252723693848 | BCE Loss: 1.0273414850234985\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 3.4987854957580566 | KNN Loss: 2.466909170150757 | BCE Loss: 1.0318762063980103\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 3.4794301986694336 | KNN Loss: 2.469111204147339 | BCE Loss: 1.0103189945220947\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 3.506164073944092 | KNN Loss: 2.4754669666290283 | BCE Loss: 1.030696988105774\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 3.4554221630096436 | KNN Loss: 2.467697858810425 | BCE Loss: 0.9877243041992188\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 3.454221248626709 | KNN Loss: 2.4536619186401367 | BCE Loss: 1.0005594491958618\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 3.485125780105591 | KNN Loss: 2.4598493576049805 | BCE Loss: 1.0252764225006104\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 3.4535117149353027 | KNN Loss: 2.446342706680298 | BCE Loss: 1.0071690082550049\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 3.498133897781372 | KNN Loss: 2.451554775238037 | BCE Loss: 1.046579122543335\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 3.467008113861084 | KNN Loss: 2.4462640285491943 | BCE Loss: 1.0207439661026\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 3.4748036861419678 | KNN Loss: 2.448378324508667 | BCE Loss: 1.0264253616333008\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 3.4851832389831543 | KNN Loss: 2.492170572280884 | BCE Loss: 0.9930127859115601\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 3.4565482139587402 | KNN Loss: 2.4242658615112305 | BCE Loss: 1.0322824716567993\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 3.5095343589782715 | KNN Loss: 2.4491467475891113 | BCE Loss: 1.0603877305984497\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 3.4592278003692627 | KNN Loss: 2.467991352081299 | BCE Loss: 0.9912365078926086\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 3.501004219055176 | KNN Loss: 2.459742784500122 | BCE Loss: 1.0412615537643433\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 3.464702606201172 | KNN Loss: 2.45212459564209 | BCE Loss: 1.012578010559082\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 3.461709499359131 | KNN Loss: 2.444638252258301 | BCE Loss: 1.01707124710083\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 3.4789252281188965 | KNN Loss: 2.4582834243774414 | BCE Loss: 1.020641803741455\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 3.4402244091033936 | KNN Loss: 2.4327445030212402 | BCE Loss: 1.0074799060821533\n",
      "Epoch   459: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 3.4520926475524902 | KNN Loss: 2.432823896408081 | BCE Loss: 1.0192686319351196\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 3.460637331008911 | KNN Loss: 2.4433774948120117 | BCE Loss: 1.0172598361968994\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 3.460613250732422 | KNN Loss: 2.4549362659454346 | BCE Loss: 1.0056771039962769\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 3.485264778137207 | KNN Loss: 2.4682862758636475 | BCE Loss: 1.0169785022735596\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 3.468337297439575 | KNN Loss: 2.46502685546875 | BCE Loss: 1.0033104419708252\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 3.4666359424591064 | KNN Loss: 2.459646463394165 | BCE Loss: 1.0069894790649414\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 3.4775784015655518 | KNN Loss: 2.4519712924957275 | BCE Loss: 1.0256071090698242\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 3.4929161071777344 | KNN Loss: 2.433264970779419 | BCE Loss: 1.0596511363983154\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 3.4877195358276367 | KNN Loss: 2.4802944660186768 | BCE Loss: 1.0074249505996704\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 3.5025625228881836 | KNN Loss: 2.4687764644622803 | BCE Loss: 1.0337861776351929\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 3.4853973388671875 | KNN Loss: 2.460045576095581 | BCE Loss: 1.0253517627716064\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 3.45670223236084 | KNN Loss: 2.4496705532073975 | BCE Loss: 1.007031798362732\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 3.4341487884521484 | KNN Loss: 2.429426908493042 | BCE Loss: 1.0047218799591064\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 3.4632980823516846 | KNN Loss: 2.461653470993042 | BCE Loss: 1.0016446113586426\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 3.509384870529175 | KNN Loss: 2.485955238342285 | BCE Loss: 1.0234296321868896\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 3.504704713821411 | KNN Loss: 2.4699978828430176 | BCE Loss: 1.0347068309783936\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 3.471680164337158 | KNN Loss: 2.439589500427246 | BCE Loss: 1.032090663909912\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 3.4927048683166504 | KNN Loss: 2.4717776775360107 | BCE Loss: 1.0209271907806396\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 3.4945483207702637 | KNN Loss: 2.499807834625244 | BCE Loss: 0.9947406053543091\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 3.466733932495117 | KNN Loss: 2.4417431354522705 | BCE Loss: 1.0249909162521362\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 3.5216100215911865 | KNN Loss: 2.4679036140441895 | BCE Loss: 1.053706407546997\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 3.4808526039123535 | KNN Loss: 2.4393911361694336 | BCE Loss: 1.04146146774292\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 3.5063514709472656 | KNN Loss: 2.4540226459503174 | BCE Loss: 1.0523289442062378\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 3.5062413215637207 | KNN Loss: 2.4720633029937744 | BCE Loss: 1.0341781377792358\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 3.501826524734497 | KNN Loss: 2.4741311073303223 | BCE Loss: 1.0276954174041748\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 3.5017294883728027 | KNN Loss: 2.4649860858917236 | BCE Loss: 1.036743402481079\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 3.460441827774048 | KNN Loss: 2.430366277694702 | BCE Loss: 1.0300755500793457\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 3.4802310466766357 | KNN Loss: 2.447768449783325 | BCE Loss: 1.0324625968933105\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 3.47534441947937 | KNN Loss: 2.4399092197418213 | BCE Loss: 1.0354351997375488\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 3.4771547317504883 | KNN Loss: 2.4543702602386475 | BCE Loss: 1.0227843523025513\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 3.4811758995056152 | KNN Loss: 2.4537551403045654 | BCE Loss: 1.0274206399917603\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 3.5178780555725098 | KNN Loss: 2.4865078926086426 | BCE Loss: 1.0313701629638672\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 3.5146732330322266 | KNN Loss: 2.485091209411621 | BCE Loss: 1.0295820236206055\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 3.4367010593414307 | KNN Loss: 2.418520212173462 | BCE Loss: 1.0181808471679688\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 3.4988036155700684 | KNN Loss: 2.4870996475219727 | BCE Loss: 1.0117039680480957\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 3.494729518890381 | KNN Loss: 2.469874382019043 | BCE Loss: 1.024855136871338\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 3.456648826599121 | KNN Loss: 2.490992307662964 | BCE Loss: 0.9656563997268677\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 3.4515769481658936 | KNN Loss: 2.4458725452423096 | BCE Loss: 1.005704402923584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 3.4834883213043213 | KNN Loss: 2.4896440505981445 | BCE Loss: 0.9938442707061768\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 3.5023272037506104 | KNN Loss: 2.4970390796661377 | BCE Loss: 1.0052881240844727\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 3.4285550117492676 | KNN Loss: 2.429630756378174 | BCE Loss: 0.9989243149757385\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 3.4695472717285156 | KNN Loss: 2.4654178619384766 | BCE Loss: 1.0041295289993286\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 3.4450631141662598 | KNN Loss: 2.422218084335327 | BCE Loss: 1.0228450298309326\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 3.459489107131958 | KNN Loss: 2.458935022354126 | BCE Loss: 1.000554084777832\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 3.4836134910583496 | KNN Loss: 2.457202911376953 | BCE Loss: 1.0264105796813965\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 3.495404005050659 | KNN Loss: 2.4928841590881348 | BCE Loss: 1.0025198459625244\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 3.485862970352173 | KNN Loss: 2.452857494354248 | BCE Loss: 1.0330054759979248\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 3.4457194805145264 | KNN Loss: 2.446739912033081 | BCE Loss: 0.9989795684814453\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 3.4797959327697754 | KNN Loss: 2.474785089492798 | BCE Loss: 1.0050108432769775\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 3.4463138580322266 | KNN Loss: 2.435368776321411 | BCE Loss: 1.0109449625015259\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 3.491015911102295 | KNN Loss: 2.4655189514160156 | BCE Loss: 1.0254969596862793\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 3.46630597114563 | KNN Loss: 2.435875177383423 | BCE Loss: 1.030430793762207\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 3.4590559005737305 | KNN Loss: 2.468376636505127 | BCE Loss: 0.9906793832778931\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 3.459653615951538 | KNN Loss: 2.453275680541992 | BCE Loss: 1.006377935409546\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 3.4763145446777344 | KNN Loss: 2.463121175765991 | BCE Loss: 1.0131933689117432\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 3.5157628059387207 | KNN Loss: 2.480238914489746 | BCE Loss: 1.0355238914489746\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 3.456435203552246 | KNN Loss: 2.448770523071289 | BCE Loss: 1.007664680480957\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 3.4869861602783203 | KNN Loss: 2.4332187175750732 | BCE Loss: 1.0537673234939575\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 3.447645664215088 | KNN Loss: 2.429877281188965 | BCE Loss: 1.0177685022354126\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 3.4834775924682617 | KNN Loss: 2.430433750152588 | BCE Loss: 1.0530438423156738\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 3.4389030933380127 | KNN Loss: 2.4256699085235596 | BCE Loss: 1.0132331848144531\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 3.4715499877929688 | KNN Loss: 2.454183340072632 | BCE Loss: 1.017366647720337\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 3.4408373832702637 | KNN Loss: 2.44227933883667 | BCE Loss: 0.9985579252243042\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 3.472933292388916 | KNN Loss: 2.471536636352539 | BCE Loss: 1.0013967752456665\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 3.4849436283111572 | KNN Loss: 2.4603793621063232 | BCE Loss: 1.024564266204834\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 3.4835822582244873 | KNN Loss: 2.4927003383636475 | BCE Loss: 0.9908819198608398\n",
      "Epoch   470: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 3.474210023880005 | KNN Loss: 2.4312150478363037 | BCE Loss: 1.0429949760437012\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 3.4643900394439697 | KNN Loss: 2.4421699047088623 | BCE Loss: 1.0222201347351074\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 3.4870197772979736 | KNN Loss: 2.4573824405670166 | BCE Loss: 1.029637336730957\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 3.5229575634002686 | KNN Loss: 2.4944162368774414 | BCE Loss: 1.0285413265228271\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 3.5145392417907715 | KNN Loss: 2.5078911781311035 | BCE Loss: 1.006648063659668\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 3.4688916206359863 | KNN Loss: 2.45622181892395 | BCE Loss: 1.0126699209213257\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 3.4731593132019043 | KNN Loss: 2.440443754196167 | BCE Loss: 1.0327156782150269\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 3.4622511863708496 | KNN Loss: 2.456099271774292 | BCE Loss: 1.006151795387268\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 3.5262932777404785 | KNN Loss: 2.4701550006866455 | BCE Loss: 1.056138277053833\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 3.49282169342041 | KNN Loss: 2.4805519580841064 | BCE Loss: 1.0122696161270142\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 3.445831537246704 | KNN Loss: 2.4280805587768555 | BCE Loss: 1.0177509784698486\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 3.4583451747894287 | KNN Loss: 2.4501545429229736 | BCE Loss: 1.008190631866455\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 3.466991662979126 | KNN Loss: 2.4531288146972656 | BCE Loss: 1.0138628482818604\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 3.439645767211914 | KNN Loss: 2.428290367126465 | BCE Loss: 1.0113555192947388\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 3.444121837615967 | KNN Loss: 2.4336607456207275 | BCE Loss: 1.0104610919952393\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 3.45424222946167 | KNN Loss: 2.4595932960510254 | BCE Loss: 0.994648814201355\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 3.4709601402282715 | KNN Loss: 2.428757667541504 | BCE Loss: 1.0422025918960571\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 3.469130754470825 | KNN Loss: 2.4764323234558105 | BCE Loss: 0.9926983714103699\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 3.458594799041748 | KNN Loss: 2.4486911296844482 | BCE Loss: 1.0099037885665894\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 3.464824914932251 | KNN Loss: 2.4582126140594482 | BCE Loss: 1.0066123008728027\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 3.4945311546325684 | KNN Loss: 2.4699130058288574 | BCE Loss: 1.024618148803711\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 3.4911270141601562 | KNN Loss: 2.4710569381713867 | BCE Loss: 1.0200700759887695\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 3.4987945556640625 | KNN Loss: 2.4689066410064697 | BCE Loss: 1.0298880338668823\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 3.4913980960845947 | KNN Loss: 2.4752533435821533 | BCE Loss: 1.0161447525024414\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 3.511237382888794 | KNN Loss: 2.474034309387207 | BCE Loss: 1.037203073501587\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 3.4531803131103516 | KNN Loss: 2.4477241039276123 | BCE Loss: 1.0054562091827393\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 3.4772794246673584 | KNN Loss: 2.4528183937072754 | BCE Loss: 1.024461030960083\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 3.454853057861328 | KNN Loss: 2.4518637657165527 | BCE Loss: 1.0029892921447754\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 3.4648656845092773 | KNN Loss: 2.444600820541382 | BCE Loss: 1.020264983177185\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 3.4601569175720215 | KNN Loss: 2.4526124000549316 | BCE Loss: 1.0075443983078003\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 3.4744954109191895 | KNN Loss: 2.4769022464752197 | BCE Loss: 0.9975931644439697\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 3.502897262573242 | KNN Loss: 2.4933695793151855 | BCE Loss: 1.009527564048767\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 3.4425268173217773 | KNN Loss: 2.4609076976776123 | BCE Loss: 0.9816192388534546\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 3.536680221557617 | KNN Loss: 2.4925568103790283 | BCE Loss: 1.0441235303878784\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 3.515315532684326 | KNN Loss: 2.4880080223083496 | BCE Loss: 1.0273075103759766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 3.5007741451263428 | KNN Loss: 2.466007947921753 | BCE Loss: 1.0347661972045898\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 3.478994846343994 | KNN Loss: 2.447139263153076 | BCE Loss: 1.0318554639816284\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 3.5263655185699463 | KNN Loss: 2.5072953701019287 | BCE Loss: 1.0190701484680176\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 3.481199264526367 | KNN Loss: 2.435866355895996 | BCE Loss: 1.0453327894210815\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 3.447815179824829 | KNN Loss: 2.4393181800842285 | BCE Loss: 1.0084969997406006\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 3.46799373626709 | KNN Loss: 2.4431240558624268 | BCE Loss: 1.0248697996139526\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 3.462148427963257 | KNN Loss: 2.473351240158081 | BCE Loss: 0.988797128200531\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 3.4834818840026855 | KNN Loss: 2.471660614013672 | BCE Loss: 1.0118211507797241\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 3.4547722339630127 | KNN Loss: 2.4607598781585693 | BCE Loss: 0.9940124154090881\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 3.4432315826416016 | KNN Loss: 2.4282331466674805 | BCE Loss: 1.014998435974121\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 3.47064471244812 | KNN Loss: 2.482414722442627 | BCE Loss: 0.9882300496101379\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 3.4986472129821777 | KNN Loss: 2.474336862564087 | BCE Loss: 1.0243103504180908\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 3.447507858276367 | KNN Loss: 2.4333901405334473 | BCE Loss: 1.0141175985336304\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 3.5115458965301514 | KNN Loss: 2.469978094100952 | BCE Loss: 1.0415678024291992\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 3.4796652793884277 | KNN Loss: 2.443239688873291 | BCE Loss: 1.0364254713058472\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 3.428626537322998 | KNN Loss: 2.419569253921509 | BCE Loss: 1.0090571641921997\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 3.4726884365081787 | KNN Loss: 2.4741475582122803 | BCE Loss: 0.9985408782958984\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 3.4573328495025635 | KNN Loss: 2.433867931365967 | BCE Loss: 1.0234649181365967\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 3.5417933464050293 | KNN Loss: 2.503868818283081 | BCE Loss: 1.0379245281219482\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 3.4974286556243896 | KNN Loss: 2.4685685634613037 | BCE Loss: 1.028860092163086\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 3.4393832683563232 | KNN Loss: 2.4249017238616943 | BCE Loss: 1.014481544494629\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 3.4865407943725586 | KNN Loss: 2.439875841140747 | BCE Loss: 1.0466649532318115\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 3.466409206390381 | KNN Loss: 2.4359607696533203 | BCE Loss: 1.0304484367370605\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 3.4640707969665527 | KNN Loss: 2.478349447250366 | BCE Loss: 0.9857214689254761\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 3.4468109607696533 | KNN Loss: 2.430706024169922 | BCE Loss: 1.0161049365997314\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 3.5015463829040527 | KNN Loss: 2.469496011734009 | BCE Loss: 1.032050371170044\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 3.484363555908203 | KNN Loss: 2.4730658531188965 | BCE Loss: 1.0112977027893066\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 3.4815707206726074 | KNN Loss: 2.4894700050354004 | BCE Loss: 0.992100715637207\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 3.5004453659057617 | KNN Loss: 2.451869010925293 | BCE Loss: 1.0485764741897583\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 3.468518018722534 | KNN Loss: 2.449857234954834 | BCE Loss: 1.0186607837677002\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 3.430101156234741 | KNN Loss: 2.422316312789917 | BCE Loss: 1.0077848434448242\n",
      "Epoch   481: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 3.4826712608337402 | KNN Loss: 2.4766628742218018 | BCE Loss: 1.006008505821228\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 3.4559521675109863 | KNN Loss: 2.445939302444458 | BCE Loss: 1.0100127458572388\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 3.494476795196533 | KNN Loss: 2.4402427673339844 | BCE Loss: 1.0542340278625488\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 3.469094753265381 | KNN Loss: 2.456210136413574 | BCE Loss: 1.0128846168518066\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 3.4680848121643066 | KNN Loss: 2.4541049003601074 | BCE Loss: 1.0139797925949097\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 3.475917339324951 | KNN Loss: 2.4448955059051514 | BCE Loss: 1.0310217142105103\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 3.4956061840057373 | KNN Loss: 2.4685330390930176 | BCE Loss: 1.0270731449127197\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 3.458991050720215 | KNN Loss: 2.449901819229126 | BCE Loss: 1.0090892314910889\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 3.428701877593994 | KNN Loss: 2.4213647842407227 | BCE Loss: 1.007336974143982\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 3.5132081508636475 | KNN Loss: 2.46073842048645 | BCE Loss: 1.0524697303771973\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 3.483414888381958 | KNN Loss: 2.450360059738159 | BCE Loss: 1.0330548286437988\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 3.4767472743988037 | KNN Loss: 2.4836723804473877 | BCE Loss: 0.993074893951416\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 3.450068473815918 | KNN Loss: 2.444833278656006 | BCE Loss: 1.005235195159912\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 3.4721269607543945 | KNN Loss: 2.4346325397491455 | BCE Loss: 1.037494421005249\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 3.4897751808166504 | KNN Loss: 2.450885057449341 | BCE Loss: 1.0388901233673096\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 3.4987850189208984 | KNN Loss: 2.454129457473755 | BCE Loss: 1.0446555614471436\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 3.4697368144989014 | KNN Loss: 2.4362802505493164 | BCE Loss: 1.033456563949585\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 3.516623020172119 | KNN Loss: 2.4454922676086426 | BCE Loss: 1.071130633354187\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 3.4989213943481445 | KNN Loss: 2.47226881980896 | BCE Loss: 1.0266525745391846\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 3.4693052768707275 | KNN Loss: 2.4460318088531494 | BCE Loss: 1.0232734680175781\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 3.507796287536621 | KNN Loss: 2.4309067726135254 | BCE Loss: 1.0768895149230957\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 3.5086655616760254 | KNN Loss: 2.48875093460083 | BCE Loss: 1.0199146270751953\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 3.494384765625 | KNN Loss: 2.50349497795105 | BCE Loss: 0.9908897876739502\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 3.4591751098632812 | KNN Loss: 2.4450855255126953 | BCE Loss: 1.014089584350586\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 3.5046520233154297 | KNN Loss: 2.4605202674865723 | BCE Loss: 1.0441316366195679\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 3.4874939918518066 | KNN Loss: 2.4610280990600586 | BCE Loss: 1.026465892791748\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 3.4679388999938965 | KNN Loss: 2.4368627071380615 | BCE Loss: 1.0310763120651245\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 3.477510452270508 | KNN Loss: 2.4395925998687744 | BCE Loss: 1.0379177331924438\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 3.456867218017578 | KNN Loss: 2.4409196376800537 | BCE Loss: 1.0159475803375244\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 3.483506441116333 | KNN Loss: 2.447143077850342 | BCE Loss: 1.0363633632659912\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 3.468271493911743 | KNN Loss: 2.4552509784698486 | BCE Loss: 1.0130205154418945\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 3.5106635093688965 | KNN Loss: 2.454375982284546 | BCE Loss: 1.0562876462936401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 3.4681904315948486 | KNN Loss: 2.44805908203125 | BCE Loss: 1.0201313495635986\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 3.4739770889282227 | KNN Loss: 2.4568309783935547 | BCE Loss: 1.017146110534668\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 3.452244997024536 | KNN Loss: 2.422790050506592 | BCE Loss: 1.0294549465179443\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 3.4274847507476807 | KNN Loss: 2.4373679161071777 | BCE Loss: 0.9901167750358582\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 3.5117275714874268 | KNN Loss: 2.470431089401245 | BCE Loss: 1.0412964820861816\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 3.469747543334961 | KNN Loss: 2.4631640911102295 | BCE Loss: 1.0065834522247314\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 3.432394504547119 | KNN Loss: 2.424036741256714 | BCE Loss: 1.0083577632904053\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 3.464578151702881 | KNN Loss: 2.436367988586426 | BCE Loss: 1.0282100439071655\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 3.5229568481445312 | KNN Loss: 2.500563144683838 | BCE Loss: 1.0223935842514038\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 3.491893768310547 | KNN Loss: 2.4703450202941895 | BCE Loss: 1.021548867225647\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 3.4580254554748535 | KNN Loss: 2.4615135192871094 | BCE Loss: 0.9965119361877441\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 3.4361352920532227 | KNN Loss: 2.443861484527588 | BCE Loss: 0.9922738075256348\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 3.4827067852020264 | KNN Loss: 2.4513442516326904 | BCE Loss: 1.031362533569336\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 3.463010311126709 | KNN Loss: 2.4274096488952637 | BCE Loss: 1.0356007814407349\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 3.469951629638672 | KNN Loss: 2.4513440132141113 | BCE Loss: 1.0186076164245605\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 3.4618985652923584 | KNN Loss: 2.4517269134521484 | BCE Loss: 1.01017165184021\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 3.4682064056396484 | KNN Loss: 2.475078582763672 | BCE Loss: 0.993127703666687\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 3.4339983463287354 | KNN Loss: 2.414790153503418 | BCE Loss: 1.0192081928253174\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 3.477566719055176 | KNN Loss: 2.4534785747528076 | BCE Loss: 1.0240880250930786\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 3.459062337875366 | KNN Loss: 2.464313507080078 | BCE Loss: 0.9947487711906433\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 3.4740631580352783 | KNN Loss: 2.437908172607422 | BCE Loss: 1.0361549854278564\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 3.4284846782684326 | KNN Loss: 2.4141428470611572 | BCE Loss: 1.0143418312072754\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 3.4313621520996094 | KNN Loss: 2.4513700008392334 | BCE Loss: 0.9799922704696655\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 3.4794156551361084 | KNN Loss: 2.47005295753479 | BCE Loss: 1.0093626976013184\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 3.438297748565674 | KNN Loss: 2.438079357147217 | BCE Loss: 1.0002182722091675\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 3.492612600326538 | KNN Loss: 2.462141752243042 | BCE Loss: 1.030470848083496\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 3.4784181118011475 | KNN Loss: 2.4627833366394043 | BCE Loss: 1.0156347751617432\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 3.500196933746338 | KNN Loss: 2.4664382934570312 | BCE Loss: 1.0337586402893066\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 3.4174904823303223 | KNN Loss: 2.4108572006225586 | BCE Loss: 1.0066332817077637\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 3.476877450942993 | KNN Loss: 2.4586315155029297 | BCE Loss: 1.0182459354400635\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 3.477539539337158 | KNN Loss: 2.4370920658111572 | BCE Loss: 1.0404475927352905\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 3.480996608734131 | KNN Loss: 2.433786392211914 | BCE Loss: 1.0472103357315063\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 3.4305403232574463 | KNN Loss: 2.451965093612671 | BCE Loss: 0.9785752296447754\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 3.475337505340576 | KNN Loss: 2.4773976802825928 | BCE Loss: 0.9979398846626282\n",
      "Epoch   492: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 3.469839572906494 | KNN Loss: 2.435237407684326 | BCE Loss: 1.034602165222168\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 3.493454933166504 | KNN Loss: 2.461280107498169 | BCE Loss: 1.0321749448776245\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 3.466364860534668 | KNN Loss: 2.4447593688964844 | BCE Loss: 1.021605372428894\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 3.479797840118408 | KNN Loss: 2.4739973545074463 | BCE Loss: 1.0058003664016724\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 3.4744648933410645 | KNN Loss: 2.4726979732513428 | BCE Loss: 1.0017669200897217\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 3.4639410972595215 | KNN Loss: 2.4221012592315674 | BCE Loss: 1.0418399572372437\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 3.4501261711120605 | KNN Loss: 2.4316675662994385 | BCE Loss: 1.018458604812622\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 3.480576753616333 | KNN Loss: 2.4617762565612793 | BCE Loss: 1.0188004970550537\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 3.487011671066284 | KNN Loss: 2.455293893814087 | BCE Loss: 1.0317177772521973\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 3.502467155456543 | KNN Loss: 2.4611544609069824 | BCE Loss: 1.04131281375885\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 3.5183942317962646 | KNN Loss: 2.479780673980713 | BCE Loss: 1.0386135578155518\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 3.475630521774292 | KNN Loss: 2.463845729827881 | BCE Loss: 1.0117847919464111\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 3.43656849861145 | KNN Loss: 2.4182353019714355 | BCE Loss: 1.0183331966400146\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 3.4618732929229736 | KNN Loss: 2.4548237323760986 | BCE Loss: 1.007049560546875\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 3.4788684844970703 | KNN Loss: 2.4545202255249023 | BCE Loss: 1.0243483781814575\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 3.4770703315734863 | KNN Loss: 2.456157922744751 | BCE Loss: 1.0209124088287354\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 3.492156505584717 | KNN Loss: 2.452965259552002 | BCE Loss: 1.0391913652420044\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 3.4887962341308594 | KNN Loss: 2.4824657440185547 | BCE Loss: 1.0063304901123047\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 3.4693737030029297 | KNN Loss: 2.4518227577209473 | BCE Loss: 1.0175509452819824\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 3.4814586639404297 | KNN Loss: 2.456630229949951 | BCE Loss: 1.0248284339904785\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 3.4913861751556396 | KNN Loss: 2.45436692237854 | BCE Loss: 1.0370192527770996\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 3.485238790512085 | KNN Loss: 2.4516284465789795 | BCE Loss: 1.0336103439331055\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 3.4949209690093994 | KNN Loss: 2.470675468444824 | BCE Loss: 1.0242455005645752\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 3.4644734859466553 | KNN Loss: 2.442201852798462 | BCE Loss: 1.0222716331481934\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 3.4497523307800293 | KNN Loss: 2.4650933742523193 | BCE Loss: 0.98465895652771\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 3.4827804565429688 | KNN Loss: 2.4713220596313477 | BCE Loss: 1.011458396911621\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 3.5086522102355957 | KNN Loss: 2.454843759536743 | BCE Loss: 1.0538084506988525\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 3.4640941619873047 | KNN Loss: 2.4429211616516113 | BCE Loss: 1.021173119544983\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 3.4573426246643066 | KNN Loss: 2.4418485164642334 | BCE Loss: 1.0154939889907837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 3.4983267784118652 | KNN Loss: 2.4660351276397705 | BCE Loss: 1.0322915315628052\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 3.459026336669922 | KNN Loss: 2.448936700820923 | BCE Loss: 1.010089635848999\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 3.456577777862549 | KNN Loss: 2.4369843006134033 | BCE Loss: 1.019593358039856\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 3.4633026123046875 | KNN Loss: 2.4450020790100098 | BCE Loss: 1.0183006525039673\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 3.49482798576355 | KNN Loss: 2.481560707092285 | BCE Loss: 1.0132672786712646\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 3.4193570613861084 | KNN Loss: 2.4318349361419678 | BCE Loss: 0.9875220656394958\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 3.443190097808838 | KNN Loss: 2.4283461570739746 | BCE Loss: 1.0148439407348633\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 3.477756977081299 | KNN Loss: 2.442638635635376 | BCE Loss: 1.0351182222366333\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 3.497002124786377 | KNN Loss: 2.464240312576294 | BCE Loss: 1.0327616930007935\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 3.5000343322753906 | KNN Loss: 2.4546291828155518 | BCE Loss: 1.0454050302505493\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 3.4483182430267334 | KNN Loss: 2.455937147140503 | BCE Loss: 0.9923810958862305\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 3.455206871032715 | KNN Loss: 2.4249820709228516 | BCE Loss: 1.0302248001098633\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 3.4821462631225586 | KNN Loss: 2.454676866531372 | BCE Loss: 1.0274693965911865\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 3.463589668273926 | KNN Loss: 2.434345006942749 | BCE Loss: 1.0292447805404663\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 3.457704544067383 | KNN Loss: 2.452352285385132 | BCE Loss: 1.0053521394729614\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 3.46671986579895 | KNN Loss: 2.4507088661193848 | BCE Loss: 1.0160109996795654\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 3.5122509002685547 | KNN Loss: 2.4550676345825195 | BCE Loss: 1.0571832656860352\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 3.463587999343872 | KNN Loss: 2.4634063243865967 | BCE Loss: 1.0001816749572754\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 3.4555113315582275 | KNN Loss: 2.4591872692108154 | BCE Loss: 0.9963240623474121\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4856,  2.3745,  2.7505,  3.4871,  3.6530,  0.4236,  2.7899,  2.3621,\n",
      "          2.5124,  2.1661,  2.3326,  2.4144,  0.6715,  1.7256,  1.4388,  1.7747,\n",
      "          2.8992,  3.2629,  2.5353,  2.0810,  1.7299,  3.1325,  2.4733,  2.7931,\n",
      "          2.7131,  1.8447,  1.3999,  1.1521,  1.0779,  0.2516, -0.5336,  1.1434,\n",
      "          0.1360,  1.0365,  1.1041,  1.1722,  1.2571,  2.9358,  0.5095,  0.9384,\n",
      "          0.7459, -0.5871, -0.1879,  2.4866,  2.4053,  0.7438, -0.2547, -0.0358,\n",
      "          1.0731,  2.3665,  2.0010,  0.1006,  1.6326,  0.4142, -0.5522,  0.5195,\n",
      "          1.5745,  1.3536,  1.5414,  1.1375,  0.4956,  1.0106,  0.2676,  1.3019,\n",
      "          1.4890,  1.8231, -2.0561,  0.0387,  2.0633,  1.9456,  2.1997,  0.2808,\n",
      "          1.5345,  2.7063,  1.9748,  1.0967,  0.2422,  0.7985,  0.2194,  1.6211,\n",
      "          0.1200,  0.5925,  2.0239, -0.3642,  0.3667, -0.9796, -2.3407, -0.5386,\n",
      "          0.2856, -1.8904,  0.4330, -0.1589, -0.9170, -0.7434,  0.4221,  1.2528,\n",
      "         -1.2124, -0.8169,  0.2822,  0.9571,  0.8336, -1.3875,  0.9657,  1.2899,\n",
      "         -1.3871, -0.9656, -0.0519, -0.2619, -1.1662, -1.8722, -0.8963, -2.7052,\n",
      "         -0.2849,  1.9583,  1.4802, -0.3910, -0.5287,  0.0876,  1.4307, -2.4534,\n",
      "          0.0808, -0.1465,  0.4692, -0.5458, -0.4462, -0.6862, -0.8442,  0.9859,\n",
      "          0.2258, -0.4463,  0.0834, -0.6554, -1.2833, -0.2633, -0.4149,  0.9177,\n",
      "         -0.1849,  0.2187, -1.9135, -0.9764, -1.2466,  0.4204, -1.9855, -0.8587,\n",
      "         -1.1358, -0.5533, -1.5245, -1.0135, -2.5112, -0.9875, -1.4135, -0.4353,\n",
      "         -1.8497,  0.5422, -1.6088, -0.7741, -3.5044,  0.2999, -0.3904, -0.5992,\n",
      "         -2.2130, -1.6181, -1.1456, -1.4652, -2.1871, -2.5646, -3.8323]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.8323, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.6530, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3fd1c20914173a444f39adee3c315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 79.45it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0467c1e346d454b9b2dc7ca2e1db835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04c466d7d1b46659337ee6342c16b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0870f3027d134ab78307b1b645b974f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "layer 9: 0.0\n",
      "layer 10: 0.0\n",
      "Epoch: 00 | Batch: 000 / 003 | Total loss: 9.629 | Reg loss: 0.014 | Tree loss: 9.629 | Accuracy: 0.000000 | 3.81 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 003 | Total loss: 9.627 | Reg loss: 0.013 | Tree loss: 9.627 | Accuracy: 0.000000 | 3.735 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 003 | Total loss: 9.623 | Reg loss: 0.012 | Tree loss: 9.623 | Accuracy: 0.000000 | 3.564 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 003 | Total loss: 9.623 | Reg loss: 0.003 | Tree loss: 9.623 | Accuracy: 0.000000 | 3.722 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 003 | Total loss: 9.622 | Reg loss: 0.003 | Tree loss: 9.622 | Accuracy: 0.000000 | 3.704 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 003 | Total loss: 9.616 | Reg loss: 0.003 | Tree loss: 9.616 | Accuracy: 0.000000 | 3.622 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 003 | Total loss: 9.621 | Reg loss: 0.003 | Tree loss: 9.621 | Accuracy: 0.000000 | 3.704 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 003 | Total loss: 9.617 | Reg loss: 0.003 | Tree loss: 9.617 | Accuracy: 0.000000 | 3.695 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 003 | Total loss: 9.617 | Reg loss: 0.003 | Tree loss: 9.617 | Accuracy: 0.000000 | 3.642 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 003 | Total loss: 9.620 | Reg loss: 0.002 | Tree loss: 9.620 | Accuracy: 0.000000 | 3.7 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 003 | Total loss: 9.616 | Reg loss: 0.002 | Tree loss: 9.616 | Accuracy: 0.000000 | 3.694 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 003 | Total loss: 9.613 | Reg loss: 0.003 | Tree loss: 9.613 | Accuracy: 0.000000 | 3.654 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 003 | Total loss: 9.619 | Reg loss: 0.002 | Tree loss: 9.619 | Accuracy: 0.000000 | 3.695 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 003 | Total loss: 9.613 | Reg loss: 0.002 | Tree loss: 9.613 | Accuracy: 0.000000 | 3.691 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 003 | Total loss: 9.613 | Reg loss: 0.003 | Tree loss: 9.613 | Accuracy: 0.000000 | 3.659 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 003 | Total loss: 9.612 | Reg loss: 0.002 | Tree loss: 9.612 | Accuracy: 0.000000 | 3.694 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 003 | Total loss: 9.610 | Reg loss: 0.002 | Tree loss: 9.610 | Accuracy: 0.000000 | 3.691 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 003 | Total loss: 9.606 | Reg loss: 0.003 | Tree loss: 9.606 | Accuracy: 0.009317 | 3.663 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 003 | Total loss: 9.609 | Reg loss: 0.003 | Tree loss: 9.609 | Accuracy: 0.000000 | 3.692 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 003 | Total loss: 9.603 | Reg loss: 0.003 | Tree loss: 9.603 | Accuracy: 0.011719 | 3.689 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 003 | Total loss: 9.601 | Reg loss: 0.003 | Tree loss: 9.601 | Accuracy: 0.043478 | 3.666 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 003 | Total loss: 9.603 | Reg loss: 0.003 | Tree loss: 9.603 | Accuracy: 0.017578 | 3.69 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 003 | Total loss: 9.600 | Reg loss: 0.003 | Tree loss: 9.600 | Accuracy: 0.089844 | 3.688 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 003 | Total loss: 9.597 | Reg loss: 0.003 | Tree loss: 9.597 | Accuracy: 0.133540 | 3.668 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 003 | Total loss: 9.599 | Reg loss: 0.003 | Tree loss: 9.599 | Accuracy: 0.095703 | 3.689 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 003 | Total loss: 9.595 | Reg loss: 0.003 | Tree loss: 9.595 | Accuracy: 0.171875 | 3.687 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 003 | Total loss: 9.593 | Reg loss: 0.004 | Tree loss: 9.593 | Accuracy: 0.229814 | 3.669 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 003 | Total loss: 9.596 | Reg loss: 0.003 | Tree loss: 9.596 | Accuracy: 0.222656 | 3.689 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 003 | Total loss: 9.591 | Reg loss: 0.004 | Tree loss: 9.591 | Accuracy: 0.248047 | 3.687 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 003 | Total loss: 9.589 | Reg loss: 0.004 | Tree loss: 9.589 | Accuracy: 0.229814 | 3.671 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 003 | Total loss: 9.591 | Reg loss: 0.004 | Tree loss: 9.591 | Accuracy: 0.234375 | 3.688 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 003 | Total loss: 9.589 | Reg loss: 0.004 | Tree loss: 9.589 | Accuracy: 0.226562 | 3.686 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 003 | Total loss: 9.586 | Reg loss: 0.004 | Tree loss: 9.586 | Accuracy: 0.245342 | 3.672 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 003 | Total loss: 9.588 | Reg loss: 0.004 | Tree loss: 9.588 | Accuracy: 0.236328 | 3.688 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batch: 001 / 003 | Total loss: 9.586 | Reg loss: 0.004 | Tree loss: 9.586 | Accuracy: 0.234375 | 3.685 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 003 | Total loss: 9.583 | Reg loss: 0.004 | Tree loss: 9.583 | Accuracy: 0.229814 | 3.672 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 003 | Total loss: 9.585 | Reg loss: 0.004 | Tree loss: 9.585 | Accuracy: 0.230469 | 3.687 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 003 | Total loss: 9.583 | Reg loss: 0.004 | Tree loss: 9.583 | Accuracy: 0.230469 | 3.685 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 003 | Total loss: 9.580 | Reg loss: 0.004 | Tree loss: 9.580 | Accuracy: 0.245342 | 3.673 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 003 | Total loss: 9.583 | Reg loss: 0.004 | Tree loss: 9.583 | Accuracy: 0.234375 | 3.686 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 003 | Total loss: 9.582 | Reg loss: 0.004 | Tree loss: 9.582 | Accuracy: 0.205078 | 3.685 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 003 | Total loss: 9.574 | Reg loss: 0.004 | Tree loss: 9.574 | Accuracy: 0.279503 | 3.673 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 003 | Total loss: 9.580 | Reg loss: 0.004 | Tree loss: 9.580 | Accuracy: 0.236328 | 3.685 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 003 | Total loss: 9.579 | Reg loss: 0.004 | Tree loss: 9.579 | Accuracy: 0.226562 | 3.684 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 003 | Total loss: 9.574 | Reg loss: 0.005 | Tree loss: 9.574 | Accuracy: 0.242236 | 3.673 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 003 | Total loss: 9.578 | Reg loss: 0.005 | Tree loss: 9.578 | Accuracy: 0.230469 | 3.685 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 003 | Total loss: 9.576 | Reg loss: 0.005 | Tree loss: 9.576 | Accuracy: 0.232422 | 3.684 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 003 | Total loss: 9.572 | Reg loss: 0.005 | Tree loss: 9.572 | Accuracy: 0.242236 | 3.675 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 003 | Total loss: 9.574 | Reg loss: 0.005 | Tree loss: 9.574 | Accuracy: 0.255859 | 3.686 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 003 | Total loss: 9.576 | Reg loss: 0.005 | Tree loss: 9.576 | Accuracy: 0.218750 | 3.686 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 003 | Total loss: 9.571 | Reg loss: 0.005 | Tree loss: 9.571 | Accuracy: 0.223602 | 3.677 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 003 | Total loss: 9.573 | Reg loss: 0.005 | Tree loss: 9.573 | Accuracy: 0.246094 | 3.687 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 003 | Total loss: 9.571 | Reg loss: 0.005 | Tree loss: 9.571 | Accuracy: 0.240234 | 3.686 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 003 | Total loss: 9.571 | Reg loss: 0.005 | Tree loss: 9.571 | Accuracy: 0.204969 | 3.677 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 003 | Total loss: 9.568 | Reg loss: 0.005 | Tree loss: 9.568 | Accuracy: 0.269531 | 3.687 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 003 | Total loss: 9.574 | Reg loss: 0.005 | Tree loss: 9.574 | Accuracy: 0.197266 | 3.686 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 003 | Total loss: 9.565 | Reg loss: 0.005 | Tree loss: 9.565 | Accuracy: 0.236025 | 3.677 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 003 | Total loss: 9.571 | Reg loss: 0.005 | Tree loss: 9.571 | Accuracy: 0.236328 | 3.687 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 003 | Total loss: 9.567 | Reg loss: 0.005 | Tree loss: 9.567 | Accuracy: 0.234375 | 3.686 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 003 | Total loss: 9.565 | Reg loss: 0.006 | Tree loss: 9.565 | Accuracy: 0.229814 | 3.678 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 003 | Total loss: 9.569 | Reg loss: 0.005 | Tree loss: 9.569 | Accuracy: 0.238281 | 3.686 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 003 | Total loss: 9.565 | Reg loss: 0.006 | Tree loss: 9.565 | Accuracy: 0.226562 | 3.686 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 003 | Total loss: 9.564 | Reg loss: 0.006 | Tree loss: 9.564 | Accuracy: 0.239130 | 3.678 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 003 | Total loss: 9.567 | Reg loss: 0.006 | Tree loss: 9.567 | Accuracy: 0.230469 | 3.686 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 003 | Total loss: 9.563 | Reg loss: 0.006 | Tree loss: 9.563 | Accuracy: 0.234375 | 3.685 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 003 | Total loss: 9.563 | Reg loss: 0.006 | Tree loss: 9.563 | Accuracy: 0.239130 | 3.678 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 003 | Total loss: 9.566 | Reg loss: 0.006 | Tree loss: 9.566 | Accuracy: 0.226562 | 3.686 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 003 | Total loss: 9.561 | Reg loss: 0.006 | Tree loss: 9.561 | Accuracy: 0.238281 | 3.686 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 003 | Total loss: 9.558 | Reg loss: 0.006 | Tree loss: 9.558 | Accuracy: 0.239130 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 000 / 003 | Total loss: 9.562 | Reg loss: 0.006 | Tree loss: 9.562 | Accuracy: 0.234375 | 3.687 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 003 | Total loss: 9.562 | Reg loss: 0.006 | Tree loss: 9.562 | Accuracy: 0.226562 | 3.686 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 003 | Total loss: 9.556 | Reg loss: 0.006 | Tree loss: 9.556 | Accuracy: 0.245342 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 003 | Total loss: 9.563 | Reg loss: 0.006 | Tree loss: 9.563 | Accuracy: 0.212891 | 3.687 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 003 | Total loss: 9.557 | Reg loss: 0.006 | Tree loss: 9.557 | Accuracy: 0.242188 | 3.686 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 003 | Total loss: 9.555 | Reg loss: 0.007 | Tree loss: 9.555 | Accuracy: 0.254658 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 003 | Total loss: 9.559 | Reg loss: 0.006 | Tree loss: 9.559 | Accuracy: 0.222656 | 3.687 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 003 | Total loss: 9.559 | Reg loss: 0.007 | Tree loss: 9.559 | Accuracy: 0.212891 | 3.686 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 003 | Total loss: 9.549 | Reg loss: 0.007 | Tree loss: 9.549 | Accuracy: 0.285714 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 003 | Total loss: 9.558 | Reg loss: 0.007 | Tree loss: 9.558 | Accuracy: 0.224609 | 3.687 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 003 | Total loss: 9.555 | Reg loss: 0.007 | Tree loss: 9.555 | Accuracy: 0.224609 | 3.686 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 003 | Total loss: 9.548 | Reg loss: 0.007 | Tree loss: 9.548 | Accuracy: 0.263975 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 003 | Total loss: 9.555 | Reg loss: 0.007 | Tree loss: 9.555 | Accuracy: 0.226562 | 3.686 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 003 | Total loss: 9.550 | Reg loss: 0.007 | Tree loss: 9.550 | Accuracy: 0.251953 | 3.685 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 003 | Total loss: 9.552 | Reg loss: 0.007 | Tree loss: 9.552 | Accuracy: 0.217391 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 003 | Total loss: 9.552 | Reg loss: 0.007 | Tree loss: 9.552 | Accuracy: 0.242188 | 3.686 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 003 | Total loss: 9.549 | Reg loss: 0.007 | Tree loss: 9.549 | Accuracy: 0.236328 | 3.685 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 003 | Total loss: 9.547 | Reg loss: 0.007 | Tree loss: 9.547 | Accuracy: 0.217391 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 003 | Total loss: 9.551 | Reg loss: 0.007 | Tree loss: 9.551 | Accuracy: 0.230469 | 3.686 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 003 | Total loss: 9.545 | Reg loss: 0.007 | Tree loss: 9.545 | Accuracy: 0.236328 | 3.685 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 003 | Total loss: 9.544 | Reg loss: 0.008 | Tree loss: 9.544 | Accuracy: 0.236025 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 003 | Total loss: 9.549 | Reg loss: 0.007 | Tree loss: 9.549 | Accuracy: 0.220703 | 3.685 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 003 | Total loss: 9.542 | Reg loss: 0.008 | Tree loss: 9.542 | Accuracy: 0.259766 | 3.685 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 003 | Total loss: 9.540 | Reg loss: 0.008 | Tree loss: 9.540 | Accuracy: 0.214286 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 003 | Total loss: 9.544 | Reg loss: 0.008 | Tree loss: 9.544 | Accuracy: 0.242188 | 3.685 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 003 | Total loss: 9.542 | Reg loss: 0.008 | Tree loss: 9.542 | Accuracy: 0.220703 | 3.684 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 003 | Total loss: 9.537 | Reg loss: 0.008 | Tree loss: 9.537 | Accuracy: 0.242236 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 003 | Total loss: 9.540 | Reg loss: 0.008 | Tree loss: 9.540 | Accuracy: 0.246094 | 3.686 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 003 | Total loss: 9.539 | Reg loss: 0.008 | Tree loss: 9.539 | Accuracy: 0.228516 | 3.685 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 003 | Total loss: 9.535 | Reg loss: 0.008 | Tree loss: 9.535 | Accuracy: 0.223602 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 003 | Total loss: 9.538 | Reg loss: 0.008 | Tree loss: 9.538 | Accuracy: 0.228516 | 3.685 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 003 | Total loss: 9.534 | Reg loss: 0.008 | Tree loss: 9.534 | Accuracy: 0.220703 | 3.685 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 003 | Total loss: 9.531 | Reg loss: 0.008 | Tree loss: 9.531 | Accuracy: 0.263975 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 003 | Total loss: 9.535 | Reg loss: 0.008 | Tree loss: 9.535 | Accuracy: 0.220703 | 3.685 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 003 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.248047 | 3.685 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 003 | Total loss: 9.527 | Reg loss: 0.009 | Tree loss: 9.527 | Accuracy: 0.232919 | 3.68 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 003 | Total loss: 9.527 | Reg loss: 0.008 | Tree loss: 9.527 | Accuracy: 0.255859 | 3.685 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 003 | Total loss: 9.527 | Reg loss: 0.009 | Tree loss: 9.527 | Accuracy: 0.232422 | 3.684 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 003 | Total loss: 9.524 | Reg loss: 0.009 | Tree loss: 9.524 | Accuracy: 0.201863 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 003 | Total loss: 9.525 | Reg loss: 0.009 | Tree loss: 9.525 | Accuracy: 0.240234 | 3.684 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 003 | Total loss: 9.519 | Reg loss: 0.009 | Tree loss: 9.519 | Accuracy: 0.236328 | 3.684 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 003 | Total loss: 9.521 | Reg loss: 0.009 | Tree loss: 9.521 | Accuracy: 0.220497 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 003 | Total loss: 9.524 | Reg loss: 0.009 | Tree loss: 9.524 | Accuracy: 0.208984 | 3.684 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 003 | Total loss: 9.514 | Reg loss: 0.009 | Tree loss: 9.514 | Accuracy: 0.257812 | 3.683 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 003 | Total loss: 9.508 | Reg loss: 0.009 | Tree loss: 9.508 | Accuracy: 0.236025 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 003 | Total loss: 9.517 | Reg loss: 0.009 | Tree loss: 9.517 | Accuracy: 0.224609 | 3.684 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 003 | Total loss: 9.510 | Reg loss: 0.009 | Tree loss: 9.510 | Accuracy: 0.234375 | 3.683 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 003 | Total loss: 9.502 | Reg loss: 0.009 | Tree loss: 9.502 | Accuracy: 0.248447 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 003 | Total loss: 9.511 | Reg loss: 0.009 | Tree loss: 9.511 | Accuracy: 0.230469 | 3.683 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 003 | Total loss: 9.500 | Reg loss: 0.009 | Tree loss: 9.500 | Accuracy: 0.257812 | 3.683 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 003 | Total loss: 9.501 | Reg loss: 0.010 | Tree loss: 9.501 | Accuracy: 0.201863 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 003 | Total loss: 9.504 | Reg loss: 0.009 | Tree loss: 9.504 | Accuracy: 0.218750 | 3.683 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 003 | Total loss: 9.496 | Reg loss: 0.010 | Tree loss: 9.496 | Accuracy: 0.253906 | 3.683 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 003 | Total loss: 9.490 | Reg loss: 0.010 | Tree loss: 9.490 | Accuracy: 0.226708 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 003 | Total loss: 9.498 | Reg loss: 0.010 | Tree loss: 9.498 | Accuracy: 0.224609 | 3.683 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 003 | Total loss: 9.487 | Reg loss: 0.010 | Tree loss: 9.487 | Accuracy: 0.238281 | 3.682 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 003 | Total loss: 9.481 | Reg loss: 0.010 | Tree loss: 9.481 | Accuracy: 0.242236 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 003 | Total loss: 9.489 | Reg loss: 0.010 | Tree loss: 9.489 | Accuracy: 0.224609 | 3.682 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 003 | Total loss: 9.480 | Reg loss: 0.010 | Tree loss: 9.480 | Accuracy: 0.240234 | 3.682 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 003 | Total loss: 9.473 | Reg loss: 0.010 | Tree loss: 9.473 | Accuracy: 0.239130 | 3.678 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 003 | Total loss: 9.477 | Reg loss: 0.010 | Tree loss: 9.477 | Accuracy: 0.240234 | 3.682 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 003 | Total loss: 9.470 | Reg loss: 0.010 | Tree loss: 9.470 | Accuracy: 0.259766 | 3.682 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 003 | Total loss: 9.471 | Reg loss: 0.010 | Tree loss: 9.471 | Accuracy: 0.183230 | 3.678 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 003 | Total loss: 9.471 | Reg loss: 0.010 | Tree loss: 9.471 | Accuracy: 0.244141 | 3.682 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 003 | Total loss: 9.461 | Reg loss: 0.010 | Tree loss: 9.461 | Accuracy: 0.240234 | 3.682 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 003 | Total loss: 9.451 | Reg loss: 0.011 | Tree loss: 9.451 | Accuracy: 0.208075 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 003 | Total loss: 9.462 | Reg loss: 0.011 | Tree loss: 9.462 | Accuracy: 0.210938 | 3.682 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 003 | Total loss: 9.450 | Reg loss: 0.011 | Tree loss: 9.450 | Accuracy: 0.246094 | 3.682 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 003 | Total loss: 9.440 | Reg loss: 0.011 | Tree loss: 9.440 | Accuracy: 0.251553 | 3.678 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 000 / 003 | Total loss: 9.456 | Reg loss: 0.011 | Tree loss: 9.456 | Accuracy: 0.214844 | 3.682 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 003 | Total loss: 9.435 | Reg loss: 0.011 | Tree loss: 9.435 | Accuracy: 0.253906 | 3.682 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 003 | Total loss: 9.422 | Reg loss: 0.011 | Tree loss: 9.422 | Accuracy: 0.232919 | 3.678 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 003 | Total loss: 9.435 | Reg loss: 0.011 | Tree loss: 9.435 | Accuracy: 0.244141 | 3.682 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 003 | Total loss: 9.425 | Reg loss: 0.011 | Tree loss: 9.425 | Accuracy: 0.228516 | 3.682 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 003 | Total loss: 9.420 | Reg loss: 0.011 | Tree loss: 9.420 | Accuracy: 0.226708 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 003 | Total loss: 9.422 | Reg loss: 0.011 | Tree loss: 9.422 | Accuracy: 0.230469 | 3.682 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 003 | Total loss: 9.409 | Reg loss: 0.011 | Tree loss: 9.409 | Accuracy: 0.236328 | 3.682 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 003 | Total loss: 9.409 | Reg loss: 0.011 | Tree loss: 9.409 | Accuracy: 0.236025 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 003 | Total loss: 9.405 | Reg loss: 0.011 | Tree loss: 9.405 | Accuracy: 0.263672 | 3.682 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 003 | Total loss: 9.404 | Reg loss: 0.011 | Tree loss: 9.404 | Accuracy: 0.210938 | 3.682 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 003 | Total loss: 9.383 | Reg loss: 0.012 | Tree loss: 9.383 | Accuracy: 0.223602 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 003 | Total loss: 9.396 | Reg loss: 0.012 | Tree loss: 9.396 | Accuracy: 0.232422 | 3.682 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 003 | Total loss: 9.381 | Reg loss: 0.012 | Tree loss: 9.381 | Accuracy: 0.226562 | 3.682 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 003 | Total loss: 9.368 | Reg loss: 0.012 | Tree loss: 9.368 | Accuracy: 0.248447 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 003 | Total loss: 9.383 | Reg loss: 0.012 | Tree loss: 9.383 | Accuracy: 0.226562 | 3.682 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 003 | Total loss: 9.362 | Reg loss: 0.012 | Tree loss: 9.362 | Accuracy: 0.248047 | 3.682 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 003 | Total loss: 9.349 | Reg loss: 0.012 | Tree loss: 9.349 | Accuracy: 0.223602 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 003 | Total loss: 9.362 | Reg loss: 0.012 | Tree loss: 9.362 | Accuracy: 0.214844 | 3.683 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 003 | Total loss: 9.349 | Reg loss: 0.012 | Tree loss: 9.349 | Accuracy: 0.236328 | 3.682 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 003 | Total loss: 9.326 | Reg loss: 0.012 | Tree loss: 9.326 | Accuracy: 0.260870 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 003 | Total loss: 9.340 | Reg loss: 0.012 | Tree loss: 9.340 | Accuracy: 0.246094 | 3.682 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 003 | Total loss: 9.326 | Reg loss: 0.012 | Tree loss: 9.326 | Accuracy: 0.236328 | 3.682 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 003 | Total loss: 9.319 | Reg loss: 0.012 | Tree loss: 9.319 | Accuracy: 0.211180 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 003 | Total loss: 9.325 | Reg loss: 0.012 | Tree loss: 9.325 | Accuracy: 0.236328 | 3.682 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 003 | Total loss: 9.307 | Reg loss: 0.012 | Tree loss: 9.307 | Accuracy: 0.222656 | 3.682 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 003 | Total loss: 9.286 | Reg loss: 0.013 | Tree loss: 9.286 | Accuracy: 0.248447 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 003 | Total loss: 9.301 | Reg loss: 0.013 | Tree loss: 9.301 | Accuracy: 0.246094 | 3.682 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 003 | Total loss: 9.284 | Reg loss: 0.013 | Tree loss: 9.284 | Accuracy: 0.242188 | 3.682 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 003 | Total loss: 9.272 | Reg loss: 0.013 | Tree loss: 9.272 | Accuracy: 0.201863 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 003 | Total loss: 9.283 | Reg loss: 0.013 | Tree loss: 9.283 | Accuracy: 0.214844 | 3.682 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 003 | Total loss: 9.264 | Reg loss: 0.013 | Tree loss: 9.264 | Accuracy: 0.228516 | 3.681 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 003 | Total loss: 9.239 | Reg loss: 0.013 | Tree loss: 9.239 | Accuracy: 0.273292 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 003 | Total loss: 9.263 | Reg loss: 0.013 | Tree loss: 9.263 | Accuracy: 0.220703 | 3.682 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 003 | Total loss: 9.237 | Reg loss: 0.013 | Tree loss: 9.237 | Accuracy: 0.232422 | 3.681 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 003 | Total loss: 9.214 | Reg loss: 0.013 | Tree loss: 9.214 | Accuracy: 0.257764 | 3.679 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 003 | Total loss: 9.240 | Reg loss: 0.013 | Tree loss: 9.240 | Accuracy: 0.226562 | 3.682 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 003 | Total loss: 9.212 | Reg loss: 0.013 | Tree loss: 9.212 | Accuracy: 0.236328 | 3.681 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 003 | Total loss: 9.186 | Reg loss: 0.013 | Tree loss: 9.186 | Accuracy: 0.242236 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 003 | Total loss: 9.212 | Reg loss: 0.013 | Tree loss: 9.212 | Accuracy: 0.232422 | 3.682 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 003 | Total loss: 9.179 | Reg loss: 0.013 | Tree loss: 9.179 | Accuracy: 0.261719 | 3.681 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 003 | Total loss: 9.175 | Reg loss: 0.013 | Tree loss: 9.175 | Accuracy: 0.192547 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 003 | Total loss: 9.186 | Reg loss: 0.013 | Tree loss: 9.186 | Accuracy: 0.207031 | 3.682 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 003 | Total loss: 9.161 | Reg loss: 0.014 | Tree loss: 9.161 | Accuracy: 0.244141 | 3.682 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 003 | Total loss: 9.135 | Reg loss: 0.014 | Tree loss: 9.135 | Accuracy: 0.260870 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 003 | Total loss: 9.163 | Reg loss: 0.014 | Tree loss: 9.163 | Accuracy: 0.216797 | 3.682 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 003 | Total loss: 9.123 | Reg loss: 0.014 | Tree loss: 9.123 | Accuracy: 0.250000 | 3.682 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 003 | Total loss: 9.113 | Reg loss: 0.014 | Tree loss: 9.113 | Accuracy: 0.236025 | 3.679 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 003 | Total loss: 9.117 | Reg loss: 0.014 | Tree loss: 9.117 | Accuracy: 0.240234 | 3.682 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 003 | Total loss: 9.109 | Reg loss: 0.014 | Tree loss: 9.109 | Accuracy: 0.234375 | 3.682 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 003 | Total loss: 9.089 | Reg loss: 0.014 | Tree loss: 9.089 | Accuracy: 0.223602 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 003 | Total loss: 9.089 | Reg loss: 0.014 | Tree loss: 9.089 | Accuracy: 0.261719 | 3.682 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 003 | Total loss: 9.075 | Reg loss: 0.014 | Tree loss: 9.075 | Accuracy: 0.226562 | 3.682 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 003 | Total loss: 9.062 | Reg loss: 0.014 | Tree loss: 9.062 | Accuracy: 0.201863 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 003 | Total loss: 9.062 | Reg loss: 0.014 | Tree loss: 9.062 | Accuracy: 0.257812 | 3.682 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 003 | Total loss: 9.055 | Reg loss: 0.014 | Tree loss: 9.055 | Accuracy: 0.212891 | 3.682 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 003 | Total loss: 9.005 | Reg loss: 0.014 | Tree loss: 9.005 | Accuracy: 0.229814 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 003 | Total loss: 9.026 | Reg loss: 0.014 | Tree loss: 9.026 | Accuracy: 0.232422 | 3.682 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 003 | Total loss: 9.021 | Reg loss: 0.014 | Tree loss: 9.021 | Accuracy: 0.212891 | 3.682 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 003 | Total loss: 8.986 | Reg loss: 0.015 | Tree loss: 8.986 | Accuracy: 0.270186 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 003 | Total loss: 8.996 | Reg loss: 0.014 | Tree loss: 8.996 | Accuracy: 0.251953 | 3.683 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 003 | Total loss: 8.986 | Reg loss: 0.015 | Tree loss: 8.986 | Accuracy: 0.226562 | 3.683 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 003 | Total loss: 8.952 | Reg loss: 0.015 | Tree loss: 8.952 | Accuracy: 0.217391 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 003 | Total loss: 8.971 | Reg loss: 0.015 | Tree loss: 8.971 | Accuracy: 0.226562 | 3.683 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 003 | Total loss: 8.941 | Reg loss: 0.015 | Tree loss: 8.941 | Accuracy: 0.238281 | 3.683 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 003 | Total loss: 8.923 | Reg loss: 0.015 | Tree loss: 8.923 | Accuracy: 0.239130 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 003 | Total loss: 8.942 | Reg loss: 0.015 | Tree loss: 8.942 | Accuracy: 0.224609 | 3.683 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 003 | Total loss: 8.907 | Reg loss: 0.015 | Tree loss: 8.907 | Accuracy: 0.242188 | 3.683 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 003 | Total loss: 8.880 | Reg loss: 0.015 | Tree loss: 8.880 | Accuracy: 0.232919 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 000 / 003 | Total loss: 8.895 | Reg loss: 0.015 | Tree loss: 8.895 | Accuracy: 0.253906 | 3.683 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 003 | Total loss: 8.876 | Reg loss: 0.015 | Tree loss: 8.876 | Accuracy: 0.234375 | 3.683 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 003 | Total loss: 8.857 | Reg loss: 0.015 | Tree loss: 8.857 | Accuracy: 0.198758 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 003 | Total loss: 8.872 | Reg loss: 0.015 | Tree loss: 8.872 | Accuracy: 0.246094 | 3.683 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 003 | Total loss: 8.832 | Reg loss: 0.015 | Tree loss: 8.832 | Accuracy: 0.234375 | 3.683 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 003 | Total loss: 8.815 | Reg loss: 0.015 | Tree loss: 8.815 | Accuracy: 0.211180 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 003 | Total loss: 8.823 | Reg loss: 0.015 | Tree loss: 8.823 | Accuracy: 0.257812 | 3.683 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 003 | Total loss: 8.811 | Reg loss: 0.015 | Tree loss: 8.811 | Accuracy: 0.220703 | 3.682 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 003 | Total loss: 8.775 | Reg loss: 0.015 | Tree loss: 8.775 | Accuracy: 0.214286 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 003 | Total loss: 8.798 | Reg loss: 0.015 | Tree loss: 8.798 | Accuracy: 0.236328 | 3.682 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 003 | Total loss: 8.759 | Reg loss: 0.015 | Tree loss: 8.759 | Accuracy: 0.226562 | 3.682 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 003 | Total loss: 8.742 | Reg loss: 0.016 | Tree loss: 8.742 | Accuracy: 0.242236 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 003 | Total loss: 8.758 | Reg loss: 0.016 | Tree loss: 8.758 | Accuracy: 0.216797 | 3.683 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 003 | Total loss: 8.731 | Reg loss: 0.016 | Tree loss: 8.731 | Accuracy: 0.244141 | 3.682 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 003 | Total loss: 8.695 | Reg loss: 0.016 | Tree loss: 8.695 | Accuracy: 0.236025 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 003 | Total loss: 8.724 | Reg loss: 0.016 | Tree loss: 8.724 | Accuracy: 0.222656 | 3.682 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 003 | Total loss: 8.693 | Reg loss: 0.016 | Tree loss: 8.693 | Accuracy: 0.244141 | 3.682 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 003 | Total loss: 8.651 | Reg loss: 0.016 | Tree loss: 8.651 | Accuracy: 0.226708 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 003 | Total loss: 8.685 | Reg loss: 0.016 | Tree loss: 8.685 | Accuracy: 0.230469 | 3.682 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 003 | Total loss: 8.660 | Reg loss: 0.016 | Tree loss: 8.660 | Accuracy: 0.218750 | 3.682 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 003 | Total loss: 8.605 | Reg loss: 0.016 | Tree loss: 8.605 | Accuracy: 0.254658 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 003 | Total loss: 8.634 | Reg loss: 0.016 | Tree loss: 8.634 | Accuracy: 0.263672 | 3.683 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 003 | Total loss: 8.615 | Reg loss: 0.016 | Tree loss: 8.615 | Accuracy: 0.218750 | 3.682 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 003 | Total loss: 8.594 | Reg loss: 0.016 | Tree loss: 8.594 | Accuracy: 0.201863 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 003 | Total loss: 8.624 | Reg loss: 0.016 | Tree loss: 8.624 | Accuracy: 0.222656 | 3.682 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 003 | Total loss: 8.564 | Reg loss: 0.016 | Tree loss: 8.564 | Accuracy: 0.230469 | 3.682 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 003 | Total loss: 8.528 | Reg loss: 0.016 | Tree loss: 8.528 | Accuracy: 0.245342 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 003 | Total loss: 8.567 | Reg loss: 0.016 | Tree loss: 8.567 | Accuracy: 0.236328 | 3.682 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 003 | Total loss: 8.536 | Reg loss: 0.016 | Tree loss: 8.536 | Accuracy: 0.224609 | 3.682 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 003 | Total loss: 8.496 | Reg loss: 0.016 | Tree loss: 8.496 | Accuracy: 0.232919 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 003 | Total loss: 8.534 | Reg loss: 0.016 | Tree loss: 8.534 | Accuracy: 0.228516 | 3.682 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 003 | Total loss: 8.478 | Reg loss: 0.016 | Tree loss: 8.478 | Accuracy: 0.248047 | 3.682 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 003 | Total loss: 8.477 | Reg loss: 0.016 | Tree loss: 8.477 | Accuracy: 0.208075 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 003 | Total loss: 8.483 | Reg loss: 0.016 | Tree loss: 8.483 | Accuracy: 0.265625 | 3.682 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 003 | Total loss: 8.462 | Reg loss: 0.016 | Tree loss: 8.462 | Accuracy: 0.208984 | 3.682 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 003 | Total loss: 8.415 | Reg loss: 0.017 | Tree loss: 8.415 | Accuracy: 0.217391 | 3.68 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 003 | Total loss: 8.444 | Reg loss: 0.017 | Tree loss: 8.444 | Accuracy: 0.242188 | 3.682 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 003 | Total loss: 8.419 | Reg loss: 0.017 | Tree loss: 8.419 | Accuracy: 0.220703 | 3.682 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 003 | Total loss: 8.377 | Reg loss: 0.017 | Tree loss: 8.377 | Accuracy: 0.229814 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 003 | Total loss: 8.436 | Reg loss: 0.017 | Tree loss: 8.436 | Accuracy: 0.207031 | 3.682 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 003 | Total loss: 8.354 | Reg loss: 0.017 | Tree loss: 8.354 | Accuracy: 0.238281 | 3.682 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 003 | Total loss: 8.323 | Reg loss: 0.017 | Tree loss: 8.323 | Accuracy: 0.239130 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 003 | Total loss: 8.362 | Reg loss: 0.017 | Tree loss: 8.362 | Accuracy: 0.205078 | 3.682 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 003 | Total loss: 8.329 | Reg loss: 0.017 | Tree loss: 8.329 | Accuracy: 0.222656 | 3.682 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 003 | Total loss: 8.309 | Reg loss: 0.017 | Tree loss: 8.309 | Accuracy: 0.236025 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 003 | Total loss: 8.334 | Reg loss: 0.017 | Tree loss: 8.334 | Accuracy: 0.203125 | 3.682 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 003 | Total loss: 8.287 | Reg loss: 0.017 | Tree loss: 8.287 | Accuracy: 0.218750 | 3.682 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 003 | Total loss: 8.251 | Reg loss: 0.017 | Tree loss: 8.251 | Accuracy: 0.242236 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 003 | Total loss: 8.291 | Reg loss: 0.017 | Tree loss: 8.291 | Accuracy: 0.201172 | 3.682 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 003 | Total loss: 8.243 | Reg loss: 0.017 | Tree loss: 8.243 | Accuracy: 0.220703 | 3.682 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 003 | Total loss: 8.218 | Reg loss: 0.017 | Tree loss: 8.218 | Accuracy: 0.245342 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 003 | Total loss: 8.245 | Reg loss: 0.017 | Tree loss: 8.245 | Accuracy: 0.240234 | 3.682 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 003 | Total loss: 8.211 | Reg loss: 0.017 | Tree loss: 8.211 | Accuracy: 0.197266 | 3.682 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 003 | Total loss: 8.171 | Reg loss: 0.017 | Tree loss: 8.171 | Accuracy: 0.220497 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 003 | Total loss: 8.188 | Reg loss: 0.017 | Tree loss: 8.188 | Accuracy: 0.222656 | 3.682 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 003 | Total loss: 8.174 | Reg loss: 0.017 | Tree loss: 8.174 | Accuracy: 0.208984 | 3.682 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 003 | Total loss: 8.148 | Reg loss: 0.017 | Tree loss: 8.148 | Accuracy: 0.229814 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 003 | Total loss: 8.154 | Reg loss: 0.017 | Tree loss: 8.154 | Accuracy: 0.218750 | 3.682 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 003 | Total loss: 8.131 | Reg loss: 0.017 | Tree loss: 8.131 | Accuracy: 0.240234 | 3.682 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 003 | Total loss: 8.097 | Reg loss: 0.017 | Tree loss: 8.097 | Accuracy: 0.186335 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 003 | Total loss: 8.102 | Reg loss: 0.017 | Tree loss: 8.102 | Accuracy: 0.244141 | 3.682 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 003 | Total loss: 8.105 | Reg loss: 0.017 | Tree loss: 8.105 | Accuracy: 0.210938 | 3.682 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 003 | Total loss: 8.048 | Reg loss: 0.017 | Tree loss: 8.048 | Accuracy: 0.195652 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 003 | Total loss: 8.074 | Reg loss: 0.017 | Tree loss: 8.074 | Accuracy: 0.210938 | 3.682 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 003 | Total loss: 8.031 | Reg loss: 0.017 | Tree loss: 8.031 | Accuracy: 0.224609 | 3.682 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 003 | Total loss: 8.037 | Reg loss: 0.018 | Tree loss: 8.037 | Accuracy: 0.223602 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 003 | Total loss: 8.029 | Reg loss: 0.017 | Tree loss: 8.029 | Accuracy: 0.230469 | 3.682 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 003 | Total loss: 8.006 | Reg loss: 0.018 | Tree loss: 8.006 | Accuracy: 0.240234 | 3.682 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 003 | Total loss: 7.976 | Reg loss: 0.018 | Tree loss: 7.976 | Accuracy: 0.164596 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 000 / 003 | Total loss: 7.985 | Reg loss: 0.018 | Tree loss: 7.985 | Accuracy: 0.244141 | 3.682 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 003 | Total loss: 7.962 | Reg loss: 0.018 | Tree loss: 7.962 | Accuracy: 0.205078 | 3.682 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 003 | Total loss: 7.943 | Reg loss: 0.018 | Tree loss: 7.943 | Accuracy: 0.195652 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 003 | Total loss: 7.948 | Reg loss: 0.018 | Tree loss: 7.948 | Accuracy: 0.220703 | 3.682 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 003 | Total loss: 7.936 | Reg loss: 0.018 | Tree loss: 7.936 | Accuracy: 0.205078 | 3.682 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 003 | Total loss: 7.872 | Reg loss: 0.018 | Tree loss: 7.872 | Accuracy: 0.226708 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 003 | Total loss: 7.918 | Reg loss: 0.018 | Tree loss: 7.918 | Accuracy: 0.208984 | 3.682 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 003 | Total loss: 7.891 | Reg loss: 0.018 | Tree loss: 7.891 | Accuracy: 0.214844 | 3.682 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 003 | Total loss: 7.820 | Reg loss: 0.018 | Tree loss: 7.820 | Accuracy: 0.232919 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 003 | Total loss: 7.870 | Reg loss: 0.018 | Tree loss: 7.870 | Accuracy: 0.197266 | 3.682 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 003 | Total loss: 7.842 | Reg loss: 0.018 | Tree loss: 7.842 | Accuracy: 0.228516 | 3.682 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 003 | Total loss: 7.803 | Reg loss: 0.018 | Tree loss: 7.803 | Accuracy: 0.223602 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 003 | Total loss: 7.852 | Reg loss: 0.018 | Tree loss: 7.852 | Accuracy: 0.191406 | 3.682 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 003 | Total loss: 7.777 | Reg loss: 0.018 | Tree loss: 7.777 | Accuracy: 0.218750 | 3.682 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 003 | Total loss: 7.763 | Reg loss: 0.018 | Tree loss: 7.763 | Accuracy: 0.236025 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 003 | Total loss: 7.766 | Reg loss: 0.018 | Tree loss: 7.766 | Accuracy: 0.224609 | 3.682 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 003 | Total loss: 7.763 | Reg loss: 0.018 | Tree loss: 7.763 | Accuracy: 0.212891 | 3.682 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 003 | Total loss: 7.753 | Reg loss: 0.018 | Tree loss: 7.753 | Accuracy: 0.204969 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 003 | Total loss: 7.743 | Reg loss: 0.018 | Tree loss: 7.743 | Accuracy: 0.208984 | 3.682 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 003 | Total loss: 7.715 | Reg loss: 0.018 | Tree loss: 7.715 | Accuracy: 0.232422 | 3.682 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 003 | Total loss: 7.695 | Reg loss: 0.018 | Tree loss: 7.695 | Accuracy: 0.186335 | 3.68 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 003 | Total loss: 7.720 | Reg loss: 0.018 | Tree loss: 7.720 | Accuracy: 0.203125 | 3.682 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 003 | Total loss: 7.663 | Reg loss: 0.018 | Tree loss: 7.663 | Accuracy: 0.218750 | 3.682 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 003 | Total loss: 7.645 | Reg loss: 0.018 | Tree loss: 7.645 | Accuracy: 0.214286 | 3.68 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672d0f0676314936bb7ca25c15e2cefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cf2f1d28bb466c9e3f0e36249d4096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf5ef068d5847399c439e4507141371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715806a31a244ddaa611e211049c2dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 11.999511480214949\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 4094\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "1346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "============== Pattern 1025 ==============\n",
      "============== Pattern 1026 ==============\n",
      "============== Pattern 1027 ==============\n",
      "============== Pattern 1028 ==============\n",
      "============== Pattern 1029 ==============\n",
      "============== Pattern 1030 ==============\n",
      "============== Pattern 1031 ==============\n",
      "============== Pattern 1032 ==============\n",
      "============== Pattern 1033 ==============\n",
      "============== Pattern 1034 ==============\n",
      "============== Pattern 1035 ==============\n",
      "============== Pattern 1036 ==============\n",
      "============== Pattern 1037 ==============\n",
      "============== Pattern 1038 ==============\n",
      "============== Pattern 1039 ==============\n",
      "============== Pattern 1040 ==============\n",
      "============== Pattern 1041 ==============\n",
      "============== Pattern 1042 ==============\n",
      "============== Pattern 1043 ==============\n",
      "============== Pattern 1044 ==============\n",
      "============== Pattern 1045 ==============\n",
      "============== Pattern 1046 ==============\n",
      "============== Pattern 1047 ==============\n",
      "============== Pattern 1048 ==============\n",
      "============== Pattern 1049 ==============\n",
      "============== Pattern 1050 ==============\n",
      "============== Pattern 1051 ==============\n",
      "============== Pattern 1052 ==============\n",
      "============== Pattern 1053 ==============\n",
      "============== Pattern 1054 ==============\n",
      "============== Pattern 1055 ==============\n",
      "============== Pattern 1056 ==============\n",
      "============== Pattern 1057 ==============\n",
      "============== Pattern 1058 ==============\n",
      "============== Pattern 1059 ==============\n",
      "============== Pattern 1060 ==============\n",
      "============== Pattern 1061 ==============\n",
      "============== Pattern 1062 ==============\n",
      "============== Pattern 1063 ==============\n",
      "============== Pattern 1064 ==============\n",
      "============== Pattern 1065 ==============\n",
      "============== Pattern 1066 ==============\n",
      "============== Pattern 1067 ==============\n",
      "============== Pattern 1068 ==============\n",
      "============== Pattern 1069 ==============\n",
      "============== Pattern 1070 ==============\n",
      "============== Pattern 1071 ==============\n",
      "============== Pattern 1072 ==============\n",
      "============== Pattern 1073 ==============\n",
      "============== Pattern 1074 ==============\n",
      "============== Pattern 1075 ==============\n",
      "============== Pattern 1076 ==============\n",
      "============== Pattern 1077 ==============\n",
      "============== Pattern 1078 ==============\n",
      "============== Pattern 1079 ==============\n",
      "============== Pattern 1080 ==============\n",
      "============== Pattern 1081 ==============\n",
      "============== Pattern 1082 ==============\n",
      "============== Pattern 1083 ==============\n",
      "============== Pattern 1084 ==============\n",
      "============== Pattern 1085 ==============\n",
      "============== Pattern 1086 ==============\n",
      "============== Pattern 1087 ==============\n",
      "============== Pattern 1088 ==============\n",
      "============== Pattern 1089 ==============\n",
      "============== Pattern 1090 ==============\n",
      "============== Pattern 1091 ==============\n",
      "============== Pattern 1092 ==============\n",
      "============== Pattern 1093 ==============\n",
      "============== Pattern 1094 ==============\n",
      "============== Pattern 1095 ==============\n",
      "============== Pattern 1096 ==============\n",
      "============== Pattern 1097 ==============\n",
      "============== Pattern 1098 ==============\n",
      "============== Pattern 1099 ==============\n",
      "============== Pattern 1100 ==============\n",
      "============== Pattern 1101 ==============\n",
      "============== Pattern 1102 ==============\n",
      "============== Pattern 1103 ==============\n",
      "============== Pattern 1104 ==============\n",
      "============== Pattern 1105 ==============\n",
      "============== Pattern 1106 ==============\n",
      "============== Pattern 1107 ==============\n",
      "============== Pattern 1108 ==============\n",
      "============== Pattern 1109 ==============\n",
      "============== Pattern 1110 ==============\n",
      "============== Pattern 1111 ==============\n",
      "============== Pattern 1112 ==============\n",
      "============== Pattern 1113 ==============\n",
      "============== Pattern 1114 ==============\n",
      "============== Pattern 1115 ==============\n",
      "============== Pattern 1116 ==============\n",
      "============== Pattern 1117 ==============\n",
      "============== Pattern 1118 ==============\n",
      "============== Pattern 1119 ==============\n",
      "============== Pattern 1120 ==============\n",
      "============== Pattern 1121 ==============\n",
      "============== Pattern 1122 ==============\n",
      "============== Pattern 1123 ==============\n",
      "============== Pattern 1124 ==============\n",
      "============== Pattern 1125 ==============\n",
      "============== Pattern 1126 ==============\n",
      "============== Pattern 1127 ==============\n",
      "============== Pattern 1128 ==============\n",
      "============== Pattern 1129 ==============\n",
      "============== Pattern 1130 ==============\n",
      "============== Pattern 1131 ==============\n",
      "============== Pattern 1132 ==============\n",
      "============== Pattern 1133 ==============\n",
      "============== Pattern 1134 ==============\n",
      "============== Pattern 1135 ==============\n",
      "============== Pattern 1136 ==============\n",
      "============== Pattern 1137 ==============\n",
      "============== Pattern 1138 ==============\n",
      "============== Pattern 1139 ==============\n",
      "============== Pattern 1140 ==============\n",
      "============== Pattern 1141 ==============\n",
      "============== Pattern 1142 ==============\n",
      "============== Pattern 1143 ==============\n",
      "============== Pattern 1144 ==============\n",
      "============== Pattern 1145 ==============\n",
      "============== Pattern 1146 ==============\n",
      "============== Pattern 1147 ==============\n",
      "============== Pattern 1148 ==============\n",
      "============== Pattern 1149 ==============\n",
      "============== Pattern 1150 ==============\n",
      "============== Pattern 1151 ==============\n",
      "============== Pattern 1152 ==============\n",
      "============== Pattern 1153 ==============\n",
      "============== Pattern 1154 ==============\n",
      "============== Pattern 1155 ==============\n",
      "============== Pattern 1156 ==============\n",
      "============== Pattern 1157 ==============\n",
      "============== Pattern 1158 ==============\n",
      "============== Pattern 1159 ==============\n",
      "============== Pattern 1160 ==============\n",
      "============== Pattern 1161 ==============\n",
      "============== Pattern 1162 ==============\n",
      "============== Pattern 1163 ==============\n",
      "============== Pattern 1164 ==============\n",
      "============== Pattern 1165 ==============\n",
      "============== Pattern 1166 ==============\n",
      "============== Pattern 1167 ==============\n",
      "============== Pattern 1168 ==============\n",
      "============== Pattern 1169 ==============\n",
      "============== Pattern 1170 ==============\n",
      "============== Pattern 1171 ==============\n",
      "============== Pattern 1172 ==============\n",
      "============== Pattern 1173 ==============\n",
      "============== Pattern 1174 ==============\n",
      "============== Pattern 1175 ==============\n",
      "============== Pattern 1176 ==============\n",
      "============== Pattern 1177 ==============\n",
      "============== Pattern 1178 ==============\n",
      "============== Pattern 1179 ==============\n",
      "============== Pattern 1180 ==============\n",
      "============== Pattern 1181 ==============\n",
      "============== Pattern 1182 ==============\n",
      "============== Pattern 1183 ==============\n",
      "============== Pattern 1184 ==============\n",
      "============== Pattern 1185 ==============\n",
      "============== Pattern 1186 ==============\n",
      "============== Pattern 1187 ==============\n",
      "============== Pattern 1188 ==============\n",
      "============== Pattern 1189 ==============\n",
      "============== Pattern 1190 ==============\n",
      "============== Pattern 1191 ==============\n",
      "============== Pattern 1192 ==============\n",
      "============== Pattern 1193 ==============\n",
      "============== Pattern 1194 ==============\n",
      "============== Pattern 1195 ==============\n",
      "============== Pattern 1196 ==============\n",
      "============== Pattern 1197 ==============\n",
      "============== Pattern 1198 ==============\n",
      "============== Pattern 1199 ==============\n",
      "============== Pattern 1200 ==============\n",
      "============== Pattern 1201 ==============\n",
      "============== Pattern 1202 ==============\n",
      "============== Pattern 1203 ==============\n",
      "============== Pattern 1204 ==============\n",
      "============== Pattern 1205 ==============\n",
      "============== Pattern 1206 ==============\n",
      "============== Pattern 1207 ==============\n",
      "============== Pattern 1208 ==============\n",
      "============== Pattern 1209 ==============\n",
      "============== Pattern 1210 ==============\n",
      "============== Pattern 1211 ==============\n",
      "============== Pattern 1212 ==============\n",
      "============== Pattern 1213 ==============\n",
      "============== Pattern 1214 ==============\n",
      "============== Pattern 1215 ==============\n",
      "============== Pattern 1216 ==============\n",
      "============== Pattern 1217 ==============\n",
      "============== Pattern 1218 ==============\n",
      "============== Pattern 1219 ==============\n",
      "============== Pattern 1220 ==============\n",
      "============== Pattern 1221 ==============\n",
      "============== Pattern 1222 ==============\n",
      "============== Pattern 1223 ==============\n",
      "============== Pattern 1224 ==============\n",
      "============== Pattern 1225 ==============\n",
      "============== Pattern 1226 ==============\n",
      "============== Pattern 1227 ==============\n",
      "============== Pattern 1228 ==============\n",
      "============== Pattern 1229 ==============\n",
      "============== Pattern 1230 ==============\n",
      "============== Pattern 1231 ==============\n",
      "============== Pattern 1232 ==============\n",
      "============== Pattern 1233 ==============\n",
      "============== Pattern 1234 ==============\n",
      "============== Pattern 1235 ==============\n",
      "============== Pattern 1236 ==============\n",
      "============== Pattern 1237 ==============\n",
      "============== Pattern 1238 ==============\n",
      "============== Pattern 1239 ==============\n",
      "============== Pattern 1240 ==============\n",
      "============== Pattern 1241 ==============\n",
      "============== Pattern 1242 ==============\n",
      "============== Pattern 1243 ==============\n",
      "============== Pattern 1244 ==============\n",
      "============== Pattern 1245 ==============\n",
      "============== Pattern 1246 ==============\n",
      "============== Pattern 1247 ==============\n",
      "============== Pattern 1248 ==============\n",
      "============== Pattern 1249 ==============\n",
      "============== Pattern 1250 ==============\n",
      "============== Pattern 1251 ==============\n",
      "============== Pattern 1252 ==============\n",
      "============== Pattern 1253 ==============\n",
      "============== Pattern 1254 ==============\n",
      "============== Pattern 1255 ==============\n",
      "============== Pattern 1256 ==============\n",
      "============== Pattern 1257 ==============\n",
      "============== Pattern 1258 ==============\n",
      "============== Pattern 1259 ==============\n",
      "============== Pattern 1260 ==============\n",
      "============== Pattern 1261 ==============\n",
      "============== Pattern 1262 ==============\n",
      "============== Pattern 1263 ==============\n",
      "============== Pattern 1264 ==============\n",
      "============== Pattern 1265 ==============\n",
      "============== Pattern 1266 ==============\n",
      "============== Pattern 1267 ==============\n",
      "============== Pattern 1268 ==============\n",
      "============== Pattern 1269 ==============\n",
      "============== Pattern 1270 ==============\n",
      "============== Pattern 1271 ==============\n",
      "============== Pattern 1272 ==============\n",
      "============== Pattern 1273 ==============\n",
      "============== Pattern 1274 ==============\n",
      "============== Pattern 1275 ==============\n",
      "============== Pattern 1276 ==============\n",
      "============== Pattern 1277 ==============\n",
      "============== Pattern 1278 ==============\n",
      "============== Pattern 1279 ==============\n",
      "============== Pattern 1280 ==============\n",
      "============== Pattern 1281 ==============\n",
      "============== Pattern 1282 ==============\n",
      "============== Pattern 1283 ==============\n",
      "============== Pattern 1284 ==============\n",
      "============== Pattern 1285 ==============\n",
      "============== Pattern 1286 ==============\n",
      "============== Pattern 1287 ==============\n",
      "============== Pattern 1288 ==============\n",
      "============== Pattern 1289 ==============\n",
      "============== Pattern 1290 ==============\n",
      "============== Pattern 1291 ==============\n",
      "============== Pattern 1292 ==============\n",
      "============== Pattern 1293 ==============\n",
      "============== Pattern 1294 ==============\n",
      "============== Pattern 1295 ==============\n",
      "============== Pattern 1296 ==============\n",
      "============== Pattern 1297 ==============\n",
      "============== Pattern 1298 ==============\n",
      "============== Pattern 1299 ==============\n",
      "============== Pattern 1300 ==============\n",
      "============== Pattern 1301 ==============\n",
      "============== Pattern 1302 ==============\n",
      "============== Pattern 1303 ==============\n",
      "============== Pattern 1304 ==============\n",
      "============== Pattern 1305 ==============\n",
      "============== Pattern 1306 ==============\n",
      "============== Pattern 1307 ==============\n",
      "============== Pattern 1308 ==============\n",
      "============== Pattern 1309 ==============\n",
      "============== Pattern 1310 ==============\n",
      "============== Pattern 1311 ==============\n",
      "============== Pattern 1312 ==============\n",
      "============== Pattern 1313 ==============\n",
      "============== Pattern 1314 ==============\n",
      "============== Pattern 1315 ==============\n",
      "============== Pattern 1316 ==============\n",
      "============== Pattern 1317 ==============\n",
      "============== Pattern 1318 ==============\n",
      "============== Pattern 1319 ==============\n",
      "============== Pattern 1320 ==============\n",
      "============== Pattern 1321 ==============\n",
      "============== Pattern 1322 ==============\n",
      "============== Pattern 1323 ==============\n",
      "============== Pattern 1324 ==============\n",
      "============== Pattern 1325 ==============\n",
      "============== Pattern 1326 ==============\n",
      "============== Pattern 1327 ==============\n",
      "============== Pattern 1328 ==============\n",
      "============== Pattern 1329 ==============\n",
      "============== Pattern 1330 ==============\n",
      "============== Pattern 1331 ==============\n",
      "============== Pattern 1332 ==============\n",
      "============== Pattern 1333 ==============\n",
      "============== Pattern 1334 ==============\n",
      "============== Pattern 1335 ==============\n",
      "============== Pattern 1336 ==============\n",
      "============== Pattern 1337 ==============\n",
      "============== Pattern 1338 ==============\n",
      "============== Pattern 1339 ==============\n",
      "============== Pattern 1340 ==============\n",
      "============== Pattern 1341 ==============\n",
      "============== Pattern 1342 ==============\n",
      "============== Pattern 1343 ==============\n",
      "============== Pattern 1344 ==============\n",
      "============== Pattern 1345 ==============\n",
      "============== Pattern 1346 ==============\n",
      "============== Pattern 1347 ==============\n",
      "============== Pattern 1348 ==============\n",
      "============== Pattern 1349 ==============\n",
      "============== Pattern 1350 ==============\n",
      "============== Pattern 1351 ==============\n",
      "============== Pattern 1352 ==============\n",
      "============== Pattern 1353 ==============\n",
      "============== Pattern 1354 ==============\n",
      "============== Pattern 1355 ==============\n",
      "============== Pattern 1356 ==============\n",
      "============== Pattern 1357 ==============\n",
      "============== Pattern 1358 ==============\n",
      "============== Pattern 1359 ==============\n",
      "============== Pattern 1360 ==============\n",
      "============== Pattern 1361 ==============\n",
      "============== Pattern 1362 ==============\n",
      "============== Pattern 1363 ==============\n",
      "============== Pattern 1364 ==============\n",
      "============== Pattern 1365 ==============\n",
      "============== Pattern 1366 ==============\n",
      "============== Pattern 1367 ==============\n",
      "============== Pattern 1368 ==============\n",
      "============== Pattern 1369 ==============\n",
      "============== Pattern 1370 ==============\n",
      "============== Pattern 1371 ==============\n",
      "============== Pattern 1372 ==============\n",
      "============== Pattern 1373 ==============\n",
      "============== Pattern 1374 ==============\n",
      "============== Pattern 1375 ==============\n",
      "============== Pattern 1376 ==============\n",
      "============== Pattern 1377 ==============\n",
      "============== Pattern 1378 ==============\n",
      "============== Pattern 1379 ==============\n",
      "============== Pattern 1380 ==============\n",
      "============== Pattern 1381 ==============\n",
      "============== Pattern 1382 ==============\n",
      "============== Pattern 1383 ==============\n",
      "============== Pattern 1384 ==============\n",
      "============== Pattern 1385 ==============\n",
      "============== Pattern 1386 ==============\n",
      "============== Pattern 1387 ==============\n",
      "============== Pattern 1388 ==============\n",
      "============== Pattern 1389 ==============\n",
      "============== Pattern 1390 ==============\n",
      "============== Pattern 1391 ==============\n",
      "============== Pattern 1392 ==============\n",
      "============== Pattern 1393 ==============\n",
      "============== Pattern 1394 ==============\n",
      "============== Pattern 1395 ==============\n",
      "============== Pattern 1396 ==============\n",
      "============== Pattern 1397 ==============\n",
      "============== Pattern 1398 ==============\n",
      "============== Pattern 1399 ==============\n",
      "============== Pattern 1400 ==============\n",
      "============== Pattern 1401 ==============\n",
      "============== Pattern 1402 ==============\n",
      "============== Pattern 1403 ==============\n",
      "============== Pattern 1404 ==============\n",
      "============== Pattern 1405 ==============\n",
      "============== Pattern 1406 ==============\n",
      "============== Pattern 1407 ==============\n",
      "============== Pattern 1408 ==============\n",
      "============== Pattern 1409 ==============\n",
      "============== Pattern 1410 ==============\n",
      "============== Pattern 1411 ==============\n",
      "============== Pattern 1412 ==============\n",
      "============== Pattern 1413 ==============\n",
      "============== Pattern 1414 ==============\n",
      "============== Pattern 1415 ==============\n",
      "============== Pattern 1416 ==============\n",
      "============== Pattern 1417 ==============\n",
      "============== Pattern 1418 ==============\n",
      "============== Pattern 1419 ==============\n",
      "============== Pattern 1420 ==============\n",
      "============== Pattern 1421 ==============\n",
      "============== Pattern 1422 ==============\n",
      "============== Pattern 1423 ==============\n",
      "============== Pattern 1424 ==============\n",
      "============== Pattern 1425 ==============\n",
      "============== Pattern 1426 ==============\n",
      "============== Pattern 1427 ==============\n",
      "============== Pattern 1428 ==============\n",
      "============== Pattern 1429 ==============\n",
      "============== Pattern 1430 ==============\n",
      "============== Pattern 1431 ==============\n",
      "============== Pattern 1432 ==============\n",
      "============== Pattern 1433 ==============\n",
      "============== Pattern 1434 ==============\n",
      "============== Pattern 1435 ==============\n",
      "============== Pattern 1436 ==============\n",
      "============== Pattern 1437 ==============\n",
      "============== Pattern 1438 ==============\n",
      "============== Pattern 1439 ==============\n",
      "============== Pattern 1440 ==============\n",
      "============== Pattern 1441 ==============\n",
      "============== Pattern 1442 ==============\n",
      "============== Pattern 1443 ==============\n",
      "============== Pattern 1444 ==============\n",
      "============== Pattern 1445 ==============\n",
      "============== Pattern 1446 ==============\n",
      "============== Pattern 1447 ==============\n",
      "============== Pattern 1448 ==============\n",
      "============== Pattern 1449 ==============\n",
      "============== Pattern 1450 ==============\n",
      "============== Pattern 1451 ==============\n",
      "============== Pattern 1452 ==============\n",
      "============== Pattern 1453 ==============\n",
      "============== Pattern 1454 ==============\n",
      "============== Pattern 1455 ==============\n",
      "============== Pattern 1456 ==============\n",
      "============== Pattern 1457 ==============\n",
      "============== Pattern 1458 ==============\n",
      "============== Pattern 1459 ==============\n",
      "============== Pattern 1460 ==============\n",
      "============== Pattern 1461 ==============\n",
      "============== Pattern 1462 ==============\n",
      "============== Pattern 1463 ==============\n",
      "============== Pattern 1464 ==============\n",
      "============== Pattern 1465 ==============\n",
      "============== Pattern 1466 ==============\n",
      "============== Pattern 1467 ==============\n",
      "============== Pattern 1468 ==============\n",
      "============== Pattern 1469 ==============\n",
      "============== Pattern 1470 ==============\n",
      "============== Pattern 1471 ==============\n",
      "============== Pattern 1472 ==============\n",
      "============== Pattern 1473 ==============\n",
      "============== Pattern 1474 ==============\n",
      "============== Pattern 1475 ==============\n",
      "============== Pattern 1476 ==============\n",
      "============== Pattern 1477 ==============\n",
      "============== Pattern 1478 ==============\n",
      "============== Pattern 1479 ==============\n",
      "============== Pattern 1480 ==============\n",
      "============== Pattern 1481 ==============\n",
      "============== Pattern 1482 ==============\n",
      "============== Pattern 1483 ==============\n",
      "============== Pattern 1484 ==============\n",
      "============== Pattern 1485 ==============\n",
      "============== Pattern 1486 ==============\n",
      "============== Pattern 1487 ==============\n",
      "============== Pattern 1488 ==============\n",
      "============== Pattern 1489 ==============\n",
      "============== Pattern 1490 ==============\n",
      "============== Pattern 1491 ==============\n",
      "============== Pattern 1492 ==============\n",
      "============== Pattern 1493 ==============\n",
      "============== Pattern 1494 ==============\n",
      "============== Pattern 1495 ==============\n",
      "============== Pattern 1496 ==============\n",
      "============== Pattern 1497 ==============\n",
      "============== Pattern 1498 ==============\n",
      "============== Pattern 1499 ==============\n",
      "============== Pattern 1500 ==============\n",
      "============== Pattern 1501 ==============\n",
      "============== Pattern 1502 ==============\n",
      "============== Pattern 1503 ==============\n",
      "============== Pattern 1504 ==============\n",
      "============== Pattern 1505 ==============\n",
      "============== Pattern 1506 ==============\n",
      "============== Pattern 1507 ==============\n",
      "============== Pattern 1508 ==============\n",
      "============== Pattern 1509 ==============\n",
      "============== Pattern 1510 ==============\n",
      "============== Pattern 1511 ==============\n",
      "============== Pattern 1512 ==============\n",
      "============== Pattern 1513 ==============\n",
      "============== Pattern 1514 ==============\n",
      "============== Pattern 1515 ==============\n",
      "============== Pattern 1516 ==============\n",
      "============== Pattern 1517 ==============\n",
      "============== Pattern 1518 ==============\n",
      "============== Pattern 1519 ==============\n",
      "============== Pattern 1520 ==============\n",
      "============== Pattern 1521 ==============\n",
      "============== Pattern 1522 ==============\n",
      "============== Pattern 1523 ==============\n",
      "============== Pattern 1524 ==============\n",
      "============== Pattern 1525 ==============\n",
      "============== Pattern 1526 ==============\n",
      "============== Pattern 1527 ==============\n",
      "============== Pattern 1528 ==============\n",
      "============== Pattern 1529 ==============\n",
      "============== Pattern 1530 ==============\n",
      "============== Pattern 1531 ==============\n",
      "============== Pattern 1532 ==============\n",
      "============== Pattern 1533 ==============\n",
      "============== Pattern 1534 ==============\n",
      "============== Pattern 1535 ==============\n",
      "============== Pattern 1536 ==============\n",
      "============== Pattern 1537 ==============\n",
      "============== Pattern 1538 ==============\n",
      "============== Pattern 1539 ==============\n",
      "============== Pattern 1540 ==============\n",
      "============== Pattern 1541 ==============\n",
      "============== Pattern 1542 ==============\n",
      "============== Pattern 1543 ==============\n",
      "============== Pattern 1544 ==============\n",
      "============== Pattern 1545 ==============\n",
      "============== Pattern 1546 ==============\n",
      "============== Pattern 1547 ==============\n",
      "============== Pattern 1548 ==============\n",
      "============== Pattern 1549 ==============\n",
      "============== Pattern 1550 ==============\n",
      "============== Pattern 1551 ==============\n",
      "============== Pattern 1552 ==============\n",
      "============== Pattern 1553 ==============\n",
      "============== Pattern 1554 ==============\n",
      "============== Pattern 1555 ==============\n",
      "============== Pattern 1556 ==============\n",
      "============== Pattern 1557 ==============\n",
      "============== Pattern 1558 ==============\n",
      "============== Pattern 1559 ==============\n",
      "============== Pattern 1560 ==============\n",
      "============== Pattern 1561 ==============\n",
      "============== Pattern 1562 ==============\n",
      "============== Pattern 1563 ==============\n",
      "============== Pattern 1564 ==============\n",
      "============== Pattern 1565 ==============\n",
      "============== Pattern 1566 ==============\n",
      "============== Pattern 1567 ==============\n",
      "============== Pattern 1568 ==============\n",
      "============== Pattern 1569 ==============\n",
      "============== Pattern 1570 ==============\n",
      "============== Pattern 1571 ==============\n",
      "============== Pattern 1572 ==============\n",
      "============== Pattern 1573 ==============\n",
      "============== Pattern 1574 ==============\n",
      "============== Pattern 1575 ==============\n",
      "============== Pattern 1576 ==============\n",
      "============== Pattern 1577 ==============\n",
      "============== Pattern 1578 ==============\n",
      "============== Pattern 1579 ==============\n",
      "============== Pattern 1580 ==============\n",
      "============== Pattern 1581 ==============\n",
      "============== Pattern 1582 ==============\n",
      "============== Pattern 1583 ==============\n",
      "============== Pattern 1584 ==============\n",
      "============== Pattern 1585 ==============\n",
      "============== Pattern 1586 ==============\n",
      "============== Pattern 1587 ==============\n",
      "============== Pattern 1588 ==============\n",
      "============== Pattern 1589 ==============\n",
      "============== Pattern 1590 ==============\n",
      "============== Pattern 1591 ==============\n",
      "============== Pattern 1592 ==============\n",
      "============== Pattern 1593 ==============\n",
      "============== Pattern 1594 ==============\n",
      "============== Pattern 1595 ==============\n",
      "============== Pattern 1596 ==============\n",
      "============== Pattern 1597 ==============\n",
      "============== Pattern 1598 ==============\n",
      "============== Pattern 1599 ==============\n",
      "============== Pattern 1600 ==============\n",
      "============== Pattern 1601 ==============\n",
      "============== Pattern 1602 ==============\n",
      "============== Pattern 1603 ==============\n",
      "============== Pattern 1604 ==============\n",
      "============== Pattern 1605 ==============\n",
      "============== Pattern 1606 ==============\n",
      "============== Pattern 1607 ==============\n",
      "============== Pattern 1608 ==============\n",
      "============== Pattern 1609 ==============\n",
      "============== Pattern 1610 ==============\n",
      "============== Pattern 1611 ==============\n",
      "============== Pattern 1612 ==============\n",
      "============== Pattern 1613 ==============\n",
      "============== Pattern 1614 ==============\n",
      "============== Pattern 1615 ==============\n",
      "============== Pattern 1616 ==============\n",
      "============== Pattern 1617 ==============\n",
      "============== Pattern 1618 ==============\n",
      "============== Pattern 1619 ==============\n",
      "============== Pattern 1620 ==============\n",
      "============== Pattern 1621 ==============\n",
      "============== Pattern 1622 ==============\n",
      "============== Pattern 1623 ==============\n",
      "============== Pattern 1624 ==============\n",
      "============== Pattern 1625 ==============\n",
      "============== Pattern 1626 ==============\n",
      "============== Pattern 1627 ==============\n",
      "============== Pattern 1628 ==============\n",
      "============== Pattern 1629 ==============\n",
      "============== Pattern 1630 ==============\n",
      "============== Pattern 1631 ==============\n",
      "============== Pattern 1632 ==============\n",
      "============== Pattern 1633 ==============\n",
      "============== Pattern 1634 ==============\n",
      "============== Pattern 1635 ==============\n",
      "============== Pattern 1636 ==============\n",
      "============== Pattern 1637 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1638 ==============\n",
      "============== Pattern 1639 ==============\n",
      "============== Pattern 1640 ==============\n",
      "============== Pattern 1641 ==============\n",
      "============== Pattern 1642 ==============\n",
      "============== Pattern 1643 ==============\n",
      "============== Pattern 1644 ==============\n",
      "============== Pattern 1645 ==============\n",
      "============== Pattern 1646 ==============\n",
      "============== Pattern 1647 ==============\n",
      "============== Pattern 1648 ==============\n",
      "============== Pattern 1649 ==============\n",
      "============== Pattern 1650 ==============\n",
      "============== Pattern 1651 ==============\n",
      "============== Pattern 1652 ==============\n",
      "============== Pattern 1653 ==============\n",
      "============== Pattern 1654 ==============\n",
      "============== Pattern 1655 ==============\n",
      "============== Pattern 1656 ==============\n",
      "============== Pattern 1657 ==============\n",
      "============== Pattern 1658 ==============\n",
      "============== Pattern 1659 ==============\n",
      "============== Pattern 1660 ==============\n",
      "============== Pattern 1661 ==============\n",
      "============== Pattern 1662 ==============\n",
      "============== Pattern 1663 ==============\n",
      "============== Pattern 1664 ==============\n",
      "============== Pattern 1665 ==============\n",
      "============== Pattern 1666 ==============\n",
      "============== Pattern 1667 ==============\n",
      "============== Pattern 1668 ==============\n",
      "============== Pattern 1669 ==============\n",
      "============== Pattern 1670 ==============\n",
      "============== Pattern 1671 ==============\n",
      "============== Pattern 1672 ==============\n",
      "============== Pattern 1673 ==============\n",
      "============== Pattern 1674 ==============\n",
      "============== Pattern 1675 ==============\n",
      "============== Pattern 1676 ==============\n",
      "============== Pattern 1677 ==============\n",
      "============== Pattern 1678 ==============\n",
      "============== Pattern 1679 ==============\n",
      "============== Pattern 1680 ==============\n",
      "============== Pattern 1681 ==============\n",
      "============== Pattern 1682 ==============\n",
      "============== Pattern 1683 ==============\n",
      "============== Pattern 1684 ==============\n",
      "============== Pattern 1685 ==============\n",
      "============== Pattern 1686 ==============\n",
      "============== Pattern 1687 ==============\n",
      "============== Pattern 1688 ==============\n",
      "============== Pattern 1689 ==============\n",
      "============== Pattern 1690 ==============\n",
      "============== Pattern 1691 ==============\n",
      "============== Pattern 1692 ==============\n",
      "============== Pattern 1693 ==============\n",
      "============== Pattern 1694 ==============\n",
      "============== Pattern 1695 ==============\n",
      "============== Pattern 1696 ==============\n",
      "============== Pattern 1697 ==============\n",
      "============== Pattern 1698 ==============\n",
      "============== Pattern 1699 ==============\n",
      "============== Pattern 1700 ==============\n",
      "============== Pattern 1701 ==============\n",
      "============== Pattern 1702 ==============\n",
      "============== Pattern 1703 ==============\n",
      "============== Pattern 1704 ==============\n",
      "============== Pattern 1705 ==============\n",
      "============== Pattern 1706 ==============\n",
      "============== Pattern 1707 ==============\n",
      "============== Pattern 1708 ==============\n",
      "============== Pattern 1709 ==============\n",
      "============== Pattern 1710 ==============\n",
      "============== Pattern 1711 ==============\n",
      "============== Pattern 1712 ==============\n",
      "============== Pattern 1713 ==============\n",
      "============== Pattern 1714 ==============\n",
      "============== Pattern 1715 ==============\n",
      "============== Pattern 1716 ==============\n",
      "============== Pattern 1717 ==============\n",
      "============== Pattern 1718 ==============\n",
      "============== Pattern 1719 ==============\n",
      "============== Pattern 1720 ==============\n",
      "============== Pattern 1721 ==============\n",
      "============== Pattern 1722 ==============\n",
      "============== Pattern 1723 ==============\n",
      "============== Pattern 1724 ==============\n",
      "============== Pattern 1725 ==============\n",
      "============== Pattern 1726 ==============\n",
      "============== Pattern 1727 ==============\n",
      "============== Pattern 1728 ==============\n",
      "============== Pattern 1729 ==============\n",
      "============== Pattern 1730 ==============\n",
      "============== Pattern 1731 ==============\n",
      "============== Pattern 1732 ==============\n",
      "============== Pattern 1733 ==============\n",
      "============== Pattern 1734 ==============\n",
      "============== Pattern 1735 ==============\n",
      "============== Pattern 1736 ==============\n",
      "============== Pattern 1737 ==============\n",
      "============== Pattern 1738 ==============\n",
      "============== Pattern 1739 ==============\n",
      "============== Pattern 1740 ==============\n",
      "============== Pattern 1741 ==============\n",
      "============== Pattern 1742 ==============\n",
      "============== Pattern 1743 ==============\n",
      "============== Pattern 1744 ==============\n",
      "============== Pattern 1745 ==============\n",
      "============== Pattern 1746 ==============\n",
      "============== Pattern 1747 ==============\n",
      "============== Pattern 1748 ==============\n",
      "============== Pattern 1749 ==============\n",
      "============== Pattern 1750 ==============\n",
      "============== Pattern 1751 ==============\n",
      "============== Pattern 1752 ==============\n",
      "============== Pattern 1753 ==============\n",
      "============== Pattern 1754 ==============\n",
      "============== Pattern 1755 ==============\n",
      "============== Pattern 1756 ==============\n",
      "============== Pattern 1757 ==============\n",
      "============== Pattern 1758 ==============\n",
      "============== Pattern 1759 ==============\n",
      "============== Pattern 1760 ==============\n",
      "============== Pattern 1761 ==============\n",
      "============== Pattern 1762 ==============\n",
      "============== Pattern 1763 ==============\n",
      "============== Pattern 1764 ==============\n",
      "============== Pattern 1765 ==============\n",
      "============== Pattern 1766 ==============\n",
      "============== Pattern 1767 ==============\n",
      "============== Pattern 1768 ==============\n",
      "============== Pattern 1769 ==============\n",
      "============== Pattern 1770 ==============\n",
      "============== Pattern 1771 ==============\n",
      "============== Pattern 1772 ==============\n",
      "============== Pattern 1773 ==============\n",
      "============== Pattern 1774 ==============\n",
      "============== Pattern 1775 ==============\n",
      "============== Pattern 1776 ==============\n",
      "============== Pattern 1777 ==============\n",
      "============== Pattern 1778 ==============\n",
      "============== Pattern 1779 ==============\n",
      "============== Pattern 1780 ==============\n",
      "============== Pattern 1781 ==============\n",
      "============== Pattern 1782 ==============\n",
      "============== Pattern 1783 ==============\n",
      "============== Pattern 1784 ==============\n",
      "============== Pattern 1785 ==============\n",
      "============== Pattern 1786 ==============\n",
      "============== Pattern 1787 ==============\n",
      "============== Pattern 1788 ==============\n",
      "============== Pattern 1789 ==============\n",
      "============== Pattern 1790 ==============\n",
      "============== Pattern 1791 ==============\n",
      "============== Pattern 1792 ==============\n",
      "============== Pattern 1793 ==============\n",
      "============== Pattern 1794 ==============\n",
      "============== Pattern 1795 ==============\n",
      "============== Pattern 1796 ==============\n",
      "============== Pattern 1797 ==============\n",
      "============== Pattern 1798 ==============\n",
      "============== Pattern 1799 ==============\n",
      "============== Pattern 1800 ==============\n",
      "============== Pattern 1801 ==============\n",
      "============== Pattern 1802 ==============\n",
      "============== Pattern 1803 ==============\n",
      "============== Pattern 1804 ==============\n",
      "============== Pattern 1805 ==============\n",
      "============== Pattern 1806 ==============\n",
      "============== Pattern 1807 ==============\n",
      "============== Pattern 1808 ==============\n",
      "============== Pattern 1809 ==============\n",
      "============== Pattern 1810 ==============\n",
      "============== Pattern 1811 ==============\n",
      "============== Pattern 1812 ==============\n",
      "============== Pattern 1813 ==============\n",
      "============== Pattern 1814 ==============\n",
      "============== Pattern 1815 ==============\n",
      "============== Pattern 1816 ==============\n",
      "============== Pattern 1817 ==============\n",
      "============== Pattern 1818 ==============\n",
      "============== Pattern 1819 ==============\n",
      "============== Pattern 1820 ==============\n",
      "============== Pattern 1821 ==============\n",
      "============== Pattern 1822 ==============\n",
      "============== Pattern 1823 ==============\n",
      "============== Pattern 1824 ==============\n",
      "============== Pattern 1825 ==============\n",
      "============== Pattern 1826 ==============\n",
      "============== Pattern 1827 ==============\n",
      "============== Pattern 1828 ==============\n",
      "============== Pattern 1829 ==============\n",
      "============== Pattern 1830 ==============\n",
      "============== Pattern 1831 ==============\n",
      "============== Pattern 1832 ==============\n",
      "============== Pattern 1833 ==============\n",
      "============== Pattern 1834 ==============\n",
      "============== Pattern 1835 ==============\n",
      "============== Pattern 1836 ==============\n",
      "============== Pattern 1837 ==============\n",
      "============== Pattern 1838 ==============\n",
      "============== Pattern 1839 ==============\n",
      "============== Pattern 1840 ==============\n",
      "============== Pattern 1841 ==============\n",
      "============== Pattern 1842 ==============\n",
      "============== Pattern 1843 ==============\n",
      "============== Pattern 1844 ==============\n",
      "============== Pattern 1845 ==============\n",
      "============== Pattern 1846 ==============\n",
      "============== Pattern 1847 ==============\n",
      "============== Pattern 1848 ==============\n",
      "============== Pattern 1849 ==============\n",
      "============== Pattern 1850 ==============\n",
      "============== Pattern 1851 ==============\n",
      "============== Pattern 1852 ==============\n",
      "============== Pattern 1853 ==============\n",
      "============== Pattern 1854 ==============\n",
      "============== Pattern 1855 ==============\n",
      "============== Pattern 1856 ==============\n",
      "============== Pattern 1857 ==============\n",
      "============== Pattern 1858 ==============\n",
      "============== Pattern 1859 ==============\n",
      "============== Pattern 1860 ==============\n",
      "============== Pattern 1861 ==============\n",
      "============== Pattern 1862 ==============\n",
      "============== Pattern 1863 ==============\n",
      "============== Pattern 1864 ==============\n",
      "============== Pattern 1865 ==============\n",
      "============== Pattern 1866 ==============\n",
      "============== Pattern 1867 ==============\n",
      "============== Pattern 1868 ==============\n",
      "============== Pattern 1869 ==============\n",
      "============== Pattern 1870 ==============\n",
      "============== Pattern 1871 ==============\n",
      "============== Pattern 1872 ==============\n",
      "============== Pattern 1873 ==============\n",
      "============== Pattern 1874 ==============\n",
      "============== Pattern 1875 ==============\n",
      "============== Pattern 1876 ==============\n",
      "============== Pattern 1877 ==============\n",
      "============== Pattern 1878 ==============\n",
      "============== Pattern 1879 ==============\n",
      "============== Pattern 1880 ==============\n",
      "============== Pattern 1881 ==============\n",
      "============== Pattern 1882 ==============\n",
      "============== Pattern 1883 ==============\n",
      "============== Pattern 1884 ==============\n",
      "============== Pattern 1885 ==============\n",
      "============== Pattern 1886 ==============\n",
      "============== Pattern 1887 ==============\n",
      "============== Pattern 1888 ==============\n",
      "============== Pattern 1889 ==============\n",
      "============== Pattern 1890 ==============\n",
      "============== Pattern 1891 ==============\n",
      "============== Pattern 1892 ==============\n",
      "============== Pattern 1893 ==============\n",
      "============== Pattern 1894 ==============\n",
      "============== Pattern 1895 ==============\n",
      "============== Pattern 1896 ==============\n",
      "============== Pattern 1897 ==============\n",
      "============== Pattern 1898 ==============\n",
      "============== Pattern 1899 ==============\n",
      "============== Pattern 1900 ==============\n",
      "============== Pattern 1901 ==============\n",
      "============== Pattern 1902 ==============\n",
      "============== Pattern 1903 ==============\n",
      "============== Pattern 1904 ==============\n",
      "============== Pattern 1905 ==============\n",
      "============== Pattern 1906 ==============\n",
      "============== Pattern 1907 ==============\n",
      "============== Pattern 1908 ==============\n",
      "============== Pattern 1909 ==============\n",
      "============== Pattern 1910 ==============\n",
      "============== Pattern 1911 ==============\n",
      "============== Pattern 1912 ==============\n",
      "============== Pattern 1913 ==============\n",
      "============== Pattern 1914 ==============\n",
      "============== Pattern 1915 ==============\n",
      "============== Pattern 1916 ==============\n",
      "============== Pattern 1917 ==============\n",
      "============== Pattern 1918 ==============\n",
      "============== Pattern 1919 ==============\n",
      "============== Pattern 1920 ==============\n",
      "============== Pattern 1921 ==============\n",
      "============== Pattern 1922 ==============\n",
      "============== Pattern 1923 ==============\n",
      "============== Pattern 1924 ==============\n",
      "============== Pattern 1925 ==============\n",
      "============== Pattern 1926 ==============\n",
      "============== Pattern 1927 ==============\n",
      "============== Pattern 1928 ==============\n",
      "============== Pattern 1929 ==============\n",
      "============== Pattern 1930 ==============\n",
      "============== Pattern 1931 ==============\n",
      "============== Pattern 1932 ==============\n",
      "============== Pattern 1933 ==============\n",
      "============== Pattern 1934 ==============\n",
      "============== Pattern 1935 ==============\n",
      "============== Pattern 1936 ==============\n",
      "============== Pattern 1937 ==============\n",
      "============== Pattern 1938 ==============\n",
      "============== Pattern 1939 ==============\n",
      "============== Pattern 1940 ==============\n",
      "============== Pattern 1941 ==============\n",
      "============== Pattern 1942 ==============\n",
      "============== Pattern 1943 ==============\n",
      "============== Pattern 1944 ==============\n",
      "============== Pattern 1945 ==============\n",
      "============== Pattern 1946 ==============\n",
      "============== Pattern 1947 ==============\n",
      "============== Pattern 1948 ==============\n",
      "============== Pattern 1949 ==============\n",
      "============== Pattern 1950 ==============\n",
      "============== Pattern 1951 ==============\n",
      "============== Pattern 1952 ==============\n",
      "============== Pattern 1953 ==============\n",
      "============== Pattern 1954 ==============\n",
      "============== Pattern 1955 ==============\n",
      "============== Pattern 1956 ==============\n",
      "============== Pattern 1957 ==============\n",
      "============== Pattern 1958 ==============\n",
      "============== Pattern 1959 ==============\n",
      "============== Pattern 1960 ==============\n",
      "============== Pattern 1961 ==============\n",
      "============== Pattern 1962 ==============\n",
      "============== Pattern 1963 ==============\n",
      "============== Pattern 1964 ==============\n",
      "============== Pattern 1965 ==============\n",
      "============== Pattern 1966 ==============\n",
      "============== Pattern 1967 ==============\n",
      "============== Pattern 1968 ==============\n",
      "============== Pattern 1969 ==============\n",
      "============== Pattern 1970 ==============\n",
      "============== Pattern 1971 ==============\n",
      "============== Pattern 1972 ==============\n",
      "============== Pattern 1973 ==============\n",
      "============== Pattern 1974 ==============\n",
      "============== Pattern 1975 ==============\n",
      "============== Pattern 1976 ==============\n",
      "============== Pattern 1977 ==============\n",
      "============== Pattern 1978 ==============\n",
      "============== Pattern 1979 ==============\n",
      "============== Pattern 1980 ==============\n",
      "============== Pattern 1981 ==============\n",
      "============== Pattern 1982 ==============\n",
      "============== Pattern 1983 ==============\n",
      "============== Pattern 1984 ==============\n",
      "============== Pattern 1985 ==============\n",
      "============== Pattern 1986 ==============\n",
      "============== Pattern 1987 ==============\n",
      "============== Pattern 1988 ==============\n",
      "============== Pattern 1989 ==============\n",
      "============== Pattern 1990 ==============\n",
      "============== Pattern 1991 ==============\n",
      "============== Pattern 1992 ==============\n",
      "============== Pattern 1993 ==============\n",
      "============== Pattern 1994 ==============\n",
      "============== Pattern 1995 ==============\n",
      "============== Pattern 1996 ==============\n",
      "============== Pattern 1997 ==============\n",
      "============== Pattern 1998 ==============\n",
      "============== Pattern 1999 ==============\n",
      "============== Pattern 2000 ==============\n",
      "============== Pattern 2001 ==============\n",
      "============== Pattern 2002 ==============\n",
      "============== Pattern 2003 ==============\n",
      "============== Pattern 2004 ==============\n",
      "============== Pattern 2005 ==============\n",
      "============== Pattern 2006 ==============\n",
      "============== Pattern 2007 ==============\n",
      "============== Pattern 2008 ==============\n",
      "============== Pattern 2009 ==============\n",
      "============== Pattern 2010 ==============\n",
      "============== Pattern 2011 ==============\n",
      "============== Pattern 2012 ==============\n",
      "============== Pattern 2013 ==============\n",
      "============== Pattern 2014 ==============\n",
      "============== Pattern 2015 ==============\n",
      "============== Pattern 2016 ==============\n",
      "============== Pattern 2017 ==============\n",
      "============== Pattern 2018 ==============\n",
      "============== Pattern 2019 ==============\n",
      "============== Pattern 2020 ==============\n",
      "============== Pattern 2021 ==============\n",
      "============== Pattern 2022 ==============\n",
      "============== Pattern 2023 ==============\n",
      "============== Pattern 2024 ==============\n",
      "============== Pattern 2025 ==============\n",
      "============== Pattern 2026 ==============\n",
      "============== Pattern 2027 ==============\n",
      "============== Pattern 2028 ==============\n",
      "============== Pattern 2029 ==============\n",
      "============== Pattern 2030 ==============\n",
      "============== Pattern 2031 ==============\n",
      "============== Pattern 2032 ==============\n",
      "============== Pattern 2033 ==============\n",
      "============== Pattern 2034 ==============\n",
      "============== Pattern 2035 ==============\n",
      "============== Pattern 2036 ==============\n",
      "============== Pattern 2037 ==============\n",
      "============== Pattern 2038 ==============\n",
      "============== Pattern 2039 ==============\n",
      "============== Pattern 2040 ==============\n",
      "============== Pattern 2041 ==============\n",
      "============== Pattern 2042 ==============\n",
      "============== Pattern 2043 ==============\n",
      "============== Pattern 2044 ==============\n",
      "============== Pattern 2045 ==============\n",
      "============== Pattern 2046 ==============\n",
      "============== Pattern 2047 ==============\n",
      "============== Pattern 2048 ==============\n",
      "============== Pattern 2049 ==============\n",
      "============== Pattern 2050 ==============\n",
      "============== Pattern 2051 ==============\n",
      "============== Pattern 2052 ==============\n",
      "============== Pattern 2053 ==============\n",
      "============== Pattern 2054 ==============\n",
      "============== Pattern 2055 ==============\n",
      "============== Pattern 2056 ==============\n",
      "============== Pattern 2057 ==============\n",
      "============== Pattern 2058 ==============\n",
      "============== Pattern 2059 ==============\n",
      "============== Pattern 2060 ==============\n",
      "============== Pattern 2061 ==============\n",
      "============== Pattern 2062 ==============\n",
      "============== Pattern 2063 ==============\n",
      "============== Pattern 2064 ==============\n",
      "============== Pattern 2065 ==============\n",
      "============== Pattern 2066 ==============\n",
      "============== Pattern 2067 ==============\n",
      "============== Pattern 2068 ==============\n",
      "============== Pattern 2069 ==============\n",
      "============== Pattern 2070 ==============\n",
      "============== Pattern 2071 ==============\n",
      "============== Pattern 2072 ==============\n",
      "============== Pattern 2073 ==============\n",
      "============== Pattern 2074 ==============\n",
      "============== Pattern 2075 ==============\n",
      "============== Pattern 2076 ==============\n",
      "============== Pattern 2077 ==============\n",
      "============== Pattern 2078 ==============\n",
      "============== Pattern 2079 ==============\n",
      "============== Pattern 2080 ==============\n",
      "============== Pattern 2081 ==============\n",
      "============== Pattern 2082 ==============\n",
      "============== Pattern 2083 ==============\n",
      "============== Pattern 2084 ==============\n",
      "============== Pattern 2085 ==============\n",
      "============== Pattern 2086 ==============\n",
      "============== Pattern 2087 ==============\n",
      "============== Pattern 2088 ==============\n",
      "============== Pattern 2089 ==============\n",
      "============== Pattern 2090 ==============\n",
      "============== Pattern 2091 ==============\n",
      "============== Pattern 2092 ==============\n",
      "============== Pattern 2093 ==============\n",
      "============== Pattern 2094 ==============\n",
      "============== Pattern 2095 ==============\n",
      "============== Pattern 2096 ==============\n",
      "============== Pattern 2097 ==============\n",
      "============== Pattern 2098 ==============\n",
      "============== Pattern 2099 ==============\n",
      "============== Pattern 2100 ==============\n",
      "============== Pattern 2101 ==============\n",
      "============== Pattern 2102 ==============\n",
      "============== Pattern 2103 ==============\n",
      "============== Pattern 2104 ==============\n",
      "============== Pattern 2105 ==============\n",
      "============== Pattern 2106 ==============\n",
      "============== Pattern 2107 ==============\n",
      "============== Pattern 2108 ==============\n",
      "============== Pattern 2109 ==============\n",
      "============== Pattern 2110 ==============\n",
      "============== Pattern 2111 ==============\n",
      "============== Pattern 2112 ==============\n",
      "============== Pattern 2113 ==============\n",
      "============== Pattern 2114 ==============\n",
      "============== Pattern 2115 ==============\n",
      "============== Pattern 2116 ==============\n",
      "============== Pattern 2117 ==============\n",
      "============== Pattern 2118 ==============\n",
      "============== Pattern 2119 ==============\n",
      "============== Pattern 2120 ==============\n",
      "============== Pattern 2121 ==============\n",
      "============== Pattern 2122 ==============\n",
      "============== Pattern 2123 ==============\n",
      "============== Pattern 2124 ==============\n",
      "============== Pattern 2125 ==============\n",
      "============== Pattern 2126 ==============\n",
      "============== Pattern 2127 ==============\n",
      "============== Pattern 2128 ==============\n",
      "============== Pattern 2129 ==============\n",
      "============== Pattern 2130 ==============\n",
      "============== Pattern 2131 ==============\n",
      "============== Pattern 2132 ==============\n",
      "============== Pattern 2133 ==============\n",
      "============== Pattern 2134 ==============\n",
      "============== Pattern 2135 ==============\n",
      "============== Pattern 2136 ==============\n",
      "============== Pattern 2137 ==============\n",
      "============== Pattern 2138 ==============\n",
      "============== Pattern 2139 ==============\n",
      "============== Pattern 2140 ==============\n",
      "============== Pattern 2141 ==============\n",
      "============== Pattern 2142 ==============\n",
      "============== Pattern 2143 ==============\n",
      "============== Pattern 2144 ==============\n",
      "============== Pattern 2145 ==============\n",
      "============== Pattern 2146 ==============\n",
      "============== Pattern 2147 ==============\n",
      "============== Pattern 2148 ==============\n",
      "============== Pattern 2149 ==============\n",
      "============== Pattern 2150 ==============\n",
      "============== Pattern 2151 ==============\n",
      "============== Pattern 2152 ==============\n",
      "============== Pattern 2153 ==============\n",
      "============== Pattern 2154 ==============\n",
      "============== Pattern 2155 ==============\n",
      "============== Pattern 2156 ==============\n",
      "============== Pattern 2157 ==============\n",
      "============== Pattern 2158 ==============\n",
      "============== Pattern 2159 ==============\n",
      "============== Pattern 2160 ==============\n",
      "============== Pattern 2161 ==============\n",
      "============== Pattern 2162 ==============\n",
      "============== Pattern 2163 ==============\n",
      "============== Pattern 2164 ==============\n",
      "============== Pattern 2165 ==============\n",
      "============== Pattern 2166 ==============\n",
      "============== Pattern 2167 ==============\n",
      "============== Pattern 2168 ==============\n",
      "============== Pattern 2169 ==============\n",
      "============== Pattern 2170 ==============\n",
      "============== Pattern 2171 ==============\n",
      "============== Pattern 2172 ==============\n",
      "============== Pattern 2173 ==============\n",
      "============== Pattern 2174 ==============\n",
      "============== Pattern 2175 ==============\n",
      "============== Pattern 2176 ==============\n",
      "============== Pattern 2177 ==============\n",
      "============== Pattern 2178 ==============\n",
      "============== Pattern 2179 ==============\n",
      "============== Pattern 2180 ==============\n",
      "============== Pattern 2181 ==============\n",
      "============== Pattern 2182 ==============\n",
      "============== Pattern 2183 ==============\n",
      "============== Pattern 2184 ==============\n",
      "============== Pattern 2185 ==============\n",
      "============== Pattern 2186 ==============\n",
      "============== Pattern 2187 ==============\n",
      "============== Pattern 2188 ==============\n",
      "============== Pattern 2189 ==============\n",
      "============== Pattern 2190 ==============\n",
      "============== Pattern 2191 ==============\n",
      "============== Pattern 2192 ==============\n",
      "============== Pattern 2193 ==============\n",
      "============== Pattern 2194 ==============\n",
      "============== Pattern 2195 ==============\n",
      "============== Pattern 2196 ==============\n",
      "============== Pattern 2197 ==============\n",
      "============== Pattern 2198 ==============\n",
      "============== Pattern 2199 ==============\n",
      "============== Pattern 2200 ==============\n",
      "============== Pattern 2201 ==============\n",
      "============== Pattern 2202 ==============\n",
      "============== Pattern 2203 ==============\n",
      "============== Pattern 2204 ==============\n",
      "============== Pattern 2205 ==============\n",
      "============== Pattern 2206 ==============\n",
      "============== Pattern 2207 ==============\n",
      "============== Pattern 2208 ==============\n",
      "============== Pattern 2209 ==============\n",
      "============== Pattern 2210 ==============\n",
      "============== Pattern 2211 ==============\n",
      "============== Pattern 2212 ==============\n",
      "============== Pattern 2213 ==============\n",
      "============== Pattern 2214 ==============\n",
      "============== Pattern 2215 ==============\n",
      "============== Pattern 2216 ==============\n",
      "============== Pattern 2217 ==============\n",
      "============== Pattern 2218 ==============\n",
      "============== Pattern 2219 ==============\n",
      "============== Pattern 2220 ==============\n",
      "============== Pattern 2221 ==============\n",
      "============== Pattern 2222 ==============\n",
      "============== Pattern 2223 ==============\n",
      "============== Pattern 2224 ==============\n",
      "============== Pattern 2225 ==============\n",
      "============== Pattern 2226 ==============\n",
      "============== Pattern 2227 ==============\n",
      "============== Pattern 2228 ==============\n",
      "============== Pattern 2229 ==============\n",
      "============== Pattern 2230 ==============\n",
      "============== Pattern 2231 ==============\n",
      "============== Pattern 2232 ==============\n",
      "============== Pattern 2233 ==============\n",
      "============== Pattern 2234 ==============\n",
      "============== Pattern 2235 ==============\n",
      "============== Pattern 2236 ==============\n",
      "============== Pattern 2237 ==============\n",
      "============== Pattern 2238 ==============\n",
      "============== Pattern 2239 ==============\n",
      "============== Pattern 2240 ==============\n",
      "============== Pattern 2241 ==============\n",
      "============== Pattern 2242 ==============\n",
      "============== Pattern 2243 ==============\n",
      "============== Pattern 2244 ==============\n",
      "============== Pattern 2245 ==============\n",
      "============== Pattern 2246 ==============\n",
      "============== Pattern 2247 ==============\n",
      "============== Pattern 2248 ==============\n",
      "============== Pattern 2249 ==============\n",
      "============== Pattern 2250 ==============\n",
      "============== Pattern 2251 ==============\n",
      "============== Pattern 2252 ==============\n",
      "============== Pattern 2253 ==============\n",
      "============== Pattern 2254 ==============\n",
      "============== Pattern 2255 ==============\n",
      "============== Pattern 2256 ==============\n",
      "============== Pattern 2257 ==============\n",
      "============== Pattern 2258 ==============\n",
      "============== Pattern 2259 ==============\n",
      "============== Pattern 2260 ==============\n",
      "============== Pattern 2261 ==============\n",
      "============== Pattern 2262 ==============\n",
      "============== Pattern 2263 ==============\n",
      "============== Pattern 2264 ==============\n",
      "============== Pattern 2265 ==============\n",
      "============== Pattern 2266 ==============\n",
      "============== Pattern 2267 ==============\n",
      "============== Pattern 2268 ==============\n",
      "============== Pattern 2269 ==============\n",
      "============== Pattern 2270 ==============\n",
      "============== Pattern 2271 ==============\n",
      "============== Pattern 2272 ==============\n",
      "============== Pattern 2273 ==============\n",
      "============== Pattern 2274 ==============\n",
      "============== Pattern 2275 ==============\n",
      "============== Pattern 2276 ==============\n",
      "============== Pattern 2277 ==============\n",
      "============== Pattern 2278 ==============\n",
      "============== Pattern 2279 ==============\n",
      "============== Pattern 2280 ==============\n",
      "============== Pattern 2281 ==============\n",
      "============== Pattern 2282 ==============\n",
      "============== Pattern 2283 ==============\n",
      "============== Pattern 2284 ==============\n",
      "============== Pattern 2285 ==============\n",
      "============== Pattern 2286 ==============\n",
      "============== Pattern 2287 ==============\n",
      "============== Pattern 2288 ==============\n",
      "============== Pattern 2289 ==============\n",
      "============== Pattern 2290 ==============\n",
      "============== Pattern 2291 ==============\n",
      "============== Pattern 2292 ==============\n",
      "============== Pattern 2293 ==============\n",
      "============== Pattern 2294 ==============\n",
      "============== Pattern 2295 ==============\n",
      "============== Pattern 2296 ==============\n",
      "============== Pattern 2297 ==============\n",
      "============== Pattern 2298 ==============\n",
      "============== Pattern 2299 ==============\n",
      "============== Pattern 2300 ==============\n",
      "============== Pattern 2301 ==============\n",
      "============== Pattern 2302 ==============\n",
      "============== Pattern 2303 ==============\n",
      "============== Pattern 2304 ==============\n",
      "============== Pattern 2305 ==============\n",
      "============== Pattern 2306 ==============\n",
      "============== Pattern 2307 ==============\n",
      "============== Pattern 2308 ==============\n",
      "============== Pattern 2309 ==============\n",
      "============== Pattern 2310 ==============\n",
      "============== Pattern 2311 ==============\n",
      "============== Pattern 2312 ==============\n",
      "============== Pattern 2313 ==============\n",
      "============== Pattern 2314 ==============\n",
      "============== Pattern 2315 ==============\n",
      "============== Pattern 2316 ==============\n",
      "============== Pattern 2317 ==============\n",
      "============== Pattern 2318 ==============\n",
      "============== Pattern 2319 ==============\n",
      "============== Pattern 2320 ==============\n",
      "============== Pattern 2321 ==============\n",
      "============== Pattern 2322 ==============\n",
      "============== Pattern 2323 ==============\n",
      "============== Pattern 2324 ==============\n",
      "============== Pattern 2325 ==============\n",
      "============== Pattern 2326 ==============\n",
      "============== Pattern 2327 ==============\n",
      "============== Pattern 2328 ==============\n",
      "============== Pattern 2329 ==============\n",
      "============== Pattern 2330 ==============\n",
      "============== Pattern 2331 ==============\n",
      "============== Pattern 2332 ==============\n",
      "============== Pattern 2333 ==============\n",
      "============== Pattern 2334 ==============\n",
      "============== Pattern 2335 ==============\n",
      "============== Pattern 2336 ==============\n",
      "============== Pattern 2337 ==============\n",
      "============== Pattern 2338 ==============\n",
      "============== Pattern 2339 ==============\n",
      "============== Pattern 2340 ==============\n",
      "============== Pattern 2341 ==============\n",
      "============== Pattern 2342 ==============\n",
      "============== Pattern 2343 ==============\n",
      "============== Pattern 2344 ==============\n",
      "============== Pattern 2345 ==============\n",
      "============== Pattern 2346 ==============\n",
      "============== Pattern 2347 ==============\n",
      "============== Pattern 2348 ==============\n",
      "============== Pattern 2349 ==============\n",
      "============== Pattern 2350 ==============\n",
      "============== Pattern 2351 ==============\n",
      "============== Pattern 2352 ==============\n",
      "============== Pattern 2353 ==============\n",
      "============== Pattern 2354 ==============\n",
      "============== Pattern 2355 ==============\n",
      "============== Pattern 2356 ==============\n",
      "============== Pattern 2357 ==============\n",
      "============== Pattern 2358 ==============\n",
      "============== Pattern 2359 ==============\n",
      "============== Pattern 2360 ==============\n",
      "============== Pattern 2361 ==============\n",
      "============== Pattern 2362 ==============\n",
      "============== Pattern 2363 ==============\n",
      "============== Pattern 2364 ==============\n",
      "============== Pattern 2365 ==============\n",
      "============== Pattern 2366 ==============\n",
      "============== Pattern 2367 ==============\n",
      "============== Pattern 2368 ==============\n",
      "============== Pattern 2369 ==============\n",
      "============== Pattern 2370 ==============\n",
      "============== Pattern 2371 ==============\n",
      "============== Pattern 2372 ==============\n",
      "============== Pattern 2373 ==============\n",
      "============== Pattern 2374 ==============\n",
      "============== Pattern 2375 ==============\n",
      "============== Pattern 2376 ==============\n",
      "============== Pattern 2377 ==============\n",
      "============== Pattern 2378 ==============\n",
      "============== Pattern 2379 ==============\n",
      "============== Pattern 2380 ==============\n",
      "============== Pattern 2381 ==============\n",
      "============== Pattern 2382 ==============\n",
      "============== Pattern 2383 ==============\n",
      "============== Pattern 2384 ==============\n",
      "============== Pattern 2385 ==============\n",
      "============== Pattern 2386 ==============\n",
      "============== Pattern 2387 ==============\n",
      "============== Pattern 2388 ==============\n",
      "============== Pattern 2389 ==============\n",
      "============== Pattern 2390 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2391 ==============\n",
      "============== Pattern 2392 ==============\n",
      "============== Pattern 2393 ==============\n",
      "============== Pattern 2394 ==============\n",
      "============== Pattern 2395 ==============\n",
      "============== Pattern 2396 ==============\n",
      "============== Pattern 2397 ==============\n",
      "============== Pattern 2398 ==============\n",
      "============== Pattern 2399 ==============\n",
      "============== Pattern 2400 ==============\n",
      "============== Pattern 2401 ==============\n",
      "============== Pattern 2402 ==============\n",
      "============== Pattern 2403 ==============\n",
      "============== Pattern 2404 ==============\n",
      "============== Pattern 2405 ==============\n",
      "============== Pattern 2406 ==============\n",
      "============== Pattern 2407 ==============\n",
      "============== Pattern 2408 ==============\n",
      "============== Pattern 2409 ==============\n",
      "============== Pattern 2410 ==============\n",
      "============== Pattern 2411 ==============\n",
      "============== Pattern 2412 ==============\n",
      "============== Pattern 2413 ==============\n",
      "============== Pattern 2414 ==============\n",
      "============== Pattern 2415 ==============\n",
      "============== Pattern 2416 ==============\n",
      "============== Pattern 2417 ==============\n",
      "============== Pattern 2418 ==============\n",
      "============== Pattern 2419 ==============\n",
      "============== Pattern 2420 ==============\n",
      "============== Pattern 2421 ==============\n",
      "============== Pattern 2422 ==============\n",
      "============== Pattern 2423 ==============\n",
      "============== Pattern 2424 ==============\n",
      "============== Pattern 2425 ==============\n",
      "============== Pattern 2426 ==============\n",
      "============== Pattern 2427 ==============\n",
      "============== Pattern 2428 ==============\n",
      "============== Pattern 2429 ==============\n",
      "============== Pattern 2430 ==============\n",
      "============== Pattern 2431 ==============\n",
      "============== Pattern 2432 ==============\n",
      "============== Pattern 2433 ==============\n",
      "============== Pattern 2434 ==============\n",
      "============== Pattern 2435 ==============\n",
      "============== Pattern 2436 ==============\n",
      "============== Pattern 2437 ==============\n",
      "============== Pattern 2438 ==============\n",
      "============== Pattern 2439 ==============\n",
      "============== Pattern 2440 ==============\n",
      "============== Pattern 2441 ==============\n",
      "============== Pattern 2442 ==============\n",
      "============== Pattern 2443 ==============\n",
      "============== Pattern 2444 ==============\n",
      "============== Pattern 2445 ==============\n",
      "============== Pattern 2446 ==============\n",
      "============== Pattern 2447 ==============\n",
      "============== Pattern 2448 ==============\n",
      "============== Pattern 2449 ==============\n",
      "============== Pattern 2450 ==============\n",
      "============== Pattern 2451 ==============\n",
      "============== Pattern 2452 ==============\n",
      "============== Pattern 2453 ==============\n",
      "============== Pattern 2454 ==============\n",
      "============== Pattern 2455 ==============\n",
      "============== Pattern 2456 ==============\n",
      "============== Pattern 2457 ==============\n",
      "============== Pattern 2458 ==============\n",
      "============== Pattern 2459 ==============\n",
      "============== Pattern 2460 ==============\n",
      "============== Pattern 2461 ==============\n",
      "============== Pattern 2462 ==============\n",
      "============== Pattern 2463 ==============\n",
      "============== Pattern 2464 ==============\n",
      "============== Pattern 2465 ==============\n",
      "============== Pattern 2466 ==============\n",
      "============== Pattern 2467 ==============\n",
      "============== Pattern 2468 ==============\n",
      "============== Pattern 2469 ==============\n",
      "============== Pattern 2470 ==============\n",
      "============== Pattern 2471 ==============\n",
      "============== Pattern 2472 ==============\n",
      "============== Pattern 2473 ==============\n",
      "============== Pattern 2474 ==============\n",
      "============== Pattern 2475 ==============\n",
      "============== Pattern 2476 ==============\n",
      "============== Pattern 2477 ==============\n",
      "============== Pattern 2478 ==============\n",
      "============== Pattern 2479 ==============\n",
      "============== Pattern 2480 ==============\n",
      "============== Pattern 2481 ==============\n",
      "============== Pattern 2482 ==============\n",
      "============== Pattern 2483 ==============\n",
      "============== Pattern 2484 ==============\n",
      "============== Pattern 2485 ==============\n",
      "============== Pattern 2486 ==============\n",
      "============== Pattern 2487 ==============\n",
      "============== Pattern 2488 ==============\n",
      "============== Pattern 2489 ==============\n",
      "============== Pattern 2490 ==============\n",
      "============== Pattern 2491 ==============\n",
      "============== Pattern 2492 ==============\n",
      "============== Pattern 2493 ==============\n",
      "============== Pattern 2494 ==============\n",
      "============== Pattern 2495 ==============\n",
      "============== Pattern 2496 ==============\n",
      "============== Pattern 2497 ==============\n",
      "============== Pattern 2498 ==============\n",
      "============== Pattern 2499 ==============\n",
      "============== Pattern 2500 ==============\n",
      "============== Pattern 2501 ==============\n",
      "============== Pattern 2502 ==============\n",
      "============== Pattern 2503 ==============\n",
      "============== Pattern 2504 ==============\n",
      "============== Pattern 2505 ==============\n",
      "============== Pattern 2506 ==============\n",
      "============== Pattern 2507 ==============\n",
      "============== Pattern 2508 ==============\n",
      "============== Pattern 2509 ==============\n",
      "============== Pattern 2510 ==============\n",
      "============== Pattern 2511 ==============\n",
      "============== Pattern 2512 ==============\n",
      "============== Pattern 2513 ==============\n",
      "============== Pattern 2514 ==============\n",
      "============== Pattern 2515 ==============\n",
      "============== Pattern 2516 ==============\n",
      "============== Pattern 2517 ==============\n",
      "============== Pattern 2518 ==============\n",
      "============== Pattern 2519 ==============\n",
      "============== Pattern 2520 ==============\n",
      "============== Pattern 2521 ==============\n",
      "============== Pattern 2522 ==============\n",
      "============== Pattern 2523 ==============\n",
      "============== Pattern 2524 ==============\n",
      "============== Pattern 2525 ==============\n",
      "============== Pattern 2526 ==============\n",
      "============== Pattern 2527 ==============\n",
      "============== Pattern 2528 ==============\n",
      "============== Pattern 2529 ==============\n",
      "============== Pattern 2530 ==============\n",
      "============== Pattern 2531 ==============\n",
      "============== Pattern 2532 ==============\n",
      "============== Pattern 2533 ==============\n",
      "============== Pattern 2534 ==============\n",
      "============== Pattern 2535 ==============\n",
      "============== Pattern 2536 ==============\n",
      "============== Pattern 2537 ==============\n",
      "============== Pattern 2538 ==============\n",
      "============== Pattern 2539 ==============\n",
      "============== Pattern 2540 ==============\n",
      "============== Pattern 2541 ==============\n",
      "============== Pattern 2542 ==============\n",
      "============== Pattern 2543 ==============\n",
      "============== Pattern 2544 ==============\n",
      "============== Pattern 2545 ==============\n",
      "============== Pattern 2546 ==============\n",
      "============== Pattern 2547 ==============\n",
      "============== Pattern 2548 ==============\n",
      "============== Pattern 2549 ==============\n",
      "============== Pattern 2550 ==============\n",
      "============== Pattern 2551 ==============\n",
      "============== Pattern 2552 ==============\n",
      "============== Pattern 2553 ==============\n",
      "============== Pattern 2554 ==============\n",
      "============== Pattern 2555 ==============\n",
      "============== Pattern 2556 ==============\n",
      "============== Pattern 2557 ==============\n",
      "============== Pattern 2558 ==============\n",
      "============== Pattern 2559 ==============\n",
      "============== Pattern 2560 ==============\n",
      "============== Pattern 2561 ==============\n",
      "============== Pattern 2562 ==============\n",
      "============== Pattern 2563 ==============\n",
      "============== Pattern 2564 ==============\n",
      "============== Pattern 2565 ==============\n",
      "============== Pattern 2566 ==============\n",
      "============== Pattern 2567 ==============\n",
      "============== Pattern 2568 ==============\n",
      "============== Pattern 2569 ==============\n",
      "============== Pattern 2570 ==============\n",
      "============== Pattern 2571 ==============\n",
      "============== Pattern 2572 ==============\n",
      "============== Pattern 2573 ==============\n",
      "============== Pattern 2574 ==============\n",
      "============== Pattern 2575 ==============\n",
      "============== Pattern 2576 ==============\n",
      "============== Pattern 2577 ==============\n",
      "============== Pattern 2578 ==============\n",
      "============== Pattern 2579 ==============\n",
      "============== Pattern 2580 ==============\n",
      "============== Pattern 2581 ==============\n",
      "============== Pattern 2582 ==============\n",
      "============== Pattern 2583 ==============\n",
      "============== Pattern 2584 ==============\n",
      "============== Pattern 2585 ==============\n",
      "============== Pattern 2586 ==============\n",
      "============== Pattern 2587 ==============\n",
      "============== Pattern 2588 ==============\n",
      "============== Pattern 2589 ==============\n",
      "============== Pattern 2590 ==============\n",
      "============== Pattern 2591 ==============\n",
      "============== Pattern 2592 ==============\n",
      "============== Pattern 2593 ==============\n",
      "============== Pattern 2594 ==============\n",
      "============== Pattern 2595 ==============\n",
      "============== Pattern 2596 ==============\n",
      "============== Pattern 2597 ==============\n",
      "============== Pattern 2598 ==============\n",
      "============== Pattern 2599 ==============\n",
      "============== Pattern 2600 ==============\n",
      "============== Pattern 2601 ==============\n",
      "============== Pattern 2602 ==============\n",
      "============== Pattern 2603 ==============\n",
      "============== Pattern 2604 ==============\n",
      "============== Pattern 2605 ==============\n",
      "============== Pattern 2606 ==============\n",
      "============== Pattern 2607 ==============\n",
      "============== Pattern 2608 ==============\n",
      "============== Pattern 2609 ==============\n",
      "============== Pattern 2610 ==============\n",
      "============== Pattern 2611 ==============\n",
      "============== Pattern 2612 ==============\n",
      "============== Pattern 2613 ==============\n",
      "============== Pattern 2614 ==============\n",
      "============== Pattern 2615 ==============\n",
      "============== Pattern 2616 ==============\n",
      "============== Pattern 2617 ==============\n",
      "============== Pattern 2618 ==============\n",
      "============== Pattern 2619 ==============\n",
      "============== Pattern 2620 ==============\n",
      "============== Pattern 2621 ==============\n",
      "============== Pattern 2622 ==============\n",
      "============== Pattern 2623 ==============\n",
      "============== Pattern 2624 ==============\n",
      "============== Pattern 2625 ==============\n",
      "============== Pattern 2626 ==============\n",
      "============== Pattern 2627 ==============\n",
      "============== Pattern 2628 ==============\n",
      "============== Pattern 2629 ==============\n",
      "============== Pattern 2630 ==============\n",
      "============== Pattern 2631 ==============\n",
      "============== Pattern 2632 ==============\n",
      "============== Pattern 2633 ==============\n",
      "============== Pattern 2634 ==============\n",
      "============== Pattern 2635 ==============\n",
      "============== Pattern 2636 ==============\n",
      "============== Pattern 2637 ==============\n",
      "============== Pattern 2638 ==============\n",
      "============== Pattern 2639 ==============\n",
      "============== Pattern 2640 ==============\n",
      "============== Pattern 2641 ==============\n",
      "============== Pattern 2642 ==============\n",
      "============== Pattern 2643 ==============\n",
      "============== Pattern 2644 ==============\n",
      "============== Pattern 2645 ==============\n",
      "============== Pattern 2646 ==============\n",
      "============== Pattern 2647 ==============\n",
      "============== Pattern 2648 ==============\n",
      "============== Pattern 2649 ==============\n",
      "============== Pattern 2650 ==============\n",
      "============== Pattern 2651 ==============\n",
      "============== Pattern 2652 ==============\n",
      "============== Pattern 2653 ==============\n",
      "============== Pattern 2654 ==============\n",
      "============== Pattern 2655 ==============\n",
      "============== Pattern 2656 ==============\n",
      "============== Pattern 2657 ==============\n",
      "============== Pattern 2658 ==============\n",
      "============== Pattern 2659 ==============\n",
      "============== Pattern 2660 ==============\n",
      "============== Pattern 2661 ==============\n",
      "============== Pattern 2662 ==============\n",
      "============== Pattern 2663 ==============\n",
      "============== Pattern 2664 ==============\n",
      "============== Pattern 2665 ==============\n",
      "============== Pattern 2666 ==============\n",
      "============== Pattern 2667 ==============\n",
      "============== Pattern 2668 ==============\n",
      "============== Pattern 2669 ==============\n",
      "============== Pattern 2670 ==============\n",
      "============== Pattern 2671 ==============\n",
      "============== Pattern 2672 ==============\n",
      "============== Pattern 2673 ==============\n",
      "============== Pattern 2674 ==============\n",
      "============== Pattern 2675 ==============\n",
      "============== Pattern 2676 ==============\n",
      "============== Pattern 2677 ==============\n",
      "============== Pattern 2678 ==============\n",
      "============== Pattern 2679 ==============\n",
      "============== Pattern 2680 ==============\n",
      "============== Pattern 2681 ==============\n",
      "============== Pattern 2682 ==============\n",
      "============== Pattern 2683 ==============\n",
      "============== Pattern 2684 ==============\n",
      "============== Pattern 2685 ==============\n",
      "============== Pattern 2686 ==============\n",
      "============== Pattern 2687 ==============\n",
      "============== Pattern 2688 ==============\n",
      "============== Pattern 2689 ==============\n",
      "============== Pattern 2690 ==============\n",
      "============== Pattern 2691 ==============\n",
      "============== Pattern 2692 ==============\n",
      "============== Pattern 2693 ==============\n",
      "============== Pattern 2694 ==============\n",
      "============== Pattern 2695 ==============\n",
      "============== Pattern 2696 ==============\n",
      "============== Pattern 2697 ==============\n",
      "============== Pattern 2698 ==============\n",
      "============== Pattern 2699 ==============\n",
      "============== Pattern 2700 ==============\n",
      "============== Pattern 2701 ==============\n",
      "============== Pattern 2702 ==============\n",
      "============== Pattern 2703 ==============\n",
      "============== Pattern 2704 ==============\n",
      "============== Pattern 2705 ==============\n",
      "============== Pattern 2706 ==============\n",
      "============== Pattern 2707 ==============\n",
      "============== Pattern 2708 ==============\n",
      "============== Pattern 2709 ==============\n",
      "============== Pattern 2710 ==============\n",
      "============== Pattern 2711 ==============\n",
      "============== Pattern 2712 ==============\n",
      "============== Pattern 2713 ==============\n",
      "============== Pattern 2714 ==============\n",
      "============== Pattern 2715 ==============\n",
      "============== Pattern 2716 ==============\n",
      "============== Pattern 2717 ==============\n",
      "============== Pattern 2718 ==============\n",
      "============== Pattern 2719 ==============\n",
      "============== Pattern 2720 ==============\n",
      "============== Pattern 2721 ==============\n",
      "============== Pattern 2722 ==============\n",
      "============== Pattern 2723 ==============\n",
      "============== Pattern 2724 ==============\n",
      "============== Pattern 2725 ==============\n",
      "============== Pattern 2726 ==============\n",
      "============== Pattern 2727 ==============\n",
      "============== Pattern 2728 ==============\n",
      "============== Pattern 2729 ==============\n",
      "============== Pattern 2730 ==============\n",
      "============== Pattern 2731 ==============\n",
      "============== Pattern 2732 ==============\n",
      "============== Pattern 2733 ==============\n",
      "============== Pattern 2734 ==============\n",
      "============== Pattern 2735 ==============\n",
      "============== Pattern 2736 ==============\n",
      "============== Pattern 2737 ==============\n",
      "============== Pattern 2738 ==============\n",
      "============== Pattern 2739 ==============\n",
      "============== Pattern 2740 ==============\n",
      "============== Pattern 2741 ==============\n",
      "============== Pattern 2742 ==============\n",
      "============== Pattern 2743 ==============\n",
      "============== Pattern 2744 ==============\n",
      "============== Pattern 2745 ==============\n",
      "============== Pattern 2746 ==============\n",
      "============== Pattern 2747 ==============\n",
      "============== Pattern 2748 ==============\n",
      "============== Pattern 2749 ==============\n",
      "============== Pattern 2750 ==============\n",
      "============== Pattern 2751 ==============\n",
      "============== Pattern 2752 ==============\n",
      "============== Pattern 2753 ==============\n",
      "============== Pattern 2754 ==============\n",
      "============== Pattern 2755 ==============\n",
      "============== Pattern 2756 ==============\n",
      "============== Pattern 2757 ==============\n",
      "============== Pattern 2758 ==============\n",
      "============== Pattern 2759 ==============\n",
      "============== Pattern 2760 ==============\n",
      "============== Pattern 2761 ==============\n",
      "============== Pattern 2762 ==============\n",
      "============== Pattern 2763 ==============\n",
      "============== Pattern 2764 ==============\n",
      "============== Pattern 2765 ==============\n",
      "============== Pattern 2766 ==============\n",
      "============== Pattern 2767 ==============\n",
      "============== Pattern 2768 ==============\n",
      "============== Pattern 2769 ==============\n",
      "============== Pattern 2770 ==============\n",
      "============== Pattern 2771 ==============\n",
      "============== Pattern 2772 ==============\n",
      "============== Pattern 2773 ==============\n",
      "============== Pattern 2774 ==============\n",
      "============== Pattern 2775 ==============\n",
      "============== Pattern 2776 ==============\n",
      "============== Pattern 2777 ==============\n",
      "============== Pattern 2778 ==============\n",
      "============== Pattern 2779 ==============\n",
      "============== Pattern 2780 ==============\n",
      "============== Pattern 2781 ==============\n",
      "============== Pattern 2782 ==============\n",
      "============== Pattern 2783 ==============\n",
      "============== Pattern 2784 ==============\n",
      "============== Pattern 2785 ==============\n",
      "============== Pattern 2786 ==============\n",
      "============== Pattern 2787 ==============\n",
      "============== Pattern 2788 ==============\n",
      "============== Pattern 2789 ==============\n",
      "============== Pattern 2790 ==============\n",
      "============== Pattern 2791 ==============\n",
      "============== Pattern 2792 ==============\n",
      "============== Pattern 2793 ==============\n",
      "============== Pattern 2794 ==============\n",
      "============== Pattern 2795 ==============\n",
      "============== Pattern 2796 ==============\n",
      "============== Pattern 2797 ==============\n",
      "============== Pattern 2798 ==============\n",
      "============== Pattern 2799 ==============\n",
      "============== Pattern 2800 ==============\n",
      "============== Pattern 2801 ==============\n",
      "============== Pattern 2802 ==============\n",
      "============== Pattern 2803 ==============\n",
      "============== Pattern 2804 ==============\n",
      "============== Pattern 2805 ==============\n",
      "============== Pattern 2806 ==============\n",
      "============== Pattern 2807 ==============\n",
      "============== Pattern 2808 ==============\n",
      "============== Pattern 2809 ==============\n",
      "============== Pattern 2810 ==============\n",
      "============== Pattern 2811 ==============\n",
      "============== Pattern 2812 ==============\n",
      "============== Pattern 2813 ==============\n",
      "============== Pattern 2814 ==============\n",
      "============== Pattern 2815 ==============\n",
      "============== Pattern 2816 ==============\n",
      "============== Pattern 2817 ==============\n",
      "============== Pattern 2818 ==============\n",
      "============== Pattern 2819 ==============\n",
      "============== Pattern 2820 ==============\n",
      "============== Pattern 2821 ==============\n",
      "============== Pattern 2822 ==============\n",
      "============== Pattern 2823 ==============\n",
      "============== Pattern 2824 ==============\n",
      "============== Pattern 2825 ==============\n",
      "============== Pattern 2826 ==============\n",
      "============== Pattern 2827 ==============\n",
      "============== Pattern 2828 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2829 ==============\n",
      "============== Pattern 2830 ==============\n",
      "============== Pattern 2831 ==============\n",
      "============== Pattern 2832 ==============\n",
      "============== Pattern 2833 ==============\n",
      "============== Pattern 2834 ==============\n",
      "============== Pattern 2835 ==============\n",
      "============== Pattern 2836 ==============\n",
      "============== Pattern 2837 ==============\n",
      "============== Pattern 2838 ==============\n",
      "============== Pattern 2839 ==============\n",
      "============== Pattern 2840 ==============\n",
      "============== Pattern 2841 ==============\n",
      "============== Pattern 2842 ==============\n",
      "============== Pattern 2843 ==============\n",
      "============== Pattern 2844 ==============\n",
      "============== Pattern 2845 ==============\n",
      "============== Pattern 2846 ==============\n",
      "============== Pattern 2847 ==============\n",
      "============== Pattern 2848 ==============\n",
      "============== Pattern 2849 ==============\n",
      "============== Pattern 2850 ==============\n",
      "============== Pattern 2851 ==============\n",
      "============== Pattern 2852 ==============\n",
      "============== Pattern 2853 ==============\n",
      "============== Pattern 2854 ==============\n",
      "============== Pattern 2855 ==============\n",
      "============== Pattern 2856 ==============\n",
      "============== Pattern 2857 ==============\n",
      "============== Pattern 2858 ==============\n",
      "============== Pattern 2859 ==============\n",
      "============== Pattern 2860 ==============\n",
      "============== Pattern 2861 ==============\n",
      "============== Pattern 2862 ==============\n",
      "============== Pattern 2863 ==============\n",
      "============== Pattern 2864 ==============\n",
      "============== Pattern 2865 ==============\n",
      "============== Pattern 2866 ==============\n",
      "============== Pattern 2867 ==============\n",
      "============== Pattern 2868 ==============\n",
      "============== Pattern 2869 ==============\n",
      "============== Pattern 2870 ==============\n",
      "============== Pattern 2871 ==============\n",
      "============== Pattern 2872 ==============\n",
      "============== Pattern 2873 ==============\n",
      "============== Pattern 2874 ==============\n",
      "============== Pattern 2875 ==============\n",
      "============== Pattern 2876 ==============\n",
      "============== Pattern 2877 ==============\n",
      "============== Pattern 2878 ==============\n",
      "============== Pattern 2879 ==============\n",
      "============== Pattern 2880 ==============\n",
      "============== Pattern 2881 ==============\n",
      "============== Pattern 2882 ==============\n",
      "============== Pattern 2883 ==============\n",
      "============== Pattern 2884 ==============\n",
      "============== Pattern 2885 ==============\n",
      "============== Pattern 2886 ==============\n",
      "============== Pattern 2887 ==============\n",
      "============== Pattern 2888 ==============\n",
      "============== Pattern 2889 ==============\n",
      "============== Pattern 2890 ==============\n",
      "============== Pattern 2891 ==============\n",
      "============== Pattern 2892 ==============\n",
      "============== Pattern 2893 ==============\n",
      "============== Pattern 2894 ==============\n",
      "============== Pattern 2895 ==============\n",
      "============== Pattern 2896 ==============\n",
      "============== Pattern 2897 ==============\n",
      "============== Pattern 2898 ==============\n",
      "============== Pattern 2899 ==============\n",
      "============== Pattern 2900 ==============\n",
      "============== Pattern 2901 ==============\n",
      "============== Pattern 2902 ==============\n",
      "============== Pattern 2903 ==============\n",
      "============== Pattern 2904 ==============\n",
      "============== Pattern 2905 ==============\n",
      "============== Pattern 2906 ==============\n",
      "============== Pattern 2907 ==============\n",
      "============== Pattern 2908 ==============\n",
      "============== Pattern 2909 ==============\n",
      "============== Pattern 2910 ==============\n",
      "============== Pattern 2911 ==============\n",
      "============== Pattern 2912 ==============\n",
      "============== Pattern 2913 ==============\n",
      "============== Pattern 2914 ==============\n",
      "============== Pattern 2915 ==============\n",
      "============== Pattern 2916 ==============\n",
      "============== Pattern 2917 ==============\n",
      "============== Pattern 2918 ==============\n",
      "============== Pattern 2919 ==============\n",
      "============== Pattern 2920 ==============\n",
      "============== Pattern 2921 ==============\n",
      "============== Pattern 2922 ==============\n",
      "============== Pattern 2923 ==============\n",
      "============== Pattern 2924 ==============\n",
      "============== Pattern 2925 ==============\n",
      "============== Pattern 2926 ==============\n",
      "============== Pattern 2927 ==============\n",
      "============== Pattern 2928 ==============\n",
      "============== Pattern 2929 ==============\n",
      "============== Pattern 2930 ==============\n",
      "============== Pattern 2931 ==============\n",
      "============== Pattern 2932 ==============\n",
      "============== Pattern 2933 ==============\n",
      "============== Pattern 2934 ==============\n",
      "============== Pattern 2935 ==============\n",
      "============== Pattern 2936 ==============\n",
      "============== Pattern 2937 ==============\n",
      "============== Pattern 2938 ==============\n",
      "============== Pattern 2939 ==============\n",
      "============== Pattern 2940 ==============\n",
      "============== Pattern 2941 ==============\n",
      "============== Pattern 2942 ==============\n",
      "============== Pattern 2943 ==============\n",
      "============== Pattern 2944 ==============\n",
      "============== Pattern 2945 ==============\n",
      "============== Pattern 2946 ==============\n",
      "============== Pattern 2947 ==============\n",
      "============== Pattern 2948 ==============\n",
      "============== Pattern 2949 ==============\n",
      "============== Pattern 2950 ==============\n",
      "============== Pattern 2951 ==============\n",
      "============== Pattern 2952 ==============\n",
      "============== Pattern 2953 ==============\n",
      "============== Pattern 2954 ==============\n",
      "============== Pattern 2955 ==============\n",
      "============== Pattern 2956 ==============\n",
      "============== Pattern 2957 ==============\n",
      "============== Pattern 2958 ==============\n",
      "============== Pattern 2959 ==============\n",
      "============== Pattern 2960 ==============\n",
      "============== Pattern 2961 ==============\n",
      "============== Pattern 2962 ==============\n",
      "============== Pattern 2963 ==============\n",
      "============== Pattern 2964 ==============\n",
      "============== Pattern 2965 ==============\n",
      "============== Pattern 2966 ==============\n",
      "============== Pattern 2967 ==============\n",
      "============== Pattern 2968 ==============\n",
      "============== Pattern 2969 ==============\n",
      "============== Pattern 2970 ==============\n",
      "============== Pattern 2971 ==============\n",
      "============== Pattern 2972 ==============\n",
      "============== Pattern 2973 ==============\n",
      "============== Pattern 2974 ==============\n",
      "============== Pattern 2975 ==============\n",
      "============== Pattern 2976 ==============\n",
      "============== Pattern 2977 ==============\n",
      "============== Pattern 2978 ==============\n",
      "============== Pattern 2979 ==============\n",
      "============== Pattern 2980 ==============\n",
      "============== Pattern 2981 ==============\n",
      "============== Pattern 2982 ==============\n",
      "============== Pattern 2983 ==============\n",
      "============== Pattern 2984 ==============\n",
      "============== Pattern 2985 ==============\n",
      "============== Pattern 2986 ==============\n",
      "============== Pattern 2987 ==============\n",
      "============== Pattern 2988 ==============\n",
      "============== Pattern 2989 ==============\n",
      "============== Pattern 2990 ==============\n",
      "============== Pattern 2991 ==============\n",
      "============== Pattern 2992 ==============\n",
      "============== Pattern 2993 ==============\n",
      "============== Pattern 2994 ==============\n",
      "============== Pattern 2995 ==============\n",
      "============== Pattern 2996 ==============\n",
      "============== Pattern 2997 ==============\n",
      "============== Pattern 2998 ==============\n",
      "============== Pattern 2999 ==============\n",
      "============== Pattern 3000 ==============\n",
      "============== Pattern 3001 ==============\n",
      "============== Pattern 3002 ==============\n",
      "============== Pattern 3003 ==============\n",
      "============== Pattern 3004 ==============\n",
      "============== Pattern 3005 ==============\n",
      "============== Pattern 3006 ==============\n",
      "============== Pattern 3007 ==============\n",
      "============== Pattern 3008 ==============\n",
      "============== Pattern 3009 ==============\n",
      "============== Pattern 3010 ==============\n",
      "============== Pattern 3011 ==============\n",
      "============== Pattern 3012 ==============\n",
      "============== Pattern 3013 ==============\n",
      "============== Pattern 3014 ==============\n",
      "============== Pattern 3015 ==============\n",
      "============== Pattern 3016 ==============\n",
      "============== Pattern 3017 ==============\n",
      "============== Pattern 3018 ==============\n",
      "============== Pattern 3019 ==============\n",
      "============== Pattern 3020 ==============\n",
      "============== Pattern 3021 ==============\n",
      "============== Pattern 3022 ==============\n",
      "============== Pattern 3023 ==============\n",
      "============== Pattern 3024 ==============\n",
      "============== Pattern 3025 ==============\n",
      "============== Pattern 3026 ==============\n",
      "============== Pattern 3027 ==============\n",
      "============== Pattern 3028 ==============\n",
      "============== Pattern 3029 ==============\n",
      "============== Pattern 3030 ==============\n",
      "============== Pattern 3031 ==============\n",
      "============== Pattern 3032 ==============\n",
      "============== Pattern 3033 ==============\n",
      "============== Pattern 3034 ==============\n",
      "============== Pattern 3035 ==============\n",
      "============== Pattern 3036 ==============\n",
      "============== Pattern 3037 ==============\n",
      "============== Pattern 3038 ==============\n",
      "============== Pattern 3039 ==============\n",
      "============== Pattern 3040 ==============\n",
      "============== Pattern 3041 ==============\n",
      "============== Pattern 3042 ==============\n",
      "============== Pattern 3043 ==============\n",
      "============== Pattern 3044 ==============\n",
      "============== Pattern 3045 ==============\n",
      "============== Pattern 3046 ==============\n",
      "============== Pattern 3047 ==============\n",
      "============== Pattern 3048 ==============\n",
      "============== Pattern 3049 ==============\n",
      "============== Pattern 3050 ==============\n",
      "============== Pattern 3051 ==============\n",
      "============== Pattern 3052 ==============\n",
      "============== Pattern 3053 ==============\n",
      "============== Pattern 3054 ==============\n",
      "============== Pattern 3055 ==============\n",
      "============== Pattern 3056 ==============\n",
      "============== Pattern 3057 ==============\n",
      "============== Pattern 3058 ==============\n",
      "============== Pattern 3059 ==============\n",
      "============== Pattern 3060 ==============\n",
      "============== Pattern 3061 ==============\n",
      "============== Pattern 3062 ==============\n",
      "============== Pattern 3063 ==============\n",
      "============== Pattern 3064 ==============\n",
      "============== Pattern 3065 ==============\n",
      "============== Pattern 3066 ==============\n",
      "============== Pattern 3067 ==============\n",
      "============== Pattern 3068 ==============\n",
      "============== Pattern 3069 ==============\n",
      "============== Pattern 3070 ==============\n",
      "============== Pattern 3071 ==============\n",
      "============== Pattern 3072 ==============\n",
      "============== Pattern 3073 ==============\n",
      "============== Pattern 3074 ==============\n",
      "============== Pattern 3075 ==============\n",
      "============== Pattern 3076 ==============\n",
      "============== Pattern 3077 ==============\n",
      "============== Pattern 3078 ==============\n",
      "============== Pattern 3079 ==============\n",
      "============== Pattern 3080 ==============\n",
      "============== Pattern 3081 ==============\n",
      "============== Pattern 3082 ==============\n",
      "============== Pattern 3083 ==============\n",
      "============== Pattern 3084 ==============\n",
      "============== Pattern 3085 ==============\n",
      "============== Pattern 3086 ==============\n",
      "============== Pattern 3087 ==============\n",
      "============== Pattern 3088 ==============\n",
      "============== Pattern 3089 ==============\n",
      "============== Pattern 3090 ==============\n",
      "============== Pattern 3091 ==============\n",
      "============== Pattern 3092 ==============\n",
      "============== Pattern 3093 ==============\n",
      "============== Pattern 3094 ==============\n",
      "============== Pattern 3095 ==============\n",
      "============== Pattern 3096 ==============\n",
      "============== Pattern 3097 ==============\n",
      "============== Pattern 3098 ==============\n",
      "============== Pattern 3099 ==============\n",
      "============== Pattern 3100 ==============\n",
      "============== Pattern 3101 ==============\n",
      "============== Pattern 3102 ==============\n",
      "============== Pattern 3103 ==============\n",
      "============== Pattern 3104 ==============\n",
      "============== Pattern 3105 ==============\n",
      "============== Pattern 3106 ==============\n",
      "============== Pattern 3107 ==============\n",
      "============== Pattern 3108 ==============\n",
      "============== Pattern 3109 ==============\n",
      "============== Pattern 3110 ==============\n",
      "============== Pattern 3111 ==============\n",
      "============== Pattern 3112 ==============\n",
      "============== Pattern 3113 ==============\n",
      "============== Pattern 3114 ==============\n",
      "============== Pattern 3115 ==============\n",
      "============== Pattern 3116 ==============\n",
      "============== Pattern 3117 ==============\n",
      "============== Pattern 3118 ==============\n",
      "============== Pattern 3119 ==============\n",
      "============== Pattern 3120 ==============\n",
      "============== Pattern 3121 ==============\n",
      "============== Pattern 3122 ==============\n",
      "============== Pattern 3123 ==============\n",
      "============== Pattern 3124 ==============\n",
      "============== Pattern 3125 ==============\n",
      "============== Pattern 3126 ==============\n",
      "============== Pattern 3127 ==============\n",
      "============== Pattern 3128 ==============\n",
      "============== Pattern 3129 ==============\n",
      "============== Pattern 3130 ==============\n",
      "============== Pattern 3131 ==============\n",
      "============== Pattern 3132 ==============\n",
      "============== Pattern 3133 ==============\n",
      "============== Pattern 3134 ==============\n",
      "============== Pattern 3135 ==============\n",
      "============== Pattern 3136 ==============\n",
      "============== Pattern 3137 ==============\n",
      "============== Pattern 3138 ==============\n",
      "============== Pattern 3139 ==============\n",
      "============== Pattern 3140 ==============\n",
      "============== Pattern 3141 ==============\n",
      "============== Pattern 3142 ==============\n",
      "============== Pattern 3143 ==============\n",
      "============== Pattern 3144 ==============\n",
      "============== Pattern 3145 ==============\n",
      "============== Pattern 3146 ==============\n",
      "============== Pattern 3147 ==============\n",
      "============== Pattern 3148 ==============\n",
      "============== Pattern 3149 ==============\n",
      "============== Pattern 3150 ==============\n",
      "============== Pattern 3151 ==============\n",
      "============== Pattern 3152 ==============\n",
      "============== Pattern 3153 ==============\n",
      "============== Pattern 3154 ==============\n",
      "============== Pattern 3155 ==============\n",
      "============== Pattern 3156 ==============\n",
      "============== Pattern 3157 ==============\n",
      "============== Pattern 3158 ==============\n",
      "============== Pattern 3159 ==============\n",
      "============== Pattern 3160 ==============\n",
      "============== Pattern 3161 ==============\n",
      "============== Pattern 3162 ==============\n",
      "============== Pattern 3163 ==============\n",
      "============== Pattern 3164 ==============\n",
      "============== Pattern 3165 ==============\n",
      "============== Pattern 3166 ==============\n",
      "============== Pattern 3167 ==============\n",
      "============== Pattern 3168 ==============\n",
      "============== Pattern 3169 ==============\n",
      "============== Pattern 3170 ==============\n",
      "============== Pattern 3171 ==============\n",
      "============== Pattern 3172 ==============\n",
      "============== Pattern 3173 ==============\n",
      "============== Pattern 3174 ==============\n",
      "============== Pattern 3175 ==============\n",
      "============== Pattern 3176 ==============\n",
      "============== Pattern 3177 ==============\n",
      "============== Pattern 3178 ==============\n",
      "============== Pattern 3179 ==============\n",
      "============== Pattern 3180 ==============\n",
      "============== Pattern 3181 ==============\n",
      "============== Pattern 3182 ==============\n",
      "============== Pattern 3183 ==============\n",
      "============== Pattern 3184 ==============\n",
      "============== Pattern 3185 ==============\n",
      "============== Pattern 3186 ==============\n",
      "============== Pattern 3187 ==============\n",
      "============== Pattern 3188 ==============\n",
      "============== Pattern 3189 ==============\n",
      "============== Pattern 3190 ==============\n",
      "============== Pattern 3191 ==============\n",
      "============== Pattern 3192 ==============\n",
      "============== Pattern 3193 ==============\n",
      "============== Pattern 3194 ==============\n",
      "============== Pattern 3195 ==============\n",
      "============== Pattern 3196 ==============\n",
      "============== Pattern 3197 ==============\n",
      "============== Pattern 3198 ==============\n",
      "============== Pattern 3199 ==============\n",
      "============== Pattern 3200 ==============\n",
      "============== Pattern 3201 ==============\n",
      "============== Pattern 3202 ==============\n",
      "============== Pattern 3203 ==============\n",
      "============== Pattern 3204 ==============\n",
      "============== Pattern 3205 ==============\n",
      "============== Pattern 3206 ==============\n",
      "============== Pattern 3207 ==============\n",
      "============== Pattern 3208 ==============\n",
      "============== Pattern 3209 ==============\n",
      "============== Pattern 3210 ==============\n",
      "============== Pattern 3211 ==============\n",
      "============== Pattern 3212 ==============\n",
      "============== Pattern 3213 ==============\n",
      "============== Pattern 3214 ==============\n",
      "============== Pattern 3215 ==============\n",
      "============== Pattern 3216 ==============\n",
      "============== Pattern 3217 ==============\n",
      "============== Pattern 3218 ==============\n",
      "============== Pattern 3219 ==============\n",
      "============== Pattern 3220 ==============\n",
      "============== Pattern 3221 ==============\n",
      "============== Pattern 3222 ==============\n",
      "============== Pattern 3223 ==============\n",
      "============== Pattern 3224 ==============\n",
      "============== Pattern 3225 ==============\n",
      "============== Pattern 3226 ==============\n",
      "============== Pattern 3227 ==============\n",
      "============== Pattern 3228 ==============\n",
      "============== Pattern 3229 ==============\n",
      "============== Pattern 3230 ==============\n",
      "============== Pattern 3231 ==============\n",
      "============== Pattern 3232 ==============\n",
      "============== Pattern 3233 ==============\n",
      "============== Pattern 3234 ==============\n",
      "============== Pattern 3235 ==============\n",
      "============== Pattern 3236 ==============\n",
      "============== Pattern 3237 ==============\n",
      "============== Pattern 3238 ==============\n",
      "============== Pattern 3239 ==============\n",
      "============== Pattern 3240 ==============\n",
      "============== Pattern 3241 ==============\n",
      "============== Pattern 3242 ==============\n",
      "============== Pattern 3243 ==============\n",
      "============== Pattern 3244 ==============\n",
      "============== Pattern 3245 ==============\n",
      "============== Pattern 3246 ==============\n",
      "============== Pattern 3247 ==============\n",
      "============== Pattern 3248 ==============\n",
      "============== Pattern 3249 ==============\n",
      "============== Pattern 3250 ==============\n",
      "============== Pattern 3251 ==============\n",
      "============== Pattern 3252 ==============\n",
      "============== Pattern 3253 ==============\n",
      "============== Pattern 3254 ==============\n",
      "============== Pattern 3255 ==============\n",
      "============== Pattern 3256 ==============\n",
      "============== Pattern 3257 ==============\n",
      "============== Pattern 3258 ==============\n",
      "============== Pattern 3259 ==============\n",
      "============== Pattern 3260 ==============\n",
      "============== Pattern 3261 ==============\n",
      "============== Pattern 3262 ==============\n",
      "============== Pattern 3263 ==============\n",
      "============== Pattern 3264 ==============\n",
      "============== Pattern 3265 ==============\n",
      "============== Pattern 3266 ==============\n",
      "============== Pattern 3267 ==============\n",
      "============== Pattern 3268 ==============\n",
      "============== Pattern 3269 ==============\n",
      "============== Pattern 3270 ==============\n",
      "============== Pattern 3271 ==============\n",
      "============== Pattern 3272 ==============\n",
      "============== Pattern 3273 ==============\n",
      "============== Pattern 3274 ==============\n",
      "============== Pattern 3275 ==============\n",
      "============== Pattern 3276 ==============\n",
      "============== Pattern 3277 ==============\n",
      "============== Pattern 3278 ==============\n",
      "============== Pattern 3279 ==============\n",
      "============== Pattern 3280 ==============\n",
      "============== Pattern 3281 ==============\n",
      "============== Pattern 3282 ==============\n",
      "============== Pattern 3283 ==============\n",
      "============== Pattern 3284 ==============\n",
      "============== Pattern 3285 ==============\n",
      "============== Pattern 3286 ==============\n",
      "============== Pattern 3287 ==============\n",
      "============== Pattern 3288 ==============\n",
      "============== Pattern 3289 ==============\n",
      "============== Pattern 3290 ==============\n",
      "============== Pattern 3291 ==============\n",
      "============== Pattern 3292 ==============\n",
      "============== Pattern 3293 ==============\n",
      "============== Pattern 3294 ==============\n",
      "============== Pattern 3295 ==============\n",
      "============== Pattern 3296 ==============\n",
      "============== Pattern 3297 ==============\n",
      "============== Pattern 3298 ==============\n",
      "============== Pattern 3299 ==============\n",
      "============== Pattern 3300 ==============\n",
      "============== Pattern 3301 ==============\n",
      "============== Pattern 3302 ==============\n",
      "============== Pattern 3303 ==============\n",
      "============== Pattern 3304 ==============\n",
      "============== Pattern 3305 ==============\n",
      "============== Pattern 3306 ==============\n",
      "============== Pattern 3307 ==============\n",
      "============== Pattern 3308 ==============\n",
      "============== Pattern 3309 ==============\n",
      "============== Pattern 3310 ==============\n",
      "============== Pattern 3311 ==============\n",
      "============== Pattern 3312 ==============\n",
      "============== Pattern 3313 ==============\n",
      "============== Pattern 3314 ==============\n",
      "============== Pattern 3315 ==============\n",
      "============== Pattern 3316 ==============\n",
      "============== Pattern 3317 ==============\n",
      "============== Pattern 3318 ==============\n",
      "============== Pattern 3319 ==============\n",
      "============== Pattern 3320 ==============\n",
      "============== Pattern 3321 ==============\n",
      "============== Pattern 3322 ==============\n",
      "============== Pattern 3323 ==============\n",
      "============== Pattern 3324 ==============\n",
      "============== Pattern 3325 ==============\n",
      "============== Pattern 3326 ==============\n",
      "============== Pattern 3327 ==============\n",
      "============== Pattern 3328 ==============\n",
      "============== Pattern 3329 ==============\n",
      "============== Pattern 3330 ==============\n",
      "============== Pattern 3331 ==============\n",
      "============== Pattern 3332 ==============\n",
      "============== Pattern 3333 ==============\n",
      "============== Pattern 3334 ==============\n",
      "============== Pattern 3335 ==============\n",
      "============== Pattern 3336 ==============\n",
      "============== Pattern 3337 ==============\n",
      "============== Pattern 3338 ==============\n",
      "============== Pattern 3339 ==============\n",
      "============== Pattern 3340 ==============\n",
      "============== Pattern 3341 ==============\n",
      "============== Pattern 3342 ==============\n",
      "============== Pattern 3343 ==============\n",
      "============== Pattern 3344 ==============\n",
      "============== Pattern 3345 ==============\n",
      "============== Pattern 3346 ==============\n",
      "============== Pattern 3347 ==============\n",
      "============== Pattern 3348 ==============\n",
      "============== Pattern 3349 ==============\n",
      "============== Pattern 3350 ==============\n",
      "============== Pattern 3351 ==============\n",
      "============== Pattern 3352 ==============\n",
      "============== Pattern 3353 ==============\n",
      "============== Pattern 3354 ==============\n",
      "============== Pattern 3355 ==============\n",
      "============== Pattern 3356 ==============\n",
      "============== Pattern 3357 ==============\n",
      "============== Pattern 3358 ==============\n",
      "============== Pattern 3359 ==============\n",
      "============== Pattern 3360 ==============\n",
      "============== Pattern 3361 ==============\n",
      "============== Pattern 3362 ==============\n",
      "============== Pattern 3363 ==============\n",
      "============== Pattern 3364 ==============\n",
      "============== Pattern 3365 ==============\n",
      "============== Pattern 3366 ==============\n",
      "============== Pattern 3367 ==============\n",
      "============== Pattern 3368 ==============\n",
      "============== Pattern 3369 ==============\n",
      "============== Pattern 3370 ==============\n",
      "============== Pattern 3371 ==============\n",
      "============== Pattern 3372 ==============\n",
      "============== Pattern 3373 ==============\n",
      "============== Pattern 3374 ==============\n",
      "============== Pattern 3375 ==============\n",
      "============== Pattern 3376 ==============\n",
      "============== Pattern 3377 ==============\n",
      "============== Pattern 3378 ==============\n",
      "============== Pattern 3379 ==============\n",
      "============== Pattern 3380 ==============\n",
      "============== Pattern 3381 ==============\n",
      "============== Pattern 3382 ==============\n",
      "============== Pattern 3383 ==============\n",
      "============== Pattern 3384 ==============\n",
      "============== Pattern 3385 ==============\n",
      "============== Pattern 3386 ==============\n",
      "============== Pattern 3387 ==============\n",
      "============== Pattern 3388 ==============\n",
      "============== Pattern 3389 ==============\n",
      "============== Pattern 3390 ==============\n",
      "============== Pattern 3391 ==============\n",
      "============== Pattern 3392 ==============\n",
      "============== Pattern 3393 ==============\n",
      "============== Pattern 3394 ==============\n",
      "============== Pattern 3395 ==============\n",
      "============== Pattern 3396 ==============\n",
      "============== Pattern 3397 ==============\n",
      "============== Pattern 3398 ==============\n",
      "============== Pattern 3399 ==============\n",
      "============== Pattern 3400 ==============\n",
      "============== Pattern 3401 ==============\n",
      "============== Pattern 3402 ==============\n",
      "============== Pattern 3403 ==============\n",
      "============== Pattern 3404 ==============\n",
      "============== Pattern 3405 ==============\n",
      "============== Pattern 3406 ==============\n",
      "============== Pattern 3407 ==============\n",
      "============== Pattern 3408 ==============\n",
      "============== Pattern 3409 ==============\n",
      "============== Pattern 3410 ==============\n",
      "============== Pattern 3411 ==============\n",
      "============== Pattern 3412 ==============\n",
      "============== Pattern 3413 ==============\n",
      "============== Pattern 3414 ==============\n",
      "============== Pattern 3415 ==============\n",
      "============== Pattern 3416 ==============\n",
      "============== Pattern 3417 ==============\n",
      "============== Pattern 3418 ==============\n",
      "============== Pattern 3419 ==============\n",
      "============== Pattern 3420 ==============\n",
      "============== Pattern 3421 ==============\n",
      "============== Pattern 3422 ==============\n",
      "============== Pattern 3423 ==============\n",
      "============== Pattern 3424 ==============\n",
      "============== Pattern 3425 ==============\n",
      "============== Pattern 3426 ==============\n",
      "============== Pattern 3427 ==============\n",
      "============== Pattern 3428 ==============\n",
      "============== Pattern 3429 ==============\n",
      "============== Pattern 3430 ==============\n",
      "============== Pattern 3431 ==============\n",
      "============== Pattern 3432 ==============\n",
      "============== Pattern 3433 ==============\n",
      "============== Pattern 3434 ==============\n",
      "============== Pattern 3435 ==============\n",
      "============== Pattern 3436 ==============\n",
      "============== Pattern 3437 ==============\n",
      "============== Pattern 3438 ==============\n",
      "============== Pattern 3439 ==============\n",
      "============== Pattern 3440 ==============\n",
      "============== Pattern 3441 ==============\n",
      "============== Pattern 3442 ==============\n",
      "============== Pattern 3443 ==============\n",
      "============== Pattern 3444 ==============\n",
      "============== Pattern 3445 ==============\n",
      "============== Pattern 3446 ==============\n",
      "============== Pattern 3447 ==============\n",
      "============== Pattern 3448 ==============\n",
      "============== Pattern 3449 ==============\n",
      "============== Pattern 3450 ==============\n",
      "============== Pattern 3451 ==============\n",
      "============== Pattern 3452 ==============\n",
      "============== Pattern 3453 ==============\n",
      "============== Pattern 3454 ==============\n",
      "============== Pattern 3455 ==============\n",
      "============== Pattern 3456 ==============\n",
      "============== Pattern 3457 ==============\n",
      "============== Pattern 3458 ==============\n",
      "============== Pattern 3459 ==============\n",
      "============== Pattern 3460 ==============\n",
      "============== Pattern 3461 ==============\n",
      "============== Pattern 3462 ==============\n",
      "============== Pattern 3463 ==============\n",
      "============== Pattern 3464 ==============\n",
      "============== Pattern 3465 ==============\n",
      "============== Pattern 3466 ==============\n",
      "============== Pattern 3467 ==============\n",
      "============== Pattern 3468 ==============\n",
      "============== Pattern 3469 ==============\n",
      "============== Pattern 3470 ==============\n",
      "============== Pattern 3471 ==============\n",
      "============== Pattern 3472 ==============\n",
      "============== Pattern 3473 ==============\n",
      "============== Pattern 3474 ==============\n",
      "============== Pattern 3475 ==============\n",
      "============== Pattern 3476 ==============\n",
      "============== Pattern 3477 ==============\n",
      "============== Pattern 3478 ==============\n",
      "============== Pattern 3479 ==============\n",
      "============== Pattern 3480 ==============\n",
      "============== Pattern 3481 ==============\n",
      "============== Pattern 3482 ==============\n",
      "============== Pattern 3483 ==============\n",
      "============== Pattern 3484 ==============\n",
      "============== Pattern 3485 ==============\n",
      "============== Pattern 3486 ==============\n",
      "============== Pattern 3487 ==============\n",
      "============== Pattern 3488 ==============\n",
      "============== Pattern 3489 ==============\n",
      "============== Pattern 3490 ==============\n",
      "============== Pattern 3491 ==============\n",
      "============== Pattern 3492 ==============\n",
      "============== Pattern 3493 ==============\n",
      "============== Pattern 3494 ==============\n",
      "============== Pattern 3495 ==============\n",
      "============== Pattern 3496 ==============\n",
      "============== Pattern 3497 ==============\n",
      "============== Pattern 3498 ==============\n",
      "============== Pattern 3499 ==============\n",
      "============== Pattern 3500 ==============\n",
      "============== Pattern 3501 ==============\n",
      "============== Pattern 3502 ==============\n",
      "============== Pattern 3503 ==============\n",
      "============== Pattern 3504 ==============\n",
      "============== Pattern 3505 ==============\n",
      "============== Pattern 3506 ==============\n",
      "============== Pattern 3507 ==============\n",
      "============== Pattern 3508 ==============\n",
      "============== Pattern 3509 ==============\n",
      "============== Pattern 3510 ==============\n",
      "============== Pattern 3511 ==============\n",
      "============== Pattern 3512 ==============\n",
      "============== Pattern 3513 ==============\n",
      "============== Pattern 3514 ==============\n",
      "============== Pattern 3515 ==============\n",
      "============== Pattern 3516 ==============\n",
      "============== Pattern 3517 ==============\n",
      "============== Pattern 3518 ==============\n",
      "============== Pattern 3519 ==============\n",
      "============== Pattern 3520 ==============\n",
      "============== Pattern 3521 ==============\n",
      "============== Pattern 3522 ==============\n",
      "============== Pattern 3523 ==============\n",
      "============== Pattern 3524 ==============\n",
      "============== Pattern 3525 ==============\n",
      "============== Pattern 3526 ==============\n",
      "============== Pattern 3527 ==============\n",
      "============== Pattern 3528 ==============\n",
      "============== Pattern 3529 ==============\n",
      "============== Pattern 3530 ==============\n",
      "============== Pattern 3531 ==============\n",
      "============== Pattern 3532 ==============\n",
      "============== Pattern 3533 ==============\n",
      "============== Pattern 3534 ==============\n",
      "============== Pattern 3535 ==============\n",
      "============== Pattern 3536 ==============\n",
      "============== Pattern 3537 ==============\n",
      "============== Pattern 3538 ==============\n",
      "============== Pattern 3539 ==============\n",
      "============== Pattern 3540 ==============\n",
      "============== Pattern 3541 ==============\n",
      "============== Pattern 3542 ==============\n",
      "============== Pattern 3543 ==============\n",
      "============== Pattern 3544 ==============\n",
      "============== Pattern 3545 ==============\n",
      "============== Pattern 3546 ==============\n",
      "============== Pattern 3547 ==============\n",
      "============== Pattern 3548 ==============\n",
      "============== Pattern 3549 ==============\n",
      "============== Pattern 3550 ==============\n",
      "============== Pattern 3551 ==============\n",
      "============== Pattern 3552 ==============\n",
      "============== Pattern 3553 ==============\n",
      "============== Pattern 3554 ==============\n",
      "============== Pattern 3555 ==============\n",
      "============== Pattern 3556 ==============\n",
      "============== Pattern 3557 ==============\n",
      "============== Pattern 3558 ==============\n",
      "============== Pattern 3559 ==============\n",
      "============== Pattern 3560 ==============\n",
      "============== Pattern 3561 ==============\n",
      "============== Pattern 3562 ==============\n",
      "============== Pattern 3563 ==============\n",
      "============== Pattern 3564 ==============\n",
      "============== Pattern 3565 ==============\n",
      "============== Pattern 3566 ==============\n",
      "============== Pattern 3567 ==============\n",
      "============== Pattern 3568 ==============\n",
      "============== Pattern 3569 ==============\n",
      "============== Pattern 3570 ==============\n",
      "============== Pattern 3571 ==============\n",
      "============== Pattern 3572 ==============\n",
      "============== Pattern 3573 ==============\n",
      "============== Pattern 3574 ==============\n",
      "============== Pattern 3575 ==============\n",
      "============== Pattern 3576 ==============\n",
      "============== Pattern 3577 ==============\n",
      "============== Pattern 3578 ==============\n",
      "============== Pattern 3579 ==============\n",
      "============== Pattern 3580 ==============\n",
      "============== Pattern 3581 ==============\n",
      "============== Pattern 3582 ==============\n",
      "============== Pattern 3583 ==============\n",
      "============== Pattern 3584 ==============\n",
      "============== Pattern 3585 ==============\n",
      "============== Pattern 3586 ==============\n",
      "============== Pattern 3587 ==============\n",
      "============== Pattern 3588 ==============\n",
      "============== Pattern 3589 ==============\n",
      "============== Pattern 3590 ==============\n",
      "============== Pattern 3591 ==============\n",
      "============== Pattern 3592 ==============\n",
      "============== Pattern 3593 ==============\n",
      "============== Pattern 3594 ==============\n",
      "============== Pattern 3595 ==============\n",
      "============== Pattern 3596 ==============\n",
      "============== Pattern 3597 ==============\n",
      "============== Pattern 3598 ==============\n",
      "============== Pattern 3599 ==============\n",
      "============== Pattern 3600 ==============\n",
      "============== Pattern 3601 ==============\n",
      "============== Pattern 3602 ==============\n",
      "============== Pattern 3603 ==============\n",
      "============== Pattern 3604 ==============\n",
      "============== Pattern 3605 ==============\n",
      "============== Pattern 3606 ==============\n",
      "============== Pattern 3607 ==============\n",
      "============== Pattern 3608 ==============\n",
      "============== Pattern 3609 ==============\n",
      "============== Pattern 3610 ==============\n",
      "============== Pattern 3611 ==============\n",
      "============== Pattern 3612 ==============\n",
      "============== Pattern 3613 ==============\n",
      "============== Pattern 3614 ==============\n",
      "============== Pattern 3615 ==============\n",
      "============== Pattern 3616 ==============\n",
      "============== Pattern 3617 ==============\n",
      "============== Pattern 3618 ==============\n",
      "============== Pattern 3619 ==============\n",
      "============== Pattern 3620 ==============\n",
      "============== Pattern 3621 ==============\n",
      "============== Pattern 3622 ==============\n",
      "============== Pattern 3623 ==============\n",
      "============== Pattern 3624 ==============\n",
      "============== Pattern 3625 ==============\n",
      "============== Pattern 3626 ==============\n",
      "============== Pattern 3627 ==============\n",
      "============== Pattern 3628 ==============\n",
      "============== Pattern 3629 ==============\n",
      "============== Pattern 3630 ==============\n",
      "============== Pattern 3631 ==============\n",
      "============== Pattern 3632 ==============\n",
      "============== Pattern 3633 ==============\n",
      "============== Pattern 3634 ==============\n",
      "============== Pattern 3635 ==============\n",
      "============== Pattern 3636 ==============\n",
      "============== Pattern 3637 ==============\n",
      "============== Pattern 3638 ==============\n",
      "============== Pattern 3639 ==============\n",
      "============== Pattern 3640 ==============\n",
      "============== Pattern 3641 ==============\n",
      "============== Pattern 3642 ==============\n",
      "============== Pattern 3643 ==============\n",
      "============== Pattern 3644 ==============\n",
      "============== Pattern 3645 ==============\n",
      "============== Pattern 3646 ==============\n",
      "============== Pattern 3647 ==============\n",
      "============== Pattern 3648 ==============\n",
      "============== Pattern 3649 ==============\n",
      "============== Pattern 3650 ==============\n",
      "============== Pattern 3651 ==============\n",
      "============== Pattern 3652 ==============\n",
      "============== Pattern 3653 ==============\n",
      "============== Pattern 3654 ==============\n",
      "============== Pattern 3655 ==============\n",
      "============== Pattern 3656 ==============\n",
      "============== Pattern 3657 ==============\n",
      "============== Pattern 3658 ==============\n",
      "============== Pattern 3659 ==============\n",
      "============== Pattern 3660 ==============\n",
      "============== Pattern 3661 ==============\n",
      "============== Pattern 3662 ==============\n",
      "============== Pattern 3663 ==============\n",
      "============== Pattern 3664 ==============\n",
      "============== Pattern 3665 ==============\n",
      "============== Pattern 3666 ==============\n",
      "============== Pattern 3667 ==============\n",
      "============== Pattern 3668 ==============\n",
      "============== Pattern 3669 ==============\n",
      "============== Pattern 3670 ==============\n",
      "============== Pattern 3671 ==============\n",
      "============== Pattern 3672 ==============\n",
      "============== Pattern 3673 ==============\n",
      "============== Pattern 3674 ==============\n",
      "============== Pattern 3675 ==============\n",
      "============== Pattern 3676 ==============\n",
      "============== Pattern 3677 ==============\n",
      "============== Pattern 3678 ==============\n",
      "============== Pattern 3679 ==============\n",
      "============== Pattern 3680 ==============\n",
      "============== Pattern 3681 ==============\n",
      "============== Pattern 3682 ==============\n",
      "============== Pattern 3683 ==============\n",
      "============== Pattern 3684 ==============\n",
      "============== Pattern 3685 ==============\n",
      "============== Pattern 3686 ==============\n",
      "============== Pattern 3687 ==============\n",
      "============== Pattern 3688 ==============\n",
      "============== Pattern 3689 ==============\n",
      "============== Pattern 3690 ==============\n",
      "============== Pattern 3691 ==============\n",
      "============== Pattern 3692 ==============\n",
      "============== Pattern 3693 ==============\n",
      "============== Pattern 3694 ==============\n",
      "============== Pattern 3695 ==============\n",
      "============== Pattern 3696 ==============\n",
      "============== Pattern 3697 ==============\n",
      "============== Pattern 3698 ==============\n",
      "============== Pattern 3699 ==============\n",
      "============== Pattern 3700 ==============\n",
      "============== Pattern 3701 ==============\n",
      "============== Pattern 3702 ==============\n",
      "============== Pattern 3703 ==============\n",
      "============== Pattern 3704 ==============\n",
      "============== Pattern 3705 ==============\n",
      "============== Pattern 3706 ==============\n",
      "============== Pattern 3707 ==============\n",
      "============== Pattern 3708 ==============\n",
      "============== Pattern 3709 ==============\n",
      "============== Pattern 3710 ==============\n",
      "============== Pattern 3711 ==============\n",
      "============== Pattern 3712 ==============\n",
      "============== Pattern 3713 ==============\n",
      "============== Pattern 3714 ==============\n",
      "============== Pattern 3715 ==============\n",
      "============== Pattern 3716 ==============\n",
      "============== Pattern 3717 ==============\n",
      "============== Pattern 3718 ==============\n",
      "============== Pattern 3719 ==============\n",
      "============== Pattern 3720 ==============\n",
      "============== Pattern 3721 ==============\n",
      "============== Pattern 3722 ==============\n",
      "============== Pattern 3723 ==============\n",
      "============== Pattern 3724 ==============\n",
      "============== Pattern 3725 ==============\n",
      "============== Pattern 3726 ==============\n",
      "============== Pattern 3727 ==============\n",
      "============== Pattern 3728 ==============\n",
      "============== Pattern 3729 ==============\n",
      "============== Pattern 3730 ==============\n",
      "============== Pattern 3731 ==============\n",
      "============== Pattern 3732 ==============\n",
      "============== Pattern 3733 ==============\n",
      "============== Pattern 3734 ==============\n",
      "============== Pattern 3735 ==============\n",
      "============== Pattern 3736 ==============\n",
      "============== Pattern 3737 ==============\n",
      "============== Pattern 3738 ==============\n",
      "============== Pattern 3739 ==============\n",
      "============== Pattern 3740 ==============\n",
      "============== Pattern 3741 ==============\n",
      "============== Pattern 3742 ==============\n",
      "============== Pattern 3743 ==============\n",
      "============== Pattern 3744 ==============\n",
      "============== Pattern 3745 ==============\n",
      "============== Pattern 3746 ==============\n",
      "============== Pattern 3747 ==============\n",
      "============== Pattern 3748 ==============\n",
      "============== Pattern 3749 ==============\n",
      "============== Pattern 3750 ==============\n",
      "============== Pattern 3751 ==============\n",
      "============== Pattern 3752 ==============\n",
      "============== Pattern 3753 ==============\n",
      "============== Pattern 3754 ==============\n",
      "============== Pattern 3755 ==============\n",
      "============== Pattern 3756 ==============\n",
      "============== Pattern 3757 ==============\n",
      "============== Pattern 3758 ==============\n",
      "============== Pattern 3759 ==============\n",
      "============== Pattern 3760 ==============\n",
      "============== Pattern 3761 ==============\n",
      "============== Pattern 3762 ==============\n",
      "============== Pattern 3763 ==============\n",
      "============== Pattern 3764 ==============\n",
      "============== Pattern 3765 ==============\n",
      "============== Pattern 3766 ==============\n",
      "============== Pattern 3767 ==============\n",
      "============== Pattern 3768 ==============\n",
      "============== Pattern 3769 ==============\n",
      "============== Pattern 3770 ==============\n",
      "============== Pattern 3771 ==============\n",
      "============== Pattern 3772 ==============\n",
      "============== Pattern 3773 ==============\n",
      "============== Pattern 3774 ==============\n",
      "============== Pattern 3775 ==============\n",
      "============== Pattern 3776 ==============\n",
      "============== Pattern 3777 ==============\n",
      "============== Pattern 3778 ==============\n",
      "============== Pattern 3779 ==============\n",
      "============== Pattern 3780 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 3781 ==============\n",
      "============== Pattern 3782 ==============\n",
      "============== Pattern 3783 ==============\n",
      "============== Pattern 3784 ==============\n",
      "============== Pattern 3785 ==============\n",
      "============== Pattern 3786 ==============\n",
      "============== Pattern 3787 ==============\n",
      "============== Pattern 3788 ==============\n",
      "============== Pattern 3789 ==============\n",
      "============== Pattern 3790 ==============\n",
      "============== Pattern 3791 ==============\n",
      "============== Pattern 3792 ==============\n",
      "============== Pattern 3793 ==============\n",
      "============== Pattern 3794 ==============\n",
      "============== Pattern 3795 ==============\n",
      "============== Pattern 3796 ==============\n",
      "============== Pattern 3797 ==============\n",
      "============== Pattern 3798 ==============\n",
      "============== Pattern 3799 ==============\n",
      "============== Pattern 3800 ==============\n",
      "============== Pattern 3801 ==============\n",
      "============== Pattern 3802 ==============\n",
      "============== Pattern 3803 ==============\n",
      "============== Pattern 3804 ==============\n",
      "============== Pattern 3805 ==============\n",
      "============== Pattern 3806 ==============\n",
      "============== Pattern 3807 ==============\n",
      "============== Pattern 3808 ==============\n",
      "============== Pattern 3809 ==============\n",
      "============== Pattern 3810 ==============\n",
      "============== Pattern 3811 ==============\n",
      "============== Pattern 3812 ==============\n",
      "============== Pattern 3813 ==============\n",
      "============== Pattern 3814 ==============\n",
      "============== Pattern 3815 ==============\n",
      "============== Pattern 3816 ==============\n",
      "============== Pattern 3817 ==============\n",
      "============== Pattern 3818 ==============\n",
      "============== Pattern 3819 ==============\n",
      "============== Pattern 3820 ==============\n",
      "============== Pattern 3821 ==============\n",
      "============== Pattern 3822 ==============\n",
      "============== Pattern 3823 ==============\n",
      "============== Pattern 3824 ==============\n",
      "============== Pattern 3825 ==============\n",
      "============== Pattern 3826 ==============\n",
      "============== Pattern 3827 ==============\n",
      "============== Pattern 3828 ==============\n",
      "============== Pattern 3829 ==============\n",
      "============== Pattern 3830 ==============\n",
      "============== Pattern 3831 ==============\n",
      "============== Pattern 3832 ==============\n",
      "============== Pattern 3833 ==============\n",
      "============== Pattern 3834 ==============\n",
      "============== Pattern 3835 ==============\n",
      "============== Pattern 3836 ==============\n",
      "============== Pattern 3837 ==============\n",
      "============== Pattern 3838 ==============\n",
      "============== Pattern 3839 ==============\n",
      "============== Pattern 3840 ==============\n",
      "============== Pattern 3841 ==============\n",
      "============== Pattern 3842 ==============\n",
      "============== Pattern 3843 ==============\n",
      "============== Pattern 3844 ==============\n",
      "============== Pattern 3845 ==============\n",
      "============== Pattern 3846 ==============\n",
      "============== Pattern 3847 ==============\n",
      "============== Pattern 3848 ==============\n",
      "============== Pattern 3849 ==============\n",
      "============== Pattern 3850 ==============\n",
      "============== Pattern 3851 ==============\n",
      "============== Pattern 3852 ==============\n",
      "============== Pattern 3853 ==============\n",
      "============== Pattern 3854 ==============\n",
      "============== Pattern 3855 ==============\n",
      "============== Pattern 3856 ==============\n",
      "============== Pattern 3857 ==============\n",
      "============== Pattern 3858 ==============\n",
      "============== Pattern 3859 ==============\n",
      "============== Pattern 3860 ==============\n",
      "============== Pattern 3861 ==============\n",
      "============== Pattern 3862 ==============\n",
      "============== Pattern 3863 ==============\n",
      "============== Pattern 3864 ==============\n",
      "============== Pattern 3865 ==============\n",
      "============== Pattern 3866 ==============\n",
      "============== Pattern 3867 ==============\n",
      "============== Pattern 3868 ==============\n",
      "============== Pattern 3869 ==============\n",
      "============== Pattern 3870 ==============\n",
      "============== Pattern 3871 ==============\n",
      "============== Pattern 3872 ==============\n",
      "============== Pattern 3873 ==============\n",
      "============== Pattern 3874 ==============\n",
      "============== Pattern 3875 ==============\n",
      "============== Pattern 3876 ==============\n",
      "============== Pattern 3877 ==============\n",
      "============== Pattern 3878 ==============\n",
      "============== Pattern 3879 ==============\n",
      "============== Pattern 3880 ==============\n",
      "============== Pattern 3881 ==============\n",
      "============== Pattern 3882 ==============\n",
      "============== Pattern 3883 ==============\n",
      "============== Pattern 3884 ==============\n",
      "============== Pattern 3885 ==============\n",
      "============== Pattern 3886 ==============\n",
      "============== Pattern 3887 ==============\n",
      "============== Pattern 3888 ==============\n",
      "============== Pattern 3889 ==============\n",
      "============== Pattern 3890 ==============\n",
      "============== Pattern 3891 ==============\n",
      "============== Pattern 3892 ==============\n",
      "============== Pattern 3893 ==============\n",
      "============== Pattern 3894 ==============\n",
      "============== Pattern 3895 ==============\n",
      "============== Pattern 3896 ==============\n",
      "============== Pattern 3897 ==============\n",
      "============== Pattern 3898 ==============\n",
      "============== Pattern 3899 ==============\n",
      "============== Pattern 3900 ==============\n",
      "============== Pattern 3901 ==============\n",
      "============== Pattern 3902 ==============\n",
      "============== Pattern 3903 ==============\n",
      "============== Pattern 3904 ==============\n",
      "============== Pattern 3905 ==============\n",
      "============== Pattern 3906 ==============\n",
      "============== Pattern 3907 ==============\n",
      "============== Pattern 3908 ==============\n",
      "============== Pattern 3909 ==============\n",
      "============== Pattern 3910 ==============\n",
      "============== Pattern 3911 ==============\n",
      "============== Pattern 3912 ==============\n",
      "============== Pattern 3913 ==============\n",
      "============== Pattern 3914 ==============\n",
      "============== Pattern 3915 ==============\n",
      "============== Pattern 3916 ==============\n",
      "============== Pattern 3917 ==============\n",
      "============== Pattern 3918 ==============\n",
      "============== Pattern 3919 ==============\n",
      "============== Pattern 3920 ==============\n",
      "============== Pattern 3921 ==============\n",
      "============== Pattern 3922 ==============\n",
      "============== Pattern 3923 ==============\n",
      "============== Pattern 3924 ==============\n",
      "============== Pattern 3925 ==============\n",
      "============== Pattern 3926 ==============\n",
      "============== Pattern 3927 ==============\n",
      "============== Pattern 3928 ==============\n",
      "============== Pattern 3929 ==============\n",
      "============== Pattern 3930 ==============\n",
      "============== Pattern 3931 ==============\n",
      "============== Pattern 3932 ==============\n",
      "============== Pattern 3933 ==============\n",
      "============== Pattern 3934 ==============\n",
      "============== Pattern 3935 ==============\n",
      "============== Pattern 3936 ==============\n",
      "============== Pattern 3937 ==============\n",
      "============== Pattern 3938 ==============\n",
      "============== Pattern 3939 ==============\n",
      "============== Pattern 3940 ==============\n",
      "============== Pattern 3941 ==============\n",
      "============== Pattern 3942 ==============\n",
      "============== Pattern 3943 ==============\n",
      "============== Pattern 3944 ==============\n",
      "============== Pattern 3945 ==============\n",
      "============== Pattern 3946 ==============\n",
      "============== Pattern 3947 ==============\n",
      "============== Pattern 3948 ==============\n",
      "============== Pattern 3949 ==============\n",
      "============== Pattern 3950 ==============\n",
      "============== Pattern 3951 ==============\n",
      "============== Pattern 3952 ==============\n",
      "============== Pattern 3953 ==============\n",
      "============== Pattern 3954 ==============\n",
      "============== Pattern 3955 ==============\n",
      "============== Pattern 3956 ==============\n",
      "============== Pattern 3957 ==============\n",
      "============== Pattern 3958 ==============\n",
      "============== Pattern 3959 ==============\n",
      "============== Pattern 3960 ==============\n",
      "============== Pattern 3961 ==============\n",
      "============== Pattern 3962 ==============\n",
      "============== Pattern 3963 ==============\n",
      "============== Pattern 3964 ==============\n",
      "============== Pattern 3965 ==============\n",
      "============== Pattern 3966 ==============\n",
      "============== Pattern 3967 ==============\n",
      "============== Pattern 3968 ==============\n",
      "============== Pattern 3969 ==============\n",
      "============== Pattern 3970 ==============\n",
      "============== Pattern 3971 ==============\n",
      "============== Pattern 3972 ==============\n",
      "============== Pattern 3973 ==============\n",
      "============== Pattern 3974 ==============\n",
      "============== Pattern 3975 ==============\n",
      "============== Pattern 3976 ==============\n",
      "============== Pattern 3977 ==============\n",
      "============== Pattern 3978 ==============\n",
      "============== Pattern 3979 ==============\n",
      "============== Pattern 3980 ==============\n",
      "============== Pattern 3981 ==============\n",
      "============== Pattern 3982 ==============\n",
      "============== Pattern 3983 ==============\n",
      "============== Pattern 3984 ==============\n",
      "============== Pattern 3985 ==============\n",
      "============== Pattern 3986 ==============\n",
      "============== Pattern 3987 ==============\n",
      "============== Pattern 3988 ==============\n",
      "============== Pattern 3989 ==============\n",
      "============== Pattern 3990 ==============\n",
      "============== Pattern 3991 ==============\n",
      "============== Pattern 3992 ==============\n",
      "============== Pattern 3993 ==============\n",
      "============== Pattern 3994 ==============\n",
      "============== Pattern 3995 ==============\n",
      "============== Pattern 3996 ==============\n",
      "============== Pattern 3997 ==============\n",
      "============== Pattern 3998 ==============\n",
      "============== Pattern 3999 ==============\n",
      "============== Pattern 4000 ==============\n",
      "============== Pattern 4001 ==============\n",
      "============== Pattern 4002 ==============\n",
      "============== Pattern 4003 ==============\n",
      "============== Pattern 4004 ==============\n",
      "============== Pattern 4005 ==============\n",
      "============== Pattern 4006 ==============\n",
      "============== Pattern 4007 ==============\n",
      "============== Pattern 4008 ==============\n",
      "============== Pattern 4009 ==============\n",
      "============== Pattern 4010 ==============\n",
      "============== Pattern 4011 ==============\n",
      "============== Pattern 4012 ==============\n",
      "============== Pattern 4013 ==============\n",
      "============== Pattern 4014 ==============\n",
      "============== Pattern 4015 ==============\n",
      "============== Pattern 4016 ==============\n",
      "============== Pattern 4017 ==============\n",
      "============== Pattern 4018 ==============\n",
      "============== Pattern 4019 ==============\n",
      "============== Pattern 4020 ==============\n",
      "============== Pattern 4021 ==============\n",
      "============== Pattern 4022 ==============\n",
      "============== Pattern 4023 ==============\n",
      "============== Pattern 4024 ==============\n",
      "============== Pattern 4025 ==============\n",
      "============== Pattern 4026 ==============\n",
      "============== Pattern 4027 ==============\n",
      "============== Pattern 4028 ==============\n",
      "============== Pattern 4029 ==============\n",
      "============== Pattern 4030 ==============\n",
      "============== Pattern 4031 ==============\n",
      "============== Pattern 4032 ==============\n",
      "============== Pattern 4033 ==============\n",
      "============== Pattern 4034 ==============\n",
      "============== Pattern 4035 ==============\n",
      "============== Pattern 4036 ==============\n",
      "============== Pattern 4037 ==============\n",
      "============== Pattern 4038 ==============\n",
      "============== Pattern 4039 ==============\n",
      "============== Pattern 4040 ==============\n",
      "============== Pattern 4041 ==============\n",
      "============== Pattern 4042 ==============\n",
      "============== Pattern 4043 ==============\n",
      "============== Pattern 4044 ==============\n",
      "============== Pattern 4045 ==============\n",
      "============== Pattern 4046 ==============\n",
      "============== Pattern 4047 ==============\n",
      "============== Pattern 4048 ==============\n",
      "============== Pattern 4049 ==============\n",
      "============== Pattern 4050 ==============\n",
      "============== Pattern 4051 ==============\n",
      "============== Pattern 4052 ==============\n",
      "============== Pattern 4053 ==============\n",
      "============== Pattern 4054 ==============\n",
      "============== Pattern 4055 ==============\n",
      "============== Pattern 4056 ==============\n",
      "============== Pattern 4057 ==============\n",
      "============== Pattern 4058 ==============\n",
      "============== Pattern 4059 ==============\n",
      "============== Pattern 4060 ==============\n",
      "============== Pattern 4061 ==============\n",
      "============== Pattern 4062 ==============\n",
      "============== Pattern 4063 ==============\n",
      "============== Pattern 4064 ==============\n",
      "============== Pattern 4065 ==============\n",
      "============== Pattern 4066 ==============\n",
      "============== Pattern 4067 ==============\n",
      "============== Pattern 4068 ==============\n",
      "============== Pattern 4069 ==============\n",
      "============== Pattern 4070 ==============\n",
      "============== Pattern 4071 ==============\n",
      "============== Pattern 4072 ==============\n",
      "============== Pattern 4073 ==============\n",
      "============== Pattern 4074 ==============\n",
      "============== Pattern 4075 ==============\n",
      "============== Pattern 4076 ==============\n",
      "============== Pattern 4077 ==============\n",
      "============== Pattern 4078 ==============\n",
      "============== Pattern 4079 ==============\n",
      "============== Pattern 4080 ==============\n",
      "============== Pattern 4081 ==============\n",
      "============== Pattern 4082 ==============\n",
      "============== Pattern 4083 ==============\n",
      "============== Pattern 4084 ==============\n",
      "============== Pattern 4085 ==============\n",
      "============== Pattern 4086 ==============\n",
      "============== Pattern 4087 ==============\n",
      "============== Pattern 4088 ==============\n",
      "============== Pattern 4089 ==============\n",
      "============== Pattern 4090 ==============\n",
      "============== Pattern 4091 ==============\n",
      "============== Pattern 4092 ==============\n",
      "============== Pattern 4093 ==============\n",
      "============== Pattern 4094 ==============\n",
      "Average comprehensibility: 53.49438202247191\n",
      "std comprehensibility: 2.2039052908750985\n",
      "var comprehensibility: 4.857198531147253\n",
      "minimum comprehensibility: 44\n",
      "maximum comprehensibility: 60\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
