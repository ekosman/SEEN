{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 32\n",
    "tree_depth = 6\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.164664268493652 | KNN Loss: 6.227475166320801 | BCE Loss: 1.937188982963562\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.172765731811523 | KNN Loss: 6.227191925048828 | BCE Loss: 1.9455742835998535\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.190939903259277 | KNN Loss: 6.227177143096924 | BCE Loss: 1.9637631177902222\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.146702766418457 | KNN Loss: 6.226847171783447 | BCE Loss: 1.9198553562164307\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.167062759399414 | KNN Loss: 6.2267584800720215 | BCE Loss: 1.9403047561645508\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.134511947631836 | KNN Loss: 6.226141452789307 | BCE Loss: 1.908370018005371\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.144550323486328 | KNN Loss: 6.225564002990723 | BCE Loss: 1.9189860820770264\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.135579109191895 | KNN Loss: 6.225414752960205 | BCE Loss: 1.9101639986038208\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.1429443359375 | KNN Loss: 6.224672317504883 | BCE Loss: 1.9182722568511963\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.116617202758789 | KNN Loss: 6.224308013916016 | BCE Loss: 1.8923096656799316\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.125385284423828 | KNN Loss: 6.223900318145752 | BCE Loss: 1.9014849662780762\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.115072250366211 | KNN Loss: 6.223859786987305 | BCE Loss: 1.8912129402160645\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.138635635375977 | KNN Loss: 6.223084926605225 | BCE Loss: 1.915550708770752\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.077466011047363 | KNN Loss: 6.222034454345703 | BCE Loss: 1.8554314374923706\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.089303970336914 | KNN Loss: 6.221939563751221 | BCE Loss: 1.8673648834228516\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 7.995275974273682 | KNN Loss: 6.219772815704346 | BCE Loss: 1.775503158569336\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.043128967285156 | KNN Loss: 6.219892978668213 | BCE Loss: 1.823236346244812\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.027948379516602 | KNN Loss: 6.219202518463135 | BCE Loss: 1.808746099472046\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.06657886505127 | KNN Loss: 6.217742919921875 | BCE Loss: 1.8488361835479736\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.022974014282227 | KNN Loss: 6.2183308601379395 | BCE Loss: 1.8046433925628662\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 7.983723163604736 | KNN Loss: 6.2158203125 | BCE Loss: 1.7679027318954468\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.01925277709961 | KNN Loss: 6.213653564453125 | BCE Loss: 1.8055987358093262\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.978992462158203 | KNN Loss: 6.2125115394592285 | BCE Loss: 1.766480803489685\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.958543300628662 | KNN Loss: 6.212337493896484 | BCE Loss: 1.7462058067321777\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.980813980102539 | KNN Loss: 6.209963321685791 | BCE Loss: 1.7708508968353271\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.980495452880859 | KNN Loss: 6.205683708190918 | BCE Loss: 1.7748119831085205\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.942522048950195 | KNN Loss: 6.204694747924805 | BCE Loss: 1.7378273010253906\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.888763427734375 | KNN Loss: 6.20051908493042 | BCE Loss: 1.688244104385376\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.892845153808594 | KNN Loss: 6.201232433319092 | BCE Loss: 1.6916128396987915\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.910223007202148 | KNN Loss: 6.195507049560547 | BCE Loss: 1.7147157192230225\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.906632423400879 | KNN Loss: 6.190896987915039 | BCE Loss: 1.7157354354858398\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.858942031860352 | KNN Loss: 6.188963413238525 | BCE Loss: 1.6699788570404053\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.808206558227539 | KNN Loss: 6.1842451095581055 | BCE Loss: 1.6239612102508545\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.812963962554932 | KNN Loss: 6.1759209632873535 | BCE Loss: 1.6370431184768677\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.76132869720459 | KNN Loss: 6.173569679260254 | BCE Loss: 1.5877587795257568\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.75046968460083 | KNN Loss: 6.165709972381592 | BCE Loss: 1.5847595930099487\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.75823974609375 | KNN Loss: 6.144941806793213 | BCE Loss: 1.6132980585098267\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.706882953643799 | KNN Loss: 6.145623207092285 | BCE Loss: 1.5612596273422241\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.686668395996094 | KNN Loss: 6.139510631561279 | BCE Loss: 1.547157883644104\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.689608573913574 | KNN Loss: 6.1263604164123535 | BCE Loss: 1.5632479190826416\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.6342973709106445 | KNN Loss: 6.113170623779297 | BCE Loss: 1.5211269855499268\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.5553107261657715 | KNN Loss: 6.087738990783691 | BCE Loss: 1.4675716161727905\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.5341691970825195 | KNN Loss: 6.0798821449279785 | BCE Loss: 1.4542872905731201\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.518590927124023 | KNN Loss: 6.04791259765625 | BCE Loss: 1.4706780910491943\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.433318614959717 | KNN Loss: 6.015438556671143 | BCE Loss: 1.4178800582885742\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.354006767272949 | KNN Loss: 5.979647159576416 | BCE Loss: 1.374359369277954\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.338798522949219 | KNN Loss: 5.968497276306152 | BCE Loss: 1.3703014850616455\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.279696941375732 | KNN Loss: 5.910751819610596 | BCE Loss: 1.3689452409744263\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.2202253341674805 | KNN Loss: 5.873941421508789 | BCE Loss: 1.3462841510772705\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.134227752685547 | KNN Loss: 5.824052333831787 | BCE Loss: 1.3101755380630493\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.069343566894531 | KNN Loss: 5.794742584228516 | BCE Loss: 1.2746007442474365\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.030437469482422 | KNN Loss: 5.746215343475342 | BCE Loss: 1.2842223644256592\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 6.951132774353027 | KNN Loss: 5.6926093101501465 | BCE Loss: 1.2585232257843018\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 6.744389533996582 | KNN Loss: 5.557407379150391 | BCE Loss: 1.1869821548461914\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 6.720200538635254 | KNN Loss: 5.515758991241455 | BCE Loss: 1.204441785812378\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 6.584157943725586 | KNN Loss: 5.396295547485352 | BCE Loss: 1.1878621578216553\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.496267318725586 | KNN Loss: 5.330117225646973 | BCE Loss: 1.1661498546600342\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.374073028564453 | KNN Loss: 5.220753192901611 | BCE Loss: 1.153320074081421\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.302599906921387 | KNN Loss: 5.151857376098633 | BCE Loss: 1.150742530822754\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.120051383972168 | KNN Loss: 4.994472026824951 | BCE Loss: 1.125579595565796\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.0432538986206055 | KNN Loss: 4.87479305267334 | BCE Loss: 1.1684608459472656\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 5.909466743469238 | KNN Loss: 4.793749809265137 | BCE Loss: 1.1157166957855225\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 5.7722930908203125 | KNN Loss: 4.6445159912109375 | BCE Loss: 1.127776861190796\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 5.767693519592285 | KNN Loss: 4.608601093292236 | BCE Loss: 1.159092664718628\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 5.5950469970703125 | KNN Loss: 4.474257469177246 | BCE Loss: 1.1207892894744873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 5.511267185211182 | KNN Loss: 4.427506923675537 | BCE Loss: 1.0837602615356445\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.47360897064209 | KNN Loss: 4.334427833557129 | BCE Loss: 1.139181137084961\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.351345062255859 | KNN Loss: 4.25172233581543 | BCE Loss: 1.0996226072311401\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.377434730529785 | KNN Loss: 4.280810356140137 | BCE Loss: 1.0966246128082275\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.298580169677734 | KNN Loss: 4.186285972595215 | BCE Loss: 1.1122944355010986\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.240612030029297 | KNN Loss: 4.130659580230713 | BCE Loss: 1.1099525690078735\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.193686485290527 | KNN Loss: 4.103450775146484 | BCE Loss: 1.090235710144043\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.167356967926025 | KNN Loss: 4.063889980316162 | BCE Loss: 1.1034668684005737\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.151479721069336 | KNN Loss: 4.054127216339111 | BCE Loss: 1.0973525047302246\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.138191223144531 | KNN Loss: 4.0497822761535645 | BCE Loss: 1.0884087085723877\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.072745323181152 | KNN Loss: 4.020143985748291 | BCE Loss: 1.0526010990142822\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.110193252563477 | KNN Loss: 4.039886474609375 | BCE Loss: 1.0703070163726807\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.030940055847168 | KNN Loss: 3.9713611602783203 | BCE Loss: 1.0595791339874268\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.074109077453613 | KNN Loss: 3.9676077365875244 | BCE Loss: 1.106501579284668\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.011546611785889 | KNN Loss: 3.9580929279327393 | BCE Loss: 1.0534535646438599\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.020097732543945 | KNN Loss: 3.9362852573394775 | BCE Loss: 1.0838122367858887\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.058810234069824 | KNN Loss: 3.977937698364258 | BCE Loss: 1.0808722972869873\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.006179332733154 | KNN Loss: 3.9235706329345703 | BCE Loss: 1.082608699798584\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 5.009613990783691 | KNN Loss: 3.9277608394622803 | BCE Loss: 1.0818531513214111\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 5.04736328125 | KNN Loss: 3.9487619400024414 | BCE Loss: 1.0986015796661377\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 4.988057613372803 | KNN Loss: 3.931586265563965 | BCE Loss: 1.0564712285995483\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.9615888595581055 | KNN Loss: 3.897219657897949 | BCE Loss: 1.0643693208694458\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.993994235992432 | KNN Loss: 3.9227843284606934 | BCE Loss: 1.0712099075317383\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.971305847167969 | KNN Loss: 3.8869526386260986 | BCE Loss: 1.0843532085418701\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 4.9892578125 | KNN Loss: 3.9018490314483643 | BCE Loss: 1.0874089002609253\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 4.950109481811523 | KNN Loss: 3.8795318603515625 | BCE Loss: 1.0705773830413818\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 4.95418643951416 | KNN Loss: 3.9016833305358887 | BCE Loss: 1.0525028705596924\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.976505756378174 | KNN Loss: 3.9148027896881104 | BCE Loss: 1.0617029666900635\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.921076774597168 | KNN Loss: 3.875640869140625 | BCE Loss: 1.045436143875122\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 5.010367393493652 | KNN Loss: 3.905688524246216 | BCE Loss: 1.1046788692474365\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.8914103507995605 | KNN Loss: 3.8448166847229004 | BCE Loss: 1.0465935468673706\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.945247650146484 | KNN Loss: 3.9003331661224365 | BCE Loss: 1.0449144840240479\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.9197845458984375 | KNN Loss: 3.857583522796631 | BCE Loss: 1.0622010231018066\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.931882858276367 | KNN Loss: 3.8691272735595703 | BCE Loss: 1.0627555847167969\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.957489490509033 | KNN Loss: 3.8787875175476074 | BCE Loss: 1.0787019729614258\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.989217758178711 | KNN Loss: 3.9141271114349365 | BCE Loss: 1.0750904083251953\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.964106559753418 | KNN Loss: 3.878824234008789 | BCE Loss: 1.0852820873260498\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.931830883026123 | KNN Loss: 3.863680839538574 | BCE Loss: 1.0681499242782593\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.962662696838379 | KNN Loss: 3.888187885284424 | BCE Loss: 1.074474811553955\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.906930923461914 | KNN Loss: 3.8658223152160645 | BCE Loss: 1.0411088466644287\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.855124473571777 | KNN Loss: 3.8045237064361572 | BCE Loss: 1.0506007671356201\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.877383708953857 | KNN Loss: 3.826537609100342 | BCE Loss: 1.0508462190628052\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.866885185241699 | KNN Loss: 3.8018531799316406 | BCE Loss: 1.0650317668914795\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.907965183258057 | KNN Loss: 3.8402934074401855 | BCE Loss: 1.0676716566085815\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.953997611999512 | KNN Loss: 3.868638515472412 | BCE Loss: 1.08535897731781\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.923214912414551 | KNN Loss: 3.85524845123291 | BCE Loss: 1.0679664611816406\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.939597129821777 | KNN Loss: 3.858354330062866 | BCE Loss: 1.081242561340332\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.903599262237549 | KNN Loss: 3.8313496112823486 | BCE Loss: 1.0722495317459106\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.94892692565918 | KNN Loss: 3.8588123321533203 | BCE Loss: 1.0901143550872803\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.926187515258789 | KNN Loss: 3.8632755279541016 | BCE Loss: 1.0629117488861084\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.901731014251709 | KNN Loss: 3.8553452491760254 | BCE Loss: 1.0463858842849731\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.913906574249268 | KNN Loss: 3.854957342147827 | BCE Loss: 1.05894935131073\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.842685699462891 | KNN Loss: 3.802189350128174 | BCE Loss: 1.040496587753296\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.8778204917907715 | KNN Loss: 3.817138195037842 | BCE Loss: 1.0606821775436401\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.893647193908691 | KNN Loss: 3.8559625148773193 | BCE Loss: 1.037684679031372\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.85302734375 | KNN Loss: 3.797325372695923 | BCE Loss: 1.0557018518447876\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.819948196411133 | KNN Loss: 3.7927920818328857 | BCE Loss: 1.027155876159668\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.894498348236084 | KNN Loss: 3.831942319869995 | BCE Loss: 1.0625560283660889\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.888752460479736 | KNN Loss: 3.8363473415374756 | BCE Loss: 1.0524051189422607\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.879786014556885 | KNN Loss: 3.8391175270080566 | BCE Loss: 1.0406683683395386\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.907094478607178 | KNN Loss: 3.832510471343994 | BCE Loss: 1.0745840072631836\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.85632848739624 | KNN Loss: 3.8242621421813965 | BCE Loss: 1.0320663452148438\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.910377502441406 | KNN Loss: 3.846572160720825 | BCE Loss: 1.0638052225112915\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.901828765869141 | KNN Loss: 3.8290750980377197 | BCE Loss: 1.0727534294128418\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.84696626663208 | KNN Loss: 3.8060038089752197 | BCE Loss: 1.0409623384475708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.867629528045654 | KNN Loss: 3.8126978874206543 | BCE Loss: 1.0549317598342896\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.894550800323486 | KNN Loss: 3.8383522033691406 | BCE Loss: 1.0561985969543457\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.868419647216797 | KNN Loss: 3.809732437133789 | BCE Loss: 1.058687448501587\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.86475944519043 | KNN Loss: 3.838819742202759 | BCE Loss: 1.0259398221969604\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.947162628173828 | KNN Loss: 3.889233350753784 | BCE Loss: 1.057929515838623\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.845092296600342 | KNN Loss: 3.7960262298583984 | BCE Loss: 1.0490660667419434\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.894075393676758 | KNN Loss: 3.827622175216675 | BCE Loss: 1.066453456878662\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.861667633056641 | KNN Loss: 3.7902114391326904 | BCE Loss: 1.071455955505371\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.845104694366455 | KNN Loss: 3.810190439224243 | BCE Loss: 1.034914255142212\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.828843116760254 | KNN Loss: 3.80112886428833 | BCE Loss: 1.0277143716812134\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.8757219314575195 | KNN Loss: 3.805996894836426 | BCE Loss: 1.0697250366210938\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.8593621253967285 | KNN Loss: 3.7957301139831543 | BCE Loss: 1.0636321306228638\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.875986576080322 | KNN Loss: 3.817215919494629 | BCE Loss: 1.0587705373764038\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.818624973297119 | KNN Loss: 3.780304431915283 | BCE Loss: 1.0383204221725464\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.878269195556641 | KNN Loss: 3.817009449005127 | BCE Loss: 1.0612597465515137\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.876855850219727 | KNN Loss: 3.8227596282958984 | BCE Loss: 1.054095983505249\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.860213756561279 | KNN Loss: 3.8220815658569336 | BCE Loss: 1.0381323099136353\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.823901653289795 | KNN Loss: 3.7812740802764893 | BCE Loss: 1.0426275730133057\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.863256931304932 | KNN Loss: 3.8238768577575684 | BCE Loss: 1.0393801927566528\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.850931167602539 | KNN Loss: 3.7973406314849854 | BCE Loss: 1.0535902976989746\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.813809394836426 | KNN Loss: 3.774986505508423 | BCE Loss: 1.0388226509094238\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.863862991333008 | KNN Loss: 3.785478353500366 | BCE Loss: 1.0783846378326416\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.883965969085693 | KNN Loss: 3.8213601112365723 | BCE Loss: 1.062605857849121\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.928005695343018 | KNN Loss: 3.8531157970428467 | BCE Loss: 1.0748897790908813\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.845555305480957 | KNN Loss: 3.8087046146392822 | BCE Loss: 1.0368504524230957\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.825048923492432 | KNN Loss: 3.7907488346099854 | BCE Loss: 1.0342999696731567\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.859426498413086 | KNN Loss: 3.7977547645568848 | BCE Loss: 1.0616717338562012\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.852141857147217 | KNN Loss: 3.7869927883148193 | BCE Loss: 1.065149188041687\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.839820861816406 | KNN Loss: 3.8049585819244385 | BCE Loss: 1.0348625183105469\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.880181789398193 | KNN Loss: 3.820722818374634 | BCE Loss: 1.0594590902328491\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.846449851989746 | KNN Loss: 3.7763710021972656 | BCE Loss: 1.0700788497924805\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.784206390380859 | KNN Loss: 3.7494332790374756 | BCE Loss: 1.0347732305526733\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.824698448181152 | KNN Loss: 3.7725744247436523 | BCE Loss: 1.052123785018921\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.793522357940674 | KNN Loss: 3.7708892822265625 | BCE Loss: 1.0226329565048218\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.862591743469238 | KNN Loss: 3.812122344970703 | BCE Loss: 1.0504693984985352\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.831060409545898 | KNN Loss: 3.8156096935272217 | BCE Loss: 1.0154509544372559\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.813572406768799 | KNN Loss: 3.781843423843384 | BCE Loss: 1.031728982925415\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.825428009033203 | KNN Loss: 3.7854321002960205 | BCE Loss: 1.0399960279464722\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.792777061462402 | KNN Loss: 3.753645658493042 | BCE Loss: 1.0391314029693604\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.846899032592773 | KNN Loss: 3.8100664615631104 | BCE Loss: 1.0368326902389526\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.965034008026123 | KNN Loss: 3.864792823791504 | BCE Loss: 1.1002411842346191\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.833344459533691 | KNN Loss: 3.7839324474334717 | BCE Loss: 1.0494120121002197\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.814523220062256 | KNN Loss: 3.7808895111083984 | BCE Loss: 1.0336337089538574\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.81978178024292 | KNN Loss: 3.758448839187622 | BCE Loss: 1.0613330602645874\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.834497928619385 | KNN Loss: 3.80204176902771 | BCE Loss: 1.0324562788009644\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.812628746032715 | KNN Loss: 3.765657424926758 | BCE Loss: 1.046971082687378\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.8246026039123535 | KNN Loss: 3.7861385345458984 | BCE Loss: 1.0384639501571655\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.809003829956055 | KNN Loss: 3.772615432739258 | BCE Loss: 1.0363883972167969\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.875125885009766 | KNN Loss: 3.8370070457458496 | BCE Loss: 1.0381190776824951\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.779827117919922 | KNN Loss: 3.7465977668762207 | BCE Loss: 1.0332293510437012\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.791346073150635 | KNN Loss: 3.7555079460144043 | BCE Loss: 1.0358381271362305\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.843469142913818 | KNN Loss: 3.7967963218688965 | BCE Loss: 1.0466728210449219\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.810885429382324 | KNN Loss: 3.7651751041412354 | BCE Loss: 1.0457103252410889\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.848252296447754 | KNN Loss: 3.7891428470611572 | BCE Loss: 1.0591092109680176\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.8069586753845215 | KNN Loss: 3.7458109855651855 | BCE Loss: 1.0611475706100464\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.826821804046631 | KNN Loss: 3.7721333503723145 | BCE Loss: 1.0546884536743164\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.801845073699951 | KNN Loss: 3.7860636711120605 | BCE Loss: 1.0157815217971802\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.862636089324951 | KNN Loss: 3.8036742210388184 | BCE Loss: 1.0589617490768433\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.827110290527344 | KNN Loss: 3.778872489929199 | BCE Loss: 1.048237681388855\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.854145526885986 | KNN Loss: 3.791585683822632 | BCE Loss: 1.0625598430633545\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.8238525390625 | KNN Loss: 3.7825851440429688 | BCE Loss: 1.0412673950195312\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.838943004608154 | KNN Loss: 3.785111665725708 | BCE Loss: 1.0538314580917358\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.798373222351074 | KNN Loss: 3.7542855739593506 | BCE Loss: 1.0440874099731445\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.8818678855896 | KNN Loss: 3.8007144927978516 | BCE Loss: 1.0811535120010376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.831170558929443 | KNN Loss: 3.7563765048980713 | BCE Loss: 1.074794054031372\n",
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.797304153442383 | KNN Loss: 3.7578086853027344 | BCE Loss: 1.0394952297210693\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.778392791748047 | KNN Loss: 3.75398850440979 | BCE Loss: 1.0244042873382568\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.798393249511719 | KNN Loss: 3.75352144241333 | BCE Loss: 1.0448715686798096\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.7605390548706055 | KNN Loss: 3.7299633026123047 | BCE Loss: 1.0305759906768799\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.825061321258545 | KNN Loss: 3.779970407485962 | BCE Loss: 1.0450907945632935\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.787725448608398 | KNN Loss: 3.7464547157287598 | BCE Loss: 1.0412707328796387\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.800629615783691 | KNN Loss: 3.7373907566070557 | BCE Loss: 1.0632390975952148\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.763833045959473 | KNN Loss: 3.7541654109954834 | BCE Loss: 1.0096676349639893\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.78436803817749 | KNN Loss: 3.768726110458374 | BCE Loss: 1.0156419277191162\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.8118391036987305 | KNN Loss: 3.7740683555603027 | BCE Loss: 1.0377705097198486\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.823817253112793 | KNN Loss: 3.762324571609497 | BCE Loss: 1.0614928007125854\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.773208141326904 | KNN Loss: 3.747027635574341 | BCE Loss: 1.026180624961853\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.7470574378967285 | KNN Loss: 3.7345447540283203 | BCE Loss: 1.0125126838684082\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.894655704498291 | KNN Loss: 3.866480827331543 | BCE Loss: 1.0281747579574585\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.794994831085205 | KNN Loss: 3.755664110183716 | BCE Loss: 1.0393307209014893\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.79047966003418 | KNN Loss: 3.7572081089019775 | BCE Loss: 1.033271312713623\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.7751078605651855 | KNN Loss: 3.74564266204834 | BCE Loss: 1.0294650793075562\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.7715606689453125 | KNN Loss: 3.763392686843872 | BCE Loss: 1.0081679821014404\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.825875282287598 | KNN Loss: 3.772129774093628 | BCE Loss: 1.0537455081939697\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.785191535949707 | KNN Loss: 3.7507827281951904 | BCE Loss: 1.0344088077545166\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.81727409362793 | KNN Loss: 3.7789294719696045 | BCE Loss: 1.0383448600769043\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.8005547523498535 | KNN Loss: 3.7520298957824707 | BCE Loss: 1.0485247373580933\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.80928897857666 | KNN Loss: 3.761495590209961 | BCE Loss: 1.0477933883666992\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.793191432952881 | KNN Loss: 3.7527971267700195 | BCE Loss: 1.0403941869735718\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.814432144165039 | KNN Loss: 3.7689907550811768 | BCE Loss: 1.0454411506652832\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.794785022735596 | KNN Loss: 3.765251636505127 | BCE Loss: 1.0295332670211792\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.805633068084717 | KNN Loss: 3.7658841609954834 | BCE Loss: 1.0397489070892334\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.820101737976074 | KNN Loss: 3.770500421524048 | BCE Loss: 1.0496010780334473\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.839265823364258 | KNN Loss: 3.7714295387268066 | BCE Loss: 1.067836046218872\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.749404430389404 | KNN Loss: 3.729602336883545 | BCE Loss: 1.0198020935058594\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.787518501281738 | KNN Loss: 3.7716565132141113 | BCE Loss: 1.015862226486206\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.807070732116699 | KNN Loss: 3.742888927459717 | BCE Loss: 1.0641820430755615\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.820505142211914 | KNN Loss: 3.7540879249572754 | BCE Loss: 1.0664169788360596\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.802638053894043 | KNN Loss: 3.7665562629699707 | BCE Loss: 1.0360816717147827\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.786792278289795 | KNN Loss: 3.747201442718506 | BCE Loss: 1.039590835571289\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.772016525268555 | KNN Loss: 3.7682974338531494 | BCE Loss: 1.0037193298339844\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.80358362197876 | KNN Loss: 3.7720084190368652 | BCE Loss: 1.0315752029418945\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.816559314727783 | KNN Loss: 3.780160903930664 | BCE Loss: 1.0363984107971191\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.788675785064697 | KNN Loss: 3.7618353366851807 | BCE Loss: 1.026840329170227\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.827207565307617 | KNN Loss: 3.7696917057037354 | BCE Loss: 1.0575156211853027\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.84928035736084 | KNN Loss: 3.811347484588623 | BCE Loss: 1.0379328727722168\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.792751312255859 | KNN Loss: 3.745191812515259 | BCE Loss: 1.0475592613220215\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.782827377319336 | KNN Loss: 3.7102601528167725 | BCE Loss: 1.0725674629211426\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.801421165466309 | KNN Loss: 3.7493507862091064 | BCE Loss: 1.0520706176757812\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.797457218170166 | KNN Loss: 3.76540470123291 | BCE Loss: 1.0320523977279663\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.795605659484863 | KNN Loss: 3.7519867420196533 | BCE Loss: 1.0436186790466309\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.842113971710205 | KNN Loss: 3.7722933292388916 | BCE Loss: 1.069820523262024\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.775296211242676 | KNN Loss: 3.753974199295044 | BCE Loss: 1.0213217735290527\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.832583427429199 | KNN Loss: 3.7686517238616943 | BCE Loss: 1.0639318227767944\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.787107467651367 | KNN Loss: 3.7611780166625977 | BCE Loss: 1.02592933177948\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.805996417999268 | KNN Loss: 3.7900404930114746 | BCE Loss: 1.0159558057785034\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.763850688934326 | KNN Loss: 3.720806121826172 | BCE Loss: 1.0430445671081543\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.789121627807617 | KNN Loss: 3.762756586074829 | BCE Loss: 1.0263651609420776\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.749051094055176 | KNN Loss: 3.7293014526367188 | BCE Loss: 1.0197495222091675\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.799430847167969 | KNN Loss: 3.7514617443084717 | BCE Loss: 1.047969102859497\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.770824432373047 | KNN Loss: 3.7323217391967773 | BCE Loss: 1.0385026931762695\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.740349292755127 | KNN Loss: 3.7145323753356934 | BCE Loss: 1.0258170366287231\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.774340629577637 | KNN Loss: 3.7440834045410156 | BCE Loss: 1.030257225036621\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.78090238571167 | KNN Loss: 3.7458608150482178 | BCE Loss: 1.0350414514541626\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.767861843109131 | KNN Loss: 3.766042709350586 | BCE Loss: 1.0018190145492554\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.757734298706055 | KNN Loss: 3.7352638244628906 | BCE Loss: 1.022470235824585\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.836258888244629 | KNN Loss: 3.7692854404449463 | BCE Loss: 1.0669732093811035\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.771924018859863 | KNN Loss: 3.74994158744812 | BCE Loss: 1.0219825506210327\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.779683589935303 | KNN Loss: 3.734494686126709 | BCE Loss: 1.0451890230178833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.838930130004883 | KNN Loss: 3.808166027069092 | BCE Loss: 1.0307643413543701\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.802123069763184 | KNN Loss: 3.764037847518921 | BCE Loss: 1.0380849838256836\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.778770923614502 | KNN Loss: 3.7332231998443604 | BCE Loss: 1.0455477237701416\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.752194881439209 | KNN Loss: 3.7203030586242676 | BCE Loss: 1.0318917036056519\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.779771327972412 | KNN Loss: 3.7534127235412598 | BCE Loss: 1.0263586044311523\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.772917747497559 | KNN Loss: 3.726386070251465 | BCE Loss: 1.0465319156646729\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.778808116912842 | KNN Loss: 3.7498276233673096 | BCE Loss: 1.0289804935455322\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.833897590637207 | KNN Loss: 3.7689502239227295 | BCE Loss: 1.064947247505188\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.817930698394775 | KNN Loss: 3.773322105407715 | BCE Loss: 1.0446085929870605\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.79508113861084 | KNN Loss: 3.7497403621673584 | BCE Loss: 1.0453407764434814\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.815133094787598 | KNN Loss: 3.761293888092041 | BCE Loss: 1.0538389682769775\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.767689228057861 | KNN Loss: 3.7348792552948 | BCE Loss: 1.0328099727630615\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.778107166290283 | KNN Loss: 3.7449769973754883 | BCE Loss: 1.033130168914795\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.766340255737305 | KNN Loss: 3.7400550842285156 | BCE Loss: 1.0262854099273682\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.794241428375244 | KNN Loss: 3.7315852642059326 | BCE Loss: 1.0626561641693115\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.788529872894287 | KNN Loss: 3.765472650527954 | BCE Loss: 1.023057222366333\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.78366756439209 | KNN Loss: 3.7439520359039307 | BCE Loss: 1.0397154092788696\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.796751976013184 | KNN Loss: 3.7526040077209473 | BCE Loss: 1.0441482067108154\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.821781158447266 | KNN Loss: 3.7751519680023193 | BCE Loss: 1.0466294288635254\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.789747714996338 | KNN Loss: 3.756425619125366 | BCE Loss: 1.0333219766616821\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.7451581954956055 | KNN Loss: 3.746377944946289 | BCE Loss: 0.9987800717353821\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.746340751647949 | KNN Loss: 3.715381383895874 | BCE Loss: 1.030959129333496\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.759918689727783 | KNN Loss: 3.7462189197540283 | BCE Loss: 1.0136997699737549\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.7821807861328125 | KNN Loss: 3.7479796409606934 | BCE Loss: 1.0342012643814087\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.770219802856445 | KNN Loss: 3.7559220790863037 | BCE Loss: 1.0142979621887207\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.822643280029297 | KNN Loss: 3.7812910079956055 | BCE Loss: 1.0413520336151123\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.748836994171143 | KNN Loss: 3.717970371246338 | BCE Loss: 1.0308666229248047\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.805352210998535 | KNN Loss: 3.766165018081665 | BCE Loss: 1.0391871929168701\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.749692916870117 | KNN Loss: 3.708979606628418 | BCE Loss: 1.0407133102416992\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.8078131675720215 | KNN Loss: 3.7333645820617676 | BCE Loss: 1.074448585510254\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.8052825927734375 | KNN Loss: 3.770197629928589 | BCE Loss: 1.0350850820541382\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.796616554260254 | KNN Loss: 3.733585834503174 | BCE Loss: 1.0630308389663696\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.7765350341796875 | KNN Loss: 3.7313053607940674 | BCE Loss: 1.045229434967041\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.782537937164307 | KNN Loss: 3.749861478805542 | BCE Loss: 1.0326764583587646\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.701342582702637 | KNN Loss: 3.699025869369507 | BCE Loss: 1.002316951751709\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.793000221252441 | KNN Loss: 3.7504279613494873 | BCE Loss: 1.042572259902954\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.741948127746582 | KNN Loss: 3.7275607585906982 | BCE Loss: 1.0143872499465942\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.778379440307617 | KNN Loss: 3.758528232574463 | BCE Loss: 1.0198512077331543\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.766404151916504 | KNN Loss: 3.7180986404418945 | BCE Loss: 1.0483057498931885\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.750486850738525 | KNN Loss: 3.728975534439087 | BCE Loss: 1.0215113162994385\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.748929977416992 | KNN Loss: 3.7254955768585205 | BCE Loss: 1.0234341621398926\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.739536285400391 | KNN Loss: 3.6928513050079346 | BCE Loss: 1.046684980392456\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.741303443908691 | KNN Loss: 3.7261598110198975 | BCE Loss: 1.0151433944702148\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.7505059242248535 | KNN Loss: 3.7226126194000244 | BCE Loss: 1.0278934240341187\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.775820255279541 | KNN Loss: 3.7441253662109375 | BCE Loss: 1.0316948890686035\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.798460006713867 | KNN Loss: 3.752720355987549 | BCE Loss: 1.0457396507263184\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.792325496673584 | KNN Loss: 3.7467617988586426 | BCE Loss: 1.0455635786056519\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.789399147033691 | KNN Loss: 3.760239601135254 | BCE Loss: 1.029159426689148\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.790860652923584 | KNN Loss: 3.745466947555542 | BCE Loss: 1.045393705368042\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.739293098449707 | KNN Loss: 3.7295937538146973 | BCE Loss: 1.0096992254257202\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.800775527954102 | KNN Loss: 3.7569420337677 | BCE Loss: 1.0438332557678223\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.765316486358643 | KNN Loss: 3.7235467433929443 | BCE Loss: 1.0417698621749878\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.780616760253906 | KNN Loss: 3.758094549179077 | BCE Loss: 1.02252197265625\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.740040302276611 | KNN Loss: 3.7290475368499756 | BCE Loss: 1.0109928846359253\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.783238410949707 | KNN Loss: 3.7431468963623047 | BCE Loss: 1.0400915145874023\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.767838478088379 | KNN Loss: 3.7333807945251465 | BCE Loss: 1.0344574451446533\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.7601423263549805 | KNN Loss: 3.7224013805389404 | BCE Loss: 1.0377411842346191\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.767499923706055 | KNN Loss: 3.724156618118286 | BCE Loss: 1.0433435440063477\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.762999534606934 | KNN Loss: 3.7149317264556885 | BCE Loss: 1.0480680465698242\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.778883934020996 | KNN Loss: 3.722278356552124 | BCE Loss: 1.056605339050293\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.80319356918335 | KNN Loss: 3.774808645248413 | BCE Loss: 1.028384804725647\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.792174816131592 | KNN Loss: 3.7682528495788574 | BCE Loss: 1.0239219665527344\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.766329288482666 | KNN Loss: 3.743218183517456 | BCE Loss: 1.02311110496521\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.775082111358643 | KNN Loss: 3.7182366847991943 | BCE Loss: 1.0568454265594482\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.696452617645264 | KNN Loss: 3.6968863010406494 | BCE Loss: 0.9995664358139038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.769696235656738 | KNN Loss: 3.7385637760162354 | BCE Loss: 1.031132698059082\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.792182922363281 | KNN Loss: 3.75553560256958 | BCE Loss: 1.0366473197937012\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.7573442459106445 | KNN Loss: 3.721357822418213 | BCE Loss: 1.0359864234924316\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.750117778778076 | KNN Loss: 3.7282280921936035 | BCE Loss: 1.0218896865844727\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.801402568817139 | KNN Loss: 3.74560546875 | BCE Loss: 1.0557971000671387\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.743971824645996 | KNN Loss: 3.7212440967559814 | BCE Loss: 1.0227277278900146\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.8330583572387695 | KNN Loss: 3.7855491638183594 | BCE Loss: 1.047508955001831\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.7697343826293945 | KNN Loss: 3.7485311031341553 | BCE Loss: 1.0212035179138184\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.8164238929748535 | KNN Loss: 3.789003372192383 | BCE Loss: 1.0274205207824707\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.733163833618164 | KNN Loss: 3.714265823364258 | BCE Loss: 1.0188980102539062\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.763564109802246 | KNN Loss: 3.7261199951171875 | BCE Loss: 1.037443995475769\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.77307653427124 | KNN Loss: 3.709613084793091 | BCE Loss: 1.0634634494781494\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.73504638671875 | KNN Loss: 3.733565330505371 | BCE Loss: 1.0014808177947998\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.747714042663574 | KNN Loss: 3.7509801387786865 | BCE Loss: 0.9967340230941772\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.718050479888916 | KNN Loss: 3.712488889694214 | BCE Loss: 1.0055617094039917\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.763681411743164 | KNN Loss: 3.7346580028533936 | BCE Loss: 1.0290234088897705\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.807199478149414 | KNN Loss: 3.7332193851470947 | BCE Loss: 1.0739803314208984\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.781715393066406 | KNN Loss: 3.7342042922973633 | BCE Loss: 1.047511100769043\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.736116409301758 | KNN Loss: 3.7108585834503174 | BCE Loss: 1.0252580642700195\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.765030384063721 | KNN Loss: 3.74674916267395 | BCE Loss: 1.0182812213897705\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.779810428619385 | KNN Loss: 3.7408924102783203 | BCE Loss: 1.038918137550354\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.739592552185059 | KNN Loss: 3.729436159133911 | BCE Loss: 1.0101563930511475\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.791923522949219 | KNN Loss: 3.7352988719940186 | BCE Loss: 1.0566245317459106\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.750099182128906 | KNN Loss: 3.7034413814544678 | BCE Loss: 1.0466575622558594\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.703669548034668 | KNN Loss: 3.6964094638824463 | BCE Loss: 1.0072598457336426\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.734740734100342 | KNN Loss: 3.7494635581970215 | BCE Loss: 0.9852770566940308\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.776044845581055 | KNN Loss: 3.7561562061309814 | BCE Loss: 1.0198886394500732\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.764218807220459 | KNN Loss: 3.7407538890838623 | BCE Loss: 1.0234649181365967\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.790916919708252 | KNN Loss: 3.7428972721099854 | BCE Loss: 1.0480196475982666\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.770401954650879 | KNN Loss: 3.72934889793396 | BCE Loss: 1.041053056716919\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.735116004943848 | KNN Loss: 3.6988303661346436 | BCE Loss: 1.0362858772277832\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.764793395996094 | KNN Loss: 3.7469675540924072 | BCE Loss: 1.0178258419036865\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.737644195556641 | KNN Loss: 3.7211625576019287 | BCE Loss: 1.0164815187454224\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.740694999694824 | KNN Loss: 3.7080984115600586 | BCE Loss: 1.032596468925476\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.757073879241943 | KNN Loss: 3.7216222286224365 | BCE Loss: 1.0354516506195068\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.720877647399902 | KNN Loss: 3.6760852336883545 | BCE Loss: 1.0447924137115479\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.793537616729736 | KNN Loss: 3.7625162601470947 | BCE Loss: 1.031021237373352\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.738914489746094 | KNN Loss: 3.7299752235412598 | BCE Loss: 1.0089390277862549\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.7207512855529785 | KNN Loss: 3.6972861289978027 | BCE Loss: 1.0234651565551758\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.738012313842773 | KNN Loss: 3.725773334503174 | BCE Loss: 1.0122389793395996\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.8050408363342285 | KNN Loss: 3.740795135498047 | BCE Loss: 1.0642457008361816\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.7901763916015625 | KNN Loss: 3.7571628093719482 | BCE Loss: 1.0330134630203247\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.771080017089844 | KNN Loss: 3.731157064437866 | BCE Loss: 1.0399231910705566\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.773826599121094 | KNN Loss: 3.726954698562622 | BCE Loss: 1.0468721389770508\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.768918037414551 | KNN Loss: 3.7220706939697266 | BCE Loss: 1.0468471050262451\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.74013614654541 | KNN Loss: 3.719184398651123 | BCE Loss: 1.0209518671035767\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.760896682739258 | KNN Loss: 3.7074050903320312 | BCE Loss: 1.0534918308258057\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.748040676116943 | KNN Loss: 3.7413480281829834 | BCE Loss: 1.00669264793396\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.773467540740967 | KNN Loss: 3.730457305908203 | BCE Loss: 1.0430103540420532\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.802160263061523 | KNN Loss: 3.7498631477355957 | BCE Loss: 1.0522973537445068\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.764824867248535 | KNN Loss: 3.723836660385132 | BCE Loss: 1.0409879684448242\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.760998725891113 | KNN Loss: 3.7305779457092285 | BCE Loss: 1.0304206609725952\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.784943103790283 | KNN Loss: 3.7545666694641113 | BCE Loss: 1.0303765535354614\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.800726890563965 | KNN Loss: 3.773231267929077 | BCE Loss: 1.0274957418441772\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.733842849731445 | KNN Loss: 3.7088840007781982 | BCE Loss: 1.024958610534668\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.760015487670898 | KNN Loss: 3.734876871109009 | BCE Loss: 1.0251388549804688\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.757990837097168 | KNN Loss: 3.739234447479248 | BCE Loss: 1.01875638961792\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.790597915649414 | KNN Loss: 3.740182399749756 | BCE Loss: 1.050415277481079\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.742197513580322 | KNN Loss: 3.7184810638427734 | BCE Loss: 1.0237164497375488\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.7665605545043945 | KNN Loss: 3.7107901573181152 | BCE Loss: 1.0557702779769897\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.712629795074463 | KNN Loss: 3.6838443279266357 | BCE Loss: 1.0287853479385376\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.727029800415039 | KNN Loss: 3.709242105484009 | BCE Loss: 1.0177878141403198\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.7847747802734375 | KNN Loss: 3.742824077606201 | BCE Loss: 1.0419509410858154\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.750338077545166 | KNN Loss: 3.7099502086639404 | BCE Loss: 1.040387749671936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.750327110290527 | KNN Loss: 3.7059431076049805 | BCE Loss: 1.044384241104126\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.752415657043457 | KNN Loss: 3.7225630283355713 | BCE Loss: 1.0298526287078857\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.778557300567627 | KNN Loss: 3.7284209728240967 | BCE Loss: 1.0501363277435303\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.766260147094727 | KNN Loss: 3.7340734004974365 | BCE Loss: 1.0321866273880005\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.7468438148498535 | KNN Loss: 3.716355800628662 | BCE Loss: 1.0304880142211914\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.746288299560547 | KNN Loss: 3.7063331604003906 | BCE Loss: 1.0399550199508667\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.728255271911621 | KNN Loss: 3.7093734741210938 | BCE Loss: 1.0188815593719482\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.7857232093811035 | KNN Loss: 3.748563528060913 | BCE Loss: 1.0371596813201904\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.742155075073242 | KNN Loss: 3.7191965579986572 | BCE Loss: 1.0229586362838745\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.75907564163208 | KNN Loss: 3.736736297607422 | BCE Loss: 1.0223392248153687\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.717739582061768 | KNN Loss: 3.7020132541656494 | BCE Loss: 1.0157262086868286\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.717548847198486 | KNN Loss: 3.702310085296631 | BCE Loss: 1.015238642692566\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.762515068054199 | KNN Loss: 3.736379384994507 | BCE Loss: 1.0261354446411133\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.748373985290527 | KNN Loss: 3.7192492485046387 | BCE Loss: 1.0291249752044678\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.748313903808594 | KNN Loss: 3.7264788150787354 | BCE Loss: 1.0218348503112793\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.769418716430664 | KNN Loss: 3.732185125350952 | BCE Loss: 1.037233591079712\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.793288230895996 | KNN Loss: 3.7649760246276855 | BCE Loss: 1.0283124446868896\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.735760688781738 | KNN Loss: 3.6834912300109863 | BCE Loss: 1.0522692203521729\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.728785514831543 | KNN Loss: 3.7298192977905273 | BCE Loss: 0.9989660978317261\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.748298645019531 | KNN Loss: 3.7351841926574707 | BCE Loss: 1.01311457157135\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.776864528656006 | KNN Loss: 3.751138210296631 | BCE Loss: 1.025726318359375\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.733407497406006 | KNN Loss: 3.7215073108673096 | BCE Loss: 1.0119001865386963\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.762816429138184 | KNN Loss: 3.725339651107788 | BCE Loss: 1.0374767780303955\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.716462135314941 | KNN Loss: 3.710369110107422 | BCE Loss: 1.0060930252075195\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.749052047729492 | KNN Loss: 3.712204933166504 | BCE Loss: 1.0368468761444092\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.749521255493164 | KNN Loss: 3.730365037918091 | BCE Loss: 1.0191564559936523\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.709589958190918 | KNN Loss: 3.6942660808563232 | BCE Loss: 1.0153241157531738\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.732296466827393 | KNN Loss: 3.7227582931518555 | BCE Loss: 1.0095382928848267\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.783771514892578 | KNN Loss: 3.7423319816589355 | BCE Loss: 1.0414395332336426\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.756654739379883 | KNN Loss: 3.715690851211548 | BCE Loss: 1.0409640073776245\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.6855621337890625 | KNN Loss: 3.683748960494995 | BCE Loss: 1.0018130540847778\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.731324195861816 | KNN Loss: 3.7035088539123535 | BCE Loss: 1.027815341949463\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.8043293952941895 | KNN Loss: 3.744117498397827 | BCE Loss: 1.0602118968963623\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.763407230377197 | KNN Loss: 3.7256920337677 | BCE Loss: 1.037715196609497\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.74334192276001 | KNN Loss: 3.737776279449463 | BCE Loss: 1.0055656433105469\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.730888366699219 | KNN Loss: 3.707488536834717 | BCE Loss: 1.0233997106552124\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.742733001708984 | KNN Loss: 3.7212138175964355 | BCE Loss: 1.021519422531128\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.731067180633545 | KNN Loss: 3.701615810394287 | BCE Loss: 1.0294512510299683\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.762938976287842 | KNN Loss: 3.741361379623413 | BCE Loss: 1.0215775966644287\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.744378089904785 | KNN Loss: 3.7403969764709473 | BCE Loss: 1.0039809942245483\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.760336875915527 | KNN Loss: 3.7117817401885986 | BCE Loss: 1.0485553741455078\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.787887096405029 | KNN Loss: 3.74497127532959 | BCE Loss: 1.0429158210754395\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.752946853637695 | KNN Loss: 3.7478134632110596 | BCE Loss: 1.0051333904266357\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.7342634201049805 | KNN Loss: 3.6949501037597656 | BCE Loss: 1.0393133163452148\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.770393371582031 | KNN Loss: 3.7384755611419678 | BCE Loss: 1.0319175720214844\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.785775661468506 | KNN Loss: 3.769526481628418 | BCE Loss: 1.0162490606307983\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.759705066680908 | KNN Loss: 3.7420108318328857 | BCE Loss: 1.0176942348480225\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.734537124633789 | KNN Loss: 3.714076519012451 | BCE Loss: 1.0204604864120483\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.785072326660156 | KNN Loss: 3.775357484817505 | BCE Loss: 1.0097150802612305\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.781245231628418 | KNN Loss: 3.7145962715148926 | BCE Loss: 1.066649079322815\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.758218288421631 | KNN Loss: 3.7195234298706055 | BCE Loss: 1.0386948585510254\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.762359142303467 | KNN Loss: 3.7321250438690186 | BCE Loss: 1.0302342176437378\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.71372652053833 | KNN Loss: 3.706522226333618 | BCE Loss: 1.0072044134140015\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.760998249053955 | KNN Loss: 3.7549238204956055 | BCE Loss: 1.0060744285583496\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.764745235443115 | KNN Loss: 3.7381837368011475 | BCE Loss: 1.0265614986419678\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.715868949890137 | KNN Loss: 3.7143948078155518 | BCE Loss: 1.0014739036560059\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.715878486633301 | KNN Loss: 3.696157217025757 | BCE Loss: 1.019721508026123\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.7771992683410645 | KNN Loss: 3.766775369644165 | BCE Loss: 1.010424017906189\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.792015552520752 | KNN Loss: 3.7477610111236572 | BCE Loss: 1.0442546606063843\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.731881618499756 | KNN Loss: 3.71378493309021 | BCE Loss: 1.018096685409546\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.7331695556640625 | KNN Loss: 3.714851140975952 | BCE Loss: 1.0183184146881104\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.762662410736084 | KNN Loss: 3.7194764614105225 | BCE Loss: 1.0431859493255615\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.76599645614624 | KNN Loss: 3.6960108280181885 | BCE Loss: 1.0699857473373413\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.738158226013184 | KNN Loss: 3.7084527015686035 | BCE Loss: 1.02970552444458\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.763399124145508 | KNN Loss: 3.7311854362487793 | BCE Loss: 1.0322136878967285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.71764612197876 | KNN Loss: 3.691009759902954 | BCE Loss: 1.0266363620758057\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.750609874725342 | KNN Loss: 3.715437650680542 | BCE Loss: 1.0351722240447998\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.739856719970703 | KNN Loss: 3.7106285095214844 | BCE Loss: 1.0292282104492188\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.816424369812012 | KNN Loss: 3.7428250312805176 | BCE Loss: 1.0735994577407837\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.742466926574707 | KNN Loss: 3.710428476333618 | BCE Loss: 1.0320383310317993\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.770359039306641 | KNN Loss: 3.7363409996032715 | BCE Loss: 1.0340182781219482\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.751928329467773 | KNN Loss: 3.716123580932617 | BCE Loss: 1.0358047485351562\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.769192695617676 | KNN Loss: 3.7200663089752197 | BCE Loss: 1.049126386642456\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.73035192489624 | KNN Loss: 3.7003161907196045 | BCE Loss: 1.0300356149673462\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.728599548339844 | KNN Loss: 3.7023136615753174 | BCE Loss: 1.0262861251831055\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.740386009216309 | KNN Loss: 3.720750093460083 | BCE Loss: 1.0196360349655151\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.755066871643066 | KNN Loss: 3.7432565689086914 | BCE Loss: 1.011810064315796\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.746467590332031 | KNN Loss: 3.7215969562530518 | BCE Loss: 1.02487051486969\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.701552391052246 | KNN Loss: 3.7030930519104004 | BCE Loss: 0.9984592795372009\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.780762195587158 | KNN Loss: 3.729139804840088 | BCE Loss: 1.0516222715377808\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.688558578491211 | KNN Loss: 3.693263292312622 | BCE Loss: 0.9952952861785889\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.721621990203857 | KNN Loss: 3.722749948501587 | BCE Loss: 0.998871922492981\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.78114128112793 | KNN Loss: 3.74513840675354 | BCE Loss: 1.0360028743743896\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.761203765869141 | KNN Loss: 3.7011430263519287 | BCE Loss: 1.0600605010986328\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.812158584594727 | KNN Loss: 3.7442052364349365 | BCE Loss: 1.0679535865783691\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.752050876617432 | KNN Loss: 3.72928786277771 | BCE Loss: 1.0227631330490112\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.783271789550781 | KNN Loss: 3.7251381874084473 | BCE Loss: 1.058133602142334\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.74526309967041 | KNN Loss: 3.73039174079895 | BCE Loss: 1.0148712396621704\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.79542875289917 | KNN Loss: 3.7634851932525635 | BCE Loss: 1.0319435596466064\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.768486022949219 | KNN Loss: 3.738583564758301 | BCE Loss: 1.029902458190918\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.721075057983398 | KNN Loss: 3.6846930980682373 | BCE Loss: 1.0363819599151611\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.7341108322143555 | KNN Loss: 3.680605888366699 | BCE Loss: 1.0535051822662354\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.738351821899414 | KNN Loss: 3.6997005939483643 | BCE Loss: 1.0386509895324707\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.761364459991455 | KNN Loss: 3.7406365871429443 | BCE Loss: 1.0207278728485107\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.758119583129883 | KNN Loss: 3.7468650341033936 | BCE Loss: 1.0112547874450684\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.755946159362793 | KNN Loss: 3.700944662094116 | BCE Loss: 1.0550017356872559\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.767372131347656 | KNN Loss: 3.717534303665161 | BCE Loss: 1.049837589263916\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.746623992919922 | KNN Loss: 3.712427854537964 | BCE Loss: 1.034196138381958\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.741423606872559 | KNN Loss: 3.7054481506347656 | BCE Loss: 1.0359752178192139\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.745181083679199 | KNN Loss: 3.724167823791504 | BCE Loss: 1.0210134983062744\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.7486066818237305 | KNN Loss: 3.721050977706909 | BCE Loss: 1.0275554656982422\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.729718208312988 | KNN Loss: 3.695225715637207 | BCE Loss: 1.0344927310943604\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.769632339477539 | KNN Loss: 3.7151002883911133 | BCE Loss: 1.0545322895050049\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.763617038726807 | KNN Loss: 3.723362684249878 | BCE Loss: 1.0402544736862183\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.7509942054748535 | KNN Loss: 3.717222213745117 | BCE Loss: 1.0337721109390259\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.745253086090088 | KNN Loss: 3.71797251701355 | BCE Loss: 1.0272806882858276\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.7601189613342285 | KNN Loss: 3.734933376312256 | BCE Loss: 1.025185465812683\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.748995780944824 | KNN Loss: 3.710071325302124 | BCE Loss: 1.0389246940612793\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.74984884262085 | KNN Loss: 3.706932544708252 | BCE Loss: 1.0429164171218872\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.726870536804199 | KNN Loss: 3.7079031467437744 | BCE Loss: 1.0189673900604248\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.7296671867370605 | KNN Loss: 3.690955638885498 | BCE Loss: 1.038711667060852\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.7893595695495605 | KNN Loss: 3.7359747886657715 | BCE Loss: 1.053384780883789\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.69708776473999 | KNN Loss: 3.696133613586426 | BCE Loss: 1.000954031944275\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.781896591186523 | KNN Loss: 3.72794771194458 | BCE Loss: 1.0539486408233643\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.753302574157715 | KNN Loss: 3.723341464996338 | BCE Loss: 1.0299609899520874\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.804076194763184 | KNN Loss: 3.757146120071411 | BCE Loss: 1.0469303131103516\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.76978063583374 | KNN Loss: 3.721376895904541 | BCE Loss: 1.0484037399291992\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.780986309051514 | KNN Loss: 3.7767629623413086 | BCE Loss: 1.004223346710205\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.78217887878418 | KNN Loss: 3.7257440090179443 | BCE Loss: 1.056434988975525\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.75056266784668 | KNN Loss: 3.713301181793213 | BCE Loss: 1.0372616052627563\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.746156692504883 | KNN Loss: 3.722564935684204 | BCE Loss: 1.0235917568206787\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.723243236541748 | KNN Loss: 3.685039758682251 | BCE Loss: 1.0382035970687866\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.729875087738037 | KNN Loss: 3.716104030609131 | BCE Loss: 1.0137711763381958\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.791165351867676 | KNN Loss: 3.767291784286499 | BCE Loss: 1.0238735675811768\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.720741271972656 | KNN Loss: 3.7049553394317627 | BCE Loss: 1.0157856941223145\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.774807929992676 | KNN Loss: 3.730675458908081 | BCE Loss: 1.0441324710845947\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.7385406494140625 | KNN Loss: 3.7030622959136963 | BCE Loss: 1.0354785919189453\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.788696765899658 | KNN Loss: 3.730865955352783 | BCE Loss: 1.057830810546875\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.748117446899414 | KNN Loss: 3.699967861175537 | BCE Loss: 1.048149824142456\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.730387210845947 | KNN Loss: 3.732558012008667 | BCE Loss: 0.9978293776512146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.734511375427246 | KNN Loss: 3.718853712081909 | BCE Loss: 1.015657901763916\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.741369247436523 | KNN Loss: 3.7101144790649414 | BCE Loss: 1.031254768371582\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.758678436279297 | KNN Loss: 3.7331881523132324 | BCE Loss: 1.0254902839660645\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.735609531402588 | KNN Loss: 3.7117559909820557 | BCE Loss: 1.0238536596298218\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.721056938171387 | KNN Loss: 3.7231664657592773 | BCE Loss: 0.9978905916213989\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.763482093811035 | KNN Loss: 3.733769655227661 | BCE Loss: 1.0297125577926636\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.735794544219971 | KNN Loss: 3.7286901473999023 | BCE Loss: 1.0071043968200684\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.729981422424316 | KNN Loss: 3.724855661392212 | BCE Loss: 1.0051259994506836\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.759917259216309 | KNN Loss: 3.7194137573242188 | BCE Loss: 1.040503740310669\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.760108947753906 | KNN Loss: 3.719956874847412 | BCE Loss: 1.040151834487915\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.778984546661377 | KNN Loss: 3.7275795936584473 | BCE Loss: 1.0514049530029297\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.727461814880371 | KNN Loss: 3.721336841583252 | BCE Loss: 1.0061249732971191\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.728270053863525 | KNN Loss: 3.693844795227051 | BCE Loss: 1.0344253778457642\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.7177815437316895 | KNN Loss: 3.693840265274048 | BCE Loss: 1.0239412784576416\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.743352890014648 | KNN Loss: 3.7234790325164795 | BCE Loss: 1.0198736190795898\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.741930961608887 | KNN Loss: 3.7185966968536377 | BCE Loss: 1.0233345031738281\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.763555526733398 | KNN Loss: 3.735546350479126 | BCE Loss: 1.028009057044983\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.79807186126709 | KNN Loss: 3.751124858856201 | BCE Loss: 1.0469470024108887\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.748978137969971 | KNN Loss: 3.7293217182159424 | BCE Loss: 1.0196564197540283\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.726550102233887 | KNN Loss: 3.73779034614563 | BCE Loss: 0.9887597560882568\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.773288249969482 | KNN Loss: 3.749086618423462 | BCE Loss: 1.0242016315460205\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.762809753417969 | KNN Loss: 3.713737726211548 | BCE Loss: 1.049072027206421\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.712367057800293 | KNN Loss: 3.6883575916290283 | BCE Loss: 1.0240092277526855\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.702644348144531 | KNN Loss: 3.6906824111938477 | BCE Loss: 1.0119619369506836\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.718658447265625 | KNN Loss: 3.718203544616699 | BCE Loss: 1.0004549026489258\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.7495527267456055 | KNN Loss: 3.699481248855591 | BCE Loss: 1.0500714778900146\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.753536224365234 | KNN Loss: 3.6970975399017334 | BCE Loss: 1.05643892288208\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.775084018707275 | KNN Loss: 3.7435054779052734 | BCE Loss: 1.0315784215927124\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.759163856506348 | KNN Loss: 3.7220871448516846 | BCE Loss: 1.0370769500732422\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.737656593322754 | KNN Loss: 3.723400592803955 | BCE Loss: 1.0142560005187988\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.749500751495361 | KNN Loss: 3.7029922008514404 | BCE Loss: 1.0465086698532104\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.7759108543396 | KNN Loss: 3.7345781326293945 | BCE Loss: 1.041332721710205\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.731144905090332 | KNN Loss: 3.7110140323638916 | BCE Loss: 1.0201306343078613\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.7249979972839355 | KNN Loss: 3.67937970161438 | BCE Loss: 1.0456184148788452\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.670230388641357 | KNN Loss: 3.6656978130340576 | BCE Loss: 1.0045326948165894\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.730403900146484 | KNN Loss: 3.7048285007476807 | BCE Loss: 1.0255755186080933\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.738072395324707 | KNN Loss: 3.724402666091919 | BCE Loss: 1.0136696100234985\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.723076820373535 | KNN Loss: 3.7023022174835205 | BCE Loss: 1.0207743644714355\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.693587303161621 | KNN Loss: 3.682586669921875 | BCE Loss: 1.011000633239746\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.7029595375061035 | KNN Loss: 3.665727138519287 | BCE Loss: 1.0372323989868164\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.686614513397217 | KNN Loss: 3.6799681186676025 | BCE Loss: 1.0066463947296143\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.748775482177734 | KNN Loss: 3.732387065887451 | BCE Loss: 1.0163884162902832\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.7150444984436035 | KNN Loss: 3.7018418312072754 | BCE Loss: 1.0132026672363281\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.7617011070251465 | KNN Loss: 3.710108995437622 | BCE Loss: 1.051592230796814\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.730447292327881 | KNN Loss: 3.72530460357666 | BCE Loss: 1.0051426887512207\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.726090431213379 | KNN Loss: 3.699418067932129 | BCE Loss: 1.0266724824905396\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.722744941711426 | KNN Loss: 3.70051908493042 | BCE Loss: 1.0222256183624268\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.729446887969971 | KNN Loss: 3.7103452682495117 | BCE Loss: 1.0191015005111694\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.774957656860352 | KNN Loss: 3.711113929748535 | BCE Loss: 1.063843846321106\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.804110527038574 | KNN Loss: 3.7444565296173096 | BCE Loss: 1.0596539974212646\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.7283430099487305 | KNN Loss: 3.697498321533203 | BCE Loss: 1.0308446884155273\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.746335029602051 | KNN Loss: 3.720796585083008 | BCE Loss: 1.0255382061004639\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.739083290100098 | KNN Loss: 3.7333436012268066 | BCE Loss: 1.0057399272918701\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.783751487731934 | KNN Loss: 3.7284529209136963 | BCE Loss: 1.0552985668182373\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.771955490112305 | KNN Loss: 3.7124035358428955 | BCE Loss: 1.05955171585083\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.7302727699279785 | KNN Loss: 3.698108196258545 | BCE Loss: 1.0321645736694336\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.741890907287598 | KNN Loss: 3.713961362838745 | BCE Loss: 1.0279293060302734\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.726902484893799 | KNN Loss: 3.707444429397583 | BCE Loss: 1.0194580554962158\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.699796676635742 | KNN Loss: 3.68308424949646 | BCE Loss: 1.0167124271392822\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.719940185546875 | KNN Loss: 3.703155994415283 | BCE Loss: 1.0167840719223022\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.73682975769043 | KNN Loss: 3.696821928024292 | BCE Loss: 1.0400075912475586\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.810643196105957 | KNN Loss: 3.755679130554199 | BCE Loss: 1.0549638271331787\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.719595432281494 | KNN Loss: 3.7035071849823 | BCE Loss: 1.0160883665084839\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.6937479972839355 | KNN Loss: 3.6845407485961914 | BCE Loss: 1.0092073678970337\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.765005111694336 | KNN Loss: 3.7095437049865723 | BCE Loss: 1.0554611682891846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.745556831359863 | KNN Loss: 3.7185747623443604 | BCE Loss: 1.026982307434082\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.793820381164551 | KNN Loss: 3.752561569213867 | BCE Loss: 1.0412588119506836\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.72690486907959 | KNN Loss: 3.6875112056732178 | BCE Loss: 1.039393424987793\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.762628555297852 | KNN Loss: 3.7398593425750732 | BCE Loss: 1.0227689743041992\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.753249645233154 | KNN Loss: 3.7064459323883057 | BCE Loss: 1.046803593635559\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.74613618850708 | KNN Loss: 3.692047119140625 | BCE Loss: 1.054089069366455\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.746635437011719 | KNN Loss: 3.700798273086548 | BCE Loss: 1.0458370447158813\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.769066333770752 | KNN Loss: 3.726808786392212 | BCE Loss: 1.04225754737854\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.7725043296813965 | KNN Loss: 3.7160491943359375 | BCE Loss: 1.056455135345459\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.779017448425293 | KNN Loss: 3.7545032501220703 | BCE Loss: 1.0245139598846436\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.77403450012207 | KNN Loss: 3.7353148460388184 | BCE Loss: 1.038719654083252\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.821475505828857 | KNN Loss: 3.780275583267212 | BCE Loss: 1.0411999225616455\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.734594821929932 | KNN Loss: 3.7104532718658447 | BCE Loss: 1.0241416692733765\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.761605739593506 | KNN Loss: 3.739354372024536 | BCE Loss: 1.0222514867782593\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.712880611419678 | KNN Loss: 3.701828718185425 | BCE Loss: 1.011051893234253\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.751895904541016 | KNN Loss: 3.748073101043701 | BCE Loss: 1.0038228034973145\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.785881042480469 | KNN Loss: 3.7146921157836914 | BCE Loss: 1.0711891651153564\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.732491493225098 | KNN Loss: 3.721752643585205 | BCE Loss: 1.0107388496398926\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.766757965087891 | KNN Loss: 3.7343075275421143 | BCE Loss: 1.032450556755066\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.723080635070801 | KNN Loss: 3.7137985229492188 | BCE Loss: 1.0092823505401611\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.712262153625488 | KNN Loss: 3.687779426574707 | BCE Loss: 1.0244824886322021\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.7283735275268555 | KNN Loss: 3.699305534362793 | BCE Loss: 1.0290679931640625\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.718006134033203 | KNN Loss: 3.6887927055358887 | BCE Loss: 1.029213547706604\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.745524883270264 | KNN Loss: 3.7266783714294434 | BCE Loss: 1.0188465118408203\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.779666900634766 | KNN Loss: 3.742100715637207 | BCE Loss: 1.0375659465789795\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.79099178314209 | KNN Loss: 3.7556755542755127 | BCE Loss: 1.0353164672851562\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.771553039550781 | KNN Loss: 3.720360040664673 | BCE Loss: 1.0511932373046875\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.7533392906188965 | KNN Loss: 3.704151153564453 | BCE Loss: 1.049188256263733\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.725072383880615 | KNN Loss: 3.7248098850250244 | BCE Loss: 1.0002623796463013\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.7019362449646 | KNN Loss: 3.690382242202759 | BCE Loss: 1.0115538835525513\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.744838237762451 | KNN Loss: 3.726935625076294 | BCE Loss: 1.0179026126861572\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.753419399261475 | KNN Loss: 3.709562063217163 | BCE Loss: 1.043857216835022\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.705355167388916 | KNN Loss: 3.7108089923858643 | BCE Loss: 0.9945463538169861\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.730328559875488 | KNN Loss: 3.6943931579589844 | BCE Loss: 1.0359355211257935\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.792474269866943 | KNN Loss: 3.753948926925659 | BCE Loss: 1.0385253429412842\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.728521823883057 | KNN Loss: 3.712034225463867 | BCE Loss: 1.0164874792099\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.74975061416626 | KNN Loss: 3.732448101043701 | BCE Loss: 1.017302393913269\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.719108581542969 | KNN Loss: 3.7119956016540527 | BCE Loss: 1.0071132183074951\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.772405624389648 | KNN Loss: 3.7195725440979004 | BCE Loss: 1.052833080291748\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.729631423950195 | KNN Loss: 3.6948304176330566 | BCE Loss: 1.0348007678985596\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.716923713684082 | KNN Loss: 3.6848199367523193 | BCE Loss: 1.0321036577224731\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.702917098999023 | KNN Loss: 3.700925350189209 | BCE Loss: 1.0019917488098145\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.764781951904297 | KNN Loss: 3.7221744060516357 | BCE Loss: 1.0426074266433716\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.745978832244873 | KNN Loss: 3.711611270904541 | BCE Loss: 1.0343674421310425\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.737349033355713 | KNN Loss: 3.713594913482666 | BCE Loss: 1.0237541198730469\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.76538610458374 | KNN Loss: 3.733245849609375 | BCE Loss: 1.0321402549743652\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.797320365905762 | KNN Loss: 3.7483279705047607 | BCE Loss: 1.0489922761917114\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.742953777313232 | KNN Loss: 3.73114013671875 | BCE Loss: 1.0118136405944824\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.8026251792907715 | KNN Loss: 3.74442982673645 | BCE Loss: 1.0581952333450317\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.733226776123047 | KNN Loss: 3.7141849994659424 | BCE Loss: 1.019041657447815\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.773955345153809 | KNN Loss: 3.7269554138183594 | BCE Loss: 1.0470001697540283\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.731386184692383 | KNN Loss: 3.72257399559021 | BCE Loss: 1.008812427520752\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.750010013580322 | KNN Loss: 3.719569206237793 | BCE Loss: 1.0304408073425293\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.739985942840576 | KNN Loss: 3.7029733657836914 | BCE Loss: 1.0370124578475952\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.738242149353027 | KNN Loss: 3.6937952041625977 | BCE Loss: 1.0444468259811401\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.715782165527344 | KNN Loss: 3.7028841972351074 | BCE Loss: 1.0128982067108154\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.7715349197387695 | KNN Loss: 3.7291319370269775 | BCE Loss: 1.0424028635025024\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.708853244781494 | KNN Loss: 3.6902449131011963 | BCE Loss: 1.0186083316802979\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.713319778442383 | KNN Loss: 3.6693131923675537 | BCE Loss: 1.04400634765625\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.70015287399292 | KNN Loss: 3.6965110301971436 | BCE Loss: 1.0036418437957764\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.737771511077881 | KNN Loss: 3.7262661457061768 | BCE Loss: 1.0115052461624146\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.791469573974609 | KNN Loss: 3.741011142730713 | BCE Loss: 1.050458312034607\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.713104724884033 | KNN Loss: 3.6891133785247803 | BCE Loss: 1.0239914655685425\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.759560585021973 | KNN Loss: 3.7163398265838623 | BCE Loss: 1.0432207584381104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.743544578552246 | KNN Loss: 3.681439161300659 | BCE Loss: 1.062105655670166\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.766185760498047 | KNN Loss: 3.7034788131713867 | BCE Loss: 1.0627071857452393\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.791109085083008 | KNN Loss: 3.7352957725524902 | BCE Loss: 1.0558130741119385\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.735445022583008 | KNN Loss: 3.7206239700317383 | BCE Loss: 1.0148212909698486\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.695124626159668 | KNN Loss: 3.6845762729644775 | BCE Loss: 1.0105481147766113\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.725499153137207 | KNN Loss: 3.697798728942871 | BCE Loss: 1.0277005434036255\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.75405740737915 | KNN Loss: 3.7240090370178223 | BCE Loss: 1.0300483703613281\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.700139999389648 | KNN Loss: 3.700690746307373 | BCE Loss: 0.9994493722915649\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.7741217613220215 | KNN Loss: 3.759995698928833 | BCE Loss: 1.014126181602478\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.732568740844727 | KNN Loss: 3.707756757736206 | BCE Loss: 1.024811863899231\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.750982284545898 | KNN Loss: 3.735677480697632 | BCE Loss: 1.0153048038482666\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.748988628387451 | KNN Loss: 3.7082619667053223 | BCE Loss: 1.0407267808914185\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.732448577880859 | KNN Loss: 3.6933445930480957 | BCE Loss: 1.0391037464141846\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.761857032775879 | KNN Loss: 3.7198851108551025 | BCE Loss: 1.0419719219207764\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.746836185455322 | KNN Loss: 3.6926984786987305 | BCE Loss: 1.0541377067565918\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.740861892700195 | KNN Loss: 3.71213960647583 | BCE Loss: 1.0287222862243652\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.727587699890137 | KNN Loss: 3.7096099853515625 | BCE Loss: 1.0179777145385742\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.78263521194458 | KNN Loss: 3.752758026123047 | BCE Loss: 1.0298773050308228\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.723662853240967 | KNN Loss: 3.687856674194336 | BCE Loss: 1.0358062982559204\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.748485565185547 | KNN Loss: 3.708789348602295 | BCE Loss: 1.0396960973739624\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.723447799682617 | KNN Loss: 3.6990017890930176 | BCE Loss: 1.0244457721710205\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.687215805053711 | KNN Loss: 3.6838321685791016 | BCE Loss: 1.0033833980560303\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.714886665344238 | KNN Loss: 3.6734845638275146 | BCE Loss: 1.041401982307434\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.746628761291504 | KNN Loss: 3.720569372177124 | BCE Loss: 1.026059627532959\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.756951808929443 | KNN Loss: 3.7137503623962402 | BCE Loss: 1.0432014465332031\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.742147445678711 | KNN Loss: 3.7360851764678955 | BCE Loss: 1.0060622692108154\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.741208553314209 | KNN Loss: 3.727367877960205 | BCE Loss: 1.013840675354004\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.7654218673706055 | KNN Loss: 3.7290384769439697 | BCE Loss: 1.0363835096359253\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.718348026275635 | KNN Loss: 3.701103448867798 | BCE Loss: 1.017244577407837\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.7257080078125 | KNN Loss: 3.7059688568115234 | BCE Loss: 1.0197389125823975\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.779637813568115 | KNN Loss: 3.7508723735809326 | BCE Loss: 1.0287654399871826\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.685204982757568 | KNN Loss: 3.682095766067505 | BCE Loss: 1.003109335899353\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.734281539916992 | KNN Loss: 3.7114369869232178 | BCE Loss: 1.0228444337844849\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.732030868530273 | KNN Loss: 3.7188901901245117 | BCE Loss: 1.0131406784057617\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.75331449508667 | KNN Loss: 3.710085868835449 | BCE Loss: 1.0432286262512207\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.734212875366211 | KNN Loss: 3.7084925174713135 | BCE Loss: 1.0257203578948975\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.704623222351074 | KNN Loss: 3.6755268573760986 | BCE Loss: 1.0290964841842651\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.743288993835449 | KNN Loss: 3.713982343673706 | BCE Loss: 1.0293068885803223\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.767297267913818 | KNN Loss: 3.7222537994384766 | BCE Loss: 1.0450433492660522\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.765251159667969 | KNN Loss: 3.7086474895477295 | BCE Loss: 1.0566039085388184\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.79235315322876 | KNN Loss: 3.7317934036254883 | BCE Loss: 1.060559868812561\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.711026668548584 | KNN Loss: 3.72312068939209 | BCE Loss: 0.9879060983657837\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.715565204620361 | KNN Loss: 3.701864242553711 | BCE Loss: 1.0137009620666504\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.731103897094727 | KNN Loss: 3.701836585998535 | BCE Loss: 1.0292670726776123\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.736917018890381 | KNN Loss: 3.7145416736602783 | BCE Loss: 1.022375464439392\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.783011436462402 | KNN Loss: 3.7546615600585938 | BCE Loss: 1.0283501148223877\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.826578617095947 | KNN Loss: 3.7644667625427246 | BCE Loss: 1.062111735343933\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.690969944000244 | KNN Loss: 3.6913323402404785 | BCE Loss: 0.9996377229690552\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.751140594482422 | KNN Loss: 3.7124550342559814 | BCE Loss: 1.0386857986450195\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.715497016906738 | KNN Loss: 3.687490940093994 | BCE Loss: 1.028005838394165\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.752800464630127 | KNN Loss: 3.702665328979492 | BCE Loss: 1.0501350164413452\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.698432922363281 | KNN Loss: 3.688859701156616 | BCE Loss: 1.0095733404159546\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.73139762878418 | KNN Loss: 3.7015883922576904 | BCE Loss: 1.0298089981079102\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.757966995239258 | KNN Loss: 3.7260611057281494 | BCE Loss: 1.0319056510925293\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.73893928527832 | KNN Loss: 3.7091143131256104 | BCE Loss: 1.02982497215271\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.741795539855957 | KNN Loss: 3.7292709350585938 | BCE Loss: 1.0125248432159424\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.757596969604492 | KNN Loss: 3.737301826477051 | BCE Loss: 1.020295262336731\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.762882709503174 | KNN Loss: 3.7327616214752197 | BCE Loss: 1.030121088027954\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.720806121826172 | KNN Loss: 3.6840364933013916 | BCE Loss: 1.0367698669433594\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.7381815910339355 | KNN Loss: 3.68672251701355 | BCE Loss: 1.0514591932296753\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.767752647399902 | KNN Loss: 3.7310404777526855 | BCE Loss: 1.0367122888565063\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.734279155731201 | KNN Loss: 3.712942600250244 | BCE Loss: 1.0213364362716675\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.740516662597656 | KNN Loss: 3.7108850479125977 | BCE Loss: 1.0296313762664795\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.782950401306152 | KNN Loss: 3.7398202419281006 | BCE Loss: 1.0431301593780518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.783628463745117 | KNN Loss: 3.7377724647521973 | BCE Loss: 1.0458557605743408\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.743024826049805 | KNN Loss: 3.6964240074157715 | BCE Loss: 1.0466006994247437\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.739471435546875 | KNN Loss: 3.7177317142486572 | BCE Loss: 1.0217397212982178\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.763558387756348 | KNN Loss: 3.7154903411865234 | BCE Loss: 1.0480680465698242\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.747273921966553 | KNN Loss: 3.7147529125213623 | BCE Loss: 1.0325210094451904\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.701626777648926 | KNN Loss: 3.6998910903930664 | BCE Loss: 1.0017355680465698\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.729096412658691 | KNN Loss: 3.7122371196746826 | BCE Loss: 1.0168592929840088\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.738917350769043 | KNN Loss: 3.690537214279175 | BCE Loss: 1.0483803749084473\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.674568176269531 | KNN Loss: 3.6755683422088623 | BCE Loss: 0.998999834060669\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.80255126953125 | KNN Loss: 3.734837532043457 | BCE Loss: 1.0677138566970825\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.719271659851074 | KNN Loss: 3.7034263610839844 | BCE Loss: 1.0158454179763794\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.740106105804443 | KNN Loss: 3.714477062225342 | BCE Loss: 1.0256290435791016\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.743052005767822 | KNN Loss: 3.701045036315918 | BCE Loss: 1.0420069694519043\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.745238780975342 | KNN Loss: 3.718984842300415 | BCE Loss: 1.0262539386749268\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.742570877075195 | KNN Loss: 3.6876964569091797 | BCE Loss: 1.0548746585845947\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.732460975646973 | KNN Loss: 3.714491844177246 | BCE Loss: 1.0179693698883057\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.768672466278076 | KNN Loss: 3.729506254196167 | BCE Loss: 1.0391662120819092\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.745429992675781 | KNN Loss: 3.7277421951293945 | BCE Loss: 1.0176877975463867\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.724228858947754 | KNN Loss: 3.719958782196045 | BCE Loss: 1.0042701959609985\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.740620136260986 | KNN Loss: 3.7238881587982178 | BCE Loss: 1.0167319774627686\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.725393772125244 | KNN Loss: 3.6942930221557617 | BCE Loss: 1.0311006307601929\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.7741594314575195 | KNN Loss: 3.7254011631011963 | BCE Loss: 1.0487582683563232\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.757811546325684 | KNN Loss: 3.7000980377197266 | BCE Loss: 1.0577137470245361\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.76511287689209 | KNN Loss: 3.732632875442505 | BCE Loss: 1.0324797630310059\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.77696418762207 | KNN Loss: 3.7188377380371094 | BCE Loss: 1.05812668800354\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.7447662353515625 | KNN Loss: 3.7202067375183105 | BCE Loss: 1.024559497833252\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.759757041931152 | KNN Loss: 3.743384599685669 | BCE Loss: 1.0163726806640625\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.724567413330078 | KNN Loss: 3.695222854614258 | BCE Loss: 1.0293444395065308\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.7423601150512695 | KNN Loss: 3.6992762088775635 | BCE Loss: 1.043083667755127\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.737485885620117 | KNN Loss: 3.6910650730133057 | BCE Loss: 1.0464205741882324\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.690011024475098 | KNN Loss: 3.6792311668395996 | BCE Loss: 1.010779857635498\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.735496520996094 | KNN Loss: 3.7182798385620117 | BCE Loss: 1.017216444015503\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.721895694732666 | KNN Loss: 3.6693241596221924 | BCE Loss: 1.0525716543197632\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.727030277252197 | KNN Loss: 3.6877400875091553 | BCE Loss: 1.039290189743042\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.7700653076171875 | KNN Loss: 3.7380404472351074 | BCE Loss: 1.03202486038208\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.792536735534668 | KNN Loss: 3.751420259475708 | BCE Loss: 1.04111647605896\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.680479526519775 | KNN Loss: 3.672133445739746 | BCE Loss: 1.0083460807800293\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.686497211456299 | KNN Loss: 3.6695871353149414 | BCE Loss: 1.016910195350647\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.74968147277832 | KNN Loss: 3.7165915966033936 | BCE Loss: 1.0330898761749268\n",
      "Epoch   125: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.691343307495117 | KNN Loss: 3.6835098266601562 | BCE Loss: 1.0078333616256714\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.731531143188477 | KNN Loss: 3.698716402053833 | BCE Loss: 1.0328147411346436\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.743678092956543 | KNN Loss: 3.7055511474609375 | BCE Loss: 1.0381267070770264\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.7227091789245605 | KNN Loss: 3.7012791633605957 | BCE Loss: 1.0214298963546753\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.746861457824707 | KNN Loss: 3.7361533641815186 | BCE Loss: 1.0107078552246094\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.720089912414551 | KNN Loss: 3.68569278717041 | BCE Loss: 1.0343971252441406\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.736600875854492 | KNN Loss: 3.7002906799316406 | BCE Loss: 1.0363099575042725\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.725390434265137 | KNN Loss: 3.6988508701324463 | BCE Loss: 1.0265395641326904\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.705984115600586 | KNN Loss: 3.6795260906219482 | BCE Loss: 1.0264579057693481\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.694786548614502 | KNN Loss: 3.6766669750213623 | BCE Loss: 1.0181195735931396\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.726219177246094 | KNN Loss: 3.69858455657959 | BCE Loss: 1.027634859085083\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.676234245300293 | KNN Loss: 3.6738381385803223 | BCE Loss: 1.0023961067199707\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.775684356689453 | KNN Loss: 3.6903443336486816 | BCE Loss: 1.0853402614593506\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.719364643096924 | KNN Loss: 3.7103962898254395 | BCE Loss: 1.0089682340621948\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.778040885925293 | KNN Loss: 3.7452142238616943 | BCE Loss: 1.0328266620635986\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.777914524078369 | KNN Loss: 3.7532262802124023 | BCE Loss: 1.0246882438659668\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.719470024108887 | KNN Loss: 3.6787381172180176 | BCE Loss: 1.0407320261001587\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.717645645141602 | KNN Loss: 3.692237138748169 | BCE Loss: 1.025408387184143\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.746233940124512 | KNN Loss: 3.731443166732788 | BCE Loss: 1.0147907733917236\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.746651649475098 | KNN Loss: 3.7083022594451904 | BCE Loss: 1.0383492708206177\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.7184553146362305 | KNN Loss: 3.6762776374816895 | BCE Loss: 1.042177677154541\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.756330490112305 | KNN Loss: 3.7073452472686768 | BCE Loss: 1.048985481262207\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.734668731689453 | KNN Loss: 3.719895839691162 | BCE Loss: 1.0147731304168701\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.733187675476074 | KNN Loss: 3.6922121047973633 | BCE Loss: 1.04097580909729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.731118202209473 | KNN Loss: 3.707352876663208 | BCE Loss: 1.0237655639648438\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.7231974601745605 | KNN Loss: 3.689502716064453 | BCE Loss: 1.0336946249008179\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.7765984535217285 | KNN Loss: 3.695077896118164 | BCE Loss: 1.0815205574035645\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.744012832641602 | KNN Loss: 3.713778257369995 | BCE Loss: 1.0302345752716064\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.722775936126709 | KNN Loss: 3.6860833168029785 | BCE Loss: 1.0366926193237305\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.740422248840332 | KNN Loss: 3.7133355140686035 | BCE Loss: 1.0270867347717285\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.711063861846924 | KNN Loss: 3.6855080127716064 | BCE Loss: 1.0255558490753174\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.722482204437256 | KNN Loss: 3.7029292583465576 | BCE Loss: 1.0195528268814087\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.71533203125 | KNN Loss: 3.7024059295654297 | BCE Loss: 1.0129259824752808\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.750058174133301 | KNN Loss: 3.6965770721435547 | BCE Loss: 1.053481101989746\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.7025651931762695 | KNN Loss: 3.689592123031616 | BCE Loss: 1.0129730701446533\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.72641658782959 | KNN Loss: 3.7010786533355713 | BCE Loss: 1.0253381729125977\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.7157883644104 | KNN Loss: 3.6774656772613525 | BCE Loss: 1.0383226871490479\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.774160385131836 | KNN Loss: 3.699899196624756 | BCE Loss: 1.0742614269256592\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.749537944793701 | KNN Loss: 3.73721981048584 | BCE Loss: 1.0123180150985718\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.774656772613525 | KNN Loss: 3.745694637298584 | BCE Loss: 1.0289620161056519\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.741577625274658 | KNN Loss: 3.710268259048462 | BCE Loss: 1.0313093662261963\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.678868293762207 | KNN Loss: 3.672170877456665 | BCE Loss: 1.006697177886963\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.719891548156738 | KNN Loss: 3.673227548599243 | BCE Loss: 1.0466639995574951\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.776576042175293 | KNN Loss: 3.7338318824768066 | BCE Loss: 1.0427439212799072\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.780527591705322 | KNN Loss: 3.749268054962158 | BCE Loss: 1.0312594175338745\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.6995110511779785 | KNN Loss: 3.6743555068969727 | BCE Loss: 1.0251556634902954\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.774542331695557 | KNN Loss: 3.7174019813537598 | BCE Loss: 1.0571402311325073\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.755919456481934 | KNN Loss: 3.7159600257873535 | BCE Loss: 1.0399595499038696\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.703570365905762 | KNN Loss: 3.7163143157958984 | BCE Loss: 0.9872562885284424\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.74737548828125 | KNN Loss: 3.71806001663208 | BCE Loss: 1.0293152332305908\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.706527233123779 | KNN Loss: 3.711759090423584 | BCE Loss: 0.994767963886261\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.730292797088623 | KNN Loss: 3.7222952842712402 | BCE Loss: 1.0079976320266724\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.721334934234619 | KNN Loss: 3.6838555335998535 | BCE Loss: 1.0374794006347656\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.766714572906494 | KNN Loss: 3.698646068572998 | BCE Loss: 1.068068504333496\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.700226306915283 | KNN Loss: 3.6809422969818115 | BCE Loss: 1.0192841291427612\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.741086483001709 | KNN Loss: 3.7121732234954834 | BCE Loss: 1.0289132595062256\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.749661922454834 | KNN Loss: 3.718787670135498 | BCE Loss: 1.030874252319336\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.6765217781066895 | KNN Loss: 3.669267177581787 | BCE Loss: 1.0072546005249023\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.724991798400879 | KNN Loss: 3.7027039527893066 | BCE Loss: 1.0222876071929932\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.754666328430176 | KNN Loss: 3.734248638153076 | BCE Loss: 1.0204174518585205\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.7115349769592285 | KNN Loss: 3.701094388961792 | BCE Loss: 1.010440707206726\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.73227071762085 | KNN Loss: 3.690781593322754 | BCE Loss: 1.0414892435073853\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.737600803375244 | KNN Loss: 3.699345350265503 | BCE Loss: 1.0382554531097412\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.680650234222412 | KNN Loss: 3.679450750350952 | BCE Loss: 1.0011993646621704\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.738175868988037 | KNN Loss: 3.705099582672119 | BCE Loss: 1.0330761671066284\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.735567092895508 | KNN Loss: 3.7007734775543213 | BCE Loss: 1.034793496131897\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.7502899169921875 | KNN Loss: 3.7076992988586426 | BCE Loss: 1.042590856552124\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.7034592628479 | KNN Loss: 3.700077533721924 | BCE Loss: 1.003381609916687\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.716678619384766 | KNN Loss: 3.722895622253418 | BCE Loss: 0.9937828183174133\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.7165913581848145 | KNN Loss: 3.6641857624053955 | BCE Loss: 1.0524057149887085\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.718218803405762 | KNN Loss: 3.6967124938964844 | BCE Loss: 1.0215065479278564\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.742851734161377 | KNN Loss: 3.7233006954193115 | BCE Loss: 1.0195510387420654\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.709240913391113 | KNN Loss: 3.690013885498047 | BCE Loss: 1.0192272663116455\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.748601913452148 | KNN Loss: 3.7016847133636475 | BCE Loss: 1.0469169616699219\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.719364643096924 | KNN Loss: 3.7128758430480957 | BCE Loss: 1.0064888000488281\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.744423866271973 | KNN Loss: 3.7248902320861816 | BCE Loss: 1.019533634185791\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.739462852478027 | KNN Loss: 3.6904044151306152 | BCE Loss: 1.0490586757659912\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.760520935058594 | KNN Loss: 3.7093453407287598 | BCE Loss: 1.051175832748413\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.733362674713135 | KNN Loss: 3.697204351425171 | BCE Loss: 1.0361584424972534\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.720785140991211 | KNN Loss: 3.71535062789917 | BCE Loss: 1.0054343938827515\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.746364593505859 | KNN Loss: 3.7017910480499268 | BCE Loss: 1.0445737838745117\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.728095054626465 | KNN Loss: 3.7000722885131836 | BCE Loss: 1.0280226469039917\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.761062145233154 | KNN Loss: 3.7119789123535156 | BCE Loss: 1.0490832328796387\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.698348522186279 | KNN Loss: 3.677762508392334 | BCE Loss: 1.0205861330032349\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.728484153747559 | KNN Loss: 3.7128844261169434 | BCE Loss: 1.0155997276306152\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.747799873352051 | KNN Loss: 3.7130298614501953 | BCE Loss: 1.0347702503204346\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.705309867858887 | KNN Loss: 3.6780214309692383 | BCE Loss: 1.0272886753082275\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.699028968811035 | KNN Loss: 3.676905632019043 | BCE Loss: 1.0221235752105713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.691895484924316 | KNN Loss: 3.6823508739471436 | BCE Loss: 1.0095444917678833\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.69080924987793 | KNN Loss: 3.6834664344787598 | BCE Loss: 1.007343053817749\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.739829063415527 | KNN Loss: 3.700241804122925 | BCE Loss: 1.0395870208740234\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.762032508850098 | KNN Loss: 3.726933479309082 | BCE Loss: 1.0350992679595947\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.701656341552734 | KNN Loss: 3.6810414791107178 | BCE Loss: 1.020614743232727\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.736347675323486 | KNN Loss: 3.7102222442626953 | BCE Loss: 1.0261255502700806\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.748898506164551 | KNN Loss: 3.7047295570373535 | BCE Loss: 1.0441687107086182\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.734158992767334 | KNN Loss: 3.6883978843688965 | BCE Loss: 1.0457611083984375\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.669527530670166 | KNN Loss: 3.6810500621795654 | BCE Loss: 0.988477349281311\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.706144332885742 | KNN Loss: 3.706225633621216 | BCE Loss: 0.9999187588691711\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.71295166015625 | KNN Loss: 3.7014899253845215 | BCE Loss: 1.0114619731903076\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.800244331359863 | KNN Loss: 3.7400901317596436 | BCE Loss: 1.0601544380187988\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.72338342666626 | KNN Loss: 3.688201665878296 | BCE Loss: 1.0351817607879639\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.68384313583374 | KNN Loss: 3.67803955078125 | BCE Loss: 1.0058037042617798\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.744992256164551 | KNN Loss: 3.739084005355835 | BCE Loss: 1.0059082508087158\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.714237213134766 | KNN Loss: 3.6838653087615967 | BCE Loss: 1.030372142791748\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.747226715087891 | KNN Loss: 3.736440896987915 | BCE Loss: 1.010785698890686\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.725033760070801 | KNN Loss: 3.7295215129852295 | BCE Loss: 0.9955120086669922\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.7622456550598145 | KNN Loss: 3.7138030529022217 | BCE Loss: 1.0484424829483032\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.730391502380371 | KNN Loss: 3.720856189727783 | BCE Loss: 1.0095354318618774\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.722154140472412 | KNN Loss: 3.684328079223633 | BCE Loss: 1.0378260612487793\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.708284854888916 | KNN Loss: 3.6748392581939697 | BCE Loss: 1.0334454774856567\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.702795505523682 | KNN Loss: 3.694647789001465 | BCE Loss: 1.0081478357315063\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.7437872886657715 | KNN Loss: 3.717892646789551 | BCE Loss: 1.0258945226669312\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.707475662231445 | KNN Loss: 3.679654836654663 | BCE Loss: 1.0278208255767822\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.711920738220215 | KNN Loss: 3.6916377544403076 | BCE Loss: 1.0202827453613281\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.73019552230835 | KNN Loss: 3.7034242153167725 | BCE Loss: 1.0267714262008667\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.768488883972168 | KNN Loss: 3.7230308055877686 | BCE Loss: 1.0454579591751099\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.753495693206787 | KNN Loss: 3.7109360694885254 | BCE Loss: 1.0425596237182617\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.73651647567749 | KNN Loss: 3.7327682971954346 | BCE Loss: 1.0037480592727661\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.762601852416992 | KNN Loss: 3.709670066833496 | BCE Loss: 1.052931547164917\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.727126121520996 | KNN Loss: 3.7042529582977295 | BCE Loss: 1.0228731632232666\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.725242614746094 | KNN Loss: 3.7102768421173096 | BCE Loss: 1.0149660110473633\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.713746547698975 | KNN Loss: 3.6953229904174805 | BCE Loss: 1.0184236764907837\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.730042934417725 | KNN Loss: 3.724104642868042 | BCE Loss: 1.005938172340393\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.716641426086426 | KNN Loss: 3.687288761138916 | BCE Loss: 1.0293526649475098\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.713396072387695 | KNN Loss: 3.6967525482177734 | BCE Loss: 1.0166435241699219\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.755011081695557 | KNN Loss: 3.741690158843994 | BCE Loss: 1.013321042060852\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.721041202545166 | KNN Loss: 3.6980581283569336 | BCE Loss: 1.0229830741882324\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.741642475128174 | KNN Loss: 3.715587615966797 | BCE Loss: 1.026054859161377\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.715930461883545 | KNN Loss: 3.710387706756592 | BCE Loss: 1.0055426359176636\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.773738861083984 | KNN Loss: 3.726854085922241 | BCE Loss: 1.0468850135803223\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.715090751647949 | KNN Loss: 3.714442491531372 | BCE Loss: 1.0006484985351562\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.73341178894043 | KNN Loss: 3.7103545665740967 | BCE Loss: 1.023056983947754\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.686532497406006 | KNN Loss: 3.6790411472320557 | BCE Loss: 1.0074913501739502\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.720341682434082 | KNN Loss: 3.6915225982666016 | BCE Loss: 1.0288190841674805\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.763866424560547 | KNN Loss: 3.7389230728149414 | BCE Loss: 1.0249435901641846\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.742898941040039 | KNN Loss: 3.7041258811950684 | BCE Loss: 1.0387732982635498\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.8033647537231445 | KNN Loss: 3.7364492416381836 | BCE Loss: 1.066915512084961\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.716515064239502 | KNN Loss: 3.708087921142578 | BCE Loss: 1.0084271430969238\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.774753570556641 | KNN Loss: 3.729748487472534 | BCE Loss: 1.0450053215026855\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.692709445953369 | KNN Loss: 3.688939332962036 | BCE Loss: 1.003770112991333\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.741152763366699 | KNN Loss: 3.7244796752929688 | BCE Loss: 1.0166728496551514\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.6835198402404785 | KNN Loss: 3.680924892425537 | BCE Loss: 1.002595067024231\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.718007564544678 | KNN Loss: 3.680337905883789 | BCE Loss: 1.0376695394515991\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.733508110046387 | KNN Loss: 3.696547746658325 | BCE Loss: 1.0369601249694824\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.688762187957764 | KNN Loss: 3.6724658012390137 | BCE Loss: 1.0162965059280396\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.741652011871338 | KNN Loss: 3.7334349155426025 | BCE Loss: 1.0082170963287354\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.752991676330566 | KNN Loss: 3.7303762435913086 | BCE Loss: 1.022615671157837\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.779943943023682 | KNN Loss: 3.7263646125793457 | BCE Loss: 1.0535792112350464\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.753565788269043 | KNN Loss: 3.724241256713867 | BCE Loss: 1.0293245315551758\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.747856140136719 | KNN Loss: 3.720904588699341 | BCE Loss: 1.026951789855957\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.698873043060303 | KNN Loss: 3.6810030937194824 | BCE Loss: 1.0178698301315308\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.713935852050781 | KNN Loss: 3.6922354698181152 | BCE Loss: 1.021700382232666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.726978302001953 | KNN Loss: 3.6990952491760254 | BCE Loss: 1.0278832912445068\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.730175018310547 | KNN Loss: 3.7058253288269043 | BCE Loss: 1.0243499279022217\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.750066757202148 | KNN Loss: 3.7111928462982178 | BCE Loss: 1.0388739109039307\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.778169631958008 | KNN Loss: 3.717513084411621 | BCE Loss: 1.0606567859649658\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.667409420013428 | KNN Loss: 3.6733477115631104 | BCE Loss: 0.9940618276596069\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.783348083496094 | KNN Loss: 3.7502939701080322 | BCE Loss: 1.0330541133880615\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.704705238342285 | KNN Loss: 3.6838345527648926 | BCE Loss: 1.020870566368103\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.74923038482666 | KNN Loss: 3.7266347408294678 | BCE Loss: 1.022595763206482\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.7336602210998535 | KNN Loss: 3.717367172241211 | BCE Loss: 1.016292929649353\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.698478698730469 | KNN Loss: 3.6737558841705322 | BCE Loss: 1.0247230529785156\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.708606719970703 | KNN Loss: 3.676635265350342 | BCE Loss: 1.0319712162017822\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.728066921234131 | KNN Loss: 3.710864543914795 | BCE Loss: 1.0172022581100464\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.741481304168701 | KNN Loss: 3.7352757453918457 | BCE Loss: 1.0062055587768555\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.716394424438477 | KNN Loss: 3.6948671340942383 | BCE Loss: 1.0215275287628174\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.7260541915893555 | KNN Loss: 3.698791742324829 | BCE Loss: 1.0272624492645264\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.739272117614746 | KNN Loss: 3.7237377166748047 | BCE Loss: 1.0155344009399414\n",
      "Epoch   153: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.69925594329834 | KNN Loss: 3.680483818054199 | BCE Loss: 1.018772006034851\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.732645511627197 | KNN Loss: 3.6748945713043213 | BCE Loss: 1.0577510595321655\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.714481353759766 | KNN Loss: 3.6961374282836914 | BCE Loss: 1.0183441638946533\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.737884044647217 | KNN Loss: 3.718191623687744 | BCE Loss: 1.0196924209594727\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.708940029144287 | KNN Loss: 3.6945395469665527 | BCE Loss: 1.0144003629684448\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.713850975036621 | KNN Loss: 3.690843105316162 | BCE Loss: 1.0230076313018799\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.723952293395996 | KNN Loss: 3.7009799480438232 | BCE Loss: 1.022972583770752\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.711417198181152 | KNN Loss: 3.6944406032562256 | BCE Loss: 1.0169767141342163\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.705741882324219 | KNN Loss: 3.6940724849700928 | BCE Loss: 1.0116692781448364\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.734591007232666 | KNN Loss: 3.692692995071411 | BCE Loss: 1.0418978929519653\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.69520378112793 | KNN Loss: 3.6726338863372803 | BCE Loss: 1.0225697755813599\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.749972343444824 | KNN Loss: 3.70176100730896 | BCE Loss: 1.0482110977172852\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.715217113494873 | KNN Loss: 3.70035457611084 | BCE Loss: 1.0148624181747437\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.747912406921387 | KNN Loss: 3.703197717666626 | BCE Loss: 1.0447146892547607\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.735577583312988 | KNN Loss: 3.7164742946624756 | BCE Loss: 1.0191030502319336\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.726240158081055 | KNN Loss: 3.695734977722168 | BCE Loss: 1.0305051803588867\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.674906253814697 | KNN Loss: 3.6612226963043213 | BCE Loss: 1.013683557510376\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.662796974182129 | KNN Loss: 3.662473440170288 | BCE Loss: 1.0003232955932617\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.7425336837768555 | KNN Loss: 3.720005750656128 | BCE Loss: 1.0225276947021484\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.7120513916015625 | KNN Loss: 3.693964958190918 | BCE Loss: 1.0180861949920654\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.754992961883545 | KNN Loss: 3.746796131134033 | BCE Loss: 1.0081968307495117\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.700030326843262 | KNN Loss: 3.684674024581909 | BCE Loss: 1.015356183052063\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.724763870239258 | KNN Loss: 3.6767401695251465 | BCE Loss: 1.0480237007141113\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.724906921386719 | KNN Loss: 3.6909172534942627 | BCE Loss: 1.033989667892456\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.677469253540039 | KNN Loss: 3.669271945953369 | BCE Loss: 1.0081974267959595\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.731733322143555 | KNN Loss: 3.723442554473877 | BCE Loss: 1.0082910060882568\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.715784072875977 | KNN Loss: 3.706467390060425 | BCE Loss: 1.0093169212341309\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.684215545654297 | KNN Loss: 3.6784920692443848 | BCE Loss: 1.0057237148284912\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.747438907623291 | KNN Loss: 3.703552722930908 | BCE Loss: 1.0438861846923828\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.67645788192749 | KNN Loss: 3.6767261028289795 | BCE Loss: 0.999731719493866\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.732656002044678 | KNN Loss: 3.7040319442749023 | BCE Loss: 1.028624176979065\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.715408802032471 | KNN Loss: 3.6897270679473877 | BCE Loss: 1.0256816148757935\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.75098180770874 | KNN Loss: 3.6952388286590576 | BCE Loss: 1.055742859840393\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.70237922668457 | KNN Loss: 3.684980869293213 | BCE Loss: 1.0173981189727783\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.7159576416015625 | KNN Loss: 3.6908576488494873 | BCE Loss: 1.0250999927520752\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.731692314147949 | KNN Loss: 3.6860008239746094 | BCE Loss: 1.0456912517547607\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.732080459594727 | KNN Loss: 3.720336437225342 | BCE Loss: 1.0117440223693848\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.728359222412109 | KNN Loss: 3.6632773876190186 | BCE Loss: 1.06508207321167\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.732382774353027 | KNN Loss: 3.724351406097412 | BCE Loss: 1.0080316066741943\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.720264434814453 | KNN Loss: 3.7008614540100098 | BCE Loss: 1.0194032192230225\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.752520561218262 | KNN Loss: 3.6930747032165527 | BCE Loss: 1.0594457387924194\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.755016326904297 | KNN Loss: 3.7104926109313965 | BCE Loss: 1.0445234775543213\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.730079174041748 | KNN Loss: 3.7051455974578857 | BCE Loss: 1.0249334573745728\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.732241153717041 | KNN Loss: 3.685035467147827 | BCE Loss: 1.0472056865692139\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.707595348358154 | KNN Loss: 3.677699565887451 | BCE Loss: 1.0298956632614136\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.714406967163086 | KNN Loss: 3.692716598510742 | BCE Loss: 1.0216902494430542\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.729450225830078 | KNN Loss: 3.7093563079833984 | BCE Loss: 1.0200939178466797\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.698210716247559 | KNN Loss: 3.6748037338256836 | BCE Loss: 1.023407220840454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.720484733581543 | KNN Loss: 3.7381415367126465 | BCE Loss: 0.9823431968688965\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.719350814819336 | KNN Loss: 3.724574327468872 | BCE Loss: 0.9947762489318848\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.696784973144531 | KNN Loss: 3.6837544441223145 | BCE Loss: 1.0130305290222168\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.747257709503174 | KNN Loss: 3.7201690673828125 | BCE Loss: 1.0270886421203613\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.718799591064453 | KNN Loss: 3.68786358833313 | BCE Loss: 1.0309357643127441\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.737159252166748 | KNN Loss: 3.6986985206604004 | BCE Loss: 1.0384607315063477\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.769438743591309 | KNN Loss: 3.712846279144287 | BCE Loss: 1.0565927028656006\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.71798038482666 | KNN Loss: 3.6960742473602295 | BCE Loss: 1.0219062566757202\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.744958877563477 | KNN Loss: 3.7153074741363525 | BCE Loss: 1.0296515226364136\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.730498790740967 | KNN Loss: 3.697166681289673 | BCE Loss: 1.033332109451294\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.741025447845459 | KNN Loss: 3.703293561935425 | BCE Loss: 1.0377320051193237\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.725188255310059 | KNN Loss: 3.6950552463531494 | BCE Loss: 1.0301332473754883\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.703570365905762 | KNN Loss: 3.6810660362243652 | BCE Loss: 1.0225040912628174\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.720244884490967 | KNN Loss: 3.7125182151794434 | BCE Loss: 1.0077265501022339\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.789780616760254 | KNN Loss: 3.7301158905029297 | BCE Loss: 1.0596646070480347\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.722155570983887 | KNN Loss: 3.6966392993927 | BCE Loss: 1.025516152381897\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.768725395202637 | KNN Loss: 3.7377164363861084 | BCE Loss: 1.0310087203979492\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.73938512802124 | KNN Loss: 3.7035973072052 | BCE Loss: 1.03578782081604\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.729077339172363 | KNN Loss: 3.709212064743042 | BCE Loss: 1.0198650360107422\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.738965034484863 | KNN Loss: 3.690699577331543 | BCE Loss: 1.0482654571533203\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.709261417388916 | KNN Loss: 3.6719117164611816 | BCE Loss: 1.0373497009277344\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.729054927825928 | KNN Loss: 3.689730644226074 | BCE Loss: 1.0393242835998535\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.696800231933594 | KNN Loss: 3.6873245239257812 | BCE Loss: 1.0094759464263916\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.737865447998047 | KNN Loss: 3.6884403228759766 | BCE Loss: 1.0494253635406494\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.723119735717773 | KNN Loss: 3.6915323734283447 | BCE Loss: 1.0315871238708496\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.773590564727783 | KNN Loss: 3.7287068367004395 | BCE Loss: 1.0448837280273438\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.735791206359863 | KNN Loss: 3.702953577041626 | BCE Loss: 1.0328373908996582\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.69814395904541 | KNN Loss: 3.6846539974212646 | BCE Loss: 1.0134902000427246\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.718392372131348 | KNN Loss: 3.6956186294555664 | BCE Loss: 1.0227735042572021\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.742190361022949 | KNN Loss: 3.711918592453003 | BCE Loss: 1.0302716493606567\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.744352340698242 | KNN Loss: 3.7126336097717285 | BCE Loss: 1.0317188501358032\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.743519306182861 | KNN Loss: 3.7117648124694824 | BCE Loss: 1.031754493713379\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.7786455154418945 | KNN Loss: 3.735480308532715 | BCE Loss: 1.0431649684906006\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.703686714172363 | KNN Loss: 3.694519281387329 | BCE Loss: 1.0091675519943237\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.74114465713501 | KNN Loss: 3.710700511932373 | BCE Loss: 1.0304441452026367\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.741901397705078 | KNN Loss: 3.7006266117095947 | BCE Loss: 1.0412746667861938\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.663994789123535 | KNN Loss: 3.6675593852996826 | BCE Loss: 0.9964353442192078\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.726431846618652 | KNN Loss: 3.7032406330108643 | BCE Loss: 1.023190975189209\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.704204559326172 | KNN Loss: 3.6712512969970703 | BCE Loss: 1.0329535007476807\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.7233662605285645 | KNN Loss: 3.709226131439209 | BCE Loss: 1.0141401290893555\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.684897422790527 | KNN Loss: 3.6808111667633057 | BCE Loss: 1.0040860176086426\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.713521957397461 | KNN Loss: 3.676403522491455 | BCE Loss: 1.0371181964874268\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.701425552368164 | KNN Loss: 3.6929054260253906 | BCE Loss: 1.0085198879241943\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.732415676116943 | KNN Loss: 3.7097420692443848 | BCE Loss: 1.0226737260818481\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.7591166496276855 | KNN Loss: 3.7133350372314453 | BCE Loss: 1.0457816123962402\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.719189643859863 | KNN Loss: 3.6867663860321045 | BCE Loss: 1.0324232578277588\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.7145795822143555 | KNN Loss: 3.6922719478607178 | BCE Loss: 1.0223078727722168\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.689516067504883 | KNN Loss: 3.686995029449463 | BCE Loss: 1.002521276473999\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.755185127258301 | KNN Loss: 3.7305667400360107 | BCE Loss: 1.02461838722229\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.810164928436279 | KNN Loss: 3.7540979385375977 | BCE Loss: 1.0560669898986816\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.785001754760742 | KNN Loss: 3.7146363258361816 | BCE Loss: 1.0703654289245605\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.710257530212402 | KNN Loss: 3.716668128967285 | BCE Loss: 0.9935896396636963\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.7422919273376465 | KNN Loss: 3.7145161628723145 | BCE Loss: 1.027775764465332\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.742857456207275 | KNN Loss: 3.7252111434936523 | BCE Loss: 1.017646312713623\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.720947265625 | KNN Loss: 3.705556869506836 | BCE Loss: 1.015390396118164\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.710594177246094 | KNN Loss: 3.687018394470215 | BCE Loss: 1.0235755443572998\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.726632595062256 | KNN Loss: 3.6776301860809326 | BCE Loss: 1.0490024089813232\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.7575860023498535 | KNN Loss: 3.7193307876586914 | BCE Loss: 1.038255214691162\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.753171443939209 | KNN Loss: 3.712308883666992 | BCE Loss: 1.0408625602722168\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.722311019897461 | KNN Loss: 3.7200429439544678 | BCE Loss: 1.0022683143615723\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.754393100738525 | KNN Loss: 3.7205324172973633 | BCE Loss: 1.0338605642318726\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.744158744812012 | KNN Loss: 3.726069211959839 | BCE Loss: 1.0180894136428833\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.73202657699585 | KNN Loss: 3.7031383514404297 | BCE Loss: 1.02888822555542\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.778557777404785 | KNN Loss: 3.7394392490386963 | BCE Loss: 1.0391185283660889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.738338947296143 | KNN Loss: 3.7039976119995117 | BCE Loss: 1.0343414545059204\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.732970714569092 | KNN Loss: 3.707430839538574 | BCE Loss: 1.0255398750305176\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.76054573059082 | KNN Loss: 3.707520008087158 | BCE Loss: 1.0530259609222412\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.693306922912598 | KNN Loss: 3.6857004165649414 | BCE Loss: 1.0076066255569458\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.778915882110596 | KNN Loss: 3.742236614227295 | BCE Loss: 1.0366792678833008\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.733243942260742 | KNN Loss: 3.7092552185058594 | BCE Loss: 1.0239884853363037\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.717403888702393 | KNN Loss: 3.691443681716919 | BCE Loss: 1.0259602069854736\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.763835906982422 | KNN Loss: 3.7244560718536377 | BCE Loss: 1.039379596710205\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.690219402313232 | KNN Loss: 3.6581265926361084 | BCE Loss: 1.032092809677124\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.690450191497803 | KNN Loss: 3.6719553470611572 | BCE Loss: 1.018494725227356\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.724422454833984 | KNN Loss: 3.6906063556671143 | BCE Loss: 1.033815860748291\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.741113662719727 | KNN Loss: 3.721121311187744 | BCE Loss: 1.0199925899505615\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.765417575836182 | KNN Loss: 3.711995840072632 | BCE Loss: 1.0534217357635498\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.687571048736572 | KNN Loss: 3.6841962337493896 | BCE Loss: 1.0033748149871826\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.743664264678955 | KNN Loss: 3.6946394443511963 | BCE Loss: 1.0490248203277588\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.738161087036133 | KNN Loss: 3.7070038318634033 | BCE Loss: 1.0311572551727295\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.676220417022705 | KNN Loss: 3.667647361755371 | BCE Loss: 1.0085731744766235\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.727109909057617 | KNN Loss: 3.7092137336730957 | BCE Loss: 1.0178964138031006\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.7332048416137695 | KNN Loss: 3.697547197341919 | BCE Loss: 1.0356574058532715\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.697343826293945 | KNN Loss: 3.6772944927215576 | BCE Loss: 1.0200495719909668\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.72896671295166 | KNN Loss: 3.7080764770507812 | BCE Loss: 1.020890235900879\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.7495293617248535 | KNN Loss: 3.700489044189453 | BCE Loss: 1.0490401983261108\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.6715087890625 | KNN Loss: 3.6709108352661133 | BCE Loss: 1.0005978345870972\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.706245422363281 | KNN Loss: 3.674701452255249 | BCE Loss: 1.0315437316894531\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.699385643005371 | KNN Loss: 3.6839466094970703 | BCE Loss: 1.0154390335083008\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.7097320556640625 | KNN Loss: 3.677732467651367 | BCE Loss: 1.0319998264312744\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.730336666107178 | KNN Loss: 3.6705148220062256 | BCE Loss: 1.0598218441009521\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.680313587188721 | KNN Loss: 3.6855127811431885 | BCE Loss: 0.9948009848594666\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.688753128051758 | KNN Loss: 3.6823689937591553 | BCE Loss: 1.0063843727111816\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.741813659667969 | KNN Loss: 3.713123321533203 | BCE Loss: 1.0286903381347656\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.711318016052246 | KNN Loss: 3.657797336578369 | BCE Loss: 1.053520679473877\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.738916397094727 | KNN Loss: 3.7152657508850098 | BCE Loss: 1.0236504077911377\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.76828670501709 | KNN Loss: 3.7365286350250244 | BCE Loss: 1.0317578315734863\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.688606262207031 | KNN Loss: 3.670398712158203 | BCE Loss: 1.0182075500488281\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.724381446838379 | KNN Loss: 3.703629493713379 | BCE Loss: 1.0207520723342896\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.747256755828857 | KNN Loss: 3.688295841217041 | BCE Loss: 1.058961033821106\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.727126598358154 | KNN Loss: 3.7110941410064697 | BCE Loss: 1.016032338142395\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.714456558227539 | KNN Loss: 3.6776506900787354 | BCE Loss: 1.0368059873580933\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.744762420654297 | KNN Loss: 3.714061737060547 | BCE Loss: 1.030700922012329\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.714827537536621 | KNN Loss: 3.701608180999756 | BCE Loss: 1.0132193565368652\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.6849799156188965 | KNN Loss: 3.672848701477051 | BCE Loss: 1.0121310949325562\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.732194900512695 | KNN Loss: 3.694864511489868 | BCE Loss: 1.0373303890228271\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.745432376861572 | KNN Loss: 3.7141618728637695 | BCE Loss: 1.0312705039978027\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.672187805175781 | KNN Loss: 3.660141706466675 | BCE Loss: 1.012046217918396\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.71364164352417 | KNN Loss: 3.7102138996124268 | BCE Loss: 1.0034278631210327\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.74321174621582 | KNN Loss: 3.71246600151062 | BCE Loss: 1.0307458639144897\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.727359294891357 | KNN Loss: 3.715919256210327 | BCE Loss: 1.0114400386810303\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.690673351287842 | KNN Loss: 3.674914836883545 | BCE Loss: 1.0157585144042969\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.710231304168701 | KNN Loss: 3.711033344268799 | BCE Loss: 0.9991978406906128\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.744823455810547 | KNN Loss: 3.699803352355957 | BCE Loss: 1.045020341873169\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.697780609130859 | KNN Loss: 3.6808862686157227 | BCE Loss: 1.0168943405151367\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.705387592315674 | KNN Loss: 3.7124247550964355 | BCE Loss: 0.9929628372192383\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.735393524169922 | KNN Loss: 3.716508388519287 | BCE Loss: 1.0188851356506348\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.724645614624023 | KNN Loss: 3.6860270500183105 | BCE Loss: 1.038618564605713\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.754190921783447 | KNN Loss: 3.7063984870910645 | BCE Loss: 1.0477923154830933\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.702025890350342 | KNN Loss: 3.6849374771118164 | BCE Loss: 1.0170884132385254\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.748347282409668 | KNN Loss: 3.71667742729187 | BCE Loss: 1.0316698551177979\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.759105682373047 | KNN Loss: 3.7186777591705322 | BCE Loss: 1.0404279232025146\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.737682342529297 | KNN Loss: 3.720668315887451 | BCE Loss: 1.0170139074325562\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.744792461395264 | KNN Loss: 3.680410861968994 | BCE Loss: 1.064381718635559\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.722774505615234 | KNN Loss: 3.7251687049865723 | BCE Loss: 0.997605562210083\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.657220840454102 | KNN Loss: 3.6656179428100586 | BCE Loss: 0.991602897644043\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.69911003112793 | KNN Loss: 3.6756272315979004 | BCE Loss: 1.0234830379486084\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.699017524719238 | KNN Loss: 3.6937687397003174 | BCE Loss: 1.0052490234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.777622699737549 | KNN Loss: 3.740391492843628 | BCE Loss: 1.037231206893921\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.732438087463379 | KNN Loss: 3.68967342376709 | BCE Loss: 1.0427649021148682\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.704916000366211 | KNN Loss: 3.6908280849456787 | BCE Loss: 1.0140880346298218\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.771174430847168 | KNN Loss: 3.725107431411743 | BCE Loss: 1.046067237854004\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.725032329559326 | KNN Loss: 3.6630303859710693 | BCE Loss: 1.0620019435882568\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.743923664093018 | KNN Loss: 3.7194745540618896 | BCE Loss: 1.024449110031128\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.750846862792969 | KNN Loss: 3.72432541847229 | BCE Loss: 1.0265214443206787\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.690490245819092 | KNN Loss: 3.7020835876464844 | BCE Loss: 0.9884064793586731\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.7450737953186035 | KNN Loss: 3.6966028213500977 | BCE Loss: 1.0484708547592163\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.731701374053955 | KNN Loss: 3.7060792446136475 | BCE Loss: 1.0256221294403076\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.7472028732299805 | KNN Loss: 3.7414956092834473 | BCE Loss: 1.005707025527954\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.72097110748291 | KNN Loss: 3.6786060333251953 | BCE Loss: 1.0423649549484253\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.765045166015625 | KNN Loss: 3.7270448207855225 | BCE Loss: 1.0380005836486816\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.75356388092041 | KNN Loss: 3.7113723754882812 | BCE Loss: 1.042191505432129\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.70866584777832 | KNN Loss: 3.700350284576416 | BCE Loss: 1.0083155632019043\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.7135844230651855 | KNN Loss: 3.699448347091675 | BCE Loss: 1.0141359567642212\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.725489616394043 | KNN Loss: 3.731132984161377 | BCE Loss: 0.9943565726280212\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.724564552307129 | KNN Loss: 3.713073253631592 | BCE Loss: 1.0114914178848267\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.734984874725342 | KNN Loss: 3.702625036239624 | BCE Loss: 1.0323597192764282\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.716753959655762 | KNN Loss: 3.681605577468872 | BCE Loss: 1.0351486206054688\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.737234115600586 | KNN Loss: 3.705080986022949 | BCE Loss: 1.0321533679962158\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.687402725219727 | KNN Loss: 3.669335126876831 | BCE Loss: 1.0180678367614746\n",
      "Epoch   186: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.722038269042969 | KNN Loss: 3.6900155544281006 | BCE Loss: 1.0320227146148682\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.7070183753967285 | KNN Loss: 3.7126262187957764 | BCE Loss: 0.9943921566009521\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.744076728820801 | KNN Loss: 3.6997878551483154 | BCE Loss: 1.044288992881775\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.684285640716553 | KNN Loss: 3.665287733078003 | BCE Loss: 1.0189979076385498\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.717648029327393 | KNN Loss: 3.661081075668335 | BCE Loss: 1.056566834449768\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.668120384216309 | KNN Loss: 3.6549596786499023 | BCE Loss: 1.0131605863571167\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.677064895629883 | KNN Loss: 3.6936655044555664 | BCE Loss: 0.9833996295928955\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.718717575073242 | KNN Loss: 3.695535898208618 | BCE Loss: 1.023181438446045\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.68986701965332 | KNN Loss: 3.6732916831970215 | BCE Loss: 1.0165750980377197\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.703256130218506 | KNN Loss: 3.6838417053222656 | BCE Loss: 1.0194145441055298\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.7350568771362305 | KNN Loss: 3.682966947555542 | BCE Loss: 1.0520899295806885\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.692536354064941 | KNN Loss: 3.683774471282959 | BCE Loss: 1.0087618827819824\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.726862907409668 | KNN Loss: 3.696056365966797 | BCE Loss: 1.030806541442871\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.697141647338867 | KNN Loss: 3.7133169174194336 | BCE Loss: 0.9838247299194336\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.753323554992676 | KNN Loss: 3.6931397914886475 | BCE Loss: 1.0601835250854492\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.747496604919434 | KNN Loss: 3.7297134399414062 | BCE Loss: 1.0177834033966064\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.753863334655762 | KNN Loss: 3.714186191558838 | BCE Loss: 1.0396769046783447\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.728610992431641 | KNN Loss: 3.6927831172943115 | BCE Loss: 1.03582763671875\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.712927341461182 | KNN Loss: 3.7031936645507812 | BCE Loss: 1.0097336769104004\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.728578090667725 | KNN Loss: 3.703380823135376 | BCE Loss: 1.0251972675323486\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.715331077575684 | KNN Loss: 3.6875393390655518 | BCE Loss: 1.0277917385101318\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.706040382385254 | KNN Loss: 3.691997528076172 | BCE Loss: 1.0140430927276611\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.710943222045898 | KNN Loss: 3.688788652420044 | BCE Loss: 1.0221543312072754\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.753666877746582 | KNN Loss: 3.7022926807403564 | BCE Loss: 1.0513739585876465\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.755748748779297 | KNN Loss: 3.722583293914795 | BCE Loss: 1.0331655740737915\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.764623641967773 | KNN Loss: 3.696115493774414 | BCE Loss: 1.0685081481933594\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.745645523071289 | KNN Loss: 3.699592113494873 | BCE Loss: 1.046053171157837\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.7244672775268555 | KNN Loss: 3.695516586303711 | BCE Loss: 1.0289504528045654\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.726531028747559 | KNN Loss: 3.700587272644043 | BCE Loss: 1.0259439945220947\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.731593608856201 | KNN Loss: 3.7265005111694336 | BCE Loss: 1.0050930976867676\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.697488784790039 | KNN Loss: 3.6802937984466553 | BCE Loss: 1.0171947479248047\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.723104000091553 | KNN Loss: 3.671818733215332 | BCE Loss: 1.0512853860855103\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.691285610198975 | KNN Loss: 3.680330753326416 | BCE Loss: 1.0109549760818481\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.755076885223389 | KNN Loss: 3.7356441020965576 | BCE Loss: 1.019432783126831\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.737214088439941 | KNN Loss: 3.7090578079223633 | BCE Loss: 1.0281562805175781\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.697790145874023 | KNN Loss: 3.6962971687316895 | BCE Loss: 1.0014930963516235\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.769931793212891 | KNN Loss: 3.7465782165527344 | BCE Loss: 1.0233533382415771\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.733427047729492 | KNN Loss: 3.7337417602539062 | BCE Loss: 0.999685525894165\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.726474761962891 | KNN Loss: 3.701253652572632 | BCE Loss: 1.0252211093902588\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.777521133422852 | KNN Loss: 3.706132411956787 | BCE Loss: 1.0713887214660645\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.68489933013916 | KNN Loss: 3.682246446609497 | BCE Loss: 1.0026531219482422\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.700726509094238 | KNN Loss: 3.679208278656006 | BCE Loss: 1.0215179920196533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.692324161529541 | KNN Loss: 3.6948978900909424 | BCE Loss: 0.9974261522293091\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.749891757965088 | KNN Loss: 3.7270169258117676 | BCE Loss: 1.0228747129440308\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.7060651779174805 | KNN Loss: 3.7034506797790527 | BCE Loss: 1.0026146173477173\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.730594635009766 | KNN Loss: 3.688636541366577 | BCE Loss: 1.0419583320617676\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.700528621673584 | KNN Loss: 3.6944522857666016 | BCE Loss: 1.0060762166976929\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.676318168640137 | KNN Loss: 3.670138359069824 | BCE Loss: 1.006179690361023\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.693743705749512 | KNN Loss: 3.68746280670166 | BCE Loss: 1.0062806606292725\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.776888847351074 | KNN Loss: 3.7225799560546875 | BCE Loss: 1.0543091297149658\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.707584381103516 | KNN Loss: 3.6786656379699707 | BCE Loss: 1.028918981552124\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.72174596786499 | KNN Loss: 3.695539712905884 | BCE Loss: 1.026206135749817\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.71004056930542 | KNN Loss: 3.699702501296997 | BCE Loss: 1.0103380680084229\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.725193500518799 | KNN Loss: 3.707394599914551 | BCE Loss: 1.017798900604248\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.7383270263671875 | KNN Loss: 3.712059736251831 | BCE Loss: 1.0262675285339355\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.765719413757324 | KNN Loss: 3.7394535541534424 | BCE Loss: 1.026266098022461\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.773808002471924 | KNN Loss: 3.7488698959350586 | BCE Loss: 1.0249379873275757\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.749305248260498 | KNN Loss: 3.7277228832244873 | BCE Loss: 1.0215823650360107\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.757788181304932 | KNN Loss: 3.709390878677368 | BCE Loss: 1.0483973026275635\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.737001895904541 | KNN Loss: 3.6797451972961426 | BCE Loss: 1.0572566986083984\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.719105243682861 | KNN Loss: 3.6830074787139893 | BCE Loss: 1.0360976457595825\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.740085601806641 | KNN Loss: 3.675511121749878 | BCE Loss: 1.0645742416381836\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.735995769500732 | KNN Loss: 3.709510087966919 | BCE Loss: 1.026485800743103\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.774352073669434 | KNN Loss: 3.762936592102051 | BCE Loss: 1.011415719985962\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.685942649841309 | KNN Loss: 3.7042691707611084 | BCE Loss: 0.9816733598709106\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.698159217834473 | KNN Loss: 3.657888650894165 | BCE Loss: 1.0402706861495972\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.759511470794678 | KNN Loss: 3.722637176513672 | BCE Loss: 1.0368741750717163\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.7396931648254395 | KNN Loss: 3.7035861015319824 | BCE Loss: 1.0361069440841675\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.687595367431641 | KNN Loss: 3.702730894088745 | BCE Loss: 0.9848646521568298\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.743024826049805 | KNN Loss: 3.7287960052490234 | BCE Loss: 1.0142288208007812\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.773959636688232 | KNN Loss: 3.733715295791626 | BCE Loss: 1.040244460105896\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.715019702911377 | KNN Loss: 3.6923537254333496 | BCE Loss: 1.0226659774780273\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.753384590148926 | KNN Loss: 3.7104640007019043 | BCE Loss: 1.0429203510284424\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.719362258911133 | KNN Loss: 3.678086042404175 | BCE Loss: 1.041275978088379\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.704583168029785 | KNN Loss: 3.707690715789795 | BCE Loss: 0.9968926906585693\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.70346212387085 | KNN Loss: 3.698615789413452 | BCE Loss: 1.0048463344573975\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.715566635131836 | KNN Loss: 3.7004942893981934 | BCE Loss: 1.015072226524353\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.779518127441406 | KNN Loss: 3.704166889190674 | BCE Loss: 1.0753514766693115\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.716771125793457 | KNN Loss: 3.6696884632110596 | BCE Loss: 1.0470824241638184\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.7562174797058105 | KNN Loss: 3.737161874771118 | BCE Loss: 1.0190556049346924\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.719188690185547 | KNN Loss: 3.6927947998046875 | BCE Loss: 1.026394009590149\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.746039390563965 | KNN Loss: 3.6874001026153564 | BCE Loss: 1.0586391687393188\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.713663101196289 | KNN Loss: 3.681671380996704 | BCE Loss: 1.031991720199585\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.746302127838135 | KNN Loss: 3.7091729640960693 | BCE Loss: 1.0371291637420654\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.710253715515137 | KNN Loss: 3.6970953941345215 | BCE Loss: 1.0131585597991943\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.7185378074646 | KNN Loss: 3.6939640045166016 | BCE Loss: 1.0245736837387085\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.726706504821777 | KNN Loss: 3.7139649391174316 | BCE Loss: 1.0127418041229248\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.7079877853393555 | KNN Loss: 3.681607484817505 | BCE Loss: 1.0263800621032715\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.732318878173828 | KNN Loss: 3.7140583992004395 | BCE Loss: 1.0182607173919678\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.729579925537109 | KNN Loss: 3.715472936630249 | BCE Loss: 1.01410710811615\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.6881585121154785 | KNN Loss: 3.672999858856201 | BCE Loss: 1.015158772468567\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.7028303146362305 | KNN Loss: 3.678208112716675 | BCE Loss: 1.0246222019195557\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.727117538452148 | KNN Loss: 3.6834523677825928 | BCE Loss: 1.0436654090881348\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.707345008850098 | KNN Loss: 3.6973907947540283 | BCE Loss: 1.0099540948867798\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.729330062866211 | KNN Loss: 3.696514368057251 | BCE Loss: 1.032815933227539\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.732637405395508 | KNN Loss: 3.713292360305786 | BCE Loss: 1.0193450450897217\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.689269065856934 | KNN Loss: 3.6662683486938477 | BCE Loss: 1.0230004787445068\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.703530311584473 | KNN Loss: 3.6673192977905273 | BCE Loss: 1.0362112522125244\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.699512481689453 | KNN Loss: 3.6827094554901123 | BCE Loss: 1.01680326461792\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.739235877990723 | KNN Loss: 3.7158939838409424 | BCE Loss: 1.0233416557312012\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.690004348754883 | KNN Loss: 3.6600306034088135 | BCE Loss: 1.0299739837646484\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.767845153808594 | KNN Loss: 3.7348453998565674 | BCE Loss: 1.032999873161316\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.774806022644043 | KNN Loss: 3.71569561958313 | BCE Loss: 1.059110164642334\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.760412693023682 | KNN Loss: 3.710268259048462 | BCE Loss: 1.0501444339752197\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.693715572357178 | KNN Loss: 3.682330369949341 | BCE Loss: 1.011385202407837\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.743691444396973 | KNN Loss: 3.7108373641967773 | BCE Loss: 1.0328539609909058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.700628280639648 | KNN Loss: 3.691561222076416 | BCE Loss: 1.0090668201446533\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.697819232940674 | KNN Loss: 3.67606258392334 | BCE Loss: 1.021756649017334\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.726080417633057 | KNN Loss: 3.70048451423645 | BCE Loss: 1.025596022605896\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.714714050292969 | KNN Loss: 3.6922948360443115 | BCE Loss: 1.0224194526672363\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.7030768394470215 | KNN Loss: 3.7001852989196777 | BCE Loss: 1.0028916597366333\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.710176467895508 | KNN Loss: 3.698385715484619 | BCE Loss: 1.0117907524108887\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.782822608947754 | KNN Loss: 3.708951950073242 | BCE Loss: 1.0738704204559326\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.726590156555176 | KNN Loss: 3.6882076263427734 | BCE Loss: 1.0383822917938232\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.735410690307617 | KNN Loss: 3.719541549682617 | BCE Loss: 1.015869379043579\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.721698760986328 | KNN Loss: 3.7037291526794434 | BCE Loss: 1.0179698467254639\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.718413829803467 | KNN Loss: 3.695676565170288 | BCE Loss: 1.0227373838424683\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.763439178466797 | KNN Loss: 3.7172834873199463 | BCE Loss: 1.0461559295654297\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.682986259460449 | KNN Loss: 3.6873483657836914 | BCE Loss: 0.995637834072113\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.711484432220459 | KNN Loss: 3.674048900604248 | BCE Loss: 1.0374356508255005\n",
      "Epoch   206: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.672268867492676 | KNN Loss: 3.663059711456299 | BCE Loss: 1.0092089176177979\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.7170538902282715 | KNN Loss: 3.694624662399292 | BCE Loss: 1.0224292278289795\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.729905128479004 | KNN Loss: 3.678866147994995 | BCE Loss: 1.0510389804840088\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.741357803344727 | KNN Loss: 3.6865456104278564 | BCE Loss: 1.0548124313354492\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.732657432556152 | KNN Loss: 3.7124760150909424 | BCE Loss: 1.0201811790466309\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.733743667602539 | KNN Loss: 3.718742847442627 | BCE Loss: 1.0150007009506226\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.717473983764648 | KNN Loss: 3.6963891983032227 | BCE Loss: 1.0210847854614258\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.702339172363281 | KNN Loss: 3.6932408809661865 | BCE Loss: 1.0090985298156738\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.679291725158691 | KNN Loss: 3.6782658100128174 | BCE Loss: 1.001025676727295\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.744991302490234 | KNN Loss: 3.689457654953003 | BCE Loss: 1.0555336475372314\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.7338128089904785 | KNN Loss: 3.686088800430298 | BCE Loss: 1.0477240085601807\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.716532230377197 | KNN Loss: 3.706512451171875 | BCE Loss: 1.0100197792053223\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.764978408813477 | KNN Loss: 3.7439818382263184 | BCE Loss: 1.0209965705871582\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.680111885070801 | KNN Loss: 3.6781578063964844 | BCE Loss: 1.0019543170928955\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.735055923461914 | KNN Loss: 3.690659999847412 | BCE Loss: 1.044396162033081\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.730386734008789 | KNN Loss: 3.7246110439300537 | BCE Loss: 1.0057755708694458\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.702389240264893 | KNN Loss: 3.677483320236206 | BCE Loss: 1.024905800819397\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.703159332275391 | KNN Loss: 3.6760149002075195 | BCE Loss: 1.027144193649292\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.717899322509766 | KNN Loss: 3.7141191959381104 | BCE Loss: 1.0037798881530762\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.675236701965332 | KNN Loss: 3.663628101348877 | BCE Loss: 1.011608600616455\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.716242790222168 | KNN Loss: 3.6903154850006104 | BCE Loss: 1.0259270668029785\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.714064598083496 | KNN Loss: 3.702023506164551 | BCE Loss: 1.0120410919189453\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.707114219665527 | KNN Loss: 3.6849374771118164 | BCE Loss: 1.0221765041351318\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.730329513549805 | KNN Loss: 3.689654588699341 | BCE Loss: 1.0406749248504639\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.734800338745117 | KNN Loss: 3.702583074569702 | BCE Loss: 1.0322171449661255\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.733170509338379 | KNN Loss: 3.7115347385406494 | BCE Loss: 1.0216360092163086\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.78111457824707 | KNN Loss: 3.7146782875061035 | BCE Loss: 1.0664362907409668\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.729007720947266 | KNN Loss: 3.705211877822876 | BCE Loss: 1.0237956047058105\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.759242057800293 | KNN Loss: 3.727024555206299 | BCE Loss: 1.0322177410125732\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.67919921875 | KNN Loss: 3.68428111076355 | BCE Loss: 0.9949183464050293\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.709925651550293 | KNN Loss: 3.685436964035034 | BCE Loss: 1.024488925933838\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.707650184631348 | KNN Loss: 3.678861379623413 | BCE Loss: 1.0287890434265137\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.696372032165527 | KNN Loss: 3.680227041244507 | BCE Loss: 1.016144871711731\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.703091144561768 | KNN Loss: 3.68229079246521 | BCE Loss: 1.0208003520965576\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.719025135040283 | KNN Loss: 3.695787191390991 | BCE Loss: 1.023237943649292\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.710724830627441 | KNN Loss: 3.6836137771606445 | BCE Loss: 1.027111291885376\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.736339569091797 | KNN Loss: 3.6970040798187256 | BCE Loss: 1.0393354892730713\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.733633995056152 | KNN Loss: 3.68273663520813 | BCE Loss: 1.0508971214294434\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.719533920288086 | KNN Loss: 3.710361957550049 | BCE Loss: 1.0091718435287476\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.7400336265563965 | KNN Loss: 3.704902410507202 | BCE Loss: 1.0351312160491943\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.7301225662231445 | KNN Loss: 3.688730239868164 | BCE Loss: 1.0413925647735596\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.741153717041016 | KNN Loss: 3.7241358757019043 | BCE Loss: 1.0170178413391113\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.742724418640137 | KNN Loss: 3.7171807289123535 | BCE Loss: 1.0255439281463623\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.782227039337158 | KNN Loss: 3.7057578563690186 | BCE Loss: 1.0764691829681396\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.683326721191406 | KNN Loss: 3.6603012084960938 | BCE Loss: 1.0230252742767334\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.728922367095947 | KNN Loss: 3.712726354598999 | BCE Loss: 1.0161960124969482\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.73835563659668 | KNN Loss: 3.7032880783081055 | BCE Loss: 1.0350675582885742\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.655216217041016 | KNN Loss: 3.657639980316162 | BCE Loss: 0.9975759983062744\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.726766586303711 | KNN Loss: 3.6977622509002686 | BCE Loss: 1.0290045738220215\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.720789909362793 | KNN Loss: 3.686005115509033 | BCE Loss: 1.0347846746444702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.731794834136963 | KNN Loss: 3.6806206703186035 | BCE Loss: 1.051174283027649\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.737914085388184 | KNN Loss: 3.7077267169952393 | BCE Loss: 1.0301876068115234\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.719261646270752 | KNN Loss: 3.711268186569214 | BCE Loss: 1.0079935789108276\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.783514976501465 | KNN Loss: 3.721160411834717 | BCE Loss: 1.062354564666748\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.723406791687012 | KNN Loss: 3.685626745223999 | BCE Loss: 1.0377800464630127\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.725470066070557 | KNN Loss: 3.697281837463379 | BCE Loss: 1.0281881093978882\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.675640106201172 | KNN Loss: 3.6669130325317383 | BCE Loss: 1.0087270736694336\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.746035575866699 | KNN Loss: 3.704097032546997 | BCE Loss: 1.0419387817382812\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.70544958114624 | KNN Loss: 3.6898136138916016 | BCE Loss: 1.0156359672546387\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.715269088745117 | KNN Loss: 3.6801652908325195 | BCE Loss: 1.0351039171218872\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.718898296356201 | KNN Loss: 3.701505184173584 | BCE Loss: 1.0173932313919067\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.713959217071533 | KNN Loss: 3.6735939979553223 | BCE Loss: 1.0403650999069214\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.71461820602417 | KNN Loss: 3.710507869720459 | BCE Loss: 1.004110336303711\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.676491737365723 | KNN Loss: 3.6478209495544434 | BCE Loss: 1.0286710262298584\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.735629081726074 | KNN Loss: 3.6950883865356445 | BCE Loss: 1.0405406951904297\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.719517707824707 | KNN Loss: 3.7017009258270264 | BCE Loss: 1.0178165435791016\n",
      "Epoch   217: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.679325103759766 | KNN Loss: 3.680506944656372 | BCE Loss: 0.9988182783126831\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.718391418457031 | KNN Loss: 3.681844711303711 | BCE Loss: 1.0365469455718994\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.723884582519531 | KNN Loss: 3.698056697845459 | BCE Loss: 1.0258276462554932\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.703438758850098 | KNN Loss: 3.6775565147399902 | BCE Loss: 1.0258822441101074\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.71851921081543 | KNN Loss: 3.708385467529297 | BCE Loss: 1.0101336240768433\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.737398147583008 | KNN Loss: 3.7159976959228516 | BCE Loss: 1.0214002132415771\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.702681541442871 | KNN Loss: 3.674772024154663 | BCE Loss: 1.027909517288208\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.688791751861572 | KNN Loss: 3.6782479286193848 | BCE Loss: 1.010543942451477\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.727021217346191 | KNN Loss: 3.70290207862854 | BCE Loss: 1.024119257926941\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.719980239868164 | KNN Loss: 3.6930148601531982 | BCE Loss: 1.0269651412963867\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.712484359741211 | KNN Loss: 3.6966514587402344 | BCE Loss: 1.0158331394195557\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.689537048339844 | KNN Loss: 3.6450157165527344 | BCE Loss: 1.0445210933685303\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.68133020401001 | KNN Loss: 3.6701974868774414 | BCE Loss: 1.0111327171325684\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.716158866882324 | KNN Loss: 3.6840381622314453 | BCE Loss: 1.032120704650879\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.667170524597168 | KNN Loss: 3.664036273956299 | BCE Loss: 1.00313401222229\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.716076850891113 | KNN Loss: 3.703326463699341 | BCE Loss: 1.0127506256103516\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.71331787109375 | KNN Loss: 3.700857162475586 | BCE Loss: 1.0124605894088745\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.718314170837402 | KNN Loss: 3.6800262928009033 | BCE Loss: 1.038287878036499\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.692225456237793 | KNN Loss: 3.670869827270508 | BCE Loss: 1.021355390548706\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.748458385467529 | KNN Loss: 3.7264654636383057 | BCE Loss: 1.021992802619934\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.7132368087768555 | KNN Loss: 3.673971176147461 | BCE Loss: 1.0392658710479736\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.694295883178711 | KNN Loss: 3.670872926712036 | BCE Loss: 1.0234229564666748\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.741669654846191 | KNN Loss: 3.731032371520996 | BCE Loss: 1.0106372833251953\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.688892364501953 | KNN Loss: 3.662625551223755 | BCE Loss: 1.0262668132781982\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.704627990722656 | KNN Loss: 3.6726717948913574 | BCE Loss: 1.0319561958312988\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.680245399475098 | KNN Loss: 3.654121160507202 | BCE Loss: 1.026124119758606\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.756386756896973 | KNN Loss: 3.7376554012298584 | BCE Loss: 1.0187315940856934\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.7166643142700195 | KNN Loss: 3.7137439250946045 | BCE Loss: 1.0029205083847046\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.684597969055176 | KNN Loss: 3.6532466411590576 | BCE Loss: 1.031351089477539\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.669567584991455 | KNN Loss: 3.6706326007843018 | BCE Loss: 0.9989349842071533\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.742072582244873 | KNN Loss: 3.6953346729278564 | BCE Loss: 1.046737790107727\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.728143692016602 | KNN Loss: 3.7070472240448 | BCE Loss: 1.0210964679718018\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.704555988311768 | KNN Loss: 3.674826145172119 | BCE Loss: 1.0297298431396484\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.681516647338867 | KNN Loss: 3.651392936706543 | BCE Loss: 1.0301239490509033\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.709751129150391 | KNN Loss: 3.656153440475464 | BCE Loss: 1.0535978078842163\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.701814651489258 | KNN Loss: 3.6841464042663574 | BCE Loss: 1.0176684856414795\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.751534461975098 | KNN Loss: 3.7237682342529297 | BCE Loss: 1.027766227722168\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.742115497589111 | KNN Loss: 3.701383113861084 | BCE Loss: 1.0407322645187378\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.727251052856445 | KNN Loss: 3.699070692062378 | BCE Loss: 1.028180480003357\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.744417667388916 | KNN Loss: 3.7106282711029053 | BCE Loss: 1.0337892770767212\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.700152397155762 | KNN Loss: 3.6916086673736572 | BCE Loss: 1.0085437297821045\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.690872669219971 | KNN Loss: 3.682711124420166 | BCE Loss: 1.0081614255905151\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.696994781494141 | KNN Loss: 3.6848268508911133 | BCE Loss: 1.0121681690216064\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.70510196685791 | KNN Loss: 3.672414779663086 | BCE Loss: 1.0326873064041138\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.7225751876831055 | KNN Loss: 3.6777729988098145 | BCE Loss: 1.044801950454712\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.7671332359313965 | KNN Loss: 3.719632148742676 | BCE Loss: 1.0475010871887207\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.702869415283203 | KNN Loss: 3.6839845180511475 | BCE Loss: 1.0188851356506348\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.742658615112305 | KNN Loss: 3.7152161598205566 | BCE Loss: 1.0274426937103271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.695624828338623 | KNN Loss: 3.698115348815918 | BCE Loss: 0.9975094795227051\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.6477813720703125 | KNN Loss: 3.665881633758545 | BCE Loss: 0.9818999767303467\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.764874458312988 | KNN Loss: 3.7062900066375732 | BCE Loss: 1.0585846900939941\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.7406721115112305 | KNN Loss: 3.6761012077331543 | BCE Loss: 1.064570665359497\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.725223064422607 | KNN Loss: 3.6855783462524414 | BCE Loss: 1.039644718170166\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.771454811096191 | KNN Loss: 3.7212390899658203 | BCE Loss: 1.0502156019210815\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.742733001708984 | KNN Loss: 3.7014875411987305 | BCE Loss: 1.041245460510254\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.691166877746582 | KNN Loss: 3.695117712020874 | BCE Loss: 0.9960492849349976\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.7097015380859375 | KNN Loss: 3.6859211921691895 | BCE Loss: 1.023780345916748\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.738358974456787 | KNN Loss: 3.729356050491333 | BCE Loss: 1.009002923965454\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.698136329650879 | KNN Loss: 3.699190378189087 | BCE Loss: 0.998945951461792\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.734069347381592 | KNN Loss: 3.693626642227173 | BCE Loss: 1.0404428243637085\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.754902362823486 | KNN Loss: 3.7098240852355957 | BCE Loss: 1.0450782775878906\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.7000627517700195 | KNN Loss: 3.7032485008239746 | BCE Loss: 0.9968143701553345\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.730656623840332 | KNN Loss: 3.689211130142212 | BCE Loss: 1.041445255279541\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.7547831535339355 | KNN Loss: 3.729914665222168 | BCE Loss: 1.0248684883117676\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.681923866271973 | KNN Loss: 3.671137809753418 | BCE Loss: 1.0107859373092651\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.689568042755127 | KNN Loss: 3.6777069568634033 | BCE Loss: 1.0118610858917236\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.7518768310546875 | KNN Loss: 3.7318949699401855 | BCE Loss: 1.019981861114502\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.687192440032959 | KNN Loss: 3.696195125579834 | BCE Loss: 0.9909974336624146\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.705167770385742 | KNN Loss: 3.729037284851074 | BCE Loss: 0.9761303663253784\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.7253522872924805 | KNN Loss: 3.693357467651367 | BCE Loss: 1.0319945812225342\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.712194442749023 | KNN Loss: 3.7156596183776855 | BCE Loss: 0.9965348243713379\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.754928112030029 | KNN Loss: 3.697903633117676 | BCE Loss: 1.057024598121643\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.713123321533203 | KNN Loss: 3.715106725692749 | BCE Loss: 0.9980164766311646\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.736502170562744 | KNN Loss: 3.694676160812378 | BCE Loss: 1.0418260097503662\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.731358528137207 | KNN Loss: 3.6960837841033936 | BCE Loss: 1.0352745056152344\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.733529567718506 | KNN Loss: 3.72385311126709 | BCE Loss: 1.0096763372421265\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.688386917114258 | KNN Loss: 3.6687066555023193 | BCE Loss: 1.0196802616119385\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.683942794799805 | KNN Loss: 3.6721770763397217 | BCE Loss: 1.011765956878662\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.722259521484375 | KNN Loss: 3.696805953979492 | BCE Loss: 1.0254533290863037\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.713895797729492 | KNN Loss: 3.6880946159362793 | BCE Loss: 1.0258010625839233\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.73382568359375 | KNN Loss: 3.724900245666504 | BCE Loss: 1.008925199508667\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.74266242980957 | KNN Loss: 3.723923683166504 | BCE Loss: 1.0187389850616455\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.769084930419922 | KNN Loss: 3.7371113300323486 | BCE Loss: 1.0319738388061523\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.725138187408447 | KNN Loss: 3.6969642639160156 | BCE Loss: 1.028173804283142\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.746501922607422 | KNN Loss: 3.6907029151916504 | BCE Loss: 1.0557987689971924\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.717668533325195 | KNN Loss: 3.6887624263763428 | BCE Loss: 1.0289063453674316\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.734862327575684 | KNN Loss: 3.711996078491211 | BCE Loss: 1.0228660106658936\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.707067012786865 | KNN Loss: 3.678312063217163 | BCE Loss: 1.0287550687789917\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.721380710601807 | KNN Loss: 3.688310384750366 | BCE Loss: 1.0330703258514404\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.714019775390625 | KNN Loss: 3.6830577850341797 | BCE Loss: 1.0309618711471558\n",
      "Epoch   232: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.736002445220947 | KNN Loss: 3.7103562355041504 | BCE Loss: 1.0256463289260864\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.767617225646973 | KNN Loss: 3.7178103923797607 | BCE Loss: 1.0498065948486328\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.708337783813477 | KNN Loss: 3.6903982162475586 | BCE Loss: 1.0179393291473389\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.7288713455200195 | KNN Loss: 3.6849937438964844 | BCE Loss: 1.0438777208328247\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.71620512008667 | KNN Loss: 3.6907730102539062 | BCE Loss: 1.0254319906234741\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.735478401184082 | KNN Loss: 3.694845676422119 | BCE Loss: 1.0406324863433838\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.708331108093262 | KNN Loss: 3.6954405307769775 | BCE Loss: 1.0128908157348633\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.723250389099121 | KNN Loss: 3.6958460807800293 | BCE Loss: 1.027404546737671\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.733931064605713 | KNN Loss: 3.6750292778015137 | BCE Loss: 1.0589019060134888\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.727548122406006 | KNN Loss: 3.691890001296997 | BCE Loss: 1.0356580018997192\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.715813159942627 | KNN Loss: 3.7093095779418945 | BCE Loss: 1.0065034627914429\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.707086563110352 | KNN Loss: 3.680116891860962 | BCE Loss: 1.0269696712493896\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.694300174713135 | KNN Loss: 3.6652719974517822 | BCE Loss: 1.029028058052063\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.7073259353637695 | KNN Loss: 3.696171760559082 | BCE Loss: 1.0111539363861084\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.736564636230469 | KNN Loss: 3.680849075317383 | BCE Loss: 1.0557153224945068\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.714942455291748 | KNN Loss: 3.681290626525879 | BCE Loss: 1.0336519479751587\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.758685111999512 | KNN Loss: 3.7164132595062256 | BCE Loss: 1.0422718524932861\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.686112403869629 | KNN Loss: 3.6740241050720215 | BCE Loss: 1.0120885372161865\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.674245834350586 | KNN Loss: 3.676746368408203 | BCE Loss: 0.9974995851516724\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.73292350769043 | KNN Loss: 3.6937828063964844 | BCE Loss: 1.0391407012939453\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.706258773803711 | KNN Loss: 3.671653985977173 | BCE Loss: 1.034604549407959\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.723742485046387 | KNN Loss: 3.6850664615631104 | BCE Loss: 1.0386757850646973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.7099289894104 | KNN Loss: 3.693317413330078 | BCE Loss: 1.0166115760803223\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.704799175262451 | KNN Loss: 3.6972432136535645 | BCE Loss: 1.0075558423995972\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.71965217590332 | KNN Loss: 3.6739585399627686 | BCE Loss: 1.0456938743591309\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.752005577087402 | KNN Loss: 3.718529224395752 | BCE Loss: 1.0334763526916504\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.718829154968262 | KNN Loss: 3.667057752609253 | BCE Loss: 1.0517714023590088\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.7447710037231445 | KNN Loss: 3.6928117275238037 | BCE Loss: 1.0519593954086304\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.730156898498535 | KNN Loss: 3.726097583770752 | BCE Loss: 1.0040595531463623\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.701536178588867 | KNN Loss: 3.697327136993408 | BCE Loss: 1.0042089223861694\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.710268974304199 | KNN Loss: 3.684523582458496 | BCE Loss: 1.0257452726364136\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.744058609008789 | KNN Loss: 3.708346366882324 | BCE Loss: 1.0357122421264648\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.722548484802246 | KNN Loss: 3.7106714248657227 | BCE Loss: 1.0118772983551025\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.707325458526611 | KNN Loss: 3.6736671924591064 | BCE Loss: 1.0336581468582153\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.701290607452393 | KNN Loss: 3.6697494983673096 | BCE Loss: 1.031541109085083\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.728493690490723 | KNN Loss: 3.7248666286468506 | BCE Loss: 1.003626823425293\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.731686592102051 | KNN Loss: 3.7292428016662598 | BCE Loss: 1.002443790435791\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.691346168518066 | KNN Loss: 3.6841790676116943 | BCE Loss: 1.007166862487793\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.755555152893066 | KNN Loss: 3.722550392150879 | BCE Loss: 1.033004879951477\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.7031731605529785 | KNN Loss: 3.689371109008789 | BCE Loss: 1.0138020515441895\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.698215484619141 | KNN Loss: 3.693113327026367 | BCE Loss: 1.0051020383834839\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.718777656555176 | KNN Loss: 3.7066574096679688 | BCE Loss: 1.0121204853057861\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.675784587860107 | KNN Loss: 3.6692707538604736 | BCE Loss: 1.0065138339996338\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.71297025680542 | KNN Loss: 3.7018003463745117 | BCE Loss: 1.0111699104309082\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.785600662231445 | KNN Loss: 3.743617057800293 | BCE Loss: 1.0419838428497314\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.723381042480469 | KNN Loss: 3.68367338180542 | BCE Loss: 1.0397077798843384\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.707281589508057 | KNN Loss: 3.6792402267456055 | BCE Loss: 1.0280413627624512\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.714594841003418 | KNN Loss: 3.698324680328369 | BCE Loss: 1.0162701606750488\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.672786235809326 | KNN Loss: 3.663618564605713 | BCE Loss: 1.0091676712036133\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.700652122497559 | KNN Loss: 3.6644062995910645 | BCE Loss: 1.0362458229064941\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.681742191314697 | KNN Loss: 3.67472505569458 | BCE Loss: 1.0070171356201172\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.683067798614502 | KNN Loss: 3.6665027141571045 | BCE Loss: 1.0165650844573975\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.757679462432861 | KNN Loss: 3.7017996311187744 | BCE Loss: 1.0558797121047974\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.735088348388672 | KNN Loss: 3.679403781890869 | BCE Loss: 1.0556848049163818\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.711369037628174 | KNN Loss: 3.6917684078216553 | BCE Loss: 1.0196006298065186\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.738121509552002 | KNN Loss: 3.707421064376831 | BCE Loss: 1.0307005643844604\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.699278354644775 | KNN Loss: 3.6618540287017822 | BCE Loss: 1.0374243259429932\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.727745532989502 | KNN Loss: 3.716503143310547 | BCE Loss: 1.0112422704696655\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.719052314758301 | KNN Loss: 3.682626724243164 | BCE Loss: 1.0364253520965576\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.739398956298828 | KNN Loss: 3.692456007003784 | BCE Loss: 1.046942949295044\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.730917930603027 | KNN Loss: 3.7194902896881104 | BCE Loss: 1.011427640914917\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.716546058654785 | KNN Loss: 3.685380697250366 | BCE Loss: 1.0311651229858398\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.690077781677246 | KNN Loss: 3.6841442584991455 | BCE Loss: 1.0059337615966797\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.668442726135254 | KNN Loss: 3.6672441959381104 | BCE Loss: 1.0011982917785645\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.756382942199707 | KNN Loss: 3.713942527770996 | BCE Loss: 1.0424401760101318\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.738636493682861 | KNN Loss: 3.6956684589385986 | BCE Loss: 1.0429681539535522\n",
      "Epoch   243: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.7372727394104 | KNN Loss: 3.692336320877075 | BCE Loss: 1.0449362993240356\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.719196319580078 | KNN Loss: 3.6810171604156494 | BCE Loss: 1.0381792783737183\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.733538627624512 | KNN Loss: 3.697492837905884 | BCE Loss: 1.0360456705093384\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.722868919372559 | KNN Loss: 3.671699285507202 | BCE Loss: 1.0511696338653564\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.686113357543945 | KNN Loss: 3.6834301948547363 | BCE Loss: 1.002683401107788\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.6981401443481445 | KNN Loss: 3.6827878952026367 | BCE Loss: 1.015352487564087\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.805019378662109 | KNN Loss: 3.7500932216644287 | BCE Loss: 1.0549259185791016\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.720352649688721 | KNN Loss: 3.701545476913452 | BCE Loss: 1.018807053565979\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.706492900848389 | KNN Loss: 3.6847925186157227 | BCE Loss: 1.021700382232666\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.715537071228027 | KNN Loss: 3.730175733566284 | BCE Loss: 0.9853613376617432\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.716645240783691 | KNN Loss: 3.6767425537109375 | BCE Loss: 1.0399028062820435\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.683787822723389 | KNN Loss: 3.6600563526153564 | BCE Loss: 1.0237314701080322\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.7368597984313965 | KNN Loss: 3.6710612773895264 | BCE Loss: 1.0657986402511597\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.74348258972168 | KNN Loss: 3.667264938354492 | BCE Loss: 1.0762176513671875\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.698479175567627 | KNN Loss: 3.687018871307373 | BCE Loss: 1.0114604234695435\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.713630199432373 | KNN Loss: 3.698817729949951 | BCE Loss: 1.0148125886917114\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.697949409484863 | KNN Loss: 3.6886885166168213 | BCE Loss: 1.009260892868042\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.702325820922852 | KNN Loss: 3.693233013153076 | BCE Loss: 1.0090930461883545\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.7112202644348145 | KNN Loss: 3.6828830242156982 | BCE Loss: 1.0283373594284058\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.727176666259766 | KNN Loss: 3.716050148010254 | BCE Loss: 1.0111267566680908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.663172721862793 | KNN Loss: 3.65230393409729 | BCE Loss: 1.010869026184082\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.708013534545898 | KNN Loss: 3.7041800022125244 | BCE Loss: 1.003833293914795\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.7195916175842285 | KNN Loss: 3.702199935913086 | BCE Loss: 1.0173918008804321\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.7378740310668945 | KNN Loss: 3.680939197540283 | BCE Loss: 1.0569348335266113\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.725154399871826 | KNN Loss: 3.6858909130096436 | BCE Loss: 1.0392636060714722\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.730515003204346 | KNN Loss: 3.7082533836364746 | BCE Loss: 1.0222615003585815\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.69728946685791 | KNN Loss: 3.6866676807403564 | BCE Loss: 1.0106215476989746\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.72092342376709 | KNN Loss: 3.7108099460601807 | BCE Loss: 1.0101134777069092\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.747274398803711 | KNN Loss: 3.71832013130188 | BCE Loss: 1.028954029083252\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.712007999420166 | KNN Loss: 3.695077896118164 | BCE Loss: 1.016930103302002\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.721461296081543 | KNN Loss: 3.7073185443878174 | BCE Loss: 1.0141427516937256\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.7193450927734375 | KNN Loss: 3.7042486667633057 | BCE Loss: 1.0150961875915527\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.734131813049316 | KNN Loss: 3.695719003677368 | BCE Loss: 1.0384130477905273\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.726594924926758 | KNN Loss: 3.7031149864196777 | BCE Loss: 1.0234798192977905\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.6790056228637695 | KNN Loss: 3.675415515899658 | BCE Loss: 1.0035903453826904\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.7160515785217285 | KNN Loss: 3.701258659362793 | BCE Loss: 1.0147929191589355\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.704875946044922 | KNN Loss: 3.6945815086364746 | BCE Loss: 1.0102941989898682\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.708492755889893 | KNN Loss: 3.7077391147613525 | BCE Loss: 1.00075364112854\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.689986228942871 | KNN Loss: 3.66115665435791 | BCE Loss: 1.0288296937942505\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.761087417602539 | KNN Loss: 3.723464250564575 | BCE Loss: 1.0376231670379639\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.690680027008057 | KNN Loss: 3.68222713470459 | BCE Loss: 1.0084528923034668\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.721447944641113 | KNN Loss: 3.6860320568084717 | BCE Loss: 1.0354158878326416\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.693732738494873 | KNN Loss: 3.668945074081421 | BCE Loss: 1.0247876644134521\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.687436103820801 | KNN Loss: 3.673067808151245 | BCE Loss: 1.0143682956695557\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.740594387054443 | KNN Loss: 3.7008936405181885 | BCE Loss: 1.0397007465362549\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.731866836547852 | KNN Loss: 3.6816155910491943 | BCE Loss: 1.0502512454986572\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.681457042694092 | KNN Loss: 3.670574903488159 | BCE Loss: 1.0108822584152222\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.686404705047607 | KNN Loss: 3.676619291305542 | BCE Loss: 1.0097852945327759\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.707371711730957 | KNN Loss: 3.7042086124420166 | BCE Loss: 1.0031630992889404\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.7249956130981445 | KNN Loss: 3.692295789718628 | BCE Loss: 1.0327000617980957\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.7463788986206055 | KNN Loss: 3.711812973022461 | BCE Loss: 1.034566044807434\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.706429481506348 | KNN Loss: 3.6983227729797363 | BCE Loss: 1.0081067085266113\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.717767238616943 | KNN Loss: 3.691626787185669 | BCE Loss: 1.0261403322219849\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.747612953186035 | KNN Loss: 3.726497173309326 | BCE Loss: 1.021116018295288\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.680551528930664 | KNN Loss: 3.6592867374420166 | BCE Loss: 1.021264672279358\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.74550724029541 | KNN Loss: 3.7064766883850098 | BCE Loss: 1.0390305519104004\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.7100019454956055 | KNN Loss: 3.6851251125335693 | BCE Loss: 1.0248769521713257\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.737688064575195 | KNN Loss: 3.711392641067505 | BCE Loss: 1.0262951850891113\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.729349136352539 | KNN Loss: 3.7033183574676514 | BCE Loss: 1.0260307788848877\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.729086875915527 | KNN Loss: 3.7072370052337646 | BCE Loss: 1.0218501091003418\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.701457977294922 | KNN Loss: 3.6772305965423584 | BCE Loss: 1.0242273807525635\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.731481075286865 | KNN Loss: 3.7163186073303223 | BCE Loss: 1.0151625871658325\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.723773002624512 | KNN Loss: 3.70113205909729 | BCE Loss: 1.0226411819458008\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.697259902954102 | KNN Loss: 3.658137083053589 | BCE Loss: 1.0391225814819336\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.694624423980713 | KNN Loss: 3.6803500652313232 | BCE Loss: 1.0142742395401\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.718012809753418 | KNN Loss: 3.6967825889587402 | BCE Loss: 1.0212302207946777\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.726850509643555 | KNN Loss: 3.714057683944702 | BCE Loss: 1.0127930641174316\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.723047256469727 | KNN Loss: 3.68408465385437 | BCE Loss: 1.0389623641967773\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.708500385284424 | KNN Loss: 3.6789913177490234 | BCE Loss: 1.02950918674469\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.700616836547852 | KNN Loss: 3.6999003887176514 | BCE Loss: 1.0007164478302002\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.70685338973999 | KNN Loss: 3.693159818649292 | BCE Loss: 1.0136935710906982\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.736268997192383 | KNN Loss: 3.7038803100585938 | BCE Loss: 1.03238844871521\n",
      "Epoch   255: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.71541690826416 | KNN Loss: 3.70880126953125 | BCE Loss: 1.0066156387329102\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.709474086761475 | KNN Loss: 3.6824581623077393 | BCE Loss: 1.0270158052444458\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.676664352416992 | KNN Loss: 3.670217275619507 | BCE Loss: 1.0064468383789062\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.707304954528809 | KNN Loss: 3.680752992630005 | BCE Loss: 1.0265519618988037\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.752588748931885 | KNN Loss: 3.6846094131469727 | BCE Loss: 1.0679792165756226\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.726948261260986 | KNN Loss: 3.7029225826263428 | BCE Loss: 1.0240256786346436\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.73358678817749 | KNN Loss: 3.6997246742248535 | BCE Loss: 1.0338622331619263\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.7330546379089355 | KNN Loss: 3.705434799194336 | BCE Loss: 1.0276199579238892\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.70120906829834 | KNN Loss: 3.6729397773742676 | BCE Loss: 1.0282691717147827\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.707878589630127 | KNN Loss: 3.7091050148010254 | BCE Loss: 0.998773455619812\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.770727634429932 | KNN Loss: 3.722242593765259 | BCE Loss: 1.0484850406646729\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.736461639404297 | KNN Loss: 3.720517873764038 | BCE Loss: 1.0159436464309692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.7534074783325195 | KNN Loss: 3.703187942504883 | BCE Loss: 1.0502197742462158\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.696640968322754 | KNN Loss: 3.6728460788726807 | BCE Loss: 1.0237948894500732\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.668384552001953 | KNN Loss: 3.6642165184020996 | BCE Loss: 1.004167914390564\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.747287750244141 | KNN Loss: 3.699702501296997 | BCE Loss: 1.0475850105285645\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.724896430969238 | KNN Loss: 3.69954252243042 | BCE Loss: 1.0253537893295288\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.736482620239258 | KNN Loss: 3.7071335315704346 | BCE Loss: 1.0293490886688232\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.725542068481445 | KNN Loss: 3.7000577449798584 | BCE Loss: 1.0254840850830078\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.71171760559082 | KNN Loss: 3.692807912826538 | BCE Loss: 1.0189099311828613\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.766046047210693 | KNN Loss: 3.7185003757476807 | BCE Loss: 1.0475456714630127\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.7160844802856445 | KNN Loss: 3.6782991886138916 | BCE Loss: 1.037785291671753\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.755405902862549 | KNN Loss: 3.732863426208496 | BCE Loss: 1.0225424766540527\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.751574516296387 | KNN Loss: 3.72786283493042 | BCE Loss: 1.0237114429473877\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.724083423614502 | KNN Loss: 3.663741111755371 | BCE Loss: 1.0603424310684204\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.71506404876709 | KNN Loss: 3.6839680671691895 | BCE Loss: 1.0310958623886108\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.733086585998535 | KNN Loss: 3.7049992084503174 | BCE Loss: 1.0280874967575073\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.707548141479492 | KNN Loss: 3.656489133834839 | BCE Loss: 1.0510592460632324\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.702430248260498 | KNN Loss: 3.706879138946533 | BCE Loss: 0.9955512881278992\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.778598308563232 | KNN Loss: 3.72312068939209 | BCE Loss: 1.0554776191711426\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.771659851074219 | KNN Loss: 3.7172164916992188 | BCE Loss: 1.054443359375\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.699161529541016 | KNN Loss: 3.6942880153656006 | BCE Loss: 1.0048733949661255\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.742827415466309 | KNN Loss: 3.728595018386841 | BCE Loss: 1.0142323970794678\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.7848100662231445 | KNN Loss: 3.758697509765625 | BCE Loss: 1.0261123180389404\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.752857685089111 | KNN Loss: 3.7262678146362305 | BCE Loss: 1.0265898704528809\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.705524444580078 | KNN Loss: 3.6919846534729004 | BCE Loss: 1.0135400295257568\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.730772018432617 | KNN Loss: 3.6988656520843506 | BCE Loss: 1.0319061279296875\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.727402687072754 | KNN Loss: 3.669569969177246 | BCE Loss: 1.0578327178955078\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.781634330749512 | KNN Loss: 3.731476068496704 | BCE Loss: 1.0501585006713867\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.746179103851318 | KNN Loss: 3.7016472816467285 | BCE Loss: 1.0445318222045898\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.774496078491211 | KNN Loss: 3.721135377883911 | BCE Loss: 1.0533604621887207\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.765662670135498 | KNN Loss: 3.7362425327301025 | BCE Loss: 1.0294201374053955\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.723247051239014 | KNN Loss: 3.7061288356781006 | BCE Loss: 1.017118215560913\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.660618782043457 | KNN Loss: 3.6679935455322266 | BCE Loss: 0.9926252961158752\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.699641227722168 | KNN Loss: 3.6861886978149414 | BCE Loss: 1.0134522914886475\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.731849670410156 | KNN Loss: 3.7197272777557373 | BCE Loss: 1.0121222734451294\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.716869354248047 | KNN Loss: 3.7027640342712402 | BCE Loss: 1.014105200767517\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.693843841552734 | KNN Loss: 3.6596899032592773 | BCE Loss: 1.0341541767120361\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.704262733459473 | KNN Loss: 3.700476884841919 | BCE Loss: 1.0037860870361328\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.758791446685791 | KNN Loss: 3.7250187397003174 | BCE Loss: 1.0337727069854736\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.675477981567383 | KNN Loss: 3.681544542312622 | BCE Loss: 0.9939335584640503\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.708456993103027 | KNN Loss: 3.681572437286377 | BCE Loss: 1.0268845558166504\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.758914470672607 | KNN Loss: 3.725855827331543 | BCE Loss: 1.0330586433410645\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.7524919509887695 | KNN Loss: 3.7071142196655273 | BCE Loss: 1.0453779697418213\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.769955635070801 | KNN Loss: 3.7045271396636963 | BCE Loss: 1.0654282569885254\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.715144157409668 | KNN Loss: 3.657487154006958 | BCE Loss: 1.0576567649841309\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.722786903381348 | KNN Loss: 3.6859326362609863 | BCE Loss: 1.0368540287017822\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.71424674987793 | KNN Loss: 3.6859469413757324 | BCE Loss: 1.0283000469207764\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.709003448486328 | KNN Loss: 3.6848325729370117 | BCE Loss: 1.0241707563400269\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.696778774261475 | KNN Loss: 3.6762471199035645 | BCE Loss: 1.0205315351486206\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.67257022857666 | KNN Loss: 3.695082187652588 | BCE Loss: 0.9774882793426514\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.663654327392578 | KNN Loss: 3.648322343826294 | BCE Loss: 1.0153319835662842\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.715838432312012 | KNN Loss: 3.686976432800293 | BCE Loss: 1.0288619995117188\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.689581871032715 | KNN Loss: 3.6901071071624756 | BCE Loss: 0.9994747638702393\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.728337287902832 | KNN Loss: 3.708592176437378 | BCE Loss: 1.019745111465454\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.70676326751709 | KNN Loss: 3.680530309677124 | BCE Loss: 1.026233196258545\n",
      "Epoch   266: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.702211380004883 | KNN Loss: 3.683046817779541 | BCE Loss: 1.019164800643921\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.736058235168457 | KNN Loss: 3.689584493637085 | BCE Loss: 1.046473741531372\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.733378887176514 | KNN Loss: 3.6855075359344482 | BCE Loss: 1.0478712320327759\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.733633041381836 | KNN Loss: 3.6988136768341064 | BCE Loss: 1.0348196029663086\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.689688682556152 | KNN Loss: 3.672747850418091 | BCE Loss: 1.016940951347351\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.752298831939697 | KNN Loss: 3.721785068511963 | BCE Loss: 1.0305137634277344\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.751121997833252 | KNN Loss: 3.7465555667877197 | BCE Loss: 1.0045665502548218\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.708907604217529 | KNN Loss: 3.680527925491333 | BCE Loss: 1.0283795595169067\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.703660011291504 | KNN Loss: 3.690264940261841 | BCE Loss: 1.0133949518203735\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.676776885986328 | KNN Loss: 3.648806095123291 | BCE Loss: 1.027970552444458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.760209083557129 | KNN Loss: 3.708247423171997 | BCE Loss: 1.0519616603851318\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.737545967102051 | KNN Loss: 3.7071492671966553 | BCE Loss: 1.0303969383239746\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.677646160125732 | KNN Loss: 3.6564791202545166 | BCE Loss: 1.0211671590805054\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.684292793273926 | KNN Loss: 3.689962863922119 | BCE Loss: 0.9943298101425171\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.703194618225098 | KNN Loss: 3.670804500579834 | BCE Loss: 1.0323903560638428\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.666982173919678 | KNN Loss: 3.6500494480133057 | BCE Loss: 1.0169328451156616\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.69497013092041 | KNN Loss: 3.67573618888855 | BCE Loss: 1.0192337036132812\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.715019702911377 | KNN Loss: 3.6817996501922607 | BCE Loss: 1.0332200527191162\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.6934332847595215 | KNN Loss: 3.6677143573760986 | BCE Loss: 1.0257189273834229\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.649805545806885 | KNN Loss: 3.6502864360809326 | BCE Loss: 0.9995191097259521\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.708545207977295 | KNN Loss: 3.7085349559783936 | BCE Loss: 1.0000102519989014\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.744332313537598 | KNN Loss: 3.700622081756592 | BCE Loss: 1.0437103509902954\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.762099266052246 | KNN Loss: 3.7272346019744873 | BCE Loss: 1.0348647832870483\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.72678804397583 | KNN Loss: 3.7016279697418213 | BCE Loss: 1.0251600742340088\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.669870376586914 | KNN Loss: 3.66914701461792 | BCE Loss: 1.0007236003875732\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.660192966461182 | KNN Loss: 3.6568613052368164 | BCE Loss: 1.0033316612243652\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.712548732757568 | KNN Loss: 3.6951904296875 | BCE Loss: 1.0173583030700684\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.733878135681152 | KNN Loss: 3.7265312671661377 | BCE Loss: 1.0073468685150146\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.698488235473633 | KNN Loss: 3.6708860397338867 | BCE Loss: 1.0276024341583252\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.7086992263793945 | KNN Loss: 3.7018797397613525 | BCE Loss: 1.006819486618042\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.680387496948242 | KNN Loss: 3.675090789794922 | BCE Loss: 1.0052964687347412\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.731628894805908 | KNN Loss: 3.682387590408325 | BCE Loss: 1.0492411851882935\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.681826114654541 | KNN Loss: 3.6586225032806396 | BCE Loss: 1.0232036113739014\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.69769287109375 | KNN Loss: 3.684584379196167 | BCE Loss: 1.013108730316162\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.700246810913086 | KNN Loss: 3.6926558017730713 | BCE Loss: 1.0075910091400146\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.707889556884766 | KNN Loss: 3.7080421447753906 | BCE Loss: 0.999847412109375\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.706305027008057 | KNN Loss: 3.6924684047698975 | BCE Loss: 1.0138367414474487\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.714879512786865 | KNN Loss: 3.677375078201294 | BCE Loss: 1.0375044345855713\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.700106620788574 | KNN Loss: 3.702317714691162 | BCE Loss: 0.9977890849113464\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.719240665435791 | KNN Loss: 3.7022180557250977 | BCE Loss: 1.0170226097106934\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.7017011642456055 | KNN Loss: 3.6756255626678467 | BCE Loss: 1.026075839996338\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.74976110458374 | KNN Loss: 3.7028286457061768 | BCE Loss: 1.0469324588775635\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.691140651702881 | KNN Loss: 3.6999056339263916 | BCE Loss: 0.9912349581718445\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.756497383117676 | KNN Loss: 3.719332456588745 | BCE Loss: 1.0371649265289307\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.716381072998047 | KNN Loss: 3.6928892135620117 | BCE Loss: 1.0234919786453247\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.707499980926514 | KNN Loss: 3.695775270462036 | BCE Loss: 1.011724829673767\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.769094467163086 | KNN Loss: 3.7415552139282227 | BCE Loss: 1.0275393724441528\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.729968070983887 | KNN Loss: 3.716285228729248 | BCE Loss: 1.0136826038360596\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.703550338745117 | KNN Loss: 3.664811849594116 | BCE Loss: 1.038738489151001\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.690070629119873 | KNN Loss: 3.663815498352051 | BCE Loss: 1.0262550115585327\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.721307754516602 | KNN Loss: 3.6952645778656006 | BCE Loss: 1.0260429382324219\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.76879358291626 | KNN Loss: 3.7245054244995117 | BCE Loss: 1.0442882776260376\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.747369766235352 | KNN Loss: 3.6995344161987305 | BCE Loss: 1.0478354692459106\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.728476524353027 | KNN Loss: 3.7078492641448975 | BCE Loss: 1.0206272602081299\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.695556640625 | KNN Loss: 3.6791932582855225 | BCE Loss: 1.016363263130188\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.725057125091553 | KNN Loss: 3.6826863288879395 | BCE Loss: 1.0423706769943237\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.746696472167969 | KNN Loss: 3.695279121398926 | BCE Loss: 1.0514171123504639\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.724705696105957 | KNN Loss: 3.7055842876434326 | BCE Loss: 1.0191211700439453\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.7142486572265625 | KNN Loss: 3.679818630218506 | BCE Loss: 1.0344300270080566\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.698904991149902 | KNN Loss: 3.701458215713501 | BCE Loss: 0.9974467158317566\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.724732398986816 | KNN Loss: 3.704080820083618 | BCE Loss: 1.0206518173217773\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.710031509399414 | KNN Loss: 3.693833827972412 | BCE Loss: 1.0161978006362915\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.663537979125977 | KNN Loss: 3.6486129760742188 | BCE Loss: 1.0149250030517578\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.696403503417969 | KNN Loss: 3.67982816696167 | BCE Loss: 1.0165754556655884\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.7046308517456055 | KNN Loss: 3.6907899379730225 | BCE Loss: 1.013841152191162\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.6864471435546875 | KNN Loss: 3.679903268814087 | BCE Loss: 1.0065441131591797\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.708683013916016 | KNN Loss: 3.690734624862671 | BCE Loss: 1.0179483890533447\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.71685266494751 | KNN Loss: 3.6856112480163574 | BCE Loss: 1.0312414169311523\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.721550941467285 | KNN Loss: 3.6563656330108643 | BCE Loss: 1.065185308456421\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.694238662719727 | KNN Loss: 3.673656940460205 | BCE Loss: 1.0205814838409424\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.670712471008301 | KNN Loss: 3.6759111881256104 | BCE Loss: 0.9948014616966248\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.725696086883545 | KNN Loss: 3.697483777999878 | BCE Loss: 1.028212308883667\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.756432056427002 | KNN Loss: 3.7039923667907715 | BCE Loss: 1.052439570426941\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.751546859741211 | KNN Loss: 3.718082904815674 | BCE Loss: 1.033463954925537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.751784324645996 | KNN Loss: 3.703399658203125 | BCE Loss: 1.048384666442871\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.735965728759766 | KNN Loss: 3.7318644523620605 | BCE Loss: 1.0041015148162842\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.726239204406738 | KNN Loss: 3.6770248413085938 | BCE Loss: 1.0492146015167236\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.666477680206299 | KNN Loss: 3.6742289066314697 | BCE Loss: 0.9922487735748291\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.684869289398193 | KNN Loss: 3.681619882583618 | BCE Loss: 1.0032495260238647\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.773609638214111 | KNN Loss: 3.7388665676116943 | BCE Loss: 1.0347431898117065\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.712279319763184 | KNN Loss: 3.675114631652832 | BCE Loss: 1.037164568901062\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.724983215332031 | KNN Loss: 3.689967393875122 | BCE Loss: 1.0350159406661987\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.758070945739746 | KNN Loss: 3.7261576652526855 | BCE Loss: 1.0319132804870605\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.7535319328308105 | KNN Loss: 3.706672191619873 | BCE Loss: 1.0468597412109375\n",
      "Epoch   280: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.68035888671875 | KNN Loss: 3.6706626415252686 | BCE Loss: 1.0096962451934814\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.6884307861328125 | KNN Loss: 3.643073081970215 | BCE Loss: 1.0453579425811768\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.678930759429932 | KNN Loss: 3.674895763397217 | BCE Loss: 1.0040349960327148\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.722048759460449 | KNN Loss: 3.6916065216064453 | BCE Loss: 1.030442237854004\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.786352157592773 | KNN Loss: 3.756441831588745 | BCE Loss: 1.0299105644226074\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.74266242980957 | KNN Loss: 3.697612762451172 | BCE Loss: 1.045049786567688\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.686406135559082 | KNN Loss: 3.6985223293304443 | BCE Loss: 0.9878837466239929\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.67860746383667 | KNN Loss: 3.6622159481048584 | BCE Loss: 1.0163915157318115\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.648655414581299 | KNN Loss: 3.6426749229431152 | BCE Loss: 1.0059806108474731\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.739790916442871 | KNN Loss: 3.7071797847747803 | BCE Loss: 1.0326108932495117\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.751354217529297 | KNN Loss: 3.71268367767334 | BCE Loss: 1.038670539855957\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.725344181060791 | KNN Loss: 3.697892189025879 | BCE Loss: 1.0274518728256226\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.709026336669922 | KNN Loss: 3.6980555057525635 | BCE Loss: 1.0109710693359375\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.701242446899414 | KNN Loss: 3.6756765842437744 | BCE Loss: 1.0255661010742188\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.687450885772705 | KNN Loss: 3.6752896308898926 | BCE Loss: 1.012161374092102\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.681839466094971 | KNN Loss: 3.660618782043457 | BCE Loss: 1.0212206840515137\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.750662803649902 | KNN Loss: 3.7457339763641357 | BCE Loss: 1.0049289464950562\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.71017599105835 | KNN Loss: 3.719106674194336 | BCE Loss: 0.991069495677948\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.753563404083252 | KNN Loss: 3.709240436553955 | BCE Loss: 1.0443229675292969\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.735294818878174 | KNN Loss: 3.688812732696533 | BCE Loss: 1.0464820861816406\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.677941799163818 | KNN Loss: 3.680004119873047 | BCE Loss: 0.9979378581047058\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.702234745025635 | KNN Loss: 3.6709930896759033 | BCE Loss: 1.031241536140442\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.6918110847473145 | KNN Loss: 3.6784000396728516 | BCE Loss: 1.0134111642837524\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.726348876953125 | KNN Loss: 3.69392466545105 | BCE Loss: 1.0324244499206543\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.739741325378418 | KNN Loss: 3.7183210849761963 | BCE Loss: 1.0214200019836426\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.72703742980957 | KNN Loss: 3.6974313259124756 | BCE Loss: 1.0296063423156738\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.686107635498047 | KNN Loss: 3.6662933826446533 | BCE Loss: 1.0198140144348145\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.679202079772949 | KNN Loss: 3.655975818634033 | BCE Loss: 1.023226261138916\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.677548408508301 | KNN Loss: 3.6662845611572266 | BCE Loss: 1.0112640857696533\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.701443195343018 | KNN Loss: 3.6788430213928223 | BCE Loss: 1.0226001739501953\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.761129379272461 | KNN Loss: 3.7089686393737793 | BCE Loss: 1.0521607398986816\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.697153091430664 | KNN Loss: 3.7006192207336426 | BCE Loss: 0.9965337514877319\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.69681453704834 | KNN Loss: 3.683896780014038 | BCE Loss: 1.0129177570343018\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.746852874755859 | KNN Loss: 3.701653480529785 | BCE Loss: 1.0451991558074951\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.738221168518066 | KNN Loss: 3.7334957122802734 | BCE Loss: 1.004725456237793\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.719736576080322 | KNN Loss: 3.6822285652160645 | BCE Loss: 1.0375080108642578\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.701453685760498 | KNN Loss: 3.6844284534454346 | BCE Loss: 1.017025113105774\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.725535869598389 | KNN Loss: 3.709413528442383 | BCE Loss: 1.0161223411560059\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.702113151550293 | KNN Loss: 3.6644554138183594 | BCE Loss: 1.037657618522644\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.688445568084717 | KNN Loss: 3.6690266132354736 | BCE Loss: 1.0194189548492432\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.722847938537598 | KNN Loss: 3.6853320598602295 | BCE Loss: 1.0375161170959473\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.703932285308838 | KNN Loss: 3.692906379699707 | BCE Loss: 1.0110259056091309\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.675340175628662 | KNN Loss: 3.665919303894043 | BCE Loss: 1.0094208717346191\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.689520359039307 | KNN Loss: 3.6749355792999268 | BCE Loss: 1.0145846605300903\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.711607456207275 | KNN Loss: 3.66677188873291 | BCE Loss: 1.0448355674743652\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.689226150512695 | KNN Loss: 3.6691009998321533 | BCE Loss: 1.020125150680542\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.697612762451172 | KNN Loss: 3.6699893474578857 | BCE Loss: 1.027623176574707\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.753354549407959 | KNN Loss: 3.7058961391448975 | BCE Loss: 1.0474584102630615\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.716976642608643 | KNN Loss: 3.682039976119995 | BCE Loss: 1.034936785697937\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.694199562072754 | KNN Loss: 3.6878042221069336 | BCE Loss: 1.0063955783843994\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.7270894050598145 | KNN Loss: 3.6941885948181152 | BCE Loss: 1.0329006910324097\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.729117393493652 | KNN Loss: 3.707697868347168 | BCE Loss: 1.0214197635650635\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.749013900756836 | KNN Loss: 3.7074921131134033 | BCE Loss: 1.041521668434143\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.691269397735596 | KNN Loss: 3.7013680934906006 | BCE Loss: 0.9899013042449951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.73099946975708 | KNN Loss: 3.683333158493042 | BCE Loss: 1.047666311264038\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.742441177368164 | KNN Loss: 3.732931137084961 | BCE Loss: 1.0095100402832031\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.689058303833008 | KNN Loss: 3.6563942432403564 | BCE Loss: 1.032664179801941\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.700741767883301 | KNN Loss: 3.6658177375793457 | BCE Loss: 1.0349242687225342\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.672262191772461 | KNN Loss: 3.650599241256714 | BCE Loss: 1.021662950515747\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.6722517013549805 | KNN Loss: 3.667447328567505 | BCE Loss: 1.0048041343688965\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.7086992263793945 | KNN Loss: 3.689448118209839 | BCE Loss: 1.0192508697509766\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.797212600708008 | KNN Loss: 3.7146687507629395 | BCE Loss: 1.0825438499450684\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.6938042640686035 | KNN Loss: 3.6913230419158936 | BCE Loss: 1.00248122215271\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.740729331970215 | KNN Loss: 3.7049355506896973 | BCE Loss: 1.0357937812805176\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.7694501876831055 | KNN Loss: 3.7246909141540527 | BCE Loss: 1.0447595119476318\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.685794830322266 | KNN Loss: 3.6612887382507324 | BCE Loss: 1.024505853652954\n",
      "Epoch   291: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.683653831481934 | KNN Loss: 3.672271728515625 | BCE Loss: 1.0113821029663086\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.729980945587158 | KNN Loss: 3.7077524662017822 | BCE Loss: 1.022228479385376\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.727781295776367 | KNN Loss: 3.684582471847534 | BCE Loss: 1.043198823928833\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.702857971191406 | KNN Loss: 3.687601327896118 | BCE Loss: 1.015256643295288\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.706614017486572 | KNN Loss: 3.6855239868164062 | BCE Loss: 1.0210901498794556\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.704428672790527 | KNN Loss: 3.692765712738037 | BCE Loss: 1.0116627216339111\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.735346794128418 | KNN Loss: 3.6905860900878906 | BCE Loss: 1.0447609424591064\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.728680610656738 | KNN Loss: 3.700762987136841 | BCE Loss: 1.0279176235198975\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.698052406311035 | KNN Loss: 3.656961679458618 | BCE Loss: 1.0410908460617065\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.664107799530029 | KNN Loss: 3.6450726985931396 | BCE Loss: 1.0190349817276\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.739404678344727 | KNN Loss: 3.7047786712646484 | BCE Loss: 1.0346258878707886\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.6828742027282715 | KNN Loss: 3.6965882778167725 | BCE Loss: 0.9862858057022095\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.752890586853027 | KNN Loss: 3.7183291912078857 | BCE Loss: 1.0345616340637207\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.689174652099609 | KNN Loss: 3.690037250518799 | BCE Loss: 0.9991373419761658\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.690341949462891 | KNN Loss: 3.681631088256836 | BCE Loss: 1.0087107419967651\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.756809711456299 | KNN Loss: 3.709506034851074 | BCE Loss: 1.0473036766052246\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.73620080947876 | KNN Loss: 3.7182199954986572 | BCE Loss: 1.0179808139801025\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.724896430969238 | KNN Loss: 3.6996114253997803 | BCE Loss: 1.025285243988037\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.69170618057251 | KNN Loss: 3.6722910404205322 | BCE Loss: 1.0194151401519775\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.70144510269165 | KNN Loss: 3.6791210174560547 | BCE Loss: 1.0223240852355957\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.7075276374816895 | KNN Loss: 3.701636552810669 | BCE Loss: 1.0058910846710205\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.7100677490234375 | KNN Loss: 3.6798276901245117 | BCE Loss: 1.0302398204803467\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.7283477783203125 | KNN Loss: 3.6787619590759277 | BCE Loss: 1.0495857000350952\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.7312726974487305 | KNN Loss: 3.686542272567749 | BCE Loss: 1.0447304248809814\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.727231502532959 | KNN Loss: 3.684943199157715 | BCE Loss: 1.0422881841659546\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.704519748687744 | KNN Loss: 3.692579507827759 | BCE Loss: 1.0119401216506958\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.703133583068848 | KNN Loss: 3.703242540359497 | BCE Loss: 0.9998911619186401\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.6750688552856445 | KNN Loss: 3.6606929302215576 | BCE Loss: 1.014376163482666\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.7317304611206055 | KNN Loss: 3.682487964630127 | BCE Loss: 1.049242377281189\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.7010722160339355 | KNN Loss: 3.667372941970825 | BCE Loss: 1.0336991548538208\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.681571960449219 | KNN Loss: 3.669292688369751 | BCE Loss: 1.0122792720794678\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.7037739753723145 | KNN Loss: 3.676520824432373 | BCE Loss: 1.027253270149231\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.671013832092285 | KNN Loss: 3.6595535278320312 | BCE Loss: 1.011460304260254\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.731661319732666 | KNN Loss: 3.703974962234497 | BCE Loss: 1.0276864767074585\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.74525260925293 | KNN Loss: 3.734628438949585 | BCE Loss: 1.0106239318847656\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.722949504852295 | KNN Loss: 3.6879587173461914 | BCE Loss: 1.034990668296814\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.749262809753418 | KNN Loss: 3.70632004737854 | BCE Loss: 1.0429428815841675\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.6505022048950195 | KNN Loss: 3.6657073497772217 | BCE Loss: 0.9847946166992188\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.6984686851501465 | KNN Loss: 3.673524856567383 | BCE Loss: 1.0249438285827637\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.741045951843262 | KNN Loss: 3.712172031402588 | BCE Loss: 1.0288736820220947\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.73335075378418 | KNN Loss: 3.703300714492798 | BCE Loss: 1.0300499200820923\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.71401834487915 | KNN Loss: 3.6914963722229004 | BCE Loss: 1.0225218534469604\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.700338363647461 | KNN Loss: 3.676274061203003 | BCE Loss: 1.024064064025879\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.748624801635742 | KNN Loss: 3.724342107772827 | BCE Loss: 1.0242829322814941\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.744091510772705 | KNN Loss: 3.6993958950042725 | BCE Loss: 1.0446956157684326\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.744886875152588 | KNN Loss: 3.6968822479248047 | BCE Loss: 1.0480046272277832\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.716241359710693 | KNN Loss: 3.6588761806488037 | BCE Loss: 1.0573651790618896\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.707073211669922 | KNN Loss: 3.680561065673828 | BCE Loss: 1.0265122652053833\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.697390079498291 | KNN Loss: 3.6834981441497803 | BCE Loss: 1.0138919353485107\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.715436935424805 | KNN Loss: 3.689429759979248 | BCE Loss: 1.0260069370269775\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.744058609008789 | KNN Loss: 3.7005972862243652 | BCE Loss: 1.0434613227844238\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.705056667327881 | KNN Loss: 3.677901029586792 | BCE Loss: 1.0271556377410889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.708841323852539 | KNN Loss: 3.688969612121582 | BCE Loss: 1.0198719501495361\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.710621356964111 | KNN Loss: 3.674962282180786 | BCE Loss: 1.0356590747833252\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.729092597961426 | KNN Loss: 3.7069194316864014 | BCE Loss: 1.0221730470657349\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.697409629821777 | KNN Loss: 3.6514501571655273 | BCE Loss: 1.0459595918655396\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.7498369216918945 | KNN Loss: 3.714813232421875 | BCE Loss: 1.0350239276885986\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.6861982345581055 | KNN Loss: 3.6597402095794678 | BCE Loss: 1.0264581441879272\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.717418670654297 | KNN Loss: 3.700214147567749 | BCE Loss: 1.0172045230865479\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.70927619934082 | KNN Loss: 3.677238941192627 | BCE Loss: 1.032037377357483\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.725172519683838 | KNN Loss: 3.7096076011657715 | BCE Loss: 1.0155647993087769\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.7178826332092285 | KNN Loss: 3.695772647857666 | BCE Loss: 1.0221099853515625\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.713541030883789 | KNN Loss: 3.6741747856140137 | BCE Loss: 1.0393662452697754\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.708510398864746 | KNN Loss: 3.6769609451293945 | BCE Loss: 1.0315492153167725\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.7327775955200195 | KNN Loss: 3.7124252319335938 | BCE Loss: 1.0203521251678467\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.735261917114258 | KNN Loss: 3.7085204124450684 | BCE Loss: 1.0267412662506104\n",
      "Epoch   302: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.748570442199707 | KNN Loss: 3.7061562538146973 | BCE Loss: 1.0424139499664307\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.709632873535156 | KNN Loss: 3.69083309173584 | BCE Loss: 1.0187995433807373\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.67405366897583 | KNN Loss: 3.66749906539917 | BCE Loss: 1.0065547227859497\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.704962253570557 | KNN Loss: 3.6746749877929688 | BCE Loss: 1.030287265777588\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.7309064865112305 | KNN Loss: 3.6940906047821045 | BCE Loss: 1.036816120147705\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.719694137573242 | KNN Loss: 3.692131280899048 | BCE Loss: 1.0275627374649048\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.670228958129883 | KNN Loss: 3.6831178665161133 | BCE Loss: 0.9871110320091248\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.678887367248535 | KNN Loss: 3.677189588546753 | BCE Loss: 1.0016976594924927\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.704968452453613 | KNN Loss: 3.684519052505493 | BCE Loss: 1.0204492807388306\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.699203014373779 | KNN Loss: 3.673187494277954 | BCE Loss: 1.0260156393051147\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.688126087188721 | KNN Loss: 3.6638433933258057 | BCE Loss: 1.0242828130722046\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.759601593017578 | KNN Loss: 3.7349143028259277 | BCE Loss: 1.02468740940094\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.6821370124816895 | KNN Loss: 3.680631637573242 | BCE Loss: 1.0015053749084473\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.741733074188232 | KNN Loss: 3.7310376167297363 | BCE Loss: 1.010695457458496\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.704180717468262 | KNN Loss: 3.675487756729126 | BCE Loss: 1.0286928415298462\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.725424766540527 | KNN Loss: 3.693042278289795 | BCE Loss: 1.0323822498321533\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.75547981262207 | KNN Loss: 3.723759889602661 | BCE Loss: 1.0317198038101196\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.7711286544799805 | KNN Loss: 3.7136728763580322 | BCE Loss: 1.0574560165405273\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.765474319458008 | KNN Loss: 3.73293399810791 | BCE Loss: 1.0325400829315186\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.732105255126953 | KNN Loss: 3.7138540744781494 | BCE Loss: 1.0182511806488037\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.732953071594238 | KNN Loss: 3.6928718090057373 | BCE Loss: 1.0400811433792114\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.682427406311035 | KNN Loss: 3.672898769378662 | BCE Loss: 1.0095288753509521\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.70691442489624 | KNN Loss: 3.674123764038086 | BCE Loss: 1.0327905416488647\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.737175941467285 | KNN Loss: 3.6879682540893555 | BCE Loss: 1.0492076873779297\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.7113423347473145 | KNN Loss: 3.675285577774048 | BCE Loss: 1.0360568761825562\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.709461212158203 | KNN Loss: 3.659017562866211 | BCE Loss: 1.0504438877105713\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.698633193969727 | KNN Loss: 3.661426305770874 | BCE Loss: 1.037207007408142\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.668890953063965 | KNN Loss: 3.674747943878174 | BCE Loss: 0.9941429495811462\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.782561302185059 | KNN Loss: 3.748589038848877 | BCE Loss: 1.0339725017547607\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.690874099731445 | KNN Loss: 3.70086669921875 | BCE Loss: 0.9900075197219849\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.7562456130981445 | KNN Loss: 3.7489583492279053 | BCE Loss: 1.0072872638702393\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.733719825744629 | KNN Loss: 3.7008557319641113 | BCE Loss: 1.0328640937805176\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.7230072021484375 | KNN Loss: 3.689056158065796 | BCE Loss: 1.033950924873352\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.694783687591553 | KNN Loss: 3.6922168731689453 | BCE Loss: 1.0025668144226074\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.710279941558838 | KNN Loss: 3.682443141937256 | BCE Loss: 1.027836799621582\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.725260257720947 | KNN Loss: 3.7130250930786133 | BCE Loss: 1.0122350454330444\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.6935014724731445 | KNN Loss: 3.6813135147094727 | BCE Loss: 1.012188196182251\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.771242141723633 | KNN Loss: 3.7331414222717285 | BCE Loss: 1.0381006002426147\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.738074779510498 | KNN Loss: 3.689356803894043 | BCE Loss: 1.0487180948257446\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.70694637298584 | KNN Loss: 3.673063278198242 | BCE Loss: 1.0338830947875977\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.7119340896606445 | KNN Loss: 3.687621831893921 | BCE Loss: 1.0243120193481445\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.738694667816162 | KNN Loss: 3.7055134773254395 | BCE Loss: 1.0331811904907227\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.741585731506348 | KNN Loss: 3.7116644382476807 | BCE Loss: 1.029921293258667\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.739583969116211 | KNN Loss: 3.719261884689331 | BCE Loss: 1.0203220844268799\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.6858015060424805 | KNN Loss: 3.6460776329040527 | BCE Loss: 1.0397236347198486\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.737818241119385 | KNN Loss: 3.7147414684295654 | BCE Loss: 1.0230766534805298\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.7574543952941895 | KNN Loss: 3.733900547027588 | BCE Loss: 1.0235538482666016\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.674285888671875 | KNN Loss: 3.66491436958313 | BCE Loss: 1.0093716382980347\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.735840797424316 | KNN Loss: 3.6937673091888428 | BCE Loss: 1.0420737266540527\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.699177265167236 | KNN Loss: 3.6683712005615234 | BCE Loss: 1.030806064605713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.6960248947143555 | KNN Loss: 3.6922380924224854 | BCE Loss: 1.0037866830825806\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.705151081085205 | KNN Loss: 3.688892364501953 | BCE Loss: 1.016258716583252\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.71847677230835 | KNN Loss: 3.6952755451202393 | BCE Loss: 1.0232012271881104\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.75611686706543 | KNN Loss: 3.7108566761016846 | BCE Loss: 1.0452601909637451\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.664460182189941 | KNN Loss: 3.658505439758301 | BCE Loss: 1.0059548616409302\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.742511749267578 | KNN Loss: 3.7012107372283936 | BCE Loss: 1.0413012504577637\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.71280574798584 | KNN Loss: 3.6783576011657715 | BCE Loss: 1.0344479084014893\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.719955921173096 | KNN Loss: 3.6877644062042236 | BCE Loss: 1.0321916341781616\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.699336528778076 | KNN Loss: 3.683737277984619 | BCE Loss: 1.015599250793457\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.7125349044799805 | KNN Loss: 3.6824514865875244 | BCE Loss: 1.0300835371017456\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.7245917320251465 | KNN Loss: 3.6984763145446777 | BCE Loss: 1.0261154174804688\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.756975173950195 | KNN Loss: 3.7167069911956787 | BCE Loss: 1.0402681827545166\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.690371036529541 | KNN Loss: 3.674457311630249 | BCE Loss: 1.015913724899292\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.763676166534424 | KNN Loss: 3.7303147315979004 | BCE Loss: 1.033361554145813\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.739315986633301 | KNN Loss: 3.6994454860687256 | BCE Loss: 1.0398705005645752\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.731931209564209 | KNN Loss: 3.7185781002044678 | BCE Loss: 1.0133531093597412\n",
      "Epoch   313: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.738485336303711 | KNN Loss: 3.6836042404174805 | BCE Loss: 1.054880976676941\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.742964744567871 | KNN Loss: 3.704749584197998 | BCE Loss: 1.0382153987884521\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.726812839508057 | KNN Loss: 3.708069086074829 | BCE Loss: 1.0187437534332275\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.695469379425049 | KNN Loss: 3.6801493167877197 | BCE Loss: 1.0153201818466187\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.692142009735107 | KNN Loss: 3.680635929107666 | BCE Loss: 1.0115060806274414\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.699162483215332 | KNN Loss: 3.682399272918701 | BCE Loss: 1.01676344871521\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.691825866699219 | KNN Loss: 3.687211513519287 | BCE Loss: 1.0046145915985107\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.736996650695801 | KNN Loss: 3.7218124866485596 | BCE Loss: 1.015183925628662\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.703545093536377 | KNN Loss: 3.66843581199646 | BCE Loss: 1.035109281539917\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.709600925445557 | KNN Loss: 3.6824512481689453 | BCE Loss: 1.0271496772766113\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.701788425445557 | KNN Loss: 3.7005362510681152 | BCE Loss: 1.0012521743774414\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.713937759399414 | KNN Loss: 3.7073793411254883 | BCE Loss: 1.0065584182739258\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.706321716308594 | KNN Loss: 3.690859317779541 | BCE Loss: 1.0154622793197632\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.726528167724609 | KNN Loss: 3.6897287368774414 | BCE Loss: 1.036799669265747\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.741581916809082 | KNN Loss: 3.6838951110839844 | BCE Loss: 1.0576868057250977\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.721921920776367 | KNN Loss: 3.677802324295044 | BCE Loss: 1.0441198348999023\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.686858177185059 | KNN Loss: 3.6904220581054688 | BCE Loss: 0.9964361786842346\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.71900749206543 | KNN Loss: 3.6834728717803955 | BCE Loss: 1.035534381866455\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.737634181976318 | KNN Loss: 3.6971547603607178 | BCE Loss: 1.0404794216156006\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.6596527099609375 | KNN Loss: 3.660822868347168 | BCE Loss: 0.9988298416137695\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.7310638427734375 | KNN Loss: 3.6834716796875 | BCE Loss: 1.0475919246673584\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.680505275726318 | KNN Loss: 3.6583971977233887 | BCE Loss: 1.0221080780029297\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.677454471588135 | KNN Loss: 3.6613240242004395 | BCE Loss: 1.0161303281784058\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.657057762145996 | KNN Loss: 3.6615705490112305 | BCE Loss: 0.9954870939254761\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.756392955780029 | KNN Loss: 3.7119200229644775 | BCE Loss: 1.0444728136062622\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.698664665222168 | KNN Loss: 3.687624931335449 | BCE Loss: 1.0110396146774292\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.742135047912598 | KNN Loss: 3.6988277435302734 | BCE Loss: 1.0433075428009033\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.71942663192749 | KNN Loss: 3.6951990127563477 | BCE Loss: 1.0242277383804321\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.689748764038086 | KNN Loss: 3.679513692855835 | BCE Loss: 1.01023530960083\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.71876335144043 | KNN Loss: 3.688657283782959 | BCE Loss: 1.0301058292388916\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.658031463623047 | KNN Loss: 3.667372465133667 | BCE Loss: 0.9906589984893799\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.69606876373291 | KNN Loss: 3.657575845718384 | BCE Loss: 1.0384929180145264\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.710318088531494 | KNN Loss: 3.6793365478515625 | BCE Loss: 1.0309815406799316\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.73608922958374 | KNN Loss: 3.6914563179016113 | BCE Loss: 1.044632911682129\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.750621795654297 | KNN Loss: 3.707340717315674 | BCE Loss: 1.043280839920044\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.741907596588135 | KNN Loss: 3.698436975479126 | BCE Loss: 1.0434707403182983\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.7124762535095215 | KNN Loss: 3.6690003871917725 | BCE Loss: 1.0434759855270386\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.731712341308594 | KNN Loss: 3.687657594680786 | BCE Loss: 1.0440547466278076\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.700222969055176 | KNN Loss: 3.6645150184631348 | BCE Loss: 1.0357081890106201\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.760546684265137 | KNN Loss: 3.7463057041168213 | BCE Loss: 1.0142407417297363\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.717567443847656 | KNN Loss: 3.6747591495513916 | BCE Loss: 1.0428085327148438\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.72840690612793 | KNN Loss: 3.691114664077759 | BCE Loss: 1.037292242050171\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.736628532409668 | KNN Loss: 3.7368335723876953 | BCE Loss: 0.9997947812080383\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.717072010040283 | KNN Loss: 3.710056781768799 | BCE Loss: 1.0070152282714844\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.7517900466918945 | KNN Loss: 3.7135369777679443 | BCE Loss: 1.0382530689239502\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.721762657165527 | KNN Loss: 3.6971189975738525 | BCE Loss: 1.0246436595916748\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.719073295593262 | KNN Loss: 3.6822683811187744 | BCE Loss: 1.0368050336837769\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.714109420776367 | KNN Loss: 3.680835485458374 | BCE Loss: 1.033273696899414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.71147346496582 | KNN Loss: 3.678046226501465 | BCE Loss: 1.033427357673645\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.734026908874512 | KNN Loss: 3.730025053024292 | BCE Loss: 1.0040016174316406\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.761735439300537 | KNN Loss: 3.736593723297119 | BCE Loss: 1.0251415967941284\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.717321395874023 | KNN Loss: 3.6874523162841797 | BCE Loss: 1.0298693180084229\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.723474502563477 | KNN Loss: 3.6831252574920654 | BCE Loss: 1.040349006652832\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.7253546714782715 | KNN Loss: 3.692889928817749 | BCE Loss: 1.032464623451233\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.695505619049072 | KNN Loss: 3.686382532119751 | BCE Loss: 1.0091230869293213\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.720823764801025 | KNN Loss: 3.691710948944092 | BCE Loss: 1.0291128158569336\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.685183525085449 | KNN Loss: 3.6766164302825928 | BCE Loss: 1.0085668563842773\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.693406581878662 | KNN Loss: 3.6702513694763184 | BCE Loss: 1.0231552124023438\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.703187465667725 | KNN Loss: 3.6946284770965576 | BCE Loss: 1.008558988571167\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.712551116943359 | KNN Loss: 3.674090623855591 | BCE Loss: 1.0384604930877686\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.701310634613037 | KNN Loss: 3.671546697616577 | BCE Loss: 1.0297638177871704\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.7284111976623535 | KNN Loss: 3.6934080123901367 | BCE Loss: 1.0350031852722168\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.719249725341797 | KNN Loss: 3.6739606857299805 | BCE Loss: 1.0452892780303955\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.723681926727295 | KNN Loss: 3.725161552429199 | BCE Loss: 0.9985203146934509\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.703567981719971 | KNN Loss: 3.672396183013916 | BCE Loss: 1.0311717987060547\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.752506256103516 | KNN Loss: 3.7258570194244385 | BCE Loss: 1.0266492366790771\n",
      "Epoch   324: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.758129596710205 | KNN Loss: 3.709254026412964 | BCE Loss: 1.0488755702972412\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.680321216583252 | KNN Loss: 3.690945863723755 | BCE Loss: 0.9893753528594971\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.705733776092529 | KNN Loss: 3.6834025382995605 | BCE Loss: 1.0223313570022583\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.700738430023193 | KNN Loss: 3.6736347675323486 | BCE Loss: 1.0271036624908447\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.708281517028809 | KNN Loss: 3.688075542449951 | BCE Loss: 1.0202057361602783\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.712296485900879 | KNN Loss: 3.6756980419158936 | BCE Loss: 1.0365986824035645\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.72866153717041 | KNN Loss: 3.6680805683135986 | BCE Loss: 1.0605812072753906\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.711230754852295 | KNN Loss: 3.6922824382781982 | BCE Loss: 1.0189484357833862\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.665937900543213 | KNN Loss: 3.66658091545105 | BCE Loss: 0.9993569254875183\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.730775833129883 | KNN Loss: 3.716522216796875 | BCE Loss: 1.0142537355422974\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.697052955627441 | KNN Loss: 3.6896450519561768 | BCE Loss: 1.0074076652526855\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.705746650695801 | KNN Loss: 3.680469512939453 | BCE Loss: 1.0252773761749268\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.7057037353515625 | KNN Loss: 3.6860270500183105 | BCE Loss: 1.019676923751831\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.68865966796875 | KNN Loss: 3.6728432178497314 | BCE Loss: 1.0158162117004395\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.7255449295043945 | KNN Loss: 3.7033486366271973 | BCE Loss: 1.0221962928771973\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.669013023376465 | KNN Loss: 3.6668450832366943 | BCE Loss: 1.0021681785583496\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.7648468017578125 | KNN Loss: 3.716433048248291 | BCE Loss: 1.0484137535095215\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.666144371032715 | KNN Loss: 3.6730639934539795 | BCE Loss: 0.9930803775787354\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.766171932220459 | KNN Loss: 3.7105534076690674 | BCE Loss: 1.055618405342102\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.6776275634765625 | KNN Loss: 3.6778297424316406 | BCE Loss: 0.9997977018356323\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.753390312194824 | KNN Loss: 3.7080395221710205 | BCE Loss: 1.0453507900238037\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.682292938232422 | KNN Loss: 3.659127712249756 | BCE Loss: 1.023165225982666\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.706186294555664 | KNN Loss: 3.7094674110412598 | BCE Loss: 0.9967189431190491\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.7075629234313965 | KNN Loss: 3.7044053077697754 | BCE Loss: 1.0031577348709106\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.827188491821289 | KNN Loss: 3.766023635864258 | BCE Loss: 1.0611648559570312\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.680502891540527 | KNN Loss: 3.660592794418335 | BCE Loss: 1.0199098587036133\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.672427177429199 | KNN Loss: 3.645899534225464 | BCE Loss: 1.0265276432037354\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.698551177978516 | KNN Loss: 3.6886067390441895 | BCE Loss: 1.0099446773529053\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.765332221984863 | KNN Loss: 3.69758939743042 | BCE Loss: 1.0677430629730225\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.669935703277588 | KNN Loss: 3.6490354537963867 | BCE Loss: 1.0209002494812012\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.690119743347168 | KNN Loss: 3.680830717086792 | BCE Loss: 1.0092887878417969\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.715070724487305 | KNN Loss: 3.6716818809509277 | BCE Loss: 1.043388843536377\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.7191314697265625 | KNN Loss: 3.7060060501098633 | BCE Loss: 1.0131254196166992\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.76540470123291 | KNN Loss: 3.713712692260742 | BCE Loss: 1.051692008972168\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.692349910736084 | KNN Loss: 3.6782939434051514 | BCE Loss: 1.0140559673309326\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.709019184112549 | KNN Loss: 3.7008039951324463 | BCE Loss: 1.008215308189392\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.761248588562012 | KNN Loss: 3.7227096557617188 | BCE Loss: 1.0385388135910034\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.693626880645752 | KNN Loss: 3.6783297061920166 | BCE Loss: 1.0152970552444458\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.7410688400268555 | KNN Loss: 3.7022178173065186 | BCE Loss: 1.038851261138916\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.683023929595947 | KNN Loss: 3.6638519763946533 | BCE Loss: 1.019171953201294\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.686553478240967 | KNN Loss: 3.6735129356384277 | BCE Loss: 1.0130406618118286\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.68975830078125 | KNN Loss: 3.6680285930633545 | BCE Loss: 1.021729826927185\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.7318830490112305 | KNN Loss: 3.6756491661071777 | BCE Loss: 1.0562338829040527\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.795759201049805 | KNN Loss: 3.7566356658935547 | BCE Loss: 1.0391236543655396\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.702802658081055 | KNN Loss: 3.687930107116699 | BCE Loss: 1.0148727893829346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.737398624420166 | KNN Loss: 3.738171339035034 | BCE Loss: 0.9992274045944214\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.729674816131592 | KNN Loss: 3.7005040645599365 | BCE Loss: 1.0291706323623657\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.722132205963135 | KNN Loss: 3.684361696243286 | BCE Loss: 1.037770390510559\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.681082725524902 | KNN Loss: 3.6716907024383545 | BCE Loss: 1.009392261505127\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.749914169311523 | KNN Loss: 3.7205870151519775 | BCE Loss: 1.029327392578125\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.7605085372924805 | KNN Loss: 3.7273824214935303 | BCE Loss: 1.0331259965896606\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.719786167144775 | KNN Loss: 3.692368268966675 | BCE Loss: 1.0274180173873901\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.730352401733398 | KNN Loss: 3.6785783767700195 | BCE Loss: 1.0517741441726685\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.743465423583984 | KNN Loss: 3.709831476211548 | BCE Loss: 1.033634066581726\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.72999906539917 | KNN Loss: 3.7304906845092773 | BCE Loss: 0.9995085000991821\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.721698760986328 | KNN Loss: 3.7207746505737305 | BCE Loss: 1.000923991203308\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.744487762451172 | KNN Loss: 3.6902194023132324 | BCE Loss: 1.0542685985565186\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.778231620788574 | KNN Loss: 3.7435696125030518 | BCE Loss: 1.0346620082855225\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.714282989501953 | KNN Loss: 3.6764440536499023 | BCE Loss: 1.0378386974334717\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.743288516998291 | KNN Loss: 3.728130578994751 | BCE Loss: 1.0151578187942505\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.73331880569458 | KNN Loss: 3.697772264480591 | BCE Loss: 1.0355465412139893\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.703336715698242 | KNN Loss: 3.68839693069458 | BCE Loss: 1.0149396657943726\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.6958513259887695 | KNN Loss: 3.660181760787964 | BCE Loss: 1.0356693267822266\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.708969593048096 | KNN Loss: 3.690279245376587 | BCE Loss: 1.0186903476715088\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.723837852478027 | KNN Loss: 3.6788289546966553 | BCE Loss: 1.0450090169906616\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.764246940612793 | KNN Loss: 3.7349579334259033 | BCE Loss: 1.0292887687683105\n",
      "Epoch   335: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.714374542236328 | KNN Loss: 3.6830122470855713 | BCE Loss: 1.031362533569336\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.676600456237793 | KNN Loss: 3.668515920639038 | BCE Loss: 1.0080845355987549\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.670734405517578 | KNN Loss: 3.6685760021209717 | BCE Loss: 1.0021584033966064\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.733777046203613 | KNN Loss: 3.6854119300842285 | BCE Loss: 1.0483651161193848\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.699489593505859 | KNN Loss: 3.6833486557006836 | BCE Loss: 1.0161409378051758\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.7037353515625 | KNN Loss: 3.6875107288360596 | BCE Loss: 1.0162248611450195\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.710862159729004 | KNN Loss: 3.694929361343384 | BCE Loss: 1.0159330368041992\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.723170280456543 | KNN Loss: 3.697704553604126 | BCE Loss: 1.025465965270996\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.695896625518799 | KNN Loss: 3.681692123413086 | BCE Loss: 1.0142046213150024\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.705955505371094 | KNN Loss: 3.6936841011047363 | BCE Loss: 1.0122711658477783\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.7313151359558105 | KNN Loss: 3.689615249633789 | BCE Loss: 1.0416998863220215\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.731536865234375 | KNN Loss: 3.695340394973755 | BCE Loss: 1.0361964702606201\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.702775001525879 | KNN Loss: 3.6669793128967285 | BCE Loss: 1.0357959270477295\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.736604690551758 | KNN Loss: 3.6928281784057617 | BCE Loss: 1.0437767505645752\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.701860427856445 | KNN Loss: 3.6738901138305664 | BCE Loss: 1.0279700756072998\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.734654426574707 | KNN Loss: 3.6991162300109863 | BCE Loss: 1.0355379581451416\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.7268877029418945 | KNN Loss: 3.7048003673553467 | BCE Loss: 1.0220872163772583\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.70238733291626 | KNN Loss: 3.7096176147460938 | BCE Loss: 0.992769718170166\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.710724830627441 | KNN Loss: 3.674196720123291 | BCE Loss: 1.0365279912948608\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.654897689819336 | KNN Loss: 3.6590514183044434 | BCE Loss: 0.9958460330963135\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.668086528778076 | KNN Loss: 3.6727495193481445 | BCE Loss: 0.9953368902206421\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.722752094268799 | KNN Loss: 3.697824478149414 | BCE Loss: 1.0249276161193848\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.659549713134766 | KNN Loss: 3.6615376472473145 | BCE Loss: 0.9980120658874512\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.724915504455566 | KNN Loss: 3.688791513442993 | BCE Loss: 1.0361239910125732\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.689533233642578 | KNN Loss: 3.6468684673309326 | BCE Loss: 1.0426647663116455\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.690558910369873 | KNN Loss: 3.6802217960357666 | BCE Loss: 1.0103371143341064\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.719332695007324 | KNN Loss: 3.698014497756958 | BCE Loss: 1.0213184356689453\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.704421043395996 | KNN Loss: 3.6721913814544678 | BCE Loss: 1.0322296619415283\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.679800987243652 | KNN Loss: 3.669612407684326 | BCE Loss: 1.0101885795593262\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.705878734588623 | KNN Loss: 3.689845085144043 | BCE Loss: 1.01603364944458\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.718056678771973 | KNN Loss: 3.685157537460327 | BCE Loss: 1.0328993797302246\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.731147289276123 | KNN Loss: 3.695194721221924 | BCE Loss: 1.0359525680541992\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.7338666915893555 | KNN Loss: 3.716625928878784 | BCE Loss: 1.0172410011291504\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.715489387512207 | KNN Loss: 3.696340560913086 | BCE Loss: 1.019148588180542\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.724151611328125 | KNN Loss: 3.683499813079834 | BCE Loss: 1.040651559829712\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.684257984161377 | KNN Loss: 3.6596672534942627 | BCE Loss: 1.0245906114578247\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.7399492263793945 | KNN Loss: 3.67022442817688 | BCE Loss: 1.0697247982025146\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.68556547164917 | KNN Loss: 3.6690540313720703 | BCE Loss: 1.0165114402770996\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.741653919219971 | KNN Loss: 3.7187249660491943 | BCE Loss: 1.0229289531707764\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.715975761413574 | KNN Loss: 3.6603965759277344 | BCE Loss: 1.055579423904419\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.71964168548584 | KNN Loss: 3.6914710998535156 | BCE Loss: 1.0281703472137451\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.6889967918396 | KNN Loss: 3.6783878803253174 | BCE Loss: 1.0106087923049927\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.778613090515137 | KNN Loss: 3.718637704849243 | BCE Loss: 1.0599753856658936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.7331647872924805 | KNN Loss: 3.7018520832061768 | BCE Loss: 1.0313127040863037\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.770681381225586 | KNN Loss: 3.7402164936065674 | BCE Loss: 1.0304646492004395\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.733707427978516 | KNN Loss: 3.7056727409362793 | BCE Loss: 1.0280349254608154\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.7370805740356445 | KNN Loss: 3.7476553916931152 | BCE Loss: 0.9894253015518188\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.730641841888428 | KNN Loss: 3.6750762462615967 | BCE Loss: 1.055565595626831\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.714796543121338 | KNN Loss: 3.686882257461548 | BCE Loss: 1.02791428565979\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.666537761688232 | KNN Loss: 3.6694324016571045 | BCE Loss: 0.9971053600311279\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.77365779876709 | KNN Loss: 3.707655906677246 | BCE Loss: 1.0660021305084229\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.714398384094238 | KNN Loss: 3.6858577728271484 | BCE Loss: 1.0285406112670898\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.650214195251465 | KNN Loss: 3.668260097503662 | BCE Loss: 0.9819540977478027\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.716451168060303 | KNN Loss: 3.719850778579712 | BCE Loss: 0.9966004490852356\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.725285530090332 | KNN Loss: 3.6966378688812256 | BCE Loss: 1.028647541999817\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.727222442626953 | KNN Loss: 3.6957037448883057 | BCE Loss: 1.0315189361572266\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.678728103637695 | KNN Loss: 3.665168285369873 | BCE Loss: 1.0135600566864014\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.706552505493164 | KNN Loss: 3.6937367916107178 | BCE Loss: 1.0128155946731567\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.717888832092285 | KNN Loss: 3.6855270862579346 | BCE Loss: 1.0323615074157715\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.713412284851074 | KNN Loss: 3.6728286743164062 | BCE Loss: 1.0405837297439575\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.719598770141602 | KNN Loss: 3.6816256046295166 | BCE Loss: 1.0379732847213745\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.691424369812012 | KNN Loss: 3.6860530376434326 | BCE Loss: 1.0053712129592896\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.7062201499938965 | KNN Loss: 3.7055294513702393 | BCE Loss: 1.0006908178329468\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.739910125732422 | KNN Loss: 3.7035467624664307 | BCE Loss: 1.036363124847412\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.750918865203857 | KNN Loss: 3.707421064376831 | BCE Loss: 1.0434978008270264\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.680329322814941 | KNN Loss: 3.66420316696167 | BCE Loss: 1.0161263942718506\n",
      "Epoch   346: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.759761810302734 | KNN Loss: 3.706760883331299 | BCE Loss: 1.0530011653900146\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.729791641235352 | KNN Loss: 3.7018513679504395 | BCE Loss: 1.027940034866333\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.749156475067139 | KNN Loss: 3.7169549465179443 | BCE Loss: 1.0322015285491943\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.700395584106445 | KNN Loss: 3.665390968322754 | BCE Loss: 1.0350044965744019\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.728838920593262 | KNN Loss: 3.709287166595459 | BCE Loss: 1.0195517539978027\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.747325897216797 | KNN Loss: 3.702378511428833 | BCE Loss: 1.0449471473693848\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.708300590515137 | KNN Loss: 3.6814541816711426 | BCE Loss: 1.0268464088439941\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.722239971160889 | KNN Loss: 3.714087963104248 | BCE Loss: 1.0081521272659302\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.713284015655518 | KNN Loss: 3.715796947479248 | BCE Loss: 0.9974870681762695\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.703441619873047 | KNN Loss: 3.701446294784546 | BCE Loss: 1.0019950866699219\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.776677131652832 | KNN Loss: 3.7382500171661377 | BCE Loss: 1.0384268760681152\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.709270000457764 | KNN Loss: 3.6764423847198486 | BCE Loss: 1.0328277349472046\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.778262138366699 | KNN Loss: 3.7106637954711914 | BCE Loss: 1.067598581314087\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.703500747680664 | KNN Loss: 3.67671275138855 | BCE Loss: 1.0267879962921143\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.656135559082031 | KNN Loss: 3.6607015132904053 | BCE Loss: 0.9954338669776917\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.690834045410156 | KNN Loss: 3.679105043411255 | BCE Loss: 1.0117287635803223\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.68721342086792 | KNN Loss: 3.6598398685455322 | BCE Loss: 1.0273735523223877\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.650525093078613 | KNN Loss: 3.6713995933532715 | BCE Loss: 0.9791254997253418\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.675947189331055 | KNN Loss: 3.6479032039642334 | BCE Loss: 1.0280442237854004\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.7498650550842285 | KNN Loss: 3.7067291736602783 | BCE Loss: 1.0431360006332397\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.6642842292785645 | KNN Loss: 3.6610982418060303 | BCE Loss: 1.0031859874725342\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.718873977661133 | KNN Loss: 3.681821823120117 | BCE Loss: 1.0370519161224365\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.706531524658203 | KNN Loss: 3.670738697052002 | BCE Loss: 1.0357930660247803\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.6645965576171875 | KNN Loss: 3.666029453277588 | BCE Loss: 0.9985673427581787\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.738692283630371 | KNN Loss: 3.710677146911621 | BCE Loss: 1.02801513671875\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.699780464172363 | KNN Loss: 3.6902213096618652 | BCE Loss: 1.009558916091919\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.71265983581543 | KNN Loss: 3.6792988777160645 | BCE Loss: 1.0333611965179443\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.7225117683410645 | KNN Loss: 3.6664562225341797 | BCE Loss: 1.0560554265975952\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.696903228759766 | KNN Loss: 3.6883926391601562 | BCE Loss: 1.0085105895996094\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.700655460357666 | KNN Loss: 3.6918418407440186 | BCE Loss: 1.0088136196136475\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.66184139251709 | KNN Loss: 3.6415860652923584 | BCE Loss: 1.020255208015442\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.703920841217041 | KNN Loss: 3.6814308166503906 | BCE Loss: 1.0224899053573608\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.718080043792725 | KNN Loss: 3.6664106845855713 | BCE Loss: 1.0516693592071533\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.704543113708496 | KNN Loss: 3.690786361694336 | BCE Loss: 1.0137569904327393\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.687892436981201 | KNN Loss: 3.6677377223968506 | BCE Loss: 1.0201547145843506\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.700435161590576 | KNN Loss: 3.683601140975952 | BCE Loss: 1.0168339014053345\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.723663330078125 | KNN Loss: 3.682922601699829 | BCE Loss: 1.040740966796875\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.715063571929932 | KNN Loss: 3.6910157203674316 | BCE Loss: 1.0240477323532104\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.71839714050293 | KNN Loss: 3.692416191101074 | BCE Loss: 1.0259811878204346\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.663269996643066 | KNN Loss: 3.6657373905181885 | BCE Loss: 0.9975324273109436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.721430778503418 | KNN Loss: 3.6900882720947266 | BCE Loss: 1.0313427448272705\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.706968307495117 | KNN Loss: 3.7064082622528076 | BCE Loss: 1.0005601644515991\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.712953567504883 | KNN Loss: 3.698476791381836 | BCE Loss: 1.014477014541626\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.768014430999756 | KNN Loss: 3.7510414123535156 | BCE Loss: 1.0169730186462402\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.710958480834961 | KNN Loss: 3.686586856842041 | BCE Loss: 1.0243713855743408\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.769651889801025 | KNN Loss: 3.700533628463745 | BCE Loss: 1.0691182613372803\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.735799789428711 | KNN Loss: 3.725614547729492 | BCE Loss: 1.0101852416992188\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.719125747680664 | KNN Loss: 3.7047383785247803 | BCE Loss: 1.0143871307373047\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.703594207763672 | KNN Loss: 3.6542763710021973 | BCE Loss: 1.0493180751800537\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.68856954574585 | KNN Loss: 3.688908338546753 | BCE Loss: 0.999661386013031\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.688382625579834 | KNN Loss: 3.664602279663086 | BCE Loss: 1.023780345916748\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.733850479125977 | KNN Loss: 3.7068753242492676 | BCE Loss: 1.026975154876709\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.7334303855896 | KNN Loss: 3.716817617416382 | BCE Loss: 1.0166127681732178\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.697108268737793 | KNN Loss: 3.6478211879730225 | BCE Loss: 1.0492868423461914\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.69363260269165 | KNN Loss: 3.6700053215026855 | BCE Loss: 1.0236272811889648\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.7604522705078125 | KNN Loss: 3.7083539962768555 | BCE Loss: 1.0520981550216675\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.694638252258301 | KNN Loss: 3.6753315925598145 | BCE Loss: 1.0193068981170654\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.728933334350586 | KNN Loss: 3.7090537548065186 | BCE Loss: 1.0198798179626465\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.720738410949707 | KNN Loss: 3.682588577270508 | BCE Loss: 1.0381495952606201\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.710515022277832 | KNN Loss: 3.6801486015319824 | BCE Loss: 1.0303664207458496\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.66428279876709 | KNN Loss: 3.676600217819214 | BCE Loss: 0.9876825213432312\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.756347179412842 | KNN Loss: 3.7416229248046875 | BCE Loss: 1.0147243738174438\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.745999336242676 | KNN Loss: 3.713745355606079 | BCE Loss: 1.0322537422180176\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.716625213623047 | KNN Loss: 3.7085952758789062 | BCE Loss: 1.0080296993255615\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.691104888916016 | KNN Loss: 3.667224884033203 | BCE Loss: 1.0238797664642334\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.700490951538086 | KNN Loss: 3.661292314529419 | BCE Loss: 1.039198875427246\n",
      "Epoch   357: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.71169376373291 | KNN Loss: 3.7049217224121094 | BCE Loss: 1.0067718029022217\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.704508304595947 | KNN Loss: 3.6747848987579346 | BCE Loss: 1.0297234058380127\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.692735195159912 | KNN Loss: 3.684389591217041 | BCE Loss: 1.0083454847335815\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.70677375793457 | KNN Loss: 3.6748263835906982 | BCE Loss: 1.0319472551345825\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.685178756713867 | KNN Loss: 3.6498780250549316 | BCE Loss: 1.0353007316589355\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.764833450317383 | KNN Loss: 3.7237236499786377 | BCE Loss: 1.0411098003387451\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.7366204261779785 | KNN Loss: 3.677504539489746 | BCE Loss: 1.059116005897522\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.721180438995361 | KNN Loss: 3.6858971118927 | BCE Loss: 1.0352833271026611\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.681650161743164 | KNN Loss: 3.68214750289917 | BCE Loss: 0.999502420425415\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.674231052398682 | KNN Loss: 3.6864023208618164 | BCE Loss: 0.9878286123275757\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.683119297027588 | KNN Loss: 3.665736675262451 | BCE Loss: 1.0173827409744263\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.695204734802246 | KNN Loss: 3.6703567504882812 | BCE Loss: 1.0248479843139648\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.6595587730407715 | KNN Loss: 3.669739246368408 | BCE Loss: 0.9898195862770081\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.739943504333496 | KNN Loss: 3.7082273960113525 | BCE Loss: 1.031716227531433\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.712101936340332 | KNN Loss: 3.6881399154663086 | BCE Loss: 1.0239619016647339\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.752690315246582 | KNN Loss: 3.7043192386627197 | BCE Loss: 1.0483713150024414\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.745977878570557 | KNN Loss: 3.697866201400757 | BCE Loss: 1.0481116771697998\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.676600933074951 | KNN Loss: 3.673333168029785 | BCE Loss: 1.003267765045166\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.672032833099365 | KNN Loss: 3.6663196086883545 | BCE Loss: 1.0057131052017212\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.723353385925293 | KNN Loss: 3.6922318935394287 | BCE Loss: 1.0311217308044434\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.732787609100342 | KNN Loss: 3.6776931285858154 | BCE Loss: 1.0550944805145264\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.731209754943848 | KNN Loss: 3.691277027130127 | BCE Loss: 1.0399329662322998\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.686339378356934 | KNN Loss: 3.6645846366882324 | BCE Loss: 1.0217549800872803\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.728681564331055 | KNN Loss: 3.7024917602539062 | BCE Loss: 1.0261895656585693\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.772109508514404 | KNN Loss: 3.7311699390411377 | BCE Loss: 1.040939450263977\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.740879058837891 | KNN Loss: 3.719325304031372 | BCE Loss: 1.0215537548065186\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.759028434753418 | KNN Loss: 3.7158429622650146 | BCE Loss: 1.0431852340698242\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.752691745758057 | KNN Loss: 3.7283432483673096 | BCE Loss: 1.0243486166000366\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.7460527420043945 | KNN Loss: 3.694622278213501 | BCE Loss: 1.051430344581604\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.726500034332275 | KNN Loss: 3.712200880050659 | BCE Loss: 1.0142990350723267\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.680908203125 | KNN Loss: 3.6802287101745605 | BCE Loss: 1.0006794929504395\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.703300476074219 | KNN Loss: 3.6989099979400635 | BCE Loss: 1.0043904781341553\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.666818618774414 | KNN Loss: 3.663015842437744 | BCE Loss: 1.0038028955459595\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.6996870040893555 | KNN Loss: 3.692166328430176 | BCE Loss: 1.0075207948684692\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.728892803192139 | KNN Loss: 3.686272382736206 | BCE Loss: 1.042620301246643\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.724913120269775 | KNN Loss: 3.684154987335205 | BCE Loss: 1.0407580137252808\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.703972816467285 | KNN Loss: 3.675781011581421 | BCE Loss: 1.0281916856765747\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.680944919586182 | KNN Loss: 3.655648946762085 | BCE Loss: 1.0252959728240967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.7037858963012695 | KNN Loss: 3.6657140254974365 | BCE Loss: 1.038071870803833\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.705646514892578 | KNN Loss: 3.6757941246032715 | BCE Loss: 1.0298523902893066\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.696738243103027 | KNN Loss: 3.6903233528137207 | BCE Loss: 1.006414771080017\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.672262191772461 | KNN Loss: 3.6548614501953125 | BCE Loss: 1.0174009799957275\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.757284641265869 | KNN Loss: 3.705980062484741 | BCE Loss: 1.0513044595718384\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.697528839111328 | KNN Loss: 3.6949398517608643 | BCE Loss: 1.0025891065597534\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.741352081298828 | KNN Loss: 3.702030897140503 | BCE Loss: 1.0393214225769043\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.698066711425781 | KNN Loss: 3.6980419158935547 | BCE Loss: 1.0000249147415161\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.663959503173828 | KNN Loss: 3.6661341190338135 | BCE Loss: 0.9978254437446594\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.737874984741211 | KNN Loss: 3.693727731704712 | BCE Loss: 1.0441474914550781\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.718102931976318 | KNN Loss: 3.698249340057373 | BCE Loss: 1.0198535919189453\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.711697578430176 | KNN Loss: 3.6779673099517822 | BCE Loss: 1.0337305068969727\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.737439155578613 | KNN Loss: 3.7362868785858154 | BCE Loss: 1.0011520385742188\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.735884666442871 | KNN Loss: 3.703538179397583 | BCE Loss: 1.0323467254638672\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.692502975463867 | KNN Loss: 3.6801881790161133 | BCE Loss: 1.012314796447754\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.724740982055664 | KNN Loss: 3.6982955932617188 | BCE Loss: 1.0264453887939453\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.732730865478516 | KNN Loss: 3.702017307281494 | BCE Loss: 1.0307133197784424\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.767090797424316 | KNN Loss: 3.7106950283050537 | BCE Loss: 1.0563957691192627\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.68971586227417 | KNN Loss: 3.6616053581237793 | BCE Loss: 1.0281105041503906\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.666341781616211 | KNN Loss: 3.681999683380127 | BCE Loss: 0.9843419194221497\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.69366455078125 | KNN Loss: 3.6636409759521484 | BCE Loss: 1.0300233364105225\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.714878082275391 | KNN Loss: 3.6820006370544434 | BCE Loss: 1.0328776836395264\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.649448394775391 | KNN Loss: 3.6488454341888428 | BCE Loss: 1.0006027221679688\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.7152838706970215 | KNN Loss: 3.660682439804077 | BCE Loss: 1.0546014308929443\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.705312252044678 | KNN Loss: 3.665147066116333 | BCE Loss: 1.0401651859283447\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.725428104400635 | KNN Loss: 3.697467803955078 | BCE Loss: 1.0279603004455566\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.67458438873291 | KNN Loss: 3.6677424907684326 | BCE Loss: 1.0068421363830566\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.70651912689209 | KNN Loss: 3.697213888168335 | BCE Loss: 1.0093050003051758\n",
      "Epoch   368: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.68799352645874 | KNN Loss: 3.678640127182007 | BCE Loss: 1.0093533992767334\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.6766533851623535 | KNN Loss: 3.6750540733337402 | BCE Loss: 1.0015993118286133\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.726805686950684 | KNN Loss: 3.6903624534606934 | BCE Loss: 1.0364429950714111\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.734402179718018 | KNN Loss: 3.730013370513916 | BCE Loss: 1.0043889284133911\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.711190223693848 | KNN Loss: 3.672360897064209 | BCE Loss: 1.0388290882110596\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.703181266784668 | KNN Loss: 3.693345785140991 | BCE Loss: 1.0098354816436768\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.701114177703857 | KNN Loss: 3.6662659645080566 | BCE Loss: 1.0348480939865112\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.667118072509766 | KNN Loss: 3.6736438274383545 | BCE Loss: 0.9934742450714111\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.717217445373535 | KNN Loss: 3.705271005630493 | BCE Loss: 1.011946439743042\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.752941131591797 | KNN Loss: 3.7145206928253174 | BCE Loss: 1.0384202003479004\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.736259460449219 | KNN Loss: 3.703113079071045 | BCE Loss: 1.0331463813781738\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.715476989746094 | KNN Loss: 3.686537027359009 | BCE Loss: 1.0289397239685059\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.761043548583984 | KNN Loss: 3.7237277030944824 | BCE Loss: 1.037316083908081\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.743107318878174 | KNN Loss: 3.712777614593506 | BCE Loss: 1.030329704284668\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.760130405426025 | KNN Loss: 3.712188959121704 | BCE Loss: 1.0479415655136108\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.675600051879883 | KNN Loss: 3.6661787033081055 | BCE Loss: 1.0094213485717773\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.725587368011475 | KNN Loss: 3.697591543197632 | BCE Loss: 1.0279958248138428\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.717236518859863 | KNN Loss: 3.694504737854004 | BCE Loss: 1.0227315425872803\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.7477593421936035 | KNN Loss: 3.7016210556030273 | BCE Loss: 1.0461384057998657\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.734780788421631 | KNN Loss: 3.6784472465515137 | BCE Loss: 1.0563335418701172\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.693034648895264 | KNN Loss: 3.696805477142334 | BCE Loss: 0.9962290525436401\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.661382675170898 | KNN Loss: 3.6689188480377197 | BCE Loss: 0.9924637079238892\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.67603063583374 | KNN Loss: 3.6762073040008545 | BCE Loss: 0.9998231530189514\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.716431617736816 | KNN Loss: 3.7094454765319824 | BCE Loss: 1.006986379623413\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.702031135559082 | KNN Loss: 3.7027347087860107 | BCE Loss: 0.9992965459823608\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.671410083770752 | KNN Loss: 3.6670618057250977 | BCE Loss: 1.0043483972549438\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.690587043762207 | KNN Loss: 3.6572675704956055 | BCE Loss: 1.033319354057312\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.6981706619262695 | KNN Loss: 3.6750271320343018 | BCE Loss: 1.0231434106826782\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.716691017150879 | KNN Loss: 3.6996772289276123 | BCE Loss: 1.0170137882232666\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.695124626159668 | KNN Loss: 3.6823840141296387 | BCE Loss: 1.0127407312393188\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.747770309448242 | KNN Loss: 3.687394380569458 | BCE Loss: 1.060375690460205\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.702984809875488 | KNN Loss: 3.6629221439361572 | BCE Loss: 1.0400629043579102\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.713781356811523 | KNN Loss: 3.6980137825012207 | BCE Loss: 1.0157675743103027\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.674297332763672 | KNN Loss: 3.6720938682556152 | BCE Loss: 1.0022037029266357\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.772400379180908 | KNN Loss: 3.742241859436035 | BCE Loss: 1.030158519744873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.699648380279541 | KNN Loss: 3.6955785751342773 | BCE Loss: 1.0040699243545532\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.715725898742676 | KNN Loss: 3.6800684928894043 | BCE Loss: 1.035657525062561\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.69816780090332 | KNN Loss: 3.676811933517456 | BCE Loss: 1.0213558673858643\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.74888801574707 | KNN Loss: 3.6960158348083496 | BCE Loss: 1.0528719425201416\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.722936630249023 | KNN Loss: 3.6922590732574463 | BCE Loss: 1.0306777954101562\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.677726745605469 | KNN Loss: 3.685103178024292 | BCE Loss: 0.9926234483718872\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.679096221923828 | KNN Loss: 3.6714377403259277 | BCE Loss: 1.0076582431793213\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.660531044006348 | KNN Loss: 3.667128324508667 | BCE Loss: 0.9934028387069702\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.735595703125 | KNN Loss: 3.7164249420166016 | BCE Loss: 1.0191709995269775\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.771758556365967 | KNN Loss: 3.724950075149536 | BCE Loss: 1.0468086004257202\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.688318729400635 | KNN Loss: 3.678253412246704 | BCE Loss: 1.0100654363632202\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.674929618835449 | KNN Loss: 3.6657333374023438 | BCE Loss: 1.0091960430145264\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.700512886047363 | KNN Loss: 3.6860432624816895 | BCE Loss: 1.0144696235656738\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.693301200866699 | KNN Loss: 3.6803102493286133 | BCE Loss: 1.012990951538086\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.729822635650635 | KNN Loss: 3.706498146057129 | BCE Loss: 1.0233243703842163\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.727831840515137 | KNN Loss: 3.6841647624969482 | BCE Loss: 1.0436670780181885\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.71691370010376 | KNN Loss: 3.7075347900390625 | BCE Loss: 1.0093787908554077\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.674287796020508 | KNN Loss: 3.67126202583313 | BCE Loss: 1.0030256509780884\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.726034641265869 | KNN Loss: 3.7067971229553223 | BCE Loss: 1.0192375183105469\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.735520362854004 | KNN Loss: 3.68867564201355 | BCE Loss: 1.0468449592590332\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.729344367980957 | KNN Loss: 3.679893732070923 | BCE Loss: 1.0494506359100342\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.712392807006836 | KNN Loss: 3.7124879360198975 | BCE Loss: 0.9999046921730042\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.697182655334473 | KNN Loss: 3.68696665763855 | BCE Loss: 1.0102159976959229\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.761249542236328 | KNN Loss: 3.732147216796875 | BCE Loss: 1.029102087020874\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.748169898986816 | KNN Loss: 3.708542823791504 | BCE Loss: 1.0396270751953125\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.748538970947266 | KNN Loss: 3.699589252471924 | BCE Loss: 1.0489494800567627\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.740856170654297 | KNN Loss: 3.7001001834869385 | BCE Loss: 1.0407559871673584\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.6897783279418945 | KNN Loss: 3.6882026195526123 | BCE Loss: 1.0015755891799927\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.770791053771973 | KNN Loss: 3.7411646842956543 | BCE Loss: 1.0296266078948975\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.6973748207092285 | KNN Loss: 3.66204833984375 | BCE Loss: 1.0353264808654785\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.696307182312012 | KNN Loss: 3.6672651767730713 | BCE Loss: 1.0290422439575195\n",
      "Epoch   379: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.719065189361572 | KNN Loss: 3.6851682662963867 | BCE Loss: 1.0338969230651855\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.679066181182861 | KNN Loss: 3.691957712173462 | BCE Loss: 0.987108588218689\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.686882495880127 | KNN Loss: 3.6747355461120605 | BCE Loss: 1.0121469497680664\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.727210521697998 | KNN Loss: 3.7056684494018555 | BCE Loss: 1.0215420722961426\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.767455101013184 | KNN Loss: 3.7134628295898438 | BCE Loss: 1.0539923906326294\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.720428943634033 | KNN Loss: 3.684051752090454 | BCE Loss: 1.036377191543579\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.730095863342285 | KNN Loss: 3.7092573642730713 | BCE Loss: 1.020838737487793\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.69348669052124 | KNN Loss: 3.6616828441619873 | BCE Loss: 1.031803846359253\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.683645248413086 | KNN Loss: 3.6581168174743652 | BCE Loss: 1.0255286693572998\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.789236545562744 | KNN Loss: 3.7368485927581787 | BCE Loss: 1.0523879528045654\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.7314348220825195 | KNN Loss: 3.6914501190185547 | BCE Loss: 1.039984941482544\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.736907005310059 | KNN Loss: 3.707160711288452 | BCE Loss: 1.0297460556030273\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.751467704772949 | KNN Loss: 3.7143824100494385 | BCE Loss: 1.0370850563049316\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.690573215484619 | KNN Loss: 3.666504383087158 | BCE Loss: 1.024068832397461\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.73419189453125 | KNN Loss: 3.689427614212036 | BCE Loss: 1.0447642803192139\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.699017524719238 | KNN Loss: 3.7050013542175293 | BCE Loss: 0.9940162897109985\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.735653877258301 | KNN Loss: 3.6832029819488525 | BCE Loss: 1.0524506568908691\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.762197017669678 | KNN Loss: 3.721759080886841 | BCE Loss: 1.0404378175735474\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.652660846710205 | KNN Loss: 3.656876564025879 | BCE Loss: 0.9957842230796814\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.737872123718262 | KNN Loss: 3.6815240383148193 | BCE Loss: 1.0563478469848633\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.681011199951172 | KNN Loss: 3.6612837314605713 | BCE Loss: 1.0197274684906006\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.682084560394287 | KNN Loss: 3.6573047637939453 | BCE Loss: 1.0247799158096313\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.7166748046875 | KNN Loss: 3.685316324234009 | BCE Loss: 1.0313584804534912\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.756338119506836 | KNN Loss: 3.7383744716644287 | BCE Loss: 1.0179638862609863\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.679842948913574 | KNN Loss: 3.66637921333313 | BCE Loss: 1.0134639739990234\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.741225242614746 | KNN Loss: 3.6780343055725098 | BCE Loss: 1.0631911754608154\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.7358856201171875 | KNN Loss: 3.7236721515655518 | BCE Loss: 1.0122137069702148\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.735939025878906 | KNN Loss: 3.6789069175720215 | BCE Loss: 1.0570321083068848\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.719223976135254 | KNN Loss: 3.7012829780578613 | BCE Loss: 1.0179411172866821\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.710873603820801 | KNN Loss: 3.7018582820892334 | BCE Loss: 1.0090152025222778\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.733656883239746 | KNN Loss: 3.6899755001068115 | BCE Loss: 1.0436811447143555\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.718031883239746 | KNN Loss: 3.6925301551818848 | BCE Loss: 1.0255018472671509\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.69685173034668 | KNN Loss: 3.670240879058838 | BCE Loss: 1.0266109704971313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.702356338500977 | KNN Loss: 3.6861188411712646 | BCE Loss: 1.016237735748291\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.75984001159668 | KNN Loss: 3.7262327671051025 | BCE Loss: 1.0336074829101562\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.707622528076172 | KNN Loss: 3.6855697631835938 | BCE Loss: 1.0220528841018677\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.716506004333496 | KNN Loss: 3.704629421234131 | BCE Loss: 1.0118765830993652\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.690215110778809 | KNN Loss: 3.6658213138580322 | BCE Loss: 1.0243937969207764\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.731057643890381 | KNN Loss: 3.714479923248291 | BCE Loss: 1.0165777206420898\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.766258239746094 | KNN Loss: 3.6951682567596436 | BCE Loss: 1.071089744567871\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.706774711608887 | KNN Loss: 3.6710591316223145 | BCE Loss: 1.0357155799865723\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.7393012046813965 | KNN Loss: 3.6868302822113037 | BCE Loss: 1.0524709224700928\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.741816520690918 | KNN Loss: 3.740694046020508 | BCE Loss: 1.0011227130889893\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.707773208618164 | KNN Loss: 3.6956896781921387 | BCE Loss: 1.0120837688446045\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.737157821655273 | KNN Loss: 3.6957383155822754 | BCE Loss: 1.041419506072998\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.673854351043701 | KNN Loss: 3.6774933338165283 | BCE Loss: 0.9963608980178833\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.729040622711182 | KNN Loss: 3.694328784942627 | BCE Loss: 1.0347118377685547\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.701089859008789 | KNN Loss: 3.706428289413452 | BCE Loss: 0.9946615695953369\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.728177547454834 | KNN Loss: 3.7011077404022217 | BCE Loss: 1.0270699262619019\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.698206901550293 | KNN Loss: 3.677659034729004 | BCE Loss: 1.02054762840271\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.7156081199646 | KNN Loss: 3.6823947429656982 | BCE Loss: 1.0332133769989014\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.692019462585449 | KNN Loss: 3.679266929626465 | BCE Loss: 1.0127527713775635\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.663638114929199 | KNN Loss: 3.6740047931671143 | BCE Loss: 0.9896332025527954\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.814573287963867 | KNN Loss: 3.746115207672119 | BCE Loss: 1.068458080291748\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.685728549957275 | KNN Loss: 3.6753790378570557 | BCE Loss: 1.0103493928909302\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.724461555480957 | KNN Loss: 3.724994659423828 | BCE Loss: 0.9994668364524841\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.722663402557373 | KNN Loss: 3.693380832672119 | BCE Loss: 1.029282569885254\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.760907173156738 | KNN Loss: 3.741846799850464 | BCE Loss: 1.0190603733062744\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.713798522949219 | KNN Loss: 3.7001755237579346 | BCE Loss: 1.0136228799819946\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.669429302215576 | KNN Loss: 3.683976173400879 | BCE Loss: 0.9854531288146973\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.662222385406494 | KNN Loss: 3.6577985286712646 | BCE Loss: 1.0044238567352295\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.724306106567383 | KNN Loss: 3.694063425064087 | BCE Loss: 1.030242919921875\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.753969192504883 | KNN Loss: 3.731555461883545 | BCE Loss: 1.022413730621338\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.652696132659912 | KNN Loss: 3.6591885089874268 | BCE Loss: 0.9935075640678406\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.711989402770996 | KNN Loss: 3.694239377975464 | BCE Loss: 1.0177502632141113\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.693778038024902 | KNN Loss: 3.6546454429626465 | BCE Loss: 1.0391323566436768\n",
      "Epoch   390: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.703853607177734 | KNN Loss: 3.685072660446167 | BCE Loss: 1.0187807083129883\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.702342987060547 | KNN Loss: 3.660053253173828 | BCE Loss: 1.0422897338867188\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.68308162689209 | KNN Loss: 3.671015739440918 | BCE Loss: 1.0120656490325928\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.746438980102539 | KNN Loss: 3.7107651233673096 | BCE Loss: 1.035673975944519\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.739695072174072 | KNN Loss: 3.698450803756714 | BCE Loss: 1.041244387626648\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.660905361175537 | KNN Loss: 3.6578943729400635 | BCE Loss: 1.003010869026184\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.7071533203125 | KNN Loss: 3.671543836593628 | BCE Loss: 1.035609483718872\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.776427745819092 | KNN Loss: 3.693021297454834 | BCE Loss: 1.0834063291549683\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.709970474243164 | KNN Loss: 3.678317070007324 | BCE Loss: 1.031653642654419\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.656088829040527 | KNN Loss: 3.653029441833496 | BCE Loss: 1.0030593872070312\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.678463459014893 | KNN Loss: 3.6882565021514893 | BCE Loss: 0.9902071356773376\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.773722171783447 | KNN Loss: 3.746147871017456 | BCE Loss: 1.0275743007659912\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.732748031616211 | KNN Loss: 3.696155309677124 | BCE Loss: 1.0365926027297974\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.740971565246582 | KNN Loss: 3.7148473262786865 | BCE Loss: 1.026124358177185\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.732496738433838 | KNN Loss: 3.6989123821258545 | BCE Loss: 1.033584475517273\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.695075988769531 | KNN Loss: 3.6843605041503906 | BCE Loss: 1.0107157230377197\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.719311714172363 | KNN Loss: 3.6997909545898438 | BCE Loss: 1.0195205211639404\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.703721046447754 | KNN Loss: 3.669389009475708 | BCE Loss: 1.0343317985534668\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.67913818359375 | KNN Loss: 3.670818567276001 | BCE Loss: 1.00831937789917\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.717923641204834 | KNN Loss: 3.698533535003662 | BCE Loss: 1.0193901062011719\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.66739559173584 | KNN Loss: 3.65727162361145 | BCE Loss: 1.0101237297058105\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.73910665512085 | KNN Loss: 3.6868505477905273 | BCE Loss: 1.0522561073303223\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.697481632232666 | KNN Loss: 3.6763014793395996 | BCE Loss: 1.0211801528930664\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.711139678955078 | KNN Loss: 3.6917192935943604 | BCE Loss: 1.0194202661514282\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.759662628173828 | KNN Loss: 3.7374868392944336 | BCE Loss: 1.022175669670105\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.667375087738037 | KNN Loss: 3.6600568294525146 | BCE Loss: 1.0073182582855225\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.694773197174072 | KNN Loss: 3.667006254196167 | BCE Loss: 1.0277669429779053\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.704785346984863 | KNN Loss: 3.66335129737854 | BCE Loss: 1.0414340496063232\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.716794013977051 | KNN Loss: 3.6962127685546875 | BCE Loss: 1.0205812454223633\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.707400321960449 | KNN Loss: 3.6814324855804443 | BCE Loss: 1.025968074798584\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.696164131164551 | KNN Loss: 3.6770761013031006 | BCE Loss: 1.0190879106521606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.721700668334961 | KNN Loss: 3.716920852661133 | BCE Loss: 1.0047798156738281\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.70701789855957 | KNN Loss: 3.6546289920806885 | BCE Loss: 1.0523889064788818\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.69720458984375 | KNN Loss: 3.6814184188842773 | BCE Loss: 1.015786051750183\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.724734783172607 | KNN Loss: 3.6906940937042236 | BCE Loss: 1.0340406894683838\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.734705924987793 | KNN Loss: 3.730337381362915 | BCE Loss: 1.0043686628341675\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.702478408813477 | KNN Loss: 3.705914258956909 | BCE Loss: 0.9965639114379883\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.707912921905518 | KNN Loss: 3.689957857131958 | BCE Loss: 1.0179550647735596\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.721449851989746 | KNN Loss: 3.7163314819335938 | BCE Loss: 1.0051183700561523\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.718476295471191 | KNN Loss: 3.684473991394043 | BCE Loss: 1.0340025424957275\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.753532409667969 | KNN Loss: 3.756343126296997 | BCE Loss: 0.9971895217895508\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.744144916534424 | KNN Loss: 3.6934685707092285 | BCE Loss: 1.0506763458251953\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.705616474151611 | KNN Loss: 3.6879231929779053 | BCE Loss: 1.017693281173706\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.705122947692871 | KNN Loss: 3.699751615524292 | BCE Loss: 1.00537109375\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.7036590576171875 | KNN Loss: 3.6703484058380127 | BCE Loss: 1.0333104133605957\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.736832618713379 | KNN Loss: 3.704683780670166 | BCE Loss: 1.0321485996246338\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.721726417541504 | KNN Loss: 3.687627077102661 | BCE Loss: 1.0340991020202637\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.710836410522461 | KNN Loss: 3.67445969581604 | BCE Loss: 1.0363764762878418\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.696389675140381 | KNN Loss: 3.6896586418151855 | BCE Loss: 1.0067309141159058\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.75892972946167 | KNN Loss: 3.714170217514038 | BCE Loss: 1.0447595119476318\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.703639507293701 | KNN Loss: 3.663346767425537 | BCE Loss: 1.0402928590774536\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.7485504150390625 | KNN Loss: 3.718071699142456 | BCE Loss: 1.0304789543151855\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.751900672912598 | KNN Loss: 3.716510534286499 | BCE Loss: 1.0353901386260986\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.700955390930176 | KNN Loss: 3.6872198581695557 | BCE Loss: 1.0137355327606201\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.718046188354492 | KNN Loss: 3.675954818725586 | BCE Loss: 1.0420911312103271\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.7153215408325195 | KNN Loss: 3.6810967922210693 | BCE Loss: 1.034224510192871\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.685177326202393 | KNN Loss: 3.6912882328033447 | BCE Loss: 0.9938890933990479\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.730343818664551 | KNN Loss: 3.696833372116089 | BCE Loss: 1.0335103273391724\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.707605838775635 | KNN Loss: 3.680534839630127 | BCE Loss: 1.0270709991455078\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.743073463439941 | KNN Loss: 3.704087734222412 | BCE Loss: 1.0389857292175293\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.709510803222656 | KNN Loss: 3.6910030841827393 | BCE Loss: 1.018507957458496\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.693323135375977 | KNN Loss: 3.674156904220581 | BCE Loss: 1.019166350364685\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.730001449584961 | KNN Loss: 3.7092409133911133 | BCE Loss: 1.0207605361938477\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.680393695831299 | KNN Loss: 3.6817378997802734 | BCE Loss: 0.9986559152603149\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.742417335510254 | KNN Loss: 3.734236478805542 | BCE Loss: 1.008180856704712\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.72725772857666 | KNN Loss: 3.6926536560058594 | BCE Loss: 1.0346038341522217\n",
      "Epoch   401: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.794029235839844 | KNN Loss: 3.76444935798645 | BCE Loss: 1.0295801162719727\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.682276725769043 | KNN Loss: 3.663689374923706 | BCE Loss: 1.018587589263916\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.7187299728393555 | KNN Loss: 3.696810245513916 | BCE Loss: 1.021919846534729\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.675919532775879 | KNN Loss: 3.6502795219421387 | BCE Loss: 1.0256400108337402\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.697915077209473 | KNN Loss: 3.692622423171997 | BCE Loss: 1.0052926540374756\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.721662521362305 | KNN Loss: 3.6879568099975586 | BCE Loss: 1.0337058305740356\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.651132106781006 | KNN Loss: 3.6543259620666504 | BCE Loss: 0.9968062043190002\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.6707892417907715 | KNN Loss: 3.6680612564086914 | BCE Loss: 1.00272798538208\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.663288593292236 | KNN Loss: 3.6823065280914307 | BCE Loss: 0.98098224401474\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.718310356140137 | KNN Loss: 3.6704111099243164 | BCE Loss: 1.0478992462158203\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.699983596801758 | KNN Loss: 3.6676526069641113 | BCE Loss: 1.0323309898376465\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.73762845993042 | KNN Loss: 3.7211852073669434 | BCE Loss: 1.0164432525634766\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.73377799987793 | KNN Loss: 3.6734704971313477 | BCE Loss: 1.060307264328003\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.701751708984375 | KNN Loss: 3.6721749305725098 | BCE Loss: 1.0295765399932861\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.706894397735596 | KNN Loss: 3.70090651512146 | BCE Loss: 1.0059877634048462\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.746692657470703 | KNN Loss: 3.705758571624756 | BCE Loss: 1.0409338474273682\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.704568386077881 | KNN Loss: 3.7045881748199463 | BCE Loss: 0.9999803304672241\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.730748176574707 | KNN Loss: 3.678150177001953 | BCE Loss: 1.052598237991333\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.732090473175049 | KNN Loss: 3.7084810733795166 | BCE Loss: 1.0236093997955322\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.737322807312012 | KNN Loss: 3.704463005065918 | BCE Loss: 1.0328600406646729\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.729315280914307 | KNN Loss: 3.719560146331787 | BCE Loss: 1.0097551345825195\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.745481491088867 | KNN Loss: 3.6965553760528564 | BCE Loss: 1.0489259958267212\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.761735439300537 | KNN Loss: 3.7381906509399414 | BCE Loss: 1.0235447883605957\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.75043249130249 | KNN Loss: 3.722764492034912 | BCE Loss: 1.0276679992675781\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.7076005935668945 | KNN Loss: 3.7120096683502197 | BCE Loss: 0.9955906867980957\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.7082839012146 | KNN Loss: 3.674624443054199 | BCE Loss: 1.0336594581604004\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.731496810913086 | KNN Loss: 3.7141754627227783 | BCE Loss: 1.0173211097717285\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.661823272705078 | KNN Loss: 3.6523494720458984 | BCE Loss: 1.0094736814498901\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.7184295654296875 | KNN Loss: 3.687138557434082 | BCE Loss: 1.0312907695770264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.686516284942627 | KNN Loss: 3.6792151927948 | BCE Loss: 1.0073010921478271\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.683380126953125 | KNN Loss: 3.692497491836548 | BCE Loss: 0.9908825755119324\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.734013557434082 | KNN Loss: 3.7130496501922607 | BCE Loss: 1.0209639072418213\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.785103797912598 | KNN Loss: 3.7416744232177734 | BCE Loss: 1.0434296131134033\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.704380989074707 | KNN Loss: 3.689220666885376 | BCE Loss: 1.0151602029800415\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.729823589324951 | KNN Loss: 3.6996164321899414 | BCE Loss: 1.0302071571350098\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.686915397644043 | KNN Loss: 3.6740875244140625 | BCE Loss: 1.0128276348114014\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.714256286621094 | KNN Loss: 3.6863956451416016 | BCE Loss: 1.0278607606887817\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.6669745445251465 | KNN Loss: 3.686805009841919 | BCE Loss: 0.9801696538925171\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.745980262756348 | KNN Loss: 3.702662467956543 | BCE Loss: 1.0433177947998047\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.698260307312012 | KNN Loss: 3.6699535846710205 | BCE Loss: 1.028306484222412\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.713669300079346 | KNN Loss: 3.707906723022461 | BCE Loss: 1.0057624578475952\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.767752647399902 | KNN Loss: 3.7193830013275146 | BCE Loss: 1.0483698844909668\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.699557304382324 | KNN Loss: 3.668891429901123 | BCE Loss: 1.0306659936904907\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.7023420333862305 | KNN Loss: 3.6710474491119385 | BCE Loss: 1.031294584274292\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.737759590148926 | KNN Loss: 3.703763246536255 | BCE Loss: 1.03399658203125\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.7560553550720215 | KNN Loss: 3.717474937438965 | BCE Loss: 1.0385804176330566\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.6990814208984375 | KNN Loss: 3.6756210327148438 | BCE Loss: 1.0234601497650146\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.728227615356445 | KNN Loss: 3.699547290802002 | BCE Loss: 1.028680443763733\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.7149553298950195 | KNN Loss: 3.6889567375183105 | BCE Loss: 1.025998592376709\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.712644577026367 | KNN Loss: 3.7075562477111816 | BCE Loss: 1.0050880908966064\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.694875240325928 | KNN Loss: 3.687269926071167 | BCE Loss: 1.0076053142547607\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.721557140350342 | KNN Loss: 3.703727960586548 | BCE Loss: 1.017829179763794\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.679976463317871 | KNN Loss: 3.672184467315674 | BCE Loss: 1.0077922344207764\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.718929290771484 | KNN Loss: 3.6821699142456055 | BCE Loss: 1.036759376525879\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.725885391235352 | KNN Loss: 3.677455186843872 | BCE Loss: 1.0484302043914795\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.670426368713379 | KNN Loss: 3.6603128910064697 | BCE Loss: 1.01011323928833\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.739167213439941 | KNN Loss: 3.6894073486328125 | BCE Loss: 1.0497597455978394\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.71196985244751 | KNN Loss: 3.6917335987091064 | BCE Loss: 1.0202363729476929\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.734311580657959 | KNN Loss: 3.704204559326172 | BCE Loss: 1.0301071405410767\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.678285598754883 | KNN Loss: 3.647880792617798 | BCE Loss: 1.030404806137085\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.70206356048584 | KNN Loss: 3.6912100315093994 | BCE Loss: 1.0108532905578613\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.669291973114014 | KNN Loss: 3.6573636531829834 | BCE Loss: 1.0119283199310303\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.689650535583496 | KNN Loss: 3.6678147315979004 | BCE Loss: 1.0218358039855957\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.714939594268799 | KNN Loss: 3.6773617267608643 | BCE Loss: 1.0375778675079346\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.711969375610352 | KNN Loss: 3.6987388134002686 | BCE Loss: 1.013230800628662\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.694838523864746 | KNN Loss: 3.6756246089935303 | BCE Loss: 1.0192140340805054\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.70552921295166 | KNN Loss: 3.6915528774261475 | BCE Loss: 1.0139763355255127\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.703939914703369 | KNN Loss: 3.6701338291168213 | BCE Loss: 1.0338060855865479\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.750099182128906 | KNN Loss: 3.7258529663085938 | BCE Loss: 1.0242462158203125\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.772727966308594 | KNN Loss: 3.7426936626434326 | BCE Loss: 1.030034065246582\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.739273548126221 | KNN Loss: 3.6937201023101807 | BCE Loss: 1.04555344581604\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.722838401794434 | KNN Loss: 3.6824488639831543 | BCE Loss: 1.0403892993927002\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.716050148010254 | KNN Loss: 3.676990032196045 | BCE Loss: 1.0390599966049194\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.729462623596191 | KNN Loss: 3.7053301334381104 | BCE Loss: 1.0241327285766602\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.680127143859863 | KNN Loss: 3.689814567565918 | BCE Loss: 0.9903124570846558\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.721024990081787 | KNN Loss: 3.685638189315796 | BCE Loss: 1.0353866815567017\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.693360328674316 | KNN Loss: 3.6837892532348633 | BCE Loss: 1.0095713138580322\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.717101097106934 | KNN Loss: 3.676422119140625 | BCE Loss: 1.0406792163848877\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.671780109405518 | KNN Loss: 3.6584208011627197 | BCE Loss: 1.0133593082427979\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.732588768005371 | KNN Loss: 3.6913561820983887 | BCE Loss: 1.0412323474884033\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.739301681518555 | KNN Loss: 3.7272253036499023 | BCE Loss: 1.012076497077942\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.678550720214844 | KNN Loss: 3.6650006771087646 | BCE Loss: 1.0135501623153687\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.723241806030273 | KNN Loss: 3.6863062381744385 | BCE Loss: 1.036935806274414\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.737421035766602 | KNN Loss: 3.708475112915039 | BCE Loss: 1.0289461612701416\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.740853309631348 | KNN Loss: 3.6935412883758545 | BCE Loss: 1.0473120212554932\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.716424465179443 | KNN Loss: 3.683933973312378 | BCE Loss: 1.0324904918670654\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.727494239807129 | KNN Loss: 3.706512928009033 | BCE Loss: 1.0209813117980957\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.749598503112793 | KNN Loss: 3.736940383911133 | BCE Loss: 1.0126583576202393\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.719308376312256 | KNN Loss: 3.7006661891937256 | BCE Loss: 1.0186421871185303\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.667813301086426 | KNN Loss: 3.6635894775390625 | BCE Loss: 1.0042238235473633\n",
      "Epoch   416: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.68360710144043 | KNN Loss: 3.648860454559326 | BCE Loss: 1.0347466468811035\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.73617696762085 | KNN Loss: 3.7111358642578125 | BCE Loss: 1.025041103363037\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.729434967041016 | KNN Loss: 3.7200348377227783 | BCE Loss: 1.0094001293182373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.71793794631958 | KNN Loss: 3.7189621925354004 | BCE Loss: 0.9989758729934692\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.751008987426758 | KNN Loss: 3.655109167098999 | BCE Loss: 1.0958995819091797\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.746922492980957 | KNN Loss: 3.7293834686279297 | BCE Loss: 1.0175390243530273\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.695389270782471 | KNN Loss: 3.6814396381378174 | BCE Loss: 1.0139496326446533\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.708314895629883 | KNN Loss: 3.6958742141723633 | BCE Loss: 1.0124404430389404\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.690959453582764 | KNN Loss: 3.699187755584717 | BCE Loss: 0.9917716383934021\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.705693244934082 | KNN Loss: 3.6715152263641357 | BCE Loss: 1.0341777801513672\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.6919355392456055 | KNN Loss: 3.6430368423461914 | BCE Loss: 1.048898696899414\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.715746879577637 | KNN Loss: 3.687598943710327 | BCE Loss: 1.0281479358673096\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.684370517730713 | KNN Loss: 3.6474061012268066 | BCE Loss: 1.0369644165039062\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.745405197143555 | KNN Loss: 3.7050459384918213 | BCE Loss: 1.0403594970703125\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.701350688934326 | KNN Loss: 3.6943418979644775 | BCE Loss: 1.0070087909698486\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.730926513671875 | KNN Loss: 3.703364849090576 | BCE Loss: 1.0275616645812988\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.723761558532715 | KNN Loss: 3.7046852111816406 | BCE Loss: 1.0190765857696533\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.727015495300293 | KNN Loss: 3.690502882003784 | BCE Loss: 1.0365127325057983\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.676334381103516 | KNN Loss: 3.674093723297119 | BCE Loss: 1.0022408962249756\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.7632904052734375 | KNN Loss: 3.699697256088257 | BCE Loss: 1.0635933876037598\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.729981422424316 | KNN Loss: 3.7179229259490967 | BCE Loss: 1.0120583772659302\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.725700855255127 | KNN Loss: 3.7122104167938232 | BCE Loss: 1.0134904384613037\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.749030113220215 | KNN Loss: 3.71358060836792 | BCE Loss: 1.0354492664337158\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.759289741516113 | KNN Loss: 3.7161953449249268 | BCE Loss: 1.0430941581726074\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.704535961151123 | KNN Loss: 3.666966199874878 | BCE Loss: 1.0375697612762451\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.715005874633789 | KNN Loss: 3.6817233562469482 | BCE Loss: 1.0332825183868408\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.681326866149902 | KNN Loss: 3.6662890911102295 | BCE Loss: 1.015038013458252\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.686285972595215 | KNN Loss: 3.665320634841919 | BCE Loss: 1.0209654569625854\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.786787509918213 | KNN Loss: 3.743049383163452 | BCE Loss: 1.0437380075454712\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.706173896789551 | KNN Loss: 3.6833765506744385 | BCE Loss: 1.0227975845336914\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.712334156036377 | KNN Loss: 3.681445837020874 | BCE Loss: 1.030888319015503\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.702249050140381 | KNN Loss: 3.6889708042144775 | BCE Loss: 1.0132782459259033\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.715834617614746 | KNN Loss: 3.705410957336426 | BCE Loss: 1.0104236602783203\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.7579803466796875 | KNN Loss: 3.7336246967315674 | BCE Loss: 1.0243558883666992\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.6999711990356445 | KNN Loss: 3.678318977355957 | BCE Loss: 1.0216519832611084\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.683371543884277 | KNN Loss: 3.6763553619384766 | BCE Loss: 1.0070161819458008\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.691506385803223 | KNN Loss: 3.674062967300415 | BCE Loss: 1.0174431800842285\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.713280200958252 | KNN Loss: 3.6954479217529297 | BCE Loss: 1.0178323984146118\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.696369647979736 | KNN Loss: 3.6812264919281006 | BCE Loss: 1.0151432752609253\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.690210342407227 | KNN Loss: 3.6803441047668457 | BCE Loss: 1.0098662376403809\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.684479236602783 | KNN Loss: 3.6789357662200928 | BCE Loss: 1.00554358959198\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.679235935211182 | KNN Loss: 3.6675825119018555 | BCE Loss: 1.0116534233093262\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.684099197387695 | KNN Loss: 3.679936647415161 | BCE Loss: 1.004162311553955\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.710461616516113 | KNN Loss: 3.678727865219116 | BCE Loss: 1.031733512878418\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.71251916885376 | KNN Loss: 3.6946561336517334 | BCE Loss: 1.0178629159927368\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.697167873382568 | KNN Loss: 3.6721014976501465 | BCE Loss: 1.0250663757324219\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.7254414558410645 | KNN Loss: 3.696927070617676 | BCE Loss: 1.0285143852233887\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.743006706237793 | KNN Loss: 3.693263530731201 | BCE Loss: 1.0497429370880127\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.730083465576172 | KNN Loss: 3.6958248615264893 | BCE Loss: 1.0342587232589722\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.731782913208008 | KNN Loss: 3.682847023010254 | BCE Loss: 1.0489356517791748\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.682835578918457 | KNN Loss: 3.6589560508728027 | BCE Loss: 1.0238795280456543\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.745125770568848 | KNN Loss: 3.7330870628356934 | BCE Loss: 1.0120389461517334\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.752036094665527 | KNN Loss: 3.7127153873443604 | BCE Loss: 1.039320468902588\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.737302303314209 | KNN Loss: 3.690876007080078 | BCE Loss: 1.0464262962341309\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.679964542388916 | KNN Loss: 3.6933741569519043 | BCE Loss: 0.9865905046463013\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.66157865524292 | KNN Loss: 3.670419216156006 | BCE Loss: 0.9911593198776245\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.734368324279785 | KNN Loss: 3.7232086658477783 | BCE Loss: 1.0111597776412964\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.694019317626953 | KNN Loss: 3.6679561138153076 | BCE Loss: 1.026063084602356\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.70416259765625 | KNN Loss: 3.669743061065674 | BCE Loss: 1.0344194173812866\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.687738418579102 | KNN Loss: 3.676757574081421 | BCE Loss: 1.0109808444976807\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.753370761871338 | KNN Loss: 3.7341508865356445 | BCE Loss: 1.0192198753356934\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.720043182373047 | KNN Loss: 3.6629528999328613 | BCE Loss: 1.0570905208587646\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.759515762329102 | KNN Loss: 3.7414329051971436 | BCE Loss: 1.018082618713379\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.7419843673706055 | KNN Loss: 3.6786561012268066 | BCE Loss: 1.0633282661437988\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.722756385803223 | KNN Loss: 3.682194471359253 | BCE Loss: 1.0405616760253906\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.679528713226318 | KNN Loss: 3.6717753410339355 | BCE Loss: 1.0077534914016724\n",
      "Epoch   427: reducing learning rate of group 0 to 1.3684e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.716227054595947 | KNN Loss: 3.7163543701171875 | BCE Loss: 0.9998728632926941\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.697392463684082 | KNN Loss: 3.6813416481018066 | BCE Loss: 1.0160508155822754\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.676826000213623 | KNN Loss: 3.6649653911590576 | BCE Loss: 1.0118606090545654\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.725605010986328 | KNN Loss: 3.6983401775360107 | BCE Loss: 1.0272650718688965\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.711603164672852 | KNN Loss: 3.688124895095825 | BCE Loss: 1.0234782695770264\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.744886875152588 | KNN Loss: 3.700871706008911 | BCE Loss: 1.0440152883529663\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.73754358291626 | KNN Loss: 3.698155403137207 | BCE Loss: 1.0393882989883423\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.7168989181518555 | KNN Loss: 3.693422317504883 | BCE Loss: 1.023476481437683\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.65634822845459 | KNN Loss: 3.646549701690674 | BCE Loss: 1.0097987651824951\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.720720291137695 | KNN Loss: 3.7195630073547363 | BCE Loss: 1.0011570453643799\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.729194641113281 | KNN Loss: 3.7047204971313477 | BCE Loss: 1.0244739055633545\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.685300827026367 | KNN Loss: 3.6689088344573975 | BCE Loss: 1.0163918733596802\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.707027435302734 | KNN Loss: 3.66861891746521 | BCE Loss: 1.0384085178375244\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.750656604766846 | KNN Loss: 3.7128353118896484 | BCE Loss: 1.0378212928771973\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.754402160644531 | KNN Loss: 3.6962952613830566 | BCE Loss: 1.0581070184707642\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.687385082244873 | KNN Loss: 3.663189172744751 | BCE Loss: 1.024195909500122\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.683928966522217 | KNN Loss: 3.6665632724761963 | BCE Loss: 1.01736581325531\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.796041011810303 | KNN Loss: 3.748234272003174 | BCE Loss: 1.0478066205978394\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.724590301513672 | KNN Loss: 3.6837871074676514 | BCE Loss: 1.0408029556274414\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.6895341873168945 | KNN Loss: 3.662606954574585 | BCE Loss: 1.0269274711608887\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.702910900115967 | KNN Loss: 3.6970784664154053 | BCE Loss: 1.005832314491272\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.684391021728516 | KNN Loss: 3.6619505882263184 | BCE Loss: 1.0224403142929077\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.713776588439941 | KNN Loss: 3.7076430320739746 | BCE Loss: 1.006133794784546\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.7321062088012695 | KNN Loss: 3.6929121017456055 | BCE Loss: 1.0391939878463745\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.6912384033203125 | KNN Loss: 3.695915460586548 | BCE Loss: 0.9953227043151855\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.784750938415527 | KNN Loss: 3.728494167327881 | BCE Loss: 1.0562567710876465\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.723654270172119 | KNN Loss: 3.7044005393981934 | BCE Loss: 1.0192537307739258\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.703582763671875 | KNN Loss: 3.699401617050171 | BCE Loss: 1.0041812658309937\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.698859214782715 | KNN Loss: 3.690809488296509 | BCE Loss: 1.008049726486206\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.688058853149414 | KNN Loss: 3.6618685722351074 | BCE Loss: 1.0261902809143066\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.7334208488464355 | KNN Loss: 3.7078046798706055 | BCE Loss: 1.02561616897583\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.667141437530518 | KNN Loss: 3.6653640270233154 | BCE Loss: 1.0017774105072021\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.675764083862305 | KNN Loss: 3.680543899536133 | BCE Loss: 0.9952203035354614\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.742839813232422 | KNN Loss: 3.6974141597747803 | BCE Loss: 1.0454254150390625\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.744941711425781 | KNN Loss: 3.687185287475586 | BCE Loss: 1.0577566623687744\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.705069541931152 | KNN Loss: 3.6860291957855225 | BCE Loss: 1.0190401077270508\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.724643230438232 | KNN Loss: 3.6722893714904785 | BCE Loss: 1.0523537397384644\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.707669258117676 | KNN Loss: 3.712594985961914 | BCE Loss: 0.9950742721557617\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.730917930603027 | KNN Loss: 3.6894783973693848 | BCE Loss: 1.0414397716522217\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.766077995300293 | KNN Loss: 3.738043785095215 | BCE Loss: 1.0280344486236572\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.726192951202393 | KNN Loss: 3.697728395462036 | BCE Loss: 1.0284645557403564\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.725007057189941 | KNN Loss: 3.7024519443511963 | BCE Loss: 1.022554874420166\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.651031017303467 | KNN Loss: 3.6556098461151123 | BCE Loss: 0.9954209923744202\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.700881481170654 | KNN Loss: 3.6808743476867676 | BCE Loss: 1.0200072526931763\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.722756862640381 | KNN Loss: 3.715085983276367 | BCE Loss: 1.0076709985733032\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.737990856170654 | KNN Loss: 3.7322885990142822 | BCE Loss: 1.0057021379470825\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.713438987731934 | KNN Loss: 3.7046632766723633 | BCE Loss: 1.0087759494781494\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.696378231048584 | KNN Loss: 3.6714839935302734 | BCE Loss: 1.0248942375183105\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.668189525604248 | KNN Loss: 3.6585092544555664 | BCE Loss: 1.0096802711486816\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.7511305809021 | KNN Loss: 3.717367649078369 | BCE Loss: 1.0337629318237305\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.701882362365723 | KNN Loss: 3.6902360916137695 | BCE Loss: 1.0116462707519531\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.687581539154053 | KNN Loss: 3.670020341873169 | BCE Loss: 1.0175611972808838\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.710898399353027 | KNN Loss: 3.6962037086486816 | BCE Loss: 1.0146946907043457\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.716155052185059 | KNN Loss: 3.6529834270477295 | BCE Loss: 1.063171625137329\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.744002342224121 | KNN Loss: 3.693845510482788 | BCE Loss: 1.050157070159912\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.746376991271973 | KNN Loss: 3.706878185272217 | BCE Loss: 1.039499044418335\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.744926929473877 | KNN Loss: 3.714637279510498 | BCE Loss: 1.0302895307540894\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.708669662475586 | KNN Loss: 3.687976837158203 | BCE Loss: 1.0206929445266724\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.7263383865356445 | KNN Loss: 3.694679021835327 | BCE Loss: 1.0316591262817383\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.668767929077148 | KNN Loss: 3.669184923171997 | BCE Loss: 0.9995827674865723\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.723215103149414 | KNN Loss: 3.681128740310669 | BCE Loss: 1.042086124420166\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.697582244873047 | KNN Loss: 3.6858279705047607 | BCE Loss: 1.0117541551589966\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.7706298828125 | KNN Loss: 3.6983585357666016 | BCE Loss: 1.0722713470458984\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.696263313293457 | KNN Loss: 3.6881051063537598 | BCE Loss: 1.0081582069396973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.724127769470215 | KNN Loss: 3.6652274131774902 | BCE Loss: 1.0589005947113037\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.693363189697266 | KNN Loss: 3.6762561798095703 | BCE Loss: 1.0171067714691162\n",
      "Epoch   438: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.667969226837158 | KNN Loss: 3.668151378631592 | BCE Loss: 0.9998180270195007\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.674234390258789 | KNN Loss: 3.6698861122131348 | BCE Loss: 1.0043482780456543\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.716817378997803 | KNN Loss: 3.725106716156006 | BCE Loss: 0.9917107224464417\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.7270402908325195 | KNN Loss: 3.684619665145874 | BCE Loss: 1.0424203872680664\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.73661994934082 | KNN Loss: 3.7208058834075928 | BCE Loss: 1.0158143043518066\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.702218055725098 | KNN Loss: 3.673799753189087 | BCE Loss: 1.0284185409545898\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.6892499923706055 | KNN Loss: 3.65732741355896 | BCE Loss: 1.0319228172302246\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.682374477386475 | KNN Loss: 3.6705055236816406 | BCE Loss: 1.0118690729141235\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.742978096008301 | KNN Loss: 3.729546308517456 | BCE Loss: 1.0134317874908447\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.691901683807373 | KNN Loss: 3.6689579486846924 | BCE Loss: 1.0229437351226807\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.682804107666016 | KNN Loss: 3.6892054080963135 | BCE Loss: 0.9935986995697021\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.72003173828125 | KNN Loss: 3.6884872913360596 | BCE Loss: 1.0315444469451904\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.750555038452148 | KNN Loss: 3.703547239303589 | BCE Loss: 1.0470080375671387\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.759341239929199 | KNN Loss: 3.7178831100463867 | BCE Loss: 1.041458249092102\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.728535175323486 | KNN Loss: 3.696809768676758 | BCE Loss: 1.031725525856018\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.708263397216797 | KNN Loss: 3.690091133117676 | BCE Loss: 1.0181725025177002\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.710775375366211 | KNN Loss: 3.7021255493164062 | BCE Loss: 1.0086498260498047\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.692661285400391 | KNN Loss: 3.6922929286956787 | BCE Loss: 1.0003681182861328\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.754866600036621 | KNN Loss: 3.706125497817993 | BCE Loss: 1.048741340637207\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.729101181030273 | KNN Loss: 3.6955952644348145 | BCE Loss: 1.033505916595459\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.686742305755615 | KNN Loss: 3.6795272827148438 | BCE Loss: 1.0072150230407715\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.7202959060668945 | KNN Loss: 3.673129081726074 | BCE Loss: 1.0471665859222412\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.758888244628906 | KNN Loss: 3.729365348815918 | BCE Loss: 1.0295226573944092\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.736046314239502 | KNN Loss: 3.717514991760254 | BCE Loss: 1.018531322479248\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.733929634094238 | KNN Loss: 3.7040154933929443 | BCE Loss: 1.0299139022827148\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.744919776916504 | KNN Loss: 3.72279953956604 | BCE Loss: 1.0221202373504639\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.77129602432251 | KNN Loss: 3.737152576446533 | BCE Loss: 1.0341434478759766\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.704317569732666 | KNN Loss: 3.717325448989868 | BCE Loss: 0.9869920015335083\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.7625885009765625 | KNN Loss: 3.7258975505828857 | BCE Loss: 1.0366909503936768\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.696383476257324 | KNN Loss: 3.7158384323120117 | BCE Loss: 0.980545163154602\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.705005645751953 | KNN Loss: 3.6860742568969727 | BCE Loss: 1.0189311504364014\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.720541954040527 | KNN Loss: 3.676982879638672 | BCE Loss: 1.043558955192566\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.734696865081787 | KNN Loss: 3.701028823852539 | BCE Loss: 1.0336679220199585\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.699627876281738 | KNN Loss: 3.670022487640381 | BCE Loss: 1.0296053886413574\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.7636518478393555 | KNN Loss: 3.7290139198303223 | BCE Loss: 1.0346381664276123\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.677318096160889 | KNN Loss: 3.6770224571228027 | BCE Loss: 1.000295639038086\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.704636096954346 | KNN Loss: 3.6815245151519775 | BCE Loss: 1.0231115818023682\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.739439487457275 | KNN Loss: 3.7046005725860596 | BCE Loss: 1.0348389148712158\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.733245849609375 | KNN Loss: 3.6880292892456055 | BCE Loss: 1.0452165603637695\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.743319034576416 | KNN Loss: 3.716510057449341 | BCE Loss: 1.0268089771270752\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.731230735778809 | KNN Loss: 3.702099084854126 | BCE Loss: 1.0291314125061035\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.706478118896484 | KNN Loss: 3.6930341720581055 | BCE Loss: 1.013443946838379\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.754024982452393 | KNN Loss: 3.7104299068450928 | BCE Loss: 1.0435950756072998\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.672157287597656 | KNN Loss: 3.65310001373291 | BCE Loss: 1.019057035446167\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.736725807189941 | KNN Loss: 3.6851866245269775 | BCE Loss: 1.0515389442443848\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.741628170013428 | KNN Loss: 3.7385129928588867 | BCE Loss: 1.003115177154541\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.691807270050049 | KNN Loss: 3.6617562770843506 | BCE Loss: 1.0300509929656982\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.711238861083984 | KNN Loss: 3.67630672454834 | BCE Loss: 1.0349323749542236\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.716618061065674 | KNN Loss: 3.676429033279419 | BCE Loss: 1.0401890277862549\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.703342437744141 | KNN Loss: 3.687875270843506 | BCE Loss: 1.0154671669006348\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.7113237380981445 | KNN Loss: 3.6864538192749023 | BCE Loss: 1.0248697996139526\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.745316982269287 | KNN Loss: 3.7130613327026367 | BCE Loss: 1.0322556495666504\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.706084728240967 | KNN Loss: 3.7086403369903564 | BCE Loss: 0.9974443912506104\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.709087371826172 | KNN Loss: 3.674783706665039 | BCE Loss: 1.0343037843704224\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.679275035858154 | KNN Loss: 3.684985399246216 | BCE Loss: 0.9942896366119385\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.7454400062561035 | KNN Loss: 3.675724983215332 | BCE Loss: 1.0697150230407715\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.759588241577148 | KNN Loss: 3.7199926376342773 | BCE Loss: 1.0395958423614502\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.759849548339844 | KNN Loss: 3.707979202270508 | BCE Loss: 1.051870346069336\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.734622001647949 | KNN Loss: 3.699154853820801 | BCE Loss: 1.0354673862457275\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.706712245941162 | KNN Loss: 3.6993203163146973 | BCE Loss: 1.0073919296264648\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.686791896820068 | KNN Loss: 3.6899263858795166 | BCE Loss: 0.9968656301498413\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.716979026794434 | KNN Loss: 3.7064614295959473 | BCE Loss: 1.0105177164077759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.748598098754883 | KNN Loss: 3.703897476196289 | BCE Loss: 1.0447006225585938\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.709958553314209 | KNN Loss: 3.684103488922119 | BCE Loss: 1.0258550643920898\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.682947158813477 | KNN Loss: 3.665107488632202 | BCE Loss: 1.0178394317626953\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.696419715881348 | KNN Loss: 3.6882333755493164 | BCE Loss: 1.0081863403320312\n",
      "Epoch   449: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.688918113708496 | KNN Loss: 3.6989476680755615 | BCE Loss: 0.9899702668190002\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.684876918792725 | KNN Loss: 3.6805100440979004 | BCE Loss: 1.0043667554855347\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.676281929016113 | KNN Loss: 3.67991304397583 | BCE Loss: 0.9963686466217041\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.760768413543701 | KNN Loss: 3.717627763748169 | BCE Loss: 1.0431406497955322\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.729166507720947 | KNN Loss: 3.7004733085632324 | BCE Loss: 1.0286933183670044\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.744453430175781 | KNN Loss: 3.702449321746826 | BCE Loss: 1.0420043468475342\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.684764862060547 | KNN Loss: 3.6676294803619385 | BCE Loss: 1.0171353816986084\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.735017776489258 | KNN Loss: 3.6990694999694824 | BCE Loss: 1.0359485149383545\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.708491325378418 | KNN Loss: 3.678419589996338 | BCE Loss: 1.0300719738006592\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.701456069946289 | KNN Loss: 3.675380229949951 | BCE Loss: 1.026076078414917\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.695599555969238 | KNN Loss: 3.686565399169922 | BCE Loss: 1.0090343952178955\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.674060344696045 | KNN Loss: 3.6779870986938477 | BCE Loss: 0.9960733652114868\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.708250522613525 | KNN Loss: 3.670229911804199 | BCE Loss: 1.0380206108093262\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.690333366394043 | KNN Loss: 3.678231954574585 | BCE Loss: 1.012101173400879\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.713207244873047 | KNN Loss: 3.6936094760894775 | BCE Loss: 1.0195977687835693\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.710036277770996 | KNN Loss: 3.674384355545044 | BCE Loss: 1.035651683807373\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.736812114715576 | KNN Loss: 3.7075393199920654 | BCE Loss: 1.0292729139328003\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.761772155761719 | KNN Loss: 3.716503620147705 | BCE Loss: 1.0452682971954346\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.703472137451172 | KNN Loss: 3.6687119007110596 | BCE Loss: 1.0347604751586914\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.743199825286865 | KNN Loss: 3.708662986755371 | BCE Loss: 1.0345368385314941\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.644689559936523 | KNN Loss: 3.6417157649993896 | BCE Loss: 1.002974033355713\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.767149925231934 | KNN Loss: 3.7200722694396973 | BCE Loss: 1.0470776557922363\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.67357063293457 | KNN Loss: 3.6658525466918945 | BCE Loss: 1.0077178478240967\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.730306625366211 | KNN Loss: 3.6854708194732666 | BCE Loss: 1.0448358058929443\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.734852313995361 | KNN Loss: 3.704895257949829 | BCE Loss: 1.0299570560455322\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.6717305183410645 | KNN Loss: 3.674654483795166 | BCE Loss: 0.9970762133598328\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.754077911376953 | KNN Loss: 3.7193727493286133 | BCE Loss: 1.0347049236297607\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.66779899597168 | KNN Loss: 3.6547186374664307 | BCE Loss: 1.0130802392959595\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.737939834594727 | KNN Loss: 3.703813076019287 | BCE Loss: 1.034126877784729\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.702656269073486 | KNN Loss: 3.6615681648254395 | BCE Loss: 1.0410879850387573\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.753654956817627 | KNN Loss: 3.700636386871338 | BCE Loss: 1.0530184507369995\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.754393100738525 | KNN Loss: 3.714853286743164 | BCE Loss: 1.0395396947860718\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.7085466384887695 | KNN Loss: 3.708204507827759 | BCE Loss: 1.0003420114517212\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.701745986938477 | KNN Loss: 3.6737637519836426 | BCE Loss: 1.027982234954834\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.662020206451416 | KNN Loss: 3.6669042110443115 | BCE Loss: 0.995116114616394\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.687582492828369 | KNN Loss: 3.683413505554199 | BCE Loss: 1.00416898727417\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.711199760437012 | KNN Loss: 3.678924798965454 | BCE Loss: 1.0322749614715576\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.765543460845947 | KNN Loss: 3.7126033306121826 | BCE Loss: 1.0529402494430542\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.716444969177246 | KNN Loss: 3.713541269302368 | BCE Loss: 1.0029038190841675\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.7593159675598145 | KNN Loss: 3.7191991806030273 | BCE Loss: 1.040116786956787\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.705307960510254 | KNN Loss: 3.692172050476074 | BCE Loss: 1.0131361484527588\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.686553478240967 | KNN Loss: 3.670667886734009 | BCE Loss: 1.015885591506958\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.768364429473877 | KNN Loss: 3.6973936557769775 | BCE Loss: 1.0709707736968994\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.66171932220459 | KNN Loss: 3.6617074012756348 | BCE Loss: 1.000011682510376\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.714111328125 | KNN Loss: 3.6946423053741455 | BCE Loss: 1.0194687843322754\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.736546516418457 | KNN Loss: 3.6951160430908203 | BCE Loss: 1.0414302349090576\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.701848983764648 | KNN Loss: 3.6631643772125244 | BCE Loss: 1.038684368133545\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.725481986999512 | KNN Loss: 3.684119701385498 | BCE Loss: 1.0413620471954346\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.7222981452941895 | KNN Loss: 3.7107815742492676 | BCE Loss: 1.0115165710449219\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.726961135864258 | KNN Loss: 3.6841166019439697 | BCE Loss: 1.0428447723388672\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.729419708251953 | KNN Loss: 3.7130286693573 | BCE Loss: 1.0163912773132324\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.719919204711914 | KNN Loss: 3.6799635887145996 | BCE Loss: 1.0399553775787354\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.720677375793457 | KNN Loss: 3.7055835723876953 | BCE Loss: 1.0150940418243408\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.692511558532715 | KNN Loss: 3.664391040802002 | BCE Loss: 1.028120756149292\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.780498504638672 | KNN Loss: 3.729731798171997 | BCE Loss: 1.050766944885254\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.731963634490967 | KNN Loss: 3.6804254055023193 | BCE Loss: 1.0515382289886475\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.691850185394287 | KNN Loss: 3.6756556034088135 | BCE Loss: 1.0161945819854736\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.692877769470215 | KNN Loss: 3.6790127754211426 | BCE Loss: 1.0138651132583618\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.721902847290039 | KNN Loss: 3.6955437660217285 | BCE Loss: 1.0263588428497314\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.720478057861328 | KNN Loss: 3.6909966468811035 | BCE Loss: 1.0294814109802246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.739774703979492 | KNN Loss: 3.69266939163208 | BCE Loss: 1.047105073928833\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.714485168457031 | KNN Loss: 3.6632633209228516 | BCE Loss: 1.0512220859527588\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.720749855041504 | KNN Loss: 3.6967527866363525 | BCE Loss: 1.0239968299865723\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.785182476043701 | KNN Loss: 3.7344589233398438 | BCE Loss: 1.0507235527038574\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.728562355041504 | KNN Loss: 3.7009694576263428 | BCE Loss: 1.027592658996582\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.679009437561035 | KNN Loss: 3.6754894256591797 | BCE Loss: 1.0035202503204346\n",
      "Epoch   460: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.720353603363037 | KNN Loss: 3.7050390243530273 | BCE Loss: 1.0153146982192993\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.718700408935547 | KNN Loss: 3.6871798038482666 | BCE Loss: 1.0315207242965698\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.699286937713623 | KNN Loss: 3.666996479034424 | BCE Loss: 1.0322904586791992\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.6782026290893555 | KNN Loss: 3.664752721786499 | BCE Loss: 1.0134496688842773\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.71182918548584 | KNN Loss: 3.7081034183502197 | BCE Loss: 1.003725528717041\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.746798515319824 | KNN Loss: 3.6824183464050293 | BCE Loss: 1.0643800497055054\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.709904193878174 | KNN Loss: 3.702324151992798 | BCE Loss: 1.007580041885376\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.759861946105957 | KNN Loss: 3.7066810131073 | BCE Loss: 1.0531809329986572\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.680813789367676 | KNN Loss: 3.6626627445220947 | BCE Loss: 1.0181512832641602\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.675882339477539 | KNN Loss: 3.6737382411956787 | BCE Loss: 1.0021438598632812\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.754533767700195 | KNN Loss: 3.7092835903167725 | BCE Loss: 1.0452499389648438\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.709959983825684 | KNN Loss: 3.671574592590332 | BCE Loss: 1.0383856296539307\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.7118144035339355 | KNN Loss: 3.6666500568389893 | BCE Loss: 1.0451642274856567\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.687689781188965 | KNN Loss: 3.686581611633301 | BCE Loss: 1.0011082887649536\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.665334701538086 | KNN Loss: 3.6719961166381836 | BCE Loss: 0.9933384656906128\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.711500644683838 | KNN Loss: 3.682868003845215 | BCE Loss: 1.028632640838623\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.7525763511657715 | KNN Loss: 3.7206263542175293 | BCE Loss: 1.0319501161575317\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.735342979431152 | KNN Loss: 3.7148168087005615 | BCE Loss: 1.0205259323120117\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.731318473815918 | KNN Loss: 3.6786553859710693 | BCE Loss: 1.0526628494262695\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.713650226593018 | KNN Loss: 3.7044517993927 | BCE Loss: 1.0091984272003174\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.657837867736816 | KNN Loss: 3.6669440269470215 | BCE Loss: 0.9908936023712158\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.708978176116943 | KNN Loss: 3.692000389099121 | BCE Loss: 1.0169776678085327\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.686981678009033 | KNN Loss: 3.678886890411377 | BCE Loss: 1.0080947875976562\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.690139293670654 | KNN Loss: 3.674640655517578 | BCE Loss: 1.0154987573623657\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.736352920532227 | KNN Loss: 3.6916000843048096 | BCE Loss: 1.044752836227417\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.716772079467773 | KNN Loss: 3.684755802154541 | BCE Loss: 1.0320161581039429\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.676931381225586 | KNN Loss: 3.666996955871582 | BCE Loss: 1.0099341869354248\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.765164852142334 | KNN Loss: 3.7180635929107666 | BCE Loss: 1.0471012592315674\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.7324628829956055 | KNN Loss: 3.7145707607269287 | BCE Loss: 1.0178920030593872\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.662439346313477 | KNN Loss: 3.65423583984375 | BCE Loss: 1.0082035064697266\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.717128753662109 | KNN Loss: 3.6985034942626953 | BCE Loss: 1.018625020980835\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.744204998016357 | KNN Loss: 3.694766044616699 | BCE Loss: 1.0494390726089478\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.689189910888672 | KNN Loss: 3.6828699111938477 | BCE Loss: 1.0063199996948242\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.781952381134033 | KNN Loss: 3.7358670234680176 | BCE Loss: 1.0460853576660156\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.724981784820557 | KNN Loss: 3.7069826126098633 | BCE Loss: 1.0179991722106934\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.701434135437012 | KNN Loss: 3.684823989868164 | BCE Loss: 1.0166099071502686\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.704455852508545 | KNN Loss: 3.6793556213378906 | BCE Loss: 1.0251003503799438\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.698568344116211 | KNN Loss: 3.679694414138794 | BCE Loss: 1.018874168395996\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.736159324645996 | KNN Loss: 3.697255849838257 | BCE Loss: 1.0389033555984497\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.726858615875244 | KNN Loss: 3.704630136489868 | BCE Loss: 1.022228479385376\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.685725212097168 | KNN Loss: 3.6913766860961914 | BCE Loss: 0.9943483471870422\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.691960334777832 | KNN Loss: 3.6819398403167725 | BCE Loss: 1.01002037525177\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.743332862854004 | KNN Loss: 3.733433723449707 | BCE Loss: 1.0098989009857178\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.740198135375977 | KNN Loss: 3.7195260524749756 | BCE Loss: 1.02067232131958\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.715250492095947 | KNN Loss: 3.6686131954193115 | BCE Loss: 1.0466371774673462\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.6902313232421875 | KNN Loss: 3.6712327003479004 | BCE Loss: 1.0189988613128662\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.764775276184082 | KNN Loss: 3.7152884006500244 | BCE Loss: 1.0494871139526367\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.697299957275391 | KNN Loss: 3.6774582862854004 | BCE Loss: 1.0198416709899902\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.71532678604126 | KNN Loss: 3.72135591506958 | BCE Loss: 0.9939708113670349\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.7162275314331055 | KNN Loss: 3.7091455459594727 | BCE Loss: 1.007082223892212\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.682646751403809 | KNN Loss: 3.6625211238861084 | BCE Loss: 1.020125389099121\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.710503578186035 | KNN Loss: 3.671957492828369 | BCE Loss: 1.0385463237762451\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.706883430480957 | KNN Loss: 3.6705055236816406 | BCE Loss: 1.0363776683807373\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.680978775024414 | KNN Loss: 3.6750292778015137 | BCE Loss: 1.0059492588043213\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.7171478271484375 | KNN Loss: 3.694692373275757 | BCE Loss: 1.0224554538726807\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.712190628051758 | KNN Loss: 3.670447826385498 | BCE Loss: 1.0417425632476807\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.67674446105957 | KNN Loss: 3.676348924636841 | BCE Loss: 1.0003957748413086\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.7302680015563965 | KNN Loss: 3.681279420852661 | BCE Loss: 1.0489885807037354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.723755836486816 | KNN Loss: 3.6922805309295654 | BCE Loss: 1.031475305557251\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.755258083343506 | KNN Loss: 3.71797251701355 | BCE Loss: 1.037285566329956\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.722499370574951 | KNN Loss: 3.7153282165527344 | BCE Loss: 1.0071711540222168\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.697483062744141 | KNN Loss: 3.651270627975464 | BCE Loss: 1.0462125539779663\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.736935615539551 | KNN Loss: 3.716346263885498 | BCE Loss: 1.0205895900726318\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.670272350311279 | KNN Loss: 3.653196096420288 | BCE Loss: 1.0170761346817017\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.682635307312012 | KNN Loss: 3.6709365844726562 | BCE Loss: 1.0116984844207764\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.701333045959473 | KNN Loss: 3.6814544200897217 | BCE Loss: 1.019878625869751\n",
      "Epoch   471: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.748342037200928 | KNN Loss: 3.698843240737915 | BCE Loss: 1.0494986772537231\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.736245632171631 | KNN Loss: 3.712282419204712 | BCE Loss: 1.0239630937576294\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.707401752471924 | KNN Loss: 3.677917003631592 | BCE Loss: 1.029484748840332\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.711759567260742 | KNN Loss: 3.6728005409240723 | BCE Loss: 1.0389587879180908\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.668095588684082 | KNN Loss: 3.684798002243042 | BCE Loss: 0.9832978248596191\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.717759132385254 | KNN Loss: 3.7025721073150635 | BCE Loss: 1.0151872634887695\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.698949813842773 | KNN Loss: 3.6804871559143066 | BCE Loss: 1.0184627771377563\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.694636344909668 | KNN Loss: 3.6806652545928955 | BCE Loss: 1.013971209526062\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.733262538909912 | KNN Loss: 3.6946725845336914 | BCE Loss: 1.0385899543762207\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.710938930511475 | KNN Loss: 3.677564859390259 | BCE Loss: 1.0333741903305054\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.676522254943848 | KNN Loss: 3.6663215160369873 | BCE Loss: 1.0102007389068604\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.690251350402832 | KNN Loss: 3.6936774253845215 | BCE Loss: 0.9965739250183105\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.74408483505249 | KNN Loss: 3.7133901119232178 | BCE Loss: 1.0306947231292725\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.748111248016357 | KNN Loss: 3.723508596420288 | BCE Loss: 1.0246026515960693\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.684269905090332 | KNN Loss: 3.6676950454711914 | BCE Loss: 1.0165748596191406\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.702645301818848 | KNN Loss: 3.684859275817871 | BCE Loss: 1.017785906791687\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.716307640075684 | KNN Loss: 3.6915488243103027 | BCE Loss: 1.0247586965560913\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.794241428375244 | KNN Loss: 3.743701696395874 | BCE Loss: 1.0505397319793701\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.682043075561523 | KNN Loss: 3.658829927444458 | BCE Loss: 1.0232133865356445\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.706944465637207 | KNN Loss: 3.68466854095459 | BCE Loss: 1.022275686264038\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.70958948135376 | KNN Loss: 3.662966012954712 | BCE Loss: 1.0466235876083374\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.7539873123168945 | KNN Loss: 3.7345409393310547 | BCE Loss: 1.019446611404419\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.695685863494873 | KNN Loss: 3.6784744262695312 | BCE Loss: 1.0172114372253418\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.692946434020996 | KNN Loss: 3.6699376106262207 | BCE Loss: 1.0230090618133545\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.707188129425049 | KNN Loss: 3.693967580795288 | BCE Loss: 1.0132205486297607\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.732720375061035 | KNN Loss: 3.6901438236236572 | BCE Loss: 1.0425763130187988\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.7141289710998535 | KNN Loss: 3.6876797676086426 | BCE Loss: 1.026449203491211\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.756096839904785 | KNN Loss: 3.697425127029419 | BCE Loss: 1.0586719512939453\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.725776195526123 | KNN Loss: 3.7127387523651123 | BCE Loss: 1.0130374431610107\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.706229209899902 | KNN Loss: 3.699002265930176 | BCE Loss: 1.0072269439697266\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.697099208831787 | KNN Loss: 3.6649229526519775 | BCE Loss: 1.03217613697052\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.721118927001953 | KNN Loss: 3.707890033721924 | BCE Loss: 1.0132288932800293\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.700350761413574 | KNN Loss: 3.691746473312378 | BCE Loss: 1.0086040496826172\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.710463523864746 | KNN Loss: 3.690871000289917 | BCE Loss: 1.0195924043655396\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.725012302398682 | KNN Loss: 3.7010512351989746 | BCE Loss: 1.0239611864089966\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.777421951293945 | KNN Loss: 3.719444990158081 | BCE Loss: 1.0579771995544434\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.741628646850586 | KNN Loss: 3.702996253967285 | BCE Loss: 1.0386325120925903\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.659615516662598 | KNN Loss: 3.6722323894500732 | BCE Loss: 0.9873833060264587\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.690774917602539 | KNN Loss: 3.6575989723205566 | BCE Loss: 1.0331758260726929\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.718820095062256 | KNN Loss: 3.6874186992645264 | BCE Loss: 1.0314013957977295\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.716739654541016 | KNN Loss: 3.6945242881774902 | BCE Loss: 1.0222156047821045\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.785217761993408 | KNN Loss: 3.7405240535736084 | BCE Loss: 1.0446937084197998\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.707675457000732 | KNN Loss: 3.717452049255371 | BCE Loss: 0.9902232885360718\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.7499542236328125 | KNN Loss: 3.7199041843414307 | BCE Loss: 1.030050277709961\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.730362415313721 | KNN Loss: 3.689040184020996 | BCE Loss: 1.0413222312927246\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.7383012771606445 | KNN Loss: 3.7070798873901367 | BCE Loss: 1.0312215089797974\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.747132301330566 | KNN Loss: 3.7040421962738037 | BCE Loss: 1.0430899858474731\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.744436740875244 | KNN Loss: 3.6970837116241455 | BCE Loss: 1.0473531484603882\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.743060111999512 | KNN Loss: 3.716989517211914 | BCE Loss: 1.0260708332061768\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.671565532684326 | KNN Loss: 3.675429582595825 | BCE Loss: 0.996135950088501\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.71971321105957 | KNN Loss: 3.6964755058288574 | BCE Loss: 1.023237943649292\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.751809120178223 | KNN Loss: 3.7142109870910645 | BCE Loss: 1.037597894668579\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.722818851470947 | KNN Loss: 3.7243404388427734 | BCE Loss: 0.9984784126281738\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.731649398803711 | KNN Loss: 3.7131614685058594 | BCE Loss: 1.0184876918792725\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.7556304931640625 | KNN Loss: 3.704848527908325 | BCE Loss: 1.0507822036743164\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.678398132324219 | KNN Loss: 3.6599156856536865 | BCE Loss: 1.0184825658798218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.723355770111084 | KNN Loss: 3.701094388961792 | BCE Loss: 1.0222612619400024\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.721074104309082 | KNN Loss: 3.7213540077209473 | BCE Loss: 0.9997198581695557\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.700201988220215 | KNN Loss: 3.6684844493865967 | BCE Loss: 1.0317176580429077\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.729432106018066 | KNN Loss: 3.6830976009368896 | BCE Loss: 1.0463342666625977\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.713353157043457 | KNN Loss: 3.698345899581909 | BCE Loss: 1.0150070190429688\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.67999792098999 | KNN Loss: 3.65663743019104 | BCE Loss: 1.0233604907989502\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.685439109802246 | KNN Loss: 3.682257890701294 | BCE Loss: 1.0031814575195312\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.680056095123291 | KNN Loss: 3.6669938564300537 | BCE Loss: 1.0130622386932373\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.74213981628418 | KNN Loss: 3.7193422317504883 | BCE Loss: 1.0227975845336914\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.7447509765625 | KNN Loss: 3.6972455978393555 | BCE Loss: 1.0475053787231445\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.634084701538086 | KNN Loss: 3.641204833984375 | BCE Loss: 0.9928798675537109\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.717449188232422 | KNN Loss: 3.6802451610565186 | BCE Loss: 1.0372041463851929\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.782068729400635 | KNN Loss: 3.744354486465454 | BCE Loss: 1.0377142429351807\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.723977088928223 | KNN Loss: 3.6661031246185303 | BCE Loss: 1.0578737258911133\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.738971710205078 | KNN Loss: 3.697021007537842 | BCE Loss: 1.0419507026672363\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.656038761138916 | KNN Loss: 3.6762473583221436 | BCE Loss: 0.9797912836074829\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.695523738861084 | KNN Loss: 3.673084020614624 | BCE Loss: 1.0224398374557495\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.751304626464844 | KNN Loss: 3.7068276405334473 | BCE Loss: 1.044476866722107\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.724771022796631 | KNN Loss: 3.6873011589050293 | BCE Loss: 1.037469744682312\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.709491729736328 | KNN Loss: 3.6965787410736084 | BCE Loss: 1.0129131078720093\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.722274303436279 | KNN Loss: 3.6942925453186035 | BCE Loss: 1.0279817581176758\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.72895622253418 | KNN Loss: 3.669440984725952 | BCE Loss: 1.0595149993896484\n",
      "Epoch   484: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.728504180908203 | KNN Loss: 3.711672782897949 | BCE Loss: 1.0168312788009644\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.73421049118042 | KNN Loss: 3.710103988647461 | BCE Loss: 1.0241063833236694\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.681027889251709 | KNN Loss: 3.6803317070007324 | BCE Loss: 1.0006961822509766\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.762002944946289 | KNN Loss: 3.720550060272217 | BCE Loss: 1.0414528846740723\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.724313735961914 | KNN Loss: 3.6743557453155518 | BCE Loss: 1.0499579906463623\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.740289688110352 | KNN Loss: 3.7013771533966064 | BCE Loss: 1.0389127731323242\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.735511302947998 | KNN Loss: 3.707571268081665 | BCE Loss: 1.027940034866333\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.707070827484131 | KNN Loss: 3.686570405960083 | BCE Loss: 1.0205004215240479\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.757627487182617 | KNN Loss: 3.729583740234375 | BCE Loss: 1.0280439853668213\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.750877380371094 | KNN Loss: 3.7028591632843018 | BCE Loss: 1.048018217086792\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.678201675415039 | KNN Loss: 3.670415163040161 | BCE Loss: 1.0077863931655884\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.648961067199707 | KNN Loss: 3.643296241760254 | BCE Loss: 1.0056648254394531\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.684641361236572 | KNN Loss: 3.6721179485321045 | BCE Loss: 1.0125232934951782\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.74599027633667 | KNN Loss: 3.7163009643554688 | BCE Loss: 1.0296891927719116\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.713381290435791 | KNN Loss: 3.68755841255188 | BCE Loss: 1.0258228778839111\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.730381011962891 | KNN Loss: 3.7009682655334473 | BCE Loss: 1.0294127464294434\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.725818157196045 | KNN Loss: 3.691518545150757 | BCE Loss: 1.0342997312545776\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.737443923950195 | KNN Loss: 3.7083890438079834 | BCE Loss: 1.0290546417236328\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.7133893966674805 | KNN Loss: 3.6828384399414062 | BCE Loss: 1.0305507183074951\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.713059902191162 | KNN Loss: 3.675588846206665 | BCE Loss: 1.0374711751937866\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.684459686279297 | KNN Loss: 3.675546884536743 | BCE Loss: 1.0089128017425537\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.749680042266846 | KNN Loss: 3.7266924381256104 | BCE Loss: 1.0229876041412354\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.704742431640625 | KNN Loss: 3.6914544105529785 | BCE Loss: 1.0132882595062256\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.708797931671143 | KNN Loss: 3.681403398513794 | BCE Loss: 1.0273945331573486\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.759033679962158 | KNN Loss: 3.7285244464874268 | BCE Loss: 1.0305092334747314\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.774177551269531 | KNN Loss: 3.7177369594573975 | BCE Loss: 1.0564405918121338\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.741959571838379 | KNN Loss: 3.727794885635376 | BCE Loss: 1.0141644477844238\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.777251243591309 | KNN Loss: 3.736976385116577 | BCE Loss: 1.0402750968933105\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.704321384429932 | KNN Loss: 3.6741504669189453 | BCE Loss: 1.0301709175109863\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.730434417724609 | KNN Loss: 3.687298059463501 | BCE Loss: 1.0431365966796875\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.776140213012695 | KNN Loss: 3.7452032566070557 | BCE Loss: 1.03093683719635\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.745072364807129 | KNN Loss: 3.734107494354248 | BCE Loss: 1.0109646320343018\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.69664192199707 | KNN Loss: 3.674375057220459 | BCE Loss: 1.0222669839859009\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.7313032150268555 | KNN Loss: 3.6959445476531982 | BCE Loss: 1.0353589057922363\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.687609672546387 | KNN Loss: 3.668813467025757 | BCE Loss: 1.0187962055206299\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.742019176483154 | KNN Loss: 3.722843885421753 | BCE Loss: 1.0191752910614014\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.749171257019043 | KNN Loss: 3.7030580043792725 | BCE Loss: 1.0461130142211914\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.714065074920654 | KNN Loss: 3.6986494064331055 | BCE Loss: 1.0154156684875488\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.718186855316162 | KNN Loss: 3.6717073917388916 | BCE Loss: 1.046479344367981\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.64189338684082 | KNN Loss: 3.6402547359466553 | BCE Loss: 1.001638412475586\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.716793060302734 | KNN Loss: 3.6936628818511963 | BCE Loss: 1.023129940032959\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.6531500816345215 | KNN Loss: 3.664323091506958 | BCE Loss: 0.9888269305229187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.713675498962402 | KNN Loss: 3.686420202255249 | BCE Loss: 1.0272551774978638\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.739717483520508 | KNN Loss: 3.6963613033294678 | BCE Loss: 1.0433560609817505\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.723142147064209 | KNN Loss: 3.700502872467041 | BCE Loss: 1.022639274597168\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.720836639404297 | KNN Loss: 3.7010433673858643 | BCE Loss: 1.0197930335998535\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.700216770172119 | KNN Loss: 3.6801035404205322 | BCE Loss: 1.0201133489608765\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.777024269104004 | KNN Loss: 3.728052854537964 | BCE Loss: 1.0489716529846191\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.656067848205566 | KNN Loss: 3.6576991081237793 | BCE Loss: 0.9983688592910767\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.736042022705078 | KNN Loss: 3.716486930847168 | BCE Loss: 1.019554853439331\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.695156574249268 | KNN Loss: 3.6761322021484375 | BCE Loss: 1.0190242528915405\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.726709365844727 | KNN Loss: 3.687159776687622 | BCE Loss: 1.0395493507385254\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.734867095947266 | KNN Loss: 3.7000808715820312 | BCE Loss: 1.0347862243652344\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.689159393310547 | KNN Loss: 3.6834824085235596 | BCE Loss: 1.0056772232055664\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.708102703094482 | KNN Loss: 3.681816577911377 | BCE Loss: 1.026286005973816\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.780923843383789 | KNN Loss: 3.7041964530944824 | BCE Loss: 1.0767271518707275\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.761817932128906 | KNN Loss: 3.721449851989746 | BCE Loss: 1.0403681993484497\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.733128547668457 | KNN Loss: 3.715585231781006 | BCE Loss: 1.017543077468872\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.6611528396606445 | KNN Loss: 3.647326946258545 | BCE Loss: 1.0138261318206787\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.678000450134277 | KNN Loss: 3.6637625694274902 | BCE Loss: 1.0142379999160767\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.7184953689575195 | KNN Loss: 3.6805126667022705 | BCE Loss: 1.037982702255249\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.687312126159668 | KNN Loss: 3.673482894897461 | BCE Loss: 1.0138294696807861\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.674714088439941 | KNN Loss: 3.6733992099761963 | BCE Loss: 1.0013148784637451\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.754551887512207 | KNN Loss: 3.687851667404175 | BCE Loss: 1.0667003393173218\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.700433731079102 | KNN Loss: 3.680431365966797 | BCE Loss: 1.0200021266937256\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.707125186920166 | KNN Loss: 3.669210195541382 | BCE Loss: 1.0379151105880737\n",
      "Epoch   495: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.697613716125488 | KNN Loss: 3.6825473308563232 | BCE Loss: 1.015066146850586\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.715579032897949 | KNN Loss: 3.6791696548461914 | BCE Loss: 1.0364091396331787\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.724841117858887 | KNN Loss: 3.6640803813934326 | BCE Loss: 1.0607609748840332\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.663426399230957 | KNN Loss: 3.6633520126342773 | BCE Loss: 1.0000745058059692\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.7595391273498535 | KNN Loss: 3.7431719303131104 | BCE Loss: 1.0163673162460327\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.733491897583008 | KNN Loss: 3.7048959732055664 | BCE Loss: 1.0285961627960205\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.698247909545898 | KNN Loss: 3.7057690620422363 | BCE Loss: 0.9924787282943726\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.695540428161621 | KNN Loss: 3.674281358718872 | BCE Loss: 1.0212593078613281\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.7261762619018555 | KNN Loss: 3.702009439468384 | BCE Loss: 1.0241669416427612\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.71955680847168 | KNN Loss: 3.684380292892456 | BCE Loss: 1.035176396369934\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.714669704437256 | KNN Loss: 3.6859121322631836 | BCE Loss: 1.0287576913833618\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.720251083374023 | KNN Loss: 3.7286629676818848 | BCE Loss: 0.9915878772735596\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.682862281799316 | KNN Loss: 3.6845014095306396 | BCE Loss: 0.9983609914779663\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.7207794189453125 | KNN Loss: 3.7027852535247803 | BCE Loss: 1.0179944038391113\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.7740373611450195 | KNN Loss: 3.7101755142211914 | BCE Loss: 1.0638619661331177\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.712190628051758 | KNN Loss: 3.6694495677948 | BCE Loss: 1.042740821838379\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.657422065734863 | KNN Loss: 3.662635087966919 | BCE Loss: 0.9947868585586548\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.667975425720215 | KNN Loss: 3.665238857269287 | BCE Loss: 1.0027364492416382\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.769082546234131 | KNN Loss: 3.748666524887085 | BCE Loss: 1.0204161405563354\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.714722156524658 | KNN Loss: 3.6958515644073486 | BCE Loss: 1.0188705921173096\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.739245891571045 | KNN Loss: 3.707977294921875 | BCE Loss: 1.03126859664917\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.727753162384033 | KNN Loss: 3.6919291019439697 | BCE Loss: 1.035823941230774\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.708835124969482 | KNN Loss: 3.677173376083374 | BCE Loss: 1.0316617488861084\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.670390605926514 | KNN Loss: 3.6593875885009766 | BCE Loss: 1.0110031366348267\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.720124244689941 | KNN Loss: 3.6832122802734375 | BCE Loss: 1.036911964416504\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.785379886627197 | KNN Loss: 3.7189669609069824 | BCE Loss: 1.0664128065109253\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.748585224151611 | KNN Loss: 3.7002203464508057 | BCE Loss: 1.0483649969100952\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.7062668800354 | KNN Loss: 3.683927297592163 | BCE Loss: 1.0223395824432373\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.698295593261719 | KNN Loss: 3.68048095703125 | BCE Loss: 1.0178143978118896\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.7500901222229 | KNN Loss: 3.730922222137451 | BCE Loss: 1.0191677808761597\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9380,  2.3906,  3.0006,  2.2567,  3.8740,  0.6361,  3.0656,  2.5721,\n",
      "          1.4370,  2.3509,  2.0332,  1.6066,  0.9440,  2.1109,  1.3601,  1.5618,\n",
      "          2.6201,  2.0559,  1.9476,  2.5626,  1.7362,  2.5003,  2.7152,  3.0096,\n",
      "          2.2980,  1.4191,  2.2821,  1.4492,  1.4496,  0.1748,  0.0623,  1.1730,\n",
      "          0.3292,  0.9314,  1.1736,  1.4804,  1.3876,  3.7322,  0.4617,  1.4594,\n",
      "          1.1210, -0.7043, -0.0831,  1.9769,  2.5086,  1.0815, -0.2059,  0.0990,\n",
      "          1.7805,  1.4562,  2.1947,  0.0654,  1.6704,  0.6176, -0.3426,  1.1495,\n",
      "          1.7938,  1.6236,  1.4176,  1.8316,  0.8881,  1.0403,  0.0438,  1.8676,\n",
      "          1.6128,  2.0330, -1.9911,  0.4503,  2.6109,  2.5046,  2.0279,  0.5732,\n",
      "          1.5381,  2.8802,  2.3922,  1.5608,  0.3403,  0.6326,  0.3242,  1.9697,\n",
      "          0.1613,  0.6509,  2.1566, -0.2382,  0.5027, -0.9883, -2.2782, -0.1516,\n",
      "          0.5367, -1.8180,  0.3130, -0.1467, -0.6027, -0.9736,  0.5923,  1.4121,\n",
      "         -0.6954, -0.6870,  0.3460,  1.4505,  0.8607, -1.5522,  0.7310,  1.1150,\n",
      "         -1.2056, -1.2434, -0.1971,  0.0635, -1.0182, -1.6071, -0.5927, -2.9087,\n",
      "         -0.3272,  1.9761,  1.7472, -0.2845, -0.4325, -0.1057,  1.8839, -2.8065,\n",
      "         -0.0585, -0.2513,  0.2141, -0.5656,  0.1797, -0.7068, -0.9958,  0.8984,\n",
      "          0.4149, -0.4510,  0.5263, -0.7448, -1.3122, -0.2682, -0.4862,  0.7993,\n",
      "         -0.1726,  0.2019, -1.9013, -0.8948, -1.2466,  0.8970, -1.8885, -0.9009,\n",
      "         -1.0910, -0.4341, -1.5313, -0.9812, -2.7720, -1.2653, -1.3399, -0.4404,\n",
      "         -1.7533,  0.6113, -1.6904, -0.3760, -3.4967,  0.4055, -0.2110, -0.6039,\n",
      "         -2.0699, -1.5357, -1.3906, -1.4779, -2.3098, -2.2675, -3.5848]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.5848, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.8740, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c690d04f2414c5d99c8857118dc8fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 82.46it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22083dfa3fa74b17bd4c89661c99404c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a519044fbef24dfeaf5aa12fd1f9ea54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b2a041623b428baeb3081c7f155b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "Epoch: 00 | Batch: 000 / 026 | Total loss: 9.665 | Reg loss: 0.007 | Tree loss: 9.665 | Accuracy: 0.000000 | 0.09 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 026 | Total loss: 9.657 | Reg loss: 0.007 | Tree loss: 9.657 | Accuracy: 0.000000 | 0.084 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 026 | Total loss: 9.645 | Reg loss: 0.007 | Tree loss: 9.645 | Accuracy: 0.000000 | 0.078 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 026 | Total loss: 9.643 | Reg loss: 0.006 | Tree loss: 9.643 | Accuracy: 0.000000 | 0.076 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 026 | Total loss: 9.624 | Reg loss: 0.006 | Tree loss: 9.624 | Accuracy: 0.000000 | 0.078 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 026 | Total loss: 9.616 | Reg loss: 0.006 | Tree loss: 9.616 | Accuracy: 0.000000 | 0.078 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 026 | Total loss: 9.618 | Reg loss: 0.006 | Tree loss: 9.618 | Accuracy: 0.000000 | 0.077 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 026 | Total loss: 9.599 | Reg loss: 0.006 | Tree loss: 9.599 | Accuracy: 0.000000 | 0.076 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 026 | Total loss: 9.591 | Reg loss: 0.006 | Tree loss: 9.591 | Accuracy: 0.000000 | 0.076 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 026 | Total loss: 9.592 | Reg loss: 0.006 | Tree loss: 9.592 | Accuracy: 0.000000 | 0.075 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 026 | Total loss: 9.587 | Reg loss: 0.006 | Tree loss: 9.587 | Accuracy: 0.000000 | 0.075 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 026 | Total loss: 9.572 | Reg loss: 0.006 | Tree loss: 9.572 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 026 | Total loss: 9.564 | Reg loss: 0.007 | Tree loss: 9.564 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 026 | Total loss: 9.555 | Reg loss: 0.007 | Tree loss: 9.555 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 026 | Total loss: 9.554 | Reg loss: 0.007 | Tree loss: 9.554 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 026 | Total loss: 9.550 | Reg loss: 0.007 | Tree loss: 9.550 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 026 | Total loss: 9.532 | Reg loss: 0.007 | Tree loss: 9.532 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 026 | Total loss: 9.530 | Reg loss: 0.007 | Tree loss: 9.530 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 026 | Total loss: 9.522 | Reg loss: 0.007 | Tree loss: 9.522 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 026 | Total loss: 9.509 | Reg loss: 0.008 | Tree loss: 9.509 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 026 | Total loss: 9.508 | Reg loss: 0.008 | Tree loss: 9.508 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 026 | Total loss: 9.487 | Reg loss: 0.008 | Tree loss: 9.487 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 026 | Total loss: 9.481 | Reg loss: 0.008 | Tree loss: 9.481 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 026 | Total loss: 9.479 | Reg loss: 0.008 | Tree loss: 9.479 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 026 | Total loss: 9.473 | Reg loss: 0.008 | Tree loss: 9.473 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 026 | Total loss: 9.473 | Reg loss: 0.009 | Tree loss: 9.473 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 01 | Batch: 000 / 026 | Total loss: 9.520 | Reg loss: 0.003 | Tree loss: 9.520 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 026 | Total loss: 9.514 | Reg loss: 0.003 | Tree loss: 9.514 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 026 | Total loss: 9.509 | Reg loss: 0.003 | Tree loss: 9.509 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 026 | Total loss: 9.496 | Reg loss: 0.004 | Tree loss: 9.496 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 026 | Total loss: 9.490 | Reg loss: 0.004 | Tree loss: 9.490 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 026 | Total loss: 9.471 | Reg loss: 0.004 | Tree loss: 9.471 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 026 | Total loss: 9.470 | Reg loss: 0.004 | Tree loss: 9.470 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 026 | Total loss: 9.456 | Reg loss: 0.004 | Tree loss: 9.456 | Accuracy: 0.000000 | 0.072 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 026 | Total loss: 9.462 | Reg loss: 0.005 | Tree loss: 9.462 | Accuracy: 0.001953 | 0.072 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 026 | Total loss: 9.453 | Reg loss: 0.005 | Tree loss: 9.453 | Accuracy: 0.000000 | 0.072 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 026 | Total loss: 9.438 | Reg loss: 0.005 | Tree loss: 9.438 | Accuracy: 0.007812 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 026 | Total loss: 9.428 | Reg loss: 0.005 | Tree loss: 9.428 | Accuracy: 0.019531 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 026 | Total loss: 9.425 | Reg loss: 0.005 | Tree loss: 9.425 | Accuracy: 0.017578 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 026 | Total loss: 9.408 | Reg loss: 0.006 | Tree loss: 9.408 | Accuracy: 0.039062 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 026 | Total loss: 9.412 | Reg loss: 0.006 | Tree loss: 9.412 | Accuracy: 0.060547 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 026 | Total loss: 9.400 | Reg loss: 0.006 | Tree loss: 9.400 | Accuracy: 0.060547 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 026 | Total loss: 9.395 | Reg loss: 0.007 | Tree loss: 9.395 | Accuracy: 0.056641 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 026 | Total loss: 9.401 | Reg loss: 0.007 | Tree loss: 9.401 | Accuracy: 0.064453 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 026 | Total loss: 9.380 | Reg loss: 0.007 | Tree loss: 9.380 | Accuracy: 0.058594 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 026 | Total loss: 9.366 | Reg loss: 0.007 | Tree loss: 9.366 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 026 | Total loss: 9.360 | Reg loss: 0.008 | Tree loss: 9.360 | Accuracy: 0.076172 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 026 | Total loss: 9.348 | Reg loss: 0.008 | Tree loss: 9.348 | Accuracy: 0.080078 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 026 | Total loss: 9.352 | Reg loss: 0.008 | Tree loss: 9.352 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 026 | Total loss: 9.337 | Reg loss: 0.008 | Tree loss: 9.337 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 026 | Total loss: 9.333 | Reg loss: 0.009 | Tree loss: 9.333 | Accuracy: 0.068359 | 0.071 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 026 | Total loss: 9.323 | Reg loss: 0.009 | Tree loss: 9.323 | Accuracy: 0.066852 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 02 | Batch: 000 / 026 | Total loss: 9.382 | Reg loss: 0.005 | Tree loss: 9.382 | Accuracy: 0.064453 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 026 | Total loss: 9.380 | Reg loss: 0.005 | Tree loss: 9.380 | Accuracy: 0.064453 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 026 | Total loss: 9.368 | Reg loss: 0.005 | Tree loss: 9.368 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 026 | Total loss: 9.363 | Reg loss: 0.005 | Tree loss: 9.363 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 026 | Total loss: 9.344 | Reg loss: 0.005 | Tree loss: 9.344 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 026 | Total loss: 9.349 | Reg loss: 0.006 | Tree loss: 9.349 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 026 | Total loss: 9.339 | Reg loss: 0.006 | Tree loss: 9.339 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 026 | Total loss: 9.338 | Reg loss: 0.006 | Tree loss: 9.338 | Accuracy: 0.058594 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 026 | Total loss: 9.325 | Reg loss: 0.006 | Tree loss: 9.325 | Accuracy: 0.058594 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 026 | Total loss: 9.313 | Reg loss: 0.006 | Tree loss: 9.313 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 026 | Total loss: 9.306 | Reg loss: 0.006 | Tree loss: 9.306 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 026 | Total loss: 9.300 | Reg loss: 0.007 | Tree loss: 9.300 | Accuracy: 0.056641 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 012 / 026 | Total loss: 9.292 | Reg loss: 0.007 | Tree loss: 9.292 | Accuracy: 0.080078 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 026 | Total loss: 9.272 | Reg loss: 0.007 | Tree loss: 9.272 | Accuracy: 0.099609 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 026 | Total loss: 9.268 | Reg loss: 0.007 | Tree loss: 9.268 | Accuracy: 0.095703 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 026 | Total loss: 9.272 | Reg loss: 0.008 | Tree loss: 9.272 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 026 | Total loss: 9.258 | Reg loss: 0.008 | Tree loss: 9.258 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 026 | Total loss: 9.250 | Reg loss: 0.008 | Tree loss: 9.250 | Accuracy: 0.078125 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 026 | Total loss: 9.244 | Reg loss: 0.008 | Tree loss: 9.244 | Accuracy: 0.095703 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 026 | Total loss: 9.235 | Reg loss: 0.009 | Tree loss: 9.235 | Accuracy: 0.074219 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 026 | Total loss: 9.225 | Reg loss: 0.009 | Tree loss: 9.225 | Accuracy: 0.060547 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 026 | Total loss: 9.213 | Reg loss: 0.009 | Tree loss: 9.213 | Accuracy: 0.101562 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 026 | Total loss: 9.209 | Reg loss: 0.009 | Tree loss: 9.209 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 026 | Total loss: 9.202 | Reg loss: 0.010 | Tree loss: 9.202 | Accuracy: 0.078125 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 026 | Total loss: 9.192 | Reg loss: 0.010 | Tree loss: 9.192 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 026 | Total loss: 9.186 | Reg loss: 0.010 | Tree loss: 9.186 | Accuracy: 0.094708 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 03 | Batch: 000 / 026 | Total loss: 9.253 | Reg loss: 0.007 | Tree loss: 9.253 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 026 | Total loss: 9.249 | Reg loss: 0.007 | Tree loss: 9.249 | Accuracy: 0.058594 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 026 | Total loss: 9.245 | Reg loss: 0.007 | Tree loss: 9.245 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 026 | Total loss: 9.220 | Reg loss: 0.007 | Tree loss: 9.220 | Accuracy: 0.091797 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 026 | Total loss: 9.223 | Reg loss: 0.007 | Tree loss: 9.223 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 026 | Total loss: 9.209 | Reg loss: 0.007 | Tree loss: 9.209 | Accuracy: 0.095703 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 026 | Total loss: 9.206 | Reg loss: 0.007 | Tree loss: 9.206 | Accuracy: 0.064453 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 026 | Total loss: 9.199 | Reg loss: 0.007 | Tree loss: 9.199 | Accuracy: 0.068359 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 026 | Total loss: 9.182 | Reg loss: 0.008 | Tree loss: 9.182 | Accuracy: 0.064453 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 026 | Total loss: 9.187 | Reg loss: 0.008 | Tree loss: 9.187 | Accuracy: 0.080078 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 026 | Total loss: 9.173 | Reg loss: 0.008 | Tree loss: 9.173 | Accuracy: 0.072266 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 026 | Total loss: 9.166 | Reg loss: 0.008 | Tree loss: 9.166 | Accuracy: 0.087891 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 026 | Total loss: 9.147 | Reg loss: 0.008 | Tree loss: 9.147 | Accuracy: 0.087891 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 026 | Total loss: 9.144 | Reg loss: 0.009 | Tree loss: 9.144 | Accuracy: 0.078125 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 026 | Total loss: 9.137 | Reg loss: 0.009 | Tree loss: 9.137 | Accuracy: 0.083984 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 026 | Total loss: 9.127 | Reg loss: 0.009 | Tree loss: 9.127 | Accuracy: 0.093750 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 026 | Total loss: 9.121 | Reg loss: 0.009 | Tree loss: 9.121 | Accuracy: 0.072266 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 026 | Total loss: 9.115 | Reg loss: 0.010 | Tree loss: 9.115 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 026 | Total loss: 9.100 | Reg loss: 0.010 | Tree loss: 9.100 | Accuracy: 0.083984 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 026 | Total loss: 9.093 | Reg loss: 0.010 | Tree loss: 9.093 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 026 | Total loss: 9.084 | Reg loss: 0.010 | Tree loss: 9.084 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 026 | Total loss: 9.071 | Reg loss: 0.011 | Tree loss: 9.071 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 026 | Total loss: 9.080 | Reg loss: 0.011 | Tree loss: 9.080 | Accuracy: 0.068359 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 026 | Total loss: 9.065 | Reg loss: 0.011 | Tree loss: 9.065 | Accuracy: 0.087891 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 026 | Total loss: 9.053 | Reg loss: 0.011 | Tree loss: 9.053 | Accuracy: 0.101562 | 0.071 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 026 | Total loss: 9.067 | Reg loss: 0.012 | Tree loss: 9.067 | Accuracy: 0.052925 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 04 | Batch: 000 / 026 | Total loss: 9.127 | Reg loss: 0.008 | Tree loss: 9.127 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 026 | Total loss: 9.115 | Reg loss: 0.008 | Tree loss: 9.115 | Accuracy: 0.062500 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 026 | Total loss: 9.103 | Reg loss: 0.008 | Tree loss: 9.103 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 026 | Total loss: 9.096 | Reg loss: 0.009 | Tree loss: 9.096 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 026 | Total loss: 9.083 | Reg loss: 0.009 | Tree loss: 9.083 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 026 | Total loss: 9.085 | Reg loss: 0.009 | Tree loss: 9.085 | Accuracy: 0.058594 | 0.071 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 026 | Total loss: 9.072 | Reg loss: 0.009 | Tree loss: 9.072 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 026 | Total loss: 9.060 | Reg loss: 0.009 | Tree loss: 9.060 | Accuracy: 0.062500 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 026 | Total loss: 9.056 | Reg loss: 0.009 | Tree loss: 9.056 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 026 | Total loss: 9.047 | Reg loss: 0.009 | Tree loss: 9.047 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 026 | Total loss: 9.037 | Reg loss: 0.010 | Tree loss: 9.037 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 026 | Total loss: 9.025 | Reg loss: 0.010 | Tree loss: 9.025 | Accuracy: 0.064453 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 026 | Total loss: 9.019 | Reg loss: 0.010 | Tree loss: 9.019 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 026 | Total loss: 9.012 | Reg loss: 0.010 | Tree loss: 9.012 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 026 | Total loss: 9.012 | Reg loss: 0.011 | Tree loss: 9.012 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 026 | Total loss: 8.990 | Reg loss: 0.011 | Tree loss: 8.990 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 026 | Total loss: 8.977 | Reg loss: 0.011 | Tree loss: 8.977 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 026 | Total loss: 8.967 | Reg loss: 0.011 | Tree loss: 8.967 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 026 | Total loss: 8.967 | Reg loss: 0.012 | Tree loss: 8.967 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 026 | Total loss: 8.952 | Reg loss: 0.012 | Tree loss: 8.952 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 026 | Total loss: 8.942 | Reg loss: 0.012 | Tree loss: 8.942 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 026 | Total loss: 8.924 | Reg loss: 0.012 | Tree loss: 8.924 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 026 | Total loss: 8.926 | Reg loss: 0.013 | Tree loss: 8.926 | Accuracy: 0.101562 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 026 | Total loss: 8.921 | Reg loss: 0.013 | Tree loss: 8.921 | Accuracy: 0.101562 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 026 | Total loss: 8.907 | Reg loss: 0.013 | Tree loss: 8.907 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 026 | Total loss: 8.899 | Reg loss: 0.013 | Tree loss: 8.899 | Accuracy: 0.097493 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Batch: 000 / 026 | Total loss: 8.989 | Reg loss: 0.010 | Tree loss: 8.989 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 026 | Total loss: 8.981 | Reg loss: 0.010 | Tree loss: 8.981 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 026 | Total loss: 8.971 | Reg loss: 0.010 | Tree loss: 8.971 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 026 | Total loss: 8.956 | Reg loss: 0.010 | Tree loss: 8.956 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 026 | Total loss: 8.950 | Reg loss: 0.010 | Tree loss: 8.950 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 026 | Total loss: 8.939 | Reg loss: 0.011 | Tree loss: 8.939 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 026 | Total loss: 8.941 | Reg loss: 0.011 | Tree loss: 8.941 | Accuracy: 0.054688 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 026 | Total loss: 8.918 | Reg loss: 0.011 | Tree loss: 8.918 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 026 | Total loss: 8.906 | Reg loss: 0.011 | Tree loss: 8.906 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 026 | Total loss: 8.908 | Reg loss: 0.011 | Tree loss: 8.908 | Accuracy: 0.062500 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 026 | Total loss: 8.892 | Reg loss: 0.011 | Tree loss: 8.892 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 026 | Total loss: 8.885 | Reg loss: 0.012 | Tree loss: 8.885 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 026 | Total loss: 8.867 | Reg loss: 0.012 | Tree loss: 8.867 | Accuracy: 0.091797 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 026 | Total loss: 8.871 | Reg loss: 0.012 | Tree loss: 8.871 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 026 | Total loss: 8.862 | Reg loss: 0.012 | Tree loss: 8.862 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 026 | Total loss: 8.843 | Reg loss: 0.013 | Tree loss: 8.843 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 026 | Total loss: 8.827 | Reg loss: 0.013 | Tree loss: 8.827 | Accuracy: 0.099609 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 026 | Total loss: 8.823 | Reg loss: 0.013 | Tree loss: 8.823 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 026 | Total loss: 8.810 | Reg loss: 0.013 | Tree loss: 8.810 | Accuracy: 0.103516 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 026 | Total loss: 8.799 | Reg loss: 0.014 | Tree loss: 8.799 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 026 | Total loss: 8.796 | Reg loss: 0.014 | Tree loss: 8.796 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 026 | Total loss: 8.783 | Reg loss: 0.014 | Tree loss: 8.783 | Accuracy: 0.107422 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 026 | Total loss: 8.777 | Reg loss: 0.014 | Tree loss: 8.777 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 026 | Total loss: 8.771 | Reg loss: 0.015 | Tree loss: 8.771 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 026 | Total loss: 8.764 | Reg loss: 0.015 | Tree loss: 8.764 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 026 | Total loss: 8.753 | Reg loss: 0.015 | Tree loss: 8.753 | Accuracy: 0.091922 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 06 | Batch: 000 / 026 | Total loss: 8.838 | Reg loss: 0.012 | Tree loss: 8.838 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 026 | Total loss: 8.845 | Reg loss: 0.012 | Tree loss: 8.845 | Accuracy: 0.058594 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 026 | Total loss: 8.830 | Reg loss: 0.012 | Tree loss: 8.830 | Accuracy: 0.058594 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 026 | Total loss: 8.828 | Reg loss: 0.012 | Tree loss: 8.828 | Accuracy: 0.060547 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 026 | Total loss: 8.805 | Reg loss: 0.012 | Tree loss: 8.805 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 026 | Total loss: 8.799 | Reg loss: 0.012 | Tree loss: 8.799 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 026 | Total loss: 8.791 | Reg loss: 0.012 | Tree loss: 8.791 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 026 | Total loss: 8.787 | Reg loss: 0.013 | Tree loss: 8.787 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 026 | Total loss: 8.769 | Reg loss: 0.013 | Tree loss: 8.769 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 026 | Total loss: 8.755 | Reg loss: 0.013 | Tree loss: 8.755 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 026 | Total loss: 8.745 | Reg loss: 0.013 | Tree loss: 8.745 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 026 | Total loss: 8.739 | Reg loss: 0.013 | Tree loss: 8.739 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 026 | Total loss: 8.719 | Reg loss: 0.014 | Tree loss: 8.719 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 026 | Total loss: 8.704 | Reg loss: 0.014 | Tree loss: 8.704 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 026 | Total loss: 8.708 | Reg loss: 0.014 | Tree loss: 8.708 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 026 | Total loss: 8.703 | Reg loss: 0.014 | Tree loss: 8.703 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 026 | Total loss: 8.674 | Reg loss: 0.015 | Tree loss: 8.674 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 026 | Total loss: 8.673 | Reg loss: 0.015 | Tree loss: 8.673 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 026 | Total loss: 8.646 | Reg loss: 0.015 | Tree loss: 8.646 | Accuracy: 0.126953 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 026 | Total loss: 8.647 | Reg loss: 0.015 | Tree loss: 8.647 | Accuracy: 0.103516 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 026 | Total loss: 8.642 | Reg loss: 0.016 | Tree loss: 8.642 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 026 | Total loss: 8.633 | Reg loss: 0.016 | Tree loss: 8.633 | Accuracy: 0.107422 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 026 | Total loss: 8.610 | Reg loss: 0.016 | Tree loss: 8.610 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 026 | Total loss: 8.591 | Reg loss: 0.017 | Tree loss: 8.591 | Accuracy: 0.125000 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 026 | Total loss: 8.589 | Reg loss: 0.017 | Tree loss: 8.589 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 026 | Total loss: 8.569 | Reg loss: 0.017 | Tree loss: 8.569 | Accuracy: 0.097493 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 07 | Batch: 000 / 026 | Total loss: 8.709 | Reg loss: 0.014 | Tree loss: 8.709 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 026 | Total loss: 8.679 | Reg loss: 0.014 | Tree loss: 8.679 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 026 | Total loss: 8.676 | Reg loss: 0.014 | Tree loss: 8.676 | Accuracy: 0.103516 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 026 | Total loss: 8.670 | Reg loss: 0.014 | Tree loss: 8.670 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 026 | Total loss: 8.653 | Reg loss: 0.014 | Tree loss: 8.653 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 026 | Total loss: 8.655 | Reg loss: 0.014 | Tree loss: 8.655 | Accuracy: 0.058594 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 026 | Total loss: 8.631 | Reg loss: 0.014 | Tree loss: 8.631 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 026 | Total loss: 8.627 | Reg loss: 0.014 | Tree loss: 8.627 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 026 | Total loss: 8.630 | Reg loss: 0.014 | Tree loss: 8.630 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 026 | Total loss: 8.601 | Reg loss: 0.015 | Tree loss: 8.601 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 026 | Total loss: 8.591 | Reg loss: 0.015 | Tree loss: 8.591 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 026 | Total loss: 8.572 | Reg loss: 0.015 | Tree loss: 8.572 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 026 | Total loss: 8.558 | Reg loss: 0.015 | Tree loss: 8.558 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 026 | Total loss: 8.555 | Reg loss: 0.016 | Tree loss: 8.555 | Accuracy: 0.091797 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 026 | Total loss: 8.539 | Reg loss: 0.016 | Tree loss: 8.539 | Accuracy: 0.082031 | 0.072 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Batch: 015 / 026 | Total loss: 8.531 | Reg loss: 0.016 | Tree loss: 8.531 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 026 | Total loss: 8.516 | Reg loss: 0.016 | Tree loss: 8.516 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 026 | Total loss: 8.517 | Reg loss: 0.016 | Tree loss: 8.517 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 026 | Total loss: 8.493 | Reg loss: 0.017 | Tree loss: 8.493 | Accuracy: 0.109375 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 026 | Total loss: 8.482 | Reg loss: 0.017 | Tree loss: 8.482 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 026 | Total loss: 8.477 | Reg loss: 0.017 | Tree loss: 8.477 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 026 | Total loss: 8.451 | Reg loss: 0.017 | Tree loss: 8.451 | Accuracy: 0.103516 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 026 | Total loss: 8.436 | Reg loss: 0.018 | Tree loss: 8.436 | Accuracy: 0.128906 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 026 | Total loss: 8.446 | Reg loss: 0.018 | Tree loss: 8.446 | Accuracy: 0.091797 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 026 | Total loss: 8.432 | Reg loss: 0.018 | Tree loss: 8.432 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 026 | Total loss: 8.406 | Reg loss: 0.019 | Tree loss: 8.406 | Accuracy: 0.125348 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 08 | Batch: 000 / 026 | Total loss: 8.554 | Reg loss: 0.015 | Tree loss: 8.554 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 026 | Total loss: 8.520 | Reg loss: 0.015 | Tree loss: 8.520 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 026 | Total loss: 8.532 | Reg loss: 0.015 | Tree loss: 8.532 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 026 | Total loss: 8.511 | Reg loss: 0.015 | Tree loss: 8.511 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 026 | Total loss: 8.499 | Reg loss: 0.016 | Tree loss: 8.499 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 026 | Total loss: 8.490 | Reg loss: 0.016 | Tree loss: 8.490 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 026 | Total loss: 8.480 | Reg loss: 0.016 | Tree loss: 8.480 | Accuracy: 0.050781 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 026 | Total loss: 8.468 | Reg loss: 0.016 | Tree loss: 8.468 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 026 | Total loss: 8.450 | Reg loss: 0.016 | Tree loss: 8.450 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 026 | Total loss: 8.431 | Reg loss: 0.016 | Tree loss: 8.431 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 026 | Total loss: 8.427 | Reg loss: 0.016 | Tree loss: 8.427 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 026 | Total loss: 8.415 | Reg loss: 0.017 | Tree loss: 8.415 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 026 | Total loss: 8.392 | Reg loss: 0.017 | Tree loss: 8.392 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 026 | Total loss: 8.387 | Reg loss: 0.017 | Tree loss: 8.387 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 026 | Total loss: 8.373 | Reg loss: 0.017 | Tree loss: 8.373 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 026 | Total loss: 8.368 | Reg loss: 0.017 | Tree loss: 8.368 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 026 | Total loss: 8.353 | Reg loss: 0.018 | Tree loss: 8.353 | Accuracy: 0.109375 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 026 | Total loss: 8.333 | Reg loss: 0.018 | Tree loss: 8.333 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 026 | Total loss: 8.328 | Reg loss: 0.018 | Tree loss: 8.328 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 026 | Total loss: 8.320 | Reg loss: 0.018 | Tree loss: 8.320 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 026 | Total loss: 8.306 | Reg loss: 0.019 | Tree loss: 8.306 | Accuracy: 0.101562 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 026 | Total loss: 8.284 | Reg loss: 0.019 | Tree loss: 8.284 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 026 | Total loss: 8.275 | Reg loss: 0.019 | Tree loss: 8.275 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 026 | Total loss: 8.272 | Reg loss: 0.019 | Tree loss: 8.272 | Accuracy: 0.111328 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 026 | Total loss: 8.261 | Reg loss: 0.020 | Tree loss: 8.261 | Accuracy: 0.115234 | 0.072 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 026 | Total loss: 8.267 | Reg loss: 0.020 | Tree loss: 8.267 | Accuracy: 0.075209 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 09 | Batch: 000 / 026 | Total loss: 8.379 | Reg loss: 0.017 | Tree loss: 8.379 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 026 | Total loss: 8.376 | Reg loss: 0.017 | Tree loss: 8.376 | Accuracy: 0.058594 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 026 | Total loss: 8.355 | Reg loss: 0.017 | Tree loss: 8.355 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 026 | Total loss: 8.352 | Reg loss: 0.017 | Tree loss: 8.352 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 026 | Total loss: 8.335 | Reg loss: 0.017 | Tree loss: 8.335 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 026 | Total loss: 8.320 | Reg loss: 0.017 | Tree loss: 8.320 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 026 | Total loss: 8.316 | Reg loss: 0.017 | Tree loss: 8.316 | Accuracy: 0.099609 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 026 | Total loss: 8.307 | Reg loss: 0.017 | Tree loss: 8.307 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 026 | Total loss: 8.296 | Reg loss: 0.017 | Tree loss: 8.296 | Accuracy: 0.099609 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 026 | Total loss: 8.277 | Reg loss: 0.018 | Tree loss: 8.277 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 026 | Total loss: 8.267 | Reg loss: 0.018 | Tree loss: 8.267 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 026 | Total loss: 8.256 | Reg loss: 0.018 | Tree loss: 8.256 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 026 | Total loss: 8.222 | Reg loss: 0.018 | Tree loss: 8.222 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 026 | Total loss: 8.220 | Reg loss: 0.018 | Tree loss: 8.220 | Accuracy: 0.101562 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 026 | Total loss: 8.221 | Reg loss: 0.019 | Tree loss: 8.221 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 026 | Total loss: 8.179 | Reg loss: 0.019 | Tree loss: 8.179 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 026 | Total loss: 8.187 | Reg loss: 0.019 | Tree loss: 8.187 | Accuracy: 0.109375 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 026 | Total loss: 8.183 | Reg loss: 0.019 | Tree loss: 8.183 | Accuracy: 0.062500 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 026 | Total loss: 8.139 | Reg loss: 0.019 | Tree loss: 8.139 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 026 | Total loss: 8.149 | Reg loss: 0.020 | Tree loss: 8.149 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 026 | Total loss: 8.136 | Reg loss: 0.020 | Tree loss: 8.136 | Accuracy: 0.113281 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 026 | Total loss: 8.112 | Reg loss: 0.020 | Tree loss: 8.112 | Accuracy: 0.111328 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 026 | Total loss: 8.123 | Reg loss: 0.020 | Tree loss: 8.123 | Accuracy: 0.099609 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 026 | Total loss: 8.085 | Reg loss: 0.020 | Tree loss: 8.085 | Accuracy: 0.132812 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 026 | Total loss: 8.079 | Reg loss: 0.021 | Tree loss: 8.079 | Accuracy: 0.113281 | 0.072 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 026 | Total loss: 8.070 | Reg loss: 0.021 | Tree loss: 8.070 | Accuracy: 0.111421 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 000 / 026 | Total loss: 8.239 | Reg loss: 0.018 | Tree loss: 8.239 | Accuracy: 0.056641 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 026 | Total loss: 8.212 | Reg loss: 0.018 | Tree loss: 8.212 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 026 | Total loss: 8.197 | Reg loss: 0.018 | Tree loss: 8.197 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 026 | Total loss: 8.194 | Reg loss: 0.018 | Tree loss: 8.194 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 026 | Total loss: 8.151 | Reg loss: 0.018 | Tree loss: 8.151 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 026 | Total loss: 8.161 | Reg loss: 0.018 | Tree loss: 8.161 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 026 | Total loss: 8.147 | Reg loss: 0.018 | Tree loss: 8.147 | Accuracy: 0.070312 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 026 | Total loss: 8.125 | Reg loss: 0.018 | Tree loss: 8.125 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 026 | Total loss: 8.136 | Reg loss: 0.019 | Tree loss: 8.136 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 026 | Total loss: 8.119 | Reg loss: 0.019 | Tree loss: 8.119 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 026 | Total loss: 8.082 | Reg loss: 0.019 | Tree loss: 8.082 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 026 | Total loss: 8.083 | Reg loss: 0.019 | Tree loss: 8.083 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 026 | Total loss: 8.063 | Reg loss: 0.019 | Tree loss: 8.063 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 026 | Total loss: 8.067 | Reg loss: 0.019 | Tree loss: 8.067 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 026 | Total loss: 8.029 | Reg loss: 0.020 | Tree loss: 8.029 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 026 | Total loss: 8.031 | Reg loss: 0.020 | Tree loss: 8.031 | Accuracy: 0.107422 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 026 | Total loss: 8.011 | Reg loss: 0.020 | Tree loss: 8.011 | Accuracy: 0.109375 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 026 | Total loss: 7.998 | Reg loss: 0.020 | Tree loss: 7.998 | Accuracy: 0.099609 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 026 | Total loss: 7.978 | Reg loss: 0.020 | Tree loss: 7.978 | Accuracy: 0.103516 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 026 | Total loss: 7.970 | Reg loss: 0.021 | Tree loss: 7.970 | Accuracy: 0.115234 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 026 | Total loss: 7.963 | Reg loss: 0.021 | Tree loss: 7.963 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 026 | Total loss: 7.953 | Reg loss: 0.021 | Tree loss: 7.953 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 026 | Total loss: 7.919 | Reg loss: 0.021 | Tree loss: 7.919 | Accuracy: 0.107422 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 026 | Total loss: 7.914 | Reg loss: 0.021 | Tree loss: 7.914 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 026 | Total loss: 7.899 | Reg loss: 0.022 | Tree loss: 7.899 | Accuracy: 0.111328 | 0.072 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 026 | Total loss: 7.849 | Reg loss: 0.022 | Tree loss: 7.849 | Accuracy: 0.144847 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 11 | Batch: 000 / 026 | Total loss: 8.059 | Reg loss: 0.019 | Tree loss: 8.059 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 026 | Total loss: 8.046 | Reg loss: 0.019 | Tree loss: 8.046 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 026 | Total loss: 8.031 | Reg loss: 0.019 | Tree loss: 8.031 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 026 | Total loss: 8.024 | Reg loss: 0.019 | Tree loss: 8.024 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 026 | Total loss: 8.014 | Reg loss: 0.019 | Tree loss: 8.014 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 026 | Total loss: 7.990 | Reg loss: 0.019 | Tree loss: 7.990 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 026 | Total loss: 7.965 | Reg loss: 0.019 | Tree loss: 7.965 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 026 | Total loss: 7.967 | Reg loss: 0.020 | Tree loss: 7.967 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 026 | Total loss: 7.949 | Reg loss: 0.020 | Tree loss: 7.949 | Accuracy: 0.097656 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 026 | Total loss: 7.940 | Reg loss: 0.020 | Tree loss: 7.940 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 026 | Total loss: 7.922 | Reg loss: 0.020 | Tree loss: 7.922 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 026 | Total loss: 7.898 | Reg loss: 0.020 | Tree loss: 7.898 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 026 | Total loss: 7.881 | Reg loss: 0.020 | Tree loss: 7.881 | Accuracy: 0.099609 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 026 | Total loss: 7.884 | Reg loss: 0.020 | Tree loss: 7.884 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 026 | Total loss: 7.851 | Reg loss: 0.021 | Tree loss: 7.851 | Accuracy: 0.107422 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 026 | Total loss: 7.852 | Reg loss: 0.021 | Tree loss: 7.852 | Accuracy: 0.095703 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 026 | Total loss: 7.829 | Reg loss: 0.021 | Tree loss: 7.829 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 026 | Total loss: 7.823 | Reg loss: 0.021 | Tree loss: 7.823 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 026 | Total loss: 7.790 | Reg loss: 0.021 | Tree loss: 7.790 | Accuracy: 0.111328 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 026 | Total loss: 7.788 | Reg loss: 0.022 | Tree loss: 7.788 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 026 | Total loss: 7.776 | Reg loss: 0.022 | Tree loss: 7.776 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 026 | Total loss: 7.742 | Reg loss: 0.022 | Tree loss: 7.742 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 026 | Total loss: 7.735 | Reg loss: 0.022 | Tree loss: 7.735 | Accuracy: 0.105469 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 026 | Total loss: 7.738 | Reg loss: 0.022 | Tree loss: 7.738 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 026 | Total loss: 7.725 | Reg loss: 0.023 | Tree loss: 7.725 | Accuracy: 0.105469 | 0.073 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 026 | Total loss: 7.704 | Reg loss: 0.023 | Tree loss: 7.704 | Accuracy: 0.111421 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 12 | Batch: 000 / 026 | Total loss: 7.868 | Reg loss: 0.020 | Tree loss: 7.868 | Accuracy: 0.070312 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 026 | Total loss: 7.874 | Reg loss: 0.020 | Tree loss: 7.874 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 026 | Total loss: 7.868 | Reg loss: 0.020 | Tree loss: 7.868 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 026 | Total loss: 7.855 | Reg loss: 0.020 | Tree loss: 7.855 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 026 | Total loss: 7.836 | Reg loss: 0.020 | Tree loss: 7.836 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 026 | Total loss: 7.823 | Reg loss: 0.020 | Tree loss: 7.823 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 026 | Total loss: 7.809 | Reg loss: 0.020 | Tree loss: 7.809 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 026 | Total loss: 7.783 | Reg loss: 0.020 | Tree loss: 7.783 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 026 | Total loss: 7.772 | Reg loss: 0.021 | Tree loss: 7.772 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 026 | Total loss: 7.744 | Reg loss: 0.021 | Tree loss: 7.744 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 026 | Total loss: 7.763 | Reg loss: 0.021 | Tree loss: 7.763 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 026 | Total loss: 7.720 | Reg loss: 0.021 | Tree loss: 7.720 | Accuracy: 0.109375 | 0.073 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 012 / 026 | Total loss: 7.713 | Reg loss: 0.021 | Tree loss: 7.713 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 026 | Total loss: 7.706 | Reg loss: 0.021 | Tree loss: 7.706 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 026 | Total loss: 7.680 | Reg loss: 0.022 | Tree loss: 7.680 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 026 | Total loss: 7.672 | Reg loss: 0.022 | Tree loss: 7.672 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 026 | Total loss: 7.645 | Reg loss: 0.022 | Tree loss: 7.645 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 026 | Total loss: 7.620 | Reg loss: 0.022 | Tree loss: 7.620 | Accuracy: 0.117188 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 026 | Total loss: 7.635 | Reg loss: 0.022 | Tree loss: 7.635 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 026 | Total loss: 7.592 | Reg loss: 0.023 | Tree loss: 7.592 | Accuracy: 0.099609 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 026 | Total loss: 7.610 | Reg loss: 0.023 | Tree loss: 7.610 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 026 | Total loss: 7.547 | Reg loss: 0.023 | Tree loss: 7.547 | Accuracy: 0.105469 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 026 | Total loss: 7.562 | Reg loss: 0.023 | Tree loss: 7.562 | Accuracy: 0.103516 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 026 | Total loss: 7.536 | Reg loss: 0.024 | Tree loss: 7.536 | Accuracy: 0.105469 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 026 | Total loss: 7.528 | Reg loss: 0.024 | Tree loss: 7.528 | Accuracy: 0.095703 | 0.073 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 026 | Total loss: 7.502 | Reg loss: 0.024 | Tree loss: 7.502 | Accuracy: 0.100279 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 13 | Batch: 000 / 026 | Total loss: 7.737 | Reg loss: 0.021 | Tree loss: 7.737 | Accuracy: 0.058594 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 026 | Total loss: 7.701 | Reg loss: 0.021 | Tree loss: 7.701 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 026 | Total loss: 7.697 | Reg loss: 0.021 | Tree loss: 7.697 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 026 | Total loss: 7.663 | Reg loss: 0.021 | Tree loss: 7.663 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 026 | Total loss: 7.661 | Reg loss: 0.021 | Tree loss: 7.661 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 026 | Total loss: 7.638 | Reg loss: 0.021 | Tree loss: 7.638 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 026 | Total loss: 7.640 | Reg loss: 0.021 | Tree loss: 7.640 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 026 | Total loss: 7.610 | Reg loss: 0.021 | Tree loss: 7.610 | Accuracy: 0.099609 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 026 | Total loss: 7.573 | Reg loss: 0.021 | Tree loss: 7.573 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 026 | Total loss: 7.598 | Reg loss: 0.022 | Tree loss: 7.598 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 026 | Total loss: 7.574 | Reg loss: 0.022 | Tree loss: 7.574 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 026 | Total loss: 7.551 | Reg loss: 0.022 | Tree loss: 7.551 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 026 | Total loss: 7.529 | Reg loss: 0.022 | Tree loss: 7.529 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 026 | Total loss: 7.507 | Reg loss: 0.022 | Tree loss: 7.507 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 026 | Total loss: 7.491 | Reg loss: 0.022 | Tree loss: 7.491 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 026 | Total loss: 7.484 | Reg loss: 0.023 | Tree loss: 7.484 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 026 | Total loss: 7.454 | Reg loss: 0.023 | Tree loss: 7.454 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 026 | Total loss: 7.453 | Reg loss: 0.023 | Tree loss: 7.453 | Accuracy: 0.097656 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 026 | Total loss: 7.429 | Reg loss: 0.023 | Tree loss: 7.429 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 026 | Total loss: 7.415 | Reg loss: 0.024 | Tree loss: 7.415 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 026 | Total loss: 7.402 | Reg loss: 0.024 | Tree loss: 7.402 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 026 | Total loss: 7.369 | Reg loss: 0.024 | Tree loss: 7.369 | Accuracy: 0.097656 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 026 | Total loss: 7.384 | Reg loss: 0.024 | Tree loss: 7.384 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 026 | Total loss: 7.370 | Reg loss: 0.025 | Tree loss: 7.370 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 026 | Total loss: 7.327 | Reg loss: 0.025 | Tree loss: 7.327 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 026 | Total loss: 7.304 | Reg loss: 0.025 | Tree loss: 7.304 | Accuracy: 0.094708 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 14 | Batch: 000 / 026 | Total loss: 7.543 | Reg loss: 0.022 | Tree loss: 7.543 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 026 | Total loss: 7.532 | Reg loss: 0.022 | Tree loss: 7.532 | Accuracy: 0.060547 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 026 | Total loss: 7.484 | Reg loss: 0.022 | Tree loss: 7.484 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 026 | Total loss: 7.466 | Reg loss: 0.022 | Tree loss: 7.466 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 026 | Total loss: 7.466 | Reg loss: 0.022 | Tree loss: 7.466 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 026 | Total loss: 7.453 | Reg loss: 0.022 | Tree loss: 7.453 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 026 | Total loss: 7.420 | Reg loss: 0.022 | Tree loss: 7.420 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 026 | Total loss: 7.394 | Reg loss: 0.022 | Tree loss: 7.394 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 026 | Total loss: 7.388 | Reg loss: 0.022 | Tree loss: 7.388 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 026 | Total loss: 7.351 | Reg loss: 0.023 | Tree loss: 7.351 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 026 | Total loss: 7.342 | Reg loss: 0.023 | Tree loss: 7.342 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 026 | Total loss: 7.346 | Reg loss: 0.023 | Tree loss: 7.346 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 026 | Total loss: 7.330 | Reg loss: 0.023 | Tree loss: 7.330 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 026 | Total loss: 7.300 | Reg loss: 0.023 | Tree loss: 7.300 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 026 | Total loss: 7.279 | Reg loss: 0.024 | Tree loss: 7.279 | Accuracy: 0.095703 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 026 | Total loss: 7.277 | Reg loss: 0.024 | Tree loss: 7.277 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 026 | Total loss: 7.241 | Reg loss: 0.024 | Tree loss: 7.241 | Accuracy: 0.099609 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 026 | Total loss: 7.232 | Reg loss: 0.024 | Tree loss: 7.232 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 026 | Total loss: 7.215 | Reg loss: 0.025 | Tree loss: 7.215 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 026 | Total loss: 7.194 | Reg loss: 0.025 | Tree loss: 7.194 | Accuracy: 0.097656 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 026 | Total loss: 7.172 | Reg loss: 0.025 | Tree loss: 7.172 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 026 | Total loss: 7.173 | Reg loss: 0.025 | Tree loss: 7.173 | Accuracy: 0.068359 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 026 | Total loss: 7.133 | Reg loss: 0.026 | Tree loss: 7.133 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 026 | Total loss: 7.117 | Reg loss: 0.026 | Tree loss: 7.117 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 026 | Total loss: 7.099 | Reg loss: 0.026 | Tree loss: 7.099 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 026 | Total loss: 7.085 | Reg loss: 0.026 | Tree loss: 7.085 | Accuracy: 0.066852 | 0.073 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 15 | Batch: 000 / 026 | Total loss: 7.326 | Reg loss: 0.023 | Tree loss: 7.326 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 026 | Total loss: 7.329 | Reg loss: 0.023 | Tree loss: 7.329 | Accuracy: 0.056641 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 026 | Total loss: 7.294 | Reg loss: 0.023 | Tree loss: 7.294 | Accuracy: 0.068359 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 026 | Total loss: 7.266 | Reg loss: 0.023 | Tree loss: 7.266 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 026 | Total loss: 7.249 | Reg loss: 0.023 | Tree loss: 7.249 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 026 | Total loss: 7.241 | Reg loss: 0.023 | Tree loss: 7.241 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 026 | Total loss: 7.205 | Reg loss: 0.023 | Tree loss: 7.205 | Accuracy: 0.097656 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 026 | Total loss: 7.194 | Reg loss: 0.023 | Tree loss: 7.194 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 026 | Total loss: 7.165 | Reg loss: 0.024 | Tree loss: 7.165 | Accuracy: 0.070312 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 026 | Total loss: 7.109 | Reg loss: 0.024 | Tree loss: 7.109 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 026 | Total loss: 7.142 | Reg loss: 0.024 | Tree loss: 7.142 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 026 | Total loss: 7.122 | Reg loss: 0.024 | Tree loss: 7.122 | Accuracy: 0.099609 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 026 | Total loss: 7.078 | Reg loss: 0.024 | Tree loss: 7.078 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 026 | Total loss: 7.067 | Reg loss: 0.025 | Tree loss: 7.067 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 026 | Total loss: 7.070 | Reg loss: 0.025 | Tree loss: 7.070 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 026 | Total loss: 7.015 | Reg loss: 0.025 | Tree loss: 7.015 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 026 | Total loss: 7.016 | Reg loss: 0.025 | Tree loss: 7.016 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 026 | Total loss: 6.986 | Reg loss: 0.026 | Tree loss: 6.986 | Accuracy: 0.064453 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 026 | Total loss: 6.971 | Reg loss: 0.026 | Tree loss: 6.971 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 026 | Total loss: 6.937 | Reg loss: 0.026 | Tree loss: 6.937 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 026 | Total loss: 6.915 | Reg loss: 0.026 | Tree loss: 6.915 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 026 | Total loss: 6.909 | Reg loss: 0.027 | Tree loss: 6.909 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 026 | Total loss: 6.869 | Reg loss: 0.027 | Tree loss: 6.869 | Accuracy: 0.113281 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 026 | Total loss: 6.866 | Reg loss: 0.027 | Tree loss: 6.866 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 026 | Total loss: 6.823 | Reg loss: 0.027 | Tree loss: 6.823 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 026 | Total loss: 6.809 | Reg loss: 0.028 | Tree loss: 6.809 | Accuracy: 0.064067 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 16 | Batch: 000 / 026 | Total loss: 7.108 | Reg loss: 0.024 | Tree loss: 7.108 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 026 | Total loss: 7.103 | Reg loss: 0.024 | Tree loss: 7.103 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 026 | Total loss: 7.062 | Reg loss: 0.024 | Tree loss: 7.062 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 026 | Total loss: 7.061 | Reg loss: 0.024 | Tree loss: 7.061 | Accuracy: 0.064453 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 026 | Total loss: 7.028 | Reg loss: 0.024 | Tree loss: 7.028 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 026 | Total loss: 6.994 | Reg loss: 0.025 | Tree loss: 6.994 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 026 | Total loss: 6.967 | Reg loss: 0.025 | Tree loss: 6.967 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 026 | Total loss: 6.933 | Reg loss: 0.025 | Tree loss: 6.933 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 026 | Total loss: 6.924 | Reg loss: 0.025 | Tree loss: 6.924 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 026 | Total loss: 6.904 | Reg loss: 0.025 | Tree loss: 6.904 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 026 | Total loss: 6.930 | Reg loss: 0.025 | Tree loss: 6.930 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 026 | Total loss: 6.894 | Reg loss: 0.026 | Tree loss: 6.894 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 026 | Total loss: 6.858 | Reg loss: 0.026 | Tree loss: 6.858 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 026 | Total loss: 6.849 | Reg loss: 0.026 | Tree loss: 6.849 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 026 | Total loss: 6.796 | Reg loss: 0.026 | Tree loss: 6.796 | Accuracy: 0.068359 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 026 | Total loss: 6.793 | Reg loss: 0.026 | Tree loss: 6.793 | Accuracy: 0.070312 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 026 | Total loss: 6.787 | Reg loss: 0.027 | Tree loss: 6.787 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 026 | Total loss: 6.753 | Reg loss: 0.027 | Tree loss: 6.753 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 026 | Total loss: 6.734 | Reg loss: 0.027 | Tree loss: 6.734 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 026 | Total loss: 6.701 | Reg loss: 0.027 | Tree loss: 6.701 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 026 | Total loss: 6.673 | Reg loss: 0.028 | Tree loss: 6.673 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 026 | Total loss: 6.659 | Reg loss: 0.028 | Tree loss: 6.659 | Accuracy: 0.097656 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 026 | Total loss: 6.675 | Reg loss: 0.028 | Tree loss: 6.675 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 026 | Total loss: 6.610 | Reg loss: 0.028 | Tree loss: 6.610 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 026 | Total loss: 6.604 | Reg loss: 0.029 | Tree loss: 6.604 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 026 | Total loss: 6.588 | Reg loss: 0.029 | Tree loss: 6.588 | Accuracy: 0.077994 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 17 | Batch: 000 / 026 | Total loss: 6.870 | Reg loss: 0.025 | Tree loss: 6.870 | Accuracy: 0.101562 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 026 | Total loss: 6.866 | Reg loss: 0.025 | Tree loss: 6.866 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 026 | Total loss: 6.812 | Reg loss: 0.026 | Tree loss: 6.812 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 026 | Total loss: 6.826 | Reg loss: 0.026 | Tree loss: 6.826 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 026 | Total loss: 6.824 | Reg loss: 0.026 | Tree loss: 6.824 | Accuracy: 0.062500 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 026 | Total loss: 6.839 | Reg loss: 0.026 | Tree loss: 6.839 | Accuracy: 0.068359 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 026 | Total loss: 6.751 | Reg loss: 0.026 | Tree loss: 6.751 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 026 | Total loss: 6.733 | Reg loss: 0.026 | Tree loss: 6.733 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 026 | Total loss: 6.697 | Reg loss: 0.026 | Tree loss: 6.697 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 026 | Total loss: 6.748 | Reg loss: 0.026 | Tree loss: 6.748 | Accuracy: 0.068359 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 026 | Total loss: 6.676 | Reg loss: 0.027 | Tree loss: 6.676 | Accuracy: 0.064453 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 026 | Total loss: 6.638 | Reg loss: 0.027 | Tree loss: 6.638 | Accuracy: 0.091797 | 0.073 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 012 / 026 | Total loss: 6.647 | Reg loss: 0.027 | Tree loss: 6.647 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 026 | Total loss: 6.591 | Reg loss: 0.027 | Tree loss: 6.591 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 026 | Total loss: 6.589 | Reg loss: 0.027 | Tree loss: 6.589 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 026 | Total loss: 6.578 | Reg loss: 0.028 | Tree loss: 6.578 | Accuracy: 0.058594 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 026 | Total loss: 6.562 | Reg loss: 0.028 | Tree loss: 6.562 | Accuracy: 0.054688 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 026 | Total loss: 6.528 | Reg loss: 0.028 | Tree loss: 6.528 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 026 | Total loss: 6.482 | Reg loss: 0.028 | Tree loss: 6.482 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 026 | Total loss: 6.488 | Reg loss: 0.028 | Tree loss: 6.488 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 026 | Total loss: 6.437 | Reg loss: 0.029 | Tree loss: 6.437 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 026 | Total loss: 6.424 | Reg loss: 0.029 | Tree loss: 6.424 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 026 | Total loss: 6.383 | Reg loss: 0.029 | Tree loss: 6.383 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 026 | Total loss: 6.450 | Reg loss: 0.029 | Tree loss: 6.450 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 026 | Total loss: 6.326 | Reg loss: 0.030 | Tree loss: 6.326 | Accuracy: 0.103516 | 0.073 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 026 | Total loss: 6.389 | Reg loss: 0.030 | Tree loss: 6.389 | Accuracy: 0.089136 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 18 | Batch: 000 / 026 | Total loss: 6.681 | Reg loss: 0.027 | Tree loss: 6.681 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 026 | Total loss: 6.656 | Reg loss: 0.027 | Tree loss: 6.656 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 026 | Total loss: 6.658 | Reg loss: 0.027 | Tree loss: 6.658 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 026 | Total loss: 6.609 | Reg loss: 0.027 | Tree loss: 6.609 | Accuracy: 0.097656 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 026 | Total loss: 6.605 | Reg loss: 0.027 | Tree loss: 6.605 | Accuracy: 0.089844 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 026 | Total loss: 6.526 | Reg loss: 0.027 | Tree loss: 6.526 | Accuracy: 0.099609 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 026 | Total loss: 6.530 | Reg loss: 0.027 | Tree loss: 6.530 | Accuracy: 0.076172 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 026 | Total loss: 6.539 | Reg loss: 0.027 | Tree loss: 6.539 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 026 | Total loss: 6.512 | Reg loss: 0.027 | Tree loss: 6.512 | Accuracy: 0.068359 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 026 | Total loss: 6.518 | Reg loss: 0.027 | Tree loss: 6.518 | Accuracy: 0.070312 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 026 | Total loss: 6.464 | Reg loss: 0.028 | Tree loss: 6.464 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 026 | Total loss: 6.421 | Reg loss: 0.028 | Tree loss: 6.421 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 026 | Total loss: 6.427 | Reg loss: 0.028 | Tree loss: 6.427 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 026 | Total loss: 6.399 | Reg loss: 0.028 | Tree loss: 6.399 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 026 | Total loss: 6.369 | Reg loss: 0.028 | Tree loss: 6.369 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 026 | Total loss: 6.342 | Reg loss: 0.028 | Tree loss: 6.342 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 026 | Total loss: 6.328 | Reg loss: 0.029 | Tree loss: 6.328 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 026 | Total loss: 6.305 | Reg loss: 0.029 | Tree loss: 6.305 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 026 | Total loss: 6.283 | Reg loss: 0.029 | Tree loss: 6.283 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 026 | Total loss: 6.230 | Reg loss: 0.029 | Tree loss: 6.230 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 026 | Total loss: 6.231 | Reg loss: 0.030 | Tree loss: 6.231 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 026 | Total loss: 6.203 | Reg loss: 0.030 | Tree loss: 6.203 | Accuracy: 0.080078 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 026 | Total loss: 6.214 | Reg loss: 0.030 | Tree loss: 6.214 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 026 | Total loss: 6.181 | Reg loss: 0.030 | Tree loss: 6.181 | Accuracy: 0.074219 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 026 | Total loss: 6.175 | Reg loss: 0.030 | Tree loss: 6.175 | Accuracy: 0.072266 | 0.073 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 026 | Total loss: 6.161 | Reg loss: 0.031 | Tree loss: 6.161 | Accuracy: 0.108635 | 0.073 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 19 | Batch: 000 / 026 | Total loss: 6.469 | Reg loss: 0.028 | Tree loss: 6.469 | Accuracy: 0.064453 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 026 | Total loss: 6.426 | Reg loss: 0.028 | Tree loss: 6.426 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 026 | Total loss: 6.443 | Reg loss: 0.028 | Tree loss: 6.443 | Accuracy: 0.066406 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 026 | Total loss: 6.433 | Reg loss: 0.028 | Tree loss: 6.433 | Accuracy: 0.083984 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 026 | Total loss: 6.373 | Reg loss: 0.028 | Tree loss: 6.373 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 026 | Total loss: 6.332 | Reg loss: 0.028 | Tree loss: 6.332 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 026 | Total loss: 6.332 | Reg loss: 0.028 | Tree loss: 6.332 | Accuracy: 0.093750 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 026 | Total loss: 6.334 | Reg loss: 0.028 | Tree loss: 6.334 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 026 | Total loss: 6.320 | Reg loss: 0.028 | Tree loss: 6.320 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 026 | Total loss: 6.250 | Reg loss: 0.028 | Tree loss: 6.250 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 026 | Total loss: 6.307 | Reg loss: 0.029 | Tree loss: 6.307 | Accuracy: 0.068359 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 026 | Total loss: 6.259 | Reg loss: 0.029 | Tree loss: 6.259 | Accuracy: 0.062500 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 026 | Total loss: 6.207 | Reg loss: 0.029 | Tree loss: 6.207 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 026 | Total loss: 6.190 | Reg loss: 0.029 | Tree loss: 6.190 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 026 | Total loss: 6.187 | Reg loss: 0.029 | Tree loss: 6.187 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 026 | Total loss: 6.133 | Reg loss: 0.029 | Tree loss: 6.133 | Accuracy: 0.056641 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 026 | Total loss: 6.109 | Reg loss: 0.029 | Tree loss: 6.109 | Accuracy: 0.064453 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 026 | Total loss: 6.069 | Reg loss: 0.030 | Tree loss: 6.069 | Accuracy: 0.082031 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 026 | Total loss: 6.100 | Reg loss: 0.030 | Tree loss: 6.100 | Accuracy: 0.085938 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 026 | Total loss: 6.048 | Reg loss: 0.030 | Tree loss: 6.048 | Accuracy: 0.091797 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 026 | Total loss: 6.027 | Reg loss: 0.030 | Tree loss: 6.027 | Accuracy: 0.087891 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 026 | Total loss: 6.026 | Reg loss: 0.030 | Tree loss: 6.026 | Accuracy: 0.078125 | 0.073 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 026 | Total loss: 5.974 | Reg loss: 0.031 | Tree loss: 5.974 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 026 | Total loss: 6.013 | Reg loss: 0.031 | Tree loss: 6.013 | Accuracy: 0.074219 | 0.072 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 024 / 026 | Total loss: 5.974 | Reg loss: 0.031 | Tree loss: 5.974 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 026 | Total loss: 5.950 | Reg loss: 0.031 | Tree loss: 5.950 | Accuracy: 0.086351 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 20 | Batch: 000 / 026 | Total loss: 6.260 | Reg loss: 0.029 | Tree loss: 6.260 | Accuracy: 0.062500 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 026 | Total loss: 6.261 | Reg loss: 0.029 | Tree loss: 6.261 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 026 | Total loss: 6.248 | Reg loss: 0.029 | Tree loss: 6.248 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 026 | Total loss: 6.186 | Reg loss: 0.029 | Tree loss: 6.186 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 026 | Total loss: 6.195 | Reg loss: 0.029 | Tree loss: 6.195 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 026 | Total loss: 6.166 | Reg loss: 0.029 | Tree loss: 6.166 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 026 | Total loss: 6.166 | Reg loss: 0.029 | Tree loss: 6.166 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 026 | Total loss: 6.133 | Reg loss: 0.029 | Tree loss: 6.133 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 026 | Total loss: 6.074 | Reg loss: 0.029 | Tree loss: 6.074 | Accuracy: 0.099609 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 026 | Total loss: 6.084 | Reg loss: 0.029 | Tree loss: 6.084 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 026 | Total loss: 6.059 | Reg loss: 0.029 | Tree loss: 6.059 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 026 | Total loss: 6.043 | Reg loss: 0.029 | Tree loss: 6.043 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 026 | Total loss: 6.022 | Reg loss: 0.030 | Tree loss: 6.022 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 026 | Total loss: 5.986 | Reg loss: 0.030 | Tree loss: 5.986 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 026 | Total loss: 5.969 | Reg loss: 0.030 | Tree loss: 5.969 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 026 | Total loss: 5.940 | Reg loss: 0.030 | Tree loss: 5.940 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 026 | Total loss: 5.937 | Reg loss: 0.030 | Tree loss: 5.937 | Accuracy: 0.064453 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 026 | Total loss: 5.895 | Reg loss: 0.030 | Tree loss: 5.895 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 026 | Total loss: 5.918 | Reg loss: 0.031 | Tree loss: 5.918 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 026 | Total loss: 5.883 | Reg loss: 0.031 | Tree loss: 5.883 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 026 | Total loss: 5.849 | Reg loss: 0.031 | Tree loss: 5.849 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 026 | Total loss: 5.827 | Reg loss: 0.031 | Tree loss: 5.827 | Accuracy: 0.099609 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 026 | Total loss: 5.844 | Reg loss: 0.031 | Tree loss: 5.844 | Accuracy: 0.062500 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 026 | Total loss: 5.827 | Reg loss: 0.031 | Tree loss: 5.827 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 026 | Total loss: 5.776 | Reg loss: 0.032 | Tree loss: 5.776 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 026 | Total loss: 5.746 | Reg loss: 0.032 | Tree loss: 5.746 | Accuracy: 0.069638 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 21 | Batch: 000 / 026 | Total loss: 6.096 | Reg loss: 0.029 | Tree loss: 6.096 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 026 | Total loss: 6.088 | Reg loss: 0.029 | Tree loss: 6.088 | Accuracy: 0.048828 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 026 | Total loss: 6.053 | Reg loss: 0.029 | Tree loss: 6.053 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 026 | Total loss: 6.024 | Reg loss: 0.029 | Tree loss: 6.024 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 026 | Total loss: 5.966 | Reg loss: 0.029 | Tree loss: 5.966 | Accuracy: 0.095703 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 026 | Total loss: 5.973 | Reg loss: 0.030 | Tree loss: 5.973 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 026 | Total loss: 5.958 | Reg loss: 0.030 | Tree loss: 5.958 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 026 | Total loss: 5.987 | Reg loss: 0.030 | Tree loss: 5.987 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 026 | Total loss: 5.905 | Reg loss: 0.030 | Tree loss: 5.905 | Accuracy: 0.091797 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 026 | Total loss: 5.878 | Reg loss: 0.030 | Tree loss: 5.878 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 026 | Total loss: 5.883 | Reg loss: 0.030 | Tree loss: 5.883 | Accuracy: 0.074219 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 026 | Total loss: 5.841 | Reg loss: 0.030 | Tree loss: 5.841 | Accuracy: 0.103516 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 026 | Total loss: 5.825 | Reg loss: 0.030 | Tree loss: 5.825 | Accuracy: 0.091797 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 026 | Total loss: 5.782 | Reg loss: 0.030 | Tree loss: 5.782 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 026 | Total loss: 5.798 | Reg loss: 0.031 | Tree loss: 5.798 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 026 | Total loss: 5.757 | Reg loss: 0.031 | Tree loss: 5.757 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 026 | Total loss: 5.750 | Reg loss: 0.031 | Tree loss: 5.750 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 026 | Total loss: 5.749 | Reg loss: 0.031 | Tree loss: 5.749 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 026 | Total loss: 5.764 | Reg loss: 0.031 | Tree loss: 5.764 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 026 | Total loss: 5.709 | Reg loss: 0.031 | Tree loss: 5.709 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 026 | Total loss: 5.680 | Reg loss: 0.031 | Tree loss: 5.680 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 026 | Total loss: 5.682 | Reg loss: 0.032 | Tree loss: 5.682 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 026 | Total loss: 5.649 | Reg loss: 0.032 | Tree loss: 5.649 | Accuracy: 0.103516 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 026 | Total loss: 5.599 | Reg loss: 0.032 | Tree loss: 5.599 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 026 | Total loss: 5.600 | Reg loss: 0.032 | Tree loss: 5.600 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 026 | Total loss: 5.586 | Reg loss: 0.032 | Tree loss: 5.586 | Accuracy: 0.083565 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 22 | Batch: 000 / 026 | Total loss: 5.900 | Reg loss: 0.030 | Tree loss: 5.900 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 026 | Total loss: 5.898 | Reg loss: 0.030 | Tree loss: 5.898 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 026 | Total loss: 5.875 | Reg loss: 0.030 | Tree loss: 5.875 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 026 | Total loss: 5.857 | Reg loss: 0.030 | Tree loss: 5.857 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 026 | Total loss: 5.845 | Reg loss: 0.030 | Tree loss: 5.845 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 026 | Total loss: 5.784 | Reg loss: 0.030 | Tree loss: 5.784 | Accuracy: 0.091797 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 026 | Total loss: 5.795 | Reg loss: 0.030 | Tree loss: 5.795 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 026 | Total loss: 5.771 | Reg loss: 0.030 | Tree loss: 5.771 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 026 | Total loss: 5.742 | Reg loss: 0.030 | Tree loss: 5.742 | Accuracy: 0.066406 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 026 | Total loss: 5.731 | Reg loss: 0.031 | Tree loss: 5.731 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 026 | Total loss: 5.714 | Reg loss: 0.031 | Tree loss: 5.714 | Accuracy: 0.085938 | 0.072 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batch: 011 / 026 | Total loss: 5.660 | Reg loss: 0.031 | Tree loss: 5.660 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 026 | Total loss: 5.639 | Reg loss: 0.031 | Tree loss: 5.639 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 026 | Total loss: 5.622 | Reg loss: 0.031 | Tree loss: 5.622 | Accuracy: 0.060547 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 026 | Total loss: 5.614 | Reg loss: 0.031 | Tree loss: 5.614 | Accuracy: 0.076172 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 026 | Total loss: 5.625 | Reg loss: 0.031 | Tree loss: 5.625 | Accuracy: 0.068359 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 026 | Total loss: 5.561 | Reg loss: 0.031 | Tree loss: 5.561 | Accuracy: 0.087891 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 026 | Total loss: 5.572 | Reg loss: 0.032 | Tree loss: 5.572 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 026 | Total loss: 5.538 | Reg loss: 0.032 | Tree loss: 5.538 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 026 | Total loss: 5.544 | Reg loss: 0.032 | Tree loss: 5.544 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 026 | Total loss: 5.517 | Reg loss: 0.032 | Tree loss: 5.517 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 026 | Total loss: 5.497 | Reg loss: 0.032 | Tree loss: 5.497 | Accuracy: 0.089844 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 026 | Total loss: 5.462 | Reg loss: 0.032 | Tree loss: 5.462 | Accuracy: 0.101562 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 026 | Total loss: 5.459 | Reg loss: 0.032 | Tree loss: 5.459 | Accuracy: 0.070312 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 026 | Total loss: 5.464 | Reg loss: 0.033 | Tree loss: 5.464 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 026 | Total loss: 5.405 | Reg loss: 0.033 | Tree loss: 5.405 | Accuracy: 0.091922 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 23 | Batch: 000 / 026 | Total loss: 5.721 | Reg loss: 0.031 | Tree loss: 5.721 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 026 | Total loss: 5.756 | Reg loss: 0.031 | Tree loss: 5.756 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 026 | Total loss: 5.698 | Reg loss: 0.031 | Tree loss: 5.698 | Accuracy: 0.064453 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 026 | Total loss: 5.670 | Reg loss: 0.031 | Tree loss: 5.670 | Accuracy: 0.078125 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 026 | Total loss: 5.683 | Reg loss: 0.031 | Tree loss: 5.683 | Accuracy: 0.064453 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 026 | Total loss: 5.630 | Reg loss: 0.031 | Tree loss: 5.630 | Accuracy: 0.085938 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 026 | Total loss: 5.607 | Reg loss: 0.031 | Tree loss: 5.607 | Accuracy: 0.080078 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 026 | Total loss: 5.590 | Reg loss: 0.031 | Tree loss: 5.590 | Accuracy: 0.072266 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 026 | Total loss: 5.603 | Reg loss: 0.031 | Tree loss: 5.603 | Accuracy: 0.097656 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 026 | Total loss: 5.564 | Reg loss: 0.031 | Tree loss: 5.564 | Accuracy: 0.093750 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 026 | Total loss: 5.544 | Reg loss: 0.031 | Tree loss: 5.544 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 026 | Total loss: 5.517 | Reg loss: 0.031 | Tree loss: 5.517 | Accuracy: 0.082031 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 026 | Total loss: 5.500 | Reg loss: 0.031 | Tree loss: 5.500 | Accuracy: 0.083984 | 0.072 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 026 | Total loss: 5.487 | Reg loss: 0.031 | Tree loss: 5.487 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 026 | Total loss: 5.456 | Reg loss: 0.032 | Tree loss: 5.456 | Accuracy: 0.074219 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 026 | Total loss: 5.435 | Reg loss: 0.032 | Tree loss: 5.435 | Accuracy: 0.072266 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 026 | Total loss: 5.439 | Reg loss: 0.032 | Tree loss: 5.439 | Accuracy: 0.080078 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 026 | Total loss: 5.368 | Reg loss: 0.032 | Tree loss: 5.368 | Accuracy: 0.095703 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 026 | Total loss: 5.424 | Reg loss: 0.032 | Tree loss: 5.424 | Accuracy: 0.072266 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 026 | Total loss: 5.370 | Reg loss: 0.032 | Tree loss: 5.370 | Accuracy: 0.097656 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 026 | Total loss: 5.332 | Reg loss: 0.032 | Tree loss: 5.332 | Accuracy: 0.066406 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 026 | Total loss: 5.299 | Reg loss: 0.032 | Tree loss: 5.299 | Accuracy: 0.089844 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 026 | Total loss: 5.281 | Reg loss: 0.033 | Tree loss: 5.281 | Accuracy: 0.074219 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 026 | Total loss: 5.293 | Reg loss: 0.033 | Tree loss: 5.293 | Accuracy: 0.058594 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 026 | Total loss: 5.287 | Reg loss: 0.033 | Tree loss: 5.287 | Accuracy: 0.060547 | 0.071 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 026 | Total loss: 5.257 | Reg loss: 0.033 | Tree loss: 5.257 | Accuracy: 0.064067 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 24 | Batch: 000 / 026 | Total loss: 5.589 | Reg loss: 0.031 | Tree loss: 5.589 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 026 | Total loss: 5.566 | Reg loss: 0.031 | Tree loss: 5.566 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 026 | Total loss: 5.546 | Reg loss: 0.031 | Tree loss: 5.546 | Accuracy: 0.080078 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 026 | Total loss: 5.497 | Reg loss: 0.031 | Tree loss: 5.497 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 026 | Total loss: 5.527 | Reg loss: 0.031 | Tree loss: 5.527 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 026 | Total loss: 5.450 | Reg loss: 0.031 | Tree loss: 5.450 | Accuracy: 0.091797 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 026 | Total loss: 5.464 | Reg loss: 0.031 | Tree loss: 5.464 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 026 | Total loss: 5.472 | Reg loss: 0.031 | Tree loss: 5.472 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 026 | Total loss: 5.438 | Reg loss: 0.032 | Tree loss: 5.438 | Accuracy: 0.076172 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 026 | Total loss: 5.382 | Reg loss: 0.032 | Tree loss: 5.382 | Accuracy: 0.101562 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 026 | Total loss: 5.380 | Reg loss: 0.032 | Tree loss: 5.380 | Accuracy: 0.089844 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 026 | Total loss: 5.354 | Reg loss: 0.032 | Tree loss: 5.354 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 026 | Total loss: 5.330 | Reg loss: 0.032 | Tree loss: 5.330 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 026 | Total loss: 5.257 | Reg loss: 0.032 | Tree loss: 5.257 | Accuracy: 0.091797 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 026 | Total loss: 5.268 | Reg loss: 0.032 | Tree loss: 5.268 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 026 | Total loss: 5.277 | Reg loss: 0.032 | Tree loss: 5.277 | Accuracy: 0.083984 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 026 | Total loss: 5.246 | Reg loss: 0.032 | Tree loss: 5.246 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 026 | Total loss: 5.249 | Reg loss: 0.032 | Tree loss: 5.249 | Accuracy: 0.072266 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 026 | Total loss: 5.282 | Reg loss: 0.032 | Tree loss: 5.282 | Accuracy: 0.048828 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 026 | Total loss: 5.230 | Reg loss: 0.033 | Tree loss: 5.230 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 026 | Total loss: 5.193 | Reg loss: 0.033 | Tree loss: 5.193 | Accuracy: 0.072266 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 026 | Total loss: 5.174 | Reg loss: 0.033 | Tree loss: 5.174 | Accuracy: 0.074219 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 026 | Total loss: 5.211 | Reg loss: 0.033 | Tree loss: 5.211 | Accuracy: 0.060547 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Batch: 023 / 026 | Total loss: 5.150 | Reg loss: 0.033 | Tree loss: 5.150 | Accuracy: 0.097656 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 026 | Total loss: 5.123 | Reg loss: 0.033 | Tree loss: 5.123 | Accuracy: 0.083984 | 0.071 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 026 | Total loss: 5.079 | Reg loss: 0.033 | Tree loss: 5.079 | Accuracy: 0.080780 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 25 | Batch: 000 / 026 | Total loss: 5.444 | Reg loss: 0.032 | Tree loss: 5.444 | Accuracy: 0.062500 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 026 | Total loss: 5.394 | Reg loss: 0.032 | Tree loss: 5.394 | Accuracy: 0.093750 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 026 | Total loss: 5.393 | Reg loss: 0.032 | Tree loss: 5.393 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 026 | Total loss: 5.365 | Reg loss: 0.032 | Tree loss: 5.365 | Accuracy: 0.074219 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 026 | Total loss: 5.324 | Reg loss: 0.032 | Tree loss: 5.324 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 026 | Total loss: 5.335 | Reg loss: 0.032 | Tree loss: 5.335 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 026 | Total loss: 5.302 | Reg loss: 0.032 | Tree loss: 5.302 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 026 | Total loss: 5.268 | Reg loss: 0.032 | Tree loss: 5.268 | Accuracy: 0.097656 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 026 | Total loss: 5.223 | Reg loss: 0.032 | Tree loss: 5.223 | Accuracy: 0.091797 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 026 | Total loss: 5.272 | Reg loss: 0.032 | Tree loss: 5.272 | Accuracy: 0.058594 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 026 | Total loss: 5.228 | Reg loss: 0.032 | Tree loss: 5.228 | Accuracy: 0.091797 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 026 | Total loss: 5.211 | Reg loss: 0.032 | Tree loss: 5.211 | Accuracy: 0.074219 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 026 | Total loss: 5.213 | Reg loss: 0.032 | Tree loss: 5.213 | Accuracy: 0.068359 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 026 | Total loss: 5.112 | Reg loss: 0.032 | Tree loss: 5.112 | Accuracy: 0.080078 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 026 | Total loss: 5.198 | Reg loss: 0.032 | Tree loss: 5.198 | Accuracy: 0.068359 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 026 | Total loss: 5.158 | Reg loss: 0.033 | Tree loss: 5.158 | Accuracy: 0.066406 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 026 | Total loss: 5.068 | Reg loss: 0.033 | Tree loss: 5.068 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 026 | Total loss: 5.094 | Reg loss: 0.033 | Tree loss: 5.094 | Accuracy: 0.082031 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 026 | Total loss: 5.046 | Reg loss: 0.033 | Tree loss: 5.046 | Accuracy: 0.066406 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 026 | Total loss: 5.114 | Reg loss: 0.033 | Tree loss: 5.114 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 026 | Total loss: 5.049 | Reg loss: 0.033 | Tree loss: 5.049 | Accuracy: 0.066406 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 026 | Total loss: 5.029 | Reg loss: 0.033 | Tree loss: 5.029 | Accuracy: 0.070312 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 026 | Total loss: 5.061 | Reg loss: 0.033 | Tree loss: 5.061 | Accuracy: 0.089844 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 026 | Total loss: 5.025 | Reg loss: 0.033 | Tree loss: 5.025 | Accuracy: 0.089844 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 026 | Total loss: 4.982 | Reg loss: 0.034 | Tree loss: 4.982 | Accuracy: 0.087891 | 0.071 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 026 | Total loss: 4.902 | Reg loss: 0.034 | Tree loss: 4.902 | Accuracy: 0.089136 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 26 | Batch: 000 / 026 | Total loss: 5.272 | Reg loss: 0.032 | Tree loss: 5.272 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 026 | Total loss: 5.235 | Reg loss: 0.032 | Tree loss: 5.235 | Accuracy: 0.085938 | 0.071 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 026 | Total loss: 5.242 | Reg loss: 0.032 | Tree loss: 5.242 | Accuracy: 0.066406 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 026 | Total loss: 5.198 | Reg loss: 0.032 | Tree loss: 5.198 | Accuracy: 0.072266 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 026 | Total loss: 5.231 | Reg loss: 0.032 | Tree loss: 5.231 | Accuracy: 0.097656 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 026 | Total loss: 5.166 | Reg loss: 0.032 | Tree loss: 5.166 | Accuracy: 0.060547 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 026 | Total loss: 5.187 | Reg loss: 0.032 | Tree loss: 5.187 | Accuracy: 0.074219 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 026 | Total loss: 5.116 | Reg loss: 0.032 | Tree loss: 5.116 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 026 | Total loss: 5.110 | Reg loss: 0.032 | Tree loss: 5.110 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 026 | Total loss: 5.106 | Reg loss: 0.032 | Tree loss: 5.106 | Accuracy: 0.072266 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 026 | Total loss: 5.123 | Reg loss: 0.032 | Tree loss: 5.123 | Accuracy: 0.056641 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 026 | Total loss: 5.074 | Reg loss: 0.033 | Tree loss: 5.074 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 026 | Total loss: 5.076 | Reg loss: 0.033 | Tree loss: 5.076 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 026 | Total loss: 4.991 | Reg loss: 0.033 | Tree loss: 4.991 | Accuracy: 0.091797 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 026 | Total loss: 5.022 | Reg loss: 0.033 | Tree loss: 5.022 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 026 | Total loss: 4.993 | Reg loss: 0.033 | Tree loss: 4.993 | Accuracy: 0.080078 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 026 | Total loss: 4.972 | Reg loss: 0.033 | Tree loss: 4.972 | Accuracy: 0.083984 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 026 | Total loss: 4.971 | Reg loss: 0.033 | Tree loss: 4.971 | Accuracy: 0.080078 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 026 | Total loss: 4.910 | Reg loss: 0.033 | Tree loss: 4.910 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 026 | Total loss: 4.905 | Reg loss: 0.033 | Tree loss: 4.905 | Accuracy: 0.083984 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 026 | Total loss: 4.868 | Reg loss: 0.033 | Tree loss: 4.868 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 026 | Total loss: 4.923 | Reg loss: 0.033 | Tree loss: 4.923 | Accuracy: 0.064453 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 026 | Total loss: 4.835 | Reg loss: 0.034 | Tree loss: 4.835 | Accuracy: 0.083984 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 026 | Total loss: 4.882 | Reg loss: 0.034 | Tree loss: 4.882 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 026 | Total loss: 4.879 | Reg loss: 0.034 | Tree loss: 4.879 | Accuracy: 0.085938 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 026 | Total loss: 4.793 | Reg loss: 0.034 | Tree loss: 4.793 | Accuracy: 0.064067 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 27 | Batch: 000 / 026 | Total loss: 5.113 | Reg loss: 0.032 | Tree loss: 5.113 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 026 | Total loss: 5.096 | Reg loss: 0.032 | Tree loss: 5.096 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 026 | Total loss: 5.123 | Reg loss: 0.032 | Tree loss: 5.123 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 026 | Total loss: 5.095 | Reg loss: 0.032 | Tree loss: 5.095 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 026 | Total loss: 5.063 | Reg loss: 0.032 | Tree loss: 5.063 | Accuracy: 0.076172 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 026 | Total loss: 4.990 | Reg loss: 0.032 | Tree loss: 4.990 | Accuracy: 0.091797 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 026 | Total loss: 5.044 | Reg loss: 0.033 | Tree loss: 5.044 | Accuracy: 0.066406 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 026 | Total loss: 5.011 | Reg loss: 0.033 | Tree loss: 5.011 | Accuracy: 0.101562 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 026 | Total loss: 4.941 | Reg loss: 0.033 | Tree loss: 4.941 | Accuracy: 0.097656 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 026 | Total loss: 4.975 | Reg loss: 0.033 | Tree loss: 4.975 | Accuracy: 0.066406 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 010 / 026 | Total loss: 4.934 | Reg loss: 0.033 | Tree loss: 4.934 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 026 | Total loss: 4.937 | Reg loss: 0.033 | Tree loss: 4.937 | Accuracy: 0.072266 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 026 | Total loss: 4.882 | Reg loss: 0.033 | Tree loss: 4.882 | Accuracy: 0.074219 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 026 | Total loss: 4.878 | Reg loss: 0.033 | Tree loss: 4.878 | Accuracy: 0.072266 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 026 | Total loss: 4.904 | Reg loss: 0.033 | Tree loss: 4.904 | Accuracy: 0.064453 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 026 | Total loss: 4.811 | Reg loss: 0.033 | Tree loss: 4.811 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 026 | Total loss: 4.825 | Reg loss: 0.033 | Tree loss: 4.825 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 026 | Total loss: 4.807 | Reg loss: 0.033 | Tree loss: 4.807 | Accuracy: 0.089844 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 026 | Total loss: 4.780 | Reg loss: 0.033 | Tree loss: 4.780 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 026 | Total loss: 4.822 | Reg loss: 0.034 | Tree loss: 4.822 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 026 | Total loss: 4.814 | Reg loss: 0.034 | Tree loss: 4.814 | Accuracy: 0.074219 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 026 | Total loss: 4.798 | Reg loss: 0.034 | Tree loss: 4.798 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 026 | Total loss: 4.741 | Reg loss: 0.034 | Tree loss: 4.741 | Accuracy: 0.058594 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 026 | Total loss: 4.749 | Reg loss: 0.034 | Tree loss: 4.749 | Accuracy: 0.070312 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 026 | Total loss: 4.700 | Reg loss: 0.034 | Tree loss: 4.700 | Accuracy: 0.076172 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 026 | Total loss: 4.714 | Reg loss: 0.034 | Tree loss: 4.714 | Accuracy: 0.058496 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 28 | Batch: 000 / 026 | Total loss: 4.942 | Reg loss: 0.033 | Tree loss: 4.942 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 026 | Total loss: 4.954 | Reg loss: 0.033 | Tree loss: 4.954 | Accuracy: 0.099609 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 026 | Total loss: 4.996 | Reg loss: 0.033 | Tree loss: 4.996 | Accuracy: 0.044922 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 026 | Total loss: 4.892 | Reg loss: 0.033 | Tree loss: 4.892 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 026 | Total loss: 4.900 | Reg loss: 0.033 | Tree loss: 4.900 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 026 | Total loss: 4.926 | Reg loss: 0.033 | Tree loss: 4.926 | Accuracy: 0.083984 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 026 | Total loss: 4.880 | Reg loss: 0.033 | Tree loss: 4.880 | Accuracy: 0.101562 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 026 | Total loss: 4.796 | Reg loss: 0.033 | Tree loss: 4.796 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 026 | Total loss: 4.839 | Reg loss: 0.033 | Tree loss: 4.839 | Accuracy: 0.080078 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 026 | Total loss: 4.829 | Reg loss: 0.033 | Tree loss: 4.829 | Accuracy: 0.085938 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 026 | Total loss: 4.769 | Reg loss: 0.033 | Tree loss: 4.769 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 026 | Total loss: 4.778 | Reg loss: 0.033 | Tree loss: 4.778 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 026 | Total loss: 4.760 | Reg loss: 0.033 | Tree loss: 4.760 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 026 | Total loss: 4.762 | Reg loss: 0.033 | Tree loss: 4.762 | Accuracy: 0.072266 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 026 | Total loss: 4.743 | Reg loss: 0.033 | Tree loss: 4.743 | Accuracy: 0.060547 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 026 | Total loss: 4.732 | Reg loss: 0.033 | Tree loss: 4.732 | Accuracy: 0.058594 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 026 | Total loss: 4.702 | Reg loss: 0.034 | Tree loss: 4.702 | Accuracy: 0.074219 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 026 | Total loss: 4.709 | Reg loss: 0.034 | Tree loss: 4.709 | Accuracy: 0.076172 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 026 | Total loss: 4.670 | Reg loss: 0.034 | Tree loss: 4.670 | Accuracy: 0.064453 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 026 | Total loss: 4.666 | Reg loss: 0.034 | Tree loss: 4.666 | Accuracy: 0.072266 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 026 | Total loss: 4.594 | Reg loss: 0.034 | Tree loss: 4.594 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 026 | Total loss: 4.664 | Reg loss: 0.034 | Tree loss: 4.664 | Accuracy: 0.060547 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 026 | Total loss: 4.610 | Reg loss: 0.034 | Tree loss: 4.610 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 026 | Total loss: 4.620 | Reg loss: 0.034 | Tree loss: 4.620 | Accuracy: 0.076172 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 026 | Total loss: 4.591 | Reg loss: 0.034 | Tree loss: 4.591 | Accuracy: 0.062500 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 026 | Total loss: 4.595 | Reg loss: 0.034 | Tree loss: 4.595 | Accuracy: 0.100279 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 29 | Batch: 000 / 026 | Total loss: 4.824 | Reg loss: 0.033 | Tree loss: 4.824 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 026 | Total loss: 4.861 | Reg loss: 0.033 | Tree loss: 4.861 | Accuracy: 0.083984 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 026 | Total loss: 4.809 | Reg loss: 0.033 | Tree loss: 4.809 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 026 | Total loss: 4.774 | Reg loss: 0.033 | Tree loss: 4.774 | Accuracy: 0.066406 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 026 | Total loss: 4.749 | Reg loss: 0.033 | Tree loss: 4.749 | Accuracy: 0.068359 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 026 | Total loss: 4.760 | Reg loss: 0.033 | Tree loss: 4.760 | Accuracy: 0.050781 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 026 | Total loss: 4.728 | Reg loss: 0.033 | Tree loss: 4.728 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 026 | Total loss: 4.674 | Reg loss: 0.033 | Tree loss: 4.674 | Accuracy: 0.080078 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 026 | Total loss: 4.727 | Reg loss: 0.033 | Tree loss: 4.727 | Accuracy: 0.068359 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 026 | Total loss: 4.664 | Reg loss: 0.033 | Tree loss: 4.664 | Accuracy: 0.085938 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 026 | Total loss: 4.641 | Reg loss: 0.033 | Tree loss: 4.641 | Accuracy: 0.083984 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 026 | Total loss: 4.662 | Reg loss: 0.033 | Tree loss: 4.662 | Accuracy: 0.066406 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 026 | Total loss: 4.631 | Reg loss: 0.034 | Tree loss: 4.631 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 026 | Total loss: 4.581 | Reg loss: 0.034 | Tree loss: 4.581 | Accuracy: 0.089844 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 026 | Total loss: 4.617 | Reg loss: 0.034 | Tree loss: 4.617 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 026 | Total loss: 4.550 | Reg loss: 0.034 | Tree loss: 4.550 | Accuracy: 0.076172 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 026 | Total loss: 4.569 | Reg loss: 0.034 | Tree loss: 4.569 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 026 | Total loss: 4.529 | Reg loss: 0.034 | Tree loss: 4.529 | Accuracy: 0.076172 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 026 | Total loss: 4.502 | Reg loss: 0.034 | Tree loss: 4.502 | Accuracy: 0.072266 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 026 | Total loss: 4.507 | Reg loss: 0.034 | Tree loss: 4.507 | Accuracy: 0.072266 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 026 | Total loss: 4.495 | Reg loss: 0.034 | Tree loss: 4.495 | Accuracy: 0.064453 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 026 | Total loss: 4.443 | Reg loss: 0.034 | Tree loss: 4.443 | Accuracy: 0.074219 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 026 | Total loss: 4.487 | Reg loss: 0.034 | Tree loss: 4.487 | Accuracy: 0.070312 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 026 | Total loss: 4.455 | Reg loss: 0.034 | Tree loss: 4.455 | Accuracy: 0.080078 | 0.069 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 026 | Total loss: 4.451 | Reg loss: 0.035 | Tree loss: 4.451 | Accuracy: 0.062500 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Batch: 025 / 026 | Total loss: 4.372 | Reg loss: 0.035 | Tree loss: 4.372 | Accuracy: 0.086351 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 30 | Batch: 000 / 026 | Total loss: 4.628 | Reg loss: 0.033 | Tree loss: 4.628 | Accuracy: 0.074219 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 026 | Total loss: 4.673 | Reg loss: 0.033 | Tree loss: 4.673 | Accuracy: 0.085938 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 026 | Total loss: 4.667 | Reg loss: 0.033 | Tree loss: 4.667 | Accuracy: 0.068359 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 026 | Total loss: 4.627 | Reg loss: 0.033 | Tree loss: 4.627 | Accuracy: 0.056641 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 026 | Total loss: 4.624 | Reg loss: 0.033 | Tree loss: 4.624 | Accuracy: 0.054688 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 026 | Total loss: 4.599 | Reg loss: 0.033 | Tree loss: 4.599 | Accuracy: 0.070312 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 026 | Total loss: 4.559 | Reg loss: 0.033 | Tree loss: 4.559 | Accuracy: 0.074219 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 026 | Total loss: 4.554 | Reg loss: 0.034 | Tree loss: 4.554 | Accuracy: 0.078125 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 026 | Total loss: 4.587 | Reg loss: 0.034 | Tree loss: 4.587 | Accuracy: 0.076172 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 026 | Total loss: 4.523 | Reg loss: 0.034 | Tree loss: 4.523 | Accuracy: 0.085938 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 026 | Total loss: 4.514 | Reg loss: 0.034 | Tree loss: 4.514 | Accuracy: 0.076172 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 026 | Total loss: 4.461 | Reg loss: 0.034 | Tree loss: 4.461 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 026 | Total loss: 4.443 | Reg loss: 0.034 | Tree loss: 4.443 | Accuracy: 0.080078 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 026 | Total loss: 4.458 | Reg loss: 0.034 | Tree loss: 4.458 | Accuracy: 0.087891 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 026 | Total loss: 4.451 | Reg loss: 0.034 | Tree loss: 4.451 | Accuracy: 0.056641 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 026 | Total loss: 4.427 | Reg loss: 0.034 | Tree loss: 4.427 | Accuracy: 0.093750 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 026 | Total loss: 4.371 | Reg loss: 0.034 | Tree loss: 4.371 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 026 | Total loss: 4.462 | Reg loss: 0.034 | Tree loss: 4.462 | Accuracy: 0.066406 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 026 | Total loss: 4.352 | Reg loss: 0.034 | Tree loss: 4.352 | Accuracy: 0.085938 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 026 | Total loss: 4.350 | Reg loss: 0.034 | Tree loss: 4.350 | Accuracy: 0.056641 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 026 | Total loss: 4.369 | Reg loss: 0.034 | Tree loss: 4.369 | Accuracy: 0.056641 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 026 | Total loss: 4.300 | Reg loss: 0.034 | Tree loss: 4.300 | Accuracy: 0.064453 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 026 | Total loss: 4.365 | Reg loss: 0.035 | Tree loss: 4.365 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 026 | Total loss: 4.314 | Reg loss: 0.035 | Tree loss: 4.314 | Accuracy: 0.078125 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 026 | Total loss: 4.304 | Reg loss: 0.035 | Tree loss: 4.304 | Accuracy: 0.078125 | 0.069 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 026 | Total loss: 4.256 | Reg loss: 0.035 | Tree loss: 4.256 | Accuracy: 0.066852 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 31 | Batch: 000 / 026 | Total loss: 4.537 | Reg loss: 0.034 | Tree loss: 4.537 | Accuracy: 0.056641 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 026 | Total loss: 4.565 | Reg loss: 0.034 | Tree loss: 4.565 | Accuracy: 0.058594 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 026 | Total loss: 4.470 | Reg loss: 0.034 | Tree loss: 4.470 | Accuracy: 0.056641 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 026 | Total loss: 4.496 | Reg loss: 0.034 | Tree loss: 4.496 | Accuracy: 0.078125 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 026 | Total loss: 4.425 | Reg loss: 0.034 | Tree loss: 4.425 | Accuracy: 0.076172 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 026 | Total loss: 4.513 | Reg loss: 0.034 | Tree loss: 4.513 | Accuracy: 0.058594 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 026 | Total loss: 4.486 | Reg loss: 0.034 | Tree loss: 4.486 | Accuracy: 0.074219 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 026 | Total loss: 4.426 | Reg loss: 0.034 | Tree loss: 4.426 | Accuracy: 0.074219 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 026 | Total loss: 4.367 | Reg loss: 0.034 | Tree loss: 4.367 | Accuracy: 0.089844 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 026 | Total loss: 4.406 | Reg loss: 0.034 | Tree loss: 4.406 | Accuracy: 0.070312 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 026 | Total loss: 4.350 | Reg loss: 0.034 | Tree loss: 4.350 | Accuracy: 0.064453 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 026 | Total loss: 4.272 | Reg loss: 0.034 | Tree loss: 4.272 | Accuracy: 0.085938 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 026 | Total loss: 4.267 | Reg loss: 0.034 | Tree loss: 4.267 | Accuracy: 0.068359 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 026 | Total loss: 4.314 | Reg loss: 0.034 | Tree loss: 4.314 | Accuracy: 0.068359 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 026 | Total loss: 4.294 | Reg loss: 0.034 | Tree loss: 4.294 | Accuracy: 0.076172 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 026 | Total loss: 4.311 | Reg loss: 0.034 | Tree loss: 4.311 | Accuracy: 0.064453 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 026 | Total loss: 4.244 | Reg loss: 0.034 | Tree loss: 4.244 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 026 | Total loss: 4.241 | Reg loss: 0.034 | Tree loss: 4.241 | Accuracy: 0.082031 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 026 | Total loss: 4.245 | Reg loss: 0.034 | Tree loss: 4.245 | Accuracy: 0.085938 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 026 | Total loss: 4.228 | Reg loss: 0.034 | Tree loss: 4.228 | Accuracy: 0.066406 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 026 | Total loss: 4.134 | Reg loss: 0.035 | Tree loss: 4.134 | Accuracy: 0.078125 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 026 | Total loss: 4.234 | Reg loss: 0.035 | Tree loss: 4.234 | Accuracy: 0.078125 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 026 | Total loss: 4.159 | Reg loss: 0.035 | Tree loss: 4.159 | Accuracy: 0.060547 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 026 | Total loss: 4.213 | Reg loss: 0.035 | Tree loss: 4.213 | Accuracy: 0.080078 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 026 | Total loss: 4.110 | Reg loss: 0.035 | Tree loss: 4.110 | Accuracy: 0.082031 | 0.069 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 026 | Total loss: 4.148 | Reg loss: 0.035 | Tree loss: 4.148 | Accuracy: 0.069638 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 32 | Batch: 000 / 026 | Total loss: 4.368 | Reg loss: 0.034 | Tree loss: 4.368 | Accuracy: 0.058594 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 026 | Total loss: 4.331 | Reg loss: 0.034 | Tree loss: 4.331 | Accuracy: 0.064453 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 026 | Total loss: 4.353 | Reg loss: 0.034 | Tree loss: 4.353 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 026 | Total loss: 4.354 | Reg loss: 0.034 | Tree loss: 4.354 | Accuracy: 0.058594 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 026 | Total loss: 4.329 | Reg loss: 0.034 | Tree loss: 4.329 | Accuracy: 0.060547 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 026 | Total loss: 4.267 | Reg loss: 0.034 | Tree loss: 4.267 | Accuracy: 0.080078 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 026 | Total loss: 4.281 | Reg loss: 0.034 | Tree loss: 4.281 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 026 | Total loss: 4.281 | Reg loss: 0.034 | Tree loss: 4.281 | Accuracy: 0.064453 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 026 | Total loss: 4.257 | Reg loss: 0.034 | Tree loss: 4.257 | Accuracy: 0.078125 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 026 | Total loss: 4.215 | Reg loss: 0.034 | Tree loss: 4.215 | Accuracy: 0.070312 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 010 / 026 | Total loss: 4.199 | Reg loss: 0.034 | Tree loss: 4.199 | Accuracy: 0.062500 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 026 | Total loss: 4.182 | Reg loss: 0.034 | Tree loss: 4.182 | Accuracy: 0.062500 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 026 | Total loss: 4.167 | Reg loss: 0.034 | Tree loss: 4.167 | Accuracy: 0.072266 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 026 | Total loss: 4.159 | Reg loss: 0.034 | Tree loss: 4.159 | Accuracy: 0.072266 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 026 | Total loss: 4.188 | Reg loss: 0.034 | Tree loss: 4.188 | Accuracy: 0.072266 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 026 | Total loss: 4.169 | Reg loss: 0.034 | Tree loss: 4.169 | Accuracy: 0.080078 | 0.069 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 026 | Total loss: 4.132 | Reg loss: 0.034 | Tree loss: 4.132 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 026 | Total loss: 4.138 | Reg loss: 0.034 | Tree loss: 4.138 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 026 | Total loss: 4.126 | Reg loss: 0.035 | Tree loss: 4.126 | Accuracy: 0.058594 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 026 | Total loss: 4.094 | Reg loss: 0.035 | Tree loss: 4.094 | Accuracy: 0.082031 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 026 | Total loss: 4.018 | Reg loss: 0.035 | Tree loss: 4.018 | Accuracy: 0.097656 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 026 | Total loss: 4.067 | Reg loss: 0.035 | Tree loss: 4.067 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 026 | Total loss: 4.006 | Reg loss: 0.035 | Tree loss: 4.006 | Accuracy: 0.091797 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 026 | Total loss: 4.064 | Reg loss: 0.035 | Tree loss: 4.064 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 026 | Total loss: 4.039 | Reg loss: 0.035 | Tree loss: 4.039 | Accuracy: 0.070312 | 0.068 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 026 | Total loss: 4.008 | Reg loss: 0.035 | Tree loss: 4.008 | Accuracy: 0.058496 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 33 | Batch: 000 / 026 | Total loss: 4.212 | Reg loss: 0.034 | Tree loss: 4.212 | Accuracy: 0.070312 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 026 | Total loss: 4.202 | Reg loss: 0.034 | Tree loss: 4.202 | Accuracy: 0.060547 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 026 | Total loss: 4.205 | Reg loss: 0.034 | Tree loss: 4.205 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 026 | Total loss: 4.169 | Reg loss: 0.034 | Tree loss: 4.169 | Accuracy: 0.056641 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 026 | Total loss: 4.138 | Reg loss: 0.034 | Tree loss: 4.138 | Accuracy: 0.076172 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 026 | Total loss: 4.154 | Reg loss: 0.034 | Tree loss: 4.154 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 026 | Total loss: 4.156 | Reg loss: 0.034 | Tree loss: 4.156 | Accuracy: 0.056641 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 026 | Total loss: 4.156 | Reg loss: 0.034 | Tree loss: 4.156 | Accuracy: 0.083984 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 026 | Total loss: 4.132 | Reg loss: 0.034 | Tree loss: 4.132 | Accuracy: 0.056641 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 026 | Total loss: 4.139 | Reg loss: 0.034 | Tree loss: 4.139 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 026 | Total loss: 3.986 | Reg loss: 0.034 | Tree loss: 3.986 | Accuracy: 0.078125 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 026 | Total loss: 4.070 | Reg loss: 0.034 | Tree loss: 4.070 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 026 | Total loss: 4.056 | Reg loss: 0.034 | Tree loss: 4.056 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 026 | Total loss: 4.036 | Reg loss: 0.034 | Tree loss: 4.036 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 026 | Total loss: 4.005 | Reg loss: 0.034 | Tree loss: 4.005 | Accuracy: 0.060547 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 026 | Total loss: 4.020 | Reg loss: 0.034 | Tree loss: 4.020 | Accuracy: 0.076172 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 026 | Total loss: 4.005 | Reg loss: 0.034 | Tree loss: 4.005 | Accuracy: 0.089844 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 026 | Total loss: 3.986 | Reg loss: 0.035 | Tree loss: 3.986 | Accuracy: 0.052734 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 026 | Total loss: 3.959 | Reg loss: 0.035 | Tree loss: 3.959 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 026 | Total loss: 4.005 | Reg loss: 0.035 | Tree loss: 4.005 | Accuracy: 0.050781 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 026 | Total loss: 3.945 | Reg loss: 0.035 | Tree loss: 3.945 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 026 | Total loss: 3.946 | Reg loss: 0.035 | Tree loss: 3.946 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 026 | Total loss: 3.891 | Reg loss: 0.035 | Tree loss: 3.891 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 026 | Total loss: 3.942 | Reg loss: 0.035 | Tree loss: 3.942 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 026 | Total loss: 3.951 | Reg loss: 0.035 | Tree loss: 3.951 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 026 | Total loss: 3.880 | Reg loss: 0.035 | Tree loss: 3.880 | Accuracy: 0.075209 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 34 | Batch: 000 / 026 | Total loss: 4.115 | Reg loss: 0.034 | Tree loss: 4.115 | Accuracy: 0.058594 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 026 | Total loss: 4.101 | Reg loss: 0.034 | Tree loss: 4.101 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 026 | Total loss: 4.075 | Reg loss: 0.034 | Tree loss: 4.075 | Accuracy: 0.085938 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 026 | Total loss: 4.099 | Reg loss: 0.034 | Tree loss: 4.099 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 026 | Total loss: 4.033 | Reg loss: 0.034 | Tree loss: 4.033 | Accuracy: 0.070312 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 026 | Total loss: 4.053 | Reg loss: 0.034 | Tree loss: 4.053 | Accuracy: 0.058594 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 026 | Total loss: 4.030 | Reg loss: 0.034 | Tree loss: 4.030 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 026 | Total loss: 3.994 | Reg loss: 0.034 | Tree loss: 3.994 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 026 | Total loss: 3.971 | Reg loss: 0.034 | Tree loss: 3.971 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 026 | Total loss: 3.952 | Reg loss: 0.034 | Tree loss: 3.952 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 026 | Total loss: 3.966 | Reg loss: 0.034 | Tree loss: 3.966 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 026 | Total loss: 3.988 | Reg loss: 0.034 | Tree loss: 3.988 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 026 | Total loss: 3.950 | Reg loss: 0.034 | Tree loss: 3.950 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 026 | Total loss: 3.897 | Reg loss: 0.034 | Tree loss: 3.897 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 026 | Total loss: 3.887 | Reg loss: 0.034 | Tree loss: 3.887 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 026 | Total loss: 3.914 | Reg loss: 0.034 | Tree loss: 3.914 | Accuracy: 0.060547 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 026 | Total loss: 3.849 | Reg loss: 0.035 | Tree loss: 3.849 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 026 | Total loss: 3.867 | Reg loss: 0.035 | Tree loss: 3.867 | Accuracy: 0.078125 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 026 | Total loss: 3.852 | Reg loss: 0.035 | Tree loss: 3.852 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 026 | Total loss: 3.837 | Reg loss: 0.035 | Tree loss: 3.837 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 026 | Total loss: 3.781 | Reg loss: 0.035 | Tree loss: 3.781 | Accuracy: 0.082031 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 026 | Total loss: 3.820 | Reg loss: 0.035 | Tree loss: 3.820 | Accuracy: 0.080078 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 022 / 026 | Total loss: 3.744 | Reg loss: 0.035 | Tree loss: 3.744 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 026 | Total loss: 3.794 | Reg loss: 0.035 | Tree loss: 3.794 | Accuracy: 0.078125 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 026 | Total loss: 3.813 | Reg loss: 0.035 | Tree loss: 3.813 | Accuracy: 0.058594 | 0.068 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 026 | Total loss: 3.816 | Reg loss: 0.035 | Tree loss: 3.816 | Accuracy: 0.052925 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 35 | Batch: 000 / 026 | Total loss: 3.980 | Reg loss: 0.034 | Tree loss: 3.980 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 026 | Total loss: 3.990 | Reg loss: 0.034 | Tree loss: 3.990 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 026 | Total loss: 3.967 | Reg loss: 0.034 | Tree loss: 3.967 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 026 | Total loss: 3.905 | Reg loss: 0.034 | Tree loss: 3.905 | Accuracy: 0.076172 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 026 | Total loss: 3.886 | Reg loss: 0.034 | Tree loss: 3.886 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 026 | Total loss: 3.939 | Reg loss: 0.034 | Tree loss: 3.939 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 026 | Total loss: 3.906 | Reg loss: 0.034 | Tree loss: 3.906 | Accuracy: 0.068359 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 026 | Total loss: 3.877 | Reg loss: 0.034 | Tree loss: 3.877 | Accuracy: 0.093750 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 026 | Total loss: 3.902 | Reg loss: 0.034 | Tree loss: 3.902 | Accuracy: 0.054688 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 026 | Total loss: 3.885 | Reg loss: 0.034 | Tree loss: 3.885 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 026 | Total loss: 3.823 | Reg loss: 0.034 | Tree loss: 3.823 | Accuracy: 0.076172 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 026 | Total loss: 3.839 | Reg loss: 0.034 | Tree loss: 3.839 | Accuracy: 0.039062 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 026 | Total loss: 3.869 | Reg loss: 0.034 | Tree loss: 3.869 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 026 | Total loss: 3.838 | Reg loss: 0.034 | Tree loss: 3.838 | Accuracy: 0.060547 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 026 | Total loss: 3.808 | Reg loss: 0.034 | Tree loss: 3.808 | Accuracy: 0.048828 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 026 | Total loss: 3.812 | Reg loss: 0.034 | Tree loss: 3.812 | Accuracy: 0.068359 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 026 | Total loss: 3.732 | Reg loss: 0.034 | Tree loss: 3.732 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 026 | Total loss: 3.777 | Reg loss: 0.035 | Tree loss: 3.777 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 026 | Total loss: 3.728 | Reg loss: 0.035 | Tree loss: 3.728 | Accuracy: 0.070312 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 026 | Total loss: 3.749 | Reg loss: 0.035 | Tree loss: 3.749 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 026 | Total loss: 3.680 | Reg loss: 0.035 | Tree loss: 3.680 | Accuracy: 0.056641 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 026 | Total loss: 3.739 | Reg loss: 0.035 | Tree loss: 3.739 | Accuracy: 0.076172 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 026 | Total loss: 3.706 | Reg loss: 0.035 | Tree loss: 3.706 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 026 | Total loss: 3.679 | Reg loss: 0.035 | Tree loss: 3.679 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 026 | Total loss: 3.650 | Reg loss: 0.035 | Tree loss: 3.650 | Accuracy: 0.070312 | 0.068 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 026 | Total loss: 3.639 | Reg loss: 0.035 | Tree loss: 3.639 | Accuracy: 0.083565 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 36 | Batch: 000 / 026 | Total loss: 3.871 | Reg loss: 0.034 | Tree loss: 3.871 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 026 | Total loss: 3.855 | Reg loss: 0.034 | Tree loss: 3.855 | Accuracy: 0.089844 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 026 | Total loss: 3.873 | Reg loss: 0.034 | Tree loss: 3.873 | Accuracy: 0.060547 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 026 | Total loss: 3.894 | Reg loss: 0.034 | Tree loss: 3.894 | Accuracy: 0.058594 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 026 | Total loss: 3.799 | Reg loss: 0.034 | Tree loss: 3.799 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 026 | Total loss: 3.782 | Reg loss: 0.034 | Tree loss: 3.782 | Accuracy: 0.078125 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 026 | Total loss: 3.786 | Reg loss: 0.034 | Tree loss: 3.786 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 026 | Total loss: 3.793 | Reg loss: 0.034 | Tree loss: 3.793 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 026 | Total loss: 3.770 | Reg loss: 0.034 | Tree loss: 3.770 | Accuracy: 0.066406 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 026 | Total loss: 3.746 | Reg loss: 0.034 | Tree loss: 3.746 | Accuracy: 0.058594 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 026 | Total loss: 3.710 | Reg loss: 0.034 | Tree loss: 3.710 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 026 | Total loss: 3.729 | Reg loss: 0.034 | Tree loss: 3.729 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 026 | Total loss: 3.693 | Reg loss: 0.034 | Tree loss: 3.693 | Accuracy: 0.062500 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 026 | Total loss: 3.726 | Reg loss: 0.034 | Tree loss: 3.726 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 026 | Total loss: 3.679 | Reg loss: 0.034 | Tree loss: 3.679 | Accuracy: 0.089844 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 026 | Total loss: 3.691 | Reg loss: 0.034 | Tree loss: 3.691 | Accuracy: 0.072266 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 026 | Total loss: 3.697 | Reg loss: 0.034 | Tree loss: 3.697 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 026 | Total loss: 3.639 | Reg loss: 0.034 | Tree loss: 3.639 | Accuracy: 0.056641 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 026 | Total loss: 3.724 | Reg loss: 0.034 | Tree loss: 3.724 | Accuracy: 0.064453 | 0.068 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 026 | Total loss: 3.646 | Reg loss: 0.035 | Tree loss: 3.646 | Accuracy: 0.066406 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 026 | Total loss: 3.606 | Reg loss: 0.035 | Tree loss: 3.606 | Accuracy: 0.062500 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 026 | Total loss: 3.663 | Reg loss: 0.035 | Tree loss: 3.663 | Accuracy: 0.074219 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 026 | Total loss: 3.595 | Reg loss: 0.035 | Tree loss: 3.595 | Accuracy: 0.060547 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 026 | Total loss: 3.558 | Reg loss: 0.035 | Tree loss: 3.558 | Accuracy: 0.076172 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 026 | Total loss: 3.608 | Reg loss: 0.035 | Tree loss: 3.608 | Accuracy: 0.062500 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 026 | Total loss: 3.575 | Reg loss: 0.035 | Tree loss: 3.575 | Accuracy: 0.061281 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 37 | Batch: 000 / 026 | Total loss: 3.783 | Reg loss: 0.034 | Tree loss: 3.783 | Accuracy: 0.078125 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 026 | Total loss: 3.752 | Reg loss: 0.034 | Tree loss: 3.752 | Accuracy: 0.089844 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 026 | Total loss: 3.745 | Reg loss: 0.034 | Tree loss: 3.745 | Accuracy: 0.097656 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 026 | Total loss: 3.742 | Reg loss: 0.034 | Tree loss: 3.742 | Accuracy: 0.087891 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 026 | Total loss: 3.685 | Reg loss: 0.034 | Tree loss: 3.685 | Accuracy: 0.085938 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 026 | Total loss: 3.696 | Reg loss: 0.034 | Tree loss: 3.696 | Accuracy: 0.072266 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 006 / 026 | Total loss: 3.777 | Reg loss: 0.034 | Tree loss: 3.777 | Accuracy: 0.082031 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 026 | Total loss: 3.722 | Reg loss: 0.034 | Tree loss: 3.722 | Accuracy: 0.080078 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 026 | Total loss: 3.681 | Reg loss: 0.034 | Tree loss: 3.681 | Accuracy: 0.103516 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 026 | Total loss: 3.683 | Reg loss: 0.034 | Tree loss: 3.683 | Accuracy: 0.111328 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 026 | Total loss: 3.644 | Reg loss: 0.034 | Tree loss: 3.644 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 026 | Total loss: 3.611 | Reg loss: 0.034 | Tree loss: 3.611 | Accuracy: 0.095703 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 026 | Total loss: 3.635 | Reg loss: 0.034 | Tree loss: 3.635 | Accuracy: 0.095703 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 026 | Total loss: 3.683 | Reg loss: 0.034 | Tree loss: 3.683 | Accuracy: 0.072266 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 026 | Total loss: 3.628 | Reg loss: 0.034 | Tree loss: 3.628 | Accuracy: 0.107422 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 026 | Total loss: 3.565 | Reg loss: 0.034 | Tree loss: 3.565 | Accuracy: 0.087891 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 026 | Total loss: 3.585 | Reg loss: 0.034 | Tree loss: 3.585 | Accuracy: 0.082031 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 026 | Total loss: 3.566 | Reg loss: 0.034 | Tree loss: 3.566 | Accuracy: 0.080078 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 026 | Total loss: 3.592 | Reg loss: 0.034 | Tree loss: 3.592 | Accuracy: 0.076172 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 026 | Total loss: 3.528 | Reg loss: 0.034 | Tree loss: 3.528 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 026 | Total loss: 3.548 | Reg loss: 0.034 | Tree loss: 3.548 | Accuracy: 0.101562 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 026 | Total loss: 3.509 | Reg loss: 0.034 | Tree loss: 3.509 | Accuracy: 0.064453 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 026 | Total loss: 3.467 | Reg loss: 0.035 | Tree loss: 3.467 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 026 | Total loss: 3.544 | Reg loss: 0.035 | Tree loss: 3.544 | Accuracy: 0.087891 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 026 | Total loss: 3.482 | Reg loss: 0.035 | Tree loss: 3.482 | Accuracy: 0.097656 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 026 | Total loss: 3.536 | Reg loss: 0.035 | Tree loss: 3.536 | Accuracy: 0.100279 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 38 | Batch: 000 / 026 | Total loss: 3.616 | Reg loss: 0.034 | Tree loss: 3.616 | Accuracy: 0.103516 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 026 | Total loss: 3.656 | Reg loss: 0.034 | Tree loss: 3.656 | Accuracy: 0.115234 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 026 | Total loss: 3.652 | Reg loss: 0.034 | Tree loss: 3.652 | Accuracy: 0.123047 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 026 | Total loss: 3.610 | Reg loss: 0.034 | Tree loss: 3.610 | Accuracy: 0.103516 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 026 | Total loss: 3.651 | Reg loss: 0.034 | Tree loss: 3.651 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 026 | Total loss: 3.605 | Reg loss: 0.034 | Tree loss: 3.605 | Accuracy: 0.119141 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 026 | Total loss: 3.628 | Reg loss: 0.034 | Tree loss: 3.628 | Accuracy: 0.113281 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 026 | Total loss: 3.610 | Reg loss: 0.034 | Tree loss: 3.610 | Accuracy: 0.097656 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 026 | Total loss: 3.556 | Reg loss: 0.034 | Tree loss: 3.556 | Accuracy: 0.121094 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 026 | Total loss: 3.643 | Reg loss: 0.034 | Tree loss: 3.643 | Accuracy: 0.082031 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 026 | Total loss: 3.612 | Reg loss: 0.034 | Tree loss: 3.612 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 026 | Total loss: 3.526 | Reg loss: 0.034 | Tree loss: 3.526 | Accuracy: 0.109375 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 026 | Total loss: 3.537 | Reg loss: 0.034 | Tree loss: 3.537 | Accuracy: 0.113281 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 026 | Total loss: 3.580 | Reg loss: 0.034 | Tree loss: 3.580 | Accuracy: 0.078125 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 026 | Total loss: 3.555 | Reg loss: 0.034 | Tree loss: 3.555 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 026 | Total loss: 3.504 | Reg loss: 0.034 | Tree loss: 3.504 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 026 | Total loss: 3.492 | Reg loss: 0.034 | Tree loss: 3.492 | Accuracy: 0.089844 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 026 | Total loss: 3.524 | Reg loss: 0.034 | Tree loss: 3.524 | Accuracy: 0.109375 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 026 | Total loss: 3.550 | Reg loss: 0.034 | Tree loss: 3.550 | Accuracy: 0.107422 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 026 | Total loss: 3.457 | Reg loss: 0.034 | Tree loss: 3.457 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 026 | Total loss: 3.514 | Reg loss: 0.034 | Tree loss: 3.514 | Accuracy: 0.101562 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 026 | Total loss: 3.426 | Reg loss: 0.034 | Tree loss: 3.426 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 026 | Total loss: 3.490 | Reg loss: 0.034 | Tree loss: 3.490 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 026 | Total loss: 3.431 | Reg loss: 0.034 | Tree loss: 3.431 | Accuracy: 0.107422 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 026 | Total loss: 3.428 | Reg loss: 0.034 | Tree loss: 3.428 | Accuracy: 0.117188 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 026 | Total loss: 3.449 | Reg loss: 0.034 | Tree loss: 3.449 | Accuracy: 0.083565 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 39 | Batch: 000 / 026 | Total loss: 3.640 | Reg loss: 0.034 | Tree loss: 3.640 | Accuracy: 0.095703 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 026 | Total loss: 3.549 | Reg loss: 0.034 | Tree loss: 3.549 | Accuracy: 0.134766 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 026 | Total loss: 3.567 | Reg loss: 0.034 | Tree loss: 3.567 | Accuracy: 0.113281 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 026 | Total loss: 3.549 | Reg loss: 0.034 | Tree loss: 3.549 | Accuracy: 0.091797 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 026 | Total loss: 3.547 | Reg loss: 0.034 | Tree loss: 3.547 | Accuracy: 0.109375 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 026 | Total loss: 3.553 | Reg loss: 0.034 | Tree loss: 3.553 | Accuracy: 0.083984 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 026 | Total loss: 3.535 | Reg loss: 0.034 | Tree loss: 3.535 | Accuracy: 0.097656 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 026 | Total loss: 3.533 | Reg loss: 0.034 | Tree loss: 3.533 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 026 | Total loss: 3.528 | Reg loss: 0.034 | Tree loss: 3.528 | Accuracy: 0.126953 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 026 | Total loss: 3.473 | Reg loss: 0.034 | Tree loss: 3.473 | Accuracy: 0.117188 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 026 | Total loss: 3.546 | Reg loss: 0.034 | Tree loss: 3.546 | Accuracy: 0.093750 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 026 | Total loss: 3.485 | Reg loss: 0.034 | Tree loss: 3.485 | Accuracy: 0.101562 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 026 | Total loss: 3.451 | Reg loss: 0.034 | Tree loss: 3.451 | Accuracy: 0.109375 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 026 | Total loss: 3.479 | Reg loss: 0.034 | Tree loss: 3.479 | Accuracy: 0.111328 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 026 | Total loss: 3.432 | Reg loss: 0.034 | Tree loss: 3.432 | Accuracy: 0.128906 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 026 | Total loss: 3.463 | Reg loss: 0.034 | Tree loss: 3.463 | Accuracy: 0.087891 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 026 | Total loss: 3.450 | Reg loss: 0.034 | Tree loss: 3.450 | Accuracy: 0.109375 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 026 | Total loss: 3.423 | Reg loss: 0.034 | Tree loss: 3.423 | Accuracy: 0.105469 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 018 / 026 | Total loss: 3.417 | Reg loss: 0.034 | Tree loss: 3.417 | Accuracy: 0.132812 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 026 | Total loss: 3.375 | Reg loss: 0.034 | Tree loss: 3.375 | Accuracy: 0.111328 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 026 | Total loss: 3.451 | Reg loss: 0.034 | Tree loss: 3.451 | Accuracy: 0.083984 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 026 | Total loss: 3.399 | Reg loss: 0.034 | Tree loss: 3.399 | Accuracy: 0.117188 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 026 | Total loss: 3.411 | Reg loss: 0.034 | Tree loss: 3.411 | Accuracy: 0.111328 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 026 | Total loss: 3.408 | Reg loss: 0.034 | Tree loss: 3.408 | Accuracy: 0.091797 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 026 | Total loss: 3.417 | Reg loss: 0.034 | Tree loss: 3.417 | Accuracy: 0.072266 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 026 | Total loss: 3.350 | Reg loss: 0.034 | Tree loss: 3.350 | Accuracy: 0.116992 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 40 | Batch: 000 / 026 | Total loss: 3.489 | Reg loss: 0.034 | Tree loss: 3.489 | Accuracy: 0.119141 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 026 | Total loss: 3.551 | Reg loss: 0.034 | Tree loss: 3.551 | Accuracy: 0.107422 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 026 | Total loss: 3.550 | Reg loss: 0.034 | Tree loss: 3.550 | Accuracy: 0.093750 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 026 | Total loss: 3.495 | Reg loss: 0.034 | Tree loss: 3.495 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 026 | Total loss: 3.516 | Reg loss: 0.034 | Tree loss: 3.516 | Accuracy: 0.113281 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 026 | Total loss: 3.438 | Reg loss: 0.034 | Tree loss: 3.438 | Accuracy: 0.136719 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 026 | Total loss: 3.473 | Reg loss: 0.034 | Tree loss: 3.473 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 026 | Total loss: 3.480 | Reg loss: 0.034 | Tree loss: 3.480 | Accuracy: 0.083984 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 026 | Total loss: 3.470 | Reg loss: 0.034 | Tree loss: 3.470 | Accuracy: 0.113281 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 026 | Total loss: 3.446 | Reg loss: 0.034 | Tree loss: 3.446 | Accuracy: 0.095703 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 026 | Total loss: 3.434 | Reg loss: 0.034 | Tree loss: 3.434 | Accuracy: 0.093750 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 026 | Total loss: 3.398 | Reg loss: 0.034 | Tree loss: 3.398 | Accuracy: 0.097656 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 026 | Total loss: 3.434 | Reg loss: 0.034 | Tree loss: 3.434 | Accuracy: 0.091797 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 026 | Total loss: 3.373 | Reg loss: 0.034 | Tree loss: 3.373 | Accuracy: 0.128906 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 026 | Total loss: 3.448 | Reg loss: 0.034 | Tree loss: 3.448 | Accuracy: 0.095703 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 026 | Total loss: 3.358 | Reg loss: 0.034 | Tree loss: 3.358 | Accuracy: 0.119141 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 026 | Total loss: 3.405 | Reg loss: 0.034 | Tree loss: 3.405 | Accuracy: 0.103516 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 026 | Total loss: 3.382 | Reg loss: 0.034 | Tree loss: 3.382 | Accuracy: 0.097656 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 026 | Total loss: 3.356 | Reg loss: 0.034 | Tree loss: 3.356 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 026 | Total loss: 3.347 | Reg loss: 0.034 | Tree loss: 3.347 | Accuracy: 0.085938 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 026 | Total loss: 3.352 | Reg loss: 0.034 | Tree loss: 3.352 | Accuracy: 0.111328 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 026 | Total loss: 3.328 | Reg loss: 0.034 | Tree loss: 3.328 | Accuracy: 0.111328 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 026 | Total loss: 3.328 | Reg loss: 0.034 | Tree loss: 3.328 | Accuracy: 0.117188 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 026 | Total loss: 3.290 | Reg loss: 0.034 | Tree loss: 3.290 | Accuracy: 0.113281 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 026 | Total loss: 3.323 | Reg loss: 0.034 | Tree loss: 3.323 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 026 | Total loss: 3.288 | Reg loss: 0.034 | Tree loss: 3.288 | Accuracy: 0.103064 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 41 | Batch: 000 / 026 | Total loss: 3.478 | Reg loss: 0.034 | Tree loss: 3.478 | Accuracy: 0.107422 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 026 | Total loss: 3.489 | Reg loss: 0.034 | Tree loss: 3.489 | Accuracy: 0.105469 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 026 | Total loss: 3.413 | Reg loss: 0.034 | Tree loss: 3.413 | Accuracy: 0.107422 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 026 | Total loss: 3.478 | Reg loss: 0.034 | Tree loss: 3.478 | Accuracy: 0.097656 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 026 | Total loss: 3.416 | Reg loss: 0.034 | Tree loss: 3.416 | Accuracy: 0.117188 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 026 | Total loss: 3.396 | Reg loss: 0.034 | Tree loss: 3.396 | Accuracy: 0.093750 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 026 | Total loss: 3.391 | Reg loss: 0.034 | Tree loss: 3.391 | Accuracy: 0.099609 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 026 | Total loss: 3.410 | Reg loss: 0.034 | Tree loss: 3.410 | Accuracy: 0.109375 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 026 | Total loss: 3.372 | Reg loss: 0.034 | Tree loss: 3.372 | Accuracy: 0.091797 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 026 | Total loss: 3.385 | Reg loss: 0.034 | Tree loss: 3.385 | Accuracy: 0.111328 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 026 | Total loss: 3.364 | Reg loss: 0.034 | Tree loss: 3.364 | Accuracy: 0.123047 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 026 | Total loss: 3.344 | Reg loss: 0.034 | Tree loss: 3.344 | Accuracy: 0.107422 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 026 | Total loss: 3.366 | Reg loss: 0.034 | Tree loss: 3.366 | Accuracy: 0.134766 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 026 | Total loss: 3.330 | Reg loss: 0.034 | Tree loss: 3.330 | Accuracy: 0.091797 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 026 | Total loss: 3.336 | Reg loss: 0.034 | Tree loss: 3.336 | Accuracy: 0.128906 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 026 | Total loss: 3.337 | Reg loss: 0.034 | Tree loss: 3.337 | Accuracy: 0.101562 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 026 | Total loss: 3.297 | Reg loss: 0.034 | Tree loss: 3.297 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 026 | Total loss: 3.319 | Reg loss: 0.034 | Tree loss: 3.319 | Accuracy: 0.105469 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 026 | Total loss: 3.293 | Reg loss: 0.034 | Tree loss: 3.293 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 026 | Total loss: 3.342 | Reg loss: 0.034 | Tree loss: 3.342 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 026 | Total loss: 3.316 | Reg loss: 0.034 | Tree loss: 3.316 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 026 | Total loss: 3.281 | Reg loss: 0.034 | Tree loss: 3.281 | Accuracy: 0.089844 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 026 | Total loss: 3.288 | Reg loss: 0.034 | Tree loss: 3.288 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 026 | Total loss: 3.310 | Reg loss: 0.034 | Tree loss: 3.310 | Accuracy: 0.072266 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 026 | Total loss: 3.256 | Reg loss: 0.034 | Tree loss: 3.256 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 026 | Total loss: 3.262 | Reg loss: 0.034 | Tree loss: 3.262 | Accuracy: 0.116992 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 42 | Batch: 000 / 026 | Total loss: 3.399 | Reg loss: 0.033 | Tree loss: 3.399 | Accuracy: 0.082031 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 026 | Total loss: 3.405 | Reg loss: 0.033 | Tree loss: 3.405 | Accuracy: 0.121094 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 026 | Total loss: 3.369 | Reg loss: 0.033 | Tree loss: 3.369 | Accuracy: 0.136719 | 0.066 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Batch: 003 / 026 | Total loss: 3.362 | Reg loss: 0.033 | Tree loss: 3.362 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 026 | Total loss: 3.412 | Reg loss: 0.033 | Tree loss: 3.412 | Accuracy: 0.087891 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 026 | Total loss: 3.340 | Reg loss: 0.033 | Tree loss: 3.340 | Accuracy: 0.095703 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 026 | Total loss: 3.375 | Reg loss: 0.033 | Tree loss: 3.375 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 026 | Total loss: 3.360 | Reg loss: 0.033 | Tree loss: 3.360 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 026 | Total loss: 3.369 | Reg loss: 0.033 | Tree loss: 3.369 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 026 | Total loss: 3.355 | Reg loss: 0.033 | Tree loss: 3.355 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 026 | Total loss: 3.306 | Reg loss: 0.033 | Tree loss: 3.306 | Accuracy: 0.136719 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 026 | Total loss: 3.333 | Reg loss: 0.033 | Tree loss: 3.333 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 026 | Total loss: 3.278 | Reg loss: 0.033 | Tree loss: 3.278 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 026 | Total loss: 3.286 | Reg loss: 0.033 | Tree loss: 3.286 | Accuracy: 0.087891 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 026 | Total loss: 3.325 | Reg loss: 0.033 | Tree loss: 3.325 | Accuracy: 0.089844 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 026 | Total loss: 3.255 | Reg loss: 0.034 | Tree loss: 3.255 | Accuracy: 0.128906 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 026 | Total loss: 3.275 | Reg loss: 0.034 | Tree loss: 3.275 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 026 | Total loss: 3.235 | Reg loss: 0.034 | Tree loss: 3.235 | Accuracy: 0.115234 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 026 | Total loss: 3.272 | Reg loss: 0.034 | Tree loss: 3.272 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 026 | Total loss: 3.256 | Reg loss: 0.034 | Tree loss: 3.256 | Accuracy: 0.091797 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 026 | Total loss: 3.251 | Reg loss: 0.034 | Tree loss: 3.251 | Accuracy: 0.095703 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 026 | Total loss: 3.220 | Reg loss: 0.034 | Tree loss: 3.220 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 026 | Total loss: 3.232 | Reg loss: 0.034 | Tree loss: 3.232 | Accuracy: 0.117188 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 026 | Total loss: 3.229 | Reg loss: 0.034 | Tree loss: 3.229 | Accuracy: 0.121094 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 026 | Total loss: 3.215 | Reg loss: 0.034 | Tree loss: 3.215 | Accuracy: 0.126953 | 0.066 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 026 | Total loss: 3.251 | Reg loss: 0.034 | Tree loss: 3.251 | Accuracy: 0.100279 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 43 | Batch: 000 / 026 | Total loss: 3.368 | Reg loss: 0.033 | Tree loss: 3.368 | Accuracy: 0.080078 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 026 | Total loss: 3.385 | Reg loss: 0.033 | Tree loss: 3.385 | Accuracy: 0.117188 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 026 | Total loss: 3.346 | Reg loss: 0.033 | Tree loss: 3.346 | Accuracy: 0.078125 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 026 | Total loss: 3.337 | Reg loss: 0.033 | Tree loss: 3.337 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 026 | Total loss: 3.300 | Reg loss: 0.033 | Tree loss: 3.300 | Accuracy: 0.103516 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 026 | Total loss: 3.329 | Reg loss: 0.033 | Tree loss: 3.329 | Accuracy: 0.134766 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 026 | Total loss: 3.301 | Reg loss: 0.033 | Tree loss: 3.301 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 026 | Total loss: 3.294 | Reg loss: 0.033 | Tree loss: 3.294 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 026 | Total loss: 3.286 | Reg loss: 0.033 | Tree loss: 3.286 | Accuracy: 0.103516 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 026 | Total loss: 3.267 | Reg loss: 0.033 | Tree loss: 3.267 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 026 | Total loss: 3.253 | Reg loss: 0.033 | Tree loss: 3.253 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 026 | Total loss: 3.272 | Reg loss: 0.033 | Tree loss: 3.272 | Accuracy: 0.130859 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 026 | Total loss: 3.197 | Reg loss: 0.033 | Tree loss: 3.197 | Accuracy: 0.128906 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 026 | Total loss: 3.278 | Reg loss: 0.033 | Tree loss: 3.278 | Accuracy: 0.119141 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 026 | Total loss: 3.230 | Reg loss: 0.033 | Tree loss: 3.230 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 026 | Total loss: 3.240 | Reg loss: 0.033 | Tree loss: 3.240 | Accuracy: 0.125000 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 026 | Total loss: 3.238 | Reg loss: 0.033 | Tree loss: 3.238 | Accuracy: 0.085938 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 026 | Total loss: 3.247 | Reg loss: 0.033 | Tree loss: 3.247 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 026 | Total loss: 3.226 | Reg loss: 0.033 | Tree loss: 3.226 | Accuracy: 0.091797 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 026 | Total loss: 3.202 | Reg loss: 0.033 | Tree loss: 3.202 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 026 | Total loss: 3.208 | Reg loss: 0.033 | Tree loss: 3.208 | Accuracy: 0.091797 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 026 | Total loss: 3.213 | Reg loss: 0.033 | Tree loss: 3.213 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 026 | Total loss: 3.211 | Reg loss: 0.034 | Tree loss: 3.211 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 026 | Total loss: 3.156 | Reg loss: 0.034 | Tree loss: 3.156 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 026 | Total loss: 3.195 | Reg loss: 0.034 | Tree loss: 3.195 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 026 | Total loss: 3.194 | Reg loss: 0.034 | Tree loss: 3.194 | Accuracy: 0.111421 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 44 | Batch: 000 / 026 | Total loss: 3.319 | Reg loss: 0.033 | Tree loss: 3.319 | Accuracy: 0.091797 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 026 | Total loss: 3.320 | Reg loss: 0.033 | Tree loss: 3.320 | Accuracy: 0.117188 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 026 | Total loss: 3.334 | Reg loss: 0.033 | Tree loss: 3.334 | Accuracy: 0.105469 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 026 | Total loss: 3.262 | Reg loss: 0.033 | Tree loss: 3.262 | Accuracy: 0.130859 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 026 | Total loss: 3.280 | Reg loss: 0.033 | Tree loss: 3.280 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 026 | Total loss: 3.267 | Reg loss: 0.033 | Tree loss: 3.267 | Accuracy: 0.105469 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 026 | Total loss: 3.265 | Reg loss: 0.033 | Tree loss: 3.265 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 026 | Total loss: 3.270 | Reg loss: 0.033 | Tree loss: 3.270 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 026 | Total loss: 3.250 | Reg loss: 0.033 | Tree loss: 3.250 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 026 | Total loss: 3.299 | Reg loss: 0.033 | Tree loss: 3.299 | Accuracy: 0.080078 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 026 | Total loss: 3.207 | Reg loss: 0.033 | Tree loss: 3.207 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 026 | Total loss: 3.207 | Reg loss: 0.033 | Tree loss: 3.207 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 026 | Total loss: 3.246 | Reg loss: 0.033 | Tree loss: 3.246 | Accuracy: 0.083984 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 026 | Total loss: 3.205 | Reg loss: 0.033 | Tree loss: 3.205 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 026 | Total loss: 3.216 | Reg loss: 0.033 | Tree loss: 3.216 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 026 | Total loss: 3.193 | Reg loss: 0.033 | Tree loss: 3.193 | Accuracy: 0.119141 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 026 | Total loss: 3.172 | Reg loss: 0.033 | Tree loss: 3.172 | Accuracy: 0.126953 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 026 | Total loss: 3.198 | Reg loss: 0.033 | Tree loss: 3.198 | Accuracy: 0.111328 | 0.066 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 018 / 026 | Total loss: 3.180 | Reg loss: 0.033 | Tree loss: 3.180 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 026 | Total loss: 3.139 | Reg loss: 0.033 | Tree loss: 3.139 | Accuracy: 0.117188 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 026 | Total loss: 3.167 | Reg loss: 0.033 | Tree loss: 3.167 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 026 | Total loss: 3.138 | Reg loss: 0.033 | Tree loss: 3.138 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 026 | Total loss: 3.144 | Reg loss: 0.033 | Tree loss: 3.144 | Accuracy: 0.085938 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 026 | Total loss: 3.146 | Reg loss: 0.033 | Tree loss: 3.146 | Accuracy: 0.103516 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 026 | Total loss: 3.151 | Reg loss: 0.033 | Tree loss: 3.151 | Accuracy: 0.119141 | 0.066 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 026 | Total loss: 3.128 | Reg loss: 0.033 | Tree loss: 3.128 | Accuracy: 0.097493 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 45 | Batch: 000 / 026 | Total loss: 3.271 | Reg loss: 0.033 | Tree loss: 3.271 | Accuracy: 0.085938 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 026 | Total loss: 3.242 | Reg loss: 0.033 | Tree loss: 3.242 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 026 | Total loss: 3.252 | Reg loss: 0.033 | Tree loss: 3.252 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 026 | Total loss: 3.293 | Reg loss: 0.033 | Tree loss: 3.293 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 026 | Total loss: 3.229 | Reg loss: 0.033 | Tree loss: 3.229 | Accuracy: 0.128906 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 026 | Total loss: 3.230 | Reg loss: 0.033 | Tree loss: 3.230 | Accuracy: 0.128906 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 026 | Total loss: 3.233 | Reg loss: 0.033 | Tree loss: 3.233 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 026 | Total loss: 3.223 | Reg loss: 0.033 | Tree loss: 3.223 | Accuracy: 0.095703 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 026 | Total loss: 3.228 | Reg loss: 0.033 | Tree loss: 3.228 | Accuracy: 0.134766 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 026 | Total loss: 3.232 | Reg loss: 0.033 | Tree loss: 3.232 | Accuracy: 0.105469 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 026 | Total loss: 3.227 | Reg loss: 0.033 | Tree loss: 3.227 | Accuracy: 0.087891 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 026 | Total loss: 3.189 | Reg loss: 0.033 | Tree loss: 3.189 | Accuracy: 0.125000 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 026 | Total loss: 3.168 | Reg loss: 0.033 | Tree loss: 3.168 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 026 | Total loss: 3.181 | Reg loss: 0.033 | Tree loss: 3.181 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 026 | Total loss: 3.180 | Reg loss: 0.033 | Tree loss: 3.180 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 026 | Total loss: 3.155 | Reg loss: 0.033 | Tree loss: 3.155 | Accuracy: 0.083984 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 026 | Total loss: 3.158 | Reg loss: 0.033 | Tree loss: 3.158 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 026 | Total loss: 3.151 | Reg loss: 0.033 | Tree loss: 3.151 | Accuracy: 0.103516 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 026 | Total loss: 3.170 | Reg loss: 0.033 | Tree loss: 3.170 | Accuracy: 0.087891 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 026 | Total loss: 3.144 | Reg loss: 0.033 | Tree loss: 3.144 | Accuracy: 0.095703 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 026 | Total loss: 3.132 | Reg loss: 0.033 | Tree loss: 3.132 | Accuracy: 0.103516 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 026 | Total loss: 3.142 | Reg loss: 0.033 | Tree loss: 3.142 | Accuracy: 0.089844 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 026 | Total loss: 3.099 | Reg loss: 0.033 | Tree loss: 3.099 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 026 | Total loss: 3.096 | Reg loss: 0.033 | Tree loss: 3.096 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 026 | Total loss: 3.063 | Reg loss: 0.033 | Tree loss: 3.063 | Accuracy: 0.123047 | 0.066 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 026 | Total loss: 3.066 | Reg loss: 0.033 | Tree loss: 3.066 | Accuracy: 0.114206 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 46 | Batch: 000 / 026 | Total loss: 3.207 | Reg loss: 0.033 | Tree loss: 3.207 | Accuracy: 0.119141 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 026 | Total loss: 3.254 | Reg loss: 0.033 | Tree loss: 3.254 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 026 | Total loss: 3.209 | Reg loss: 0.033 | Tree loss: 3.209 | Accuracy: 0.123047 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 026 | Total loss: 3.166 | Reg loss: 0.033 | Tree loss: 3.166 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 026 | Total loss: 3.206 | Reg loss: 0.033 | Tree loss: 3.206 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 026 | Total loss: 3.218 | Reg loss: 0.033 | Tree loss: 3.218 | Accuracy: 0.089844 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 026 | Total loss: 3.225 | Reg loss: 0.033 | Tree loss: 3.225 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 026 | Total loss: 3.179 | Reg loss: 0.033 | Tree loss: 3.179 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 026 | Total loss: 3.212 | Reg loss: 0.033 | Tree loss: 3.212 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 026 | Total loss: 3.144 | Reg loss: 0.033 | Tree loss: 3.144 | Accuracy: 0.083984 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 026 | Total loss: 3.143 | Reg loss: 0.033 | Tree loss: 3.143 | Accuracy: 0.142578 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 026 | Total loss: 3.165 | Reg loss: 0.033 | Tree loss: 3.165 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 026 | Total loss: 3.142 | Reg loss: 0.033 | Tree loss: 3.142 | Accuracy: 0.105469 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 026 | Total loss: 3.135 | Reg loss: 0.033 | Tree loss: 3.135 | Accuracy: 0.128906 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 026 | Total loss: 3.129 | Reg loss: 0.033 | Tree loss: 3.129 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 026 | Total loss: 3.181 | Reg loss: 0.033 | Tree loss: 3.181 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 026 | Total loss: 3.132 | Reg loss: 0.033 | Tree loss: 3.132 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 026 | Total loss: 3.144 | Reg loss: 0.033 | Tree loss: 3.144 | Accuracy: 0.091797 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 026 | Total loss: 3.104 | Reg loss: 0.033 | Tree loss: 3.104 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 026 | Total loss: 3.105 | Reg loss: 0.033 | Tree loss: 3.105 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 026 | Total loss: 3.137 | Reg loss: 0.033 | Tree loss: 3.137 | Accuracy: 0.072266 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 026 | Total loss: 3.043 | Reg loss: 0.033 | Tree loss: 3.043 | Accuracy: 0.132812 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 026 | Total loss: 3.101 | Reg loss: 0.033 | Tree loss: 3.101 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 026 | Total loss: 3.090 | Reg loss: 0.033 | Tree loss: 3.090 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 026 | Total loss: 3.077 | Reg loss: 0.033 | Tree loss: 3.077 | Accuracy: 0.105469 | 0.066 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 026 | Total loss: 3.080 | Reg loss: 0.033 | Tree loss: 3.080 | Accuracy: 0.114206 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 47 | Batch: 000 / 026 | Total loss: 3.207 | Reg loss: 0.033 | Tree loss: 3.207 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 026 | Total loss: 3.211 | Reg loss: 0.033 | Tree loss: 3.211 | Accuracy: 0.095703 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 026 | Total loss: 3.161 | Reg loss: 0.033 | Tree loss: 3.161 | Accuracy: 0.119141 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 026 | Total loss: 3.222 | Reg loss: 0.033 | Tree loss: 3.222 | Accuracy: 0.107422 | 0.066 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 004 / 026 | Total loss: 3.171 | Reg loss: 0.033 | Tree loss: 3.171 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 026 | Total loss: 3.183 | Reg loss: 0.033 | Tree loss: 3.183 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 026 | Total loss: 3.163 | Reg loss: 0.033 | Tree loss: 3.163 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 026 | Total loss: 3.153 | Reg loss: 0.033 | Tree loss: 3.153 | Accuracy: 0.109375 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 026 | Total loss: 3.158 | Reg loss: 0.033 | Tree loss: 3.158 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 026 | Total loss: 3.143 | Reg loss: 0.033 | Tree loss: 3.143 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 026 | Total loss: 3.128 | Reg loss: 0.033 | Tree loss: 3.128 | Accuracy: 0.121094 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 026 | Total loss: 3.112 | Reg loss: 0.033 | Tree loss: 3.112 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 026 | Total loss: 3.101 | Reg loss: 0.033 | Tree loss: 3.101 | Accuracy: 0.117188 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 026 | Total loss: 3.125 | Reg loss: 0.033 | Tree loss: 3.125 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 026 | Total loss: 3.102 | Reg loss: 0.033 | Tree loss: 3.102 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 026 | Total loss: 3.127 | Reg loss: 0.033 | Tree loss: 3.127 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 026 | Total loss: 3.102 | Reg loss: 0.033 | Tree loss: 3.102 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 026 | Total loss: 3.084 | Reg loss: 0.033 | Tree loss: 3.084 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 026 | Total loss: 3.028 | Reg loss: 0.033 | Tree loss: 3.028 | Accuracy: 0.138672 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 026 | Total loss: 3.097 | Reg loss: 0.033 | Tree loss: 3.097 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 026 | Total loss: 3.088 | Reg loss: 0.033 | Tree loss: 3.088 | Accuracy: 0.103516 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 026 | Total loss: 3.085 | Reg loss: 0.033 | Tree loss: 3.085 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 026 | Total loss: 3.065 | Reg loss: 0.033 | Tree loss: 3.065 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 026 | Total loss: 3.068 | Reg loss: 0.033 | Tree loss: 3.068 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 026 | Total loss: 3.063 | Reg loss: 0.033 | Tree loss: 3.063 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 026 | Total loss: 3.000 | Reg loss: 0.033 | Tree loss: 3.000 | Accuracy: 0.133705 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 48 | Batch: 000 / 026 | Total loss: 3.205 | Reg loss: 0.032 | Tree loss: 3.205 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 026 | Total loss: 3.147 | Reg loss: 0.032 | Tree loss: 3.147 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 026 | Total loss: 3.161 | Reg loss: 0.032 | Tree loss: 3.161 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 026 | Total loss: 3.164 | Reg loss: 0.032 | Tree loss: 3.164 | Accuracy: 0.097656 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 026 | Total loss: 3.186 | Reg loss: 0.032 | Tree loss: 3.186 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 026 | Total loss: 3.126 | Reg loss: 0.032 | Tree loss: 3.126 | Accuracy: 0.121094 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 026 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.093750 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 026 | Total loss: 3.123 | Reg loss: 0.032 | Tree loss: 3.123 | Accuracy: 0.103516 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 026 | Total loss: 3.081 | Reg loss: 0.032 | Tree loss: 3.081 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 026 | Total loss: 3.111 | Reg loss: 0.032 | Tree loss: 3.111 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 026 | Total loss: 3.141 | Reg loss: 0.032 | Tree loss: 3.141 | Accuracy: 0.095703 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 026 | Total loss: 3.072 | Reg loss: 0.032 | Tree loss: 3.072 | Accuracy: 0.115234 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 026 | Total loss: 3.102 | Reg loss: 0.032 | Tree loss: 3.102 | Accuracy: 0.115234 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 026 | Total loss: 3.136 | Reg loss: 0.033 | Tree loss: 3.136 | Accuracy: 0.089844 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 026 | Total loss: 3.093 | Reg loss: 0.033 | Tree loss: 3.093 | Accuracy: 0.101562 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 026 | Total loss: 3.083 | Reg loss: 0.033 | Tree loss: 3.083 | Accuracy: 0.113281 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 026 | Total loss: 3.060 | Reg loss: 0.033 | Tree loss: 3.060 | Accuracy: 0.107422 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 026 | Total loss: 3.053 | Reg loss: 0.033 | Tree loss: 3.053 | Accuracy: 0.111328 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 026 | Total loss: 3.079 | Reg loss: 0.033 | Tree loss: 3.079 | Accuracy: 0.115234 | 0.066 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 026 | Total loss: 3.076 | Reg loss: 0.033 | Tree loss: 3.076 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 026 | Total loss: 3.065 | Reg loss: 0.033 | Tree loss: 3.065 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 026 | Total loss: 3.060 | Reg loss: 0.033 | Tree loss: 3.060 | Accuracy: 0.093750 | 0.065 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 026 | Total loss: 3.025 | Reg loss: 0.033 | Tree loss: 3.025 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 026 | Total loss: 3.023 | Reg loss: 0.033 | Tree loss: 3.023 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 026 | Total loss: 2.997 | Reg loss: 0.033 | Tree loss: 2.997 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 026 | Total loss: 2.983 | Reg loss: 0.033 | Tree loss: 2.983 | Accuracy: 0.150418 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 49 | Batch: 000 / 026 | Total loss: 3.178 | Reg loss: 0.032 | Tree loss: 3.178 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 026 | Total loss: 3.150 | Reg loss: 0.032 | Tree loss: 3.150 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 026 | Total loss: 3.144 | Reg loss: 0.032 | Tree loss: 3.144 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 026 | Total loss: 3.151 | Reg loss: 0.032 | Tree loss: 3.151 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 026 | Total loss: 3.125 | Reg loss: 0.032 | Tree loss: 3.125 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 026 | Total loss: 3.111 | Reg loss: 0.032 | Tree loss: 3.111 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 026 | Total loss: 3.136 | Reg loss: 0.032 | Tree loss: 3.136 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 026 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 026 | Total loss: 3.119 | Reg loss: 0.032 | Tree loss: 3.119 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 026 | Total loss: 3.105 | Reg loss: 0.032 | Tree loss: 3.105 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 026 | Total loss: 3.105 | Reg loss: 0.032 | Tree loss: 3.105 | Accuracy: 0.085938 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 026 | Total loss: 3.093 | Reg loss: 0.032 | Tree loss: 3.093 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 026 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.140625 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.093750 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 026 | Total loss: 3.077 | Reg loss: 0.032 | Tree loss: 3.077 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 026 | Total loss: 3.025 | Reg loss: 0.032 | Tree loss: 3.025 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 026 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.126953 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 017 / 026 | Total loss: 3.053 | Reg loss: 0.032 | Tree loss: 3.053 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 026 | Total loss: 3.019 | Reg loss: 0.032 | Tree loss: 3.019 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 026 | Total loss: 3.037 | Reg loss: 0.032 | Tree loss: 3.037 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 026 | Total loss: 3.022 | Reg loss: 0.033 | Tree loss: 3.022 | Accuracy: 0.130859 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 026 | Total loss: 3.007 | Reg loss: 0.033 | Tree loss: 3.007 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 026 | Total loss: 3.004 | Reg loss: 0.033 | Tree loss: 3.004 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 026 | Total loss: 3.010 | Reg loss: 0.033 | Tree loss: 3.010 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 026 | Total loss: 3.017 | Reg loss: 0.033 | Tree loss: 3.017 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 026 | Total loss: 2.980 | Reg loss: 0.033 | Tree loss: 2.980 | Accuracy: 0.116992 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 50 | Batch: 000 / 026 | Total loss: 3.136 | Reg loss: 0.032 | Tree loss: 3.136 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 026 | Total loss: 3.126 | Reg loss: 0.032 | Tree loss: 3.126 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 026 | Total loss: 3.094 | Reg loss: 0.032 | Tree loss: 3.094 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 026 | Total loss: 3.124 | Reg loss: 0.032 | Tree loss: 3.124 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 026 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 026 | Total loss: 3.102 | Reg loss: 0.032 | Tree loss: 3.102 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 026 | Total loss: 3.096 | Reg loss: 0.032 | Tree loss: 3.096 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 026 | Total loss: 3.082 | Reg loss: 0.032 | Tree loss: 3.082 | Accuracy: 0.128906 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 026 | Total loss: 3.093 | Reg loss: 0.032 | Tree loss: 3.093 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 026 | Total loss: 3.074 | Reg loss: 0.032 | Tree loss: 3.074 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 026 | Total loss: 3.062 | Reg loss: 0.032 | Tree loss: 3.062 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 026 | Total loss: 3.036 | Reg loss: 0.032 | Tree loss: 3.036 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 026 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.099609 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 026 | Total loss: 3.029 | Reg loss: 0.032 | Tree loss: 3.029 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 026 | Total loss: 3.028 | Reg loss: 0.032 | Tree loss: 3.028 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 026 | Total loss: 3.013 | Reg loss: 0.032 | Tree loss: 3.013 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 026 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.130859 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 026 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 026 | Total loss: 2.986 | Reg loss: 0.032 | Tree loss: 2.986 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 026 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 026 | Total loss: 3.012 | Reg loss: 0.032 | Tree loss: 3.012 | Accuracy: 0.103064 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 51 | Batch: 000 / 026 | Total loss: 3.190 | Reg loss: 0.032 | Tree loss: 3.190 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 026 | Total loss: 3.096 | Reg loss: 0.032 | Tree loss: 3.096 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 026 | Total loss: 3.107 | Reg loss: 0.032 | Tree loss: 3.107 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 026 | Total loss: 3.070 | Reg loss: 0.032 | Tree loss: 3.070 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 026 | Total loss: 3.128 | Reg loss: 0.032 | Tree loss: 3.128 | Accuracy: 0.089844 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 026 | Total loss: 3.120 | Reg loss: 0.032 | Tree loss: 3.120 | Accuracy: 0.085938 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 026 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.138672 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 026 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 026 | Total loss: 3.099 | Reg loss: 0.032 | Tree loss: 3.099 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 026 | Total loss: 3.005 | Reg loss: 0.032 | Tree loss: 3.005 | Accuracy: 0.136719 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 026 | Total loss: 3.053 | Reg loss: 0.032 | Tree loss: 3.053 | Accuracy: 0.130859 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.097656 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 026 | Total loss: 2.991 | Reg loss: 0.032 | Tree loss: 2.991 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 026 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 026 | Total loss: 2.992 | Reg loss: 0.032 | Tree loss: 2.992 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 026 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 026 | Total loss: 3.005 | Reg loss: 0.032 | Tree loss: 3.005 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 026 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.099609 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.146484 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 026 | Total loss: 2.983 | Reg loss: 0.032 | Tree loss: 2.983 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 026 | Total loss: 3.001 | Reg loss: 0.032 | Tree loss: 3.001 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 026 | Total loss: 2.953 | Reg loss: 0.032 | Tree loss: 2.953 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 026 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 026 | Total loss: 2.963 | Reg loss: 0.032 | Tree loss: 2.963 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 026 | Total loss: 2.983 | Reg loss: 0.032 | Tree loss: 2.983 | Accuracy: 0.128134 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 52 | Batch: 000 / 026 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 026 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.107422 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Batch: 002 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 026 | Total loss: 3.116 | Reg loss: 0.032 | Tree loss: 3.116 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 026 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 026 | Total loss: 3.050 | Reg loss: 0.032 | Tree loss: 3.050 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 026 | Total loss: 3.067 | Reg loss: 0.032 | Tree loss: 3.067 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 026 | Total loss: 3.042 | Reg loss: 0.032 | Tree loss: 3.042 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 026 | Total loss: 3.044 | Reg loss: 0.032 | Tree loss: 3.044 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 026 | Total loss: 3.022 | Reg loss: 0.032 | Tree loss: 3.022 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 026 | Total loss: 2.996 | Reg loss: 0.032 | Tree loss: 2.996 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 026 | Total loss: 2.983 | Reg loss: 0.032 | Tree loss: 2.983 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 026 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.130859 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 026 | Total loss: 2.965 | Reg loss: 0.032 | Tree loss: 2.965 | Accuracy: 0.150391 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 026 | Total loss: 2.961 | Reg loss: 0.032 | Tree loss: 2.961 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 026 | Total loss: 2.981 | Reg loss: 0.032 | Tree loss: 2.981 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 026 | Total loss: 2.952 | Reg loss: 0.032 | Tree loss: 2.952 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 026 | Total loss: 2.961 | Reg loss: 0.032 | Tree loss: 2.961 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 026 | Total loss: 2.990 | Reg loss: 0.032 | Tree loss: 2.990 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 026 | Total loss: 2.984 | Reg loss: 0.032 | Tree loss: 2.984 | Accuracy: 0.089136 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 53 | Batch: 000 / 026 | Total loss: 3.095 | Reg loss: 0.032 | Tree loss: 3.095 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 026 | Total loss: 3.035 | Reg loss: 0.032 | Tree loss: 3.035 | Accuracy: 0.146484 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 026 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 026 | Total loss: 3.084 | Reg loss: 0.032 | Tree loss: 3.084 | Accuracy: 0.099609 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 026 | Total loss: 3.043 | Reg loss: 0.032 | Tree loss: 3.043 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 026 | Total loss: 3.042 | Reg loss: 0.032 | Tree loss: 3.042 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 026 | Total loss: 3.031 | Reg loss: 0.032 | Tree loss: 3.031 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 026 | Total loss: 3.009 | Reg loss: 0.032 | Tree loss: 3.009 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.144531 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 026 | Total loss: 3.017 | Reg loss: 0.032 | Tree loss: 3.017 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 026 | Total loss: 3.007 | Reg loss: 0.032 | Tree loss: 3.007 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 026 | Total loss: 3.001 | Reg loss: 0.032 | Tree loss: 3.001 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 026 | Total loss: 3.002 | Reg loss: 0.032 | Tree loss: 3.002 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 026 | Total loss: 2.976 | Reg loss: 0.032 | Tree loss: 2.976 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 026 | Total loss: 2.995 | Reg loss: 0.032 | Tree loss: 2.995 | Accuracy: 0.097656 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 026 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.136719 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 026 | Total loss: 2.955 | Reg loss: 0.032 | Tree loss: 2.955 | Accuracy: 0.134766 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 026 | Total loss: 2.960 | Reg loss: 0.032 | Tree loss: 2.960 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.091797 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.140625 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 026 | Total loss: 2.955 | Reg loss: 0.032 | Tree loss: 2.955 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 026 | Total loss: 2.941 | Reg loss: 0.032 | Tree loss: 2.941 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 026 | Total loss: 2.931 | Reg loss: 0.032 | Tree loss: 2.931 | Accuracy: 0.116992 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 54 | Batch: 000 / 026 | Total loss: 3.024 | Reg loss: 0.032 | Tree loss: 3.024 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 026 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.093750 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 026 | Total loss: 3.062 | Reg loss: 0.032 | Tree loss: 3.062 | Accuracy: 0.089844 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 026 | Total loss: 3.052 | Reg loss: 0.032 | Tree loss: 3.052 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 026 | Total loss: 3.023 | Reg loss: 0.032 | Tree loss: 3.023 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 026 | Total loss: 3.033 | Reg loss: 0.032 | Tree loss: 3.033 | Accuracy: 0.099609 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 026 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 026 | Total loss: 3.012 | Reg loss: 0.032 | Tree loss: 3.012 | Accuracy: 0.134766 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 026 | Total loss: 2.999 | Reg loss: 0.032 | Tree loss: 2.999 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 026 | Total loss: 3.006 | Reg loss: 0.032 | Tree loss: 3.006 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 026 | Total loss: 3.003 | Reg loss: 0.032 | Tree loss: 3.003 | Accuracy: 0.091797 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 014 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 026 | Total loss: 2.947 | Reg loss: 0.032 | Tree loss: 2.947 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 026 | Total loss: 2.967 | Reg loss: 0.032 | Tree loss: 2.967 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 026 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 026 | Total loss: 2.949 | Reg loss: 0.032 | Tree loss: 2.949 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 026 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.164062 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 026 | Total loss: 2.971 | Reg loss: 0.032 | Tree loss: 2.971 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 026 | Total loss: 2.969 | Reg loss: 0.032 | Tree loss: 2.969 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 026 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 026 | Total loss: 2.907 | Reg loss: 0.032 | Tree loss: 2.907 | Accuracy: 0.116992 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 55 | Batch: 000 / 026 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 026 | Total loss: 3.076 | Reg loss: 0.032 | Tree loss: 3.076 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 026 | Total loss: 3.030 | Reg loss: 0.032 | Tree loss: 3.030 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 026 | Total loss: 3.040 | Reg loss: 0.032 | Tree loss: 3.040 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 026 | Total loss: 3.027 | Reg loss: 0.032 | Tree loss: 3.027 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 026 | Total loss: 3.044 | Reg loss: 0.032 | Tree loss: 3.044 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 026 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 026 | Total loss: 3.010 | Reg loss: 0.032 | Tree loss: 3.010 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 026 | Total loss: 2.980 | Reg loss: 0.032 | Tree loss: 2.980 | Accuracy: 0.128906 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 026 | Total loss: 2.971 | Reg loss: 0.032 | Tree loss: 2.971 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 026 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 026 | Total loss: 2.955 | Reg loss: 0.032 | Tree loss: 2.955 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 026 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.140625 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 026 | Total loss: 2.968 | Reg loss: 0.032 | Tree loss: 2.968 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 026 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 026 | Total loss: 2.949 | Reg loss: 0.032 | Tree loss: 2.949 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 026 | Total loss: 2.900 | Reg loss: 0.032 | Tree loss: 2.900 | Accuracy: 0.138672 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 026 | Total loss: 2.961 | Reg loss: 0.032 | Tree loss: 2.961 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 026 | Total loss: 2.904 | Reg loss: 0.032 | Tree loss: 2.904 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.093750 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 026 | Total loss: 2.907 | Reg loss: 0.032 | Tree loss: 2.907 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 026 | Total loss: 2.913 | Reg loss: 0.032 | Tree loss: 2.913 | Accuracy: 0.133705 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 56 | Batch: 000 / 026 | Total loss: 3.002 | Reg loss: 0.032 | Tree loss: 3.002 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 026 | Total loss: 2.989 | Reg loss: 0.032 | Tree loss: 2.989 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 026 | Total loss: 2.999 | Reg loss: 0.032 | Tree loss: 2.999 | Accuracy: 0.130859 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 026 | Total loss: 3.032 | Reg loss: 0.032 | Tree loss: 3.032 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 026 | Total loss: 3.017 | Reg loss: 0.032 | Tree loss: 3.017 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 026 | Total loss: 3.024 | Reg loss: 0.032 | Tree loss: 3.024 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 026 | Total loss: 2.978 | Reg loss: 0.032 | Tree loss: 2.978 | Accuracy: 0.128906 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 026 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.136719 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 026 | Total loss: 2.964 | Reg loss: 0.032 | Tree loss: 2.964 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 026 | Total loss: 2.977 | Reg loss: 0.032 | Tree loss: 2.977 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 026 | Total loss: 2.923 | Reg loss: 0.032 | Tree loss: 2.923 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 026 | Total loss: 2.972 | Reg loss: 0.032 | Tree loss: 2.972 | Accuracy: 0.125000 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 026 | Total loss: 2.966 | Reg loss: 0.032 | Tree loss: 2.966 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 026 | Total loss: 2.962 | Reg loss: 0.032 | Tree loss: 2.962 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 026 | Total loss: 2.949 | Reg loss: 0.032 | Tree loss: 2.949 | Accuracy: 0.138672 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 026 | Total loss: 2.950 | Reg loss: 0.032 | Tree loss: 2.950 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.144531 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 026 | Total loss: 2.904 | Reg loss: 0.032 | Tree loss: 2.904 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 026 | Total loss: 2.904 | Reg loss: 0.032 | Tree loss: 2.904 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 026 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 026 | Total loss: 2.927 | Reg loss: 0.032 | Tree loss: 2.927 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 026 | Total loss: 2.937 | Reg loss: 0.032 | Tree loss: 2.937 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 026 | Total loss: 2.946 | Reg loss: 0.032 | Tree loss: 2.946 | Accuracy: 0.097656 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 026 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 026 | Total loss: 2.894 | Reg loss: 0.032 | Tree loss: 2.894 | Accuracy: 0.114206 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 57 | Batch: 000 / 026 | Total loss: 3.029 | Reg loss: 0.031 | Tree loss: 3.029 | Accuracy: 0.083984 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 026 | Total loss: 3.002 | Reg loss: 0.031 | Tree loss: 3.002 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 026 | Total loss: 3.002 | Reg loss: 0.031 | Tree loss: 3.002 | Accuracy: 0.082031 | 0.065 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 003 / 026 | Total loss: 2.989 | Reg loss: 0.031 | Tree loss: 2.989 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 026 | Total loss: 2.997 | Reg loss: 0.031 | Tree loss: 2.997 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 026 | Total loss: 3.005 | Reg loss: 0.031 | Tree loss: 3.005 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 026 | Total loss: 2.970 | Reg loss: 0.031 | Tree loss: 2.970 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 026 | Total loss: 2.997 | Reg loss: 0.031 | Tree loss: 2.997 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 026 | Total loss: 2.984 | Reg loss: 0.031 | Tree loss: 2.984 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 026 | Total loss: 2.983 | Reg loss: 0.031 | Tree loss: 2.983 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 026 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 026 | Total loss: 2.932 | Reg loss: 0.032 | Tree loss: 2.932 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 026 | Total loss: 2.944 | Reg loss: 0.032 | Tree loss: 2.944 | Accuracy: 0.130859 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 026 | Total loss: 2.939 | Reg loss: 0.032 | Tree loss: 2.939 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 026 | Total loss: 2.924 | Reg loss: 0.032 | Tree loss: 2.924 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 026 | Total loss: 2.919 | Reg loss: 0.032 | Tree loss: 2.919 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 026 | Total loss: 2.914 | Reg loss: 0.032 | Tree loss: 2.914 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 026 | Total loss: 2.925 | Reg loss: 0.032 | Tree loss: 2.925 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 026 | Total loss: 2.879 | Reg loss: 0.032 | Tree loss: 2.879 | Accuracy: 0.144531 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 026 | Total loss: 2.905 | Reg loss: 0.032 | Tree loss: 2.905 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 026 | Total loss: 2.916 | Reg loss: 0.032 | Tree loss: 2.916 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 026 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 026 | Total loss: 2.896 | Reg loss: 0.032 | Tree loss: 2.896 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 026 | Total loss: 2.885 | Reg loss: 0.032 | Tree loss: 2.885 | Accuracy: 0.134766 | 0.065 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 026 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.116992 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 58 | Batch: 000 / 026 | Total loss: 2.968 | Reg loss: 0.031 | Tree loss: 2.968 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 026 | Total loss: 3.036 | Reg loss: 0.031 | Tree loss: 3.036 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 026 | Total loss: 3.014 | Reg loss: 0.031 | Tree loss: 3.014 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 026 | Total loss: 2.996 | Reg loss: 0.031 | Tree loss: 2.996 | Accuracy: 0.117188 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 026 | Total loss: 2.970 | Reg loss: 0.031 | Tree loss: 2.970 | Accuracy: 0.107422 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 026 | Total loss: 2.978 | Reg loss: 0.031 | Tree loss: 2.978 | Accuracy: 0.111328 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 026 | Total loss: 2.984 | Reg loss: 0.031 | Tree loss: 2.984 | Accuracy: 0.123047 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 026 | Total loss: 2.980 | Reg loss: 0.031 | Tree loss: 2.980 | Accuracy: 0.115234 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 026 | Total loss: 2.972 | Reg loss: 0.031 | Tree loss: 2.972 | Accuracy: 0.095703 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 026 | Total loss: 2.974 | Reg loss: 0.031 | Tree loss: 2.974 | Accuracy: 0.103516 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 026 | Total loss: 2.958 | Reg loss: 0.031 | Tree loss: 2.958 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 026 | Total loss: 2.906 | Reg loss: 0.031 | Tree loss: 2.906 | Accuracy: 0.142578 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 026 | Total loss: 2.920 | Reg loss: 0.031 | Tree loss: 2.920 | Accuracy: 0.105469 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 026 | Total loss: 2.932 | Reg loss: 0.031 | Tree loss: 2.932 | Accuracy: 0.126953 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.099609 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 026 | Total loss: 2.929 | Reg loss: 0.032 | Tree loss: 2.929 | Accuracy: 0.144531 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 026 | Total loss: 2.862 | Reg loss: 0.032 | Tree loss: 2.862 | Accuracy: 0.121094 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 026 | Total loss: 2.948 | Reg loss: 0.032 | Tree loss: 2.948 | Accuracy: 0.097656 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 026 | Total loss: 2.921 | Reg loss: 0.032 | Tree loss: 2.921 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 026 | Total loss: 2.915 | Reg loss: 0.032 | Tree loss: 2.915 | Accuracy: 0.113281 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 026 | Total loss: 2.874 | Reg loss: 0.032 | Tree loss: 2.874 | Accuracy: 0.132812 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 026 | Total loss: 2.908 | Reg loss: 0.032 | Tree loss: 2.908 | Accuracy: 0.093750 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 026 | Total loss: 2.897 | Reg loss: 0.032 | Tree loss: 2.897 | Accuracy: 0.091797 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 026 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.101562 | 0.065 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 026 | Total loss: 2.860 | Reg loss: 0.032 | Tree loss: 2.860 | Accuracy: 0.152344 | 0.064 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 026 | Total loss: 2.892 | Reg loss: 0.032 | Tree loss: 2.892 | Accuracy: 0.122563 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 59 | Batch: 000 / 026 | Total loss: 2.984 | Reg loss: 0.031 | Tree loss: 2.984 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 026 | Total loss: 3.007 | Reg loss: 0.031 | Tree loss: 3.007 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 026 | Total loss: 2.954 | Reg loss: 0.031 | Tree loss: 2.954 | Accuracy: 0.130859 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 026 | Total loss: 2.965 | Reg loss: 0.031 | Tree loss: 2.965 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 026 | Total loss: 2.953 | Reg loss: 0.031 | Tree loss: 2.953 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 026 | Total loss: 2.962 | Reg loss: 0.031 | Tree loss: 2.962 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 026 | Total loss: 2.972 | Reg loss: 0.031 | Tree loss: 2.972 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 026 | Total loss: 2.982 | Reg loss: 0.031 | Tree loss: 2.982 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 026 | Total loss: 2.949 | Reg loss: 0.031 | Tree loss: 2.949 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 026 | Total loss: 2.945 | Reg loss: 0.031 | Tree loss: 2.945 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 026 | Total loss: 2.954 | Reg loss: 0.031 | Tree loss: 2.954 | Accuracy: 0.091797 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 026 | Total loss: 2.914 | Reg loss: 0.031 | Tree loss: 2.914 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 026 | Total loss: 2.935 | Reg loss: 0.031 | Tree loss: 2.935 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 026 | Total loss: 2.937 | Reg loss: 0.031 | Tree loss: 2.937 | Accuracy: 0.138672 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 026 | Total loss: 2.915 | Reg loss: 0.031 | Tree loss: 2.915 | Accuracy: 0.125000 | 0.064 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Batch: 015 / 026 | Total loss: 2.919 | Reg loss: 0.031 | Tree loss: 2.919 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 026 | Total loss: 2.890 | Reg loss: 0.031 | Tree loss: 2.890 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 026 | Total loss: 2.922 | Reg loss: 0.031 | Tree loss: 2.922 | Accuracy: 0.099609 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 026 | Total loss: 2.937 | Reg loss: 0.031 | Tree loss: 2.937 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 026 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 026 | Total loss: 2.881 | Reg loss: 0.032 | Tree loss: 2.881 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 026 | Total loss: 2.874 | Reg loss: 0.032 | Tree loss: 2.874 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 026 | Total loss: 2.890 | Reg loss: 0.032 | Tree loss: 2.890 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 026 | Total loss: 2.888 | Reg loss: 0.032 | Tree loss: 2.888 | Accuracy: 0.101562 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 026 | Total loss: 2.886 | Reg loss: 0.032 | Tree loss: 2.886 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 026 | Total loss: 2.821 | Reg loss: 0.032 | Tree loss: 2.821 | Accuracy: 0.130919 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 60 | Batch: 000 / 026 | Total loss: 2.975 | Reg loss: 0.031 | Tree loss: 2.975 | Accuracy: 0.099609 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 026 | Total loss: 2.988 | Reg loss: 0.031 | Tree loss: 2.988 | Accuracy: 0.117188 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 026 | Total loss: 2.953 | Reg loss: 0.031 | Tree loss: 2.953 | Accuracy: 0.154297 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 026 | Total loss: 2.950 | Reg loss: 0.031 | Tree loss: 2.950 | Accuracy: 0.144531 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 026 | Total loss: 2.972 | Reg loss: 0.031 | Tree loss: 2.972 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 026 | Total loss: 2.974 | Reg loss: 0.031 | Tree loss: 2.974 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 026 | Total loss: 2.977 | Reg loss: 0.031 | Tree loss: 2.977 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 026 | Total loss: 2.955 | Reg loss: 0.031 | Tree loss: 2.955 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 026 | Total loss: 2.935 | Reg loss: 0.031 | Tree loss: 2.935 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 026 | Total loss: 2.939 | Reg loss: 0.031 | Tree loss: 2.939 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 026 | Total loss: 2.940 | Reg loss: 0.031 | Tree loss: 2.940 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 026 | Total loss: 2.911 | Reg loss: 0.031 | Tree loss: 2.911 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 026 | Total loss: 2.933 | Reg loss: 0.031 | Tree loss: 2.933 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 026 | Total loss: 2.933 | Reg loss: 0.031 | Tree loss: 2.933 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 026 | Total loss: 2.917 | Reg loss: 0.031 | Tree loss: 2.917 | Accuracy: 0.117188 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 026 | Total loss: 2.885 | Reg loss: 0.031 | Tree loss: 2.885 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 026 | Total loss: 2.883 | Reg loss: 0.031 | Tree loss: 2.883 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 026 | Total loss: 2.897 | Reg loss: 0.031 | Tree loss: 2.897 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 026 | Total loss: 2.921 | Reg loss: 0.031 | Tree loss: 2.921 | Accuracy: 0.089844 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 026 | Total loss: 2.883 | Reg loss: 0.031 | Tree loss: 2.883 | Accuracy: 0.089844 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 026 | Total loss: 2.843 | Reg loss: 0.032 | Tree loss: 2.843 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 026 | Total loss: 2.883 | Reg loss: 0.032 | Tree loss: 2.883 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 026 | Total loss: 2.879 | Reg loss: 0.032 | Tree loss: 2.879 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 026 | Total loss: 2.866 | Reg loss: 0.032 | Tree loss: 2.866 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 026 | Total loss: 2.820 | Reg loss: 0.032 | Tree loss: 2.820 | Accuracy: 0.142061 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 61 | Batch: 000 / 026 | Total loss: 3.002 | Reg loss: 0.031 | Tree loss: 3.002 | Accuracy: 0.091797 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 026 | Total loss: 2.959 | Reg loss: 0.031 | Tree loss: 2.959 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 026 | Total loss: 2.991 | Reg loss: 0.031 | Tree loss: 2.991 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 026 | Total loss: 2.960 | Reg loss: 0.031 | Tree loss: 2.960 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 026 | Total loss: 2.948 | Reg loss: 0.031 | Tree loss: 2.948 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 026 | Total loss: 2.966 | Reg loss: 0.031 | Tree loss: 2.966 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 026 | Total loss: 2.908 | Reg loss: 0.031 | Tree loss: 2.908 | Accuracy: 0.130859 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 026 | Total loss: 2.935 | Reg loss: 0.031 | Tree loss: 2.935 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 026 | Total loss: 2.919 | Reg loss: 0.031 | Tree loss: 2.919 | Accuracy: 0.134766 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 026 | Total loss: 2.923 | Reg loss: 0.031 | Tree loss: 2.923 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 026 | Total loss: 2.932 | Reg loss: 0.031 | Tree loss: 2.932 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 026 | Total loss: 2.898 | Reg loss: 0.031 | Tree loss: 2.898 | Accuracy: 0.099609 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 026 | Total loss: 2.891 | Reg loss: 0.031 | Tree loss: 2.891 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 026 | Total loss: 2.883 | Reg loss: 0.031 | Tree loss: 2.883 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 026 | Total loss: 2.918 | Reg loss: 0.031 | Tree loss: 2.918 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 026 | Total loss: 2.878 | Reg loss: 0.031 | Tree loss: 2.878 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 026 | Total loss: 2.926 | Reg loss: 0.031 | Tree loss: 2.926 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 026 | Total loss: 2.908 | Reg loss: 0.031 | Tree loss: 2.908 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 026 | Total loss: 2.862 | Reg loss: 0.031 | Tree loss: 2.862 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 026 | Total loss: 2.868 | Reg loss: 0.031 | Tree loss: 2.868 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 026 | Total loss: 2.849 | Reg loss: 0.031 | Tree loss: 2.849 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 026 | Total loss: 2.866 | Reg loss: 0.031 | Tree loss: 2.866 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 026 | Total loss: 2.826 | Reg loss: 0.032 | Tree loss: 2.826 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 026 | Total loss: 2.842 | Reg loss: 0.032 | Tree loss: 2.842 | Accuracy: 0.091922 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 62 | Batch: 000 / 026 | Total loss: 2.982 | Reg loss: 0.031 | Tree loss: 2.982 | Accuracy: 0.132812 | 0.064 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 001 / 026 | Total loss: 2.981 | Reg loss: 0.031 | Tree loss: 2.981 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 026 | Total loss: 2.969 | Reg loss: 0.031 | Tree loss: 2.969 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 026 | Total loss: 2.958 | Reg loss: 0.031 | Tree loss: 2.958 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 026 | Total loss: 2.945 | Reg loss: 0.031 | Tree loss: 2.945 | Accuracy: 0.136719 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 026 | Total loss: 2.939 | Reg loss: 0.031 | Tree loss: 2.939 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 026 | Total loss: 2.960 | Reg loss: 0.031 | Tree loss: 2.960 | Accuracy: 0.087891 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 026 | Total loss: 2.920 | Reg loss: 0.031 | Tree loss: 2.920 | Accuracy: 0.136719 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 026 | Total loss: 2.929 | Reg loss: 0.031 | Tree loss: 2.929 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 026 | Total loss: 2.937 | Reg loss: 0.031 | Tree loss: 2.937 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 026 | Total loss: 2.883 | Reg loss: 0.031 | Tree loss: 2.883 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 026 | Total loss: 2.893 | Reg loss: 0.031 | Tree loss: 2.893 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 026 | Total loss: 2.901 | Reg loss: 0.031 | Tree loss: 2.901 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 026 | Total loss: 2.895 | Reg loss: 0.031 | Tree loss: 2.895 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 026 | Total loss: 2.886 | Reg loss: 0.031 | Tree loss: 2.886 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 026 | Total loss: 2.894 | Reg loss: 0.031 | Tree loss: 2.894 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 026 | Total loss: 2.878 | Reg loss: 0.031 | Tree loss: 2.878 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 026 | Total loss: 2.877 | Reg loss: 0.031 | Tree loss: 2.877 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 026 | Total loss: 2.881 | Reg loss: 0.031 | Tree loss: 2.881 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 026 | Total loss: 2.860 | Reg loss: 0.031 | Tree loss: 2.860 | Accuracy: 0.101562 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 026 | Total loss: 2.872 | Reg loss: 0.031 | Tree loss: 2.872 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 026 | Total loss: 2.852 | Reg loss: 0.031 | Tree loss: 2.852 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 026 | Total loss: 2.914 | Reg loss: 0.031 | Tree loss: 2.914 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.117188 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 026 | Total loss: 2.833 | Reg loss: 0.031 | Tree loss: 2.833 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 026 | Total loss: 2.839 | Reg loss: 0.031 | Tree loss: 2.839 | Accuracy: 0.122563 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 63 | Batch: 000 / 026 | Total loss: 2.937 | Reg loss: 0.031 | Tree loss: 2.937 | Accuracy: 0.152344 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 026 | Total loss: 2.964 | Reg loss: 0.031 | Tree loss: 2.964 | Accuracy: 0.130859 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 026 | Total loss: 2.942 | Reg loss: 0.031 | Tree loss: 2.942 | Accuracy: 0.091797 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 026 | Total loss: 2.899 | Reg loss: 0.031 | Tree loss: 2.899 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 026 | Total loss: 2.912 | Reg loss: 0.031 | Tree loss: 2.912 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 026 | Total loss: 2.922 | Reg loss: 0.031 | Tree loss: 2.922 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 026 | Total loss: 2.927 | Reg loss: 0.031 | Tree loss: 2.927 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 026 | Total loss: 2.899 | Reg loss: 0.031 | Tree loss: 2.899 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 026 | Total loss: 2.923 | Reg loss: 0.031 | Tree loss: 2.923 | Accuracy: 0.089844 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 026 | Total loss: 2.924 | Reg loss: 0.031 | Tree loss: 2.924 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 026 | Total loss: 2.904 | Reg loss: 0.031 | Tree loss: 2.904 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 026 | Total loss: 2.868 | Reg loss: 0.031 | Tree loss: 2.868 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 026 | Total loss: 2.892 | Reg loss: 0.031 | Tree loss: 2.892 | Accuracy: 0.136719 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 026 | Total loss: 2.895 | Reg loss: 0.031 | Tree loss: 2.895 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 026 | Total loss: 2.864 | Reg loss: 0.031 | Tree loss: 2.864 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 026 | Total loss: 2.883 | Reg loss: 0.031 | Tree loss: 2.883 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 026 | Total loss: 2.882 | Reg loss: 0.031 | Tree loss: 2.882 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 026 | Total loss: 2.880 | Reg loss: 0.031 | Tree loss: 2.880 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 026 | Total loss: 2.852 | Reg loss: 0.031 | Tree loss: 2.852 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 026 | Total loss: 2.843 | Reg loss: 0.031 | Tree loss: 2.843 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 026 | Total loss: 2.855 | Reg loss: 0.031 | Tree loss: 2.855 | Accuracy: 0.094708 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 64 | Batch: 000 / 026 | Total loss: 2.945 | Reg loss: 0.031 | Tree loss: 2.945 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 026 | Total loss: 2.970 | Reg loss: 0.031 | Tree loss: 2.970 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 026 | Total loss: 2.946 | Reg loss: 0.031 | Tree loss: 2.946 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 026 | Total loss: 2.917 | Reg loss: 0.031 | Tree loss: 2.917 | Accuracy: 0.142578 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 026 | Total loss: 2.891 | Reg loss: 0.031 | Tree loss: 2.891 | Accuracy: 0.142578 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 026 | Total loss: 2.910 | Reg loss: 0.031 | Tree loss: 2.910 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 026 | Total loss: 2.932 | Reg loss: 0.031 | Tree loss: 2.932 | Accuracy: 0.144531 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 026 | Total loss: 2.947 | Reg loss: 0.031 | Tree loss: 2.947 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 026 | Total loss: 2.907 | Reg loss: 0.031 | Tree loss: 2.907 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 026 | Total loss: 2.892 | Reg loss: 0.031 | Tree loss: 2.892 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.089844 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 026 | Total loss: 2.874 | Reg loss: 0.031 | Tree loss: 2.874 | Accuracy: 0.101562 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 026 | Total loss: 2.867 | Reg loss: 0.031 | Tree loss: 2.867 | Accuracy: 0.134766 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 026 | Total loss: 2.891 | Reg loss: 0.031 | Tree loss: 2.891 | Accuracy: 0.103516 | 0.064 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Batch: 015 / 026 | Total loss: 2.872 | Reg loss: 0.031 | Tree loss: 2.872 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 026 | Total loss: 2.874 | Reg loss: 0.031 | Tree loss: 2.874 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 026 | Total loss: 2.909 | Reg loss: 0.031 | Tree loss: 2.909 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 026 | Total loss: 2.872 | Reg loss: 0.031 | Tree loss: 2.872 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 026 | Total loss: 2.864 | Reg loss: 0.031 | Tree loss: 2.864 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 026 | Total loss: 2.836 | Reg loss: 0.031 | Tree loss: 2.836 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 026 | Total loss: 2.850 | Reg loss: 0.031 | Tree loss: 2.850 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 026 | Total loss: 2.813 | Reg loss: 0.031 | Tree loss: 2.813 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 026 | Total loss: 2.820 | Reg loss: 0.031 | Tree loss: 2.820 | Accuracy: 0.111421 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 65 | Batch: 000 / 026 | Total loss: 2.963 | Reg loss: 0.031 | Tree loss: 2.963 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 026 | Total loss: 2.946 | Reg loss: 0.031 | Tree loss: 2.946 | Accuracy: 0.136719 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 026 | Total loss: 2.949 | Reg loss: 0.031 | Tree loss: 2.949 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 026 | Total loss: 2.929 | Reg loss: 0.031 | Tree loss: 2.929 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 026 | Total loss: 2.936 | Reg loss: 0.031 | Tree loss: 2.936 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 026 | Total loss: 2.943 | Reg loss: 0.031 | Tree loss: 2.943 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 026 | Total loss: 2.894 | Reg loss: 0.031 | Tree loss: 2.894 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 026 | Total loss: 2.920 | Reg loss: 0.031 | Tree loss: 2.920 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 026 | Total loss: 2.946 | Reg loss: 0.031 | Tree loss: 2.946 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.089844 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 026 | Total loss: 2.890 | Reg loss: 0.031 | Tree loss: 2.890 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 026 | Total loss: 2.890 | Reg loss: 0.031 | Tree loss: 2.890 | Accuracy: 0.152344 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 026 | Total loss: 2.897 | Reg loss: 0.031 | Tree loss: 2.897 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 026 | Total loss: 2.853 | Reg loss: 0.031 | Tree loss: 2.853 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 026 | Total loss: 2.850 | Reg loss: 0.031 | Tree loss: 2.850 | Accuracy: 0.146484 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 026 | Total loss: 2.845 | Reg loss: 0.031 | Tree loss: 2.845 | Accuracy: 0.150391 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 026 | Total loss: 2.873 | Reg loss: 0.031 | Tree loss: 2.873 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 026 | Total loss: 2.877 | Reg loss: 0.031 | Tree loss: 2.877 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 026 | Total loss: 2.836 | Reg loss: 0.031 | Tree loss: 2.836 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 026 | Total loss: 2.827 | Reg loss: 0.031 | Tree loss: 2.827 | Accuracy: 0.134766 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 026 | Total loss: 2.851 | Reg loss: 0.031 | Tree loss: 2.851 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 026 | Total loss: 2.847 | Reg loss: 0.031 | Tree loss: 2.847 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 026 | Total loss: 2.860 | Reg loss: 0.031 | Tree loss: 2.860 | Accuracy: 0.066406 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 026 | Total loss: 2.827 | Reg loss: 0.031 | Tree loss: 2.827 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 026 | Total loss: 2.808 | Reg loss: 0.031 | Tree loss: 2.808 | Accuracy: 0.136490 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 66 | Batch: 000 / 026 | Total loss: 2.940 | Reg loss: 0.031 | Tree loss: 2.940 | Accuracy: 0.144531 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 026 | Total loss: 2.954 | Reg loss: 0.031 | Tree loss: 2.954 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 026 | Total loss: 2.906 | Reg loss: 0.031 | Tree loss: 2.906 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 026 | Total loss: 2.944 | Reg loss: 0.031 | Tree loss: 2.944 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 026 | Total loss: 2.946 | Reg loss: 0.031 | Tree loss: 2.946 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 026 | Total loss: 2.902 | Reg loss: 0.031 | Tree loss: 2.902 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 026 | Total loss: 2.889 | Reg loss: 0.031 | Tree loss: 2.889 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 026 | Total loss: 2.903 | Reg loss: 0.031 | Tree loss: 2.903 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.148438 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 026 | Total loss: 2.890 | Reg loss: 0.031 | Tree loss: 2.890 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 026 | Total loss: 2.912 | Reg loss: 0.031 | Tree loss: 2.912 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 026 | Total loss: 2.877 | Reg loss: 0.031 | Tree loss: 2.877 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 026 | Total loss: 2.910 | Reg loss: 0.031 | Tree loss: 2.910 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 026 | Total loss: 2.879 | Reg loss: 0.031 | Tree loss: 2.879 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 026 | Total loss: 2.838 | Reg loss: 0.031 | Tree loss: 2.838 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 026 | Total loss: 2.856 | Reg loss: 0.031 | Tree loss: 2.856 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.117188 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 026 | Total loss: 2.854 | Reg loss: 0.031 | Tree loss: 2.854 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 026 | Total loss: 2.837 | Reg loss: 0.031 | Tree loss: 2.837 | Accuracy: 0.130859 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 026 | Total loss: 2.831 | Reg loss: 0.031 | Tree loss: 2.831 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 026 | Total loss: 2.840 | Reg loss: 0.031 | Tree loss: 2.840 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 026 | Total loss: 2.793 | Reg loss: 0.031 | Tree loss: 2.793 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 026 | Total loss: 2.834 | Reg loss: 0.031 | Tree loss: 2.834 | Accuracy: 0.097493 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 67 | Batch: 000 / 026 | Total loss: 2.934 | Reg loss: 0.031 | Tree loss: 2.934 | Accuracy: 0.111328 | 0.064 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 001 / 026 | Total loss: 2.940 | Reg loss: 0.031 | Tree loss: 2.940 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 026 | Total loss: 2.959 | Reg loss: 0.031 | Tree loss: 2.959 | Accuracy: 0.087891 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 026 | Total loss: 2.877 | Reg loss: 0.031 | Tree loss: 2.877 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 026 | Total loss: 2.949 | Reg loss: 0.031 | Tree loss: 2.949 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 026 | Total loss: 2.889 | Reg loss: 0.031 | Tree loss: 2.889 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 026 | Total loss: 2.897 | Reg loss: 0.031 | Tree loss: 2.897 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 026 | Total loss: 2.926 | Reg loss: 0.031 | Tree loss: 2.926 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 026 | Total loss: 2.890 | Reg loss: 0.031 | Tree loss: 2.890 | Accuracy: 0.140625 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 026 | Total loss: 2.890 | Reg loss: 0.031 | Tree loss: 2.890 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 026 | Total loss: 2.892 | Reg loss: 0.031 | Tree loss: 2.892 | Accuracy: 0.134766 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 026 | Total loss: 2.876 | Reg loss: 0.031 | Tree loss: 2.876 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 026 | Total loss: 2.867 | Reg loss: 0.031 | Tree loss: 2.867 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 026 | Total loss: 2.873 | Reg loss: 0.031 | Tree loss: 2.873 | Accuracy: 0.091797 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 026 | Total loss: 2.871 | Reg loss: 0.031 | Tree loss: 2.871 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 026 | Total loss: 2.858 | Reg loss: 0.031 | Tree loss: 2.858 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 026 | Total loss: 2.852 | Reg loss: 0.031 | Tree loss: 2.852 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 026 | Total loss: 2.840 | Reg loss: 0.031 | Tree loss: 2.840 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 026 | Total loss: 2.844 | Reg loss: 0.031 | Tree loss: 2.844 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 026 | Total loss: 2.861 | Reg loss: 0.031 | Tree loss: 2.861 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 026 | Total loss: 2.811 | Reg loss: 0.031 | Tree loss: 2.811 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 026 | Total loss: 2.821 | Reg loss: 0.031 | Tree loss: 2.821 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 026 | Total loss: 2.804 | Reg loss: 0.031 | Tree loss: 2.804 | Accuracy: 0.105850 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 68 | Batch: 000 / 026 | Total loss: 2.922 | Reg loss: 0.031 | Tree loss: 2.922 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 026 | Total loss: 2.938 | Reg loss: 0.031 | Tree loss: 2.938 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 026 | Total loss: 2.916 | Reg loss: 0.031 | Tree loss: 2.916 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 026 | Total loss: 2.938 | Reg loss: 0.031 | Tree loss: 2.938 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 026 | Total loss: 2.887 | Reg loss: 0.031 | Tree loss: 2.887 | Accuracy: 0.117188 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 026 | Total loss: 2.903 | Reg loss: 0.031 | Tree loss: 2.903 | Accuracy: 0.087891 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 026 | Total loss: 2.887 | Reg loss: 0.031 | Tree loss: 2.887 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 026 | Total loss: 2.893 | Reg loss: 0.031 | Tree loss: 2.893 | Accuracy: 0.140625 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 026 | Total loss: 2.886 | Reg loss: 0.031 | Tree loss: 2.886 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 026 | Total loss: 2.861 | Reg loss: 0.031 | Tree loss: 2.861 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 026 | Total loss: 2.849 | Reg loss: 0.031 | Tree loss: 2.849 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 026 | Total loss: 2.856 | Reg loss: 0.031 | Tree loss: 2.856 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 026 | Total loss: 2.827 | Reg loss: 0.031 | Tree loss: 2.827 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 026 | Total loss: 2.851 | Reg loss: 0.031 | Tree loss: 2.851 | Accuracy: 0.136719 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 026 | Total loss: 2.840 | Reg loss: 0.031 | Tree loss: 2.840 | Accuracy: 0.138672 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 026 | Total loss: 2.828 | Reg loss: 0.031 | Tree loss: 2.828 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 026 | Total loss: 2.882 | Reg loss: 0.031 | Tree loss: 2.882 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 026 | Total loss: 2.836 | Reg loss: 0.031 | Tree loss: 2.836 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 026 | Total loss: 2.834 | Reg loss: 0.031 | Tree loss: 2.834 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 026 | Total loss: 2.827 | Reg loss: 0.031 | Tree loss: 2.827 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 026 | Total loss: 2.829 | Reg loss: 0.031 | Tree loss: 2.829 | Accuracy: 0.089136 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 69 | Batch: 000 / 026 | Total loss: 2.958 | Reg loss: 0.031 | Tree loss: 2.958 | Accuracy: 0.087891 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 026 | Total loss: 2.920 | Reg loss: 0.031 | Tree loss: 2.920 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 026 | Total loss: 2.911 | Reg loss: 0.031 | Tree loss: 2.911 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 026 | Total loss: 2.902 | Reg loss: 0.031 | Tree loss: 2.902 | Accuracy: 0.117188 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.128906 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 026 | Total loss: 2.900 | Reg loss: 0.031 | Tree loss: 2.900 | Accuracy: 0.103516 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 026 | Total loss: 2.899 | Reg loss: 0.031 | Tree loss: 2.899 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 026 | Total loss: 2.873 | Reg loss: 0.031 | Tree loss: 2.873 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 026 | Total loss: 2.883 | Reg loss: 0.031 | Tree loss: 2.883 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 026 | Total loss: 2.864 | Reg loss: 0.031 | Tree loss: 2.864 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 026 | Total loss: 2.876 | Reg loss: 0.031 | Tree loss: 2.876 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.101562 | 0.064 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 013 / 026 | Total loss: 2.868 | Reg loss: 0.031 | Tree loss: 2.868 | Accuracy: 0.146484 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 026 | Total loss: 2.867 | Reg loss: 0.031 | Tree loss: 2.867 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 026 | Total loss: 2.826 | Reg loss: 0.031 | Tree loss: 2.826 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 026 | Total loss: 2.836 | Reg loss: 0.031 | Tree loss: 2.836 | Accuracy: 0.101562 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 026 | Total loss: 2.856 | Reg loss: 0.031 | Tree loss: 2.856 | Accuracy: 0.097656 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 026 | Total loss: 2.877 | Reg loss: 0.031 | Tree loss: 2.877 | Accuracy: 0.093750 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 026 | Total loss: 2.819 | Reg loss: 0.031 | Tree loss: 2.819 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 026 | Total loss: 2.807 | Reg loss: 0.031 | Tree loss: 2.807 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 026 | Total loss: 2.803 | Reg loss: 0.031 | Tree loss: 2.803 | Accuracy: 0.138672 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 026 | Total loss: 2.806 | Reg loss: 0.031 | Tree loss: 2.806 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 026 | Total loss: 2.845 | Reg loss: 0.031 | Tree loss: 2.845 | Accuracy: 0.130859 | 0.064 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 026 | Total loss: 2.793 | Reg loss: 0.031 | Tree loss: 2.793 | Accuracy: 0.103064 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 70 | Batch: 000 / 026 | Total loss: 2.914 | Reg loss: 0.031 | Tree loss: 2.914 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 026 | Total loss: 2.911 | Reg loss: 0.031 | Tree loss: 2.911 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 026 | Total loss: 2.937 | Reg loss: 0.031 | Tree loss: 2.937 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 026 | Total loss: 2.918 | Reg loss: 0.031 | Tree loss: 2.918 | Accuracy: 0.101562 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 026 | Total loss: 2.889 | Reg loss: 0.031 | Tree loss: 2.889 | Accuracy: 0.121094 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 026 | Total loss: 2.889 | Reg loss: 0.031 | Tree loss: 2.889 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 026 | Total loss: 2.875 | Reg loss: 0.031 | Tree loss: 2.875 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 026 | Total loss: 2.873 | Reg loss: 0.031 | Tree loss: 2.873 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 026 | Total loss: 2.850 | Reg loss: 0.031 | Tree loss: 2.850 | Accuracy: 0.144531 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 026 | Total loss: 2.873 | Reg loss: 0.031 | Tree loss: 2.873 | Accuracy: 0.132812 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 026 | Total loss: 2.871 | Reg loss: 0.031 | Tree loss: 2.871 | Accuracy: 0.119141 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 026 | Total loss: 2.846 | Reg loss: 0.031 | Tree loss: 2.846 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 026 | Total loss: 2.862 | Reg loss: 0.031 | Tree loss: 2.862 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 026 | Total loss: 2.840 | Reg loss: 0.031 | Tree loss: 2.840 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 026 | Total loss: 2.858 | Reg loss: 0.031 | Tree loss: 2.858 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 026 | Total loss: 2.836 | Reg loss: 0.031 | Tree loss: 2.836 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 026 | Total loss: 2.826 | Reg loss: 0.031 | Tree loss: 2.826 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 026 | Total loss: 2.862 | Reg loss: 0.031 | Tree loss: 2.862 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 026 | Total loss: 2.863 | Reg loss: 0.031 | Tree loss: 2.863 | Accuracy: 0.095703 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 026 | Total loss: 2.826 | Reg loss: 0.031 | Tree loss: 2.826 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 026 | Total loss: 2.838 | Reg loss: 0.031 | Tree loss: 2.838 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 026 | Total loss: 2.858 | Reg loss: 0.031 | Tree loss: 2.858 | Accuracy: 0.089844 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 026 | Total loss: 2.824 | Reg loss: 0.031 | Tree loss: 2.824 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 026 | Total loss: 2.831 | Reg loss: 0.031 | Tree loss: 2.831 | Accuracy: 0.109375 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 026 | Total loss: 2.824 | Reg loss: 0.031 | Tree loss: 2.824 | Accuracy: 0.140625 | 0.064 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 026 | Total loss: 2.804 | Reg loss: 0.031 | Tree loss: 2.804 | Accuracy: 0.105850 | 0.064 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 71 | Batch: 000 / 026 | Total loss: 2.889 | Reg loss: 0.031 | Tree loss: 2.889 | Accuracy: 0.123047 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 026 | Total loss: 2.928 | Reg loss: 0.031 | Tree loss: 2.928 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 026 | Total loss: 2.942 | Reg loss: 0.031 | Tree loss: 2.942 | Accuracy: 0.107422 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 026 | Total loss: 2.896 | Reg loss: 0.031 | Tree loss: 2.896 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 026 | Total loss: 2.909 | Reg loss: 0.031 | Tree loss: 2.909 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 026 | Total loss: 2.862 | Reg loss: 0.031 | Tree loss: 2.862 | Accuracy: 0.126953 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 026 | Total loss: 2.912 | Reg loss: 0.031 | Tree loss: 2.912 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 026 | Total loss: 2.865 | Reg loss: 0.031 | Tree loss: 2.865 | Accuracy: 0.113281 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 026 | Total loss: 2.878 | Reg loss: 0.031 | Tree loss: 2.878 | Accuracy: 0.138672 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 026 | Total loss: 2.886 | Reg loss: 0.031 | Tree loss: 2.886 | Accuracy: 0.101562 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 026 | Total loss: 2.866 | Reg loss: 0.031 | Tree loss: 2.866 | Accuracy: 0.115234 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 026 | Total loss: 2.855 | Reg loss: 0.031 | Tree loss: 2.855 | Accuracy: 0.111328 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 026 | Total loss: 2.873 | Reg loss: 0.031 | Tree loss: 2.873 | Accuracy: 0.105469 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 026 | Total loss: 2.864 | Reg loss: 0.031 | Tree loss: 2.864 | Accuracy: 0.091797 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.125000 | 0.064 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 026 | Total loss: 2.834 | Reg loss: 0.031 | Tree loss: 2.834 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 026 | Total loss: 2.815 | Reg loss: 0.031 | Tree loss: 2.815 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 026 | Total loss: 2.841 | Reg loss: 0.031 | Tree loss: 2.841 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 026 | Total loss: 2.823 | Reg loss: 0.031 | Tree loss: 2.823 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 026 | Total loss: 2.824 | Reg loss: 0.031 | Tree loss: 2.824 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 026 | Total loss: 2.809 | Reg loss: 0.031 | Tree loss: 2.809 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 026 | Total loss: 2.814 | Reg loss: 0.031 | Tree loss: 2.814 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 026 | Total loss: 2.813 | Reg loss: 0.031 | Tree loss: 2.813 | Accuracy: 0.125348 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 72 | Batch: 000 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.113281 | 0.063 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 001 / 026 | Total loss: 2.893 | Reg loss: 0.031 | Tree loss: 2.893 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 026 | Total loss: 2.887 | Reg loss: 0.031 | Tree loss: 2.887 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 026 | Total loss: 2.899 | Reg loss: 0.031 | Tree loss: 2.899 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 026 | Total loss: 2.882 | Reg loss: 0.031 | Tree loss: 2.882 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 026 | Total loss: 2.878 | Reg loss: 0.031 | Tree loss: 2.878 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 026 | Total loss: 2.894 | Reg loss: 0.031 | Tree loss: 2.894 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 026 | Total loss: 2.874 | Reg loss: 0.031 | Tree loss: 2.874 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 026 | Total loss: 2.886 | Reg loss: 0.031 | Tree loss: 2.886 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 026 | Total loss: 2.884 | Reg loss: 0.031 | Tree loss: 2.884 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 026 | Total loss: 2.862 | Reg loss: 0.031 | Tree loss: 2.862 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 026 | Total loss: 2.881 | Reg loss: 0.031 | Tree loss: 2.881 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 026 | Total loss: 2.849 | Reg loss: 0.031 | Tree loss: 2.849 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 026 | Total loss: 2.856 | Reg loss: 0.031 | Tree loss: 2.856 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 026 | Total loss: 2.863 | Reg loss: 0.031 | Tree loss: 2.863 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 026 | Total loss: 2.844 | Reg loss: 0.031 | Tree loss: 2.844 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 026 | Total loss: 2.818 | Reg loss: 0.031 | Tree loss: 2.818 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 026 | Total loss: 2.808 | Reg loss: 0.031 | Tree loss: 2.808 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 026 | Total loss: 2.847 | Reg loss: 0.031 | Tree loss: 2.847 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 026 | Total loss: 2.834 | Reg loss: 0.031 | Tree loss: 2.834 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 026 | Total loss: 2.807 | Reg loss: 0.031 | Tree loss: 2.807 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 026 | Total loss: 2.816 | Reg loss: 0.031 | Tree loss: 2.816 | Accuracy: 0.085938 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 026 | Total loss: 2.797 | Reg loss: 0.031 | Tree loss: 2.797 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 026 | Total loss: 2.786 | Reg loss: 0.031 | Tree loss: 2.786 | Accuracy: 0.097493 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 73 | Batch: 000 / 026 | Total loss: 2.941 | Reg loss: 0.031 | Tree loss: 2.941 | Accuracy: 0.085938 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 026 | Total loss: 2.913 | Reg loss: 0.031 | Tree loss: 2.913 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 026 | Total loss: 2.901 | Reg loss: 0.031 | Tree loss: 2.901 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 026 | Total loss: 2.869 | Reg loss: 0.031 | Tree loss: 2.869 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 026 | Total loss: 2.885 | Reg loss: 0.031 | Tree loss: 2.885 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 026 | Total loss: 2.867 | Reg loss: 0.031 | Tree loss: 2.867 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 026 | Total loss: 2.862 | Reg loss: 0.031 | Tree loss: 2.862 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 026 | Total loss: 2.855 | Reg loss: 0.031 | Tree loss: 2.855 | Accuracy: 0.140625 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 026 | Total loss: 2.871 | Reg loss: 0.031 | Tree loss: 2.871 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 026 | Total loss: 2.874 | Reg loss: 0.031 | Tree loss: 2.874 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 026 | Total loss: 2.854 | Reg loss: 0.031 | Tree loss: 2.854 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 026 | Total loss: 2.871 | Reg loss: 0.031 | Tree loss: 2.871 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 026 | Total loss: 2.845 | Reg loss: 0.031 | Tree loss: 2.845 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 026 | Total loss: 2.823 | Reg loss: 0.031 | Tree loss: 2.823 | Accuracy: 0.087891 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 026 | Total loss: 2.852 | Reg loss: 0.031 | Tree loss: 2.852 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 026 | Total loss: 2.828 | Reg loss: 0.031 | Tree loss: 2.828 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 026 | Total loss: 2.814 | Reg loss: 0.031 | Tree loss: 2.814 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 026 | Total loss: 2.801 | Reg loss: 0.031 | Tree loss: 2.801 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 026 | Total loss: 2.831 | Reg loss: 0.031 | Tree loss: 2.831 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 026 | Total loss: 2.804 | Reg loss: 0.031 | Tree loss: 2.804 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 026 | Total loss: 2.791 | Reg loss: 0.031 | Tree loss: 2.791 | Accuracy: 0.142578 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 026 | Total loss: 2.836 | Reg loss: 0.031 | Tree loss: 2.836 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 026 | Total loss: 2.777 | Reg loss: 0.031 | Tree loss: 2.777 | Accuracy: 0.122563 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 74 | Batch: 000 / 026 | Total loss: 2.920 | Reg loss: 0.031 | Tree loss: 2.920 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 026 | Total loss: 2.895 | Reg loss: 0.031 | Tree loss: 2.895 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 026 | Total loss: 2.890 | Reg loss: 0.031 | Tree loss: 2.890 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 026 | Total loss: 2.883 | Reg loss: 0.031 | Tree loss: 2.883 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 026 | Total loss: 2.873 | Reg loss: 0.031 | Tree loss: 2.873 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 026 | Total loss: 2.896 | Reg loss: 0.031 | Tree loss: 2.896 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 026 | Total loss: 2.904 | Reg loss: 0.031 | Tree loss: 2.904 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 026 | Total loss: 2.895 | Reg loss: 0.031 | Tree loss: 2.895 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 026 | Total loss: 2.840 | Reg loss: 0.031 | Tree loss: 2.840 | Accuracy: 0.138672 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 026 | Total loss: 2.849 | Reg loss: 0.031 | Tree loss: 2.849 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 026 | Total loss: 2.866 | Reg loss: 0.031 | Tree loss: 2.866 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.126953 | 0.063 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 | Batch: 013 / 026 | Total loss: 2.817 | Reg loss: 0.031 | Tree loss: 2.817 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 026 | Total loss: 2.827 | Reg loss: 0.031 | Tree loss: 2.827 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 026 | Total loss: 2.792 | Reg loss: 0.031 | Tree loss: 2.792 | Accuracy: 0.152344 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 026 | Total loss: 2.832 | Reg loss: 0.031 | Tree loss: 2.832 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 026 | Total loss: 2.823 | Reg loss: 0.031 | Tree loss: 2.823 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 026 | Total loss: 2.824 | Reg loss: 0.031 | Tree loss: 2.824 | Accuracy: 0.093750 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 026 | Total loss: 2.817 | Reg loss: 0.031 | Tree loss: 2.817 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 026 | Total loss: 2.830 | Reg loss: 0.031 | Tree loss: 2.830 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 026 | Total loss: 2.773 | Reg loss: 0.031 | Tree loss: 2.773 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 026 | Total loss: 2.804 | Reg loss: 0.031 | Tree loss: 2.804 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 026 | Total loss: 2.803 | Reg loss: 0.031 | Tree loss: 2.803 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 026 | Total loss: 2.847 | Reg loss: 0.031 | Tree loss: 2.847 | Accuracy: 0.091922 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 75 | Batch: 000 / 026 | Total loss: 2.911 | Reg loss: 0.031 | Tree loss: 2.911 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 026 | Total loss: 2.919 | Reg loss: 0.031 | Tree loss: 2.919 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 026 | Total loss: 2.912 | Reg loss: 0.031 | Tree loss: 2.912 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 026 | Total loss: 2.866 | Reg loss: 0.031 | Tree loss: 2.866 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 026 | Total loss: 2.897 | Reg loss: 0.031 | Tree loss: 2.897 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 026 | Total loss: 2.853 | Reg loss: 0.031 | Tree loss: 2.853 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 026 | Total loss: 2.889 | Reg loss: 0.031 | Tree loss: 2.889 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 026 | Total loss: 2.905 | Reg loss: 0.031 | Tree loss: 2.905 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 026 | Total loss: 2.843 | Reg loss: 0.031 | Tree loss: 2.843 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 026 | Total loss: 2.844 | Reg loss: 0.031 | Tree loss: 2.844 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 026 | Total loss: 2.832 | Reg loss: 0.031 | Tree loss: 2.832 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 026 | Total loss: 2.823 | Reg loss: 0.031 | Tree loss: 2.823 | Accuracy: 0.140625 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 026 | Total loss: 2.824 | Reg loss: 0.031 | Tree loss: 2.824 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 026 | Total loss: 2.826 | Reg loss: 0.031 | Tree loss: 2.826 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 026 | Total loss: 2.856 | Reg loss: 0.031 | Tree loss: 2.856 | Accuracy: 0.089844 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 026 | Total loss: 2.797 | Reg loss: 0.031 | Tree loss: 2.797 | Accuracy: 0.093750 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 026 | Total loss: 2.792 | Reg loss: 0.031 | Tree loss: 2.792 | Accuracy: 0.148438 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 026 | Total loss: 2.827 | Reg loss: 0.031 | Tree loss: 2.827 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 026 | Total loss: 2.799 | Reg loss: 0.031 | Tree loss: 2.799 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 026 | Total loss: 2.802 | Reg loss: 0.031 | Tree loss: 2.802 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 026 | Total loss: 2.785 | Reg loss: 0.031 | Tree loss: 2.785 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 026 | Total loss: 2.803 | Reg loss: 0.031 | Tree loss: 2.803 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.103064 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 76 | Batch: 000 / 026 | Total loss: 2.938 | Reg loss: 0.031 | Tree loss: 2.938 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 026 | Total loss: 2.941 | Reg loss: 0.031 | Tree loss: 2.941 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 026 | Total loss: 2.864 | Reg loss: 0.031 | Tree loss: 2.864 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 026 | Total loss: 2.888 | Reg loss: 0.031 | Tree loss: 2.888 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 026 | Total loss: 2.895 | Reg loss: 0.031 | Tree loss: 2.895 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 026 | Total loss: 2.894 | Reg loss: 0.031 | Tree loss: 2.894 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 026 | Total loss: 2.836 | Reg loss: 0.031 | Tree loss: 2.836 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 026 | Total loss: 2.871 | Reg loss: 0.031 | Tree loss: 2.871 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 026 | Total loss: 2.830 | Reg loss: 0.031 | Tree loss: 2.830 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 026 | Total loss: 2.860 | Reg loss: 0.031 | Tree loss: 2.860 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 026 | Total loss: 2.887 | Reg loss: 0.031 | Tree loss: 2.887 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 026 | Total loss: 2.819 | Reg loss: 0.031 | Tree loss: 2.819 | Accuracy: 0.140625 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 026 | Total loss: 2.857 | Reg loss: 0.031 | Tree loss: 2.857 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 026 | Total loss: 2.808 | Reg loss: 0.031 | Tree loss: 2.808 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 026 | Total loss: 2.813 | Reg loss: 0.031 | Tree loss: 2.813 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 026 | Total loss: 2.840 | Reg loss: 0.031 | Tree loss: 2.840 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 026 | Total loss: 2.815 | Reg loss: 0.031 | Tree loss: 2.815 | Accuracy: 0.074219 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 026 | Total loss: 2.838 | Reg loss: 0.031 | Tree loss: 2.838 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 026 | Total loss: 2.821 | Reg loss: 0.031 | Tree loss: 2.821 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 026 | Total loss: 2.797 | Reg loss: 0.031 | Tree loss: 2.797 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 026 | Total loss: 2.821 | Reg loss: 0.031 | Tree loss: 2.821 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 026 | Total loss: 2.770 | Reg loss: 0.031 | Tree loss: 2.770 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 026 | Total loss: 2.762 | Reg loss: 0.031 | Tree loss: 2.762 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 026 | Total loss: 2.779 | Reg loss: 0.031 | Tree loss: 2.779 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 026 | Total loss: 2.824 | Reg loss: 0.031 | Tree loss: 2.824 | Accuracy: 0.116992 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 77 | Batch: 000 / 026 | Total loss: 2.911 | Reg loss: 0.031 | Tree loss: 2.911 | Accuracy: 0.105469 | 0.063 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 001 / 026 | Total loss: 2.892 | Reg loss: 0.031 | Tree loss: 2.892 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 026 | Total loss: 2.867 | Reg loss: 0.031 | Tree loss: 2.867 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 026 | Total loss: 2.938 | Reg loss: 0.031 | Tree loss: 2.938 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 026 | Total loss: 2.877 | Reg loss: 0.031 | Tree loss: 2.877 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 026 | Total loss: 2.894 | Reg loss: 0.031 | Tree loss: 2.894 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 026 | Total loss: 2.859 | Reg loss: 0.031 | Tree loss: 2.859 | Accuracy: 0.136719 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 026 | Total loss: 2.838 | Reg loss: 0.031 | Tree loss: 2.838 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 026 | Total loss: 2.887 | Reg loss: 0.031 | Tree loss: 2.887 | Accuracy: 0.089844 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 026 | Total loss: 2.852 | Reg loss: 0.031 | Tree loss: 2.852 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 026 | Total loss: 2.847 | Reg loss: 0.031 | Tree loss: 2.847 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 026 | Total loss: 2.842 | Reg loss: 0.031 | Tree loss: 2.842 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 026 | Total loss: 2.820 | Reg loss: 0.031 | Tree loss: 2.820 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 026 | Total loss: 2.837 | Reg loss: 0.031 | Tree loss: 2.837 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 026 | Total loss: 2.850 | Reg loss: 0.031 | Tree loss: 2.850 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 026 | Total loss: 2.828 | Reg loss: 0.031 | Tree loss: 2.828 | Accuracy: 0.136719 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 026 | Total loss: 2.820 | Reg loss: 0.031 | Tree loss: 2.820 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 026 | Total loss: 2.828 | Reg loss: 0.031 | Tree loss: 2.828 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 026 | Total loss: 2.781 | Reg loss: 0.031 | Tree loss: 2.781 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 026 | Total loss: 2.800 | Reg loss: 0.031 | Tree loss: 2.800 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 026 | Total loss: 2.809 | Reg loss: 0.031 | Tree loss: 2.809 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 026 | Total loss: 2.792 | Reg loss: 0.031 | Tree loss: 2.792 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 026 | Total loss: 2.780 | Reg loss: 0.031 | Tree loss: 2.780 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 026 | Total loss: 2.780 | Reg loss: 0.031 | Tree loss: 2.780 | Accuracy: 0.138672 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 026 | Total loss: 2.776 | Reg loss: 0.031 | Tree loss: 2.776 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 026 | Total loss: 2.791 | Reg loss: 0.031 | Tree loss: 2.791 | Accuracy: 0.100279 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 78 | Batch: 000 / 026 | Total loss: 2.883 | Reg loss: 0.030 | Tree loss: 2.883 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 026 | Total loss: 2.889 | Reg loss: 0.030 | Tree loss: 2.889 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 026 | Total loss: 2.857 | Reg loss: 0.030 | Tree loss: 2.857 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 026 | Total loss: 2.880 | Reg loss: 0.030 | Tree loss: 2.880 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 026 | Total loss: 2.929 | Reg loss: 0.030 | Tree loss: 2.929 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 026 | Total loss: 2.855 | Reg loss: 0.030 | Tree loss: 2.855 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 026 | Total loss: 2.879 | Reg loss: 0.031 | Tree loss: 2.879 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 026 | Total loss: 2.867 | Reg loss: 0.031 | Tree loss: 2.867 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 026 | Total loss: 2.830 | Reg loss: 0.031 | Tree loss: 2.830 | Accuracy: 0.142578 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 026 | Total loss: 2.825 | Reg loss: 0.031 | Tree loss: 2.825 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 026 | Total loss: 2.854 | Reg loss: 0.031 | Tree loss: 2.854 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 026 | Total loss: 2.870 | Reg loss: 0.031 | Tree loss: 2.870 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 026 | Total loss: 2.843 | Reg loss: 0.031 | Tree loss: 2.843 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 026 | Total loss: 2.826 | Reg loss: 0.031 | Tree loss: 2.826 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 026 | Total loss: 2.840 | Reg loss: 0.031 | Tree loss: 2.840 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 026 | Total loss: 2.831 | Reg loss: 0.031 | Tree loss: 2.831 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 026 | Total loss: 2.824 | Reg loss: 0.031 | Tree loss: 2.824 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 026 | Total loss: 2.788 | Reg loss: 0.031 | Tree loss: 2.788 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 026 | Total loss: 2.819 | Reg loss: 0.031 | Tree loss: 2.819 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 026 | Total loss: 2.786 | Reg loss: 0.031 | Tree loss: 2.786 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 026 | Total loss: 2.800 | Reg loss: 0.031 | Tree loss: 2.800 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 026 | Total loss: 2.797 | Reg loss: 0.031 | Tree loss: 2.797 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 026 | Total loss: 2.774 | Reg loss: 0.031 | Tree loss: 2.774 | Accuracy: 0.136719 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 026 | Total loss: 2.797 | Reg loss: 0.031 | Tree loss: 2.797 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 026 | Total loss: 2.787 | Reg loss: 0.031 | Tree loss: 2.787 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 026 | Total loss: 2.807 | Reg loss: 0.031 | Tree loss: 2.807 | Accuracy: 0.111421 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 79 | Batch: 000 / 026 | Total loss: 2.907 | Reg loss: 0.030 | Tree loss: 2.907 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 026 | Total loss: 2.881 | Reg loss: 0.030 | Tree loss: 2.881 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 026 | Total loss: 2.887 | Reg loss: 0.030 | Tree loss: 2.887 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 026 | Total loss: 2.918 | Reg loss: 0.030 | Tree loss: 2.918 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 026 | Total loss: 2.852 | Reg loss: 0.030 | Tree loss: 2.852 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 026 | Total loss: 2.851 | Reg loss: 0.030 | Tree loss: 2.851 | Accuracy: 0.144531 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 026 | Total loss: 2.889 | Reg loss: 0.030 | Tree loss: 2.889 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 026 | Total loss: 2.866 | Reg loss: 0.030 | Tree loss: 2.866 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 026 | Total loss: 2.858 | Reg loss: 0.030 | Tree loss: 2.858 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 026 | Total loss: 2.854 | Reg loss: 0.031 | Tree loss: 2.854 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 026 | Total loss: 2.849 | Reg loss: 0.031 | Tree loss: 2.849 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 026 | Total loss: 2.848 | Reg loss: 0.031 | Tree loss: 2.848 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 026 | Total loss: 2.810 | Reg loss: 0.031 | Tree loss: 2.810 | Accuracy: 0.115234 | 0.063 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 014 / 026 | Total loss: 2.826 | Reg loss: 0.031 | Tree loss: 2.826 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 026 | Total loss: 2.802 | Reg loss: 0.031 | Tree loss: 2.802 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 026 | Total loss: 2.798 | Reg loss: 0.031 | Tree loss: 2.798 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 026 | Total loss: 2.785 | Reg loss: 0.031 | Tree loss: 2.785 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 026 | Total loss: 2.826 | Reg loss: 0.031 | Tree loss: 2.826 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 026 | Total loss: 2.794 | Reg loss: 0.031 | Tree loss: 2.794 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 026 | Total loss: 2.815 | Reg loss: 0.031 | Tree loss: 2.815 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 026 | Total loss: 2.791 | Reg loss: 0.031 | Tree loss: 2.791 | Accuracy: 0.087891 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 026 | Total loss: 2.806 | Reg loss: 0.031 | Tree loss: 2.806 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 026 | Total loss: 2.750 | Reg loss: 0.031 | Tree loss: 2.750 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 026 | Total loss: 2.787 | Reg loss: 0.031 | Tree loss: 2.787 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 026 | Total loss: 2.776 | Reg loss: 0.031 | Tree loss: 2.776 | Accuracy: 0.103064 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 80 | Batch: 000 / 026 | Total loss: 2.936 | Reg loss: 0.030 | Tree loss: 2.936 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 026 | Total loss: 2.866 | Reg loss: 0.030 | Tree loss: 2.866 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 026 | Total loss: 2.880 | Reg loss: 0.030 | Tree loss: 2.880 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 026 | Total loss: 2.862 | Reg loss: 0.030 | Tree loss: 2.862 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 026 | Total loss: 2.898 | Reg loss: 0.030 | Tree loss: 2.898 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 026 | Total loss: 2.848 | Reg loss: 0.030 | Tree loss: 2.848 | Accuracy: 0.142578 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 026 | Total loss: 2.867 | Reg loss: 0.030 | Tree loss: 2.867 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 026 | Total loss: 2.854 | Reg loss: 0.030 | Tree loss: 2.854 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 026 | Total loss: 2.861 | Reg loss: 0.030 | Tree loss: 2.861 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 026 | Total loss: 2.834 | Reg loss: 0.030 | Tree loss: 2.834 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 026 | Total loss: 2.811 | Reg loss: 0.030 | Tree loss: 2.811 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 026 | Total loss: 2.850 | Reg loss: 0.030 | Tree loss: 2.850 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 026 | Total loss: 2.798 | Reg loss: 0.031 | Tree loss: 2.798 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 026 | Total loss: 2.838 | Reg loss: 0.031 | Tree loss: 2.838 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 026 | Total loss: 2.808 | Reg loss: 0.031 | Tree loss: 2.808 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 026 | Total loss: 2.803 | Reg loss: 0.031 | Tree loss: 2.803 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 026 | Total loss: 2.820 | Reg loss: 0.031 | Tree loss: 2.820 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 026 | Total loss: 2.816 | Reg loss: 0.031 | Tree loss: 2.816 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 026 | Total loss: 2.861 | Reg loss: 0.031 | Tree loss: 2.861 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 026 | Total loss: 2.767 | Reg loss: 0.031 | Tree loss: 2.767 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 026 | Total loss: 2.828 | Reg loss: 0.031 | Tree loss: 2.828 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 026 | Total loss: 2.756 | Reg loss: 0.031 | Tree loss: 2.756 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 026 | Total loss: 2.798 | Reg loss: 0.031 | Tree loss: 2.798 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 026 | Total loss: 2.779 | Reg loss: 0.031 | Tree loss: 2.779 | Accuracy: 0.146484 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 026 | Total loss: 2.776 | Reg loss: 0.031 | Tree loss: 2.776 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 026 | Total loss: 2.783 | Reg loss: 0.031 | Tree loss: 2.783 | Accuracy: 0.108635 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 81 | Batch: 000 / 026 | Total loss: 2.898 | Reg loss: 0.030 | Tree loss: 2.898 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 026 | Total loss: 2.876 | Reg loss: 0.030 | Tree loss: 2.876 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 026 | Total loss: 2.870 | Reg loss: 0.030 | Tree loss: 2.870 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 026 | Total loss: 2.861 | Reg loss: 0.030 | Tree loss: 2.861 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 026 | Total loss: 2.893 | Reg loss: 0.030 | Tree loss: 2.893 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 026 | Total loss: 2.873 | Reg loss: 0.030 | Tree loss: 2.873 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 026 | Total loss: 2.872 | Reg loss: 0.030 | Tree loss: 2.872 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 026 | Total loss: 2.863 | Reg loss: 0.030 | Tree loss: 2.863 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 026 | Total loss: 2.858 | Reg loss: 0.030 | Tree loss: 2.858 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 026 | Total loss: 2.835 | Reg loss: 0.030 | Tree loss: 2.835 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 026 | Total loss: 2.850 | Reg loss: 0.030 | Tree loss: 2.850 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 026 | Total loss: 2.797 | Reg loss: 0.030 | Tree loss: 2.797 | Accuracy: 0.150391 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 026 | Total loss: 2.806 | Reg loss: 0.030 | Tree loss: 2.806 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 026 | Total loss: 2.839 | Reg loss: 0.031 | Tree loss: 2.839 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 026 | Total loss: 2.795 | Reg loss: 0.031 | Tree loss: 2.795 | Accuracy: 0.136719 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 026 | Total loss: 2.812 | Reg loss: 0.031 | Tree loss: 2.812 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 026 | Total loss: 2.797 | Reg loss: 0.031 | Tree loss: 2.797 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 026 | Total loss: 2.798 | Reg loss: 0.031 | Tree loss: 2.798 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 026 | Total loss: 2.801 | Reg loss: 0.031 | Tree loss: 2.801 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 026 | Total loss: 2.813 | Reg loss: 0.031 | Tree loss: 2.813 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 026 | Total loss: 2.797 | Reg loss: 0.031 | Tree loss: 2.797 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 026 | Total loss: 2.776 | Reg loss: 0.031 | Tree loss: 2.776 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 026 | Total loss: 2.809 | Reg loss: 0.031 | Tree loss: 2.809 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 026 | Total loss: 2.802 | Reg loss: 0.031 | Tree loss: 2.802 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 026 | Total loss: 2.782 | Reg loss: 0.031 | Tree loss: 2.782 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 026 | Total loss: 2.759 | Reg loss: 0.031 | Tree loss: 2.759 | Accuracy: 0.114206 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 82 | Batch: 000 / 026 | Total loss: 2.917 | Reg loss: 0.030 | Tree loss: 2.917 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 026 | Total loss: 2.892 | Reg loss: 0.030 | Tree loss: 2.892 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.160156 | 0.063 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 003 / 026 | Total loss: 2.863 | Reg loss: 0.030 | Tree loss: 2.863 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 026 | Total loss: 2.898 | Reg loss: 0.030 | Tree loss: 2.898 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 026 | Total loss: 2.872 | Reg loss: 0.030 | Tree loss: 2.872 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 026 | Total loss: 2.826 | Reg loss: 0.030 | Tree loss: 2.826 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 026 | Total loss: 2.875 | Reg loss: 0.030 | Tree loss: 2.875 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 026 | Total loss: 2.851 | Reg loss: 0.030 | Tree loss: 2.851 | Accuracy: 0.087891 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 026 | Total loss: 2.831 | Reg loss: 0.030 | Tree loss: 2.831 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 026 | Total loss: 2.819 | Reg loss: 0.030 | Tree loss: 2.819 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 026 | Total loss: 2.807 | Reg loss: 0.030 | Tree loss: 2.807 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 026 | Total loss: 2.839 | Reg loss: 0.030 | Tree loss: 2.839 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 026 | Total loss: 2.819 | Reg loss: 0.030 | Tree loss: 2.819 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 026 | Total loss: 2.822 | Reg loss: 0.030 | Tree loss: 2.822 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 026 | Total loss: 2.835 | Reg loss: 0.031 | Tree loss: 2.835 | Accuracy: 0.089844 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 026 | Total loss: 2.796 | Reg loss: 0.031 | Tree loss: 2.796 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 026 | Total loss: 2.790 | Reg loss: 0.031 | Tree loss: 2.790 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 026 | Total loss: 2.772 | Reg loss: 0.031 | Tree loss: 2.772 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 026 | Total loss: 2.802 | Reg loss: 0.031 | Tree loss: 2.802 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 026 | Total loss: 2.801 | Reg loss: 0.031 | Tree loss: 2.801 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 026 | Total loss: 2.778 | Reg loss: 0.031 | Tree loss: 2.778 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 026 | Total loss: 2.788 | Reg loss: 0.031 | Tree loss: 2.788 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 026 | Total loss: 2.813 | Reg loss: 0.031 | Tree loss: 2.813 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 026 | Total loss: 2.770 | Reg loss: 0.031 | Tree loss: 2.770 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 026 | Total loss: 2.761 | Reg loss: 0.031 | Tree loss: 2.761 | Accuracy: 0.130919 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 83 | Batch: 000 / 026 | Total loss: 2.888 | Reg loss: 0.030 | Tree loss: 2.888 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 026 | Total loss: 2.874 | Reg loss: 0.030 | Tree loss: 2.874 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 026 | Total loss: 2.871 | Reg loss: 0.030 | Tree loss: 2.871 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 026 | Total loss: 2.868 | Reg loss: 0.030 | Tree loss: 2.868 | Accuracy: 0.123047 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 026 | Total loss: 2.893 | Reg loss: 0.030 | Tree loss: 2.893 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 026 | Total loss: 2.848 | Reg loss: 0.030 | Tree loss: 2.848 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 026 | Total loss: 2.853 | Reg loss: 0.030 | Tree loss: 2.853 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 026 | Total loss: 2.856 | Reg loss: 0.030 | Tree loss: 2.856 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 026 | Total loss: 2.848 | Reg loss: 0.030 | Tree loss: 2.848 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 026 | Total loss: 2.825 | Reg loss: 0.030 | Tree loss: 2.825 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 026 | Total loss: 2.811 | Reg loss: 0.030 | Tree loss: 2.811 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 026 | Total loss: 2.804 | Reg loss: 0.030 | Tree loss: 2.804 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 026 | Total loss: 2.839 | Reg loss: 0.030 | Tree loss: 2.839 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 026 | Total loss: 2.775 | Reg loss: 0.030 | Tree loss: 2.775 | Accuracy: 0.140625 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 026 | Total loss: 2.823 | Reg loss: 0.030 | Tree loss: 2.823 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 026 | Total loss: 2.798 | Reg loss: 0.031 | Tree loss: 2.798 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 026 | Total loss: 2.831 | Reg loss: 0.031 | Tree loss: 2.831 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 026 | Total loss: 2.810 | Reg loss: 0.031 | Tree loss: 2.810 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 026 | Total loss: 2.788 | Reg loss: 0.031 | Tree loss: 2.788 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 026 | Total loss: 2.805 | Reg loss: 0.031 | Tree loss: 2.805 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 026 | Total loss: 2.778 | Reg loss: 0.031 | Tree loss: 2.778 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 026 | Total loss: 2.756 | Reg loss: 0.031 | Tree loss: 2.756 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 026 | Total loss: 2.801 | Reg loss: 0.031 | Tree loss: 2.801 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 026 | Total loss: 2.777 | Reg loss: 0.031 | Tree loss: 2.777 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 026 | Total loss: 2.770 | Reg loss: 0.031 | Tree loss: 2.770 | Accuracy: 0.072423 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 84 | Batch: 000 / 026 | Total loss: 2.861 | Reg loss: 0.030 | Tree loss: 2.861 | Accuracy: 0.093750 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 026 | Total loss: 2.884 | Reg loss: 0.030 | Tree loss: 2.884 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 026 | Total loss: 2.920 | Reg loss: 0.030 | Tree loss: 2.920 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 026 | Total loss: 2.868 | Reg loss: 0.030 | Tree loss: 2.868 | Accuracy: 0.154297 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 026 | Total loss: 2.852 | Reg loss: 0.030 | Tree loss: 2.852 | Accuracy: 0.138672 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 026 | Total loss: 2.875 | Reg loss: 0.030 | Tree loss: 2.875 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 026 | Total loss: 2.843 | Reg loss: 0.030 | Tree loss: 2.843 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 026 | Total loss: 2.856 | Reg loss: 0.030 | Tree loss: 2.856 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 026 | Total loss: 2.836 | Reg loss: 0.030 | Tree loss: 2.836 | Accuracy: 0.083984 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 026 | Total loss: 2.800 | Reg loss: 0.030 | Tree loss: 2.800 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 026 | Total loss: 2.820 | Reg loss: 0.030 | Tree loss: 2.820 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 026 | Total loss: 2.812 | Reg loss: 0.030 | Tree loss: 2.812 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 026 | Total loss: 2.796 | Reg loss: 0.030 | Tree loss: 2.796 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 026 | Total loss: 2.801 | Reg loss: 0.030 | Tree loss: 2.801 | Accuracy: 0.117188 | 0.063 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 015 / 026 | Total loss: 2.849 | Reg loss: 0.030 | Tree loss: 2.849 | Accuracy: 0.097656 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 026 | Total loss: 2.784 | Reg loss: 0.030 | Tree loss: 2.784 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 026 | Total loss: 2.800 | Reg loss: 0.030 | Tree loss: 2.800 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 026 | Total loss: 2.775 | Reg loss: 0.031 | Tree loss: 2.775 | Accuracy: 0.140625 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 026 | Total loss: 2.770 | Reg loss: 0.031 | Tree loss: 2.770 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 026 | Total loss: 2.802 | Reg loss: 0.031 | Tree loss: 2.802 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 026 | Total loss: 2.802 | Reg loss: 0.031 | Tree loss: 2.802 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 026 | Total loss: 2.802 | Reg loss: 0.031 | Tree loss: 2.802 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 026 | Total loss: 2.794 | Reg loss: 0.031 | Tree loss: 2.794 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 026 | Total loss: 2.774 | Reg loss: 0.031 | Tree loss: 2.774 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 026 | Total loss: 2.771 | Reg loss: 0.031 | Tree loss: 2.771 | Accuracy: 0.105850 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 85 | Batch: 000 / 026 | Total loss: 2.909 | Reg loss: 0.030 | Tree loss: 2.909 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 026 | Total loss: 2.869 | Reg loss: 0.030 | Tree loss: 2.869 | Accuracy: 0.128906 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 026 | Total loss: 2.867 | Reg loss: 0.030 | Tree loss: 2.867 | Accuracy: 0.125000 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 026 | Total loss: 2.849 | Reg loss: 0.030 | Tree loss: 2.849 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 026 | Total loss: 2.909 | Reg loss: 0.030 | Tree loss: 2.909 | Accuracy: 0.091797 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 026 | Total loss: 2.876 | Reg loss: 0.030 | Tree loss: 2.876 | Accuracy: 0.140625 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 026 | Total loss: 2.826 | Reg loss: 0.030 | Tree loss: 2.826 | Accuracy: 0.146484 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 026 | Total loss: 2.844 | Reg loss: 0.030 | Tree loss: 2.844 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 026 | Total loss: 2.828 | Reg loss: 0.030 | Tree loss: 2.828 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 026 | Total loss: 2.830 | Reg loss: 0.030 | Tree loss: 2.830 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 026 | Total loss: 2.852 | Reg loss: 0.030 | Tree loss: 2.852 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 026 | Total loss: 2.859 | Reg loss: 0.030 | Tree loss: 2.859 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 026 | Total loss: 2.835 | Reg loss: 0.030 | Tree loss: 2.835 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 026 | Total loss: 2.848 | Reg loss: 0.030 | Tree loss: 2.848 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 026 | Total loss: 2.794 | Reg loss: 0.030 | Tree loss: 2.794 | Accuracy: 0.109375 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 026 | Total loss: 2.789 | Reg loss: 0.030 | Tree loss: 2.789 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 026 | Total loss: 2.790 | Reg loss: 0.030 | Tree loss: 2.790 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 026 | Total loss: 2.800 | Reg loss: 0.030 | Tree loss: 2.800 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 026 | Total loss: 2.792 | Reg loss: 0.030 | Tree loss: 2.792 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 026 | Total loss: 2.758 | Reg loss: 0.030 | Tree loss: 2.758 | Accuracy: 0.144531 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 026 | Total loss: 2.747 | Reg loss: 0.031 | Tree loss: 2.747 | Accuracy: 0.115234 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 026 | Total loss: 2.791 | Reg loss: 0.031 | Tree loss: 2.791 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 026 | Total loss: 2.770 | Reg loss: 0.031 | Tree loss: 2.770 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 026 | Total loss: 2.770 | Reg loss: 0.031 | Tree loss: 2.770 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 026 | Total loss: 2.768 | Reg loss: 0.031 | Tree loss: 2.768 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 026 | Total loss: 2.779 | Reg loss: 0.031 | Tree loss: 2.779 | Accuracy: 0.116992 | 0.063 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 86 | Batch: 000 / 026 | Total loss: 2.866 | Reg loss: 0.030 | Tree loss: 2.866 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 026 | Total loss: 2.875 | Reg loss: 0.030 | Tree loss: 2.875 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 026 | Total loss: 2.888 | Reg loss: 0.030 | Tree loss: 2.888 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 026 | Total loss: 2.887 | Reg loss: 0.030 | Tree loss: 2.887 | Accuracy: 0.099609 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.130859 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 026 | Total loss: 2.858 | Reg loss: 0.030 | Tree loss: 2.858 | Accuracy: 0.113281 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 026 | Total loss: 2.849 | Reg loss: 0.030 | Tree loss: 2.849 | Accuracy: 0.107422 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 026 | Total loss: 2.840 | Reg loss: 0.030 | Tree loss: 2.840 | Accuracy: 0.134766 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 026 | Total loss: 2.837 | Reg loss: 0.030 | Tree loss: 2.837 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 026 | Total loss: 2.834 | Reg loss: 0.030 | Tree loss: 2.834 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 026 | Total loss: 2.818 | Reg loss: 0.030 | Tree loss: 2.818 | Accuracy: 0.105469 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 026 | Total loss: 2.804 | Reg loss: 0.030 | Tree loss: 2.804 | Accuracy: 0.126953 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 026 | Total loss: 2.791 | Reg loss: 0.030 | Tree loss: 2.791 | Accuracy: 0.132812 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 026 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.101562 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 026 | Total loss: 2.814 | Reg loss: 0.030 | Tree loss: 2.814 | Accuracy: 0.119141 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.093750 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 026 | Total loss: 2.801 | Reg loss: 0.030 | Tree loss: 2.801 | Accuracy: 0.144531 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 026 | Total loss: 2.774 | Reg loss: 0.030 | Tree loss: 2.774 | Accuracy: 0.121094 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 026 | Total loss: 2.813 | Reg loss: 0.030 | Tree loss: 2.813 | Accuracy: 0.103516 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 026 | Total loss: 2.786 | Reg loss: 0.030 | Tree loss: 2.786 | Accuracy: 0.095703 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 026 | Total loss: 2.796 | Reg loss: 0.031 | Tree loss: 2.796 | Accuracy: 0.111328 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 026 | Total loss: 2.773 | Reg loss: 0.031 | Tree loss: 2.773 | Accuracy: 0.117188 | 0.063 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 026 | Total loss: 2.783 | Reg loss: 0.031 | Tree loss: 2.783 | Accuracy: 0.093750 | 0.062 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 026 | Total loss: 2.771 | Reg loss: 0.031 | Tree loss: 2.771 | Accuracy: 0.126953 | 0.062 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 026 | Total loss: 2.756 | Reg loss: 0.031 | Tree loss: 2.756 | Accuracy: 0.097493 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 000 / 026 | Total loss: 2.887 | Reg loss: 0.030 | Tree loss: 2.887 | Accuracy: 0.093750 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 026 | Total loss: 2.896 | Reg loss: 0.030 | Tree loss: 2.896 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 026 | Total loss: 2.867 | Reg loss: 0.030 | Tree loss: 2.867 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 026 | Total loss: 2.898 | Reg loss: 0.030 | Tree loss: 2.898 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 026 | Total loss: 2.855 | Reg loss: 0.030 | Tree loss: 2.855 | Accuracy: 0.128906 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 026 | Total loss: 2.880 | Reg loss: 0.030 | Tree loss: 2.880 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 026 | Total loss: 2.870 | Reg loss: 0.030 | Tree loss: 2.870 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 026 | Total loss: 2.820 | Reg loss: 0.030 | Tree loss: 2.820 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 026 | Total loss: 2.827 | Reg loss: 0.030 | Tree loss: 2.827 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 026 | Total loss: 2.797 | Reg loss: 0.030 | Tree loss: 2.797 | Accuracy: 0.140625 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 026 | Total loss: 2.848 | Reg loss: 0.030 | Tree loss: 2.848 | Accuracy: 0.095703 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 026 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.095703 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 026 | Total loss: 2.823 | Reg loss: 0.030 | Tree loss: 2.823 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 026 | Total loss: 2.773 | Reg loss: 0.030 | Tree loss: 2.773 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 026 | Total loss: 2.807 | Reg loss: 0.030 | Tree loss: 2.807 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 026 | Total loss: 2.802 | Reg loss: 0.030 | Tree loss: 2.802 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 026 | Total loss: 2.828 | Reg loss: 0.030 | Tree loss: 2.828 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 026 | Total loss: 2.807 | Reg loss: 0.030 | Tree loss: 2.807 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 026 | Total loss: 2.774 | Reg loss: 0.030 | Tree loss: 2.774 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 026 | Total loss: 2.777 | Reg loss: 0.030 | Tree loss: 2.777 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 026 | Total loss: 2.764 | Reg loss: 0.030 | Tree loss: 2.764 | Accuracy: 0.130859 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 026 | Total loss: 2.743 | Reg loss: 0.031 | Tree loss: 2.743 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 026 | Total loss: 2.804 | Reg loss: 0.031 | Tree loss: 2.804 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 026 | Total loss: 2.731 | Reg loss: 0.031 | Tree loss: 2.731 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 026 | Total loss: 2.794 | Reg loss: 0.031 | Tree loss: 2.794 | Accuracy: 0.130919 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 88 | Batch: 000 / 026 | Total loss: 2.890 | Reg loss: 0.030 | Tree loss: 2.890 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 026 | Total loss: 2.891 | Reg loss: 0.030 | Tree loss: 2.891 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 026 | Total loss: 2.872 | Reg loss: 0.030 | Tree loss: 2.872 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 026 | Total loss: 2.860 | Reg loss: 0.030 | Tree loss: 2.860 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 026 | Total loss: 2.843 | Reg loss: 0.030 | Tree loss: 2.843 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 026 | Total loss: 2.855 | Reg loss: 0.030 | Tree loss: 2.855 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 026 | Total loss: 2.854 | Reg loss: 0.030 | Tree loss: 2.854 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 026 | Total loss: 2.800 | Reg loss: 0.030 | Tree loss: 2.800 | Accuracy: 0.136719 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 026 | Total loss: 2.849 | Reg loss: 0.030 | Tree loss: 2.849 | Accuracy: 0.082031 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 026 | Total loss: 2.862 | Reg loss: 0.030 | Tree loss: 2.862 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 026 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 026 | Total loss: 2.803 | Reg loss: 0.030 | Tree loss: 2.803 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 026 | Total loss: 2.792 | Reg loss: 0.030 | Tree loss: 2.792 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 026 | Total loss: 2.802 | Reg loss: 0.030 | Tree loss: 2.802 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 026 | Total loss: 2.809 | Reg loss: 0.030 | Tree loss: 2.809 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 026 | Total loss: 2.789 | Reg loss: 0.030 | Tree loss: 2.789 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 026 | Total loss: 2.775 | Reg loss: 0.030 | Tree loss: 2.775 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 026 | Total loss: 2.789 | Reg loss: 0.030 | Tree loss: 2.789 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 026 | Total loss: 2.788 | Reg loss: 0.030 | Tree loss: 2.788 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 026 | Total loss: 2.801 | Reg loss: 0.030 | Tree loss: 2.801 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 026 | Total loss: 2.793 | Reg loss: 0.030 | Tree loss: 2.793 | Accuracy: 0.144531 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 026 | Total loss: 2.763 | Reg loss: 0.030 | Tree loss: 2.763 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 026 | Total loss: 2.747 | Reg loss: 0.030 | Tree loss: 2.747 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 026 | Total loss: 2.786 | Reg loss: 0.031 | Tree loss: 2.786 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 026 | Total loss: 2.743 | Reg loss: 0.031 | Tree loss: 2.743 | Accuracy: 0.128134 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 89 | Batch: 000 / 026 | Total loss: 2.876 | Reg loss: 0.030 | Tree loss: 2.876 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 026 | Total loss: 2.907 | Reg loss: 0.030 | Tree loss: 2.907 | Accuracy: 0.091797 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 026 | Total loss: 2.903 | Reg loss: 0.030 | Tree loss: 2.903 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 026 | Total loss: 2.883 | Reg loss: 0.030 | Tree loss: 2.883 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 026 | Total loss: 2.830 | Reg loss: 0.030 | Tree loss: 2.830 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 026 | Total loss: 2.832 | Reg loss: 0.030 | Tree loss: 2.832 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 026 | Total loss: 2.904 | Reg loss: 0.030 | Tree loss: 2.904 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 026 | Total loss: 2.830 | Reg loss: 0.030 | Tree loss: 2.830 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 026 | Total loss: 2.832 | Reg loss: 0.030 | Tree loss: 2.832 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 026 | Total loss: 2.842 | Reg loss: 0.030 | Tree loss: 2.842 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 026 | Total loss: 2.816 | Reg loss: 0.030 | Tree loss: 2.816 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 026 | Total loss: 2.811 | Reg loss: 0.030 | Tree loss: 2.811 | Accuracy: 0.115234 | 0.062 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 012 / 026 | Total loss: 2.798 | Reg loss: 0.030 | Tree loss: 2.798 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 026 | Total loss: 2.781 | Reg loss: 0.030 | Tree loss: 2.781 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 026 | Total loss: 2.815 | Reg loss: 0.030 | Tree loss: 2.815 | Accuracy: 0.087891 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 026 | Total loss: 2.806 | Reg loss: 0.030 | Tree loss: 2.806 | Accuracy: 0.128906 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 026 | Total loss: 2.781 | Reg loss: 0.030 | Tree loss: 2.781 | Accuracy: 0.130859 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 026 | Total loss: 2.773 | Reg loss: 0.030 | Tree loss: 2.773 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 026 | Total loss: 2.776 | Reg loss: 0.030 | Tree loss: 2.776 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 026 | Total loss: 2.768 | Reg loss: 0.030 | Tree loss: 2.768 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 026 | Total loss: 2.785 | Reg loss: 0.030 | Tree loss: 2.785 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 026 | Total loss: 2.769 | Reg loss: 0.030 | Tree loss: 2.769 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 026 | Total loss: 2.754 | Reg loss: 0.030 | Tree loss: 2.754 | Accuracy: 0.126953 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 026 | Total loss: 2.773 | Reg loss: 0.030 | Tree loss: 2.773 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 026 | Total loss: 2.773 | Reg loss: 0.030 | Tree loss: 2.773 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 026 | Total loss: 2.761 | Reg loss: 0.031 | Tree loss: 2.761 | Accuracy: 0.125348 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 90 | Batch: 000 / 026 | Total loss: 2.872 | Reg loss: 0.030 | Tree loss: 2.872 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 026 | Total loss: 2.889 | Reg loss: 0.030 | Tree loss: 2.889 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 026 | Total loss: 2.910 | Reg loss: 0.030 | Tree loss: 2.910 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 026 | Total loss: 2.884 | Reg loss: 0.030 | Tree loss: 2.884 | Accuracy: 0.095703 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 026 | Total loss: 2.852 | Reg loss: 0.030 | Tree loss: 2.852 | Accuracy: 0.142578 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 026 | Total loss: 2.849 | Reg loss: 0.030 | Tree loss: 2.849 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 026 | Total loss: 2.834 | Reg loss: 0.030 | Tree loss: 2.834 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 026 | Total loss: 2.835 | Reg loss: 0.030 | Tree loss: 2.835 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 026 | Total loss: 2.828 | Reg loss: 0.030 | Tree loss: 2.828 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 026 | Total loss: 2.809 | Reg loss: 0.030 | Tree loss: 2.809 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 026 | Total loss: 2.847 | Reg loss: 0.030 | Tree loss: 2.847 | Accuracy: 0.072266 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 026 | Total loss: 2.821 | Reg loss: 0.030 | Tree loss: 2.821 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 026 | Total loss: 2.789 | Reg loss: 0.030 | Tree loss: 2.789 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 026 | Total loss: 2.791 | Reg loss: 0.030 | Tree loss: 2.791 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 026 | Total loss: 2.803 | Reg loss: 0.030 | Tree loss: 2.803 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 026 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 026 | Total loss: 2.789 | Reg loss: 0.030 | Tree loss: 2.789 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 026 | Total loss: 2.812 | Reg loss: 0.030 | Tree loss: 2.812 | Accuracy: 0.093750 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 026 | Total loss: 2.770 | Reg loss: 0.030 | Tree loss: 2.770 | Accuracy: 0.128906 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 026 | Total loss: 2.776 | Reg loss: 0.030 | Tree loss: 2.776 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 026 | Total loss: 2.784 | Reg loss: 0.030 | Tree loss: 2.784 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 026 | Total loss: 2.775 | Reg loss: 0.030 | Tree loss: 2.775 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 026 | Total loss: 2.762 | Reg loss: 0.030 | Tree loss: 2.762 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 026 | Total loss: 2.737 | Reg loss: 0.030 | Tree loss: 2.737 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 026 | Total loss: 2.772 | Reg loss: 0.030 | Tree loss: 2.772 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 026 | Total loss: 2.743 | Reg loss: 0.030 | Tree loss: 2.743 | Accuracy: 0.119777 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 91 | Batch: 000 / 026 | Total loss: 2.912 | Reg loss: 0.030 | Tree loss: 2.912 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 026 | Total loss: 2.889 | Reg loss: 0.030 | Tree loss: 2.889 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 026 | Total loss: 2.900 | Reg loss: 0.030 | Tree loss: 2.900 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 026 | Total loss: 2.872 | Reg loss: 0.030 | Tree loss: 2.872 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 026 | Total loss: 2.868 | Reg loss: 0.030 | Tree loss: 2.868 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 026 | Total loss: 2.822 | Reg loss: 0.030 | Tree loss: 2.822 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 026 | Total loss: 2.819 | Reg loss: 0.030 | Tree loss: 2.819 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 026 | Total loss: 2.809 | Reg loss: 0.030 | Tree loss: 2.809 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 026 | Total loss: 2.833 | Reg loss: 0.030 | Tree loss: 2.833 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 026 | Total loss: 2.805 | Reg loss: 0.030 | Tree loss: 2.805 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 026 | Total loss: 2.854 | Reg loss: 0.030 | Tree loss: 2.854 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 026 | Total loss: 2.791 | Reg loss: 0.030 | Tree loss: 2.791 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 026 | Total loss: 2.804 | Reg loss: 0.030 | Tree loss: 2.804 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 026 | Total loss: 2.793 | Reg loss: 0.030 | Tree loss: 2.793 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 026 | Total loss: 2.807 | Reg loss: 0.030 | Tree loss: 2.807 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 026 | Total loss: 2.820 | Reg loss: 0.030 | Tree loss: 2.820 | Accuracy: 0.091797 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 026 | Total loss: 2.800 | Reg loss: 0.030 | Tree loss: 2.800 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 026 | Total loss: 2.812 | Reg loss: 0.030 | Tree loss: 2.812 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 026 | Total loss: 2.772 | Reg loss: 0.030 | Tree loss: 2.772 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 026 | Total loss: 2.783 | Reg loss: 0.030 | Tree loss: 2.783 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 026 | Total loss: 2.774 | Reg loss: 0.030 | Tree loss: 2.774 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 026 | Total loss: 2.765 | Reg loss: 0.030 | Tree loss: 2.765 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 026 | Total loss: 2.763 | Reg loss: 0.030 | Tree loss: 2.763 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 026 | Total loss: 2.739 | Reg loss: 0.030 | Tree loss: 2.739 | Accuracy: 0.125000 | 0.062 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 024 / 026 | Total loss: 2.739 | Reg loss: 0.030 | Tree loss: 2.739 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 026 | Total loss: 2.759 | Reg loss: 0.030 | Tree loss: 2.759 | Accuracy: 0.114206 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 92 | Batch: 000 / 026 | Total loss: 2.860 | Reg loss: 0.030 | Tree loss: 2.860 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 026 | Total loss: 2.887 | Reg loss: 0.030 | Tree loss: 2.887 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 026 | Total loss: 2.866 | Reg loss: 0.030 | Tree loss: 2.866 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 026 | Total loss: 2.864 | Reg loss: 0.030 | Tree loss: 2.864 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 026 | Total loss: 2.848 | Reg loss: 0.030 | Tree loss: 2.848 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 026 | Total loss: 2.817 | Reg loss: 0.030 | Tree loss: 2.817 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 026 | Total loss: 2.864 | Reg loss: 0.030 | Tree loss: 2.864 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 026 | Total loss: 2.843 | Reg loss: 0.030 | Tree loss: 2.843 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 026 | Total loss: 2.813 | Reg loss: 0.030 | Tree loss: 2.813 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 026 | Total loss: 2.783 | Reg loss: 0.030 | Tree loss: 2.783 | Accuracy: 0.095703 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 026 | Total loss: 2.832 | Reg loss: 0.030 | Tree loss: 2.832 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 026 | Total loss: 2.832 | Reg loss: 0.030 | Tree loss: 2.832 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 026 | Total loss: 2.779 | Reg loss: 0.030 | Tree loss: 2.779 | Accuracy: 0.144531 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 026 | Total loss: 2.813 | Reg loss: 0.030 | Tree loss: 2.813 | Accuracy: 0.126953 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 026 | Total loss: 2.801 | Reg loss: 0.030 | Tree loss: 2.801 | Accuracy: 0.089844 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 026 | Total loss: 2.803 | Reg loss: 0.030 | Tree loss: 2.803 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 026 | Total loss: 2.788 | Reg loss: 0.030 | Tree loss: 2.788 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 026 | Total loss: 2.756 | Reg loss: 0.030 | Tree loss: 2.756 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 026 | Total loss: 2.827 | Reg loss: 0.030 | Tree loss: 2.827 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 026 | Total loss: 2.799 | Reg loss: 0.030 | Tree loss: 2.799 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 026 | Total loss: 2.774 | Reg loss: 0.030 | Tree loss: 2.774 | Accuracy: 0.130859 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 026 | Total loss: 2.773 | Reg loss: 0.030 | Tree loss: 2.773 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 026 | Total loss: 2.777 | Reg loss: 0.030 | Tree loss: 2.777 | Accuracy: 0.154297 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 026 | Total loss: 2.743 | Reg loss: 0.030 | Tree loss: 2.743 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 026 | Total loss: 2.768 | Reg loss: 0.030 | Tree loss: 2.768 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 026 | Total loss: 2.764 | Reg loss: 0.030 | Tree loss: 2.764 | Accuracy: 0.105850 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 93 | Batch: 000 / 026 | Total loss: 2.879 | Reg loss: 0.030 | Tree loss: 2.879 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 026 | Total loss: 2.875 | Reg loss: 0.030 | Tree loss: 2.875 | Accuracy: 0.130859 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 026 | Total loss: 2.879 | Reg loss: 0.030 | Tree loss: 2.879 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 026 | Total loss: 2.827 | Reg loss: 0.030 | Tree loss: 2.827 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 026 | Total loss: 2.845 | Reg loss: 0.030 | Tree loss: 2.845 | Accuracy: 0.085938 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 026 | Total loss: 2.826 | Reg loss: 0.030 | Tree loss: 2.826 | Accuracy: 0.128906 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 026 | Total loss: 2.859 | Reg loss: 0.030 | Tree loss: 2.859 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 026 | Total loss: 2.836 | Reg loss: 0.030 | Tree loss: 2.836 | Accuracy: 0.091797 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 026 | Total loss: 2.796 | Reg loss: 0.030 | Tree loss: 2.796 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 026 | Total loss: 2.796 | Reg loss: 0.030 | Tree loss: 2.796 | Accuracy: 0.128906 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 026 | Total loss: 2.812 | Reg loss: 0.030 | Tree loss: 2.812 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 026 | Total loss: 2.796 | Reg loss: 0.030 | Tree loss: 2.796 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 026 | Total loss: 2.835 | Reg loss: 0.030 | Tree loss: 2.835 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 026 | Total loss: 2.804 | Reg loss: 0.030 | Tree loss: 2.804 | Accuracy: 0.150391 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 026 | Total loss: 2.774 | Reg loss: 0.030 | Tree loss: 2.774 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 026 | Total loss: 2.780 | Reg loss: 0.030 | Tree loss: 2.780 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 026 | Total loss: 2.828 | Reg loss: 0.030 | Tree loss: 2.828 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 026 | Total loss: 2.814 | Reg loss: 0.030 | Tree loss: 2.814 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 026 | Total loss: 2.765 | Reg loss: 0.030 | Tree loss: 2.765 | Accuracy: 0.136719 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 026 | Total loss: 2.777 | Reg loss: 0.030 | Tree loss: 2.777 | Accuracy: 0.126953 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 026 | Total loss: 2.779 | Reg loss: 0.030 | Tree loss: 2.779 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 026 | Total loss: 2.750 | Reg loss: 0.030 | Tree loss: 2.750 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 026 | Total loss: 2.775 | Reg loss: 0.030 | Tree loss: 2.775 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 026 | Total loss: 2.780 | Reg loss: 0.030 | Tree loss: 2.780 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 026 | Total loss: 2.776 | Reg loss: 0.030 | Tree loss: 2.776 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 026 | Total loss: 2.773 | Reg loss: 0.030 | Tree loss: 2.773 | Accuracy: 0.111421 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 94 | Batch: 000 / 026 | Total loss: 2.880 | Reg loss: 0.030 | Tree loss: 2.880 | Accuracy: 0.095703 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 026 | Total loss: 2.887 | Reg loss: 0.030 | Tree loss: 2.887 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 026 | Total loss: 2.837 | Reg loss: 0.030 | Tree loss: 2.837 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 026 | Total loss: 2.865 | Reg loss: 0.030 | Tree loss: 2.865 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 026 | Total loss: 2.844 | Reg loss: 0.030 | Tree loss: 2.844 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 026 | Total loss: 2.859 | Reg loss: 0.030 | Tree loss: 2.859 | Accuracy: 0.095703 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 026 | Total loss: 2.837 | Reg loss: 0.030 | Tree loss: 2.837 | Accuracy: 0.144531 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 026 | Total loss: 2.840 | Reg loss: 0.030 | Tree loss: 2.840 | Accuracy: 0.136719 | 0.062 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 008 / 026 | Total loss: 2.822 | Reg loss: 0.030 | Tree loss: 2.822 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 026 | Total loss: 2.834 | Reg loss: 0.030 | Tree loss: 2.834 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.091797 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 026 | Total loss: 2.802 | Reg loss: 0.030 | Tree loss: 2.802 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 026 | Total loss: 2.798 | Reg loss: 0.030 | Tree loss: 2.798 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 026 | Total loss: 2.815 | Reg loss: 0.030 | Tree loss: 2.815 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 026 | Total loss: 2.802 | Reg loss: 0.030 | Tree loss: 2.802 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 026 | Total loss: 2.775 | Reg loss: 0.030 | Tree loss: 2.775 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 026 | Total loss: 2.781 | Reg loss: 0.030 | Tree loss: 2.781 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 026 | Total loss: 2.777 | Reg loss: 0.030 | Tree loss: 2.777 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 026 | Total loss: 2.810 | Reg loss: 0.030 | Tree loss: 2.810 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 026 | Total loss: 2.738 | Reg loss: 0.030 | Tree loss: 2.738 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 026 | Total loss: 2.784 | Reg loss: 0.030 | Tree loss: 2.784 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 026 | Total loss: 2.765 | Reg loss: 0.030 | Tree loss: 2.765 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 026 | Total loss: 2.726 | Reg loss: 0.030 | Tree loss: 2.726 | Accuracy: 0.154297 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 026 | Total loss: 2.736 | Reg loss: 0.030 | Tree loss: 2.736 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 026 | Total loss: 2.756 | Reg loss: 0.030 | Tree loss: 2.756 | Accuracy: 0.108635 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 95 | Batch: 000 / 026 | Total loss: 2.847 | Reg loss: 0.030 | Tree loss: 2.847 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 026 | Total loss: 2.863 | Reg loss: 0.030 | Tree loss: 2.863 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 026 | Total loss: 2.878 | Reg loss: 0.030 | Tree loss: 2.878 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 026 | Total loss: 2.844 | Reg loss: 0.030 | Tree loss: 2.844 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 026 | Total loss: 2.879 | Reg loss: 0.030 | Tree loss: 2.879 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 026 | Total loss: 2.865 | Reg loss: 0.030 | Tree loss: 2.865 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 026 | Total loss: 2.845 | Reg loss: 0.030 | Tree loss: 2.845 | Accuracy: 0.126953 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 026 | Total loss: 2.840 | Reg loss: 0.030 | Tree loss: 2.840 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 026 | Total loss: 2.847 | Reg loss: 0.030 | Tree loss: 2.847 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 026 | Total loss: 2.841 | Reg loss: 0.030 | Tree loss: 2.841 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 026 | Total loss: 2.850 | Reg loss: 0.030 | Tree loss: 2.850 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 026 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 026 | Total loss: 2.803 | Reg loss: 0.030 | Tree loss: 2.803 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 026 | Total loss: 2.794 | Reg loss: 0.030 | Tree loss: 2.794 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 026 | Total loss: 2.816 | Reg loss: 0.030 | Tree loss: 2.816 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 026 | Total loss: 2.760 | Reg loss: 0.030 | Tree loss: 2.760 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 026 | Total loss: 2.765 | Reg loss: 0.030 | Tree loss: 2.765 | Accuracy: 0.150391 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 026 | Total loss: 2.766 | Reg loss: 0.030 | Tree loss: 2.766 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 026 | Total loss: 2.770 | Reg loss: 0.030 | Tree loss: 2.770 | Accuracy: 0.091797 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 026 | Total loss: 2.745 | Reg loss: 0.030 | Tree loss: 2.745 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 026 | Total loss: 2.770 | Reg loss: 0.030 | Tree loss: 2.770 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 026 | Total loss: 2.756 | Reg loss: 0.030 | Tree loss: 2.756 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 026 | Total loss: 2.720 | Reg loss: 0.030 | Tree loss: 2.720 | Accuracy: 0.148438 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 026 | Total loss: 2.746 | Reg loss: 0.030 | Tree loss: 2.746 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.077994 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 96 | Batch: 000 / 026 | Total loss: 2.884 | Reg loss: 0.030 | Tree loss: 2.884 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 026 | Total loss: 2.889 | Reg loss: 0.030 | Tree loss: 2.889 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 026 | Total loss: 2.872 | Reg loss: 0.030 | Tree loss: 2.872 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 026 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 026 | Total loss: 2.856 | Reg loss: 0.030 | Tree loss: 2.856 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 026 | Total loss: 2.843 | Reg loss: 0.030 | Tree loss: 2.843 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 026 | Total loss: 2.825 | Reg loss: 0.030 | Tree loss: 2.825 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 026 | Total loss: 2.821 | Reg loss: 0.030 | Tree loss: 2.821 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 026 | Total loss: 2.834 | Reg loss: 0.030 | Tree loss: 2.834 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 026 | Total loss: 2.855 | Reg loss: 0.030 | Tree loss: 2.855 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 026 | Total loss: 2.800 | Reg loss: 0.030 | Tree loss: 2.800 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 026 | Total loss: 2.822 | Reg loss: 0.030 | Tree loss: 2.822 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 026 | Total loss: 2.787 | Reg loss: 0.030 | Tree loss: 2.787 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 026 | Total loss: 2.780 | Reg loss: 0.030 | Tree loss: 2.780 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 026 | Total loss: 2.789 | Reg loss: 0.030 | Tree loss: 2.789 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 026 | Total loss: 2.785 | Reg loss: 0.030 | Tree loss: 2.785 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 026 | Total loss: 2.757 | Reg loss: 0.030 | Tree loss: 2.757 | Accuracy: 0.125000 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 026 | Total loss: 2.787 | Reg loss: 0.030 | Tree loss: 2.787 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 026 | Total loss: 2.751 | Reg loss: 0.030 | Tree loss: 2.751 | Accuracy: 0.132812 | 0.062 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Batch: 020 / 026 | Total loss: 2.780 | Reg loss: 0.030 | Tree loss: 2.780 | Accuracy: 0.080078 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 026 | Total loss: 2.761 | Reg loss: 0.030 | Tree loss: 2.761 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 026 | Total loss: 2.787 | Reg loss: 0.030 | Tree loss: 2.787 | Accuracy: 0.115234 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 026 | Total loss: 2.765 | Reg loss: 0.030 | Tree loss: 2.765 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 026 | Total loss: 2.738 | Reg loss: 0.030 | Tree loss: 2.738 | Accuracy: 0.132812 | 0.062 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 026 | Total loss: 2.745 | Reg loss: 0.030 | Tree loss: 2.745 | Accuracy: 0.144847 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 97 | Batch: 000 / 026 | Total loss: 2.866 | Reg loss: 0.030 | Tree loss: 2.866 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 026 | Total loss: 2.847 | Reg loss: 0.030 | Tree loss: 2.847 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 026 | Total loss: 2.898 | Reg loss: 0.030 | Tree loss: 2.898 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 026 | Total loss: 2.861 | Reg loss: 0.030 | Tree loss: 2.861 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 026 | Total loss: 2.858 | Reg loss: 0.030 | Tree loss: 2.858 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 026 | Total loss: 2.810 | Reg loss: 0.030 | Tree loss: 2.810 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 026 | Total loss: 2.826 | Reg loss: 0.030 | Tree loss: 2.826 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 026 | Total loss: 2.848 | Reg loss: 0.030 | Tree loss: 2.848 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 026 | Total loss: 2.852 | Reg loss: 0.030 | Tree loss: 2.852 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 026 | Total loss: 2.814 | Reg loss: 0.030 | Tree loss: 2.814 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 026 | Total loss: 2.819 | Reg loss: 0.030 | Tree loss: 2.819 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 026 | Total loss: 2.798 | Reg loss: 0.030 | Tree loss: 2.798 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 026 | Total loss: 2.781 | Reg loss: 0.030 | Tree loss: 2.781 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 026 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 026 | Total loss: 2.766 | Reg loss: 0.030 | Tree loss: 2.766 | Accuracy: 0.130859 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 026 | Total loss: 2.801 | Reg loss: 0.030 | Tree loss: 2.801 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 026 | Total loss: 2.799 | Reg loss: 0.030 | Tree loss: 2.799 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 026 | Total loss: 2.761 | Reg loss: 0.030 | Tree loss: 2.761 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 026 | Total loss: 2.745 | Reg loss: 0.030 | Tree loss: 2.745 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 026 | Total loss: 2.795 | Reg loss: 0.030 | Tree loss: 2.795 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 026 | Total loss: 2.779 | Reg loss: 0.030 | Tree loss: 2.779 | Accuracy: 0.085938 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 026 | Total loss: 2.791 | Reg loss: 0.030 | Tree loss: 2.791 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 026 | Total loss: 2.746 | Reg loss: 0.030 | Tree loss: 2.746 | Accuracy: 0.130859 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 026 | Total loss: 2.767 | Reg loss: 0.030 | Tree loss: 2.767 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 026 | Total loss: 2.761 | Reg loss: 0.030 | Tree loss: 2.761 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 026 | Total loss: 2.710 | Reg loss: 0.030 | Tree loss: 2.710 | Accuracy: 0.125348 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 98 | Batch: 000 / 026 | Total loss: 2.855 | Reg loss: 0.030 | Tree loss: 2.855 | Accuracy: 0.121094 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 026 | Total loss: 2.893 | Reg loss: 0.030 | Tree loss: 2.893 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 026 | Total loss: 2.869 | Reg loss: 0.030 | Tree loss: 2.869 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 026 | Total loss: 2.843 | Reg loss: 0.030 | Tree loss: 2.843 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 026 | Total loss: 2.860 | Reg loss: 0.030 | Tree loss: 2.860 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 026 | Total loss: 2.831 | Reg loss: 0.030 | Tree loss: 2.831 | Accuracy: 0.128906 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 026 | Total loss: 2.826 | Reg loss: 0.030 | Tree loss: 2.826 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 026 | Total loss: 2.849 | Reg loss: 0.030 | Tree loss: 2.849 | Accuracy: 0.087891 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 026 | Total loss: 2.814 | Reg loss: 0.030 | Tree loss: 2.814 | Accuracy: 0.134766 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 026 | Total loss: 2.809 | Reg loss: 0.030 | Tree loss: 2.809 | Accuracy: 0.095703 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 026 | Total loss: 2.835 | Reg loss: 0.030 | Tree loss: 2.835 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 026 | Total loss: 2.797 | Reg loss: 0.030 | Tree loss: 2.797 | Accuracy: 0.109375 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 026 | Total loss: 2.802 | Reg loss: 0.030 | Tree loss: 2.802 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 026 | Total loss: 2.798 | Reg loss: 0.030 | Tree loss: 2.798 | Accuracy: 0.093750 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 026 | Total loss: 2.795 | Reg loss: 0.030 | Tree loss: 2.795 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 026 | Total loss: 2.798 | Reg loss: 0.030 | Tree loss: 2.798 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 026 | Total loss: 2.794 | Reg loss: 0.030 | Tree loss: 2.794 | Accuracy: 0.123047 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 026 | Total loss: 2.755 | Reg loss: 0.030 | Tree loss: 2.755 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 026 | Total loss: 2.733 | Reg loss: 0.030 | Tree loss: 2.733 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 026 | Total loss: 2.778 | Reg loss: 0.030 | Tree loss: 2.778 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 026 | Total loss: 2.767 | Reg loss: 0.030 | Tree loss: 2.767 | Accuracy: 0.148438 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 026 | Total loss: 2.733 | Reg loss: 0.030 | Tree loss: 2.733 | Accuracy: 0.128906 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 026 | Total loss: 2.757 | Reg loss: 0.030 | Tree loss: 2.757 | Accuracy: 0.126953 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 026 | Total loss: 2.760 | Reg loss: 0.030 | Tree loss: 2.760 | Accuracy: 0.136719 | 0.062 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 026 | Total loss: 2.758 | Reg loss: 0.030 | Tree loss: 2.758 | Accuracy: 0.111421 | 0.062 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 99 | Batch: 000 / 026 | Total loss: 2.871 | Reg loss: 0.030 | Tree loss: 2.871 | Accuracy: 0.138672 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 026 | Total loss: 2.873 | Reg loss: 0.030 | Tree loss: 2.873 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 026 | Total loss: 2.828 | Reg loss: 0.030 | Tree loss: 2.828 | Accuracy: 0.136719 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 026 | Total loss: 2.859 | Reg loss: 0.030 | Tree loss: 2.859 | Accuracy: 0.107422 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 026 | Total loss: 2.875 | Reg loss: 0.030 | Tree loss: 2.875 | Accuracy: 0.117188 | 0.062 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 005 / 026 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.142578 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 026 | Total loss: 2.850 | Reg loss: 0.030 | Tree loss: 2.850 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 026 | Total loss: 2.834 | Reg loss: 0.030 | Tree loss: 2.834 | Accuracy: 0.097656 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 026 | Total loss: 2.827 | Reg loss: 0.030 | Tree loss: 2.827 | Accuracy: 0.103516 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 026 | Total loss: 2.814 | Reg loss: 0.030 | Tree loss: 2.814 | Accuracy: 0.113281 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 026 | Total loss: 2.837 | Reg loss: 0.030 | Tree loss: 2.837 | Accuracy: 0.091797 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 026 | Total loss: 2.818 | Reg loss: 0.030 | Tree loss: 2.818 | Accuracy: 0.089844 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 026 | Total loss: 2.781 | Reg loss: 0.030 | Tree loss: 2.781 | Accuracy: 0.099609 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 026 | Total loss: 2.799 | Reg loss: 0.030 | Tree loss: 2.799 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 026 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.150391 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 026 | Total loss: 2.796 | Reg loss: 0.030 | Tree loss: 2.796 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 026 | Total loss: 2.803 | Reg loss: 0.030 | Tree loss: 2.803 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 026 | Total loss: 2.770 | Reg loss: 0.030 | Tree loss: 2.770 | Accuracy: 0.111328 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 026 | Total loss: 2.779 | Reg loss: 0.030 | Tree loss: 2.779 | Accuracy: 0.101562 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 026 | Total loss: 2.743 | Reg loss: 0.030 | Tree loss: 2.743 | Accuracy: 0.142578 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 026 | Total loss: 2.743 | Reg loss: 0.030 | Tree loss: 2.743 | Accuracy: 0.146484 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 026 | Total loss: 2.750 | Reg loss: 0.030 | Tree loss: 2.750 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 026 | Total loss: 2.786 | Reg loss: 0.030 | Tree loss: 2.786 | Accuracy: 0.105469 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 026 | Total loss: 2.748 | Reg loss: 0.030 | Tree loss: 2.748 | Accuracy: 0.117188 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 026 | Total loss: 2.742 | Reg loss: 0.030 | Tree loss: 2.742 | Accuracy: 0.119141 | 0.062 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 026 | Total loss: 2.745 | Reg loss: 0.030 | Tree loss: 2.745 | Accuracy: 0.116992 | 0.062 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c8aaee42064a76b02e2e0d7a5fe781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6767e7280354fd8973da02c948f081e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3333bc42ec4dbbbcc3af4d595830f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d05c23bc5744c7b95ea811ed730bc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 5.595238095238095\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 42\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "9744\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "3415\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "Average comprehensibility: 26.047619047619047\n",
      "std comprehensibility: 3.2215966239205422\n",
      "var comprehensibility: 10.378684807256237\n",
      "minimum comprehensibility: 18\n",
      "maximum comprehensibility: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
