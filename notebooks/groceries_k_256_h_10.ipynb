{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 256\n",
    "tree_depth = 10\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.164484977722168 | KNN Loss: 6.233161449432373 | BCE Loss: 1.931323766708374\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.180647850036621 | KNN Loss: 6.2330732345581055 | BCE Loss: 1.9475743770599365\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.156098365783691 | KNN Loss: 6.233052730560303 | BCE Loss: 1.9230459928512573\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.168828964233398 | KNN Loss: 6.232822418212891 | BCE Loss: 1.936007022857666\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.191194534301758 | KNN Loss: 6.232855319976807 | BCE Loss: 1.9583394527435303\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.178317070007324 | KNN Loss: 6.232844352722168 | BCE Loss: 1.9454725980758667\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.167888641357422 | KNN Loss: 6.2326531410217285 | BCE Loss: 1.935235619544983\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.10910415649414 | KNN Loss: 6.23281192779541 | BCE Loss: 1.8762925863265991\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.098739624023438 | KNN Loss: 6.2327985763549805 | BCE Loss: 1.865940809249878\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.141820907592773 | KNN Loss: 6.232586860656738 | BCE Loss: 1.909233570098877\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.099678993225098 | KNN Loss: 6.232430934906006 | BCE Loss: 1.8672479391098022\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.089191436767578 | KNN Loss: 6.2324724197387695 | BCE Loss: 1.8567190170288086\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.099985122680664 | KNN Loss: 6.2324137687683105 | BCE Loss: 1.8675708770751953\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.105389595031738 | KNN Loss: 6.232148170471191 | BCE Loss: 1.8732411861419678\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.0897216796875 | KNN Loss: 6.232430458068848 | BCE Loss: 1.8572909832000732\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.036067008972168 | KNN Loss: 6.232306003570557 | BCE Loss: 1.8037607669830322\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.079413414001465 | KNN Loss: 6.232236385345459 | BCE Loss: 1.8471771478652954\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.052001953125 | KNN Loss: 6.2323455810546875 | BCE Loss: 1.8196568489074707\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.081453323364258 | KNN Loss: 6.23206090927124 | BCE Loss: 1.8493921756744385\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.036130905151367 | KNN Loss: 6.231955528259277 | BCE Loss: 1.8041753768920898\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.03782844543457 | KNN Loss: 6.232043743133545 | BCE Loss: 1.8057849407196045\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.006780624389648 | KNN Loss: 6.23189640045166 | BCE Loss: 1.7748839855194092\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.022738456726074 | KNN Loss: 6.231937885284424 | BCE Loss: 1.7908005714416504\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.97171688079834 | KNN Loss: 6.231855392456055 | BCE Loss: 1.7398616075515747\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.984252452850342 | KNN Loss: 6.231667518615723 | BCE Loss: 1.7525848150253296\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.992218971252441 | KNN Loss: 6.231520652770996 | BCE Loss: 1.7606985569000244\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.933624267578125 | KNN Loss: 6.231611728668213 | BCE Loss: 1.702012300491333\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.9881744384765625 | KNN Loss: 6.231358051300049 | BCE Loss: 1.7568161487579346\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.981368541717529 | KNN Loss: 6.231101989746094 | BCE Loss: 1.7502665519714355\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.944596290588379 | KNN Loss: 6.231103897094727 | BCE Loss: 1.7134926319122314\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.934881210327148 | KNN Loss: 6.230947017669678 | BCE Loss: 1.7039340734481812\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.932301998138428 | KNN Loss: 6.230843544006348 | BCE Loss: 1.70145845413208\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.872594833374023 | KNN Loss: 6.230825901031494 | BCE Loss: 1.6417688131332397\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.910157203674316 | KNN Loss: 6.230522155761719 | BCE Loss: 1.6796348094940186\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.908782482147217 | KNN Loss: 6.230816841125488 | BCE Loss: 1.677965760231018\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.904254913330078 | KNN Loss: 6.230409622192383 | BCE Loss: 1.6738451719284058\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.847460746765137 | KNN Loss: 6.230702877044678 | BCE Loss: 1.6167576313018799\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.841198921203613 | KNN Loss: 6.230469226837158 | BCE Loss: 1.6107299327850342\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.80493688583374 | KNN Loss: 6.230141639709473 | BCE Loss: 1.5747952461242676\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.7838592529296875 | KNN Loss: 6.229913711547852 | BCE Loss: 1.553945779800415\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.838254928588867 | KNN Loss: 6.22920560836792 | BCE Loss: 1.6090490818023682\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.843879699707031 | KNN Loss: 6.229453086853027 | BCE Loss: 1.6144263744354248\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.770959854125977 | KNN Loss: 6.228670597076416 | BCE Loss: 1.5422892570495605\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.793466567993164 | KNN Loss: 6.228456497192383 | BCE Loss: 1.5650101900100708\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.724935531616211 | KNN Loss: 6.228640079498291 | BCE Loss: 1.4962952136993408\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.745862007141113 | KNN Loss: 6.228019714355469 | BCE Loss: 1.5178422927856445\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.671238899230957 | KNN Loss: 6.228222370147705 | BCE Loss: 1.443016529083252\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.646368980407715 | KNN Loss: 6.227056980133057 | BCE Loss: 1.4193121194839478\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.652807712554932 | KNN Loss: 6.226591110229492 | BCE Loss: 1.426216721534729\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.635251522064209 | KNN Loss: 6.226932048797607 | BCE Loss: 1.4083195924758911\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.609889984130859 | KNN Loss: 6.226551055908203 | BCE Loss: 1.3833386898040771\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.604094505310059 | KNN Loss: 6.225959300994873 | BCE Loss: 1.3781349658966064\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.576658725738525 | KNN Loss: 6.225449562072754 | BCE Loss: 1.3512091636657715\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.56131649017334 | KNN Loss: 6.225047588348389 | BCE Loss: 1.3362691402435303\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.518230438232422 | KNN Loss: 6.224005699157715 | BCE Loss: 1.2942249774932861\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.519364356994629 | KNN Loss: 6.223055839538574 | BCE Loss: 1.2963085174560547\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.500175476074219 | KNN Loss: 6.223172664642334 | BCE Loss: 1.2770029306411743\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.465943813323975 | KNN Loss: 6.221642017364502 | BCE Loss: 1.2443017959594727\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.445682048797607 | KNN Loss: 6.220733642578125 | BCE Loss: 1.2249482870101929\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.4200944900512695 | KNN Loss: 6.219675064086914 | BCE Loss: 1.2004191875457764\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.4182538986206055 | KNN Loss: 6.219200611114502 | BCE Loss: 1.1990532875061035\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.391160488128662 | KNN Loss: 6.216925621032715 | BCE Loss: 1.1742349863052368\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.395838260650635 | KNN Loss: 6.217556476593018 | BCE Loss: 1.1782816648483276\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.359443664550781 | KNN Loss: 6.214324474334717 | BCE Loss: 1.145119309425354\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.377110481262207 | KNN Loss: 6.215729713439941 | BCE Loss: 1.1613807678222656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.361576080322266 | KNN Loss: 6.212650775909424 | BCE Loss: 1.148925542831421\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.327655792236328 | KNN Loss: 6.208916664123535 | BCE Loss: 1.118739128112793\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.329785346984863 | KNN Loss: 6.207803249359131 | BCE Loss: 1.1219818592071533\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 7.311932563781738 | KNN Loss: 6.206353664398193 | BCE Loss: 1.105578899383545\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 7.303010940551758 | KNN Loss: 6.206175327301025 | BCE Loss: 1.0968353748321533\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 7.301403045654297 | KNN Loss: 6.20162296295166 | BCE Loss: 1.0997802019119263\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 7.295412540435791 | KNN Loss: 6.200352191925049 | BCE Loss: 1.0950604677200317\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 7.2853102684021 | KNN Loss: 6.198827266693115 | BCE Loss: 1.0864830017089844\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 7.321405410766602 | KNN Loss: 6.194389820098877 | BCE Loss: 1.1270155906677246\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 7.314347267150879 | KNN Loss: 6.1907758712768555 | BCE Loss: 1.123571515083313\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 7.273083686828613 | KNN Loss: 6.185291767120361 | BCE Loss: 1.087791919708252\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 7.238108158111572 | KNN Loss: 6.183235168457031 | BCE Loss: 1.0548728704452515\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 7.278857231140137 | KNN Loss: 6.178854465484619 | BCE Loss: 1.1000027656555176\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 7.285258769989014 | KNN Loss: 6.175023555755615 | BCE Loss: 1.110235333442688\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 7.258528232574463 | KNN Loss: 6.171766757965088 | BCE Loss: 1.0867615938186646\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 7.246779441833496 | KNN Loss: 6.166545391082764 | BCE Loss: 1.0802338123321533\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 7.234769821166992 | KNN Loss: 6.1572957038879395 | BCE Loss: 1.0774739980697632\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 7.2266106605529785 | KNN Loss: 6.150173187255859 | BCE Loss: 1.0764374732971191\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 7.214743137359619 | KNN Loss: 6.144619941711426 | BCE Loss: 1.0701231956481934\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 7.239932060241699 | KNN Loss: 6.139106273651123 | BCE Loss: 1.1008259057998657\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 7.204570770263672 | KNN Loss: 6.1261305809021 | BCE Loss: 1.0784404277801514\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 7.199316024780273 | KNN Loss: 6.120120048522949 | BCE Loss: 1.0791958570480347\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 7.146212577819824 | KNN Loss: 6.113532066345215 | BCE Loss: 1.0326805114746094\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 7.193488121032715 | KNN Loss: 6.102010250091553 | BCE Loss: 1.091477870941162\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 7.191462516784668 | KNN Loss: 6.096312522888184 | BCE Loss: 1.0951497554779053\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 7.175269603729248 | KNN Loss: 6.081890106201172 | BCE Loss: 1.0933796167373657\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 7.1239118576049805 | KNN Loss: 6.062166213989258 | BCE Loss: 1.0617454051971436\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 7.1450934410095215 | KNN Loss: 6.068448543548584 | BCE Loss: 1.076645016670227\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 7.121232032775879 | KNN Loss: 6.052329063415527 | BCE Loss: 1.0689027309417725\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 7.0960493087768555 | KNN Loss: 6.028609275817871 | BCE Loss: 1.0674399137496948\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 7.104001522064209 | KNN Loss: 6.026814937591553 | BCE Loss: 1.0771867036819458\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 7.083309173583984 | KNN Loss: 6.003523349761963 | BCE Loss: 1.0797855854034424\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 7.063623905181885 | KNN Loss: 5.996456623077393 | BCE Loss: 1.0671672821044922\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 7.073298931121826 | KNN Loss: 5.986927032470703 | BCE Loss: 1.086371898651123\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 7.03999137878418 | KNN Loss: 5.9556193351745605 | BCE Loss: 1.0843722820281982\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 7.011731147766113 | KNN Loss: 5.9382147789001465 | BCE Loss: 1.0735161304473877\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 6.987308979034424 | KNN Loss: 5.9251909255981445 | BCE Loss: 1.0621180534362793\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 7.008673667907715 | KNN Loss: 5.929172039031982 | BCE Loss: 1.0795016288757324\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.993802547454834 | KNN Loss: 5.918994903564453 | BCE Loss: 1.0748077630996704\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.9723968505859375 | KNN Loss: 5.912355422973633 | BCE Loss: 1.0600415468215942\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.9175004959106445 | KNN Loss: 5.871438503265381 | BCE Loss: 1.0460617542266846\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.931777477264404 | KNN Loss: 5.8592376708984375 | BCE Loss: 1.0725399255752563\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.9353227615356445 | KNN Loss: 5.836527347564697 | BCE Loss: 1.0987951755523682\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.867009162902832 | KNN Loss: 5.827131748199463 | BCE Loss: 1.0398772954940796\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.898630142211914 | KNN Loss: 5.837486267089844 | BCE Loss: 1.0611438751220703\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.901994705200195 | KNN Loss: 5.84706449508667 | BCE Loss: 1.0549300909042358\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.841554641723633 | KNN Loss: 5.816264629364014 | BCE Loss: 1.0252900123596191\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.858946800231934 | KNN Loss: 5.806850910186768 | BCE Loss: 1.0520961284637451\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.854459762573242 | KNN Loss: 5.797861576080322 | BCE Loss: 1.0565979480743408\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.86079216003418 | KNN Loss: 5.809170246124268 | BCE Loss: 1.051621675491333\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.836554527282715 | KNN Loss: 5.782543182373047 | BCE Loss: 1.054011583328247\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.90809965133667 | KNN Loss: 5.824323654174805 | BCE Loss: 1.0837759971618652\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.831399917602539 | KNN Loss: 5.774032115936279 | BCE Loss: 1.0573678016662598\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.818122863769531 | KNN Loss: 5.762875080108643 | BCE Loss: 1.0552476644515991\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.835575103759766 | KNN Loss: 5.76716947555542 | BCE Loss: 1.0684056282043457\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.8299078941345215 | KNN Loss: 5.75238037109375 | BCE Loss: 1.0775275230407715\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.8245625495910645 | KNN Loss: 5.770694255828857 | BCE Loss: 1.053868293762207\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.819448947906494 | KNN Loss: 5.760681629180908 | BCE Loss: 1.058767318725586\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.8570990562438965 | KNN Loss: 5.784967422485352 | BCE Loss: 1.072131633758545\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.785677909851074 | KNN Loss: 5.722614765167236 | BCE Loss: 1.063063383102417\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.816728115081787 | KNN Loss: 5.754720687866211 | BCE Loss: 1.0620074272155762\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.788985252380371 | KNN Loss: 5.749851703643799 | BCE Loss: 1.0391335487365723\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.831375598907471 | KNN Loss: 5.759349822998047 | BCE Loss: 1.0720256567001343\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.799229621887207 | KNN Loss: 5.735077857971191 | BCE Loss: 1.0641518831253052\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.773416996002197 | KNN Loss: 5.716677188873291 | BCE Loss: 1.0567398071289062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.777968406677246 | KNN Loss: 5.711792945861816 | BCE Loss: 1.0661754608154297\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.72640323638916 | KNN Loss: 5.690771579742432 | BCE Loss: 1.0356318950653076\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.781209945678711 | KNN Loss: 5.708673000335693 | BCE Loss: 1.0725367069244385\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.753395080566406 | KNN Loss: 5.69069766998291 | BCE Loss: 1.0626976490020752\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.7406487464904785 | KNN Loss: 5.6938862800598145 | BCE Loss: 1.0467623472213745\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.773418426513672 | KNN Loss: 5.704648971557617 | BCE Loss: 1.0687696933746338\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.787770748138428 | KNN Loss: 5.717114448547363 | BCE Loss: 1.0706562995910645\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.706891059875488 | KNN Loss: 5.66427755355835 | BCE Loss: 1.0426132678985596\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.748944282531738 | KNN Loss: 5.703129291534424 | BCE Loss: 1.0458152294158936\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.767324924468994 | KNN Loss: 5.7009758949279785 | BCE Loss: 1.0663490295410156\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.747530937194824 | KNN Loss: 5.685843467712402 | BCE Loss: 1.0616874694824219\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.710274696350098 | KNN Loss: 5.66556978225708 | BCE Loss: 1.0447051525115967\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.727248191833496 | KNN Loss: 5.667303562164307 | BCE Loss: 1.0599443912506104\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.799108505249023 | KNN Loss: 5.724016189575195 | BCE Loss: 1.0750925540924072\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.704181671142578 | KNN Loss: 5.652093887329102 | BCE Loss: 1.0520879030227661\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.732803821563721 | KNN Loss: 5.672434329986572 | BCE Loss: 1.0603694915771484\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.713273525238037 | KNN Loss: 5.65986442565918 | BCE Loss: 1.0534089803695679\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.790225028991699 | KNN Loss: 5.761824131011963 | BCE Loss: 1.0284006595611572\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.7404022216796875 | KNN Loss: 5.675439834594727 | BCE Loss: 1.064962387084961\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.702783584594727 | KNN Loss: 5.65167760848999 | BCE Loss: 1.0511062145233154\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.705452919006348 | KNN Loss: 5.650770664215088 | BCE Loss: 1.0546820163726807\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.729922294616699 | KNN Loss: 5.673621654510498 | BCE Loss: 1.0563007593154907\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.723123073577881 | KNN Loss: 5.657871723175049 | BCE Loss: 1.0652514696121216\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.722116470336914 | KNN Loss: 5.67374849319458 | BCE Loss: 1.0483677387237549\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.703521728515625 | KNN Loss: 5.6529364585876465 | BCE Loss: 1.0505850315093994\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.703795433044434 | KNN Loss: 5.639252662658691 | BCE Loss: 1.0645427703857422\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.730018138885498 | KNN Loss: 5.667967796325684 | BCE Loss: 1.0620503425598145\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.707627773284912 | KNN Loss: 5.652507305145264 | BCE Loss: 1.0551204681396484\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.740480422973633 | KNN Loss: 5.680540561676025 | BCE Loss: 1.0599397420883179\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.744994163513184 | KNN Loss: 5.689342498779297 | BCE Loss: 1.0556514263153076\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.696545600891113 | KNN Loss: 5.634311199188232 | BCE Loss: 1.0622341632843018\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.747539043426514 | KNN Loss: 5.683170795440674 | BCE Loss: 1.0643681287765503\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.703675270080566 | KNN Loss: 5.648178577423096 | BCE Loss: 1.0554969310760498\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.737560272216797 | KNN Loss: 5.6854047775268555 | BCE Loss: 1.0521553754806519\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.705929279327393 | KNN Loss: 5.646844863891602 | BCE Loss: 1.059084415435791\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.68203592300415 | KNN Loss: 5.640995979309082 | BCE Loss: 1.0410399436950684\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.75175666809082 | KNN Loss: 5.68913459777832 | BCE Loss: 1.0626220703125\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.697366714477539 | KNN Loss: 5.652358531951904 | BCE Loss: 1.0450084209442139\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.673374176025391 | KNN Loss: 5.64058256149292 | BCE Loss: 1.0327917337417603\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.721218109130859 | KNN Loss: 5.688819408416748 | BCE Loss: 1.0323987007141113\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.686387062072754 | KNN Loss: 5.630163669586182 | BCE Loss: 1.0562232732772827\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.659801006317139 | KNN Loss: 5.621469497680664 | BCE Loss: 1.0383315086364746\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.731273651123047 | KNN Loss: 5.667365074157715 | BCE Loss: 1.063908338546753\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.6826491355896 | KNN Loss: 5.6326375007629395 | BCE Loss: 1.0500116348266602\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.727453231811523 | KNN Loss: 5.653817176818848 | BCE Loss: 1.0736358165740967\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.685407638549805 | KNN Loss: 5.640958786010742 | BCE Loss: 1.0444490909576416\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.697185516357422 | KNN Loss: 5.65449857711792 | BCE Loss: 1.0426867008209229\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.6686015129089355 | KNN Loss: 5.621578216552734 | BCE Loss: 1.0470232963562012\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.702081680297852 | KNN Loss: 5.642640590667725 | BCE Loss: 1.059441089630127\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.6878767013549805 | KNN Loss: 5.63167142868042 | BCE Loss: 1.0562055110931396\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.740774631500244 | KNN Loss: 5.679457664489746 | BCE Loss: 1.0613168478012085\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.67917537689209 | KNN Loss: 5.634116172790527 | BCE Loss: 1.045059323310852\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.764060020446777 | KNN Loss: 5.68588399887085 | BCE Loss: 1.0781757831573486\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.717808723449707 | KNN Loss: 5.67799186706543 | BCE Loss: 1.0398168563842773\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.671133518218994 | KNN Loss: 5.62038516998291 | BCE Loss: 1.050748348236084\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.719778537750244 | KNN Loss: 5.674257278442383 | BCE Loss: 1.0455211400985718\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.679387092590332 | KNN Loss: 5.633068561553955 | BCE Loss: 1.0463186502456665\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.693717956542969 | KNN Loss: 5.631330966949463 | BCE Loss: 1.0623867511749268\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.677569389343262 | KNN Loss: 5.648061752319336 | BCE Loss: 1.0295076370239258\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.685279369354248 | KNN Loss: 5.639383316040039 | BCE Loss: 1.045896053314209\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.676037788391113 | KNN Loss: 5.61646032333374 | BCE Loss: 1.0595777034759521\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.696872711181641 | KNN Loss: 5.646355628967285 | BCE Loss: 1.0505170822143555\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.686057090759277 | KNN Loss: 5.636465549468994 | BCE Loss: 1.0495915412902832\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.837697982788086 | KNN Loss: 5.766584873199463 | BCE Loss: 1.0711129903793335\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.706643104553223 | KNN Loss: 5.631006240844727 | BCE Loss: 1.0756367444992065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.650391101837158 | KNN Loss: 5.624721050262451 | BCE Loss: 1.0256701707839966\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.719789028167725 | KNN Loss: 5.673291206359863 | BCE Loss: 1.0464977025985718\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.676051139831543 | KNN Loss: 5.617538928985596 | BCE Loss: 1.0585122108459473\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.666265487670898 | KNN Loss: 5.619668483734131 | BCE Loss: 1.046596884727478\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.716798782348633 | KNN Loss: 5.635405540466309 | BCE Loss: 1.0813934803009033\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.654577732086182 | KNN Loss: 5.638788223266602 | BCE Loss: 1.01578950881958\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.737710952758789 | KNN Loss: 5.6912736892700195 | BCE Loss: 1.046437382698059\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.695381164550781 | KNN Loss: 5.65086555480957 | BCE Loss: 1.044515609741211\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.676520347595215 | KNN Loss: 5.617735385894775 | BCE Loss: 1.0587849617004395\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.702472686767578 | KNN Loss: 5.656309127807617 | BCE Loss: 1.046163558959961\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.708218574523926 | KNN Loss: 5.632637977600098 | BCE Loss: 1.0755807161331177\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.691624641418457 | KNN Loss: 5.65019416809082 | BCE Loss: 1.0414302349090576\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.717576503753662 | KNN Loss: 5.6280364990234375 | BCE Loss: 1.0895400047302246\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.690218925476074 | KNN Loss: 5.61377477645874 | BCE Loss: 1.076444149017334\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.641798496246338 | KNN Loss: 5.6105122566223145 | BCE Loss: 1.0312862396240234\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.671800136566162 | KNN Loss: 5.632259368896484 | BCE Loss: 1.0395407676696777\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.645694732666016 | KNN Loss: 5.614650726318359 | BCE Loss: 1.0310437679290771\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.735911846160889 | KNN Loss: 5.673097133636475 | BCE Loss: 1.062814712524414\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.695511817932129 | KNN Loss: 5.636807918548584 | BCE Loss: 1.058704137802124\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.727775573730469 | KNN Loss: 5.668022632598877 | BCE Loss: 1.0597528219223022\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.719851016998291 | KNN Loss: 5.672811031341553 | BCE Loss: 1.0470399856567383\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.772800445556641 | KNN Loss: 5.7215375900268555 | BCE Loss: 1.0512628555297852\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.669371604919434 | KNN Loss: 5.6266045570373535 | BCE Loss: 1.042766809463501\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.6886444091796875 | KNN Loss: 5.6313276290893555 | BCE Loss: 1.057316780090332\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.718757629394531 | KNN Loss: 5.668198108673096 | BCE Loss: 1.0505595207214355\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.650570869445801 | KNN Loss: 5.619604587554932 | BCE Loss: 1.0309665203094482\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.660455226898193 | KNN Loss: 5.610673427581787 | BCE Loss: 1.0497817993164062\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.661282539367676 | KNN Loss: 5.6045074462890625 | BCE Loss: 1.0567750930786133\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.685814380645752 | KNN Loss: 5.652205467224121 | BCE Loss: 1.0336089134216309\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.680326461791992 | KNN Loss: 5.617886543273926 | BCE Loss: 1.062440037727356\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.720719337463379 | KNN Loss: 5.681577205657959 | BCE Loss: 1.0391420125961304\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.695936679840088 | KNN Loss: 5.641434669494629 | BCE Loss: 1.0545021295547485\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.802597999572754 | KNN Loss: 5.7608771324157715 | BCE Loss: 1.0417206287384033\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.664716720581055 | KNN Loss: 5.615206718444824 | BCE Loss: 1.0495097637176514\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.676091194152832 | KNN Loss: 5.622325420379639 | BCE Loss: 1.0537655353546143\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.718968868255615 | KNN Loss: 5.649855136871338 | BCE Loss: 1.069113850593567\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.694424629211426 | KNN Loss: 5.656097412109375 | BCE Loss: 1.0383274555206299\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.688525199890137 | KNN Loss: 5.618571758270264 | BCE Loss: 1.069953441619873\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.6815361976623535 | KNN Loss: 5.646562576293945 | BCE Loss: 1.0349736213684082\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.749609470367432 | KNN Loss: 5.654479026794434 | BCE Loss: 1.0951305627822876\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.662966251373291 | KNN Loss: 5.63755464553833 | BCE Loss: 1.0254117250442505\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.712864875793457 | KNN Loss: 5.655727386474609 | BCE Loss: 1.0571372509002686\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.767078399658203 | KNN Loss: 5.7258782386779785 | BCE Loss: 1.0412001609802246\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.712874889373779 | KNN Loss: 5.672580242156982 | BCE Loss: 1.0402945280075073\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.657548904418945 | KNN Loss: 5.612630844116211 | BCE Loss: 1.0449178218841553\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.683426856994629 | KNN Loss: 5.620982646942139 | BCE Loss: 1.0624440908432007\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.699868202209473 | KNN Loss: 5.6579670906066895 | BCE Loss: 1.041900873184204\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.674754619598389 | KNN Loss: 5.618468761444092 | BCE Loss: 1.0562857389450073\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.693001747131348 | KNN Loss: 5.6489081382751465 | BCE Loss: 1.0440936088562012\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.6598920822143555 | KNN Loss: 5.616909027099609 | BCE Loss: 1.0429832935333252\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.730301856994629 | KNN Loss: 5.702071666717529 | BCE Loss: 1.0282304286956787\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.659482955932617 | KNN Loss: 5.644955635070801 | BCE Loss: 1.0145273208618164\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.695330619812012 | KNN Loss: 5.655960559844971 | BCE Loss: 1.039370059967041\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.736905574798584 | KNN Loss: 5.68278694152832 | BCE Loss: 1.0541185140609741\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.734236717224121 | KNN Loss: 5.654553413391113 | BCE Loss: 1.0796831846237183\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.6722564697265625 | KNN Loss: 5.630895614624023 | BCE Loss: 1.04136061668396\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.680674076080322 | KNN Loss: 5.627634048461914 | BCE Loss: 1.0530399084091187\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.706789970397949 | KNN Loss: 5.639275074005127 | BCE Loss: 1.0675150156021118\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.69377326965332 | KNN Loss: 5.653349876403809 | BCE Loss: 1.0404233932495117\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.6424665451049805 | KNN Loss: 5.6136250495910645 | BCE Loss: 1.0288416147232056\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.707521438598633 | KNN Loss: 5.642322540283203 | BCE Loss: 1.0651986598968506\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.690030097961426 | KNN Loss: 5.641267776489258 | BCE Loss: 1.048762559890747\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.689274787902832 | KNN Loss: 5.624711513519287 | BCE Loss: 1.0645633935928345\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.702579498291016 | KNN Loss: 5.633663654327393 | BCE Loss: 1.0689160823822021\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.71608304977417 | KNN Loss: 5.683464050292969 | BCE Loss: 1.0326189994812012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.675329208374023 | KNN Loss: 5.626804828643799 | BCE Loss: 1.0485246181488037\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.788908958435059 | KNN Loss: 5.719067096710205 | BCE Loss: 1.0698418617248535\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.6926984786987305 | KNN Loss: 5.659687042236328 | BCE Loss: 1.0330116748809814\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.709036350250244 | KNN Loss: 5.644835472106934 | BCE Loss: 1.0642008781433105\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.69319486618042 | KNN Loss: 5.628274917602539 | BCE Loss: 1.0649198293685913\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.7110772132873535 | KNN Loss: 5.625705242156982 | BCE Loss: 1.085371971130371\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.660208225250244 | KNN Loss: 5.618839740753174 | BCE Loss: 1.0413684844970703\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.687494277954102 | KNN Loss: 5.6368865966796875 | BCE Loss: 1.0506078004837036\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.662155628204346 | KNN Loss: 5.642423629760742 | BCE Loss: 1.019732117652893\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.684610366821289 | KNN Loss: 5.627180576324463 | BCE Loss: 1.0574300289154053\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.646480083465576 | KNN Loss: 5.609307289123535 | BCE Loss: 1.0371729135513306\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.646841049194336 | KNN Loss: 5.611989974975586 | BCE Loss: 1.034850835800171\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.676809310913086 | KNN Loss: 5.605846881866455 | BCE Loss: 1.0709624290466309\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.76890754699707 | KNN Loss: 5.682896137237549 | BCE Loss: 1.086011290550232\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.7233567237854 | KNN Loss: 5.651199817657471 | BCE Loss: 1.0721569061279297\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.652393341064453 | KNN Loss: 5.612939357757568 | BCE Loss: 1.0394537448883057\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.6409454345703125 | KNN Loss: 5.604979515075684 | BCE Loss: 1.035965919494629\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.6368021965026855 | KNN Loss: 5.608835220336914 | BCE Loss: 1.0279669761657715\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.65044641494751 | KNN Loss: 5.596920967102051 | BCE Loss: 1.053525447845459\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.656010150909424 | KNN Loss: 5.601279258728027 | BCE Loss: 1.054730772972107\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.698546886444092 | KNN Loss: 5.652101993560791 | BCE Loss: 1.0464448928833008\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.765194892883301 | KNN Loss: 5.701918125152588 | BCE Loss: 1.063277006149292\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.659672260284424 | KNN Loss: 5.604955673217773 | BCE Loss: 1.05471670627594\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.680487155914307 | KNN Loss: 5.633388996124268 | BCE Loss: 1.047098159790039\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.765053749084473 | KNN Loss: 5.696249485015869 | BCE Loss: 1.0688040256500244\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.648448467254639 | KNN Loss: 5.632238388061523 | BCE Loss: 1.0162100791931152\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.675898551940918 | KNN Loss: 5.627870082855225 | BCE Loss: 1.0480287075042725\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.686957836151123 | KNN Loss: 5.638351917266846 | BCE Loss: 1.0486059188842773\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.635472774505615 | KNN Loss: 5.605323314666748 | BCE Loss: 1.0301494598388672\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.671867847442627 | KNN Loss: 5.626075744628906 | BCE Loss: 1.0457921028137207\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.689830780029297 | KNN Loss: 5.6533331871032715 | BCE Loss: 1.0364978313446045\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.678562164306641 | KNN Loss: 5.634331703186035 | BCE Loss: 1.0442304611206055\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.699984550476074 | KNN Loss: 5.638904094696045 | BCE Loss: 1.0610802173614502\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.708443641662598 | KNN Loss: 5.634212017059326 | BCE Loss: 1.074231505393982\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.742308139801025 | KNN Loss: 5.6889967918396 | BCE Loss: 1.0533113479614258\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.672839164733887 | KNN Loss: 5.6128363609313965 | BCE Loss: 1.0600028038024902\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.777913570404053 | KNN Loss: 5.736365795135498 | BCE Loss: 1.0415477752685547\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.699367046356201 | KNN Loss: 5.633828163146973 | BCE Loss: 1.0655388832092285\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.783290386199951 | KNN Loss: 5.721782684326172 | BCE Loss: 1.0615077018737793\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.674310207366943 | KNN Loss: 5.630953788757324 | BCE Loss: 1.0433564186096191\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.7282185554504395 | KNN Loss: 5.651106357574463 | BCE Loss: 1.0771121978759766\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.707254886627197 | KNN Loss: 5.653134822845459 | BCE Loss: 1.0541199445724487\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.697650909423828 | KNN Loss: 5.639989852905273 | BCE Loss: 1.0576609373092651\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.689845561981201 | KNN Loss: 5.611689567565918 | BCE Loss: 1.0781559944152832\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.654489040374756 | KNN Loss: 5.624075889587402 | BCE Loss: 1.0304131507873535\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.682221412658691 | KNN Loss: 5.616075038909912 | BCE Loss: 1.0661463737487793\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.712587833404541 | KNN Loss: 5.652066230773926 | BCE Loss: 1.0605216026306152\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.6362199783325195 | KNN Loss: 5.617189407348633 | BCE Loss: 1.0190306901931763\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.718191146850586 | KNN Loss: 5.6370086669921875 | BCE Loss: 1.0811823606491089\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.691141128540039 | KNN Loss: 5.64892578125 | BCE Loss: 1.042215347290039\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.67366361618042 | KNN Loss: 5.604944229125977 | BCE Loss: 1.0687192678451538\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.656616687774658 | KNN Loss: 5.613029956817627 | BCE Loss: 1.0435867309570312\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.646078586578369 | KNN Loss: 5.60320520401001 | BCE Loss: 1.042873501777649\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.6751203536987305 | KNN Loss: 5.611062526702881 | BCE Loss: 1.0640575885772705\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.664758682250977 | KNN Loss: 5.609686851501465 | BCE Loss: 1.0550720691680908\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.670952796936035 | KNN Loss: 5.616703033447266 | BCE Loss: 1.0542495250701904\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.662174224853516 | KNN Loss: 5.601968765258789 | BCE Loss: 1.0602056980133057\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.745510101318359 | KNN Loss: 5.675673484802246 | BCE Loss: 1.0698368549346924\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.638915061950684 | KNN Loss: 5.59881067276001 | BCE Loss: 1.0401041507720947\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.725950241088867 | KNN Loss: 5.662744522094727 | BCE Loss: 1.0632059574127197\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.683204174041748 | KNN Loss: 5.619012832641602 | BCE Loss: 1.0641913414001465\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.788771152496338 | KNN Loss: 5.715563774108887 | BCE Loss: 1.0732073783874512\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.782780170440674 | KNN Loss: 5.728994846343994 | BCE Loss: 1.0537854433059692\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.662919044494629 | KNN Loss: 5.628469944000244 | BCE Loss: 1.0344493389129639\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.71120548248291 | KNN Loss: 5.658401012420654 | BCE Loss: 1.0528044700622559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.667268753051758 | KNN Loss: 5.6239237785339355 | BCE Loss: 1.0433449745178223\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.678684711456299 | KNN Loss: 5.625412464141846 | BCE Loss: 1.0532722473144531\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.744381904602051 | KNN Loss: 5.686888694763184 | BCE Loss: 1.0574932098388672\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.666080474853516 | KNN Loss: 5.619416236877441 | BCE Loss: 1.0466644763946533\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.723796367645264 | KNN Loss: 5.659749984741211 | BCE Loss: 1.0640463829040527\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.65497350692749 | KNN Loss: 5.604964733123779 | BCE Loss: 1.0500088930130005\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.676738739013672 | KNN Loss: 5.629705905914307 | BCE Loss: 1.0470328330993652\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.687737941741943 | KNN Loss: 5.671445846557617 | BCE Loss: 1.0162922143936157\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.687358856201172 | KNN Loss: 5.629242420196533 | BCE Loss: 1.0581166744232178\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.7469587326049805 | KNN Loss: 5.67062520980835 | BCE Loss: 1.0763332843780518\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.719329833984375 | KNN Loss: 5.675755023956299 | BCE Loss: 1.0435750484466553\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.738128662109375 | KNN Loss: 5.6765007972717285 | BCE Loss: 1.0616278648376465\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.662172317504883 | KNN Loss: 5.616628646850586 | BCE Loss: 1.0455436706542969\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.660648345947266 | KNN Loss: 5.61079740524292 | BCE Loss: 1.0498509407043457\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.663917541503906 | KNN Loss: 5.621398448944092 | BCE Loss: 1.0425188541412354\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.685518264770508 | KNN Loss: 5.612642765045166 | BCE Loss: 1.0728756189346313\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.655611038208008 | KNN Loss: 5.622207164764404 | BCE Loss: 1.0334038734436035\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.713643550872803 | KNN Loss: 5.673495769500732 | BCE Loss: 1.0401476621627808\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.683103561401367 | KNN Loss: 5.625795841217041 | BCE Loss: 1.0573079586029053\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.662449836730957 | KNN Loss: 5.619141578674316 | BCE Loss: 1.0433080196380615\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.70854377746582 | KNN Loss: 5.632555961608887 | BCE Loss: 1.0759880542755127\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.655474662780762 | KNN Loss: 5.60889196395874 | BCE Loss: 1.0465824604034424\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.765521049499512 | KNN Loss: 5.700950622558594 | BCE Loss: 1.064570665359497\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.65693473815918 | KNN Loss: 5.613597869873047 | BCE Loss: 1.0433367490768433\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.695230484008789 | KNN Loss: 5.635778903961182 | BCE Loss: 1.0594518184661865\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.679657459259033 | KNN Loss: 5.642642974853516 | BCE Loss: 1.0370144844055176\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.713306427001953 | KNN Loss: 5.65139102935791 | BCE Loss: 1.061915397644043\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.821468353271484 | KNN Loss: 5.748551368713379 | BCE Loss: 1.0729172229766846\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.662631034851074 | KNN Loss: 5.606184005737305 | BCE Loss: 1.056447148323059\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.636381149291992 | KNN Loss: 5.589286804199219 | BCE Loss: 1.0470943450927734\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.753087043762207 | KNN Loss: 5.705798149108887 | BCE Loss: 1.0472888946533203\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.668542861938477 | KNN Loss: 5.628225803375244 | BCE Loss: 1.0403170585632324\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.631283760070801 | KNN Loss: 5.6127166748046875 | BCE Loss: 1.0185672044754028\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.662458419799805 | KNN Loss: 5.60417366027832 | BCE Loss: 1.0582846403121948\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.658633232116699 | KNN Loss: 5.611746311187744 | BCE Loss: 1.0468868017196655\n",
      "Epoch    60: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.648073196411133 | KNN Loss: 5.5960612297058105 | BCE Loss: 1.0520117282867432\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.6744384765625 | KNN Loss: 5.6288743019104 | BCE Loss: 1.0455639362335205\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.7208251953125 | KNN Loss: 5.657136917114258 | BCE Loss: 1.0636881589889526\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.6845526695251465 | KNN Loss: 5.641921520233154 | BCE Loss: 1.0426310300827026\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.669367790222168 | KNN Loss: 5.645511627197266 | BCE Loss: 1.0238564014434814\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.715898513793945 | KNN Loss: 5.67008638381958 | BCE Loss: 1.0458121299743652\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.643199920654297 | KNN Loss: 5.600985527038574 | BCE Loss: 1.0422145128250122\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.66948127746582 | KNN Loss: 5.624654293060303 | BCE Loss: 1.044826865196228\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.650774002075195 | KNN Loss: 5.620842933654785 | BCE Loss: 1.029930830001831\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.740964412689209 | KNN Loss: 5.68767786026001 | BCE Loss: 1.0532864332199097\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.666904449462891 | KNN Loss: 5.610842704772949 | BCE Loss: 1.0560619831085205\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.695368766784668 | KNN Loss: 5.6429667472839355 | BCE Loss: 1.0524022579193115\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.674919128417969 | KNN Loss: 5.626245975494385 | BCE Loss: 1.048673152923584\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.682890892028809 | KNN Loss: 5.6319355964660645 | BCE Loss: 1.0509554147720337\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.682769298553467 | KNN Loss: 5.621674537658691 | BCE Loss: 1.0610946416854858\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.758825778961182 | KNN Loss: 5.69844388961792 | BCE Loss: 1.0603820085525513\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.688823699951172 | KNN Loss: 5.635857105255127 | BCE Loss: 1.0529663562774658\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.65588903427124 | KNN Loss: 5.6098151206970215 | BCE Loss: 1.0460739135742188\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.712067604064941 | KNN Loss: 5.660459518432617 | BCE Loss: 1.0516079664230347\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.665596961975098 | KNN Loss: 5.613786220550537 | BCE Loss: 1.0518105030059814\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.710142135620117 | KNN Loss: 5.63904333114624 | BCE Loss: 1.0710985660552979\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.643818378448486 | KNN Loss: 5.613762855529785 | BCE Loss: 1.0300556421279907\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.676424503326416 | KNN Loss: 5.608552932739258 | BCE Loss: 1.0678715705871582\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.657812118530273 | KNN Loss: 5.617071151733398 | BCE Loss: 1.040741205215454\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.664711952209473 | KNN Loss: 5.621210098266602 | BCE Loss: 1.0435020923614502\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.693706512451172 | KNN Loss: 5.616659641265869 | BCE Loss: 1.0770468711853027\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.6734724044799805 | KNN Loss: 5.629537105560303 | BCE Loss: 1.0439352989196777\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.635159969329834 | KNN Loss: 5.609280586242676 | BCE Loss: 1.0258795022964478\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.76500129699707 | KNN Loss: 5.719198226928711 | BCE Loss: 1.0458033084869385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.685281753540039 | KNN Loss: 5.612063407897949 | BCE Loss: 1.0732183456420898\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.726638317108154 | KNN Loss: 5.666286945343018 | BCE Loss: 1.0603513717651367\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.692787170410156 | KNN Loss: 5.638363361358643 | BCE Loss: 1.0544236898422241\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.717414855957031 | KNN Loss: 5.65566873550415 | BCE Loss: 1.06174635887146\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.657205104827881 | KNN Loss: 5.624779224395752 | BCE Loss: 1.032425880432129\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.687451362609863 | KNN Loss: 5.6171555519104 | BCE Loss: 1.0702956914901733\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.72709846496582 | KNN Loss: 5.661682605743408 | BCE Loss: 1.065415620803833\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.644546985626221 | KNN Loss: 5.6008830070495605 | BCE Loss: 1.0436639785766602\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.717241287231445 | KNN Loss: 5.644264221191406 | BCE Loss: 1.07297682762146\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.709955215454102 | KNN Loss: 5.653675079345703 | BCE Loss: 1.0562803745269775\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.64180850982666 | KNN Loss: 5.610872745513916 | BCE Loss: 1.0309360027313232\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.706292152404785 | KNN Loss: 5.667106628417969 | BCE Loss: 1.0391857624053955\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.660385608673096 | KNN Loss: 5.6102752685546875 | BCE Loss: 1.0501102209091187\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.725388526916504 | KNN Loss: 5.651376724243164 | BCE Loss: 1.074012041091919\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.676662445068359 | KNN Loss: 5.61214017868042 | BCE Loss: 1.0645222663879395\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.675962448120117 | KNN Loss: 5.6442999839782715 | BCE Loss: 1.0316622257232666\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.660336494445801 | KNN Loss: 5.618607044219971 | BCE Loss: 1.0417296886444092\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.681829929351807 | KNN Loss: 5.64711332321167 | BCE Loss: 1.0347164869308472\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.69843053817749 | KNN Loss: 5.648698329925537 | BCE Loss: 1.0497322082519531\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.645047664642334 | KNN Loss: 5.610834121704102 | BCE Loss: 1.034213662147522\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.6869401931762695 | KNN Loss: 5.619863986968994 | BCE Loss: 1.0670764446258545\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.654484748840332 | KNN Loss: 5.610289096832275 | BCE Loss: 1.044195532798767\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.651728630065918 | KNN Loss: 5.605816841125488 | BCE Loss: 1.0459116697311401\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.668088912963867 | KNN Loss: 5.624416351318359 | BCE Loss: 1.043672800064087\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.701189041137695 | KNN Loss: 5.646915435791016 | BCE Loss: 1.0542737245559692\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.7472147941589355 | KNN Loss: 5.691449165344238 | BCE Loss: 1.0557655096054077\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.695899963378906 | KNN Loss: 5.631768226623535 | BCE Loss: 1.064131736755371\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.6550188064575195 | KNN Loss: 5.6054511070251465 | BCE Loss: 1.049567461013794\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.654573440551758 | KNN Loss: 5.619804382324219 | BCE Loss: 1.03476881980896\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.697345733642578 | KNN Loss: 5.64241361618042 | BCE Loss: 1.0549321174621582\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.720867156982422 | KNN Loss: 5.620337009429932 | BCE Loss: 1.1005299091339111\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.661125183105469 | KNN Loss: 5.618208408355713 | BCE Loss: 1.0429167747497559\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.701817989349365 | KNN Loss: 5.662806987762451 | BCE Loss: 1.0390111207962036\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.690871715545654 | KNN Loss: 5.633777618408203 | BCE Loss: 1.0570942163467407\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.719104766845703 | KNN Loss: 5.669427394866943 | BCE Loss: 1.0496776103973389\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.681327819824219 | KNN Loss: 5.604507923126221 | BCE Loss: 1.0768197774887085\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.711304187774658 | KNN Loss: 5.662956714630127 | BCE Loss: 1.0483474731445312\n",
      "Epoch    71: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.647160530090332 | KNN Loss: 5.613177299499512 | BCE Loss: 1.0339829921722412\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.704890727996826 | KNN Loss: 5.602926731109619 | BCE Loss: 1.1019641160964966\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.679905414581299 | KNN Loss: 5.650097370147705 | BCE Loss: 1.0298080444335938\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.656701564788818 | KNN Loss: 5.622150897979736 | BCE Loss: 1.0345507860183716\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.699119567871094 | KNN Loss: 5.646177291870117 | BCE Loss: 1.0529420375823975\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.6697564125061035 | KNN Loss: 5.61994743347168 | BCE Loss: 1.0498089790344238\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.671713829040527 | KNN Loss: 5.6406145095825195 | BCE Loss: 1.0310993194580078\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.676086902618408 | KNN Loss: 5.610633373260498 | BCE Loss: 1.0654535293579102\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.660077095031738 | KNN Loss: 5.617031097412109 | BCE Loss: 1.043046236038208\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.6919026374816895 | KNN Loss: 5.611793041229248 | BCE Loss: 1.0801095962524414\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.75667667388916 | KNN Loss: 5.689517021179199 | BCE Loss: 1.067159652709961\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.702273845672607 | KNN Loss: 5.631904602050781 | BCE Loss: 1.0703692436218262\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.678714752197266 | KNN Loss: 5.624200820922852 | BCE Loss: 1.0545141696929932\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.710754871368408 | KNN Loss: 5.646064758300781 | BCE Loss: 1.0646902322769165\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.6844801902771 | KNN Loss: 5.653074741363525 | BCE Loss: 1.0314055681228638\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.647027969360352 | KNN Loss: 5.626608371734619 | BCE Loss: 1.0204194784164429\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.65579080581665 | KNN Loss: 5.63317346572876 | BCE Loss: 1.0226173400878906\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.632580280303955 | KNN Loss: 5.60407018661499 | BCE Loss: 1.0285102128982544\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.674336910247803 | KNN Loss: 5.631327152252197 | BCE Loss: 1.043009877204895\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.733611106872559 | KNN Loss: 5.67094612121582 | BCE Loss: 1.0626651048660278\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.733438491821289 | KNN Loss: 5.677116870880127 | BCE Loss: 1.056321382522583\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.638247013092041 | KNN Loss: 5.60057258605957 | BCE Loss: 1.0376744270324707\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.6671319007873535 | KNN Loss: 5.611800670623779 | BCE Loss: 1.0553312301635742\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.672727584838867 | KNN Loss: 5.625786304473877 | BCE Loss: 1.0469410419464111\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.64814567565918 | KNN Loss: 5.618621349334717 | BCE Loss: 1.029524564743042\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.738076686859131 | KNN Loss: 5.65941858291626 | BCE Loss: 1.078658103942871\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.6888628005981445 | KNN Loss: 5.637556076049805 | BCE Loss: 1.0513064861297607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.773924827575684 | KNN Loss: 5.709336757659912 | BCE Loss: 1.0645878314971924\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.703548908233643 | KNN Loss: 5.641159534454346 | BCE Loss: 1.0623892545700073\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.657436847686768 | KNN Loss: 5.605563640594482 | BCE Loss: 1.0518732070922852\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.646927833557129 | KNN Loss: 5.598230838775635 | BCE Loss: 1.0486972332000732\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.710811614990234 | KNN Loss: 5.649511337280273 | BCE Loss: 1.06130051612854\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.77372932434082 | KNN Loss: 5.7055559158325195 | BCE Loss: 1.0681736469268799\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.673055171966553 | KNN Loss: 5.621921062469482 | BCE Loss: 1.0511339902877808\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.67494010925293 | KNN Loss: 5.614178657531738 | BCE Loss: 1.0607616901397705\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.678499221801758 | KNN Loss: 5.603954315185547 | BCE Loss: 1.07454514503479\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.69638729095459 | KNN Loss: 5.646817207336426 | BCE Loss: 1.0495703220367432\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.784493446350098 | KNN Loss: 5.712646007537842 | BCE Loss: 1.071847677230835\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.675363540649414 | KNN Loss: 5.608606815338135 | BCE Loss: 1.0667569637298584\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.634056091308594 | KNN Loss: 5.608160495758057 | BCE Loss: 1.0258954763412476\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.720402240753174 | KNN Loss: 5.674384593963623 | BCE Loss: 1.0460177659988403\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.783573150634766 | KNN Loss: 5.731193542480469 | BCE Loss: 1.0523796081542969\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.6767144203186035 | KNN Loss: 5.628756523132324 | BCE Loss: 1.0479580163955688\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.669177532196045 | KNN Loss: 5.62703275680542 | BCE Loss: 1.042144775390625\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.675008296966553 | KNN Loss: 5.614816665649414 | BCE Loss: 1.0601917505264282\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.710845470428467 | KNN Loss: 5.64443302154541 | BCE Loss: 1.0664124488830566\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.683271408081055 | KNN Loss: 5.631486415863037 | BCE Loss: 1.0517849922180176\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.699613094329834 | KNN Loss: 5.643063068389893 | BCE Loss: 1.0565500259399414\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.772825241088867 | KNN Loss: 5.71244478225708 | BCE Loss: 1.060380458831787\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.7219085693359375 | KNN Loss: 5.675826072692871 | BCE Loss: 1.0460827350616455\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.646004676818848 | KNN Loss: 5.600166320800781 | BCE Loss: 1.0458381175994873\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.654384613037109 | KNN Loss: 5.613514423370361 | BCE Loss: 1.040870189666748\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.710177421569824 | KNN Loss: 5.644059658050537 | BCE Loss: 1.066117525100708\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.673617362976074 | KNN Loss: 5.6365966796875 | BCE Loss: 1.0370206832885742\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.6700286865234375 | KNN Loss: 5.603869438171387 | BCE Loss: 1.0661594867706299\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.659488677978516 | KNN Loss: 5.6085968017578125 | BCE Loss: 1.050891637802124\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.620769500732422 | KNN Loss: 5.602822303771973 | BCE Loss: 1.0179470777511597\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.695368766784668 | KNN Loss: 5.613053321838379 | BCE Loss: 1.08231520652771\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.6787848472595215 | KNN Loss: 5.634423732757568 | BCE Loss: 1.0443611145019531\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.696167945861816 | KNN Loss: 5.636923313140869 | BCE Loss: 1.0592443943023682\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.721676826477051 | KNN Loss: 5.68290901184082 | BCE Loss: 1.0387678146362305\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.68756103515625 | KNN Loss: 5.645573616027832 | BCE Loss: 1.041987657546997\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.672527313232422 | KNN Loss: 5.6296586990356445 | BCE Loss: 1.0428683757781982\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.675083160400391 | KNN Loss: 5.612636089324951 | BCE Loss: 1.0624468326568604\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.681571960449219 | KNN Loss: 5.660075664520264 | BCE Loss: 1.021496057510376\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.686140537261963 | KNN Loss: 5.631348609924316 | BCE Loss: 1.054791808128357\n",
      "Epoch    82: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.683333396911621 | KNN Loss: 5.610958576202393 | BCE Loss: 1.0723750591278076\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.772778034210205 | KNN Loss: 5.722774982452393 | BCE Loss: 1.0500030517578125\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.641731262207031 | KNN Loss: 5.621793270111084 | BCE Loss: 1.0199379920959473\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.699196815490723 | KNN Loss: 5.607180118560791 | BCE Loss: 1.0920169353485107\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.699872016906738 | KNN Loss: 5.664106845855713 | BCE Loss: 1.0357649326324463\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.637786865234375 | KNN Loss: 5.6077470779418945 | BCE Loss: 1.0300397872924805\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.736052513122559 | KNN Loss: 5.675943851470947 | BCE Loss: 1.0601084232330322\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.663167476654053 | KNN Loss: 5.620486736297607 | BCE Loss: 1.0426808595657349\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.684216499328613 | KNN Loss: 5.632293224334717 | BCE Loss: 1.0519230365753174\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.724785804748535 | KNN Loss: 5.660393238067627 | BCE Loss: 1.0643926858901978\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.684548377990723 | KNN Loss: 5.63913631439209 | BCE Loss: 1.045412302017212\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.6599273681640625 | KNN Loss: 5.632950782775879 | BCE Loss: 1.0269765853881836\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.720115661621094 | KNN Loss: 5.66581392288208 | BCE Loss: 1.0543015003204346\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.638401508331299 | KNN Loss: 5.6117472648620605 | BCE Loss: 1.0266543626785278\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.6512885093688965 | KNN Loss: 5.607443809509277 | BCE Loss: 1.0438446998596191\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.642232894897461 | KNN Loss: 5.61652946472168 | BCE Loss: 1.0257035493850708\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.65654182434082 | KNN Loss: 5.612353801727295 | BCE Loss: 1.0441880226135254\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.6737775802612305 | KNN Loss: 5.608313083648682 | BCE Loss: 1.0654643774032593\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.655633449554443 | KNN Loss: 5.605972766876221 | BCE Loss: 1.0496606826782227\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.674678802490234 | KNN Loss: 5.606285095214844 | BCE Loss: 1.068393588066101\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.666826248168945 | KNN Loss: 5.62822151184082 | BCE Loss: 1.038604974746704\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.671610355377197 | KNN Loss: 5.616527557373047 | BCE Loss: 1.05508291721344\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.673819541931152 | KNN Loss: 5.651941776275635 | BCE Loss: 1.0218777656555176\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.684395790100098 | KNN Loss: 5.624303817749023 | BCE Loss: 1.0600918531417847\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.750308036804199 | KNN Loss: 5.660789489746094 | BCE Loss: 1.0895187854766846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.7097487449646 | KNN Loss: 5.640717506408691 | BCE Loss: 1.0690313577651978\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.644429683685303 | KNN Loss: 5.5963006019592285 | BCE Loss: 1.0481290817260742\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.676712989807129 | KNN Loss: 5.64622688293457 | BCE Loss: 1.0304858684539795\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.676248550415039 | KNN Loss: 5.6216278076171875 | BCE Loss: 1.0546207427978516\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.660740852355957 | KNN Loss: 5.619797706604004 | BCE Loss: 1.040942907333374\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.682594299316406 | KNN Loss: 5.64323091506958 | BCE Loss: 1.0393636226654053\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.641056060791016 | KNN Loss: 5.615077495574951 | BCE Loss: 1.0259785652160645\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.66670036315918 | KNN Loss: 5.618285179138184 | BCE Loss: 1.048415184020996\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.687010765075684 | KNN Loss: 5.645791053771973 | BCE Loss: 1.041219711303711\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.705605506896973 | KNN Loss: 5.640068531036377 | BCE Loss: 1.0655372142791748\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.658458709716797 | KNN Loss: 5.617849826812744 | BCE Loss: 1.0406088829040527\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.672585487365723 | KNN Loss: 5.62368106842041 | BCE Loss: 1.048904299736023\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.666774749755859 | KNN Loss: 5.627189636230469 | BCE Loss: 1.0395848751068115\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.700512886047363 | KNN Loss: 5.6621880531311035 | BCE Loss: 1.0383250713348389\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.641452789306641 | KNN Loss: 5.61939001083374 | BCE Loss: 1.0220627784729004\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.703280925750732 | KNN Loss: 5.621273517608643 | BCE Loss: 1.0820074081420898\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.668988227844238 | KNN Loss: 5.614311695098877 | BCE Loss: 1.0546765327453613\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.71622371673584 | KNN Loss: 5.64667272567749 | BCE Loss: 1.0695512294769287\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.697518348693848 | KNN Loss: 5.655900955200195 | BCE Loss: 1.0416176319122314\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.649491786956787 | KNN Loss: 5.603418350219727 | BCE Loss: 1.0460734367370605\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.689449310302734 | KNN Loss: 5.651942253112793 | BCE Loss: 1.037507176399231\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.674117088317871 | KNN Loss: 5.62750768661499 | BCE Loss: 1.04660964012146\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.667757987976074 | KNN Loss: 5.615544319152832 | BCE Loss: 1.0522137880325317\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.673544883728027 | KNN Loss: 5.641412734985352 | BCE Loss: 1.0321319103240967\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.707175254821777 | KNN Loss: 5.639060020446777 | BCE Loss: 1.068115234375\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.673926830291748 | KNN Loss: 5.620879173278809 | BCE Loss: 1.0530476570129395\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.6369476318359375 | KNN Loss: 5.604894638061523 | BCE Loss: 1.032052755355835\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.670269966125488 | KNN Loss: 5.60121488571167 | BCE Loss: 1.0690548419952393\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.68748664855957 | KNN Loss: 5.635112285614014 | BCE Loss: 1.0523746013641357\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.703618049621582 | KNN Loss: 5.639746189117432 | BCE Loss: 1.0638716220855713\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.692220211029053 | KNN Loss: 5.627127647399902 | BCE Loss: 1.0650925636291504\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.676722049713135 | KNN Loss: 5.612775802612305 | BCE Loss: 1.06394624710083\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.680783271789551 | KNN Loss: 5.608365058898926 | BCE Loss: 1.0724180936813354\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.673097610473633 | KNN Loss: 5.6183342933654785 | BCE Loss: 1.0547630786895752\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 6.693026542663574 | KNN Loss: 5.614206314086914 | BCE Loss: 1.0788201093673706\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.71921443939209 | KNN Loss: 5.701462268829346 | BCE Loss: 1.0177524089813232\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.676177024841309 | KNN Loss: 5.615392208099365 | BCE Loss: 1.0607850551605225\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.6603875160217285 | KNN Loss: 5.604869365692139 | BCE Loss: 1.0555181503295898\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.740772247314453 | KNN Loss: 5.687170505523682 | BCE Loss: 1.0536019802093506\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.681856155395508 | KNN Loss: 5.652627468109131 | BCE Loss: 1.029228925704956\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.677554130554199 | KNN Loss: 5.629868507385254 | BCE Loss: 1.0476858615875244\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.7257080078125 | KNN Loss: 5.687971591949463 | BCE Loss: 1.0377365350723267\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.670426368713379 | KNN Loss: 5.601755142211914 | BCE Loss: 1.068671464920044\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.71347713470459 | KNN Loss: 5.638432502746582 | BCE Loss: 1.0750447511672974\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.666799545288086 | KNN Loss: 5.612705230712891 | BCE Loss: 1.0540945529937744\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.682192802429199 | KNN Loss: 5.611304759979248 | BCE Loss: 1.0708882808685303\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.697486400604248 | KNN Loss: 5.661282539367676 | BCE Loss: 1.0362038612365723\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.712484359741211 | KNN Loss: 5.639514923095703 | BCE Loss: 1.0729694366455078\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.731485366821289 | KNN Loss: 5.672970294952393 | BCE Loss: 1.0585148334503174\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.677906036376953 | KNN Loss: 5.625217437744141 | BCE Loss: 1.0526885986328125\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.646941661834717 | KNN Loss: 5.610859394073486 | BCE Loss: 1.0360822677612305\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.7091593742370605 | KNN Loss: 5.658018112182617 | BCE Loss: 1.0511411428451538\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.696752071380615 | KNN Loss: 5.644469738006592 | BCE Loss: 1.0522822141647339\n",
      "Epoch    95: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.654642581939697 | KNN Loss: 5.608401298522949 | BCE Loss: 1.046241283416748\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.685918807983398 | KNN Loss: 5.62664270401001 | BCE Loss: 1.0592759847640991\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.668844223022461 | KNN Loss: 5.624849319458008 | BCE Loss: 1.0439949035644531\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.686196804046631 | KNN Loss: 5.63223934173584 | BCE Loss: 1.053957462310791\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.712222099304199 | KNN Loss: 5.66082239151001 | BCE Loss: 1.0513995885849\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.680337905883789 | KNN Loss: 5.6245646476745605 | BCE Loss: 1.0557730197906494\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.737588882446289 | KNN Loss: 5.702050685882568 | BCE Loss: 1.0355379581451416\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.68975305557251 | KNN Loss: 5.633919715881348 | BCE Loss: 1.055833339691162\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.693795204162598 | KNN Loss: 5.6403961181640625 | BCE Loss: 1.0533990859985352\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.710817813873291 | KNN Loss: 5.641927719116211 | BCE Loss: 1.06889009475708\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.668116569519043 | KNN Loss: 5.606433868408203 | BCE Loss: 1.061682939529419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.675928115844727 | KNN Loss: 5.6319499015808105 | BCE Loss: 1.043978214263916\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.682690143585205 | KNN Loss: 5.6225786209106445 | BCE Loss: 1.060111403465271\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.8442206382751465 | KNN Loss: 5.748156547546387 | BCE Loss: 1.0960640907287598\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.668389320373535 | KNN Loss: 5.611451148986816 | BCE Loss: 1.0569381713867188\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.693245887756348 | KNN Loss: 5.627976417541504 | BCE Loss: 1.0652692317962646\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.662095069885254 | KNN Loss: 5.625235080718994 | BCE Loss: 1.0368598699569702\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.646385192871094 | KNN Loss: 5.615398406982422 | BCE Loss: 1.030987024307251\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.7444658279418945 | KNN Loss: 5.680598735809326 | BCE Loss: 1.0638673305511475\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.675553321838379 | KNN Loss: 5.605571746826172 | BCE Loss: 1.069981336593628\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.634142875671387 | KNN Loss: 5.601783752441406 | BCE Loss: 1.0323593616485596\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.666473388671875 | KNN Loss: 5.639351844787598 | BCE Loss: 1.027121663093567\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.69615364074707 | KNN Loss: 5.635906219482422 | BCE Loss: 1.0602471828460693\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.66549015045166 | KNN Loss: 5.625770568847656 | BCE Loss: 1.0397193431854248\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.657867431640625 | KNN Loss: 5.600849151611328 | BCE Loss: 1.0570183992385864\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.66475772857666 | KNN Loss: 5.643893241882324 | BCE Loss: 1.0208642482757568\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.68129301071167 | KNN Loss: 5.605657577514648 | BCE Loss: 1.0756354331970215\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.663808345794678 | KNN Loss: 5.615078926086426 | BCE Loss: 1.048729419708252\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.745532035827637 | KNN Loss: 5.686777591705322 | BCE Loss: 1.058754563331604\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.679545879364014 | KNN Loss: 5.6150617599487305 | BCE Loss: 1.0644841194152832\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.746781349182129 | KNN Loss: 5.690683364868164 | BCE Loss: 1.056098222732544\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.7274489402771 | KNN Loss: 5.670168876647949 | BCE Loss: 1.0572799444198608\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.688477039337158 | KNN Loss: 5.605725288391113 | BCE Loss: 1.082751750946045\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.676483154296875 | KNN Loss: 5.65557336807251 | BCE Loss: 1.0209100246429443\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.652126312255859 | KNN Loss: 5.616008281707764 | BCE Loss: 1.0361177921295166\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.739867210388184 | KNN Loss: 5.669249534606934 | BCE Loss: 1.070617914199829\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.651686191558838 | KNN Loss: 5.620026588439941 | BCE Loss: 1.0316596031188965\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.697902679443359 | KNN Loss: 5.611495018005371 | BCE Loss: 1.0864074230194092\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.75648307800293 | KNN Loss: 5.677611351013184 | BCE Loss: 1.0788719654083252\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.704360008239746 | KNN Loss: 5.634483337402344 | BCE Loss: 1.0698769092559814\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.707840919494629 | KNN Loss: 5.659580707550049 | BCE Loss: 1.04826021194458\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.656551837921143 | KNN Loss: 5.612971782684326 | BCE Loss: 1.0435800552368164\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.640380859375 | KNN Loss: 5.611823558807373 | BCE Loss: 1.0285571813583374\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.670700550079346 | KNN Loss: 5.6255269050598145 | BCE Loss: 1.0451736450195312\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.6321916580200195 | KNN Loss: 5.601426601409912 | BCE Loss: 1.0307648181915283\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.668673515319824 | KNN Loss: 5.618323802947998 | BCE Loss: 1.0503499507904053\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.68343448638916 | KNN Loss: 5.6075215339660645 | BCE Loss: 1.0759131908416748\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.680370330810547 | KNN Loss: 5.611593723297119 | BCE Loss: 1.0687766075134277\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.654120445251465 | KNN Loss: 5.606057167053223 | BCE Loss: 1.048063039779663\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.66239070892334 | KNN Loss: 5.614745140075684 | BCE Loss: 1.0476453304290771\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.677020072937012 | KNN Loss: 5.628564834594727 | BCE Loss: 1.048454999923706\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.6484551429748535 | KNN Loss: 5.632389545440674 | BCE Loss: 1.0160655975341797\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.677216053009033 | KNN Loss: 5.6206841468811035 | BCE Loss: 1.0565320253372192\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.742854118347168 | KNN Loss: 5.697917938232422 | BCE Loss: 1.044935941696167\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.650546550750732 | KNN Loss: 5.627511024475098 | BCE Loss: 1.0230355262756348\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.64656925201416 | KNN Loss: 5.619368076324463 | BCE Loss: 1.0272012948989868\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.648966312408447 | KNN Loss: 5.598346710205078 | BCE Loss: 1.0506197214126587\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.650264739990234 | KNN Loss: 5.61341667175293 | BCE Loss: 1.0368478298187256\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.672101974487305 | KNN Loss: 5.639094829559326 | BCE Loss: 1.0330069065093994\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.726677894592285 | KNN Loss: 5.683806419372559 | BCE Loss: 1.0428715944290161\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.682663440704346 | KNN Loss: 5.6180620193481445 | BCE Loss: 1.0646014213562012\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.75832986831665 | KNN Loss: 5.712953090667725 | BCE Loss: 1.0453766584396362\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.74760627746582 | KNN Loss: 5.676470756530762 | BCE Loss: 1.0711357593536377\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.68234395980835 | KNN Loss: 5.622249603271484 | BCE Loss: 1.0600943565368652\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.659884452819824 | KNN Loss: 5.6218390464782715 | BCE Loss: 1.0380451679229736\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.6517462730407715 | KNN Loss: 5.606983661651611 | BCE Loss: 1.0447626113891602\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.68776273727417 | KNN Loss: 5.647564888000488 | BCE Loss: 1.0401978492736816\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.689364433288574 | KNN Loss: 5.639447212219238 | BCE Loss: 1.049917221069336\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.704445838928223 | KNN Loss: 5.6598334312438965 | BCE Loss: 1.0446124076843262\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.677488327026367 | KNN Loss: 5.611265182495117 | BCE Loss: 1.066223382949829\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.668673038482666 | KNN Loss: 5.6041340827941895 | BCE Loss: 1.0645390748977661\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.612496376037598 | KNN Loss: 5.597824573516846 | BCE Loss: 1.0146715641021729\n",
      "Epoch   107: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.706746578216553 | KNN Loss: 5.620820045471191 | BCE Loss: 1.0859265327453613\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.641037940979004 | KNN Loss: 5.603215217590332 | BCE Loss: 1.037822961807251\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.668468952178955 | KNN Loss: 5.626266002655029 | BCE Loss: 1.0422029495239258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.733925819396973 | KNN Loss: 5.682546138763428 | BCE Loss: 1.0513794422149658\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.773036479949951 | KNN Loss: 5.717731475830078 | BCE Loss: 1.0553048849105835\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.671138763427734 | KNN Loss: 5.612814903259277 | BCE Loss: 1.0583237409591675\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.693307876586914 | KNN Loss: 5.627022743225098 | BCE Loss: 1.0662853717803955\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.632239818572998 | KNN Loss: 5.6046319007873535 | BCE Loss: 1.027607798576355\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.6662702560424805 | KNN Loss: 5.615923881530762 | BCE Loss: 1.0503461360931396\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.706708908081055 | KNN Loss: 5.655028343200684 | BCE Loss: 1.051680326461792\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.68624210357666 | KNN Loss: 5.662965774536133 | BCE Loss: 1.0232763290405273\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.722146034240723 | KNN Loss: 5.72104549407959 | BCE Loss: 1.0011005401611328\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.67894983291626 | KNN Loss: 5.623099327087402 | BCE Loss: 1.0558503866195679\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.63800048828125 | KNN Loss: 5.601640701293945 | BCE Loss: 1.0363600254058838\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.6868181228637695 | KNN Loss: 5.643866062164307 | BCE Loss: 1.0429519414901733\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.701252460479736 | KNN Loss: 5.641026496887207 | BCE Loss: 1.0602259635925293\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.667909622192383 | KNN Loss: 5.62019157409668 | BCE Loss: 1.047717809677124\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.649649620056152 | KNN Loss: 5.642978191375732 | BCE Loss: 1.00667142868042\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.6976847648620605 | KNN Loss: 5.630497455596924 | BCE Loss: 1.0671874284744263\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.752973556518555 | KNN Loss: 5.674161434173584 | BCE Loss: 1.0788123607635498\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.68430233001709 | KNN Loss: 5.646524429321289 | BCE Loss: 1.0377776622772217\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.6509857177734375 | KNN Loss: 5.630095481872559 | BCE Loss: 1.0208899974822998\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.700328350067139 | KNN Loss: 5.651099681854248 | BCE Loss: 1.049228549003601\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.663214683532715 | KNN Loss: 5.610487937927246 | BCE Loss: 1.0527265071868896\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.65180778503418 | KNN Loss: 5.596792697906494 | BCE Loss: 1.0550150871276855\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.76934289932251 | KNN Loss: 5.7267303466796875 | BCE Loss: 1.0426126718521118\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.7010345458984375 | KNN Loss: 5.646289348602295 | BCE Loss: 1.0547451972961426\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.68766975402832 | KNN Loss: 5.633122444152832 | BCE Loss: 1.0545470714569092\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.660130977630615 | KNN Loss: 5.60769510269165 | BCE Loss: 1.0524359941482544\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.682336807250977 | KNN Loss: 5.6067986488342285 | BCE Loss: 1.075538158416748\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.7041401863098145 | KNN Loss: 5.632521629333496 | BCE Loss: 1.0716184377670288\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.6665849685668945 | KNN Loss: 5.614168643951416 | BCE Loss: 1.0524160861968994\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.61870813369751 | KNN Loss: 5.602355003356934 | BCE Loss: 1.0163531303405762\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.721469402313232 | KNN Loss: 5.656858921051025 | BCE Loss: 1.064610481262207\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.665468215942383 | KNN Loss: 5.621680736541748 | BCE Loss: 1.0437875986099243\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.7574968338012695 | KNN Loss: 5.705880165100098 | BCE Loss: 1.0516167879104614\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.694097518920898 | KNN Loss: 5.628325939178467 | BCE Loss: 1.0657713413238525\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.740226745605469 | KNN Loss: 5.689629554748535 | BCE Loss: 1.0505974292755127\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.757694244384766 | KNN Loss: 5.694474697113037 | BCE Loss: 1.0632197856903076\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.681596755981445 | KNN Loss: 5.622488021850586 | BCE Loss: 1.0591089725494385\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 6.727547645568848 | KNN Loss: 5.693493366241455 | BCE Loss: 1.0340540409088135\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.6843156814575195 | KNN Loss: 5.638408660888672 | BCE Loss: 1.0459070205688477\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.709349632263184 | KNN Loss: 5.653002738952637 | BCE Loss: 1.0563466548919678\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.701025009155273 | KNN Loss: 5.645088195800781 | BCE Loss: 1.055936574935913\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.705075263977051 | KNN Loss: 5.644311428070068 | BCE Loss: 1.0607635974884033\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.669640064239502 | KNN Loss: 5.602053165435791 | BCE Loss: 1.0675867795944214\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.667313575744629 | KNN Loss: 5.618271827697754 | BCE Loss: 1.049041986465454\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.686548233032227 | KNN Loss: 5.642272472381592 | BCE Loss: 1.0442759990692139\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.697742462158203 | KNN Loss: 5.641656398773193 | BCE Loss: 1.0560860633850098\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 6.69416618347168 | KNN Loss: 5.631062030792236 | BCE Loss: 1.0631043910980225\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.72764778137207 | KNN Loss: 5.658368110656738 | BCE Loss: 1.069279432296753\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.690070152282715 | KNN Loss: 5.652132034301758 | BCE Loss: 1.0379382371902466\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.706786632537842 | KNN Loss: 5.650131702423096 | BCE Loss: 1.056654930114746\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.666651248931885 | KNN Loss: 5.626114368438721 | BCE Loss: 1.040536880493164\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.668197154998779 | KNN Loss: 5.614286422729492 | BCE Loss: 1.053910732269287\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.707846641540527 | KNN Loss: 5.627535343170166 | BCE Loss: 1.0803114175796509\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 6.629513740539551 | KNN Loss: 5.613539695739746 | BCE Loss: 1.0159740447998047\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.694933891296387 | KNN Loss: 5.639899253845215 | BCE Loss: 1.055034875869751\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.663112640380859 | KNN Loss: 5.6108174324035645 | BCE Loss: 1.0522950887680054\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 6.679625034332275 | KNN Loss: 5.620707035064697 | BCE Loss: 1.0589178800582886\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.734827995300293 | KNN Loss: 5.691535949707031 | BCE Loss: 1.0432918071746826\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.647052764892578 | KNN Loss: 5.603231430053711 | BCE Loss: 1.0438215732574463\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.659035682678223 | KNN Loss: 5.609965801239014 | BCE Loss: 1.049069881439209\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.639352798461914 | KNN Loss: 5.6097412109375 | BCE Loss: 1.0296118259429932\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.639404773712158 | KNN Loss: 5.614351272583008 | BCE Loss: 1.0250535011291504\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.714210033416748 | KNN Loss: 5.682837963104248 | BCE Loss: 1.0313720703125\n",
      "Epoch   118: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.687798500061035 | KNN Loss: 5.647119998931885 | BCE Loss: 1.0406787395477295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 6.690972328186035 | KNN Loss: 5.633313179016113 | BCE Loss: 1.057659387588501\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.655465602874756 | KNN Loss: 5.597350597381592 | BCE Loss: 1.0581148862838745\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.643095970153809 | KNN Loss: 5.589795112609863 | BCE Loss: 1.0533010959625244\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.687753677368164 | KNN Loss: 5.645359992980957 | BCE Loss: 1.0423938035964966\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.676441192626953 | KNN Loss: 5.600766181945801 | BCE Loss: 1.0756747722625732\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.698936462402344 | KNN Loss: 5.625314235687256 | BCE Loss: 1.073622465133667\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.703783988952637 | KNN Loss: 5.646705150604248 | BCE Loss: 1.0570785999298096\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.687049388885498 | KNN Loss: 5.648042678833008 | BCE Loss: 1.0390067100524902\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.660706520080566 | KNN Loss: 5.616678237915039 | BCE Loss: 1.0440285205841064\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.69365930557251 | KNN Loss: 5.637372970581055 | BCE Loss: 1.0562864542007446\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.710625648498535 | KNN Loss: 5.653682231903076 | BCE Loss: 1.0569431781768799\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.692175388336182 | KNN Loss: 5.645979404449463 | BCE Loss: 1.0461959838867188\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.716447353363037 | KNN Loss: 5.616403102874756 | BCE Loss: 1.1000443696975708\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.641880035400391 | KNN Loss: 5.6084442138671875 | BCE Loss: 1.0334358215332031\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.686283111572266 | KNN Loss: 5.668642520904541 | BCE Loss: 1.0176403522491455\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.6783552169799805 | KNN Loss: 5.609399318695068 | BCE Loss: 1.0689557790756226\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.668360710144043 | KNN Loss: 5.650489330291748 | BCE Loss: 1.017871618270874\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.680426597595215 | KNN Loss: 5.638821125030518 | BCE Loss: 1.0416054725646973\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.731166839599609 | KNN Loss: 5.635298728942871 | BCE Loss: 1.0958683490753174\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.674552917480469 | KNN Loss: 5.6149001121521 | BCE Loss: 1.0596528053283691\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.7212018966674805 | KNN Loss: 5.671829700469971 | BCE Loss: 1.0493724346160889\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.649111747741699 | KNN Loss: 5.643120288848877 | BCE Loss: 1.0059914588928223\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.747260570526123 | KNN Loss: 5.663114070892334 | BCE Loss: 1.084146499633789\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.637971878051758 | KNN Loss: 5.629359722137451 | BCE Loss: 1.0086121559143066\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.641817569732666 | KNN Loss: 5.6029276847839355 | BCE Loss: 1.038889765739441\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.680366516113281 | KNN Loss: 5.623184680938721 | BCE Loss: 1.0571818351745605\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.6986002922058105 | KNN Loss: 5.662949562072754 | BCE Loss: 1.0356507301330566\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.664487838745117 | KNN Loss: 5.618427276611328 | BCE Loss: 1.046060562133789\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.694852352142334 | KNN Loss: 5.6553754806518555 | BCE Loss: 1.0394768714904785\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.717334747314453 | KNN Loss: 5.666511535644531 | BCE Loss: 1.0508230924606323\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.662631988525391 | KNN Loss: 5.625510215759277 | BCE Loss: 1.0371217727661133\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.698965549468994 | KNN Loss: 5.641646862030029 | BCE Loss: 1.0573186874389648\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.658366680145264 | KNN Loss: 5.610444068908691 | BCE Loss: 1.0479226112365723\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.716305732727051 | KNN Loss: 5.667886257171631 | BCE Loss: 1.0484195947647095\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.65397310256958 | KNN Loss: 5.605411529541016 | BCE Loss: 1.0485615730285645\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.696157455444336 | KNN Loss: 5.65136194229126 | BCE Loss: 1.044795274734497\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.702033042907715 | KNN Loss: 5.6587815284729 | BCE Loss: 1.0432517528533936\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.79632568359375 | KNN Loss: 5.70432186126709 | BCE Loss: 1.0920038223266602\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.709460258483887 | KNN Loss: 5.658656120300293 | BCE Loss: 1.0508040189743042\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.657670021057129 | KNN Loss: 5.610022068023682 | BCE Loss: 1.0476481914520264\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.674745559692383 | KNN Loss: 5.617117881774902 | BCE Loss: 1.0576274394989014\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.706772804260254 | KNN Loss: 5.646808624267578 | BCE Loss: 1.0599640607833862\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.63589334487915 | KNN Loss: 5.60527229309082 | BCE Loss: 1.0306209325790405\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.735466957092285 | KNN Loss: 5.64616060256958 | BCE Loss: 1.0893062353134155\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.690400123596191 | KNN Loss: 5.630428314208984 | BCE Loss: 1.059971809387207\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.650447368621826 | KNN Loss: 5.613152027130127 | BCE Loss: 1.0372954607009888\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.694247245788574 | KNN Loss: 5.619871616363525 | BCE Loss: 1.074375867843628\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.784618377685547 | KNN Loss: 5.729355335235596 | BCE Loss: 1.0552630424499512\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.655218601226807 | KNN Loss: 5.609109401702881 | BCE Loss: 1.0461091995239258\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.653093338012695 | KNN Loss: 5.616990566253662 | BCE Loss: 1.0361027717590332\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.668935775756836 | KNN Loss: 5.60875940322876 | BCE Loss: 1.0601764917373657\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.748424053192139 | KNN Loss: 5.699442386627197 | BCE Loss: 1.0489815473556519\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.699957847595215 | KNN Loss: 5.646396636962891 | BCE Loss: 1.0535614490509033\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.684762954711914 | KNN Loss: 5.630192756652832 | BCE Loss: 1.054570198059082\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.7557373046875 | KNN Loss: 5.713931083679199 | BCE Loss: 1.0418064594268799\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.67738676071167 | KNN Loss: 5.648342609405518 | BCE Loss: 1.0290441513061523\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.759748935699463 | KNN Loss: 5.721131801605225 | BCE Loss: 1.0386170148849487\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.716156959533691 | KNN Loss: 5.658542156219482 | BCE Loss: 1.0576145648956299\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.707846641540527 | KNN Loss: 5.664231777191162 | BCE Loss: 1.0436148643493652\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.689640998840332 | KNN Loss: 5.62855863571167 | BCE Loss: 1.061082363128662\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.700928688049316 | KNN Loss: 5.624227523803711 | BCE Loss: 1.0767011642456055\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.729071617126465 | KNN Loss: 5.684662342071533 | BCE Loss: 1.044409155845642\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.64398193359375 | KNN Loss: 5.611761569976807 | BCE Loss: 1.032220482826233\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.71329402923584 | KNN Loss: 5.659936904907227 | BCE Loss: 1.0533568859100342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.705039978027344 | KNN Loss: 5.68022346496582 | BCE Loss: 1.0248167514801025\n",
      "Epoch   129: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.657160758972168 | KNN Loss: 5.610757827758789 | BCE Loss: 1.046403169631958\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.646566867828369 | KNN Loss: 5.61391019821167 | BCE Loss: 1.0326565504074097\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.717144966125488 | KNN Loss: 5.698241233825684 | BCE Loss: 1.0189037322998047\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.739503860473633 | KNN Loss: 5.692935466766357 | BCE Loss: 1.046568512916565\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.642569541931152 | KNN Loss: 5.597662925720215 | BCE Loss: 1.044906497001648\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 6.669338226318359 | KNN Loss: 5.624062538146973 | BCE Loss: 1.0452758073806763\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.644662857055664 | KNN Loss: 5.606582164764404 | BCE Loss: 1.0380808115005493\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.676976203918457 | KNN Loss: 5.61561393737793 | BCE Loss: 1.0613622665405273\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.665027141571045 | KNN Loss: 5.619287014007568 | BCE Loss: 1.0457401275634766\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.660606861114502 | KNN Loss: 5.625990390777588 | BCE Loss: 1.0346163511276245\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.688404083251953 | KNN Loss: 5.641345500946045 | BCE Loss: 1.0470585823059082\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.695651054382324 | KNN Loss: 5.637455940246582 | BCE Loss: 1.0581951141357422\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.656102180480957 | KNN Loss: 5.606015205383301 | BCE Loss: 1.0500867366790771\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.713167667388916 | KNN Loss: 5.663043022155762 | BCE Loss: 1.0501247644424438\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.733315944671631 | KNN Loss: 5.649556636810303 | BCE Loss: 1.0837591886520386\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.757871150970459 | KNN Loss: 5.709987163543701 | BCE Loss: 1.0478841066360474\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.636232376098633 | KNN Loss: 5.608098030090332 | BCE Loss: 1.0281343460083008\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.681107521057129 | KNN Loss: 5.6422953605651855 | BCE Loss: 1.0388123989105225\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.664880752563477 | KNN Loss: 5.609390735626221 | BCE Loss: 1.0554900169372559\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.727931022644043 | KNN Loss: 5.671995639801025 | BCE Loss: 1.0559356212615967\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.692756652832031 | KNN Loss: 5.665622711181641 | BCE Loss: 1.0271339416503906\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.6674675941467285 | KNN Loss: 5.601097583770752 | BCE Loss: 1.0663701295852661\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.673315048217773 | KNN Loss: 5.628401279449463 | BCE Loss: 1.0449137687683105\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.726161003112793 | KNN Loss: 5.653562545776367 | BCE Loss: 1.0725984573364258\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.681431770324707 | KNN Loss: 5.644309997558594 | BCE Loss: 1.0371218919754028\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.706587791442871 | KNN Loss: 5.643596649169922 | BCE Loss: 1.0629909038543701\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.667908668518066 | KNN Loss: 5.63640832901001 | BCE Loss: 1.0315003395080566\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.694845199584961 | KNN Loss: 5.626911640167236 | BCE Loss: 1.0679335594177246\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.670300483703613 | KNN Loss: 5.612861633300781 | BCE Loss: 1.0574389696121216\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.6856818199157715 | KNN Loss: 5.60793924331665 | BCE Loss: 1.0777426958084106\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.638649940490723 | KNN Loss: 5.599571704864502 | BCE Loss: 1.0390782356262207\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.668779373168945 | KNN Loss: 5.643300533294678 | BCE Loss: 1.0254786014556885\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.625904083251953 | KNN Loss: 5.601673603057861 | BCE Loss: 1.0242305994033813\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.6559367179870605 | KNN Loss: 5.615536689758301 | BCE Loss: 1.0403999090194702\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.668894290924072 | KNN Loss: 5.615385055541992 | BCE Loss: 1.05350923538208\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.68528938293457 | KNN Loss: 5.652945041656494 | BCE Loss: 1.032344102859497\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.728082180023193 | KNN Loss: 5.649394512176514 | BCE Loss: 1.0786876678466797\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.779923915863037 | KNN Loss: 5.748592376708984 | BCE Loss: 1.0313314199447632\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.7031683921813965 | KNN Loss: 5.6322760581970215 | BCE Loss: 1.070892333984375\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.64331579208374 | KNN Loss: 5.5990705490112305 | BCE Loss: 1.0442452430725098\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.65224027633667 | KNN Loss: 5.606354236602783 | BCE Loss: 1.0458860397338867\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.604191780090332 | KNN Loss: 5.602700233459473 | BCE Loss: 1.0014917850494385\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.695137977600098 | KNN Loss: 5.611589431762695 | BCE Loss: 1.0835487842559814\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.759374141693115 | KNN Loss: 5.686869144439697 | BCE Loss: 1.072504997253418\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.708186149597168 | KNN Loss: 5.678319454193115 | BCE Loss: 1.0298668146133423\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.697001934051514 | KNN Loss: 5.678619861602783 | BCE Loss: 1.018381953239441\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.790288925170898 | KNN Loss: 5.712345600128174 | BCE Loss: 1.0779435634613037\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.6880717277526855 | KNN Loss: 5.623863697052002 | BCE Loss: 1.0642080307006836\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.687082767486572 | KNN Loss: 5.646892547607422 | BCE Loss: 1.0401901006698608\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.651595592498779 | KNN Loss: 5.623626232147217 | BCE Loss: 1.027969479560852\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.677093505859375 | KNN Loss: 5.64177131652832 | BCE Loss: 1.0353220701217651\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.6766676902771 | KNN Loss: 5.622347354888916 | BCE Loss: 1.054320216178894\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.635810852050781 | KNN Loss: 5.619713306427002 | BCE Loss: 1.0160977840423584\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.695950984954834 | KNN Loss: 5.623182773590088 | BCE Loss: 1.072768211364746\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 6.720624923706055 | KNN Loss: 5.693698883056641 | BCE Loss: 1.0269261598587036\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.648072719573975 | KNN Loss: 5.610838890075684 | BCE Loss: 1.0372339487075806\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.691802024841309 | KNN Loss: 5.6406660079956055 | BCE Loss: 1.0511360168457031\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.717661380767822 | KNN Loss: 5.668700218200684 | BCE Loss: 1.0489611625671387\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.6614179611206055 | KNN Loss: 5.618900299072266 | BCE Loss: 1.0425174236297607\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.6305413246154785 | KNN Loss: 5.614201068878174 | BCE Loss: 1.0163403749465942\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.6755805015563965 | KNN Loss: 5.612836837768555 | BCE Loss: 1.0627437829971313\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.673739910125732 | KNN Loss: 5.6283278465271 | BCE Loss: 1.0454120635986328\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.650591850280762 | KNN Loss: 5.612263202667236 | BCE Loss: 1.0383288860321045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.665410041809082 | KNN Loss: 5.608802795410156 | BCE Loss: 1.0566071271896362\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.633964538574219 | KNN Loss: 5.602878093719482 | BCE Loss: 1.0310863256454468\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.724414825439453 | KNN Loss: 5.664705276489258 | BCE Loss: 1.0597097873687744\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.637143611907959 | KNN Loss: 5.619965076446533 | BCE Loss: 1.0171785354614258\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.742663860321045 | KNN Loss: 5.677785396575928 | BCE Loss: 1.0648784637451172\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.692472457885742 | KNN Loss: 5.632762908935547 | BCE Loss: 1.0597093105316162\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.73406982421875 | KNN Loss: 5.661208152770996 | BCE Loss: 1.072861909866333\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.7213850021362305 | KNN Loss: 5.658390045166016 | BCE Loss: 1.0629950761795044\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.702475547790527 | KNN Loss: 5.653384208679199 | BCE Loss: 1.0490912199020386\n",
      "Epoch   141: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.747175693511963 | KNN Loss: 5.697658061981201 | BCE Loss: 1.0495177507400513\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.647899627685547 | KNN Loss: 5.628037929534912 | BCE Loss: 1.0198618173599243\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.66496467590332 | KNN Loss: 5.637901306152344 | BCE Loss: 1.0270633697509766\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.687148571014404 | KNN Loss: 5.6465325355529785 | BCE Loss: 1.0406161546707153\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.690051555633545 | KNN Loss: 5.638834476470947 | BCE Loss: 1.051216959953308\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 6.6687421798706055 | KNN Loss: 5.6108503341674805 | BCE Loss: 1.057891845703125\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.6656107902526855 | KNN Loss: 5.619378089904785 | BCE Loss: 1.04623281955719\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.647867679595947 | KNN Loss: 5.6130547523498535 | BCE Loss: 1.0348130464553833\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.705004692077637 | KNN Loss: 5.641920566558838 | BCE Loss: 1.0630841255187988\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.714890003204346 | KNN Loss: 5.647033214569092 | BCE Loss: 1.0678566694259644\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.799860000610352 | KNN Loss: 5.736449241638184 | BCE Loss: 1.0634105205535889\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.743161201477051 | KNN Loss: 5.6960954666137695 | BCE Loss: 1.0470657348632812\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.680001258850098 | KNN Loss: 5.622790813446045 | BCE Loss: 1.0572106838226318\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.7564239501953125 | KNN Loss: 5.694607734680176 | BCE Loss: 1.0618162155151367\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 6.719655513763428 | KNN Loss: 5.687652111053467 | BCE Loss: 1.0320035219192505\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.633201599121094 | KNN Loss: 5.605261325836182 | BCE Loss: 1.0279401540756226\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.6977739334106445 | KNN Loss: 5.648068904876709 | BCE Loss: 1.0497047901153564\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.665187358856201 | KNN Loss: 5.612083911895752 | BCE Loss: 1.0531035661697388\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.644324779510498 | KNN Loss: 5.607699394226074 | BCE Loss: 1.0366255044937134\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 6.671971321105957 | KNN Loss: 5.622526168823242 | BCE Loss: 1.0494451522827148\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.664398193359375 | KNN Loss: 5.625558376312256 | BCE Loss: 1.0388400554656982\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.662360191345215 | KNN Loss: 5.607978820800781 | BCE Loss: 1.0543813705444336\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.798844337463379 | KNN Loss: 5.723243713378906 | BCE Loss: 1.0756008625030518\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.6525397300720215 | KNN Loss: 5.625007152557373 | BCE Loss: 1.0275324583053589\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.629522323608398 | KNN Loss: 5.605676651000977 | BCE Loss: 1.0238455533981323\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.635959148406982 | KNN Loss: 5.611999034881592 | BCE Loss: 1.0239601135253906\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.66275691986084 | KNN Loss: 5.610496997833252 | BCE Loss: 1.052259922027588\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.663689136505127 | KNN Loss: 5.624081134796143 | BCE Loss: 1.0396080017089844\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.681797981262207 | KNN Loss: 5.615529537200928 | BCE Loss: 1.0662683248519897\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 6.708544731140137 | KNN Loss: 5.616983413696289 | BCE Loss: 1.0915615558624268\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.690281867980957 | KNN Loss: 5.641968727111816 | BCE Loss: 1.0483129024505615\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.679279327392578 | KNN Loss: 5.6081013679504395 | BCE Loss: 1.0711779594421387\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.681558609008789 | KNN Loss: 5.651407241821289 | BCE Loss: 1.0301514863967896\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.7103681564331055 | KNN Loss: 5.658219337463379 | BCE Loss: 1.0521488189697266\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.839917182922363 | KNN Loss: 5.775747776031494 | BCE Loss: 1.0641695261001587\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.653288841247559 | KNN Loss: 5.597767353057861 | BCE Loss: 1.0555217266082764\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.64019250869751 | KNN Loss: 5.605121612548828 | BCE Loss: 1.035070776939392\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 6.690653324127197 | KNN Loss: 5.655765533447266 | BCE Loss: 1.0348877906799316\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.706457138061523 | KNN Loss: 5.671496868133545 | BCE Loss: 1.034960389137268\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.672032833099365 | KNN Loss: 5.616884231567383 | BCE Loss: 1.0551484823226929\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.645950794219971 | KNN Loss: 5.595394611358643 | BCE Loss: 1.0505561828613281\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.731575012207031 | KNN Loss: 5.6894378662109375 | BCE Loss: 1.0421371459960938\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.6794233322143555 | KNN Loss: 5.614757537841797 | BCE Loss: 1.0646655559539795\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.6653547286987305 | KNN Loss: 5.617721080780029 | BCE Loss: 1.047633409500122\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.726994037628174 | KNN Loss: 5.662435531616211 | BCE Loss: 1.064558506011963\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.703914642333984 | KNN Loss: 5.627257347106934 | BCE Loss: 1.0766575336456299\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.7386956214904785 | KNN Loss: 5.672427177429199 | BCE Loss: 1.0662684440612793\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.7491655349731445 | KNN Loss: 5.697915554046631 | BCE Loss: 1.0512499809265137\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.65512752532959 | KNN Loss: 5.613054275512695 | BCE Loss: 1.0420734882354736\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.711221694946289 | KNN Loss: 5.640456676483154 | BCE Loss: 1.0707651376724243\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.714555740356445 | KNN Loss: 5.656821250915527 | BCE Loss: 1.0577346086502075\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.742260932922363 | KNN Loss: 5.700073719024658 | BCE Loss: 1.042186975479126\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.70991325378418 | KNN Loss: 5.64334774017334 | BCE Loss: 1.0665653944015503\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.6738481521606445 | KNN Loss: 5.635094165802002 | BCE Loss: 1.0387539863586426\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.682842254638672 | KNN Loss: 5.659332275390625 | BCE Loss: 1.0235099792480469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.680704116821289 | KNN Loss: 5.618550777435303 | BCE Loss: 1.0621533393859863\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.6456403732299805 | KNN Loss: 5.617905616760254 | BCE Loss: 1.0277347564697266\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.703197002410889 | KNN Loss: 5.628095626831055 | BCE Loss: 1.0751012563705444\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.682847499847412 | KNN Loss: 5.633007526397705 | BCE Loss: 1.049839973449707\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.686898231506348 | KNN Loss: 5.640496730804443 | BCE Loss: 1.0464016199111938\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.712689399719238 | KNN Loss: 5.67117977142334 | BCE Loss: 1.0415095090866089\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.645291328430176 | KNN Loss: 5.596818923950195 | BCE Loss: 1.0484721660614014\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.711159706115723 | KNN Loss: 5.631913661956787 | BCE Loss: 1.079246163368225\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.671274662017822 | KNN Loss: 5.636780738830566 | BCE Loss: 1.0344940423965454\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.694765090942383 | KNN Loss: 5.651368141174316 | BCE Loss: 1.0433967113494873\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.68247127532959 | KNN Loss: 5.63849401473999 | BCE Loss: 1.0439772605895996\n",
      "Epoch   152: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.677518367767334 | KNN Loss: 5.631338119506836 | BCE Loss: 1.0461803674697876\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.672931671142578 | KNN Loss: 5.6172075271606445 | BCE Loss: 1.0557239055633545\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.712508678436279 | KNN Loss: 5.654420375823975 | BCE Loss: 1.0580881834030151\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 6.645622253417969 | KNN Loss: 5.606504917144775 | BCE Loss: 1.0391172170639038\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.61494255065918 | KNN Loss: 5.605319023132324 | BCE Loss: 1.0096235275268555\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.642718315124512 | KNN Loss: 5.601259231567383 | BCE Loss: 1.0414588451385498\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.689373970031738 | KNN Loss: 5.633893013000488 | BCE Loss: 1.05548095703125\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.66661262512207 | KNN Loss: 5.607694625854492 | BCE Loss: 1.0589179992675781\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.77729606628418 | KNN Loss: 5.712325096130371 | BCE Loss: 1.064970850944519\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.632657051086426 | KNN Loss: 5.604643821716309 | BCE Loss: 1.028012990951538\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.66929817199707 | KNN Loss: 5.605841159820557 | BCE Loss: 1.0634567737579346\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.667756080627441 | KNN Loss: 5.6246657371521 | BCE Loss: 1.0430903434753418\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.673137187957764 | KNN Loss: 5.63884162902832 | BCE Loss: 1.0342955589294434\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.692677021026611 | KNN Loss: 5.623117923736572 | BCE Loss: 1.069559097290039\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.6399431228637695 | KNN Loss: 5.595306873321533 | BCE Loss: 1.0446362495422363\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.64189338684082 | KNN Loss: 5.612393856048584 | BCE Loss: 1.0294994115829468\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.714526653289795 | KNN Loss: 5.674398899078369 | BCE Loss: 1.0401277542114258\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.692132949829102 | KNN Loss: 5.649847984313965 | BCE Loss: 1.0422847270965576\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.6708784103393555 | KNN Loss: 5.606637954711914 | BCE Loss: 1.0642406940460205\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.685469627380371 | KNN Loss: 5.625490188598633 | BCE Loss: 1.0599795579910278\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.671837329864502 | KNN Loss: 5.60103702545166 | BCE Loss: 1.0708003044128418\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.724679946899414 | KNN Loss: 5.6794514656066895 | BCE Loss: 1.045228362083435\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.678116321563721 | KNN Loss: 5.605920314788818 | BCE Loss: 1.0721958875656128\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.668678283691406 | KNN Loss: 5.626718044281006 | BCE Loss: 1.0419604778289795\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.68646240234375 | KNN Loss: 5.640039920806885 | BCE Loss: 1.0464222431182861\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.648338317871094 | KNN Loss: 5.631282806396484 | BCE Loss: 1.017055630683899\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.7985124588012695 | KNN Loss: 5.714365482330322 | BCE Loss: 1.0841469764709473\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.70893669128418 | KNN Loss: 5.647048473358154 | BCE Loss: 1.0618884563446045\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.662152290344238 | KNN Loss: 5.6532440185546875 | BCE Loss: 1.0089085102081299\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.737860202789307 | KNN Loss: 5.670010566711426 | BCE Loss: 1.0678496360778809\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.694793701171875 | KNN Loss: 5.653470993041992 | BCE Loss: 1.041322946548462\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.690618515014648 | KNN Loss: 5.643528938293457 | BCE Loss: 1.0470898151397705\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.674803733825684 | KNN Loss: 5.62636661529541 | BCE Loss: 1.0484373569488525\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 6.713386535644531 | KNN Loss: 5.6637187004089355 | BCE Loss: 1.0496675968170166\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.687712669372559 | KNN Loss: 5.629100799560547 | BCE Loss: 1.0586116313934326\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.696581840515137 | KNN Loss: 5.624118804931641 | BCE Loss: 1.072462797164917\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.661688804626465 | KNN Loss: 5.637180805206299 | BCE Loss: 1.0245081186294556\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.688003063201904 | KNN Loss: 5.638326644897461 | BCE Loss: 1.0496764183044434\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.6734232902526855 | KNN Loss: 5.642811298370361 | BCE Loss: 1.0306119918823242\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.6964826583862305 | KNN Loss: 5.630597114562988 | BCE Loss: 1.0658856630325317\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.695758819580078 | KNN Loss: 5.645946979522705 | BCE Loss: 1.049811840057373\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.723903656005859 | KNN Loss: 5.673094272613525 | BCE Loss: 1.0508091449737549\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.612517833709717 | KNN Loss: 5.592543125152588 | BCE Loss: 1.019974708557129\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.6839280128479 | KNN Loss: 5.625531196594238 | BCE Loss: 1.0583969354629517\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.7697343826293945 | KNN Loss: 5.674315452575684 | BCE Loss: 1.095418930053711\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.714942932128906 | KNN Loss: 5.661593914031982 | BCE Loss: 1.0533487796783447\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.677379608154297 | KNN Loss: 5.63695764541626 | BCE Loss: 1.040421724319458\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.706631183624268 | KNN Loss: 5.6450324058532715 | BCE Loss: 1.0615986585617065\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.649296283721924 | KNN Loss: 5.616750240325928 | BCE Loss: 1.0325461626052856\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.724422454833984 | KNN Loss: 5.691007137298584 | BCE Loss: 1.03341543674469\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 6.694403648376465 | KNN Loss: 5.611911296844482 | BCE Loss: 1.0824921131134033\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.657944679260254 | KNN Loss: 5.609701633453369 | BCE Loss: 1.0482432842254639\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.612613201141357 | KNN Loss: 5.599456787109375 | BCE Loss: 1.0131562948226929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.612914562225342 | KNN Loss: 5.594588279724121 | BCE Loss: 1.0183262825012207\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.6717939376831055 | KNN Loss: 5.6015143394470215 | BCE Loss: 1.0702794790267944\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.6479411125183105 | KNN Loss: 5.614067077636719 | BCE Loss: 1.0338740348815918\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.656642913818359 | KNN Loss: 5.597691059112549 | BCE Loss: 1.0589516162872314\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.7353386878967285 | KNN Loss: 5.640489101409912 | BCE Loss: 1.0948495864868164\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.638606071472168 | KNN Loss: 5.623013019561768 | BCE Loss: 1.0155930519104004\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.6692023277282715 | KNN Loss: 5.608076095581055 | BCE Loss: 1.0611262321472168\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.699557781219482 | KNN Loss: 5.655820369720459 | BCE Loss: 1.0437374114990234\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.690611362457275 | KNN Loss: 5.615036487579346 | BCE Loss: 1.0755748748779297\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.656999111175537 | KNN Loss: 5.610888957977295 | BCE Loss: 1.0461101531982422\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.687575340270996 | KNN Loss: 5.6286702156066895 | BCE Loss: 1.0589048862457275\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.704468250274658 | KNN Loss: 5.650078296661377 | BCE Loss: 1.0543899536132812\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.809458255767822 | KNN Loss: 5.757688522338867 | BCE Loss: 1.0517698526382446\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.631365776062012 | KNN Loss: 5.60276985168457 | BCE Loss: 1.028596043586731\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.686013221740723 | KNN Loss: 5.628910064697266 | BCE Loss: 1.057103157043457\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.704061031341553 | KNN Loss: 5.672049522399902 | BCE Loss: 1.03201162815094\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.691281318664551 | KNN Loss: 5.6037445068359375 | BCE Loss: 1.0875365734100342\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.661576271057129 | KNN Loss: 5.633760929107666 | BCE Loss: 1.0278152227401733\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.641138553619385 | KNN Loss: 5.6051344871521 | BCE Loss: 1.0360041856765747\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.7402191162109375 | KNN Loss: 5.682069301605225 | BCE Loss: 1.0581499338150024\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 6.643148899078369 | KNN Loss: 5.605804920196533 | BCE Loss: 1.037343978881836\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.768979549407959 | KNN Loss: 5.710624694824219 | BCE Loss: 1.0583548545837402\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.739513397216797 | KNN Loss: 5.661085605621338 | BCE Loss: 1.0784279108047485\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 6.662551403045654 | KNN Loss: 5.62880277633667 | BCE Loss: 1.033748745918274\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.6576361656188965 | KNN Loss: 5.626733779907227 | BCE Loss: 1.03090238571167\n",
      "Epoch   165: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.743500232696533 | KNN Loss: 5.691713809967041 | BCE Loss: 1.0517863035202026\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.707684516906738 | KNN Loss: 5.636366367340088 | BCE Loss: 1.0713183879852295\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.7256083488464355 | KNN Loss: 5.694705009460449 | BCE Loss: 1.0309033393859863\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.738409996032715 | KNN Loss: 5.695554256439209 | BCE Loss: 1.042855978012085\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.664859771728516 | KNN Loss: 5.61765718460083 | BCE Loss: 1.0472023487091064\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.686291694641113 | KNN Loss: 5.660294532775879 | BCE Loss: 1.0259974002838135\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.669694900512695 | KNN Loss: 5.62624454498291 | BCE Loss: 1.0434503555297852\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.713454246520996 | KNN Loss: 5.657751560211182 | BCE Loss: 1.0557029247283936\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.709804058074951 | KNN Loss: 5.64585542678833 | BCE Loss: 1.0639487504959106\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.655379295349121 | KNN Loss: 5.614718437194824 | BCE Loss: 1.0406608581542969\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.737034797668457 | KNN Loss: 5.702245235443115 | BCE Loss: 1.0347895622253418\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.678948402404785 | KNN Loss: 5.610676288604736 | BCE Loss: 1.0682721138000488\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.793529987335205 | KNN Loss: 5.738338947296143 | BCE Loss: 1.0551910400390625\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 6.732765197753906 | KNN Loss: 5.70819091796875 | BCE Loss: 1.0245741605758667\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.651602745056152 | KNN Loss: 5.608870506286621 | BCE Loss: 1.0427320003509521\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.7111358642578125 | KNN Loss: 5.655375957489014 | BCE Loss: 1.0557600259780884\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.699155807495117 | KNN Loss: 5.667630195617676 | BCE Loss: 1.0315253734588623\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.724091053009033 | KNN Loss: 5.693324565887451 | BCE Loss: 1.0307663679122925\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.693276882171631 | KNN Loss: 5.614556312561035 | BCE Loss: 1.0787206888198853\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.705909729003906 | KNN Loss: 5.658426284790039 | BCE Loss: 1.0474835634231567\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.69541072845459 | KNN Loss: 5.638535499572754 | BCE Loss: 1.056875228881836\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.695021629333496 | KNN Loss: 5.616837501525879 | BCE Loss: 1.0781840085983276\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.645200729370117 | KNN Loss: 5.607330322265625 | BCE Loss: 1.037870168685913\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.678702354431152 | KNN Loss: 5.645421981811523 | BCE Loss: 1.033280372619629\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.64322566986084 | KNN Loss: 5.604731559753418 | BCE Loss: 1.038494348526001\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.675217628479004 | KNN Loss: 5.615019798278809 | BCE Loss: 1.0601978302001953\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.648813247680664 | KNN Loss: 5.599907398223877 | BCE Loss: 1.048905849456787\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.663717269897461 | KNN Loss: 5.632153511047363 | BCE Loss: 1.0315635204315186\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.729833602905273 | KNN Loss: 5.688399791717529 | BCE Loss: 1.0414339303970337\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.771075248718262 | KNN Loss: 5.704826831817627 | BCE Loss: 1.0662485361099243\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.737361907958984 | KNN Loss: 5.709348201751709 | BCE Loss: 1.0280139446258545\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.669316291809082 | KNN Loss: 5.6332268714904785 | BCE Loss: 1.036089301109314\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.732443332672119 | KNN Loss: 5.689940929412842 | BCE Loss: 1.0425022840499878\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 6.7117767333984375 | KNN Loss: 5.652171611785889 | BCE Loss: 1.0596051216125488\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.675942420959473 | KNN Loss: 5.6116533279418945 | BCE Loss: 1.0642893314361572\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 6.664906024932861 | KNN Loss: 5.609750270843506 | BCE Loss: 1.055155634880066\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.67425537109375 | KNN Loss: 5.6460490226745605 | BCE Loss: 1.0282063484191895\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.713510513305664 | KNN Loss: 5.658201217651367 | BCE Loss: 1.055309534072876\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.693103790283203 | KNN Loss: 5.62655782699585 | BCE Loss: 1.0665462017059326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.67775821685791 | KNN Loss: 5.641509532928467 | BCE Loss: 1.036248803138733\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.671025276184082 | KNN Loss: 5.6101555824279785 | BCE Loss: 1.0608699321746826\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.734766006469727 | KNN Loss: 5.6900787353515625 | BCE Loss: 1.044687032699585\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 6.689410209655762 | KNN Loss: 5.611419200897217 | BCE Loss: 1.0779911279678345\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.635105133056641 | KNN Loss: 5.596543788909912 | BCE Loss: 1.0385611057281494\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.841293811798096 | KNN Loss: 5.768370151519775 | BCE Loss: 1.0729236602783203\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.74234676361084 | KNN Loss: 5.680455684661865 | BCE Loss: 1.0618908405303955\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.657424449920654 | KNN Loss: 5.617287635803223 | BCE Loss: 1.040136694908142\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.711926460266113 | KNN Loss: 5.654865264892578 | BCE Loss: 1.0570614337921143\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.734441757202148 | KNN Loss: 5.679966926574707 | BCE Loss: 1.054474949836731\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.656153202056885 | KNN Loss: 5.607572555541992 | BCE Loss: 1.0485806465148926\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.644488334655762 | KNN Loss: 5.600308895111084 | BCE Loss: 1.0441794395446777\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.7072224617004395 | KNN Loss: 5.671816349029541 | BCE Loss: 1.0354061126708984\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.640106201171875 | KNN Loss: 5.6139020919799805 | BCE Loss: 1.026203989982605\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.760200500488281 | KNN Loss: 5.692394256591797 | BCE Loss: 1.067806363105774\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.647744178771973 | KNN Loss: 5.602250099182129 | BCE Loss: 1.0454938411712646\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 6.656607627868652 | KNN Loss: 5.613054275512695 | BCE Loss: 1.043553113937378\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.650633335113525 | KNN Loss: 5.6078667640686035 | BCE Loss: 1.0427666902542114\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.679042816162109 | KNN Loss: 5.656603813171387 | BCE Loss: 1.0224390029907227\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.657690048217773 | KNN Loss: 5.603176116943359 | BCE Loss: 1.0545138120651245\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 6.734066486358643 | KNN Loss: 5.682896614074707 | BCE Loss: 1.0511698722839355\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.673168182373047 | KNN Loss: 5.612722873687744 | BCE Loss: 1.0604454278945923\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.687170028686523 | KNN Loss: 5.65224552154541 | BCE Loss: 1.0349242687225342\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.710888862609863 | KNN Loss: 5.643508434295654 | BCE Loss: 1.0673801898956299\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.6804728507995605 | KNN Loss: 5.636005401611328 | BCE Loss: 1.0444674491882324\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.683971881866455 | KNN Loss: 5.651534080505371 | BCE Loss: 1.0324379205703735\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.646764755249023 | KNN Loss: 5.603289604187012 | BCE Loss: 1.0434751510620117\n",
      "Epoch   176: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.68003511428833 | KNN Loss: 5.659591197967529 | BCE Loss: 1.0204439163208008\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.747313499450684 | KNN Loss: 5.691284656524658 | BCE Loss: 1.0560288429260254\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.670148849487305 | KNN Loss: 5.6194281578063965 | BCE Loss: 1.0507206916809082\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.804039478302002 | KNN Loss: 5.757473468780518 | BCE Loss: 1.0465658903121948\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.695559024810791 | KNN Loss: 5.651747226715088 | BCE Loss: 1.0438119173049927\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.645332336425781 | KNN Loss: 5.601461410522461 | BCE Loss: 1.0438711643218994\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.72074031829834 | KNN Loss: 5.667769908905029 | BCE Loss: 1.0529705286026\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.6987104415893555 | KNN Loss: 5.66471529006958 | BCE Loss: 1.0339953899383545\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.639984130859375 | KNN Loss: 5.605798721313477 | BCE Loss: 1.0341851711273193\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.696881294250488 | KNN Loss: 5.64694881439209 | BCE Loss: 1.0499322414398193\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.653207778930664 | KNN Loss: 5.603757858276367 | BCE Loss: 1.049450159072876\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 6.742684364318848 | KNN Loss: 5.679405212402344 | BCE Loss: 1.063279151916504\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 6.656557559967041 | KNN Loss: 5.626461029052734 | BCE Loss: 1.030096411705017\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.706225395202637 | KNN Loss: 5.647193908691406 | BCE Loss: 1.059031367301941\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.634097576141357 | KNN Loss: 5.600439548492432 | BCE Loss: 1.0336580276489258\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.659928798675537 | KNN Loss: 5.623371124267578 | BCE Loss: 1.036557674407959\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 6.669975280761719 | KNN Loss: 5.62096643447876 | BCE Loss: 1.0490086078643799\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.74836540222168 | KNN Loss: 5.690790176391602 | BCE Loss: 1.0575754642486572\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.6785359382629395 | KNN Loss: 5.639670372009277 | BCE Loss: 1.0388654470443726\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.754066467285156 | KNN Loss: 5.705544471740723 | BCE Loss: 1.0485217571258545\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.691821098327637 | KNN Loss: 5.644153118133545 | BCE Loss: 1.047668218612671\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.643289566040039 | KNN Loss: 5.610071182250977 | BCE Loss: 1.0332183837890625\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.635638236999512 | KNN Loss: 5.614165306091309 | BCE Loss: 1.0214728116989136\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 6.697208404541016 | KNN Loss: 5.622359752655029 | BCE Loss: 1.0748488903045654\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 6.6710615158081055 | KNN Loss: 5.63097620010376 | BCE Loss: 1.0400854349136353\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.785838603973389 | KNN Loss: 5.699405670166016 | BCE Loss: 1.0864330530166626\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.695301055908203 | KNN Loss: 5.626026153564453 | BCE Loss: 1.06927490234375\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.6541595458984375 | KNN Loss: 5.63082218170166 | BCE Loss: 1.0233372449874878\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.7218098640441895 | KNN Loss: 5.652980327606201 | BCE Loss: 1.0688296556472778\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.776657581329346 | KNN Loss: 5.711851596832275 | BCE Loss: 1.0648058652877808\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.718109130859375 | KNN Loss: 5.668379306793213 | BCE Loss: 1.0497300624847412\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.6477556228637695 | KNN Loss: 5.602874755859375 | BCE Loss: 1.0448808670043945\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.649832725524902 | KNN Loss: 5.612471103668213 | BCE Loss: 1.0373618602752686\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.693358421325684 | KNN Loss: 5.635622501373291 | BCE Loss: 1.0577356815338135\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.717788219451904 | KNN Loss: 5.68938684463501 | BCE Loss: 1.028401255607605\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.670108795166016 | KNN Loss: 5.610257148742676 | BCE Loss: 1.0598517656326294\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.666306018829346 | KNN Loss: 5.6265363693237305 | BCE Loss: 1.0397696495056152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.651412010192871 | KNN Loss: 5.6018195152282715 | BCE Loss: 1.0495922565460205\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 6.647519111633301 | KNN Loss: 5.61251163482666 | BCE Loss: 1.0350077152252197\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.702489852905273 | KNN Loss: 5.669201374053955 | BCE Loss: 1.0332882404327393\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 6.726588726043701 | KNN Loss: 5.670341491699219 | BCE Loss: 1.0562472343444824\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.694048881530762 | KNN Loss: 5.6458048820495605 | BCE Loss: 1.048243761062622\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.750754356384277 | KNN Loss: 5.671268939971924 | BCE Loss: 1.0794851779937744\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.6449079513549805 | KNN Loss: 5.614165782928467 | BCE Loss: 1.0307421684265137\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.756049633026123 | KNN Loss: 5.684309482574463 | BCE Loss: 1.0717401504516602\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.664844989776611 | KNN Loss: 5.621970176696777 | BCE Loss: 1.0428749322891235\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.674289226531982 | KNN Loss: 5.633329391479492 | BCE Loss: 1.0409599542617798\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.656844139099121 | KNN Loss: 5.637477397918701 | BCE Loss: 1.019366979598999\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.6647233963012695 | KNN Loss: 5.622454643249512 | BCE Loss: 1.042268991470337\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.663504600524902 | KNN Loss: 5.628305435180664 | BCE Loss: 1.0351991653442383\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.663955211639404 | KNN Loss: 5.599987506866455 | BCE Loss: 1.0639678239822388\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.6525678634643555 | KNN Loss: 5.60506534576416 | BCE Loss: 1.0475022792816162\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.677175521850586 | KNN Loss: 5.632599830627441 | BCE Loss: 1.0445756912231445\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.736810207366943 | KNN Loss: 5.67356014251709 | BCE Loss: 1.0632500648498535\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 6.6662163734436035 | KNN Loss: 5.630529880523682 | BCE Loss: 1.0356863737106323\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 6.668928146362305 | KNN Loss: 5.604956150054932 | BCE Loss: 1.063971996307373\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 6.655248165130615 | KNN Loss: 5.630709171295166 | BCE Loss: 1.0245391130447388\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.668216228485107 | KNN Loss: 5.600467205047607 | BCE Loss: 1.0677491426467896\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.646291732788086 | KNN Loss: 5.604552745819092 | BCE Loss: 1.0417389869689941\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.689795017242432 | KNN Loss: 5.629331588745117 | BCE Loss: 1.0604634284973145\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 6.686145782470703 | KNN Loss: 5.630003452301025 | BCE Loss: 1.0561423301696777\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.674353122711182 | KNN Loss: 5.616394519805908 | BCE Loss: 1.0579584836959839\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 6.704577445983887 | KNN Loss: 5.6521525382995605 | BCE Loss: 1.0524251461029053\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.645325183868408 | KNN Loss: 5.639692306518555 | BCE Loss: 1.0056328773498535\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.6776299476623535 | KNN Loss: 5.6286420822143555 | BCE Loss: 1.048987865447998\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.633420467376709 | KNN Loss: 5.604712963104248 | BCE Loss: 1.0287073850631714\n",
      "Epoch   187: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.6556243896484375 | KNN Loss: 5.6043548583984375 | BCE Loss: 1.051269769668579\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 6.688471794128418 | KNN Loss: 5.6326680183410645 | BCE Loss: 1.0558040142059326\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.630669116973877 | KNN Loss: 5.604398250579834 | BCE Loss: 1.026270866394043\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.677361488342285 | KNN Loss: 5.629436492919922 | BCE Loss: 1.0479249954223633\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.725187301635742 | KNN Loss: 5.652750015258789 | BCE Loss: 1.0724374055862427\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.676504135131836 | KNN Loss: 5.616245746612549 | BCE Loss: 1.0602582693099976\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.652702331542969 | KNN Loss: 5.612224102020264 | BCE Loss: 1.0404784679412842\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.727437973022461 | KNN Loss: 5.657874584197998 | BCE Loss: 1.0695632696151733\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.664332389831543 | KNN Loss: 5.6295623779296875 | BCE Loss: 1.0347702503204346\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.720594882965088 | KNN Loss: 5.644697666168213 | BCE Loss: 1.075897216796875\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.681692123413086 | KNN Loss: 5.614850044250488 | BCE Loss: 1.0668418407440186\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.6899094581604 | KNN Loss: 5.6428542137146 | BCE Loss: 1.0470552444458008\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.680987358093262 | KNN Loss: 5.645609378814697 | BCE Loss: 1.035377860069275\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.684998035430908 | KNN Loss: 5.636903285980225 | BCE Loss: 1.048094630241394\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.750829696655273 | KNN Loss: 5.678597450256348 | BCE Loss: 1.0722324848175049\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.665884494781494 | KNN Loss: 5.61572265625 | BCE Loss: 1.0501618385314941\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.710731506347656 | KNN Loss: 5.67396879196167 | BCE Loss: 1.0367624759674072\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.669697284698486 | KNN Loss: 5.628769874572754 | BCE Loss: 1.0409274101257324\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.655635833740234 | KNN Loss: 5.60164213180542 | BCE Loss: 1.0539937019348145\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.6699724197387695 | KNN Loss: 5.607966423034668 | BCE Loss: 1.0620062351226807\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.661954879760742 | KNN Loss: 5.613164901733398 | BCE Loss: 1.0487899780273438\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.6497111320495605 | KNN Loss: 5.619139671325684 | BCE Loss: 1.030571460723877\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.668605804443359 | KNN Loss: 5.617877006530762 | BCE Loss: 1.050728678703308\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.684360980987549 | KNN Loss: 5.607911109924316 | BCE Loss: 1.0764498710632324\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.668876647949219 | KNN Loss: 5.609130859375 | BCE Loss: 1.0597456693649292\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.672337532043457 | KNN Loss: 5.625394344329834 | BCE Loss: 1.046943187713623\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.699809551239014 | KNN Loss: 5.68758487701416 | BCE Loss: 1.0122246742248535\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.6983747482299805 | KNN Loss: 5.671802997589111 | BCE Loss: 1.02657151222229\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.753274917602539 | KNN Loss: 5.649038314819336 | BCE Loss: 1.1042364835739136\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.696446895599365 | KNN Loss: 5.637922286987305 | BCE Loss: 1.0585246086120605\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.747960567474365 | KNN Loss: 5.685082912445068 | BCE Loss: 1.0628775358200073\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 6.6536760330200195 | KNN Loss: 5.601914882659912 | BCE Loss: 1.0517613887786865\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.689614772796631 | KNN Loss: 5.619970798492432 | BCE Loss: 1.0696438550949097\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.667563438415527 | KNN Loss: 5.599293231964111 | BCE Loss: 1.068270206451416\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.719585418701172 | KNN Loss: 5.711810111999512 | BCE Loss: 1.0077753067016602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.677616119384766 | KNN Loss: 5.613646030426025 | BCE Loss: 1.0639702081680298\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.707725524902344 | KNN Loss: 5.670441627502441 | BCE Loss: 1.0372836589813232\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.633491039276123 | KNN Loss: 5.600869178771973 | BCE Loss: 1.03262197971344\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.727217674255371 | KNN Loss: 5.666467666625977 | BCE Loss: 1.0607502460479736\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.638514518737793 | KNN Loss: 5.6036810874938965 | BCE Loss: 1.034833550453186\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.653772830963135 | KNN Loss: 5.6080403327941895 | BCE Loss: 1.0457323789596558\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.69126558303833 | KNN Loss: 5.625704765319824 | BCE Loss: 1.0655606985092163\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.671272277832031 | KNN Loss: 5.638047218322754 | BCE Loss: 1.0332252979278564\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.676095962524414 | KNN Loss: 5.627617359161377 | BCE Loss: 1.048478603363037\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.633716583251953 | KNN Loss: 5.6066813468933105 | BCE Loss: 1.027035117149353\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.7204389572143555 | KNN Loss: 5.666983127593994 | BCE Loss: 1.0534559488296509\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.685953140258789 | KNN Loss: 5.623251914978027 | BCE Loss: 1.0627014636993408\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.660930633544922 | KNN Loss: 5.601381301879883 | BCE Loss: 1.05954909324646\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.661026477813721 | KNN Loss: 5.606602668762207 | BCE Loss: 1.0544239282608032\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.701747894287109 | KNN Loss: 5.632290363311768 | BCE Loss: 1.0694574117660522\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.670530319213867 | KNN Loss: 5.622206687927246 | BCE Loss: 1.048323392868042\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.6643877029418945 | KNN Loss: 5.620177268981934 | BCE Loss: 1.0442101955413818\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.658653736114502 | KNN Loss: 5.6152825355529785 | BCE Loss: 1.0433710813522339\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.635119915008545 | KNN Loss: 5.600228786468506 | BCE Loss: 1.0348912477493286\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 6.736432075500488 | KNN Loss: 5.686480522155762 | BCE Loss: 1.0499515533447266\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.663163185119629 | KNN Loss: 5.6104230880737305 | BCE Loss: 1.0527403354644775\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.664428234100342 | KNN Loss: 5.610418796539307 | BCE Loss: 1.0540094375610352\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.780092239379883 | KNN Loss: 5.703363418579102 | BCE Loss: 1.0767285823822021\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 6.624324798583984 | KNN Loss: 5.607307434082031 | BCE Loss: 1.0170176029205322\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.679446220397949 | KNN Loss: 5.62973165512085 | BCE Loss: 1.04971444606781\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 6.669841289520264 | KNN Loss: 5.630133152008057 | BCE Loss: 1.0397082567214966\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.653409957885742 | KNN Loss: 5.606291770935059 | BCE Loss: 1.0471179485321045\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.722615718841553 | KNN Loss: 5.641812801361084 | BCE Loss: 1.0808029174804688\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.723798751831055 | KNN Loss: 5.669084548950195 | BCE Loss: 1.0547142028808594\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.652585029602051 | KNN Loss: 5.609080791473389 | BCE Loss: 1.043503999710083\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.698818206787109 | KNN Loss: 5.653771877288818 | BCE Loss: 1.045046091079712\n",
      "Epoch   198: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.648794174194336 | KNN Loss: 5.597482681274414 | BCE Loss: 1.0513112545013428\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.749733924865723 | KNN Loss: 5.673168659210205 | BCE Loss: 1.0765650272369385\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.668599605560303 | KNN Loss: 5.616216659545898 | BCE Loss: 1.0523829460144043\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.816025733947754 | KNN Loss: 5.7474470138549805 | BCE Loss: 1.0685786008834839\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.705670356750488 | KNN Loss: 5.6444292068481445 | BCE Loss: 1.0612413883209229\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.719970226287842 | KNN Loss: 5.667072296142578 | BCE Loss: 1.0528980493545532\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.708946704864502 | KNN Loss: 5.634011268615723 | BCE Loss: 1.0749353170394897\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.743977069854736 | KNN Loss: 5.712136268615723 | BCE Loss: 1.0318406820297241\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 6.711578369140625 | KNN Loss: 5.63309383392334 | BCE Loss: 1.0784844160079956\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 6.682566165924072 | KNN Loss: 5.607836723327637 | BCE Loss: 1.0747294425964355\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.6588969230651855 | KNN Loss: 5.603983402252197 | BCE Loss: 1.0549135208129883\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.681437015533447 | KNN Loss: 5.619924068450928 | BCE Loss: 1.06151282787323\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.670207977294922 | KNN Loss: 5.608685493469238 | BCE Loss: 1.0615226030349731\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.684136390686035 | KNN Loss: 5.627570629119873 | BCE Loss: 1.0565658807754517\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.707675457000732 | KNN Loss: 5.642884254455566 | BCE Loss: 1.064791202545166\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.695118427276611 | KNN Loss: 5.632323741912842 | BCE Loss: 1.06279456615448\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.651793479919434 | KNN Loss: 5.623702526092529 | BCE Loss: 1.0280907154083252\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.654413223266602 | KNN Loss: 5.607848644256592 | BCE Loss: 1.0465646982192993\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.668869495391846 | KNN Loss: 5.612083911895752 | BCE Loss: 1.0567855834960938\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.664851188659668 | KNN Loss: 5.627078533172607 | BCE Loss: 1.0377728939056396\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 6.674558639526367 | KNN Loss: 5.631710052490234 | BCE Loss: 1.0428485870361328\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.669732093811035 | KNN Loss: 5.599454402923584 | BCE Loss: 1.0702776908874512\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.675395965576172 | KNN Loss: 5.613006591796875 | BCE Loss: 1.0623893737792969\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.673270225524902 | KNN Loss: 5.618331432342529 | BCE Loss: 1.0549386739730835\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 6.729293346405029 | KNN Loss: 5.669144630432129 | BCE Loss: 1.06014883518219\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.7012529373168945 | KNN Loss: 5.659261226654053 | BCE Loss: 1.041991949081421\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.729205131530762 | KNN Loss: 5.684345722198486 | BCE Loss: 1.0448594093322754\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.704381942749023 | KNN Loss: 5.667958736419678 | BCE Loss: 1.0364229679107666\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.656277656555176 | KNN Loss: 5.605295658111572 | BCE Loss: 1.050981879234314\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.696089267730713 | KNN Loss: 5.633272647857666 | BCE Loss: 1.0628166198730469\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.728692054748535 | KNN Loss: 5.6454901695251465 | BCE Loss: 1.0832021236419678\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.728168964385986 | KNN Loss: 5.666370868682861 | BCE Loss: 1.061798095703125\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 6.68106746673584 | KNN Loss: 5.6280598640441895 | BCE Loss: 1.0530078411102295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.724959373474121 | KNN Loss: 5.652915000915527 | BCE Loss: 1.0720441341400146\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 6.6394805908203125 | KNN Loss: 5.609781265258789 | BCE Loss: 1.0296993255615234\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 6.677958011627197 | KNN Loss: 5.6209635734558105 | BCE Loss: 1.0569944381713867\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.652341842651367 | KNN Loss: 5.620996952056885 | BCE Loss: 1.0313451290130615\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.716287136077881 | KNN Loss: 5.656905651092529 | BCE Loss: 1.0593814849853516\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.696322441101074 | KNN Loss: 5.644847393035889 | BCE Loss: 1.0514748096466064\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.734745502471924 | KNN Loss: 5.676151752471924 | BCE Loss: 1.0585936307907104\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 6.672548294067383 | KNN Loss: 5.610565185546875 | BCE Loss: 1.0619832277297974\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.710981845855713 | KNN Loss: 5.686337471008301 | BCE Loss: 1.0246444940567017\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.6852617263793945 | KNN Loss: 5.641851425170898 | BCE Loss: 1.0434101819992065\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.6863908767700195 | KNN Loss: 5.641475677490234 | BCE Loss: 1.044914960861206\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.6776885986328125 | KNN Loss: 5.626165866851807 | BCE Loss: 1.0515227317810059\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.784150123596191 | KNN Loss: 5.72526216506958 | BCE Loss: 1.0588878393173218\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.701909065246582 | KNN Loss: 5.662013530731201 | BCE Loss: 1.0398952960968018\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.669118404388428 | KNN Loss: 5.611682415008545 | BCE Loss: 1.0574361085891724\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.68272590637207 | KNN Loss: 5.641177654266357 | BCE Loss: 1.041548252105713\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 6.677556991577148 | KNN Loss: 5.623177528381348 | BCE Loss: 1.0543792247772217\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 6.669378280639648 | KNN Loss: 5.611545562744141 | BCE Loss: 1.0578327178955078\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.711371898651123 | KNN Loss: 5.667515277862549 | BCE Loss: 1.0438567399978638\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.675527095794678 | KNN Loss: 5.618580341339111 | BCE Loss: 1.0569467544555664\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.676877975463867 | KNN Loss: 5.624005317687988 | BCE Loss: 1.052872657775879\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.712242126464844 | KNN Loss: 5.655902862548828 | BCE Loss: 1.056339144706726\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.698787689208984 | KNN Loss: 5.648959159851074 | BCE Loss: 1.0498287677764893\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.662398338317871 | KNN Loss: 5.611600399017334 | BCE Loss: 1.0507981777191162\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.6641693115234375 | KNN Loss: 5.609944820404053 | BCE Loss: 1.0542242527008057\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.670449256896973 | KNN Loss: 5.604135990142822 | BCE Loss: 1.0663130283355713\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.712827682495117 | KNN Loss: 5.6456427574157715 | BCE Loss: 1.0671848058700562\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.65101432800293 | KNN Loss: 5.607550621032715 | BCE Loss: 1.0434638261795044\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.745957374572754 | KNN Loss: 5.715149402618408 | BCE Loss: 1.0308077335357666\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.676135540008545 | KNN Loss: 5.634721755981445 | BCE Loss: 1.0414137840270996\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.622748851776123 | KNN Loss: 5.6090922355651855 | BCE Loss: 1.0136566162109375\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.682979106903076 | KNN Loss: 5.636194705963135 | BCE Loss: 1.0467842817306519\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.657835483551025 | KNN Loss: 5.624695301055908 | BCE Loss: 1.0331401824951172\n",
      "Epoch   209: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.666247844696045 | KNN Loss: 5.5998077392578125 | BCE Loss: 1.0664401054382324\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.681759357452393 | KNN Loss: 5.612828254699707 | BCE Loss: 1.0689311027526855\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.690977096557617 | KNN Loss: 5.644207954406738 | BCE Loss: 1.0467689037322998\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.681385040283203 | KNN Loss: 5.623678684234619 | BCE Loss: 1.057706356048584\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.662368297576904 | KNN Loss: 5.619278907775879 | BCE Loss: 1.0430892705917358\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.68905782699585 | KNN Loss: 5.662716865539551 | BCE Loss: 1.0263409614562988\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.6995320320129395 | KNN Loss: 5.668652057647705 | BCE Loss: 1.030880093574524\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 6.694449424743652 | KNN Loss: 5.60952091217041 | BCE Loss: 1.0849286317825317\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.787334442138672 | KNN Loss: 5.739892482757568 | BCE Loss: 1.0474421977996826\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.65507173538208 | KNN Loss: 5.6037917137146 | BCE Loss: 1.05128014087677\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.755767822265625 | KNN Loss: 5.712388038635254 | BCE Loss: 1.043379545211792\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.6882243156433105 | KNN Loss: 5.6123809814453125 | BCE Loss: 1.075843334197998\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.720745086669922 | KNN Loss: 5.652288913726807 | BCE Loss: 1.0684559345245361\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.670018196105957 | KNN Loss: 5.620741367340088 | BCE Loss: 1.0492770671844482\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.656151294708252 | KNN Loss: 5.609621047973633 | BCE Loss: 1.0465301275253296\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.662397384643555 | KNN Loss: 5.618376731872559 | BCE Loss: 1.044020652770996\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.6864142417907715 | KNN Loss: 5.658854961395264 | BCE Loss: 1.0275591611862183\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.663393020629883 | KNN Loss: 5.608396053314209 | BCE Loss: 1.0549970865249634\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.73026704788208 | KNN Loss: 5.674263954162598 | BCE Loss: 1.0560029745101929\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.648185729980469 | KNN Loss: 5.603570461273193 | BCE Loss: 1.0446151494979858\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.653476238250732 | KNN Loss: 5.614386558532715 | BCE Loss: 1.0390896797180176\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.664974212646484 | KNN Loss: 5.6134233474731445 | BCE Loss: 1.0515508651733398\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.649647235870361 | KNN Loss: 5.615684509277344 | BCE Loss: 1.0339627265930176\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.671549320220947 | KNN Loss: 5.613121509552002 | BCE Loss: 1.0584278106689453\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.629036903381348 | KNN Loss: 5.600575923919678 | BCE Loss: 1.0284607410430908\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.689802646636963 | KNN Loss: 5.603013038635254 | BCE Loss: 1.086789608001709\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.703495979309082 | KNN Loss: 5.6388349533081055 | BCE Loss: 1.0646611452102661\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.661405086517334 | KNN Loss: 5.607044219970703 | BCE Loss: 1.0543609857559204\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.656843185424805 | KNN Loss: 5.615016937255859 | BCE Loss: 1.0418261289596558\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.763943195343018 | KNN Loss: 5.698476314544678 | BCE Loss: 1.0654670000076294\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 6.656236171722412 | KNN Loss: 5.611557483673096 | BCE Loss: 1.0446786880493164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.7908759117126465 | KNN Loss: 5.709783554077148 | BCE Loss: 1.081092357635498\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.6126627922058105 | KNN Loss: 5.599368095397949 | BCE Loss: 1.0132948160171509\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.672238826751709 | KNN Loss: 5.605334281921387 | BCE Loss: 1.0669045448303223\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.667666435241699 | KNN Loss: 5.623753070831299 | BCE Loss: 1.0439136028289795\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.726398944854736 | KNN Loss: 5.660894870758057 | BCE Loss: 1.0655040740966797\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.646176338195801 | KNN Loss: 5.617110252380371 | BCE Loss: 1.0290660858154297\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.659642219543457 | KNN Loss: 5.6121416091918945 | BCE Loss: 1.0475003719329834\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.707139015197754 | KNN Loss: 5.6739630699157715 | BCE Loss: 1.0331757068634033\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.653419494628906 | KNN Loss: 5.600222587585449 | BCE Loss: 1.053196668624878\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.65489387512207 | KNN Loss: 5.606074810028076 | BCE Loss: 1.0488190650939941\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.732601165771484 | KNN Loss: 5.675642967224121 | BCE Loss: 1.0569583177566528\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.640399932861328 | KNN Loss: 5.611195087432861 | BCE Loss: 1.0292046070098877\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.635918617248535 | KNN Loss: 5.59623384475708 | BCE Loss: 1.039684534072876\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.722663879394531 | KNN Loss: 5.6485795974731445 | BCE Loss: 1.0740840435028076\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.684569358825684 | KNN Loss: 5.634458541870117 | BCE Loss: 1.0501108169555664\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.673151016235352 | KNN Loss: 5.617330074310303 | BCE Loss: 1.0558209419250488\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.652926445007324 | KNN Loss: 5.6061811447143555 | BCE Loss: 1.0467454195022583\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 6.645111560821533 | KNN Loss: 5.608321666717529 | BCE Loss: 1.036789894104004\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.65855598449707 | KNN Loss: 5.615718841552734 | BCE Loss: 1.042837381362915\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.691473007202148 | KNN Loss: 5.6402482986450195 | BCE Loss: 1.0512244701385498\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.677414894104004 | KNN Loss: 5.647720813751221 | BCE Loss: 1.0296943187713623\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.683530330657959 | KNN Loss: 5.6107025146484375 | BCE Loss: 1.0728278160095215\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.662531852722168 | KNN Loss: 5.60905647277832 | BCE Loss: 1.0534756183624268\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.714785575866699 | KNN Loss: 5.659022331237793 | BCE Loss: 1.0557630062103271\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.646951675415039 | KNN Loss: 5.613919734954834 | BCE Loss: 1.033031940460205\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.710137367248535 | KNN Loss: 5.654152870178223 | BCE Loss: 1.0559842586517334\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.643730163574219 | KNN Loss: 5.596669673919678 | BCE Loss: 1.0470606088638306\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.643467426300049 | KNN Loss: 5.60418176651001 | BCE Loss: 1.039285659790039\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.671810150146484 | KNN Loss: 5.615121841430664 | BCE Loss: 1.0566885471343994\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.674698829650879 | KNN Loss: 5.614053249359131 | BCE Loss: 1.060645341873169\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.672677516937256 | KNN Loss: 5.600677013397217 | BCE Loss: 1.072000503540039\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.738303184509277 | KNN Loss: 5.680273532867432 | BCE Loss: 1.0580298900604248\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.685799598693848 | KNN Loss: 5.623892307281494 | BCE Loss: 1.0619072914123535\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.683555603027344 | KNN Loss: 5.632252216339111 | BCE Loss: 1.0513036251068115\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.652597904205322 | KNN Loss: 5.615911960601807 | BCE Loss: 1.0366859436035156\n",
      "Epoch   220: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.684633255004883 | KNN Loss: 5.632129669189453 | BCE Loss: 1.0525035858154297\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 6.654924392700195 | KNN Loss: 5.6286091804504395 | BCE Loss: 1.0263152122497559\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.649663925170898 | KNN Loss: 5.612643718719482 | BCE Loss: 1.037019968032837\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.763750076293945 | KNN Loss: 5.733124256134033 | BCE Loss: 1.030625820159912\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.664327621459961 | KNN Loss: 5.6000566482543945 | BCE Loss: 1.0642707347869873\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.742739200592041 | KNN Loss: 5.664835453033447 | BCE Loss: 1.0779038667678833\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.669683456420898 | KNN Loss: 5.642170429229736 | BCE Loss: 1.027512788772583\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.672133445739746 | KNN Loss: 5.626399040222168 | BCE Loss: 1.0457344055175781\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.646336555480957 | KNN Loss: 5.608220100402832 | BCE Loss: 1.038116455078125\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 6.678484916687012 | KNN Loss: 5.640320301055908 | BCE Loss: 1.0381646156311035\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.6375579833984375 | KNN Loss: 5.60983943939209 | BCE Loss: 1.0277186632156372\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.785140037536621 | KNN Loss: 5.696882247924805 | BCE Loss: 1.0882580280303955\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.739088535308838 | KNN Loss: 5.645730972290039 | BCE Loss: 1.0933576822280884\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.729996681213379 | KNN Loss: 5.674254894256592 | BCE Loss: 1.0557420253753662\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.629717826843262 | KNN Loss: 5.6132426261901855 | BCE Loss: 1.0164750814437866\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.666415691375732 | KNN Loss: 5.60678243637085 | BCE Loss: 1.0596331357955933\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.636294364929199 | KNN Loss: 5.6016340255737305 | BCE Loss: 1.0346601009368896\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.746986389160156 | KNN Loss: 5.70086145401001 | BCE Loss: 1.046124815940857\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.7716898918151855 | KNN Loss: 5.715086460113525 | BCE Loss: 1.0566034317016602\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.658049583435059 | KNN Loss: 5.620102882385254 | BCE Loss: 1.0379468202590942\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.655311584472656 | KNN Loss: 5.607895374298096 | BCE Loss: 1.04741632938385\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 6.689939498901367 | KNN Loss: 5.657891273498535 | BCE Loss: 1.032047986984253\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.683466911315918 | KNN Loss: 5.637362003326416 | BCE Loss: 1.046104907989502\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 6.65278959274292 | KNN Loss: 5.6247053146362305 | BCE Loss: 1.0280842781066895\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.736471176147461 | KNN Loss: 5.67630672454834 | BCE Loss: 1.0601646900177002\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.71345853805542 | KNN Loss: 5.641768455505371 | BCE Loss: 1.0716902017593384\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.741440773010254 | KNN Loss: 5.660091876983643 | BCE Loss: 1.0813486576080322\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.760123252868652 | KNN Loss: 5.701577663421631 | BCE Loss: 1.0585458278656006\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.673088073730469 | KNN Loss: 5.626608371734619 | BCE Loss: 1.04647958278656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.663578987121582 | KNN Loss: 5.6256513595581055 | BCE Loss: 1.0379278659820557\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.642977714538574 | KNN Loss: 5.610443115234375 | BCE Loss: 1.0325344800949097\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.660835266113281 | KNN Loss: 5.607780456542969 | BCE Loss: 1.053054928779602\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.733063220977783 | KNN Loss: 5.67727518081665 | BCE Loss: 1.0557880401611328\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.730774879455566 | KNN Loss: 5.68580436706543 | BCE Loss: 1.0449705123901367\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.6830949783325195 | KNN Loss: 5.656432151794434 | BCE Loss: 1.026663064956665\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.7849836349487305 | KNN Loss: 5.719709396362305 | BCE Loss: 1.0652741193771362\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.803089141845703 | KNN Loss: 5.719107151031494 | BCE Loss: 1.083982229232788\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 6.655023574829102 | KNN Loss: 5.611629486083984 | BCE Loss: 1.0433942079544067\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 6.649672985076904 | KNN Loss: 5.607079029083252 | BCE Loss: 1.0425939559936523\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.674358367919922 | KNN Loss: 5.627040386199951 | BCE Loss: 1.0473182201385498\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.68320894241333 | KNN Loss: 5.651262283325195 | BCE Loss: 1.0319465398788452\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 6.705159664154053 | KNN Loss: 5.615920066833496 | BCE Loss: 1.0892395973205566\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.674119472503662 | KNN Loss: 5.630502223968506 | BCE Loss: 1.0436172485351562\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.721407890319824 | KNN Loss: 5.6535725593566895 | BCE Loss: 1.0678353309631348\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.623970031738281 | KNN Loss: 5.605138301849365 | BCE Loss: 1.018831491470337\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.759756565093994 | KNN Loss: 5.677807331085205 | BCE Loss: 1.0819491147994995\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.642009735107422 | KNN Loss: 5.605000019073486 | BCE Loss: 1.037009596824646\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.6600189208984375 | KNN Loss: 5.614616870880127 | BCE Loss: 1.0454020500183105\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.766380310058594 | KNN Loss: 5.695085048675537 | BCE Loss: 1.071295142173767\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.701498985290527 | KNN Loss: 5.642843246459961 | BCE Loss: 1.0586559772491455\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 6.698376178741455 | KNN Loss: 5.650893688201904 | BCE Loss: 1.0474823713302612\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.648550987243652 | KNN Loss: 5.614678859710693 | BCE Loss: 1.0338722467422485\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.674103260040283 | KNN Loss: 5.6048407554626465 | BCE Loss: 1.0692625045776367\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.705921649932861 | KNN Loss: 5.636874198913574 | BCE Loss: 1.0690475702285767\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.676578521728516 | KNN Loss: 5.654176712036133 | BCE Loss: 1.022402048110962\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.745250701904297 | KNN Loss: 5.691256523132324 | BCE Loss: 1.053994059562683\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.693281173706055 | KNN Loss: 5.639202117919922 | BCE Loss: 1.054079294204712\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 6.755213737487793 | KNN Loss: 5.696690082550049 | BCE Loss: 1.0585235357284546\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.681394100189209 | KNN Loss: 5.62948751449585 | BCE Loss: 1.0519065856933594\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.690485954284668 | KNN Loss: 5.615633964538574 | BCE Loss: 1.0748517513275146\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.693489074707031 | KNN Loss: 5.627257823944092 | BCE Loss: 1.0662310123443604\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.694196701049805 | KNN Loss: 5.657024383544922 | BCE Loss: 1.0371720790863037\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.691296577453613 | KNN Loss: 5.626673698425293 | BCE Loss: 1.0646231174468994\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.65029764175415 | KNN Loss: 5.617142200469971 | BCE Loss: 1.0331554412841797\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.677733421325684 | KNN Loss: 5.6296563148498535 | BCE Loss: 1.0480773448944092\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.646327018737793 | KNN Loss: 5.604316711425781 | BCE Loss: 1.0420100688934326\n",
      "Epoch   231: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.669291019439697 | KNN Loss: 5.651931285858154 | BCE Loss: 1.0173598527908325\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.763921737670898 | KNN Loss: 5.729154586791992 | BCE Loss: 1.0347671508789062\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.6407952308654785 | KNN Loss: 5.604047775268555 | BCE Loss: 1.0367474555969238\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.686360836029053 | KNN Loss: 5.639222621917725 | BCE Loss: 1.0471382141113281\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.71658992767334 | KNN Loss: 5.650835990905762 | BCE Loss: 1.0657539367675781\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 6.6847100257873535 | KNN Loss: 5.644486427307129 | BCE Loss: 1.040223479270935\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.713123798370361 | KNN Loss: 5.6650567054748535 | BCE Loss: 1.0480670928955078\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.648682117462158 | KNN Loss: 5.6165361404418945 | BCE Loss: 1.0321458578109741\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.703438758850098 | KNN Loss: 5.650589942932129 | BCE Loss: 1.0528488159179688\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.679932594299316 | KNN Loss: 5.648857593536377 | BCE Loss: 1.0310750007629395\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.6892805099487305 | KNN Loss: 5.639164447784424 | BCE Loss: 1.0501158237457275\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.686307430267334 | KNN Loss: 5.616065502166748 | BCE Loss: 1.070241928100586\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.656280994415283 | KNN Loss: 5.619325160980225 | BCE Loss: 1.0369558334350586\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.669044494628906 | KNN Loss: 5.6163482666015625 | BCE Loss: 1.0526962280273438\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.708244323730469 | KNN Loss: 5.65724515914917 | BCE Loss: 1.0509992837905884\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.679146766662598 | KNN Loss: 5.621738433837891 | BCE Loss: 1.0574085712432861\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.640677452087402 | KNN Loss: 5.625837802886963 | BCE Loss: 1.0148396492004395\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.668067932128906 | KNN Loss: 5.623205184936523 | BCE Loss: 1.0448625087738037\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.659569263458252 | KNN Loss: 5.621454238891602 | BCE Loss: 1.0381149053573608\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.7364068031311035 | KNN Loss: 5.684177398681641 | BCE Loss: 1.0522292852401733\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.6577534675598145 | KNN Loss: 5.6204118728637695 | BCE Loss: 1.0373414754867554\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.669033050537109 | KNN Loss: 5.615335941314697 | BCE Loss: 1.053696870803833\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.693655967712402 | KNN Loss: 5.637458324432373 | BCE Loss: 1.0561976432800293\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.708528995513916 | KNN Loss: 5.644207954406738 | BCE Loss: 1.0643211603164673\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.803090572357178 | KNN Loss: 5.715517997741699 | BCE Loss: 1.087572693824768\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.641806602478027 | KNN Loss: 5.601425647735596 | BCE Loss: 1.0403809547424316\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.7325897216796875 | KNN Loss: 5.682730674743652 | BCE Loss: 1.049858808517456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.691985607147217 | KNN Loss: 5.617237567901611 | BCE Loss: 1.074748158454895\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.690059661865234 | KNN Loss: 5.649933815002441 | BCE Loss: 1.0401256084442139\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.705994606018066 | KNN Loss: 5.646218776702881 | BCE Loss: 1.0597758293151855\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.616951942443848 | KNN Loss: 5.601201057434082 | BCE Loss: 1.0157508850097656\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.677674293518066 | KNN Loss: 5.629677772521973 | BCE Loss: 1.0479965209960938\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.647692680358887 | KNN Loss: 5.608232021331787 | BCE Loss: 1.0394604206085205\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.667832851409912 | KNN Loss: 5.644240379333496 | BCE Loss: 1.023592472076416\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.713674545288086 | KNN Loss: 5.65777063369751 | BCE Loss: 1.0559041500091553\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 6.697310924530029 | KNN Loss: 5.614246368408203 | BCE Loss: 1.0830645561218262\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.670083045959473 | KNN Loss: 5.625143051147461 | BCE Loss: 1.0449402332305908\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 6.658058166503906 | KNN Loss: 5.632103443145752 | BCE Loss: 1.0259547233581543\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.729398727416992 | KNN Loss: 5.654288291931152 | BCE Loss: 1.075110673904419\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.719344139099121 | KNN Loss: 5.691874980926514 | BCE Loss: 1.0274689197540283\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.680932521820068 | KNN Loss: 5.619424819946289 | BCE Loss: 1.0615077018737793\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.62185001373291 | KNN Loss: 5.607999324798584 | BCE Loss: 1.0138506889343262\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.646514892578125 | KNN Loss: 5.607660293579102 | BCE Loss: 1.0388545989990234\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.697524070739746 | KNN Loss: 5.610451698303223 | BCE Loss: 1.0870723724365234\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.683548450469971 | KNN Loss: 5.621037483215332 | BCE Loss: 1.0625109672546387\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.72772216796875 | KNN Loss: 5.691164493560791 | BCE Loss: 1.036557674407959\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.663244247436523 | KNN Loss: 5.619279861450195 | BCE Loss: 1.0439646244049072\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.739180564880371 | KNN Loss: 5.6948676109313965 | BCE Loss: 1.0443129539489746\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.6616973876953125 | KNN Loss: 5.612183570861816 | BCE Loss: 1.0495139360427856\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.706969261169434 | KNN Loss: 5.659764289855957 | BCE Loss: 1.0472052097320557\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.6907525062561035 | KNN Loss: 5.635519027709961 | BCE Loss: 1.055233359336853\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.699300289154053 | KNN Loss: 5.664703845977783 | BCE Loss: 1.0345964431762695\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.688662528991699 | KNN Loss: 5.613308906555176 | BCE Loss: 1.075353741645813\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.668312072753906 | KNN Loss: 5.621052265167236 | BCE Loss: 1.0472596883773804\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.690595626831055 | KNN Loss: 5.644999980926514 | BCE Loss: 1.045595645904541\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.665841102600098 | KNN Loss: 5.64547872543335 | BCE Loss: 1.020362138748169\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.674480438232422 | KNN Loss: 5.605852127075195 | BCE Loss: 1.0686280727386475\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.658568382263184 | KNN Loss: 5.602290153503418 | BCE Loss: 1.0562784671783447\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.663778305053711 | KNN Loss: 5.6004509925842285 | BCE Loss: 1.063327431678772\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.747174263000488 | KNN Loss: 5.702254295349121 | BCE Loss: 1.044919729232788\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.657511234283447 | KNN Loss: 5.6086859703063965 | BCE Loss: 1.0488252639770508\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 6.66608190536499 | KNN Loss: 5.627138137817383 | BCE Loss: 1.0389437675476074\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.745270252227783 | KNN Loss: 5.691159248352051 | BCE Loss: 1.0541110038757324\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.678832054138184 | KNN Loss: 5.637123107910156 | BCE Loss: 1.0417089462280273\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.675710678100586 | KNN Loss: 5.6575140953063965 | BCE Loss: 1.0181968212127686\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.691982269287109 | KNN Loss: 5.655707359313965 | BCE Loss: 1.0362751483917236\n",
      "Epoch   242: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.65977144241333 | KNN Loss: 5.617701053619385 | BCE Loss: 1.0420705080032349\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.657330513000488 | KNN Loss: 5.597220420837402 | BCE Loss: 1.060110330581665\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.713204860687256 | KNN Loss: 5.668521404266357 | BCE Loss: 1.0446834564208984\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.67502498626709 | KNN Loss: 5.637145042419434 | BCE Loss: 1.0378797054290771\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.653725624084473 | KNN Loss: 5.60912561416626 | BCE Loss: 1.044600009918213\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.6531805992126465 | KNN Loss: 5.618563652038574 | BCE Loss: 1.0346170663833618\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.6590070724487305 | KNN Loss: 5.604519367218018 | BCE Loss: 1.054487705230713\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.6208882331848145 | KNN Loss: 5.612067699432373 | BCE Loss: 1.0088204145431519\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.631314277648926 | KNN Loss: 5.601292610168457 | BCE Loss: 1.0300219058990479\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.649545192718506 | KNN Loss: 5.60105037689209 | BCE Loss: 1.048494815826416\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.648684978485107 | KNN Loss: 5.60771369934082 | BCE Loss: 1.040971279144287\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.664780616760254 | KNN Loss: 5.6188201904296875 | BCE Loss: 1.0459604263305664\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.7378950119018555 | KNN Loss: 5.673534393310547 | BCE Loss: 1.0643608570098877\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.683845043182373 | KNN Loss: 5.659964561462402 | BCE Loss: 1.0238803625106812\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.653704643249512 | KNN Loss: 5.607592582702637 | BCE Loss: 1.046111822128296\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.653286457061768 | KNN Loss: 5.6144561767578125 | BCE Loss: 1.0388303995132446\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.699331283569336 | KNN Loss: 5.664247989654541 | BCE Loss: 1.035083293914795\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.683850288391113 | KNN Loss: 5.647094249725342 | BCE Loss: 1.0367560386657715\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.6527018547058105 | KNN Loss: 5.601243019104004 | BCE Loss: 1.0514589548110962\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.7004075050354 | KNN Loss: 5.641667366027832 | BCE Loss: 1.0587401390075684\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.708585739135742 | KNN Loss: 5.645711421966553 | BCE Loss: 1.0628745555877686\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.733819484710693 | KNN Loss: 5.664797306060791 | BCE Loss: 1.0690220594406128\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.735452175140381 | KNN Loss: 5.7028632164001465 | BCE Loss: 1.0325888395309448\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.681715488433838 | KNN Loss: 5.657328128814697 | BCE Loss: 1.0243874788284302\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.73701286315918 | KNN Loss: 5.662070274353027 | BCE Loss: 1.0749425888061523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.6944146156311035 | KNN Loss: 5.655323028564453 | BCE Loss: 1.0390915870666504\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.670848846435547 | KNN Loss: 5.634727478027344 | BCE Loss: 1.0361213684082031\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.644354820251465 | KNN Loss: 5.612605094909668 | BCE Loss: 1.0317498445510864\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.702712535858154 | KNN Loss: 5.651481628417969 | BCE Loss: 1.0512309074401855\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.640658855438232 | KNN Loss: 5.616275310516357 | BCE Loss: 1.0243836641311646\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.682989597320557 | KNN Loss: 5.650564193725586 | BCE Loss: 1.0324254035949707\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.63939905166626 | KNN Loss: 5.599682807922363 | BCE Loss: 1.0397162437438965\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 6.716461658477783 | KNN Loss: 5.66948127746582 | BCE Loss: 1.0469802618026733\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.7022705078125 | KNN Loss: 5.676478862762451 | BCE Loss: 1.0257914066314697\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.651662349700928 | KNN Loss: 5.623216152191162 | BCE Loss: 1.028446078300476\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.67465877532959 | KNN Loss: 5.657365322113037 | BCE Loss: 1.0172936916351318\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.646392822265625 | KNN Loss: 5.628624439239502 | BCE Loss: 1.017768144607544\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.838186264038086 | KNN Loss: 5.771137714385986 | BCE Loss: 1.06704843044281\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.654915809631348 | KNN Loss: 5.604008197784424 | BCE Loss: 1.0509076118469238\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.721071243286133 | KNN Loss: 5.663863658905029 | BCE Loss: 1.0572075843811035\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.665058135986328 | KNN Loss: 5.6146769523620605 | BCE Loss: 1.0503809452056885\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.733484745025635 | KNN Loss: 5.676381587982178 | BCE Loss: 1.057103157043457\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.661685943603516 | KNN Loss: 5.618223667144775 | BCE Loss: 1.0434622764587402\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.658192157745361 | KNN Loss: 5.6137847900390625 | BCE Loss: 1.0444073677062988\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.690426826477051 | KNN Loss: 5.627387523651123 | BCE Loss: 1.0630390644073486\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.674737930297852 | KNN Loss: 5.647887706756592 | BCE Loss: 1.0268501043319702\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.6505446434021 | KNN Loss: 5.597667694091797 | BCE Loss: 1.0528768301010132\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.716795921325684 | KNN Loss: 5.6516900062561035 | BCE Loss: 1.0651060342788696\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.6742634773254395 | KNN Loss: 5.633505821228027 | BCE Loss: 1.040757656097412\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.7322998046875 | KNN Loss: 5.66664981842041 | BCE Loss: 1.065650224685669\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.641105651855469 | KNN Loss: 5.601896286010742 | BCE Loss: 1.0392093658447266\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.744363784790039 | KNN Loss: 5.691951274871826 | BCE Loss: 1.0524123907089233\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.6795244216918945 | KNN Loss: 5.644587993621826 | BCE Loss: 1.0349363088607788\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.676387786865234 | KNN Loss: 5.618303298950195 | BCE Loss: 1.058084487915039\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.681177139282227 | KNN Loss: 5.658580303192139 | BCE Loss: 1.022596836090088\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.752205848693848 | KNN Loss: 5.661072731018066 | BCE Loss: 1.0911331176757812\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.6909074783325195 | KNN Loss: 5.6326494216918945 | BCE Loss: 1.0582579374313354\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.701550483703613 | KNN Loss: 5.6477837562561035 | BCE Loss: 1.0537667274475098\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.664509296417236 | KNN Loss: 5.610466480255127 | BCE Loss: 1.054042935371399\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.655606269836426 | KNN Loss: 5.62014627456665 | BCE Loss: 1.0354597568511963\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.7207183837890625 | KNN Loss: 5.684139728546143 | BCE Loss: 1.0365784168243408\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.685174465179443 | KNN Loss: 5.623668193817139 | BCE Loss: 1.0615063905715942\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.722123622894287 | KNN Loss: 5.640411376953125 | BCE Loss: 1.0817123651504517\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.662376880645752 | KNN Loss: 5.631341934204102 | BCE Loss: 1.0310349464416504\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.74562406539917 | KNN Loss: 5.67669677734375 | BCE Loss: 1.06892728805542\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 6.748227119445801 | KNN Loss: 5.682677268981934 | BCE Loss: 1.0655500888824463\n",
      "Epoch   253: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.688276767730713 | KNN Loss: 5.630441665649414 | BCE Loss: 1.0578351020812988\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.698762893676758 | KNN Loss: 5.635664463043213 | BCE Loss: 1.063098430633545\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.686789512634277 | KNN Loss: 5.643760681152344 | BCE Loss: 1.0430288314819336\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.661900997161865 | KNN Loss: 5.627834320068359 | BCE Loss: 1.0340667963027954\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.715706825256348 | KNN Loss: 5.6522536277771 | BCE Loss: 1.0634533166885376\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.693155288696289 | KNN Loss: 5.66671895980835 | BCE Loss: 1.0264363288879395\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.698495864868164 | KNN Loss: 5.654264450073242 | BCE Loss: 1.044231653213501\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.7477827072143555 | KNN Loss: 5.685755252838135 | BCE Loss: 1.0620274543762207\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.690834045410156 | KNN Loss: 5.612135410308838 | BCE Loss: 1.0786983966827393\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.660410404205322 | KNN Loss: 5.610201835632324 | BCE Loss: 1.050208568572998\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.703200817108154 | KNN Loss: 5.645472049713135 | BCE Loss: 1.05772864818573\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.662693023681641 | KNN Loss: 5.610722064971924 | BCE Loss: 1.0519708395004272\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.666996002197266 | KNN Loss: 5.615955829620361 | BCE Loss: 1.0510402917861938\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.689197063446045 | KNN Loss: 5.610052585601807 | BCE Loss: 1.0791445970535278\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.731128692626953 | KNN Loss: 5.673604488372803 | BCE Loss: 1.0575244426727295\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.659297466278076 | KNN Loss: 5.607688903808594 | BCE Loss: 1.051608681678772\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.648324966430664 | KNN Loss: 5.615417957305908 | BCE Loss: 1.0329070091247559\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.69536018371582 | KNN Loss: 5.6457977294921875 | BCE Loss: 1.0495622158050537\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.6667962074279785 | KNN Loss: 5.638934135437012 | BCE Loss: 1.0278620719909668\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.665814399719238 | KNN Loss: 5.635860443115234 | BCE Loss: 1.0299540758132935\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.762240409851074 | KNN Loss: 5.706622123718262 | BCE Loss: 1.0556185245513916\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.719525337219238 | KNN Loss: 5.669511318206787 | BCE Loss: 1.0500140190124512\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.679826736450195 | KNN Loss: 5.623791694641113 | BCE Loss: 1.0560352802276611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.657812118530273 | KNN Loss: 5.602855205535889 | BCE Loss: 1.0549571514129639\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.6382341384887695 | KNN Loss: 5.605401039123535 | BCE Loss: 1.032833218574524\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.656836032867432 | KNN Loss: 5.612751007080078 | BCE Loss: 1.0440850257873535\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.648529052734375 | KNN Loss: 5.615478515625 | BCE Loss: 1.033050298690796\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.6168999671936035 | KNN Loss: 5.607003211975098 | BCE Loss: 1.0098967552185059\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.6676435470581055 | KNN Loss: 5.622049808502197 | BCE Loss: 1.0455937385559082\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.68458890914917 | KNN Loss: 5.627805233001709 | BCE Loss: 1.0567837953567505\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.723412036895752 | KNN Loss: 5.6740498542785645 | BCE Loss: 1.0493621826171875\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.686829566955566 | KNN Loss: 5.630751132965088 | BCE Loss: 1.0560781955718994\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.693187236785889 | KNN Loss: 5.620419979095459 | BCE Loss: 1.0727671384811401\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.693118095397949 | KNN Loss: 5.632319927215576 | BCE Loss: 1.060797929763794\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.652586936950684 | KNN Loss: 5.613112926483154 | BCE Loss: 1.0394742488861084\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.736739635467529 | KNN Loss: 5.669625282287598 | BCE Loss: 1.0671144723892212\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.648250102996826 | KNN Loss: 5.607680797576904 | BCE Loss: 1.0405691862106323\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.650982856750488 | KNN Loss: 5.60617208480835 | BCE Loss: 1.0448107719421387\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.7080888748168945 | KNN Loss: 5.63174295425415 | BCE Loss: 1.0763459205627441\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.700461387634277 | KNN Loss: 5.643754959106445 | BCE Loss: 1.0567065477371216\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.659233093261719 | KNN Loss: 5.626316547393799 | BCE Loss: 1.03291654586792\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.700850009918213 | KNN Loss: 5.656506061553955 | BCE Loss: 1.0443438291549683\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.708646774291992 | KNN Loss: 5.653266429901123 | BCE Loss: 1.05538010597229\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.707382678985596 | KNN Loss: 5.665902137756348 | BCE Loss: 1.0414806604385376\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.647828102111816 | KNN Loss: 5.614135265350342 | BCE Loss: 1.0336930751800537\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.722567081451416 | KNN Loss: 5.661289691925049 | BCE Loss: 1.0612773895263672\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.7836432456970215 | KNN Loss: 5.752315998077393 | BCE Loss: 1.0313273668289185\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.698686599731445 | KNN Loss: 5.662384510040283 | BCE Loss: 1.0363019704818726\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.693652153015137 | KNN Loss: 5.642935752868652 | BCE Loss: 1.0507166385650635\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.705546855926514 | KNN Loss: 5.677343845367432 | BCE Loss: 1.0282031297683716\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.637106895446777 | KNN Loss: 5.6046953201293945 | BCE Loss: 1.032411813735962\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.67091178894043 | KNN Loss: 5.6221089363098145 | BCE Loss: 1.0488030910491943\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.717949867248535 | KNN Loss: 5.632472038269043 | BCE Loss: 1.0854780673980713\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.673797130584717 | KNN Loss: 5.638405799865723 | BCE Loss: 1.0353914499282837\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.654224872589111 | KNN Loss: 5.627427101135254 | BCE Loss: 1.0267976522445679\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.695926189422607 | KNN Loss: 5.657081604003906 | BCE Loss: 1.0388447046279907\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.6874847412109375 | KNN Loss: 5.6364569664001465 | BCE Loss: 1.051027536392212\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.637991428375244 | KNN Loss: 5.603174209594727 | BCE Loss: 1.0348172187805176\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.7073974609375 | KNN Loss: 5.647416591644287 | BCE Loss: 1.0599806308746338\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.739414215087891 | KNN Loss: 5.677545547485352 | BCE Loss: 1.06186842918396\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.753573417663574 | KNN Loss: 5.689825057983398 | BCE Loss: 1.0637485980987549\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.767400741577148 | KNN Loss: 5.737146377563477 | BCE Loss: 1.0302543640136719\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.626253604888916 | KNN Loss: 5.604342460632324 | BCE Loss: 1.0219111442565918\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.660781383514404 | KNN Loss: 5.629404067993164 | BCE Loss: 1.0313773155212402\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.7800703048706055 | KNN Loss: 5.6927947998046875 | BCE Loss: 1.0872752666473389\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.67306661605835 | KNN Loss: 5.616792678833008 | BCE Loss: 1.0562738180160522\n",
      "Epoch   264: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.650362491607666 | KNN Loss: 5.612628936767578 | BCE Loss: 1.037733554840088\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.636427402496338 | KNN Loss: 5.601786136627197 | BCE Loss: 1.0346413850784302\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.675278186798096 | KNN Loss: 5.620673179626465 | BCE Loss: 1.0546051263809204\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.710417747497559 | KNN Loss: 5.659213066101074 | BCE Loss: 1.0512046813964844\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.664661407470703 | KNN Loss: 5.611390113830566 | BCE Loss: 1.0532715320587158\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.65987491607666 | KNN Loss: 5.621950149536133 | BCE Loss: 1.0379245281219482\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.63638973236084 | KNN Loss: 5.599332332611084 | BCE Loss: 1.0370571613311768\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.638156890869141 | KNN Loss: 5.609807968139648 | BCE Loss: 1.0283489227294922\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.642452716827393 | KNN Loss: 5.620087623596191 | BCE Loss: 1.0223649740219116\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.69147253036499 | KNN Loss: 5.640324592590332 | BCE Loss: 1.0511478185653687\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.663263320922852 | KNN Loss: 5.610472202301025 | BCE Loss: 1.0527911186218262\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.682013511657715 | KNN Loss: 5.627084255218506 | BCE Loss: 1.054929256439209\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.704514026641846 | KNN Loss: 5.650814533233643 | BCE Loss: 1.0536996126174927\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.683047294616699 | KNN Loss: 5.6271867752075195 | BCE Loss: 1.0558604001998901\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.6899261474609375 | KNN Loss: 5.645300388336182 | BCE Loss: 1.0446257591247559\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.686567783355713 | KNN Loss: 5.640562534332275 | BCE Loss: 1.0460052490234375\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.679568767547607 | KNN Loss: 5.618392467498779 | BCE Loss: 1.0611761808395386\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.674709796905518 | KNN Loss: 5.606685161590576 | BCE Loss: 1.0680245161056519\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.694132328033447 | KNN Loss: 5.632811069488525 | BCE Loss: 1.0613213777542114\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.656163692474365 | KNN Loss: 5.606617450714111 | BCE Loss: 1.0495461225509644\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.661620616912842 | KNN Loss: 5.641704559326172 | BCE Loss: 1.01991605758667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.672837257385254 | KNN Loss: 5.607428073883057 | BCE Loss: 1.0654094219207764\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.648698806762695 | KNN Loss: 5.599656105041504 | BCE Loss: 1.0490429401397705\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.69840145111084 | KNN Loss: 5.632015705108643 | BCE Loss: 1.0663855075836182\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 6.664953231811523 | KNN Loss: 5.635253429412842 | BCE Loss: 1.0297000408172607\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.713943958282471 | KNN Loss: 5.678980827331543 | BCE Loss: 1.0349632501602173\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.682404041290283 | KNN Loss: 5.6667962074279785 | BCE Loss: 1.0156078338623047\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.651074409484863 | KNN Loss: 5.612020969390869 | BCE Loss: 1.039053201675415\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.6873297691345215 | KNN Loss: 5.620919227600098 | BCE Loss: 1.0664106607437134\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.7130608558654785 | KNN Loss: 5.602932929992676 | BCE Loss: 1.1101278066635132\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 6.670529365539551 | KNN Loss: 5.621315002441406 | BCE Loss: 1.0492146015167236\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.650387763977051 | KNN Loss: 5.612532615661621 | BCE Loss: 1.0378553867340088\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 6.777190685272217 | KNN Loss: 5.713569641113281 | BCE Loss: 1.0636210441589355\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.738324165344238 | KNN Loss: 5.6976752281188965 | BCE Loss: 1.0406489372253418\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.6824846267700195 | KNN Loss: 5.606228828430176 | BCE Loss: 1.0762555599212646\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 6.722170829772949 | KNN Loss: 5.656024932861328 | BCE Loss: 1.066145896911621\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.728877544403076 | KNN Loss: 5.6612019538879395 | BCE Loss: 1.0676754713058472\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.681098937988281 | KNN Loss: 5.645486354827881 | BCE Loss: 1.0356125831604004\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.687503814697266 | KNN Loss: 5.626222133636475 | BCE Loss: 1.0612819194793701\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.684057712554932 | KNN Loss: 5.623929500579834 | BCE Loss: 1.0601283311843872\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 6.659502983093262 | KNN Loss: 5.619157791137695 | BCE Loss: 1.0403451919555664\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.665926933288574 | KNN Loss: 5.618370532989502 | BCE Loss: 1.0475566387176514\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.6633501052856445 | KNN Loss: 5.602130889892578 | BCE Loss: 1.0612189769744873\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.662734508514404 | KNN Loss: 5.6136298179626465 | BCE Loss: 1.0491046905517578\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.640609264373779 | KNN Loss: 5.599992275238037 | BCE Loss: 1.0406169891357422\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.6687116622924805 | KNN Loss: 5.615573406219482 | BCE Loss: 1.053138256072998\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.683131694793701 | KNN Loss: 5.651398658752441 | BCE Loss: 1.0317329168319702\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.634232044219971 | KNN Loss: 5.6095452308654785 | BCE Loss: 1.0246868133544922\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 6.679821968078613 | KNN Loss: 5.633599758148193 | BCE Loss: 1.046222448348999\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.7343597412109375 | KNN Loss: 5.664458274841309 | BCE Loss: 1.0699013471603394\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.692066669464111 | KNN Loss: 5.641742706298828 | BCE Loss: 1.0503240823745728\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.654815673828125 | KNN Loss: 5.609874248504639 | BCE Loss: 1.0449411869049072\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.711482524871826 | KNN Loss: 5.647563934326172 | BCE Loss: 1.0639185905456543\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.665807247161865 | KNN Loss: 5.605881690979004 | BCE Loss: 1.0599255561828613\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.731895446777344 | KNN Loss: 5.688246250152588 | BCE Loss: 1.043649435043335\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.6418938636779785 | KNN Loss: 5.60046911239624 | BCE Loss: 1.0414248704910278\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.647801876068115 | KNN Loss: 5.619784832000732 | BCE Loss: 1.0280171632766724\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 6.794287204742432 | KNN Loss: 5.718794345855713 | BCE Loss: 1.0754928588867188\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 6.721977233886719 | KNN Loss: 5.66589879989624 | BCE Loss: 1.056078553199768\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.663665771484375 | KNN Loss: 5.602994918823242 | BCE Loss: 1.0606709718704224\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 6.680960655212402 | KNN Loss: 5.617475509643555 | BCE Loss: 1.0634853839874268\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.6924943923950195 | KNN Loss: 5.6597490310668945 | BCE Loss: 1.0327454805374146\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.647919654846191 | KNN Loss: 5.604442119598389 | BCE Loss: 1.0434772968292236\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.637105941772461 | KNN Loss: 5.612741470336914 | BCE Loss: 1.0243642330169678\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.775489807128906 | KNN Loss: 5.749680519104004 | BCE Loss: 1.0258090496063232\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.681342601776123 | KNN Loss: 5.619793891906738 | BCE Loss: 1.0615487098693848\n",
      "Epoch   275: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.754763603210449 | KNN Loss: 5.681330680847168 | BCE Loss: 1.0734326839447021\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.8114447593688965 | KNN Loss: 5.746232986450195 | BCE Loss: 1.0652118921279907\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.651269912719727 | KNN Loss: 5.60982608795166 | BCE Loss: 1.041443943977356\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.6558051109313965 | KNN Loss: 5.626478672027588 | BCE Loss: 1.029326319694519\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 6.717638969421387 | KNN Loss: 5.643941879272461 | BCE Loss: 1.0736969709396362\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.667206764221191 | KNN Loss: 5.6257219314575195 | BCE Loss: 1.0414847135543823\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.653903007507324 | KNN Loss: 5.616786956787109 | BCE Loss: 1.0371158123016357\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 6.67970085144043 | KNN Loss: 5.613729953765869 | BCE Loss: 1.0659711360931396\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.698362350463867 | KNN Loss: 5.654114246368408 | BCE Loss: 1.0442478656768799\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.646271705627441 | KNN Loss: 5.600741386413574 | BCE Loss: 1.0455305576324463\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.702372074127197 | KNN Loss: 5.64902400970459 | BCE Loss: 1.0533479452133179\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.750180244445801 | KNN Loss: 5.726950168609619 | BCE Loss: 1.0232298374176025\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.650286674499512 | KNN Loss: 5.616537570953369 | BCE Loss: 1.0337493419647217\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.645442962646484 | KNN Loss: 5.607006072998047 | BCE Loss: 1.0384368896484375\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.69681453704834 | KNN Loss: 5.654857635498047 | BCE Loss: 1.041956901550293\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.667573928833008 | KNN Loss: 5.627886772155762 | BCE Loss: 1.0396873950958252\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.68239164352417 | KNN Loss: 5.631095886230469 | BCE Loss: 1.0512957572937012\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.678072929382324 | KNN Loss: 5.614476680755615 | BCE Loss: 1.063596487045288\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.692458152770996 | KNN Loss: 5.643717288970947 | BCE Loss: 1.048741102218628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.659604072570801 | KNN Loss: 5.625123500823975 | BCE Loss: 1.0344805717468262\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.674835205078125 | KNN Loss: 5.6158318519592285 | BCE Loss: 1.0590031147003174\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.741918563842773 | KNN Loss: 5.694171905517578 | BCE Loss: 1.0477468967437744\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.673861026763916 | KNN Loss: 5.608985424041748 | BCE Loss: 1.064875602722168\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.650136947631836 | KNN Loss: 5.604300022125244 | BCE Loss: 1.0458369255065918\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.615821838378906 | KNN Loss: 5.6042609214782715 | BCE Loss: 1.0115611553192139\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.648772239685059 | KNN Loss: 5.602992534637451 | BCE Loss: 1.0457799434661865\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.711928844451904 | KNN Loss: 5.639384746551514 | BCE Loss: 1.0725440979003906\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.715874671936035 | KNN Loss: 5.671176910400391 | BCE Loss: 1.0446979999542236\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.683405876159668 | KNN Loss: 5.633288860321045 | BCE Loss: 1.0501168966293335\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.698709487915039 | KNN Loss: 5.659759521484375 | BCE Loss: 1.0389502048492432\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.679595947265625 | KNN Loss: 5.605027675628662 | BCE Loss: 1.074568510055542\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.684518337249756 | KNN Loss: 5.607598304748535 | BCE Loss: 1.0769201517105103\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.689097881317139 | KNN Loss: 5.660293102264404 | BCE Loss: 1.0288047790527344\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.714399814605713 | KNN Loss: 5.655167579650879 | BCE Loss: 1.0592323541641235\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.675178050994873 | KNN Loss: 5.623647212982178 | BCE Loss: 1.0515309572219849\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.682025909423828 | KNN Loss: 5.645238876342773 | BCE Loss: 1.0367870330810547\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 6.667171478271484 | KNN Loss: 5.610255718231201 | BCE Loss: 1.0569159984588623\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.679470539093018 | KNN Loss: 5.629269123077393 | BCE Loss: 1.050201416015625\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.675069808959961 | KNN Loss: 5.6324381828308105 | BCE Loss: 1.0426313877105713\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.709103584289551 | KNN Loss: 5.6550798416137695 | BCE Loss: 1.0540238618850708\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.803879261016846 | KNN Loss: 5.762174606323242 | BCE Loss: 1.0417046546936035\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.705099105834961 | KNN Loss: 5.677670955657959 | BCE Loss: 1.0274279117584229\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.654566764831543 | KNN Loss: 5.620704174041748 | BCE Loss: 1.033862829208374\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.709334373474121 | KNN Loss: 5.646640300750732 | BCE Loss: 1.0626943111419678\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 6.731834888458252 | KNN Loss: 5.685415744781494 | BCE Loss: 1.0464191436767578\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.660974502563477 | KNN Loss: 5.598884582519531 | BCE Loss: 1.0620896816253662\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.637900352478027 | KNN Loss: 5.611636161804199 | BCE Loss: 1.0262644290924072\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.682546615600586 | KNN Loss: 5.637150287628174 | BCE Loss: 1.045396089553833\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.696277618408203 | KNN Loss: 5.637710094451904 | BCE Loss: 1.058567762374878\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.6209588050842285 | KNN Loss: 5.615153789520264 | BCE Loss: 1.0058050155639648\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.696228981018066 | KNN Loss: 5.667967319488525 | BCE Loss: 1.028261661529541\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.679903030395508 | KNN Loss: 5.630423069000244 | BCE Loss: 1.0494798421859741\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.664399147033691 | KNN Loss: 5.601190567016602 | BCE Loss: 1.0632085800170898\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.698481559753418 | KNN Loss: 5.658634662628174 | BCE Loss: 1.039846658706665\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.744223594665527 | KNN Loss: 5.686795234680176 | BCE Loss: 1.0574281215667725\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.706058025360107 | KNN Loss: 5.651930809020996 | BCE Loss: 1.0541273355484009\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.659198760986328 | KNN Loss: 5.6106085777282715 | BCE Loss: 1.0485903024673462\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.759286880493164 | KNN Loss: 5.68911075592041 | BCE Loss: 1.0701758861541748\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.7080912590026855 | KNN Loss: 5.616495609283447 | BCE Loss: 1.0915956497192383\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.636109352111816 | KNN Loss: 5.611063480377197 | BCE Loss: 1.02504563331604\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.687995910644531 | KNN Loss: 5.654473781585693 | BCE Loss: 1.033522367477417\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.728867530822754 | KNN Loss: 5.661393165588379 | BCE Loss: 1.067474365234375\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.702329635620117 | KNN Loss: 5.639772891998291 | BCE Loss: 1.0625568628311157\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.73070764541626 | KNN Loss: 5.670492172241211 | BCE Loss: 1.0602154731750488\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.645571231842041 | KNN Loss: 5.623720169067383 | BCE Loss: 1.0218510627746582\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.6660380363464355 | KNN Loss: 5.608442306518555 | BCE Loss: 1.0575956106185913\n",
      "Epoch   286: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.686496257781982 | KNN Loss: 5.641454696655273 | BCE Loss: 1.0450414419174194\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 6.691886901855469 | KNN Loss: 5.651276588439941 | BCE Loss: 1.0406103134155273\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.760379791259766 | KNN Loss: 5.703427791595459 | BCE Loss: 1.0569519996643066\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.69666051864624 | KNN Loss: 5.654473304748535 | BCE Loss: 1.0421870946884155\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.676183700561523 | KNN Loss: 5.650166988372803 | BCE Loss: 1.0260165929794312\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.651438236236572 | KNN Loss: 5.60625696182251 | BCE Loss: 1.0451812744140625\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.640576362609863 | KNN Loss: 5.6031928062438965 | BCE Loss: 1.0373836755752563\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.714813232421875 | KNN Loss: 5.625384330749512 | BCE Loss: 1.0894291400909424\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.712406158447266 | KNN Loss: 5.685089588165283 | BCE Loss: 1.027316689491272\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 6.686216354370117 | KNN Loss: 5.612390518188477 | BCE Loss: 1.0738258361816406\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.694206237792969 | KNN Loss: 5.617519855499268 | BCE Loss: 1.0766866207122803\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.644235610961914 | KNN Loss: 5.61815071105957 | BCE Loss: 1.0260846614837646\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 6.660781383514404 | KNN Loss: 5.624021530151367 | BCE Loss: 1.0367599725723267\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.691176891326904 | KNN Loss: 5.6326212882995605 | BCE Loss: 1.0585556030273438\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.665883541107178 | KNN Loss: 5.621370315551758 | BCE Loss: 1.04451322555542\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 6.653790473937988 | KNN Loss: 5.60951566696167 | BCE Loss: 1.0442750453948975\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.715305328369141 | KNN Loss: 5.681650161743164 | BCE Loss: 1.0336554050445557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.695335388183594 | KNN Loss: 5.658448219299316 | BCE Loss: 1.0368874073028564\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.712217330932617 | KNN Loss: 5.66303825378418 | BCE Loss: 1.0491788387298584\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.705946922302246 | KNN Loss: 5.654061317443848 | BCE Loss: 1.0518854856491089\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.682750701904297 | KNN Loss: 5.609925270080566 | BCE Loss: 1.0728254318237305\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.662166118621826 | KNN Loss: 5.616677284240723 | BCE Loss: 1.0454888343811035\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.689208030700684 | KNN Loss: 5.65395975112915 | BCE Loss: 1.035248041152954\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 6.703148365020752 | KNN Loss: 5.64250373840332 | BCE Loss: 1.0606447458267212\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.661749362945557 | KNN Loss: 5.609600067138672 | BCE Loss: 1.0521492958068848\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.6527910232543945 | KNN Loss: 5.611159324645996 | BCE Loss: 1.0416316986083984\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.656700134277344 | KNN Loss: 5.6048173904418945 | BCE Loss: 1.0518827438354492\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.698184013366699 | KNN Loss: 5.669416427612305 | BCE Loss: 1.0287678241729736\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.695671081542969 | KNN Loss: 5.632695198059082 | BCE Loss: 1.0629761219024658\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.7673115730285645 | KNN Loss: 5.70838737487793 | BCE Loss: 1.0589243173599243\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.696542739868164 | KNN Loss: 5.618542194366455 | BCE Loss: 1.0780003070831299\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.710501670837402 | KNN Loss: 5.657703876495361 | BCE Loss: 1.052797794342041\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.670056343078613 | KNN Loss: 5.609236240386963 | BCE Loss: 1.0608203411102295\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.664648056030273 | KNN Loss: 5.6176676750183105 | BCE Loss: 1.0469801425933838\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 6.670325756072998 | KNN Loss: 5.610219955444336 | BCE Loss: 1.060105800628662\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.657628059387207 | KNN Loss: 5.656496524810791 | BCE Loss: 1.001131296157837\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.682857513427734 | KNN Loss: 5.599300861358643 | BCE Loss: 1.083556890487671\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.6507697105407715 | KNN Loss: 5.624679088592529 | BCE Loss: 1.0260907411575317\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.695467472076416 | KNN Loss: 5.66060209274292 | BCE Loss: 1.034865379333496\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.737386226654053 | KNN Loss: 5.675651550292969 | BCE Loss: 1.061734676361084\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.712888717651367 | KNN Loss: 5.696627616882324 | BCE Loss: 1.016261339187622\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.724123954772949 | KNN Loss: 5.652627468109131 | BCE Loss: 1.0714962482452393\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.743152141571045 | KNN Loss: 5.682570457458496 | BCE Loss: 1.0605818033218384\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.641249656677246 | KNN Loss: 5.613664627075195 | BCE Loss: 1.0275852680206299\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.650749206542969 | KNN Loss: 5.614536762237549 | BCE Loss: 1.0362122058868408\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.7210187911987305 | KNN Loss: 5.6971211433410645 | BCE Loss: 1.023897409439087\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.662707328796387 | KNN Loss: 5.615293025970459 | BCE Loss: 1.0474143028259277\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.662193298339844 | KNN Loss: 5.60906982421875 | BCE Loss: 1.0531232357025146\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.6770195960998535 | KNN Loss: 5.612001895904541 | BCE Loss: 1.065017580986023\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.686527252197266 | KNN Loss: 5.612260341644287 | BCE Loss: 1.0742671489715576\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.659801483154297 | KNN Loss: 5.619502067565918 | BCE Loss: 1.0402991771697998\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 6.704719543457031 | KNN Loss: 5.638142108917236 | BCE Loss: 1.0665775537490845\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.662053108215332 | KNN Loss: 5.605808734893799 | BCE Loss: 1.056244134902954\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.741504192352295 | KNN Loss: 5.683322429656982 | BCE Loss: 1.058181643486023\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.707378387451172 | KNN Loss: 5.639617443084717 | BCE Loss: 1.067760944366455\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.6555986404418945 | KNN Loss: 5.621044635772705 | BCE Loss: 1.0345537662506104\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.668751239776611 | KNN Loss: 5.622097969055176 | BCE Loss: 1.046653151512146\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.684139251708984 | KNN Loss: 5.602165699005127 | BCE Loss: 1.0819733142852783\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.628983497619629 | KNN Loss: 5.607337951660156 | BCE Loss: 1.0216455459594727\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.678913116455078 | KNN Loss: 5.619802474975586 | BCE Loss: 1.059110403060913\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.645211219787598 | KNN Loss: 5.613862991333008 | BCE Loss: 1.0313479900360107\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.643446922302246 | KNN Loss: 5.604597568511963 | BCE Loss: 1.038849115371704\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.764308929443359 | KNN Loss: 5.687077522277832 | BCE Loss: 1.0772314071655273\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.70376443862915 | KNN Loss: 5.643535137176514 | BCE Loss: 1.0602294206619263\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.676570892333984 | KNN Loss: 5.62296724319458 | BCE Loss: 1.0536038875579834\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 6.7001237869262695 | KNN Loss: 5.674835681915283 | BCE Loss: 1.0252881050109863\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.650996208190918 | KNN Loss: 5.616791725158691 | BCE Loss: 1.0342046022415161\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.694088935852051 | KNN Loss: 5.638906955718994 | BCE Loss: 1.0551822185516357\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.7118449211120605 | KNN Loss: 5.660613059997559 | BCE Loss: 1.0512319803237915\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.6452131271362305 | KNN Loss: 5.6123738288879395 | BCE Loss: 1.0328395366668701\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.665876388549805 | KNN Loss: 5.61053466796875 | BCE Loss: 1.0553417205810547\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.649094104766846 | KNN Loss: 5.6108503341674805 | BCE Loss: 1.0382437705993652\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.667656898498535 | KNN Loss: 5.605291843414307 | BCE Loss: 1.062364935874939\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.611045837402344 | KNN Loss: 5.59419584274292 | BCE Loss: 1.016850233078003\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.693371295928955 | KNN Loss: 5.630362510681152 | BCE Loss: 1.0630087852478027\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.693461894989014 | KNN Loss: 5.623213768005371 | BCE Loss: 1.0702481269836426\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.629850387573242 | KNN Loss: 5.606602191925049 | BCE Loss: 1.0232479572296143\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.681245803833008 | KNN Loss: 5.644646167755127 | BCE Loss: 1.03659987449646\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.7950029373168945 | KNN Loss: 5.744444370269775 | BCE Loss: 1.0505584478378296\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.664033889770508 | KNN Loss: 5.62591552734375 | BCE Loss: 1.0381184816360474\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.681792259216309 | KNN Loss: 5.654675006866455 | BCE Loss: 1.027117371559143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.700072288513184 | KNN Loss: 5.637031555175781 | BCE Loss: 1.0630409717559814\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.698983669281006 | KNN Loss: 5.643558502197266 | BCE Loss: 1.0554251670837402\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.674539566040039 | KNN Loss: 5.618405342102051 | BCE Loss: 1.0561344623565674\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.68757438659668 | KNN Loss: 5.665266990661621 | BCE Loss: 1.0223076343536377\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.6887359619140625 | KNN Loss: 5.615756511688232 | BCE Loss: 1.0729796886444092\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.677878379821777 | KNN Loss: 5.636390209197998 | BCE Loss: 1.0414880514144897\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.664612293243408 | KNN Loss: 5.626784801483154 | BCE Loss: 1.037827491760254\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.682285308837891 | KNN Loss: 5.617672443389893 | BCE Loss: 1.0646131038665771\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.637054920196533 | KNN Loss: 5.611239433288574 | BCE Loss: 1.025815486907959\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.691476821899414 | KNN Loss: 5.6345696449279785 | BCE Loss: 1.0569069385528564\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.678079605102539 | KNN Loss: 5.620077610015869 | BCE Loss: 1.0580018758773804\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.689750671386719 | KNN Loss: 5.636293888092041 | BCE Loss: 1.0534565448760986\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.721500873565674 | KNN Loss: 5.668671607971191 | BCE Loss: 1.0528292655944824\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.659107685089111 | KNN Loss: 5.614428997039795 | BCE Loss: 1.0446786880493164\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.719857215881348 | KNN Loss: 5.696104526519775 | BCE Loss: 1.0237528085708618\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 6.710755348205566 | KNN Loss: 5.640598297119141 | BCE Loss: 1.0701569318771362\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.6786417961120605 | KNN Loss: 5.640839576721191 | BCE Loss: 1.0378023386001587\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.721866607666016 | KNN Loss: 5.659205913543701 | BCE Loss: 1.0626604557037354\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.670550346374512 | KNN Loss: 5.633865833282471 | BCE Loss: 1.036684513092041\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.675210475921631 | KNN Loss: 5.618675231933594 | BCE Loss: 1.056535243988037\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.740006446838379 | KNN Loss: 5.689469337463379 | BCE Loss: 1.0505372285842896\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.703674793243408 | KNN Loss: 5.6802825927734375 | BCE Loss: 1.0233920812606812\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.75829553604126 | KNN Loss: 5.696712493896484 | BCE Loss: 1.0615829229354858\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.648368835449219 | KNN Loss: 5.612329483032227 | BCE Loss: 1.0360395908355713\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.629524230957031 | KNN Loss: 5.611779689788818 | BCE Loss: 1.017744779586792\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.781323432922363 | KNN Loss: 5.70570707321167 | BCE Loss: 1.0756163597106934\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.682868480682373 | KNN Loss: 5.656029224395752 | BCE Loss: 1.026839256286621\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.756664276123047 | KNN Loss: 5.670879364013672 | BCE Loss: 1.0857847929000854\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.655513763427734 | KNN Loss: 5.61348819732666 | BCE Loss: 1.0420255661010742\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.6724724769592285 | KNN Loss: 5.631174087524414 | BCE Loss: 1.0412983894348145\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.639573097229004 | KNN Loss: 5.609687328338623 | BCE Loss: 1.0298855304718018\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.702820777893066 | KNN Loss: 5.659975528717041 | BCE Loss: 1.0428454875946045\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.633258819580078 | KNN Loss: 5.605373859405518 | BCE Loss: 1.0278847217559814\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.670452117919922 | KNN Loss: 5.615283012390137 | BCE Loss: 1.0551691055297852\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.665214538574219 | KNN Loss: 5.618692398071289 | BCE Loss: 1.0465223789215088\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.75691556930542 | KNN Loss: 5.712118148803711 | BCE Loss: 1.044797420501709\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.686546802520752 | KNN Loss: 5.606301784515381 | BCE Loss: 1.080245018005371\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.747333526611328 | KNN Loss: 5.682452201843262 | BCE Loss: 1.0648815631866455\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.664752006530762 | KNN Loss: 5.603827476501465 | BCE Loss: 1.0609246492385864\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.669377326965332 | KNN Loss: 5.621205806732178 | BCE Loss: 1.0481717586517334\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.72199821472168 | KNN Loss: 5.671638011932373 | BCE Loss: 1.0503599643707275\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.652115345001221 | KNN Loss: 5.636775970458984 | BCE Loss: 1.0153393745422363\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 6.696323394775391 | KNN Loss: 5.611979007720947 | BCE Loss: 1.0843443870544434\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.69820499420166 | KNN Loss: 5.651393890380859 | BCE Loss: 1.0468108654022217\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.670167922973633 | KNN Loss: 5.632025241851807 | BCE Loss: 1.0381429195404053\n",
      "Epoch   307: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.718514442443848 | KNN Loss: 5.678667068481445 | BCE Loss: 1.0398472547531128\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.697206497192383 | KNN Loss: 5.632273197174072 | BCE Loss: 1.0649330615997314\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.731255531311035 | KNN Loss: 5.671384811401367 | BCE Loss: 1.0598706007003784\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.655198574066162 | KNN Loss: 5.635763645172119 | BCE Loss: 1.0194350481033325\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.67780065536499 | KNN Loss: 5.623974323272705 | BCE Loss: 1.0538263320922852\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.6588826179504395 | KNN Loss: 5.631452560424805 | BCE Loss: 1.0274300575256348\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 6.734432697296143 | KNN Loss: 5.70944881439209 | BCE Loss: 1.0249837636947632\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.701715469360352 | KNN Loss: 5.667476654052734 | BCE Loss: 1.0342388153076172\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.669203281402588 | KNN Loss: 5.605936527252197 | BCE Loss: 1.063266634941101\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.801353454589844 | KNN Loss: 5.7159528732299805 | BCE Loss: 1.0854008197784424\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.746349334716797 | KNN Loss: 5.695005893707275 | BCE Loss: 1.0513436794281006\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.749034881591797 | KNN Loss: 5.676204681396484 | BCE Loss: 1.0728304386138916\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.791466236114502 | KNN Loss: 5.74672794342041 | BCE Loss: 1.0447382926940918\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.655807971954346 | KNN Loss: 5.602180480957031 | BCE Loss: 1.053627610206604\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.67400598526001 | KNN Loss: 5.615444183349609 | BCE Loss: 1.0585618019104004\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 6.695387840270996 | KNN Loss: 5.641024589538574 | BCE Loss: 1.0543632507324219\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.64542293548584 | KNN Loss: 5.603635311126709 | BCE Loss: 1.0417877435684204\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.665121078491211 | KNN Loss: 5.609133243560791 | BCE Loss: 1.0559879541397095\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.689678192138672 | KNN Loss: 5.628918647766113 | BCE Loss: 1.0607595443725586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.699848175048828 | KNN Loss: 5.639166831970215 | BCE Loss: 1.0606813430786133\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.656959056854248 | KNN Loss: 5.60576057434082 | BCE Loss: 1.0511984825134277\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.655472278594971 | KNN Loss: 5.604700565338135 | BCE Loss: 1.050771713256836\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.6800689697265625 | KNN Loss: 5.610294342041016 | BCE Loss: 1.0697745084762573\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.683248519897461 | KNN Loss: 5.634279727935791 | BCE Loss: 1.048969030380249\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.658801078796387 | KNN Loss: 5.616293907165527 | BCE Loss: 1.0425069332122803\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.645790100097656 | KNN Loss: 5.608813285827637 | BCE Loss: 1.0369765758514404\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.67181396484375 | KNN Loss: 5.612786293029785 | BCE Loss: 1.0590274333953857\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.685721397399902 | KNN Loss: 5.612029552459717 | BCE Loss: 1.0736916065216064\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.649515151977539 | KNN Loss: 5.6078033447265625 | BCE Loss: 1.0417115688323975\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.766041278839111 | KNN Loss: 5.692684173583984 | BCE Loss: 1.0733569860458374\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.678119659423828 | KNN Loss: 5.6203508377075195 | BCE Loss: 1.0577688217163086\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.712959289550781 | KNN Loss: 5.618582248687744 | BCE Loss: 1.0943772792816162\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.663794040679932 | KNN Loss: 5.605366230010986 | BCE Loss: 1.0584278106689453\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 6.684004306793213 | KNN Loss: 5.628614902496338 | BCE Loss: 1.055389404296875\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.663423538208008 | KNN Loss: 5.605197429656982 | BCE Loss: 1.0582259893417358\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.670696258544922 | KNN Loss: 5.643393516540527 | BCE Loss: 1.0273027420043945\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.708337783813477 | KNN Loss: 5.654648780822754 | BCE Loss: 1.0536890029907227\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.678432941436768 | KNN Loss: 5.634034156799316 | BCE Loss: 1.0443987846374512\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.687376976013184 | KNN Loss: 5.6240315437316895 | BCE Loss: 1.0633456707000732\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.665448188781738 | KNN Loss: 5.640246391296387 | BCE Loss: 1.0252019166946411\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.759603977203369 | KNN Loss: 5.688554286956787 | BCE Loss: 1.071049690246582\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.659068584442139 | KNN Loss: 5.641403675079346 | BCE Loss: 1.017664909362793\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 6.700760841369629 | KNN Loss: 5.663297176361084 | BCE Loss: 1.037463903427124\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.647508144378662 | KNN Loss: 5.607123374938965 | BCE Loss: 1.0403846502304077\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.711679458618164 | KNN Loss: 5.6670145988464355 | BCE Loss: 1.0446648597717285\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.71645975112915 | KNN Loss: 5.659126281738281 | BCE Loss: 1.0573335886001587\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.655994892120361 | KNN Loss: 5.602465629577637 | BCE Loss: 1.0535292625427246\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.653117656707764 | KNN Loss: 5.617002487182617 | BCE Loss: 1.0361151695251465\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.709083557128906 | KNN Loss: 5.664257526397705 | BCE Loss: 1.0448262691497803\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.662965774536133 | KNN Loss: 5.618472576141357 | BCE Loss: 1.0444929599761963\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.6471052169799805 | KNN Loss: 5.6150803565979 | BCE Loss: 1.0320247411727905\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 6.675380706787109 | KNN Loss: 5.630068778991699 | BCE Loss: 1.0453121662139893\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.667159557342529 | KNN Loss: 5.622124195098877 | BCE Loss: 1.0450353622436523\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.7589921951293945 | KNN Loss: 5.729122161865234 | BCE Loss: 1.0298702716827393\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.666532516479492 | KNN Loss: 5.620612621307373 | BCE Loss: 1.0459200143814087\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.634076118469238 | KNN Loss: 5.601337909698486 | BCE Loss: 1.0327380895614624\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.673421859741211 | KNN Loss: 5.613887786865234 | BCE Loss: 1.0595341920852661\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.729618549346924 | KNN Loss: 5.65811014175415 | BCE Loss: 1.071508526802063\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 6.673617362976074 | KNN Loss: 5.634429931640625 | BCE Loss: 1.0391871929168701\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.6394944190979 | KNN Loss: 5.604671001434326 | BCE Loss: 1.0348235368728638\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.647685527801514 | KNN Loss: 5.632796287536621 | BCE Loss: 1.0148892402648926\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.6762800216674805 | KNN Loss: 5.613954067230225 | BCE Loss: 1.062326192855835\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.682656288146973 | KNN Loss: 5.620214939117432 | BCE Loss: 1.062441110610962\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.68485164642334 | KNN Loss: 5.633166313171387 | BCE Loss: 1.0516854524612427\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 6.741253852844238 | KNN Loss: 5.699415683746338 | BCE Loss: 1.0418381690979004\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.687885284423828 | KNN Loss: 5.634521961212158 | BCE Loss: 1.0533632040023804\n",
      "Epoch   318: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.701960563659668 | KNN Loss: 5.646473407745361 | BCE Loss: 1.0554869174957275\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.6424241065979 | KNN Loss: 5.605949401855469 | BCE Loss: 1.036474585533142\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.690098762512207 | KNN Loss: 5.632506847381592 | BCE Loss: 1.0575921535491943\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.683923721313477 | KNN Loss: 5.652431011199951 | BCE Loss: 1.0314927101135254\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.721226692199707 | KNN Loss: 5.667646884918213 | BCE Loss: 1.0535800457000732\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 6.628806114196777 | KNN Loss: 5.620361804962158 | BCE Loss: 1.0084443092346191\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.715991973876953 | KNN Loss: 5.669049263000488 | BCE Loss: 1.0469424724578857\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.731479644775391 | KNN Loss: 5.638008117675781 | BCE Loss: 1.0934717655181885\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.7283101081848145 | KNN Loss: 5.683441162109375 | BCE Loss: 1.044869065284729\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.674254417419434 | KNN Loss: 5.607900619506836 | BCE Loss: 1.0663535594940186\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.725512504577637 | KNN Loss: 5.67103385925293 | BCE Loss: 1.054478645324707\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.735337257385254 | KNN Loss: 5.710290908813477 | BCE Loss: 1.0250461101531982\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.772414684295654 | KNN Loss: 5.706592559814453 | BCE Loss: 1.0658221244812012\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.7233734130859375 | KNN Loss: 5.688742637634277 | BCE Loss: 1.034630537033081\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.636111259460449 | KNN Loss: 5.605098724365234 | BCE Loss: 1.031012773513794\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.69931697845459 | KNN Loss: 5.639605522155762 | BCE Loss: 1.059711217880249\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.682314872741699 | KNN Loss: 5.609006881713867 | BCE Loss: 1.0733082294464111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.68550968170166 | KNN Loss: 5.6514506340026855 | BCE Loss: 1.0340590476989746\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.697296142578125 | KNN Loss: 5.636620998382568 | BCE Loss: 1.0606751441955566\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.6705427169799805 | KNN Loss: 5.61781644821167 | BCE Loss: 1.052726149559021\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.667546272277832 | KNN Loss: 5.653421401977539 | BCE Loss: 1.0141249895095825\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.659872055053711 | KNN Loss: 5.611155033111572 | BCE Loss: 1.0487172603607178\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.671212196350098 | KNN Loss: 5.612053394317627 | BCE Loss: 1.0591585636138916\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 6.650148391723633 | KNN Loss: 5.625871658325195 | BCE Loss: 1.024276614189148\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.645895481109619 | KNN Loss: 5.631824016571045 | BCE Loss: 1.0140714645385742\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.771130561828613 | KNN Loss: 5.708644390106201 | BCE Loss: 1.0624864101409912\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.6619553565979 | KNN Loss: 5.620484828948975 | BCE Loss: 1.0414706468582153\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.732767581939697 | KNN Loss: 5.7114715576171875 | BCE Loss: 1.0212960243225098\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 6.73255729675293 | KNN Loss: 5.671706199645996 | BCE Loss: 1.0608513355255127\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.769000053405762 | KNN Loss: 5.723325252532959 | BCE Loss: 1.0456745624542236\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.675083160400391 | KNN Loss: 5.62209415435791 | BCE Loss: 1.0529892444610596\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.672612190246582 | KNN Loss: 5.624754905700684 | BCE Loss: 1.0478575229644775\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.641057968139648 | KNN Loss: 5.603492259979248 | BCE Loss: 1.0375659465789795\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.78443717956543 | KNN Loss: 5.725257873535156 | BCE Loss: 1.059179425239563\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.7018632888793945 | KNN Loss: 5.650923252105713 | BCE Loss: 1.0509402751922607\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 6.644824981689453 | KNN Loss: 5.6015520095825195 | BCE Loss: 1.0432727336883545\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.730569839477539 | KNN Loss: 5.626032829284668 | BCE Loss: 1.104537010192871\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 6.6781768798828125 | KNN Loss: 5.64716911315918 | BCE Loss: 1.0310077667236328\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.659956932067871 | KNN Loss: 5.62378454208374 | BCE Loss: 1.03617262840271\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.676211833953857 | KNN Loss: 5.6283674240112305 | BCE Loss: 1.047844409942627\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.6693854331970215 | KNN Loss: 5.603971481323242 | BCE Loss: 1.0654140710830688\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.706523418426514 | KNN Loss: 5.649703502655029 | BCE Loss: 1.056820034980774\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.692138671875 | KNN Loss: 5.622343063354492 | BCE Loss: 1.0697954893112183\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.654238700866699 | KNN Loss: 5.608448505401611 | BCE Loss: 1.0457903146743774\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.660865783691406 | KNN Loss: 5.631388187408447 | BCE Loss: 1.029477596282959\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.664371490478516 | KNN Loss: 5.617300510406494 | BCE Loss: 1.0470712184906006\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.67813777923584 | KNN Loss: 5.617020130157471 | BCE Loss: 1.0611176490783691\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.688918113708496 | KNN Loss: 5.6406331062316895 | BCE Loss: 1.0482850074768066\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.78680419921875 | KNN Loss: 5.752011299133301 | BCE Loss: 1.0347927808761597\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.652805328369141 | KNN Loss: 5.626949310302734 | BCE Loss: 1.0258557796478271\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.748510360717773 | KNN Loss: 5.637078285217285 | BCE Loss: 1.1114318370819092\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.661592960357666 | KNN Loss: 5.599250316619873 | BCE Loss: 1.0623427629470825\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.747617721557617 | KNN Loss: 5.694697380065918 | BCE Loss: 1.0529205799102783\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.671257019042969 | KNN Loss: 5.63236141204834 | BCE Loss: 1.0388957262039185\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.641372203826904 | KNN Loss: 5.613960266113281 | BCE Loss: 1.0274118185043335\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.700005531311035 | KNN Loss: 5.640796184539795 | BCE Loss: 1.0592095851898193\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.637704849243164 | KNN Loss: 5.621192455291748 | BCE Loss: 1.016512393951416\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.6695556640625 | KNN Loss: 5.59784460067749 | BCE Loss: 1.0717113018035889\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.656096458435059 | KNN Loss: 5.622895240783691 | BCE Loss: 1.033200979232788\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.6584649085998535 | KNN Loss: 5.617023944854736 | BCE Loss: 1.0414409637451172\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.766425132751465 | KNN Loss: 5.715173244476318 | BCE Loss: 1.0512518882751465\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.721225738525391 | KNN Loss: 5.67453145980835 | BCE Loss: 1.0466945171356201\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.732356548309326 | KNN Loss: 5.6798834800720215 | BCE Loss: 1.0524730682373047\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.654760360717773 | KNN Loss: 5.619213104248047 | BCE Loss: 1.0355470180511475\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 6.680253028869629 | KNN Loss: 5.618635654449463 | BCE Loss: 1.0616174936294556\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 6.704998970031738 | KNN Loss: 5.625083923339844 | BCE Loss: 1.0799148082733154\n",
      "Epoch   329: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.726789474487305 | KNN Loss: 5.631432056427002 | BCE Loss: 1.0953571796417236\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.670779228210449 | KNN Loss: 5.631680488586426 | BCE Loss: 1.039098858833313\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.628203868865967 | KNN Loss: 5.598540782928467 | BCE Loss: 1.0296629667282104\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.690098762512207 | KNN Loss: 5.657191276550293 | BCE Loss: 1.032907485961914\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.668667793273926 | KNN Loss: 5.6231794357299805 | BCE Loss: 1.0454883575439453\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.639226913452148 | KNN Loss: 5.600948333740234 | BCE Loss: 1.038278579711914\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.680401802062988 | KNN Loss: 5.646182060241699 | BCE Loss: 1.0342199802398682\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.618844509124756 | KNN Loss: 5.595302104949951 | BCE Loss: 1.0235424041748047\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.6442718505859375 | KNN Loss: 5.602823257446289 | BCE Loss: 1.0414488315582275\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.671338081359863 | KNN Loss: 5.628015518188477 | BCE Loss: 1.0433228015899658\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.66676664352417 | KNN Loss: 5.610641956329346 | BCE Loss: 1.0561245679855347\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.689316749572754 | KNN Loss: 5.617316722869873 | BCE Loss: 1.0719997882843018\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.690953254699707 | KNN Loss: 5.655010223388672 | BCE Loss: 1.0359431505203247\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.700454235076904 | KNN Loss: 5.616110801696777 | BCE Loss: 1.084343433380127\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.654630661010742 | KNN Loss: 5.6028571128845215 | BCE Loss: 1.0517736673355103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.640481948852539 | KNN Loss: 5.621628761291504 | BCE Loss: 1.0188534259796143\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.6848273277282715 | KNN Loss: 5.608381271362305 | BCE Loss: 1.0764459371566772\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.666048049926758 | KNN Loss: 5.645476818084717 | BCE Loss: 1.0205714702606201\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.643307685852051 | KNN Loss: 5.614668369293213 | BCE Loss: 1.028639316558838\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.653417110443115 | KNN Loss: 5.599168300628662 | BCE Loss: 1.0542488098144531\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.667682647705078 | KNN Loss: 5.597994327545166 | BCE Loss: 1.0696882009506226\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.632024765014648 | KNN Loss: 5.605566501617432 | BCE Loss: 1.0264580249786377\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.685467720031738 | KNN Loss: 5.640016078948975 | BCE Loss: 1.0454518795013428\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.651698112487793 | KNN Loss: 5.605360507965088 | BCE Loss: 1.046337366104126\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.643476486206055 | KNN Loss: 5.608992576599121 | BCE Loss: 1.0344836711883545\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.670078277587891 | KNN Loss: 5.626465797424316 | BCE Loss: 1.0436127185821533\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.671596527099609 | KNN Loss: 5.610508918762207 | BCE Loss: 1.0610873699188232\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.658915996551514 | KNN Loss: 5.610304355621338 | BCE Loss: 1.0486115217208862\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.69500207901001 | KNN Loss: 5.65132999420166 | BCE Loss: 1.04367196559906\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.729108810424805 | KNN Loss: 5.674972057342529 | BCE Loss: 1.0541369915008545\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.745589256286621 | KNN Loss: 5.677957057952881 | BCE Loss: 1.0676321983337402\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.668915271759033 | KNN Loss: 5.615577697753906 | BCE Loss: 1.0533376932144165\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.671849250793457 | KNN Loss: 5.60808801651001 | BCE Loss: 1.0637609958648682\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.718085289001465 | KNN Loss: 5.6632866859436035 | BCE Loss: 1.0547988414764404\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.639045238494873 | KNN Loss: 5.614870071411133 | BCE Loss: 1.0241751670837402\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.678528785705566 | KNN Loss: 5.614907741546631 | BCE Loss: 1.0636208057403564\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.67233943939209 | KNN Loss: 5.595670223236084 | BCE Loss: 1.076669454574585\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.703310966491699 | KNN Loss: 5.680386066436768 | BCE Loss: 1.0229251384735107\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.670467376708984 | KNN Loss: 5.647169589996338 | BCE Loss: 1.023297905921936\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.661004066467285 | KNN Loss: 5.6102519035339355 | BCE Loss: 1.0507524013519287\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.651181697845459 | KNN Loss: 5.5989861488342285 | BCE Loss: 1.0521955490112305\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.637834072113037 | KNN Loss: 5.61931848526001 | BCE Loss: 1.0185155868530273\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.68538761138916 | KNN Loss: 5.630127429962158 | BCE Loss: 1.0552599430084229\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.764296531677246 | KNN Loss: 5.718059539794922 | BCE Loss: 1.0462369918823242\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.687838554382324 | KNN Loss: 5.631070613861084 | BCE Loss: 1.0567680597305298\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.839022636413574 | KNN Loss: 5.743627548217773 | BCE Loss: 1.0953953266143799\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.704898834228516 | KNN Loss: 5.675405979156494 | BCE Loss: 1.0294926166534424\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.652368545532227 | KNN Loss: 5.618529796600342 | BCE Loss: 1.0338385105133057\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.6416335105896 | KNN Loss: 5.60383415222168 | BCE Loss: 1.03779935836792\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.674837112426758 | KNN Loss: 5.642559051513672 | BCE Loss: 1.0322778224945068\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.82432746887207 | KNN Loss: 5.7795281410217285 | BCE Loss: 1.0447993278503418\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 6.760824203491211 | KNN Loss: 5.679974555969238 | BCE Loss: 1.0808494091033936\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.651806831359863 | KNN Loss: 5.598425388336182 | BCE Loss: 1.053381323814392\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.662567615509033 | KNN Loss: 5.620726585388184 | BCE Loss: 1.0418410301208496\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.705224990844727 | KNN Loss: 5.662256240844727 | BCE Loss: 1.042968511581421\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.736921787261963 | KNN Loss: 5.685163974761963 | BCE Loss: 1.0517578125\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.703408241271973 | KNN Loss: 5.642089366912842 | BCE Loss: 1.0613187551498413\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.692811965942383 | KNN Loss: 5.646731853485107 | BCE Loss: 1.0460801124572754\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 6.675014495849609 | KNN Loss: 5.615704536437988 | BCE Loss: 1.0593101978302002\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 6.682081699371338 | KNN Loss: 5.6303606033325195 | BCE Loss: 1.051721215248108\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.659112930297852 | KNN Loss: 5.6102681159973145 | BCE Loss: 1.048844575881958\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.6756439208984375 | KNN Loss: 5.607412815093994 | BCE Loss: 1.0682311058044434\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.6511006355285645 | KNN Loss: 5.6258063316345215 | BCE Loss: 1.025294303894043\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.655491828918457 | KNN Loss: 5.623239994049072 | BCE Loss: 1.0322519540786743\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.644815444946289 | KNN Loss: 5.606132984161377 | BCE Loss: 1.038682460784912\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.657120704650879 | KNN Loss: 5.648537635803223 | BCE Loss: 1.0085828304290771\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.665273666381836 | KNN Loss: 5.631407737731934 | BCE Loss: 1.0338661670684814\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 6.666733264923096 | KNN Loss: 5.61735725402832 | BCE Loss: 1.0493760108947754\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.707871437072754 | KNN Loss: 5.656907558441162 | BCE Loss: 1.0509638786315918\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.709705829620361 | KNN Loss: 5.71041202545166 | BCE Loss: 0.9992936253547668\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.699610233306885 | KNN Loss: 5.667359828948975 | BCE Loss: 1.0322505235671997\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.690370559692383 | KNN Loss: 5.648313045501709 | BCE Loss: 1.0420573949813843\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.6498870849609375 | KNN Loss: 5.612932205200195 | BCE Loss: 1.0369548797607422\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.669756889343262 | KNN Loss: 5.612812042236328 | BCE Loss: 1.0569449663162231\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.654899597167969 | KNN Loss: 5.622573375701904 | BCE Loss: 1.0323264598846436\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.68259859085083 | KNN Loss: 5.628451824188232 | BCE Loss: 1.054146647453308\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.698657989501953 | KNN Loss: 5.661750793457031 | BCE Loss: 1.036907434463501\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.688048362731934 | KNN Loss: 5.637770652770996 | BCE Loss: 1.0502779483795166\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.743882179260254 | KNN Loss: 5.68362283706665 | BCE Loss: 1.0602591037750244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.702144622802734 | KNN Loss: 5.638906002044678 | BCE Loss: 1.0632383823394775\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.697080135345459 | KNN Loss: 5.62741231918335 | BCE Loss: 1.0696678161621094\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.746339797973633 | KNN Loss: 5.701863765716553 | BCE Loss: 1.04447603225708\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.698399543762207 | KNN Loss: 5.645187854766846 | BCE Loss: 1.0532116889953613\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.739989280700684 | KNN Loss: 5.704217433929443 | BCE Loss: 1.0357716083526611\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.669395923614502 | KNN Loss: 5.614299297332764 | BCE Loss: 1.0550966262817383\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.655205726623535 | KNN Loss: 5.639425754547119 | BCE Loss: 1.015779972076416\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.658651828765869 | KNN Loss: 5.630436420440674 | BCE Loss: 1.0282155275344849\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.701438903808594 | KNN Loss: 5.693606376647949 | BCE Loss: 1.0078325271606445\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.669602394104004 | KNN Loss: 5.619334697723389 | BCE Loss: 1.0502674579620361\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.691362380981445 | KNN Loss: 5.672939300537109 | BCE Loss: 1.018423318862915\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.659478664398193 | KNN Loss: 5.625308990478516 | BCE Loss: 1.0341696739196777\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.668509006500244 | KNN Loss: 5.617438793182373 | BCE Loss: 1.0510703325271606\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.674765586853027 | KNN Loss: 5.621311664581299 | BCE Loss: 1.0534536838531494\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.664004802703857 | KNN Loss: 5.616190433502197 | BCE Loss: 1.0478143692016602\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 6.666019916534424 | KNN Loss: 5.631312847137451 | BCE Loss: 1.034706950187683\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.7486982345581055 | KNN Loss: 5.685091495513916 | BCE Loss: 1.063606858253479\n",
      "Epoch   345: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.653729438781738 | KNN Loss: 5.6065673828125 | BCE Loss: 1.0471618175506592\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.662741661071777 | KNN Loss: 5.60013484954834 | BCE Loss: 1.0626065731048584\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.731684684753418 | KNN Loss: 5.65720272064209 | BCE Loss: 1.0744822025299072\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.725867748260498 | KNN Loss: 5.678357124328613 | BCE Loss: 1.0475105047225952\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.677281856536865 | KNN Loss: 5.616757869720459 | BCE Loss: 1.0605239868164062\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.6753926277160645 | KNN Loss: 5.631680488586426 | BCE Loss: 1.0437121391296387\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.753060817718506 | KNN Loss: 5.6656575202941895 | BCE Loss: 1.0874032974243164\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 6.6361308097839355 | KNN Loss: 5.610019683837891 | BCE Loss: 1.026111125946045\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.740283012390137 | KNN Loss: 5.678936958312988 | BCE Loss: 1.0613459348678589\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.650564670562744 | KNN Loss: 5.613593578338623 | BCE Loss: 1.036971092224121\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.637307167053223 | KNN Loss: 5.603077411651611 | BCE Loss: 1.0342297554016113\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.640984535217285 | KNN Loss: 5.612510681152344 | BCE Loss: 1.0284740924835205\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 6.64241886138916 | KNN Loss: 5.616268157958984 | BCE Loss: 1.0261509418487549\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.677525520324707 | KNN Loss: 5.619929313659668 | BCE Loss: 1.0575964450836182\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.700896263122559 | KNN Loss: 5.651017665863037 | BCE Loss: 1.0498785972595215\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.662919998168945 | KNN Loss: 5.626598834991455 | BCE Loss: 1.0363209247589111\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.651396751403809 | KNN Loss: 5.616215705871582 | BCE Loss: 1.0351810455322266\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.656228065490723 | KNN Loss: 5.610800266265869 | BCE Loss: 1.0454275608062744\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.63346529006958 | KNN Loss: 5.615551471710205 | BCE Loss: 1.017913818359375\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.684809684753418 | KNN Loss: 5.629439353942871 | BCE Loss: 1.0553700923919678\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.6943793296813965 | KNN Loss: 5.635892391204834 | BCE Loss: 1.058486819267273\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.668125629425049 | KNN Loss: 5.622901916503906 | BCE Loss: 1.0452237129211426\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.687714576721191 | KNN Loss: 5.6267523765563965 | BCE Loss: 1.0609619617462158\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.682512283325195 | KNN Loss: 5.623114585876465 | BCE Loss: 1.059397578239441\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.663823127746582 | KNN Loss: 5.626669406890869 | BCE Loss: 1.0371534824371338\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.644643306732178 | KNN Loss: 5.603007793426514 | BCE Loss: 1.041635513305664\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.675461769104004 | KNN Loss: 5.626334190368652 | BCE Loss: 1.0491276979446411\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.6803483963012695 | KNN Loss: 5.607391834259033 | BCE Loss: 1.0729568004608154\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 6.678631782531738 | KNN Loss: 5.631697654724121 | BCE Loss: 1.0469343662261963\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.732546329498291 | KNN Loss: 5.675074100494385 | BCE Loss: 1.0574722290039062\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.702980995178223 | KNN Loss: 5.6406025886535645 | BCE Loss: 1.0623786449432373\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.656268119812012 | KNN Loss: 5.604485511779785 | BCE Loss: 1.0517823696136475\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.70142936706543 | KNN Loss: 5.6720356941223145 | BCE Loss: 1.0293936729431152\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.6309494972229 | KNN Loss: 5.5996785163879395 | BCE Loss: 1.0312708616256714\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.676816940307617 | KNN Loss: 5.6160888671875 | BCE Loss: 1.060727834701538\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.726925849914551 | KNN Loss: 5.692173480987549 | BCE Loss: 1.034752607345581\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.66676664352417 | KNN Loss: 5.6140217781066895 | BCE Loss: 1.0527448654174805\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.688661575317383 | KNN Loss: 5.6393327713012695 | BCE Loss: 1.0493288040161133\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.707786560058594 | KNN Loss: 5.626031398773193 | BCE Loss: 1.0817550420761108\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.684789657592773 | KNN Loss: 5.662977695465088 | BCE Loss: 1.0218119621276855\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.6584978103637695 | KNN Loss: 5.637252330780029 | BCE Loss: 1.0212457180023193\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.739141464233398 | KNN Loss: 5.684933662414551 | BCE Loss: 1.0542079210281372\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.681989669799805 | KNN Loss: 5.650732517242432 | BCE Loss: 1.0312572717666626\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.667819976806641 | KNN Loss: 5.615812301635742 | BCE Loss: 1.0520076751708984\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.639930248260498 | KNN Loss: 5.598193645477295 | BCE Loss: 1.0417366027832031\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.714017391204834 | KNN Loss: 5.691305160522461 | BCE Loss: 1.022712230682373\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.696187496185303 | KNN Loss: 5.659214496612549 | BCE Loss: 1.036972999572754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.695579528808594 | KNN Loss: 5.6214704513549805 | BCE Loss: 1.0741093158721924\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.737795352935791 | KNN Loss: 5.681590557098389 | BCE Loss: 1.0562047958374023\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 6.747066974639893 | KNN Loss: 5.67075777053833 | BCE Loss: 1.076309084892273\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.6765851974487305 | KNN Loss: 5.6425042152404785 | BCE Loss: 1.034081220626831\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.687179088592529 | KNN Loss: 5.636568546295166 | BCE Loss: 1.0506104230880737\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.628294944763184 | KNN Loss: 5.6126389503479 | BCE Loss: 1.0156559944152832\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.666232109069824 | KNN Loss: 5.623336315155029 | BCE Loss: 1.042895793914795\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.660545825958252 | KNN Loss: 5.62753963470459 | BCE Loss: 1.033006191253662\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.696713447570801 | KNN Loss: 5.667545318603516 | BCE Loss: 1.0291683673858643\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.677331447601318 | KNN Loss: 5.617298603057861 | BCE Loss: 1.060032844543457\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.634617328643799 | KNN Loss: 5.609277725219727 | BCE Loss: 1.0253396034240723\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.715118408203125 | KNN Loss: 5.680013656616211 | BCE Loss: 1.035104513168335\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.7010698318481445 | KNN Loss: 5.646899700164795 | BCE Loss: 1.05417001247406\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.681404113769531 | KNN Loss: 5.6287078857421875 | BCE Loss: 1.0526959896087646\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.678680896759033 | KNN Loss: 5.655029296875 | BCE Loss: 1.0236515998840332\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.701430320739746 | KNN Loss: 5.6570024490356445 | BCE Loss: 1.0444276332855225\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.671962738037109 | KNN Loss: 5.619081974029541 | BCE Loss: 1.052880883216858\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.660719394683838 | KNN Loss: 5.622949600219727 | BCE Loss: 1.0377697944641113\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.68187952041626 | KNN Loss: 5.635956287384033 | BCE Loss: 1.0459232330322266\n",
      "Epoch   356: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.634115219116211 | KNN Loss: 5.601452350616455 | BCE Loss: 1.032663106918335\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.654134273529053 | KNN Loss: 5.627352714538574 | BCE Loss: 1.026781678199768\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.657536506652832 | KNN Loss: 5.6123223304748535 | BCE Loss: 1.0452141761779785\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.691638469696045 | KNN Loss: 5.663619518280029 | BCE Loss: 1.0280189514160156\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.636876583099365 | KNN Loss: 5.6004486083984375 | BCE Loss: 1.0364278554916382\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.683059215545654 | KNN Loss: 5.6271820068359375 | BCE Loss: 1.0558772087097168\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.664070129394531 | KNN Loss: 5.6189775466918945 | BCE Loss: 1.0450923442840576\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.697574138641357 | KNN Loss: 5.642469882965088 | BCE Loss: 1.0551042556762695\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.700662612915039 | KNN Loss: 5.653235912322998 | BCE Loss: 1.047426700592041\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 6.674826622009277 | KNN Loss: 5.618881702423096 | BCE Loss: 1.0559446811676025\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.643828392028809 | KNN Loss: 5.605341911315918 | BCE Loss: 1.0384867191314697\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.6902875900268555 | KNN Loss: 5.620772838592529 | BCE Loss: 1.069514513015747\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.668906211853027 | KNN Loss: 5.609353065490723 | BCE Loss: 1.0595532655715942\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.678166389465332 | KNN Loss: 5.616889953613281 | BCE Loss: 1.0612764358520508\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.629153728485107 | KNN Loss: 5.60269832611084 | BCE Loss: 1.0264554023742676\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.641402244567871 | KNN Loss: 5.617032527923584 | BCE Loss: 1.024369478225708\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.681309700012207 | KNN Loss: 5.615828514099121 | BCE Loss: 1.0654809474945068\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.693083763122559 | KNN Loss: 5.625268459320068 | BCE Loss: 1.0678155422210693\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.750590801239014 | KNN Loss: 5.698728084564209 | BCE Loss: 1.0518625974655151\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.710794925689697 | KNN Loss: 5.653391361236572 | BCE Loss: 1.057403564453125\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.665919303894043 | KNN Loss: 5.61471700668335 | BCE Loss: 1.0512021780014038\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 6.715592384338379 | KNN Loss: 5.657678604125977 | BCE Loss: 1.0579137802124023\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.652889728546143 | KNN Loss: 5.613937854766846 | BCE Loss: 1.0389518737792969\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.654668807983398 | KNN Loss: 5.615631103515625 | BCE Loss: 1.0390379428863525\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.7222723960876465 | KNN Loss: 5.665742874145508 | BCE Loss: 1.0565295219421387\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.7639336585998535 | KNN Loss: 5.706604957580566 | BCE Loss: 1.0573285818099976\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.6889567375183105 | KNN Loss: 5.634082794189453 | BCE Loss: 1.0548738241195679\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.672746181488037 | KNN Loss: 5.626281261444092 | BCE Loss: 1.0464649200439453\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.6647233963012695 | KNN Loss: 5.606152057647705 | BCE Loss: 1.0585715770721436\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 6.737581729888916 | KNN Loss: 5.666433811187744 | BCE Loss: 1.0711479187011719\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.67009162902832 | KNN Loss: 5.625783443450928 | BCE Loss: 1.0443083047866821\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 6.654840469360352 | KNN Loss: 5.610974311828613 | BCE Loss: 1.0438659191131592\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.697473526000977 | KNN Loss: 5.664133548736572 | BCE Loss: 1.0333397388458252\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.721790313720703 | KNN Loss: 5.650349140167236 | BCE Loss: 1.0714409351348877\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.672844409942627 | KNN Loss: 5.613504409790039 | BCE Loss: 1.0593401193618774\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.639375686645508 | KNN Loss: 5.61090612411499 | BCE Loss: 1.0284698009490967\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.658329010009766 | KNN Loss: 5.612736701965332 | BCE Loss: 1.0455920696258545\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.689453125 | KNN Loss: 5.660921096801758 | BCE Loss: 1.028531789779663\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.655451774597168 | KNN Loss: 5.611457347869873 | BCE Loss: 1.0439943075180054\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.7479329109191895 | KNN Loss: 5.698044300079346 | BCE Loss: 1.0498884916305542\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.667409420013428 | KNN Loss: 5.604762554168701 | BCE Loss: 1.0626469850540161\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.687161445617676 | KNN Loss: 5.645731449127197 | BCE Loss: 1.0414302349090576\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.670212268829346 | KNN Loss: 5.6012678146362305 | BCE Loss: 1.0689443349838257\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.6956939697265625 | KNN Loss: 5.657362461090088 | BCE Loss: 1.0383312702178955\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.668305397033691 | KNN Loss: 5.6084885597229 | BCE Loss: 1.059816598892212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.684708118438721 | KNN Loss: 5.643117904663086 | BCE Loss: 1.0415902137756348\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.659982681274414 | KNN Loss: 5.632632732391357 | BCE Loss: 1.0273501873016357\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.689106464385986 | KNN Loss: 5.635043621063232 | BCE Loss: 1.0540629625320435\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.699442386627197 | KNN Loss: 5.647862911224365 | BCE Loss: 1.051579475402832\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.638387680053711 | KNN Loss: 5.6126708984375 | BCE Loss: 1.025716781616211\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.709596633911133 | KNN Loss: 5.636284351348877 | BCE Loss: 1.073312520980835\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.690317630767822 | KNN Loss: 5.64185094833374 | BCE Loss: 1.0484668016433716\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.731231689453125 | KNN Loss: 5.649199962615967 | BCE Loss: 1.0820319652557373\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.724353313446045 | KNN Loss: 5.648194313049316 | BCE Loss: 1.0761590003967285\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.724569320678711 | KNN Loss: 5.669187545776367 | BCE Loss: 1.0553815364837646\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.765460968017578 | KNN Loss: 5.701897621154785 | BCE Loss: 1.0635631084442139\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.67566442489624 | KNN Loss: 5.6188178062438965 | BCE Loss: 1.0568467378616333\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.6812310218811035 | KNN Loss: 5.616294860839844 | BCE Loss: 1.0649361610412598\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 6.650518417358398 | KNN Loss: 5.620882511138916 | BCE Loss: 1.0296356678009033\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.6286187171936035 | KNN Loss: 5.606534481048584 | BCE Loss: 1.02208411693573\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.717692852020264 | KNN Loss: 5.663763523101807 | BCE Loss: 1.0539294481277466\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.708366870880127 | KNN Loss: 5.654780387878418 | BCE Loss: 1.0535866022109985\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.669435977935791 | KNN Loss: 5.621343612670898 | BCE Loss: 1.0480923652648926\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 6.7148895263671875 | KNN Loss: 5.656902313232422 | BCE Loss: 1.0579869747161865\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.683928489685059 | KNN Loss: 5.64470100402832 | BCE Loss: 1.0392274856567383\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.697485446929932 | KNN Loss: 5.631775856018066 | BCE Loss: 1.0657094717025757\n",
      "Epoch   367: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.752005100250244 | KNN Loss: 5.66896915435791 | BCE Loss: 1.0830360651016235\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.690201759338379 | KNN Loss: 5.615200519561768 | BCE Loss: 1.0750010013580322\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.695164680480957 | KNN Loss: 5.6131744384765625 | BCE Loss: 1.0819900035858154\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.733509063720703 | KNN Loss: 5.694750785827637 | BCE Loss: 1.0387580394744873\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.651878356933594 | KNN Loss: 5.629016399383545 | BCE Loss: 1.0228619575500488\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.721336364746094 | KNN Loss: 5.667503356933594 | BCE Loss: 1.053832769393921\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.764043807983398 | KNN Loss: 5.720247745513916 | BCE Loss: 1.0437959432601929\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.688652992248535 | KNN Loss: 5.627462387084961 | BCE Loss: 1.0611908435821533\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.64204740524292 | KNN Loss: 5.614604473114014 | BCE Loss: 1.0274428129196167\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.745857238769531 | KNN Loss: 5.669128894805908 | BCE Loss: 1.0767284631729126\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.649030685424805 | KNN Loss: 5.609816551208496 | BCE Loss: 1.0392142534255981\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.673040866851807 | KNN Loss: 5.61435079574585 | BCE Loss: 1.0586901903152466\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.6695051193237305 | KNN Loss: 5.615141868591309 | BCE Loss: 1.0543630123138428\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.671865463256836 | KNN Loss: 5.612791061401367 | BCE Loss: 1.0590746402740479\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.6697187423706055 | KNN Loss: 5.625863075256348 | BCE Loss: 1.043855905532837\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.681728363037109 | KNN Loss: 5.612987518310547 | BCE Loss: 1.068740963935852\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.777575492858887 | KNN Loss: 5.701926231384277 | BCE Loss: 1.075649380683899\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.723522186279297 | KNN Loss: 5.652989387512207 | BCE Loss: 1.0705326795578003\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.659238338470459 | KNN Loss: 5.620099067687988 | BCE Loss: 1.0391392707824707\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.6751708984375 | KNN Loss: 5.6036577224731445 | BCE Loss: 1.0715131759643555\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.631143093109131 | KNN Loss: 5.59579610824585 | BCE Loss: 1.0353468656539917\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.687911033630371 | KNN Loss: 5.621789932250977 | BCE Loss: 1.0661213397979736\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.652606010437012 | KNN Loss: 5.617794513702393 | BCE Loss: 1.0348117351531982\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.663381576538086 | KNN Loss: 5.611076354980469 | BCE Loss: 1.0523052215576172\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.708922386169434 | KNN Loss: 5.651787281036377 | BCE Loss: 1.0571348667144775\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.657970428466797 | KNN Loss: 5.626979351043701 | BCE Loss: 1.0309911966323853\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.766045093536377 | KNN Loss: 5.695711135864258 | BCE Loss: 1.0703339576721191\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.802917003631592 | KNN Loss: 5.74050235748291 | BCE Loss: 1.062414526939392\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 6.672987937927246 | KNN Loss: 5.603867053985596 | BCE Loss: 1.0691206455230713\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.648158073425293 | KNN Loss: 5.603809833526611 | BCE Loss: 1.0443483591079712\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.642247676849365 | KNN Loss: 5.607817649841309 | BCE Loss: 1.0344301462173462\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.7361016273498535 | KNN Loss: 5.683193683624268 | BCE Loss: 1.052907943725586\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.730815887451172 | KNN Loss: 5.678908348083496 | BCE Loss: 1.0519073009490967\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.720983982086182 | KNN Loss: 5.655911922454834 | BCE Loss: 1.065071940422058\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.691658973693848 | KNN Loss: 5.641707897186279 | BCE Loss: 1.0499508380889893\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.645153045654297 | KNN Loss: 5.629528522491455 | BCE Loss: 1.015624761581421\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.6740288734436035 | KNN Loss: 5.620323181152344 | BCE Loss: 1.0537058115005493\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.684330463409424 | KNN Loss: 5.626031398773193 | BCE Loss: 1.05829918384552\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.642869472503662 | KNN Loss: 5.600117206573486 | BCE Loss: 1.0427521467208862\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 6.683143138885498 | KNN Loss: 5.615270614624023 | BCE Loss: 1.067872405052185\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.7053022384643555 | KNN Loss: 5.652919769287109 | BCE Loss: 1.052382469177246\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.6507439613342285 | KNN Loss: 5.596042156219482 | BCE Loss: 1.054701805114746\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.700542449951172 | KNN Loss: 5.628606796264648 | BCE Loss: 1.0719356536865234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.657561302185059 | KNN Loss: 5.61644983291626 | BCE Loss: 1.041111707687378\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.661438941955566 | KNN Loss: 5.606329441070557 | BCE Loss: 1.0551093816757202\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.664345741271973 | KNN Loss: 5.604970455169678 | BCE Loss: 1.059375286102295\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 6.641066551208496 | KNN Loss: 5.618422031402588 | BCE Loss: 1.0226445198059082\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.684414863586426 | KNN Loss: 5.6556782722473145 | BCE Loss: 1.0287363529205322\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.700927257537842 | KNN Loss: 5.642199993133545 | BCE Loss: 1.0587271451950073\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 6.705483436584473 | KNN Loss: 5.661896705627441 | BCE Loss: 1.0435867309570312\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.67356014251709 | KNN Loss: 5.643574237823486 | BCE Loss: 1.0299859046936035\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.712912559509277 | KNN Loss: 5.664193630218506 | BCE Loss: 1.0487191677093506\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.669623374938965 | KNN Loss: 5.65425443649292 | BCE Loss: 1.015369176864624\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.679769039154053 | KNN Loss: 5.627937316894531 | BCE Loss: 1.0518317222595215\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.724688529968262 | KNN Loss: 5.657532215118408 | BCE Loss: 1.067156434059143\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.644522666931152 | KNN Loss: 5.615007400512695 | BCE Loss: 1.0295155048370361\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.6847124099731445 | KNN Loss: 5.650404930114746 | BCE Loss: 1.0343073606491089\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.759296894073486 | KNN Loss: 5.698166847229004 | BCE Loss: 1.0611300468444824\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.681234836578369 | KNN Loss: 5.645079612731934 | BCE Loss: 1.036155343055725\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.672199249267578 | KNN Loss: 5.646644592285156 | BCE Loss: 1.0255547761917114\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.674658298492432 | KNN Loss: 5.613628387451172 | BCE Loss: 1.0610299110412598\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.667547225952148 | KNN Loss: 5.631943702697754 | BCE Loss: 1.0356037616729736\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.771785259246826 | KNN Loss: 5.7050909996032715 | BCE Loss: 1.0666942596435547\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.68544864654541 | KNN Loss: 5.666390419006348 | BCE Loss: 1.0190582275390625\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.677456855773926 | KNN Loss: 5.643675804138184 | BCE Loss: 1.0337811708450317\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.719618797302246 | KNN Loss: 5.675623416900635 | BCE Loss: 1.0439953804016113\n",
      "Epoch   378: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.689369201660156 | KNN Loss: 5.6323442459106445 | BCE Loss: 1.0570247173309326\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.630045413970947 | KNN Loss: 5.618401527404785 | BCE Loss: 1.0116440057754517\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.663270950317383 | KNN Loss: 5.637272834777832 | BCE Loss: 1.0259978771209717\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.685896873474121 | KNN Loss: 5.627519607543945 | BCE Loss: 1.0583772659301758\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.664494514465332 | KNN Loss: 5.6156535148620605 | BCE Loss: 1.0488409996032715\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.693727970123291 | KNN Loss: 5.619024753570557 | BCE Loss: 1.074703335762024\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.6787004470825195 | KNN Loss: 5.599970817565918 | BCE Loss: 1.0787298679351807\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.666343688964844 | KNN Loss: 5.636923789978027 | BCE Loss: 1.0294201374053955\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.694140434265137 | KNN Loss: 5.63960599899292 | BCE Loss: 1.0545343160629272\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.656402111053467 | KNN Loss: 5.662987232208252 | BCE Loss: 0.9934147000312805\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.6817731857299805 | KNN Loss: 5.627121925354004 | BCE Loss: 1.0546513795852661\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.6520843505859375 | KNN Loss: 5.611993312835693 | BCE Loss: 1.0400910377502441\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.715009689331055 | KNN Loss: 5.667521953582764 | BCE Loss: 1.047487735748291\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.679089069366455 | KNN Loss: 5.627665042877197 | BCE Loss: 1.0514240264892578\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.649174690246582 | KNN Loss: 5.607326030731201 | BCE Loss: 1.04184889793396\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.6489949226379395 | KNN Loss: 5.601337432861328 | BCE Loss: 1.0476574897766113\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.731473922729492 | KNN Loss: 5.664963245391846 | BCE Loss: 1.0665109157562256\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.679255485534668 | KNN Loss: 5.623469829559326 | BCE Loss: 1.0557856559753418\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.696961402893066 | KNN Loss: 5.652881622314453 | BCE Loss: 1.0440800189971924\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.689263343811035 | KNN Loss: 5.628182411193848 | BCE Loss: 1.061081051826477\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.713369846343994 | KNN Loss: 5.669390678405762 | BCE Loss: 1.0439791679382324\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.706549167633057 | KNN Loss: 5.642230033874512 | BCE Loss: 1.0643190145492554\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.680876731872559 | KNN Loss: 5.611710548400879 | BCE Loss: 1.0691664218902588\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.6757636070251465 | KNN Loss: 5.623960494995117 | BCE Loss: 1.0518031120300293\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.676692962646484 | KNN Loss: 5.613211154937744 | BCE Loss: 1.0634816884994507\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.697293281555176 | KNN Loss: 5.61086368560791 | BCE Loss: 1.086429476737976\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.647608757019043 | KNN Loss: 5.60036563873291 | BCE Loss: 1.047243356704712\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.693820953369141 | KNN Loss: 5.648631572723389 | BCE Loss: 1.045189380645752\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.680704116821289 | KNN Loss: 5.6311211585998535 | BCE Loss: 1.0495829582214355\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.712067604064941 | KNN Loss: 5.657028675079346 | BCE Loss: 1.0550388097763062\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 6.688778877258301 | KNN Loss: 5.664497375488281 | BCE Loss: 1.024281620979309\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.772647380828857 | KNN Loss: 5.6967926025390625 | BCE Loss: 1.0758548974990845\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.737671852111816 | KNN Loss: 5.677401542663574 | BCE Loss: 1.0602705478668213\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.7684736251831055 | KNN Loss: 5.720126152038574 | BCE Loss: 1.0483477115631104\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.727630138397217 | KNN Loss: 5.6764326095581055 | BCE Loss: 1.0511975288391113\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.666975975036621 | KNN Loss: 5.617434978485107 | BCE Loss: 1.0495408773422241\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.672119617462158 | KNN Loss: 5.648179054260254 | BCE Loss: 1.0239405632019043\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 6.714254379272461 | KNN Loss: 5.651052474975586 | BCE Loss: 1.063202142715454\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.7068705558776855 | KNN Loss: 5.64426851272583 | BCE Loss: 1.062602162361145\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 6.648899555206299 | KNN Loss: 5.608013153076172 | BCE Loss: 1.040886402130127\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.6481733322143555 | KNN Loss: 5.61090612411499 | BCE Loss: 1.0372670888900757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.713467597961426 | KNN Loss: 5.680002212524414 | BCE Loss: 1.0334655046463013\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.670261383056641 | KNN Loss: 5.607552528381348 | BCE Loss: 1.062709093093872\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.681747913360596 | KNN Loss: 5.635590553283691 | BCE Loss: 1.0461574792861938\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.688969135284424 | KNN Loss: 5.64265775680542 | BCE Loss: 1.0463112592697144\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.640787124633789 | KNN Loss: 5.6023454666137695 | BCE Loss: 1.0384414196014404\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.720127582550049 | KNN Loss: 5.6583356857299805 | BCE Loss: 1.0617918968200684\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.681565284729004 | KNN Loss: 5.5961012840271 | BCE Loss: 1.0854637622833252\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.627669334411621 | KNN Loss: 5.597972393035889 | BCE Loss: 1.0296967029571533\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 6.717634677886963 | KNN Loss: 5.640988826751709 | BCE Loss: 1.076645851135254\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.739100933074951 | KNN Loss: 5.698125839233398 | BCE Loss: 1.0409750938415527\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.653355598449707 | KNN Loss: 5.597034931182861 | BCE Loss: 1.0563206672668457\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.741345405578613 | KNN Loss: 5.681634902954102 | BCE Loss: 1.0597105026245117\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.6590375900268555 | KNN Loss: 5.623584747314453 | BCE Loss: 1.0354526042938232\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 6.678840637207031 | KNN Loss: 5.641735076904297 | BCE Loss: 1.0371054410934448\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.71538782119751 | KNN Loss: 5.651157855987549 | BCE Loss: 1.064229965209961\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.66229248046875 | KNN Loss: 5.601351737976074 | BCE Loss: 1.0609409809112549\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.692908763885498 | KNN Loss: 5.634949684143066 | BCE Loss: 1.0579591989517212\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.670584201812744 | KNN Loss: 5.619012832641602 | BCE Loss: 1.051571249961853\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.739798545837402 | KNN Loss: 5.692474842071533 | BCE Loss: 1.0473239421844482\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.720451831817627 | KNN Loss: 5.641071796417236 | BCE Loss: 1.0793801546096802\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.658632278442383 | KNN Loss: 5.612981796264648 | BCE Loss: 1.0456502437591553\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.726757049560547 | KNN Loss: 5.668307781219482 | BCE Loss: 1.0584490299224854\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.683007717132568 | KNN Loss: 5.623824596405029 | BCE Loss: 1.0591832399368286\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.668139457702637 | KNN Loss: 5.635067939758301 | BCE Loss: 1.0330712795257568\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.626551628112793 | KNN Loss: 5.599846839904785 | BCE Loss: 1.0267045497894287\n",
      "Epoch   389: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.665423393249512 | KNN Loss: 5.614906311035156 | BCE Loss: 1.0505168437957764\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.711794853210449 | KNN Loss: 5.684478282928467 | BCE Loss: 1.027316689491272\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.714520454406738 | KNN Loss: 5.679490566253662 | BCE Loss: 1.035029649734497\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.76392126083374 | KNN Loss: 5.6833624839782715 | BCE Loss: 1.0805586576461792\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 6.6633782386779785 | KNN Loss: 5.644186019897461 | BCE Loss: 1.0191922187805176\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 6.715456962585449 | KNN Loss: 5.666077613830566 | BCE Loss: 1.049379587173462\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 6.654789447784424 | KNN Loss: 5.604712009429932 | BCE Loss: 1.0500775575637817\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.72463321685791 | KNN Loss: 5.674567222595215 | BCE Loss: 1.0500659942626953\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 6.613389492034912 | KNN Loss: 5.601449966430664 | BCE Loss: 1.011939525604248\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 6.711056709289551 | KNN Loss: 5.664541244506836 | BCE Loss: 1.046515703201294\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.692875862121582 | KNN Loss: 5.630369663238525 | BCE Loss: 1.0625063180923462\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.682680130004883 | KNN Loss: 5.634365081787109 | BCE Loss: 1.0483152866363525\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.671825408935547 | KNN Loss: 5.631558418273926 | BCE Loss: 1.040266752243042\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.722394943237305 | KNN Loss: 5.658679008483887 | BCE Loss: 1.063716173171997\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.703973293304443 | KNN Loss: 5.6726813316345215 | BCE Loss: 1.0312919616699219\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.68389892578125 | KNN Loss: 5.633053779602051 | BCE Loss: 1.0508449077606201\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.66190242767334 | KNN Loss: 5.632686614990234 | BCE Loss: 1.0292155742645264\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.654561996459961 | KNN Loss: 5.606211185455322 | BCE Loss: 1.0483508110046387\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.667819023132324 | KNN Loss: 5.6110100746154785 | BCE Loss: 1.0568091869354248\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.683078289031982 | KNN Loss: 5.61761999130249 | BCE Loss: 1.0654584169387817\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.641051292419434 | KNN Loss: 5.605679988861084 | BCE Loss: 1.0353710651397705\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.702301025390625 | KNN Loss: 5.624950885772705 | BCE Loss: 1.077350378036499\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.65770149230957 | KNN Loss: 5.617293834686279 | BCE Loss: 1.0404078960418701\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.722179889678955 | KNN Loss: 5.672327995300293 | BCE Loss: 1.049851894378662\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.623525619506836 | KNN Loss: 5.611590385437012 | BCE Loss: 1.0119352340698242\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.690621376037598 | KNN Loss: 5.623169422149658 | BCE Loss: 1.0674519538879395\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.723090648651123 | KNN Loss: 5.68613862991333 | BCE Loss: 1.0369521379470825\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.677130699157715 | KNN Loss: 5.645346641540527 | BCE Loss: 1.0317840576171875\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.69286584854126 | KNN Loss: 5.634988784790039 | BCE Loss: 1.0578769445419312\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 6.694415092468262 | KNN Loss: 5.641028881072998 | BCE Loss: 1.0533864498138428\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 6.691885948181152 | KNN Loss: 5.63261079788208 | BCE Loss: 1.0592753887176514\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 6.658291339874268 | KNN Loss: 5.620837688446045 | BCE Loss: 1.0374536514282227\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.700979232788086 | KNN Loss: 5.646883487701416 | BCE Loss: 1.0540955066680908\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.662848472595215 | KNN Loss: 5.6237263679504395 | BCE Loss: 1.039122223854065\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.716437816619873 | KNN Loss: 5.648259162902832 | BCE Loss: 1.068178653717041\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.652384281158447 | KNN Loss: 5.603344440460205 | BCE Loss: 1.0490398406982422\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 6.661283493041992 | KNN Loss: 5.615244388580322 | BCE Loss: 1.0460388660430908\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.676241397857666 | KNN Loss: 5.614932060241699 | BCE Loss: 1.0613093376159668\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.649738311767578 | KNN Loss: 5.614180088043213 | BCE Loss: 1.0355584621429443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 6.6987128257751465 | KNN Loss: 5.642199993133545 | BCE Loss: 1.056512713432312\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.684201240539551 | KNN Loss: 5.613606929779053 | BCE Loss: 1.070594310760498\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 6.676967620849609 | KNN Loss: 5.63129997253418 | BCE Loss: 1.0456678867340088\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.698118209838867 | KNN Loss: 5.6217241287231445 | BCE Loss: 1.0763940811157227\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.666566848754883 | KNN Loss: 5.608283996582031 | BCE Loss: 1.0582828521728516\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.641812324523926 | KNN Loss: 5.6164045333862305 | BCE Loss: 1.0254075527191162\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.696036338806152 | KNN Loss: 5.622082233428955 | BCE Loss: 1.0739542245864868\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.692816257476807 | KNN Loss: 5.640416145324707 | BCE Loss: 1.05239999294281\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 6.677811622619629 | KNN Loss: 5.649168491363525 | BCE Loss: 1.028643012046814\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.668241500854492 | KNN Loss: 5.608442306518555 | BCE Loss: 1.0597989559173584\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.673935890197754 | KNN Loss: 5.632249355316162 | BCE Loss: 1.041686773300171\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.657993316650391 | KNN Loss: 5.610649585723877 | BCE Loss: 1.0473434925079346\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.737648963928223 | KNN Loss: 5.643411636352539 | BCE Loss: 1.0942375659942627\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.684640884399414 | KNN Loss: 5.622259616851807 | BCE Loss: 1.0623810291290283\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.649278163909912 | KNN Loss: 5.617849349975586 | BCE Loss: 1.0314288139343262\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.660984039306641 | KNN Loss: 5.625730037689209 | BCE Loss: 1.0352542400360107\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.741316318511963 | KNN Loss: 5.706748962402344 | BCE Loss: 1.0345673561096191\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.754532337188721 | KNN Loss: 5.705617427825928 | BCE Loss: 1.048914909362793\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.690611839294434 | KNN Loss: 5.658044815063477 | BCE Loss: 1.0325672626495361\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.70261287689209 | KNN Loss: 5.613835334777832 | BCE Loss: 1.0887773036956787\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.714334487915039 | KNN Loss: 5.6167097091674805 | BCE Loss: 1.0976247787475586\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 6.687117099761963 | KNN Loss: 5.626420974731445 | BCE Loss: 1.0606961250305176\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 6.668309211730957 | KNN Loss: 5.625368595123291 | BCE Loss: 1.042940616607666\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.628028869628906 | KNN Loss: 5.623331546783447 | BCE Loss: 1.0046974420547485\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.7432661056518555 | KNN Loss: 5.6421027183532715 | BCE Loss: 1.101163625717163\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.710695266723633 | KNN Loss: 5.660181999206543 | BCE Loss: 1.0505132675170898\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.656024932861328 | KNN Loss: 5.618305683135986 | BCE Loss: 1.037719488143921\n",
      "Epoch   400: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.660436153411865 | KNN Loss: 5.616182327270508 | BCE Loss: 1.0442538261413574\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.644322395324707 | KNN Loss: 5.619112014770508 | BCE Loss: 1.0252106189727783\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.658336639404297 | KNN Loss: 5.620212554931641 | BCE Loss: 1.0381243228912354\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.69658899307251 | KNN Loss: 5.6407694816589355 | BCE Loss: 1.0558195114135742\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.738260746002197 | KNN Loss: 5.6946516036987305 | BCE Loss: 1.0436092615127563\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.683879852294922 | KNN Loss: 5.636817932128906 | BCE Loss: 1.0470621585845947\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.660210609436035 | KNN Loss: 5.62485933303833 | BCE Loss: 1.035351037979126\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.685301780700684 | KNN Loss: 5.636419773101807 | BCE Loss: 1.0488821268081665\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.684381008148193 | KNN Loss: 5.632868766784668 | BCE Loss: 1.0515122413635254\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.661127090454102 | KNN Loss: 5.620656490325928 | BCE Loss: 1.0404704809188843\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.763038158416748 | KNN Loss: 5.7220940589904785 | BCE Loss: 1.0409440994262695\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.731559753417969 | KNN Loss: 5.657210350036621 | BCE Loss: 1.0743496417999268\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.666372299194336 | KNN Loss: 5.639122009277344 | BCE Loss: 1.027250051498413\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.634613513946533 | KNN Loss: 5.602148056030273 | BCE Loss: 1.0324655771255493\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.65367317199707 | KNN Loss: 5.630962371826172 | BCE Loss: 1.022710919380188\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.690125465393066 | KNN Loss: 5.628275394439697 | BCE Loss: 1.0618500709533691\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.710853576660156 | KNN Loss: 5.643054008483887 | BCE Loss: 1.0677998065948486\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.680081844329834 | KNN Loss: 5.612051010131836 | BCE Loss: 1.0680309534072876\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.68584680557251 | KNN Loss: 5.613807201385498 | BCE Loss: 1.0720396041870117\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 6.698537826538086 | KNN Loss: 5.659214019775391 | BCE Loss: 1.0393239259719849\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.662971019744873 | KNN Loss: 5.626047134399414 | BCE Loss: 1.036923885345459\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.693001747131348 | KNN Loss: 5.672387599945068 | BCE Loss: 1.0206143856048584\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.683060646057129 | KNN Loss: 5.60233211517334 | BCE Loss: 1.08072829246521\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.641974449157715 | KNN Loss: 5.608489990234375 | BCE Loss: 1.0334845781326294\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.646955966949463 | KNN Loss: 5.605259895324707 | BCE Loss: 1.0416959524154663\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.726075172424316 | KNN Loss: 5.679484844207764 | BCE Loss: 1.0465902090072632\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.690064430236816 | KNN Loss: 5.632367134094238 | BCE Loss: 1.0576971769332886\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.788052082061768 | KNN Loss: 5.685322284698486 | BCE Loss: 1.1027296781539917\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.6941351890563965 | KNN Loss: 5.594568729400635 | BCE Loss: 1.0995664596557617\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.6611647605896 | KNN Loss: 5.6319260597229 | BCE Loss: 1.0292387008666992\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.693142414093018 | KNN Loss: 5.6714606285095215 | BCE Loss: 1.021681785583496\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 6.736157417297363 | KNN Loss: 5.671674728393555 | BCE Loss: 1.0644824504852295\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.664814472198486 | KNN Loss: 5.610422134399414 | BCE Loss: 1.0543923377990723\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.631647109985352 | KNN Loss: 5.620328426361084 | BCE Loss: 1.0113184452056885\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.728023052215576 | KNN Loss: 5.66021203994751 | BCE Loss: 1.067811131477356\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.650811672210693 | KNN Loss: 5.611632823944092 | BCE Loss: 1.0391789674758911\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 6.743627071380615 | KNN Loss: 5.685519218444824 | BCE Loss: 1.0581077337265015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 6.660782814025879 | KNN Loss: 5.630023002624512 | BCE Loss: 1.0307598114013672\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.725547790527344 | KNN Loss: 5.635313510894775 | BCE Loss: 1.090234398841858\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 6.670483589172363 | KNN Loss: 5.610896587371826 | BCE Loss: 1.0595868825912476\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 6.642815113067627 | KNN Loss: 5.604629993438721 | BCE Loss: 1.0381850004196167\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.663578510284424 | KNN Loss: 5.64147424697876 | BCE Loss: 1.0221043825149536\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.687146186828613 | KNN Loss: 5.636116027832031 | BCE Loss: 1.0510303974151611\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.680265426635742 | KNN Loss: 5.6080498695373535 | BCE Loss: 1.0722153186798096\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.681831359863281 | KNN Loss: 5.6144609451293945 | BCE Loss: 1.0673704147338867\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.645861625671387 | KNN Loss: 5.63419246673584 | BCE Loss: 1.0116689205169678\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.65096378326416 | KNN Loss: 5.608687400817871 | BCE Loss: 1.0422766208648682\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.694761753082275 | KNN Loss: 5.665600299835205 | BCE Loss: 1.0291614532470703\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.688704967498779 | KNN Loss: 5.640460014343262 | BCE Loss: 1.0482449531555176\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.644084930419922 | KNN Loss: 5.610809326171875 | BCE Loss: 1.0332756042480469\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.716311454772949 | KNN Loss: 5.687728404998779 | BCE Loss: 1.0285829305648804\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.6697516441345215 | KNN Loss: 5.612913608551025 | BCE Loss: 1.056838035583496\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.687595367431641 | KNN Loss: 5.647496223449707 | BCE Loss: 1.0400989055633545\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.661960601806641 | KNN Loss: 5.611562252044678 | BCE Loss: 1.050398349761963\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.666316509246826 | KNN Loss: 5.6195902824401855 | BCE Loss: 1.0467262268066406\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.721472263336182 | KNN Loss: 5.677170276641846 | BCE Loss: 1.0443021059036255\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.684809684753418 | KNN Loss: 5.612040042877197 | BCE Loss: 1.0727698802947998\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.651088237762451 | KNN Loss: 5.604184627532959 | BCE Loss: 1.0469037294387817\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.700610637664795 | KNN Loss: 5.608508110046387 | BCE Loss: 1.0921025276184082\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.70961332321167 | KNN Loss: 5.663122177124023 | BCE Loss: 1.046491026878357\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.723531723022461 | KNN Loss: 5.653568744659424 | BCE Loss: 1.069962978363037\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 6.692758560180664 | KNN Loss: 5.64860725402832 | BCE Loss: 1.0441513061523438\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.746915340423584 | KNN Loss: 5.692409038543701 | BCE Loss: 1.0545061826705933\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.677785873413086 | KNN Loss: 5.649361610412598 | BCE Loss: 1.0284241437911987\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.716670036315918 | KNN Loss: 5.653110504150391 | BCE Loss: 1.0635595321655273\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.679542541503906 | KNN Loss: 5.642404079437256 | BCE Loss: 1.0371387004852295\n",
      "Epoch   411: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.671412944793701 | KNN Loss: 5.635636329650879 | BCE Loss: 1.0357766151428223\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.7022199630737305 | KNN Loss: 5.649850845336914 | BCE Loss: 1.0523691177368164\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.691213607788086 | KNN Loss: 5.637686729431152 | BCE Loss: 1.0535268783569336\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.7304840087890625 | KNN Loss: 5.6739420890808105 | BCE Loss: 1.0565420389175415\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.7484564781188965 | KNN Loss: 5.673870086669922 | BCE Loss: 1.074586272239685\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 6.763709545135498 | KNN Loss: 5.705419063568115 | BCE Loss: 1.0582906007766724\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.682269096374512 | KNN Loss: 5.616181373596191 | BCE Loss: 1.0660874843597412\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.679877281188965 | KNN Loss: 5.638985633850098 | BCE Loss: 1.040891408920288\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.674755573272705 | KNN Loss: 5.6043853759765625 | BCE Loss: 1.0703703165054321\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.76027250289917 | KNN Loss: 5.688572883605957 | BCE Loss: 1.0716997385025024\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.72905969619751 | KNN Loss: 5.658176898956299 | BCE Loss: 1.070882797241211\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.701350688934326 | KNN Loss: 5.637353420257568 | BCE Loss: 1.0639971494674683\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.761164665222168 | KNN Loss: 5.7103776931762695 | BCE Loss: 1.0507868528366089\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.731518268585205 | KNN Loss: 5.663943767547607 | BCE Loss: 1.0675745010375977\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.652151107788086 | KNN Loss: 5.62749719619751 | BCE Loss: 1.024653673171997\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.687634468078613 | KNN Loss: 5.6630353927612305 | BCE Loss: 1.0245988368988037\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.7671990394592285 | KNN Loss: 5.692202568054199 | BCE Loss: 1.0749965906143188\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.644807815551758 | KNN Loss: 5.615257263183594 | BCE Loss: 1.029550313949585\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.664292335510254 | KNN Loss: 5.609622955322266 | BCE Loss: 1.0546692609786987\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.767144203186035 | KNN Loss: 5.6982951164245605 | BCE Loss: 1.0688488483428955\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.678764343261719 | KNN Loss: 5.616062641143799 | BCE Loss: 1.062701940536499\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.638874530792236 | KNN Loss: 5.595867156982422 | BCE Loss: 1.043007254600525\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.632663726806641 | KNN Loss: 5.605490684509277 | BCE Loss: 1.0271730422973633\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.664933681488037 | KNN Loss: 5.615481853485107 | BCE Loss: 1.0494519472122192\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.651942729949951 | KNN Loss: 5.605189323425293 | BCE Loss: 1.0467534065246582\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.690913677215576 | KNN Loss: 5.624575614929199 | BCE Loss: 1.0663379430770874\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.638721466064453 | KNN Loss: 5.6069440841674805 | BCE Loss: 1.0317776203155518\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.653024196624756 | KNN Loss: 5.609976768493652 | BCE Loss: 1.043047308921814\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.699676513671875 | KNN Loss: 5.635813236236572 | BCE Loss: 1.0638630390167236\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.700538635253906 | KNN Loss: 5.65354585647583 | BCE Loss: 1.0469928979873657\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.700107574462891 | KNN Loss: 5.653787612915039 | BCE Loss: 1.0463201999664307\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.6517205238342285 | KNN Loss: 5.629785060882568 | BCE Loss: 1.0219353437423706\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.753994464874268 | KNN Loss: 5.6723713874816895 | BCE Loss: 1.0816229581832886\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.698000907897949 | KNN Loss: 5.6334404945373535 | BCE Loss: 1.0645605325698853\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 6.681286334991455 | KNN Loss: 5.632123947143555 | BCE Loss: 1.04916250705719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.662468433380127 | KNN Loss: 5.6211724281311035 | BCE Loss: 1.0412960052490234\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.740629196166992 | KNN Loss: 5.683051586151123 | BCE Loss: 1.0575777292251587\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.644497871398926 | KNN Loss: 5.602814197540283 | BCE Loss: 1.0416839122772217\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.684334754943848 | KNN Loss: 5.643134593963623 | BCE Loss: 1.0411999225616455\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.765189170837402 | KNN Loss: 5.70035982131958 | BCE Loss: 1.0648292303085327\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.679823398590088 | KNN Loss: 5.6223673820495605 | BCE Loss: 1.0574560165405273\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.6344170570373535 | KNN Loss: 5.600895404815674 | BCE Loss: 1.0335215330123901\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.668328285217285 | KNN Loss: 5.633907318115234 | BCE Loss: 1.0344210863113403\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.771526336669922 | KNN Loss: 5.680421829223633 | BCE Loss: 1.0911047458648682\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.667762756347656 | KNN Loss: 5.605475902557373 | BCE Loss: 1.062286615371704\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.743220329284668 | KNN Loss: 5.676647186279297 | BCE Loss: 1.0665733814239502\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 6.726299285888672 | KNN Loss: 5.659051418304443 | BCE Loss: 1.0672481060028076\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.701140403747559 | KNN Loss: 5.662847518920898 | BCE Loss: 1.0382931232452393\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.6585540771484375 | KNN Loss: 5.619546890258789 | BCE Loss: 1.0390069484710693\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.658010482788086 | KNN Loss: 5.628407955169678 | BCE Loss: 1.029602289199829\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.846810340881348 | KNN Loss: 5.7879133224487305 | BCE Loss: 1.0588972568511963\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.764869689941406 | KNN Loss: 5.679503917694092 | BCE Loss: 1.085365653038025\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.735297203063965 | KNN Loss: 5.668494701385498 | BCE Loss: 1.0668023824691772\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.66616678237915 | KNN Loss: 5.621501445770264 | BCE Loss: 1.0446654558181763\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.7169294357299805 | KNN Loss: 5.663978576660156 | BCE Loss: 1.0529507398605347\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.714827060699463 | KNN Loss: 5.6155195236206055 | BCE Loss: 1.0993075370788574\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.751282215118408 | KNN Loss: 5.6948347091674805 | BCE Loss: 1.0564475059509277\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.64726448059082 | KNN Loss: 5.60383415222168 | BCE Loss: 1.0434303283691406\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.689751625061035 | KNN Loss: 5.643228054046631 | BCE Loss: 1.0465238094329834\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.715354919433594 | KNN Loss: 5.681385040283203 | BCE Loss: 1.0339698791503906\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.732853889465332 | KNN Loss: 5.669501781463623 | BCE Loss: 1.063352108001709\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.639023780822754 | KNN Loss: 5.616293430328369 | BCE Loss: 1.0227302312850952\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.7661824226379395 | KNN Loss: 5.701725006103516 | BCE Loss: 1.0644574165344238\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.689053535461426 | KNN Loss: 5.630049228668213 | BCE Loss: 1.059004545211792\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.655852317810059 | KNN Loss: 5.622742652893066 | BCE Loss: 1.0331097841262817\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.686393737792969 | KNN Loss: 5.637901306152344 | BCE Loss: 1.0484923124313354\n",
      "Epoch   422: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.737757682800293 | KNN Loss: 5.709867477416992 | BCE Loss: 1.0278903245925903\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.702117443084717 | KNN Loss: 5.669257640838623 | BCE Loss: 1.0328599214553833\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.651607513427734 | KNN Loss: 5.6096272468566895 | BCE Loss: 1.0419801473617554\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.734945774078369 | KNN Loss: 5.698546409606934 | BCE Loss: 1.036399245262146\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 6.673128128051758 | KNN Loss: 5.608660697937012 | BCE Loss: 1.064467430114746\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.638053894042969 | KNN Loss: 5.611494064331055 | BCE Loss: 1.026559829711914\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.66671085357666 | KNN Loss: 5.622078895568848 | BCE Loss: 1.0446319580078125\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.631712913513184 | KNN Loss: 5.6014580726623535 | BCE Loss: 1.0302547216415405\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.666175842285156 | KNN Loss: 5.6128058433532715 | BCE Loss: 1.0533702373504639\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.700725555419922 | KNN Loss: 5.625364303588867 | BCE Loss: 1.0753612518310547\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.673286437988281 | KNN Loss: 5.610474586486816 | BCE Loss: 1.0628116130828857\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.642492294311523 | KNN Loss: 5.642292022705078 | BCE Loss: 1.0002005100250244\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.695310115814209 | KNN Loss: 5.658154487609863 | BCE Loss: 1.0371556282043457\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.793600559234619 | KNN Loss: 5.735330581665039 | BCE Loss: 1.0582698583602905\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.720933437347412 | KNN Loss: 5.6722731590271 | BCE Loss: 1.048660159111023\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.70258092880249 | KNN Loss: 5.673877716064453 | BCE Loss: 1.0287030935287476\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.647049903869629 | KNN Loss: 5.608848571777344 | BCE Loss: 1.0382014513015747\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 6.737301349639893 | KNN Loss: 5.679254531860352 | BCE Loss: 1.058046817779541\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 6.767326831817627 | KNN Loss: 5.703044414520264 | BCE Loss: 1.0642822980880737\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.714993476867676 | KNN Loss: 5.684517860412598 | BCE Loss: 1.0304758548736572\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.6889495849609375 | KNN Loss: 5.612519264221191 | BCE Loss: 1.076430320739746\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.663129806518555 | KNN Loss: 5.6106438636779785 | BCE Loss: 1.0524858236312866\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.675628185272217 | KNN Loss: 5.623426914215088 | BCE Loss: 1.052201271057129\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.62550163269043 | KNN Loss: 5.606103897094727 | BCE Loss: 1.0193977355957031\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.688174724578857 | KNN Loss: 5.6126389503479 | BCE Loss: 1.0755356550216675\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.688234329223633 | KNN Loss: 5.631931781768799 | BCE Loss: 1.0563026666641235\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.659123420715332 | KNN Loss: 5.616008281707764 | BCE Loss: 1.0431149005889893\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.695302963256836 | KNN Loss: 5.648097991943359 | BCE Loss: 1.0472047328948975\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.628718376159668 | KNN Loss: 5.613485336303711 | BCE Loss: 1.0152332782745361\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 6.724529266357422 | KNN Loss: 5.69497537612915 | BCE Loss: 1.0295541286468506\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.67025089263916 | KNN Loss: 5.622069835662842 | BCE Loss: 1.0481810569763184\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.641617298126221 | KNN Loss: 5.605778217315674 | BCE Loss: 1.0358390808105469\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.670685768127441 | KNN Loss: 5.617065906524658 | BCE Loss: 1.053619623184204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.734557151794434 | KNN Loss: 5.708924293518066 | BCE Loss: 1.0256330966949463\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.654829025268555 | KNN Loss: 5.6163649559021 | BCE Loss: 1.0384643077850342\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.65602970123291 | KNN Loss: 5.600555896759033 | BCE Loss: 1.0554739236831665\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.622618198394775 | KNN Loss: 5.600786209106445 | BCE Loss: 1.0218318700790405\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.678993225097656 | KNN Loss: 5.610630989074707 | BCE Loss: 1.0683622360229492\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.718871593475342 | KNN Loss: 5.6543660163879395 | BCE Loss: 1.0645055770874023\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.674161434173584 | KNN Loss: 5.634287357330322 | BCE Loss: 1.0398739576339722\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.674372673034668 | KNN Loss: 5.616437911987305 | BCE Loss: 1.0579346418380737\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.6468706130981445 | KNN Loss: 5.603363513946533 | BCE Loss: 1.0435070991516113\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.67542839050293 | KNN Loss: 5.638364791870117 | BCE Loss: 1.0370633602142334\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.691886901855469 | KNN Loss: 5.632657051086426 | BCE Loss: 1.0592299699783325\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.663934230804443 | KNN Loss: 5.622501850128174 | BCE Loss: 1.04143226146698\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.656663417816162 | KNN Loss: 5.604294300079346 | BCE Loss: 1.0523689985275269\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.6751885414123535 | KNN Loss: 5.621190071105957 | BCE Loss: 1.053998351097107\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.653964042663574 | KNN Loss: 5.6035284996032715 | BCE Loss: 1.0504357814788818\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.65537166595459 | KNN Loss: 5.612729549407959 | BCE Loss: 1.0426418781280518\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.669281959533691 | KNN Loss: 5.6222310066223145 | BCE Loss: 1.047050952911377\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.676328659057617 | KNN Loss: 5.631686210632324 | BCE Loss: 1.044642448425293\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.745636940002441 | KNN Loss: 5.67045783996582 | BCE Loss: 1.0751793384552002\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.646242618560791 | KNN Loss: 5.606365203857422 | BCE Loss: 1.0398775339126587\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.645933151245117 | KNN Loss: 5.60749626159668 | BCE Loss: 1.0384371280670166\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.64376163482666 | KNN Loss: 5.61546516418457 | BCE Loss: 1.028296709060669\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.677823066711426 | KNN Loss: 5.639440059661865 | BCE Loss: 1.0383832454681396\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 6.641360282897949 | KNN Loss: 5.608548164367676 | BCE Loss: 1.0328123569488525\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.7039594650268555 | KNN Loss: 5.625936031341553 | BCE Loss: 1.0780235528945923\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.670642852783203 | KNN Loss: 5.616779804229736 | BCE Loss: 1.0538631677627563\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.668520927429199 | KNN Loss: 5.63598108291626 | BCE Loss: 1.0325396060943604\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.731393814086914 | KNN Loss: 5.661321640014648 | BCE Loss: 1.0700722932815552\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.730472564697266 | KNN Loss: 5.652285575866699 | BCE Loss: 1.0781868696212769\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 6.660896301269531 | KNN Loss: 5.616058826446533 | BCE Loss: 1.044837236404419\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.69659423828125 | KNN Loss: 5.652416229248047 | BCE Loss: 1.044177770614624\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.697628498077393 | KNN Loss: 5.629040241241455 | BCE Loss: 1.0685882568359375\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.654247283935547 | KNN Loss: 5.622619152069092 | BCE Loss: 1.0316283702850342\n",
      "Epoch   433: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.649692535400391 | KNN Loss: 5.610154628753662 | BCE Loss: 1.039537787437439\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.635125160217285 | KNN Loss: 5.616816997528076 | BCE Loss: 1.0183079242706299\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.672913074493408 | KNN Loss: 5.642117500305176 | BCE Loss: 1.0307955741882324\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.628978729248047 | KNN Loss: 5.605923652648926 | BCE Loss: 1.023055076599121\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.675998687744141 | KNN Loss: 5.608520984649658 | BCE Loss: 1.0674774646759033\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.662148475646973 | KNN Loss: 5.61030387878418 | BCE Loss: 1.0518443584442139\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.757272720336914 | KNN Loss: 5.7051262855529785 | BCE Loss: 1.0521461963653564\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.668381690979004 | KNN Loss: 5.599837779998779 | BCE Loss: 1.0685441493988037\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.676033020019531 | KNN Loss: 5.642828464508057 | BCE Loss: 1.0332045555114746\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.689289093017578 | KNN Loss: 5.614348411560059 | BCE Loss: 1.0749406814575195\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.648780822753906 | KNN Loss: 5.609958648681641 | BCE Loss: 1.0388221740722656\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.679197311401367 | KNN Loss: 5.628529071807861 | BCE Loss: 1.050668478012085\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.658500671386719 | KNN Loss: 5.596883296966553 | BCE Loss: 1.061617374420166\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.665041923522949 | KNN Loss: 5.611416339874268 | BCE Loss: 1.0536253452301025\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 6.684099197387695 | KNN Loss: 5.620145320892334 | BCE Loss: 1.0639536380767822\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.627743721008301 | KNN Loss: 5.62072229385376 | BCE Loss: 1.0070213079452515\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.681257724761963 | KNN Loss: 5.641712665557861 | BCE Loss: 1.0395450592041016\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.649385929107666 | KNN Loss: 5.627594947814941 | BCE Loss: 1.0217909812927246\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.793829917907715 | KNN Loss: 5.74418306350708 | BCE Loss: 1.0496470928192139\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.808075904846191 | KNN Loss: 5.7555341720581055 | BCE Loss: 1.0525418519973755\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.6383867263793945 | KNN Loss: 5.604869842529297 | BCE Loss: 1.0335168838500977\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.692941665649414 | KNN Loss: 5.65484619140625 | BCE Loss: 1.038095474243164\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.675102233886719 | KNN Loss: 5.6100053787231445 | BCE Loss: 1.0650967359542847\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.754500389099121 | KNN Loss: 5.695855140686035 | BCE Loss: 1.0586450099945068\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 6.7040934562683105 | KNN Loss: 5.651085376739502 | BCE Loss: 1.053007960319519\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.746294021606445 | KNN Loss: 5.682098865509033 | BCE Loss: 1.064195156097412\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.740216255187988 | KNN Loss: 5.700118541717529 | BCE Loss: 1.0400974750518799\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.657875061035156 | KNN Loss: 5.609608173370361 | BCE Loss: 1.0482666492462158\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.709103107452393 | KNN Loss: 5.659000873565674 | BCE Loss: 1.0501022338867188\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.684688091278076 | KNN Loss: 5.629410266876221 | BCE Loss: 1.055277943611145\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.655633449554443 | KNN Loss: 5.612093448638916 | BCE Loss: 1.0435400009155273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.7002787590026855 | KNN Loss: 5.645789623260498 | BCE Loss: 1.0544891357421875\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.699611663818359 | KNN Loss: 5.6629557609558105 | BCE Loss: 1.0366556644439697\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.684426307678223 | KNN Loss: 5.6183881759643555 | BCE Loss: 1.0660383701324463\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.675989151000977 | KNN Loss: 5.623812675476074 | BCE Loss: 1.0521767139434814\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.678472518920898 | KNN Loss: 5.646635055541992 | BCE Loss: 1.0318377017974854\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.706539154052734 | KNN Loss: 5.634246349334717 | BCE Loss: 1.0722928047180176\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.640414714813232 | KNN Loss: 5.60800838470459 | BCE Loss: 1.0324063301086426\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.63592529296875 | KNN Loss: 5.605398178100586 | BCE Loss: 1.030526876449585\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.638415336608887 | KNN Loss: 5.614998817443848 | BCE Loss: 1.023416519165039\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.651215553283691 | KNN Loss: 5.615786075592041 | BCE Loss: 1.0354297161102295\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.6984405517578125 | KNN Loss: 5.63435173034668 | BCE Loss: 1.0640885829925537\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.68975830078125 | KNN Loss: 5.642134666442871 | BCE Loss: 1.0476237535476685\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.711233139038086 | KNN Loss: 5.634521007537842 | BCE Loss: 1.0767122507095337\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.676446914672852 | KNN Loss: 5.616161346435547 | BCE Loss: 1.0602858066558838\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.695694446563721 | KNN Loss: 5.657981872558594 | BCE Loss: 1.037712574005127\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.651806354522705 | KNN Loss: 5.596771717071533 | BCE Loss: 1.0550345182418823\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.698392868041992 | KNN Loss: 5.663074970245361 | BCE Loss: 1.0353180170059204\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.699315071105957 | KNN Loss: 5.683905124664307 | BCE Loss: 1.0154097080230713\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.676820755004883 | KNN Loss: 5.62498140335083 | BCE Loss: 1.0518391132354736\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.714044094085693 | KNN Loss: 5.642908573150635 | BCE Loss: 1.0711355209350586\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.64526891708374 | KNN Loss: 5.611271381378174 | BCE Loss: 1.0339974164962769\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.697948455810547 | KNN Loss: 5.6482014656066895 | BCE Loss: 1.0497472286224365\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.665602207183838 | KNN Loss: 5.612843990325928 | BCE Loss: 1.0527582168579102\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.677463054656982 | KNN Loss: 5.615562915802002 | BCE Loss: 1.0619001388549805\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.6514973640441895 | KNN Loss: 5.61136531829834 | BCE Loss: 1.0401320457458496\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.694741725921631 | KNN Loss: 5.661360740661621 | BCE Loss: 1.0333808660507202\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.653862476348877 | KNN Loss: 5.616490840911865 | BCE Loss: 1.0373716354370117\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.696875095367432 | KNN Loss: 5.621492385864258 | BCE Loss: 1.0753827095031738\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.675368309020996 | KNN Loss: 5.629154205322266 | BCE Loss: 1.0462138652801514\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.688666820526123 | KNN Loss: 5.630228042602539 | BCE Loss: 1.0584386587142944\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 6.659938812255859 | KNN Loss: 5.611433982849121 | BCE Loss: 1.0485050678253174\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.6063995361328125 | KNN Loss: 5.599429607391357 | BCE Loss: 1.0069701671600342\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.661937713623047 | KNN Loss: 5.604332447052002 | BCE Loss: 1.0576051473617554\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.765636920928955 | KNN Loss: 5.720827102661133 | BCE Loss: 1.0448096990585327\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.6328935623168945 | KNN Loss: 5.6059441566467285 | BCE Loss: 1.026949405670166\n",
      "Epoch   444: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.668993949890137 | KNN Loss: 5.642630100250244 | BCE Loss: 1.0263640880584717\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.628064155578613 | KNN Loss: 5.6129326820373535 | BCE Loss: 1.0151317119598389\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.742378234863281 | KNN Loss: 5.66048002243042 | BCE Loss: 1.0818983316421509\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.709902763366699 | KNN Loss: 5.661191463470459 | BCE Loss: 1.0487115383148193\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.637404441833496 | KNN Loss: 5.616517066955566 | BCE Loss: 1.0208871364593506\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.708013534545898 | KNN Loss: 5.669018745422363 | BCE Loss: 1.0389947891235352\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.654158115386963 | KNN Loss: 5.624472618103027 | BCE Loss: 1.029685616493225\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.689979553222656 | KNN Loss: 5.650869846343994 | BCE Loss: 1.0391095876693726\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.690603256225586 | KNN Loss: 5.650108337402344 | BCE Loss: 1.0404947996139526\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.70081901550293 | KNN Loss: 5.648580074310303 | BCE Loss: 1.0522387027740479\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.691327095031738 | KNN Loss: 5.640289306640625 | BCE Loss: 1.0510375499725342\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.732792854309082 | KNN Loss: 5.668146133422852 | BCE Loss: 1.0646469593048096\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.7016096115112305 | KNN Loss: 5.655240535736084 | BCE Loss: 1.0463688373565674\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.710077285766602 | KNN Loss: 5.660696506500244 | BCE Loss: 1.0493807792663574\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.6882734298706055 | KNN Loss: 5.644070148468018 | BCE Loss: 1.044203281402588\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.659801959991455 | KNN Loss: 5.600303649902344 | BCE Loss: 1.0594983100891113\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 6.647848129272461 | KNN Loss: 5.6110334396362305 | BCE Loss: 1.0368146896362305\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.665675163269043 | KNN Loss: 5.637764930725098 | BCE Loss: 1.0279104709625244\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.721151351928711 | KNN Loss: 5.668551921844482 | BCE Loss: 1.0525996685028076\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.69132661819458 | KNN Loss: 5.620631694793701 | BCE Loss: 1.070694923400879\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.7584919929504395 | KNN Loss: 5.66459846496582 | BCE Loss: 1.0938936471939087\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.704770088195801 | KNN Loss: 5.654298782348633 | BCE Loss: 1.050471544265747\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 6.744321346282959 | KNN Loss: 5.683173179626465 | BCE Loss: 1.0611481666564941\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 6.652295112609863 | KNN Loss: 5.614385604858398 | BCE Loss: 1.0379095077514648\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.685622692108154 | KNN Loss: 5.655104637145996 | BCE Loss: 1.0305181741714478\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.639345169067383 | KNN Loss: 5.610034942626953 | BCE Loss: 1.0293102264404297\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.659731388092041 | KNN Loss: 5.595123767852783 | BCE Loss: 1.0646076202392578\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.687708854675293 | KNN Loss: 5.63299036026001 | BCE Loss: 1.0547184944152832\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.659574508666992 | KNN Loss: 5.615450382232666 | BCE Loss: 1.0441240072250366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.693940162658691 | KNN Loss: 5.643997669219971 | BCE Loss: 1.0499427318572998\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.675162315368652 | KNN Loss: 5.601166248321533 | BCE Loss: 1.07399582862854\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.631659984588623 | KNN Loss: 5.604273319244385 | BCE Loss: 1.0273866653442383\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.649306297302246 | KNN Loss: 5.607953071594238 | BCE Loss: 1.0413532257080078\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 6.697667598724365 | KNN Loss: 5.652248859405518 | BCE Loss: 1.0454188585281372\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 6.626442909240723 | KNN Loss: 5.607489585876465 | BCE Loss: 1.0189530849456787\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 6.717464447021484 | KNN Loss: 5.687590599060059 | BCE Loss: 1.0298739671707153\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.659090042114258 | KNN Loss: 5.6447601318359375 | BCE Loss: 1.0143296718597412\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.706103324890137 | KNN Loss: 5.662818908691406 | BCE Loss: 1.0432841777801514\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.69017219543457 | KNN Loss: 5.641828536987305 | BCE Loss: 1.0483436584472656\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.708767890930176 | KNN Loss: 5.644041061401367 | BCE Loss: 1.0647268295288086\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 6.75581169128418 | KNN Loss: 5.711300849914551 | BCE Loss: 1.0445106029510498\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.692915439605713 | KNN Loss: 5.6053972244262695 | BCE Loss: 1.0875180959701538\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.688331127166748 | KNN Loss: 5.637406349182129 | BCE Loss: 1.0509246587753296\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.637569904327393 | KNN Loss: 5.614925384521484 | BCE Loss: 1.0226445198059082\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 6.652274131774902 | KNN Loss: 5.607391357421875 | BCE Loss: 1.0448825359344482\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 6.714812278747559 | KNN Loss: 5.650238037109375 | BCE Loss: 1.0645744800567627\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 6.636689186096191 | KNN Loss: 5.600396633148193 | BCE Loss: 1.0362926721572876\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.700164794921875 | KNN Loss: 5.622552394866943 | BCE Loss: 1.077612280845642\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.685111999511719 | KNN Loss: 5.638096809387207 | BCE Loss: 1.0470151901245117\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.648428440093994 | KNN Loss: 5.600471496582031 | BCE Loss: 1.0479568243026733\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.687105178833008 | KNN Loss: 5.640573024749756 | BCE Loss: 1.0465319156646729\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.6948981285095215 | KNN Loss: 5.630067348480225 | BCE Loss: 1.0648306608200073\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 6.651219367980957 | KNN Loss: 5.603254795074463 | BCE Loss: 1.0479648113250732\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.691408157348633 | KNN Loss: 5.671853065490723 | BCE Loss: 1.019554853439331\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.690504550933838 | KNN Loss: 5.619434833526611 | BCE Loss: 1.0710697174072266\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.657808303833008 | KNN Loss: 5.6275811195373535 | BCE Loss: 1.0302271842956543\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.633949279785156 | KNN Loss: 5.604020118713379 | BCE Loss: 1.0299293994903564\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.670685291290283 | KNN Loss: 5.648214340209961 | BCE Loss: 1.0224709510803223\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.652602672576904 | KNN Loss: 5.618869781494141 | BCE Loss: 1.0337330102920532\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.643050193786621 | KNN Loss: 5.609963893890381 | BCE Loss: 1.0330864191055298\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 6.672836780548096 | KNN Loss: 5.621974468231201 | BCE Loss: 1.050862431526184\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.626338958740234 | KNN Loss: 5.600278377532959 | BCE Loss: 1.0260604619979858\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 6.752066612243652 | KNN Loss: 5.661073684692383 | BCE Loss: 1.0909931659698486\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 6.713558673858643 | KNN Loss: 5.654006481170654 | BCE Loss: 1.0595523118972778\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.64813232421875 | KNN Loss: 5.595757484436035 | BCE Loss: 1.0523746013641357\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.808625221252441 | KNN Loss: 5.768516540527344 | BCE Loss: 1.0401084423065186\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.643215179443359 | KNN Loss: 5.609508514404297 | BCE Loss: 1.0337064266204834\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.683467388153076 | KNN Loss: 5.643949031829834 | BCE Loss: 1.0395183563232422\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.651262283325195 | KNN Loss: 5.618804931640625 | BCE Loss: 1.0324573516845703\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.7103986740112305 | KNN Loss: 5.610401153564453 | BCE Loss: 1.0999974012374878\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.710716724395752 | KNN Loss: 5.662898063659668 | BCE Loss: 1.0478187799453735\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.6612372398376465 | KNN Loss: 5.612665176391602 | BCE Loss: 1.0485719442367554\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.6684722900390625 | KNN Loss: 5.601653099060059 | BCE Loss: 1.066819429397583\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.66141939163208 | KNN Loss: 5.6053056716918945 | BCE Loss: 1.0561137199401855\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.637184143066406 | KNN Loss: 5.609489917755127 | BCE Loss: 1.0276939868927002\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.666696548461914 | KNN Loss: 5.617860317230225 | BCE Loss: 1.0488359928131104\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 6.718443393707275 | KNN Loss: 5.662644386291504 | BCE Loss: 1.0557990074157715\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 6.660736083984375 | KNN Loss: 5.601980686187744 | BCE Loss: 1.0587551593780518\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.711671352386475 | KNN Loss: 5.6666460037231445 | BCE Loss: 1.0450252294540405\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.712320327758789 | KNN Loss: 5.674915790557861 | BCE Loss: 1.0374045372009277\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.662069320678711 | KNN Loss: 5.6179046630859375 | BCE Loss: 1.044164776802063\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.663227081298828 | KNN Loss: 5.602268218994141 | BCE Loss: 1.060958743095398\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.815896987915039 | KNN Loss: 5.740474224090576 | BCE Loss: 1.0754225254058838\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.63964319229126 | KNN Loss: 5.611122131347656 | BCE Loss: 1.028521180152893\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.638723373413086 | KNN Loss: 5.61336612701416 | BCE Loss: 1.0253572463989258\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.680763244628906 | KNN Loss: 5.6164093017578125 | BCE Loss: 1.0643539428710938\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.697382926940918 | KNN Loss: 5.663290023803711 | BCE Loss: 1.034092903137207\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.705084323883057 | KNN Loss: 5.6557111740112305 | BCE Loss: 1.0493732690811157\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.710875988006592 | KNN Loss: 5.638920307159424 | BCE Loss: 1.0719555616378784\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.739320755004883 | KNN Loss: 5.686075687408447 | BCE Loss: 1.0532450675964355\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.660121917724609 | KNN Loss: 5.629785537719727 | BCE Loss: 1.0303361415863037\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.645467758178711 | KNN Loss: 5.614211082458496 | BCE Loss: 1.0312567949295044\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.647397041320801 | KNN Loss: 5.630826950073242 | BCE Loss: 1.016569972038269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.695256233215332 | KNN Loss: 5.636814117431641 | BCE Loss: 1.0584423542022705\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.671474933624268 | KNN Loss: 5.605099678039551 | BCE Loss: 1.0663752555847168\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.645374774932861 | KNN Loss: 5.601078033447266 | BCE Loss: 1.0442967414855957\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.737398624420166 | KNN Loss: 5.681279182434082 | BCE Loss: 1.056119441986084\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.646589279174805 | KNN Loss: 5.604992389678955 | BCE Loss: 1.0415971279144287\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 6.741371154785156 | KNN Loss: 5.673711776733398 | BCE Loss: 1.067659616470337\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.6905717849731445 | KNN Loss: 5.645541667938232 | BCE Loss: 1.045029878616333\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.740664482116699 | KNN Loss: 5.651845932006836 | BCE Loss: 1.0888187885284424\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.680734634399414 | KNN Loss: 5.6347174644470215 | BCE Loss: 1.0460169315338135\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.655536651611328 | KNN Loss: 5.606877326965332 | BCE Loss: 1.048659086227417\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.666770935058594 | KNN Loss: 5.627015590667725 | BCE Loss: 1.0397555828094482\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.694711685180664 | KNN Loss: 5.642931938171387 | BCE Loss: 1.0517796277999878\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.651415824890137 | KNN Loss: 5.607239723205566 | BCE Loss: 1.0441763401031494\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.6769328117370605 | KNN Loss: 5.60955810546875 | BCE Loss: 1.0673747062683105\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.716312408447266 | KNN Loss: 5.6443352699279785 | BCE Loss: 1.071977138519287\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.69244384765625 | KNN Loss: 5.63462495803833 | BCE Loss: 1.057819128036499\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.761115550994873 | KNN Loss: 5.723137855529785 | BCE Loss: 1.037977695465088\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.636513710021973 | KNN Loss: 5.608465194702148 | BCE Loss: 1.0280483961105347\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.725300312042236 | KNN Loss: 5.680021286010742 | BCE Loss: 1.0452790260314941\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.62337064743042 | KNN Loss: 5.59208869934082 | BCE Loss: 1.0312820672988892\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.7367143630981445 | KNN Loss: 5.6531171798706055 | BCE Loss: 1.0835973024368286\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 6.666480541229248 | KNN Loss: 5.6294097900390625 | BCE Loss: 1.037070631980896\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.807070732116699 | KNN Loss: 5.73391580581665 | BCE Loss: 1.0731548070907593\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.671605110168457 | KNN Loss: 5.636510372161865 | BCE Loss: 1.0350947380065918\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.656205177307129 | KNN Loss: 5.6211771965026855 | BCE Loss: 1.0350277423858643\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.660961627960205 | KNN Loss: 5.630993843078613 | BCE Loss: 1.0299679040908813\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.686186790466309 | KNN Loss: 5.630318641662598 | BCE Loss: 1.0558680295944214\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.686492919921875 | KNN Loss: 5.6525468826293945 | BCE Loss: 1.0339457988739014\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 6.696879863739014 | KNN Loss: 5.6616106033325195 | BCE Loss: 1.0352692604064941\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.716880798339844 | KNN Loss: 5.650511741638184 | BCE Loss: 1.0663692951202393\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.701974868774414 | KNN Loss: 5.611022472381592 | BCE Loss: 1.0909526348114014\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.674141883850098 | KNN Loss: 5.629340171813965 | BCE Loss: 1.0448017120361328\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.631438255310059 | KNN Loss: 5.614761829376221 | BCE Loss: 1.0166761875152588\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.7068891525268555 | KNN Loss: 5.64098596572876 | BCE Loss: 1.0659031867980957\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.737593173980713 | KNN Loss: 5.676224708557129 | BCE Loss: 1.061368465423584\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.718564033508301 | KNN Loss: 5.669613838195801 | BCE Loss: 1.048950433731079\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.666491985321045 | KNN Loss: 5.655710220336914 | BCE Loss: 1.0107817649841309\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.6198296546936035 | KNN Loss: 5.613117218017578 | BCE Loss: 1.0067124366760254\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.7432756423950195 | KNN Loss: 5.674710273742676 | BCE Loss: 1.0685653686523438\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.703644275665283 | KNN Loss: 5.635378837585449 | BCE Loss: 1.0682653188705444\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 6.636564254760742 | KNN Loss: 5.608277797698975 | BCE Loss: 1.0282862186431885\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.671970844268799 | KNN Loss: 5.622797966003418 | BCE Loss: 1.0491728782653809\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.664455413818359 | KNN Loss: 5.618436336517334 | BCE Loss: 1.0460193157196045\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.673669815063477 | KNN Loss: 5.6320905685424805 | BCE Loss: 1.041579008102417\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.721006393432617 | KNN Loss: 5.6516547203063965 | BCE Loss: 1.0693517923355103\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.640912055969238 | KNN Loss: 5.619202136993408 | BCE Loss: 1.02170991897583\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.662053108215332 | KNN Loss: 5.614287376403809 | BCE Loss: 1.0477659702301025\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 6.743631839752197 | KNN Loss: 5.698598861694336 | BCE Loss: 1.0450329780578613\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.670651435852051 | KNN Loss: 5.610859394073486 | BCE Loss: 1.0597920417785645\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.705099105834961 | KNN Loss: 5.640974044799805 | BCE Loss: 1.0641248226165771\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.67991304397583 | KNN Loss: 5.640227317810059 | BCE Loss: 1.0396857261657715\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.657637119293213 | KNN Loss: 5.60585880279541 | BCE Loss: 1.0517783164978027\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.681013107299805 | KNN Loss: 5.644818305969238 | BCE Loss: 1.0361950397491455\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.658950328826904 | KNN Loss: 5.608057975769043 | BCE Loss: 1.0508924722671509\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.656222820281982 | KNN Loss: 5.613288879394531 | BCE Loss: 1.0429339408874512\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.644056797027588 | KNN Loss: 5.611655235290527 | BCE Loss: 1.0324015617370605\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.728856563568115 | KNN Loss: 5.646120071411133 | BCE Loss: 1.0827364921569824\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.647950172424316 | KNN Loss: 5.6053314208984375 | BCE Loss: 1.0426185131072998\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.683802604675293 | KNN Loss: 5.638518333435059 | BCE Loss: 1.0452841520309448\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.664842128753662 | KNN Loss: 5.614302635192871 | BCE Loss: 1.0505393743515015\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.639983654022217 | KNN Loss: 5.610996246337891 | BCE Loss: 1.0289874076843262\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.694535255432129 | KNN Loss: 5.634647846221924 | BCE Loss: 1.0598876476287842\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.713607311248779 | KNN Loss: 5.666191101074219 | BCE Loss: 1.0474162101745605\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.654677391052246 | KNN Loss: 5.607480049133301 | BCE Loss: 1.0471973419189453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.6612348556518555 | KNN Loss: 5.609503746032715 | BCE Loss: 1.0517313480377197\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.639632225036621 | KNN Loss: 5.60650634765625 | BCE Loss: 1.0331257581710815\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.713498115539551 | KNN Loss: 5.690204620361328 | BCE Loss: 1.0232934951782227\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.689447402954102 | KNN Loss: 5.658040523529053 | BCE Loss: 1.031407117843628\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.685724258422852 | KNN Loss: 5.6364359855651855 | BCE Loss: 1.0492883920669556\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.699162483215332 | KNN Loss: 5.618096828460693 | BCE Loss: 1.0810654163360596\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.6565656661987305 | KNN Loss: 5.613607406616211 | BCE Loss: 1.0429580211639404\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.690430164337158 | KNN Loss: 5.645288944244385 | BCE Loss: 1.0451412200927734\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.697671890258789 | KNN Loss: 5.638006210327148 | BCE Loss: 1.0596656799316406\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.652085781097412 | KNN Loss: 5.610690593719482 | BCE Loss: 1.0413951873779297\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.744317054748535 | KNN Loss: 5.69657039642334 | BCE Loss: 1.0477468967437744\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.659038066864014 | KNN Loss: 5.626469135284424 | BCE Loss: 1.0325688123703003\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.668649673461914 | KNN Loss: 5.595308780670166 | BCE Loss: 1.073340892791748\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.635221004486084 | KNN Loss: 5.60574197769165 | BCE Loss: 1.0294790267944336\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.674466133117676 | KNN Loss: 5.636559009552002 | BCE Loss: 1.0379068851470947\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.665272235870361 | KNN Loss: 5.603962421417236 | BCE Loss: 1.0613099336624146\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.63416862487793 | KNN Loss: 5.630123615264893 | BCE Loss: 1.0040448904037476\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.6999006271362305 | KNN Loss: 5.654831409454346 | BCE Loss: 1.0450689792633057\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.665003776550293 | KNN Loss: 5.638868808746338 | BCE Loss: 1.0261350870132446\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.713886260986328 | KNN Loss: 5.639646530151367 | BCE Loss: 1.0742398500442505\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.648284435272217 | KNN Loss: 5.603948593139648 | BCE Loss: 1.0443358421325684\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.685926914215088 | KNN Loss: 5.666059494018555 | BCE Loss: 1.0198674201965332\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.861397743225098 | KNN Loss: 5.772711277008057 | BCE Loss: 1.088686227798462\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.698443412780762 | KNN Loss: 5.629072666168213 | BCE Loss: 1.0693708658218384\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.680572509765625 | KNN Loss: 5.62828254699707 | BCE Loss: 1.0522902011871338\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.691126346588135 | KNN Loss: 5.626810073852539 | BCE Loss: 1.0643163919448853\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.654305458068848 | KNN Loss: 5.6071391105651855 | BCE Loss: 1.0471665859222412\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.673824310302734 | KNN Loss: 5.6268134117126465 | BCE Loss: 1.0470106601715088\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.664534091949463 | KNN Loss: 5.625942230224609 | BCE Loss: 1.038591980934143\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.655253887176514 | KNN Loss: 5.616750717163086 | BCE Loss: 1.0385031700134277\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.75588321685791 | KNN Loss: 5.674497604370117 | BCE Loss: 1.0813853740692139\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.6481547355651855 | KNN Loss: 5.60516357421875 | BCE Loss: 1.042991280555725\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.655334949493408 | KNN Loss: 5.616026878356934 | BCE Loss: 1.0393080711364746\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.705173492431641 | KNN Loss: 5.641088962554932 | BCE Loss: 1.064084529876709\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.676713943481445 | KNN Loss: 5.632462024688721 | BCE Loss: 1.0442519187927246\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.719478130340576 | KNN Loss: 5.674344539642334 | BCE Loss: 1.0451337099075317\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.664142608642578 | KNN Loss: 5.6155171394348145 | BCE Loss: 1.0486254692077637\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.665966987609863 | KNN Loss: 5.614062786102295 | BCE Loss: 1.0519039630889893\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.679643630981445 | KNN Loss: 5.650297164916992 | BCE Loss: 1.0293464660644531\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.764154434204102 | KNN Loss: 5.703665733337402 | BCE Loss: 1.0604889392852783\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.667696952819824 | KNN Loss: 5.609926223754883 | BCE Loss: 1.0577707290649414\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.651453018188477 | KNN Loss: 5.606998920440674 | BCE Loss: 1.0444539785385132\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.711661338806152 | KNN Loss: 5.689831256866455 | BCE Loss: 1.0218298435211182\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.746274471282959 | KNN Loss: 5.673138618469238 | BCE Loss: 1.0731358528137207\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.646796226501465 | KNN Loss: 5.6117658615112305 | BCE Loss: 1.0350301265716553\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.721200942993164 | KNN Loss: 5.697227954864502 | BCE Loss: 1.023972988128662\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.670515060424805 | KNN Loss: 5.613898754119873 | BCE Loss: 1.0566163063049316\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.656399726867676 | KNN Loss: 5.625549793243408 | BCE Loss: 1.0308500528335571\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.714791774749756 | KNN Loss: 5.642893314361572 | BCE Loss: 1.0718984603881836\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.69173002243042 | KNN Loss: 5.614169120788574 | BCE Loss: 1.0775610208511353\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.6835222244262695 | KNN Loss: 5.647859573364258 | BCE Loss: 1.0356628894805908\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.646091938018799 | KNN Loss: 5.61654806137085 | BCE Loss: 1.0295438766479492\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.664626121520996 | KNN Loss: 5.599635124206543 | BCE Loss: 1.0649911165237427\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.6726460456848145 | KNN Loss: 5.632683753967285 | BCE Loss: 1.0399624109268188\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 6.6767168045043945 | KNN Loss: 5.628672122955322 | BCE Loss: 1.0480444431304932\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.683065891265869 | KNN Loss: 5.636721134185791 | BCE Loss: 1.0463447570800781\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.666074752807617 | KNN Loss: 5.599328517913818 | BCE Loss: 1.0667462348937988\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.6993818283081055 | KNN Loss: 5.665910243988037 | BCE Loss: 1.0334718227386475\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.638153553009033 | KNN Loss: 5.6173858642578125 | BCE Loss: 1.0207675695419312\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.67741060256958 | KNN Loss: 5.6228814125061035 | BCE Loss: 1.054529070854187\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.651708126068115 | KNN Loss: 5.6222381591796875 | BCE Loss: 1.0294698476791382\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.681036949157715 | KNN Loss: 5.613889694213867 | BCE Loss: 1.067147135734558\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.682635307312012 | KNN Loss: 5.636685848236084 | BCE Loss: 1.0459492206573486\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 6.738744735717773 | KNN Loss: 5.6687188148498535 | BCE Loss: 1.07002592086792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.666149616241455 | KNN Loss: 5.60941743850708 | BCE Loss: 1.056732177734375\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.658149719238281 | KNN Loss: 5.624458312988281 | BCE Loss: 1.0336915254592896\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.652418613433838 | KNN Loss: 5.60536527633667 | BCE Loss: 1.0470534563064575\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.755138397216797 | KNN Loss: 5.71406364440918 | BCE Loss: 1.0410747528076172\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.677190780639648 | KNN Loss: 5.620389461517334 | BCE Loss: 1.0568015575408936\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.673486709594727 | KNN Loss: 5.606107234954834 | BCE Loss: 1.0673794746398926\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.66376256942749 | KNN Loss: 5.61820650100708 | BCE Loss: 1.0455561876296997\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.731047630310059 | KNN Loss: 5.666060447692871 | BCE Loss: 1.0649874210357666\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.682127475738525 | KNN Loss: 5.633039951324463 | BCE Loss: 1.0490875244140625\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.762608528137207 | KNN Loss: 5.716298580169678 | BCE Loss: 1.0463099479675293\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.664058685302734 | KNN Loss: 5.614241600036621 | BCE Loss: 1.0498168468475342\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.654038906097412 | KNN Loss: 5.605318546295166 | BCE Loss: 1.048720359802246\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.6498870849609375 | KNN Loss: 5.621809482574463 | BCE Loss: 1.0280778408050537\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.64040994644165 | KNN Loss: 5.6040825843811035 | BCE Loss: 1.0363273620605469\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.634366989135742 | KNN Loss: 5.60158109664917 | BCE Loss: 1.0327858924865723\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.673415184020996 | KNN Loss: 5.627170562744141 | BCE Loss: 1.0462446212768555\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.6818647384643555 | KNN Loss: 5.645635604858398 | BCE Loss: 1.0362290143966675\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.650974273681641 | KNN Loss: 5.615817070007324 | BCE Loss: 1.0351574420928955\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.695589542388916 | KNN Loss: 5.666494846343994 | BCE Loss: 1.0290945768356323\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.697406768798828 | KNN Loss: 5.640121936798096 | BCE Loss: 1.0572850704193115\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.636964797973633 | KNN Loss: 5.602355480194092 | BCE Loss: 1.0346095561981201\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.677813529968262 | KNN Loss: 5.637935638427734 | BCE Loss: 1.0398781299591064\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.689153671264648 | KNN Loss: 5.629663944244385 | BCE Loss: 1.0594899654388428\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.738169193267822 | KNN Loss: 5.694553375244141 | BCE Loss: 1.0436158180236816\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.689311981201172 | KNN Loss: 5.656826972961426 | BCE Loss: 1.032484769821167\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 6.671489238739014 | KNN Loss: 5.6047682762146 | BCE Loss: 1.066720962524414\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.722864151000977 | KNN Loss: 5.655691623687744 | BCE Loss: 1.0671722888946533\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.6951165199279785 | KNN Loss: 5.670587539672852 | BCE Loss: 1.0245288610458374\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.667139053344727 | KNN Loss: 5.633419513702393 | BCE Loss: 1.0337193012237549\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 6.675432205200195 | KNN Loss: 5.644772052764893 | BCE Loss: 1.0306603908538818\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.658201217651367 | KNN Loss: 5.613023281097412 | BCE Loss: 1.045177936553955\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.714717864990234 | KNN Loss: 5.658359050750732 | BCE Loss: 1.0563589334487915\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.731423377990723 | KNN Loss: 5.68413782119751 | BCE Loss: 1.0472853183746338\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.654393672943115 | KNN Loss: 5.628846168518066 | BCE Loss: 1.0255473852157593\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.652004241943359 | KNN Loss: 5.598652362823486 | BCE Loss: 1.0533521175384521\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.6782989501953125 | KNN Loss: 5.635869026184082 | BCE Loss: 1.0424299240112305\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.6944732666015625 | KNN Loss: 5.647963523864746 | BCE Loss: 1.0465096235275269\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.63740873336792 | KNN Loss: 5.60088586807251 | BCE Loss: 1.0365227460861206\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 6.700982570648193 | KNN Loss: 5.664525032043457 | BCE Loss: 1.0364574193954468\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.710174083709717 | KNN Loss: 5.668176651000977 | BCE Loss: 1.0419973134994507\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.710671901702881 | KNN Loss: 5.665422439575195 | BCE Loss: 1.0452494621276855\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.710173606872559 | KNN Loss: 5.633176803588867 | BCE Loss: 1.0769968032836914\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.7098259925842285 | KNN Loss: 5.635630130767822 | BCE Loss: 1.0741958618164062\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 6.708323955535889 | KNN Loss: 5.659800052642822 | BCE Loss: 1.0485239028930664\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.628523349761963 | KNN Loss: 5.62565279006958 | BCE Loss: 1.0028704404830933\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.654717922210693 | KNN Loss: 5.624972820281982 | BCE Loss: 1.029745101928711\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.7597174644470215 | KNN Loss: 5.682658672332764 | BCE Loss: 1.0770587921142578\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.651947975158691 | KNN Loss: 5.601970672607422 | BCE Loss: 1.0499775409698486\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.651888847351074 | KNN Loss: 5.6184492111206055 | BCE Loss: 1.0334393978118896\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.6234025955200195 | KNN Loss: 5.60243558883667 | BCE Loss: 1.02096688747406\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.697576522827148 | KNN Loss: 5.622876167297363 | BCE Loss: 1.074700117111206\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.726552963256836 | KNN Loss: 5.683713436126709 | BCE Loss: 1.042839527130127\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 6.637765884399414 | KNN Loss: 5.601984024047852 | BCE Loss: 1.0357818603515625\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.676721572875977 | KNN Loss: 5.612324237823486 | BCE Loss: 1.0643970966339111\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.671273708343506 | KNN Loss: 5.624160289764404 | BCE Loss: 1.0471134185791016\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.656348705291748 | KNN Loss: 5.634739875793457 | BCE Loss: 1.021608829498291\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.651352882385254 | KNN Loss: 5.621023178100586 | BCE Loss: 1.030329942703247\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.683466911315918 | KNN Loss: 5.617753505706787 | BCE Loss: 1.0657134056091309\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.710231781005859 | KNN Loss: 5.661434173583984 | BCE Loss: 1.048797369003296\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.609633445739746 | KNN Loss: 5.596384525299072 | BCE Loss: 1.013249158859253\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.648681163787842 | KNN Loss: 5.594063758850098 | BCE Loss: 1.0546172857284546\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.640739440917969 | KNN Loss: 5.613418102264404 | BCE Loss: 1.0273213386535645\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.6929731369018555 | KNN Loss: 5.641582012176514 | BCE Loss: 1.0513908863067627\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.685269355773926 | KNN Loss: 5.631868839263916 | BCE Loss: 1.0534002780914307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.668487548828125 | KNN Loss: 5.611492156982422 | BCE Loss: 1.0569953918457031\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 6.717453956604004 | KNN Loss: 5.6381916999816895 | BCE Loss: 1.0792620182037354\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.6683526039123535 | KNN Loss: 5.634644031524658 | BCE Loss: 1.0337084531784058\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.681865692138672 | KNN Loss: 5.645390033721924 | BCE Loss: 1.0364757776260376\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.676312446594238 | KNN Loss: 5.6179609298706055 | BCE Loss: 1.0583512783050537\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.69525146484375 | KNN Loss: 5.647629737854004 | BCE Loss: 1.047621488571167\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.663219451904297 | KNN Loss: 5.6334547996521 | BCE Loss: 1.0297648906707764\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.682117462158203 | KNN Loss: 5.639921188354492 | BCE Loss: 1.04219651222229\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 6.69290828704834 | KNN Loss: 5.633712291717529 | BCE Loss: 1.0591962337493896\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.732357501983643 | KNN Loss: 5.675613880157471 | BCE Loss: 1.0567437410354614\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.702086925506592 | KNN Loss: 5.640573024749756 | BCE Loss: 1.061513900756836\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.639244079589844 | KNN Loss: 5.597009658813477 | BCE Loss: 1.042234182357788\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.6554951667785645 | KNN Loss: 5.620741844177246 | BCE Loss: 1.0347533226013184\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.78605842590332 | KNN Loss: 5.7171196937561035 | BCE Loss: 1.068938970565796\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.6570868492126465 | KNN Loss: 5.606760025024414 | BCE Loss: 1.0503267049789429\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.671587944030762 | KNN Loss: 5.627048969268799 | BCE Loss: 1.0445387363433838\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.643181800842285 | KNN Loss: 5.604147434234619 | BCE Loss: 1.0390346050262451\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.639064311981201 | KNN Loss: 5.6046528816223145 | BCE Loss: 1.0344113111495972\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.652948379516602 | KNN Loss: 5.613815784454346 | BCE Loss: 1.039132833480835\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 6.705230712890625 | KNN Loss: 5.649559020996094 | BCE Loss: 1.0556719303131104\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.649701118469238 | KNN Loss: 5.621883392333984 | BCE Loss: 1.027817726135254\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 6.685067176818848 | KNN Loss: 5.656604290008545 | BCE Loss: 1.0284626483917236\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.729898452758789 | KNN Loss: 5.6340131759643555 | BCE Loss: 1.0958850383758545\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.682612419128418 | KNN Loss: 5.630822658538818 | BCE Loss: 1.0517897605895996\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.648218631744385 | KNN Loss: 5.61843204498291 | BCE Loss: 1.0297867059707642\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.698122024536133 | KNN Loss: 5.654582500457764 | BCE Loss: 1.0435396432876587\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.643819808959961 | KNN Loss: 5.605040073394775 | BCE Loss: 1.0387797355651855\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.72661018371582 | KNN Loss: 5.667539119720459 | BCE Loss: 1.0590709447860718\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.668842315673828 | KNN Loss: 5.613009929656982 | BCE Loss: 1.0558326244354248\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.6606292724609375 | KNN Loss: 5.623922348022461 | BCE Loss: 1.0367071628570557\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.678706645965576 | KNN Loss: 5.616524696350098 | BCE Loss: 1.062182068824768\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.650559425354004 | KNN Loss: 5.601795196533203 | BCE Loss: 1.0487644672393799\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.684299468994141 | KNN Loss: 5.6245880126953125 | BCE Loss: 1.059711217880249\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 6.676980972290039 | KNN Loss: 5.61298131942749 | BCE Loss: 1.0639994144439697\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.705986022949219 | KNN Loss: 5.667825698852539 | BCE Loss: 1.0381600856781006\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.63926887512207 | KNN Loss: 5.607033729553223 | BCE Loss: 1.0322349071502686\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.660216331481934 | KNN Loss: 5.612666606903076 | BCE Loss: 1.0475499629974365\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.704285621643066 | KNN Loss: 5.653050422668457 | BCE Loss: 1.0512350797653198\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.689863681793213 | KNN Loss: 5.621865272521973 | BCE Loss: 1.0679984092712402\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.651680946350098 | KNN Loss: 5.62780237197876 | BCE Loss: 1.023878574371338\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.6553239822387695 | KNN Loss: 5.6175336837768555 | BCE Loss: 1.037790060043335\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.624910354614258 | KNN Loss: 5.606879711151123 | BCE Loss: 1.0180307626724243\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.629283428192139 | KNN Loss: 5.606049537658691 | BCE Loss: 1.0232340097427368\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.694648742675781 | KNN Loss: 5.638913154602051 | BCE Loss: 1.0557358264923096\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.673854827880859 | KNN Loss: 5.622323036193848 | BCE Loss: 1.0515317916870117\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.668185710906982 | KNN Loss: 5.641145706176758 | BCE Loss: 1.0270400047302246\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.663665294647217 | KNN Loss: 5.609945774078369 | BCE Loss: 1.053719401359558\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.646241664886475 | KNN Loss: 5.614370822906494 | BCE Loss: 1.031870722770691\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.676609039306641 | KNN Loss: 5.632793426513672 | BCE Loss: 1.0438156127929688\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.6340155601501465 | KNN Loss: 5.602967739105225 | BCE Loss: 1.0310478210449219\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.72690486907959 | KNN Loss: 5.681274890899658 | BCE Loss: 1.0456299781799316\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0258,  3.9439,  2.6377,  3.6324,  3.5231,  0.6968,  2.7194,  2.2294,\n",
      "          2.3823,  2.0247,  2.2558,  2.2561,  0.7992,  1.9055,  1.3400,  1.4725,\n",
      "          2.8682,  3.3029,  2.8738,  2.3719,  1.7438,  3.0170,  2.3850,  2.6840,\n",
      "          2.6220,  1.7341,  2.1848,  1.4313,  1.5433,  0.3170, -0.2358,  0.9944,\n",
      "          0.1910,  0.9046,  1.5290,  1.4944,  0.9864,  3.3969,  0.8239,  1.3225,\n",
      "          0.9583, -0.7440, -0.2944,  2.3565,  2.2295,  0.7441, -0.2419,  0.1186,\n",
      "          1.5079,  2.5429,  1.8524,  0.0965,  1.4037,  0.5762, -0.6752,  1.1513,\n",
      "          1.4866,  1.3908,  1.3941,  1.9165,  0.5962,  0.8472,  0.1199,  1.7733,\n",
      "          1.3448,  1.6887, -1.9134,  0.2970,  2.3554,  2.2118,  2.6246,  0.4108,\n",
      "          1.3821,  2.5102,  2.0509,  1.3257,  0.2335,  0.7602,  0.2090,  1.6089,\n",
      "          0.0213,  0.3772,  1.8677, -0.4050,  0.2225, -1.1053, -2.4161, -0.2930,\n",
      "          0.5203, -1.8782,  0.4301, -0.1449, -0.6335, -0.9953,  0.5809,  1.2839,\n",
      "         -0.7181, -0.7370,  0.3606,  1.1681,  0.6455, -1.2688,  0.9010,  1.1447,\n",
      "         -1.2499, -1.1449, -0.1561,  0.0492, -1.0550, -1.7461, -0.4389, -2.6834,\n",
      "         -0.4146,  1.8185,  1.6057, -0.3008, -0.6410,  0.0249,  1.5650, -2.5700,\n",
      "          0.1862, -0.2121,  0.4282, -0.7417,  0.0142, -0.7765, -1.0355,  0.9940,\n",
      "          0.2626, -0.5699,  0.3477, -0.6712, -1.3647, -0.3192, -0.5417,  0.8336,\n",
      "         -0.5162,  0.1270, -2.0165, -1.0144, -1.4460,  0.6109, -1.9604, -0.9987,\n",
      "         -1.0370, -0.6600, -1.6424, -1.1025, -2.3482, -1.0300, -1.3317, -0.3849,\n",
      "         -1.8343,  0.4614, -1.5718, -0.5311, -3.0570,  0.1738, -0.2047, -0.8110,\n",
      "         -2.3066, -1.7953, -1.2406, -1.3932, -2.3714, -2.4117, -3.0964]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.0964, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.9439, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6806bed8ef15486a94dc9d7fcf56c021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 83.52it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f115b1e370a54964b2ad7ab55ea24e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9eea62af8f4efa9501bf1c8b724750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd4a0c58a374835af361ab43b891e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "Epoch: 00 | Batch: 000 / 030 | Total loss: 9.618 | Reg loss: 0.012 | Tree loss: 9.618 | Accuracy: 0.000000 | 1.236 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 030 | Total loss: 9.605 | Reg loss: 0.011 | Tree loss: 9.605 | Accuracy: 0.000000 | 1.038 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 030 | Total loss: 9.592 | Reg loss: 0.010 | Tree loss: 9.592 | Accuracy: 0.000000 | 0.977 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 030 | Total loss: 9.579 | Reg loss: 0.010 | Tree loss: 9.579 | Accuracy: 0.000000 | 0.948 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 030 | Total loss: 9.566 | Reg loss: 0.009 | Tree loss: 9.566 | Accuracy: 0.000000 | 0.933 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 030 | Total loss: 9.554 | Reg loss: 0.009 | Tree loss: 9.554 | Accuracy: 0.009766 | 0.925 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 030 | Total loss: 9.542 | Reg loss: 0.009 | Tree loss: 9.542 | Accuracy: 0.052734 | 0.918 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 030 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.130859 | 0.913 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 030 | Total loss: 9.516 | Reg loss: 0.008 | Tree loss: 9.516 | Accuracy: 0.283203 | 0.91 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 030 | Total loss: 9.504 | Reg loss: 0.008 | Tree loss: 9.504 | Accuracy: 0.433594 | 0.908 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 030 | Total loss: 9.492 | Reg loss: 0.008 | Tree loss: 9.492 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 030 | Total loss: 9.482 | Reg loss: 0.009 | Tree loss: 9.482 | Accuracy: 0.527344 | 0.905 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 030 | Total loss: 9.468 | Reg loss: 0.009 | Tree loss: 9.468 | Accuracy: 0.582031 | 0.903 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 030 | Total loss: 9.458 | Reg loss: 0.009 | Tree loss: 9.458 | Accuracy: 0.566406 | 0.901 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 030 | Total loss: 9.450 | Reg loss: 0.009 | Tree loss: 9.450 | Accuracy: 0.558594 | 0.901 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 030 | Total loss: 9.438 | Reg loss: 0.010 | Tree loss: 9.438 | Accuracy: 0.552734 | 0.901 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 030 | Total loss: 9.421 | Reg loss: 0.010 | Tree loss: 9.421 | Accuracy: 0.576172 | 0.9 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 030 | Total loss: 9.413 | Reg loss: 0.010 | Tree loss: 9.413 | Accuracy: 0.556641 | 0.9 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 030 | Total loss: 9.401 | Reg loss: 0.011 | Tree loss: 9.401 | Accuracy: 0.539062 | 0.899 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 030 | Total loss: 9.389 | Reg loss: 0.011 | Tree loss: 9.389 | Accuracy: 0.576172 | 0.898 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 030 | Total loss: 9.377 | Reg loss: 0.011 | Tree loss: 9.377 | Accuracy: 0.566406 | 0.898 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 030 | Total loss: 9.374 | Reg loss: 0.012 | Tree loss: 9.374 | Accuracy: 0.546875 | 0.898 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 030 | Total loss: 9.353 | Reg loss: 0.012 | Tree loss: 9.353 | Accuracy: 0.570312 | 0.898 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 030 | Total loss: 9.341 | Reg loss: 0.013 | Tree loss: 9.341 | Accuracy: 0.576172 | 0.898 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 030 | Total loss: 9.338 | Reg loss: 0.013 | Tree loss: 9.338 | Accuracy: 0.554688 | 0.897 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 030 | Total loss: 9.322 | Reg loss: 0.013 | Tree loss: 9.322 | Accuracy: 0.580078 | 0.897 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 030 | Total loss: 9.311 | Reg loss: 0.014 | Tree loss: 9.311 | Accuracy: 0.597656 | 0.897 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 030 | Total loss: 9.300 | Reg loss: 0.014 | Tree loss: 9.300 | Accuracy: 0.595703 | 0.896 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 030 | Total loss: 9.288 | Reg loss: 0.014 | Tree loss: 9.288 | Accuracy: 0.580078 | 0.895 sec/iter\n",
      "Epoch: 00 | Batch: 029 / 030 | Total loss: 9.275 | Reg loss: 0.015 | Tree loss: 9.275 | Accuracy: 0.600000 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 030 | Total loss: 9.437 | Reg loss: 0.006 | Tree loss: 9.437 | Accuracy: 0.583984 | 0.915 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 030 | Total loss: 9.430 | Reg loss: 0.006 | Tree loss: 9.430 | Accuracy: 0.548828 | 0.914 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 030 | Total loss: 9.417 | Reg loss: 0.006 | Tree loss: 9.417 | Accuracy: 0.593750 | 0.912 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 030 | Total loss: 9.406 | Reg loss: 0.006 | Tree loss: 9.406 | Accuracy: 0.591797 | 0.91 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 030 | Total loss: 9.398 | Reg loss: 0.007 | Tree loss: 9.398 | Accuracy: 0.517578 | 0.909 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 030 | Total loss: 9.383 | Reg loss: 0.007 | Tree loss: 9.383 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 030 | Total loss: 9.369 | Reg loss: 0.007 | Tree loss: 9.369 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 030 | Total loss: 9.360 | Reg loss: 0.008 | Tree loss: 9.360 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 030 | Total loss: 9.351 | Reg loss: 0.008 | Tree loss: 9.351 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 030 | Total loss: 9.335 | Reg loss: 0.009 | Tree loss: 9.335 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 030 | Total loss: 9.321 | Reg loss: 0.009 | Tree loss: 9.321 | Accuracy: 0.587891 | 0.905 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 030 | Total loss: 9.313 | Reg loss: 0.010 | Tree loss: 9.313 | Accuracy: 0.550781 | 0.905 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 030 | Total loss: 9.294 | Reg loss: 0.010 | Tree loss: 9.294 | Accuracy: 0.585938 | 0.905 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 030 | Total loss: 9.288 | Reg loss: 0.011 | Tree loss: 9.288 | Accuracy: 0.587891 | 0.904 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 030 | Total loss: 9.278 | Reg loss: 0.011 | Tree loss: 9.278 | Accuracy: 0.544922 | 0.904 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 030 | Total loss: 9.255 | Reg loss: 0.012 | Tree loss: 9.255 | Accuracy: 0.607422 | 0.903 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 030 | Total loss: 9.250 | Reg loss: 0.012 | Tree loss: 9.250 | Accuracy: 0.580078 | 0.903 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 030 | Total loss: 9.240 | Reg loss: 0.013 | Tree loss: 9.240 | Accuracy: 0.558594 | 0.903 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 030 | Total loss: 9.223 | Reg loss: 0.013 | Tree loss: 9.223 | Accuracy: 0.607422 | 0.902 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 030 | Total loss: 9.221 | Reg loss: 0.013 | Tree loss: 9.221 | Accuracy: 0.564453 | 0.902 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 030 | Total loss: 9.206 | Reg loss: 0.014 | Tree loss: 9.206 | Accuracy: 0.558594 | 0.902 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 030 | Total loss: 9.189 | Reg loss: 0.014 | Tree loss: 9.189 | Accuracy: 0.595703 | 0.902 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 030 | Total loss: 9.184 | Reg loss: 0.015 | Tree loss: 9.184 | Accuracy: 0.552734 | 0.902 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 030 | Total loss: 9.176 | Reg loss: 0.015 | Tree loss: 9.176 | Accuracy: 0.562500 | 0.901 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 030 | Total loss: 9.159 | Reg loss: 0.016 | Tree loss: 9.159 | Accuracy: 0.576172 | 0.901 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 030 | Total loss: 9.138 | Reg loss: 0.016 | Tree loss: 9.138 | Accuracy: 0.599609 | 0.901 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 030 | Total loss: 9.136 | Reg loss: 0.017 | Tree loss: 9.136 | Accuracy: 0.550781 | 0.901 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 030 | Total loss: 9.131 | Reg loss: 0.017 | Tree loss: 9.131 | Accuracy: 0.576172 | 0.901 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 030 | Total loss: 9.103 | Reg loss: 0.017 | Tree loss: 9.103 | Accuracy: 0.626953 | 0.9 sec/iter\n",
      "Epoch: 01 | Batch: 029 / 030 | Total loss: 9.085 | Reg loss: 0.018 | Tree loss: 9.085 | Accuracy: 0.561905 | 0.9 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 030 | Total loss: 9.282 | Reg loss: 0.009 | Tree loss: 9.282 | Accuracy: 0.599609 | 0.909 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 001 / 030 | Total loss: 9.272 | Reg loss: 0.009 | Tree loss: 9.272 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 030 | Total loss: 9.260 | Reg loss: 0.009 | Tree loss: 9.260 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 030 | Total loss: 9.249 | Reg loss: 0.010 | Tree loss: 9.249 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 030 | Total loss: 9.234 | Reg loss: 0.010 | Tree loss: 9.234 | Accuracy: 0.599609 | 0.906 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 030 | Total loss: 9.230 | Reg loss: 0.010 | Tree loss: 9.230 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 030 | Total loss: 9.220 | Reg loss: 0.010 | Tree loss: 9.220 | Accuracy: 0.542969 | 0.906 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 030 | Total loss: 9.199 | Reg loss: 0.011 | Tree loss: 9.199 | Accuracy: 0.593750 | 0.905 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 030 | Total loss: 9.190 | Reg loss: 0.011 | Tree loss: 9.190 | Accuracy: 0.589844 | 0.905 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 030 | Total loss: 9.173 | Reg loss: 0.012 | Tree loss: 9.173 | Accuracy: 0.570312 | 0.905 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 030 | Total loss: 9.159 | Reg loss: 0.012 | Tree loss: 9.159 | Accuracy: 0.587891 | 0.904 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 030 | Total loss: 9.150 | Reg loss: 0.013 | Tree loss: 9.150 | Accuracy: 0.568359 | 0.904 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 030 | Total loss: 9.139 | Reg loss: 0.013 | Tree loss: 9.139 | Accuracy: 0.574219 | 0.904 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 030 | Total loss: 9.122 | Reg loss: 0.014 | Tree loss: 9.122 | Accuracy: 0.568359 | 0.904 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 030 | Total loss: 9.119 | Reg loss: 0.014 | Tree loss: 9.119 | Accuracy: 0.544922 | 0.904 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 030 | Total loss: 9.094 | Reg loss: 0.015 | Tree loss: 9.094 | Accuracy: 0.585938 | 0.904 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 030 | Total loss: 9.085 | Reg loss: 0.015 | Tree loss: 9.085 | Accuracy: 0.564453 | 0.903 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 030 | Total loss: 9.071 | Reg loss: 0.016 | Tree loss: 9.071 | Accuracy: 0.574219 | 0.903 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 030 | Total loss: 9.059 | Reg loss: 0.016 | Tree loss: 9.059 | Accuracy: 0.576172 | 0.903 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 030 | Total loss: 9.056 | Reg loss: 0.017 | Tree loss: 9.056 | Accuracy: 0.515625 | 0.903 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 030 | Total loss: 9.039 | Reg loss: 0.017 | Tree loss: 9.039 | Accuracy: 0.582031 | 0.903 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 030 | Total loss: 9.020 | Reg loss: 0.018 | Tree loss: 9.020 | Accuracy: 0.587891 | 0.903 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 030 | Total loss: 9.004 | Reg loss: 0.018 | Tree loss: 9.004 | Accuracy: 0.566406 | 0.903 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 030 | Total loss: 8.990 | Reg loss: 0.019 | Tree loss: 8.990 | Accuracy: 0.589844 | 0.902 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 030 | Total loss: 8.990 | Reg loss: 0.019 | Tree loss: 8.990 | Accuracy: 0.582031 | 0.902 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 030 | Total loss: 8.959 | Reg loss: 0.020 | Tree loss: 8.959 | Accuracy: 0.587891 | 0.902 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 030 | Total loss: 8.961 | Reg loss: 0.020 | Tree loss: 8.961 | Accuracy: 0.548828 | 0.902 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 030 | Total loss: 8.932 | Reg loss: 0.021 | Tree loss: 8.932 | Accuracy: 0.599609 | 0.902 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 030 | Total loss: 8.939 | Reg loss: 0.021 | Tree loss: 8.939 | Accuracy: 0.552734 | 0.902 sec/iter\n",
      "Epoch: 02 | Batch: 029 / 030 | Total loss: 8.941 | Reg loss: 0.022 | Tree loss: 8.941 | Accuracy: 0.561905 | 0.902 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 030 | Total loss: 9.134 | Reg loss: 0.012 | Tree loss: 9.134 | Accuracy: 0.542969 | 0.909 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 030 | Total loss: 9.124 | Reg loss: 0.012 | Tree loss: 9.124 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 030 | Total loss: 9.108 | Reg loss: 0.013 | Tree loss: 9.108 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 030 | Total loss: 9.103 | Reg loss: 0.013 | Tree loss: 9.103 | Accuracy: 0.527344 | 0.907 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 030 | Total loss: 9.078 | Reg loss: 0.013 | Tree loss: 9.078 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 030 | Total loss: 9.066 | Reg loss: 0.013 | Tree loss: 9.066 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 030 | Total loss: 9.056 | Reg loss: 0.014 | Tree loss: 9.056 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 030 | Total loss: 9.044 | Reg loss: 0.014 | Tree loss: 9.044 | Accuracy: 0.564453 | 0.906 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 030 | Total loss: 9.014 | Reg loss: 0.014 | Tree loss: 9.014 | Accuracy: 0.595703 | 0.906 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 030 | Total loss: 9.011 | Reg loss: 0.015 | Tree loss: 9.011 | Accuracy: 0.566406 | 0.906 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 030 | Total loss: 8.997 | Reg loss: 0.015 | Tree loss: 8.997 | Accuracy: 0.560547 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 030 | Total loss: 8.981 | Reg loss: 0.016 | Tree loss: 8.981 | Accuracy: 0.560547 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 030 | Total loss: 8.962 | Reg loss: 0.016 | Tree loss: 8.962 | Accuracy: 0.593750 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 030 | Total loss: 8.941 | Reg loss: 0.017 | Tree loss: 8.941 | Accuracy: 0.611328 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 030 | Total loss: 8.931 | Reg loss: 0.017 | Tree loss: 8.931 | Accuracy: 0.597656 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 030 | Total loss: 8.920 | Reg loss: 0.018 | Tree loss: 8.920 | Accuracy: 0.572266 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 030 | Total loss: 8.900 | Reg loss: 0.018 | Tree loss: 8.900 | Accuracy: 0.589844 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 030 | Total loss: 8.890 | Reg loss: 0.019 | Tree loss: 8.890 | Accuracy: 0.574219 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 030 | Total loss: 8.865 | Reg loss: 0.019 | Tree loss: 8.865 | Accuracy: 0.589844 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 030 | Total loss: 8.859 | Reg loss: 0.020 | Tree loss: 8.859 | Accuracy: 0.568359 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 030 | Total loss: 8.846 | Reg loss: 0.020 | Tree loss: 8.846 | Accuracy: 0.558594 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 030 | Total loss: 8.826 | Reg loss: 0.021 | Tree loss: 8.826 | Accuracy: 0.578125 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 030 | Total loss: 8.815 | Reg loss: 0.021 | Tree loss: 8.815 | Accuracy: 0.570312 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 030 | Total loss: 8.809 | Reg loss: 0.022 | Tree loss: 8.809 | Accuracy: 0.560547 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 030 | Total loss: 8.796 | Reg loss: 0.022 | Tree loss: 8.796 | Accuracy: 0.541016 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 030 | Total loss: 8.768 | Reg loss: 0.023 | Tree loss: 8.768 | Accuracy: 0.580078 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 030 | Total loss: 8.754 | Reg loss: 0.023 | Tree loss: 8.754 | Accuracy: 0.591797 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 030 | Total loss: 8.725 | Reg loss: 0.024 | Tree loss: 8.725 | Accuracy: 0.597656 | 0.903 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 030 | Total loss: 8.732 | Reg loss: 0.024 | Tree loss: 8.732 | Accuracy: 0.562500 | 0.903 sec/iter\n",
      "Epoch: 03 | Batch: 029 / 030 | Total loss: 8.717 | Reg loss: 0.024 | Tree loss: 8.717 | Accuracy: 0.561905 | 0.903 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 030 | Total loss: 8.959 | Reg loss: 0.016 | Tree loss: 8.959 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 030 | Total loss: 8.943 | Reg loss: 0.016 | Tree loss: 8.943 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 030 | Total loss: 8.942 | Reg loss: 0.016 | Tree loss: 8.942 | Accuracy: 0.552734 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 003 / 030 | Total loss: 8.926 | Reg loss: 0.016 | Tree loss: 8.926 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 030 | Total loss: 8.912 | Reg loss: 0.016 | Tree loss: 8.912 | Accuracy: 0.556641 | 0.906 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 030 | Total loss: 8.893 | Reg loss: 0.016 | Tree loss: 8.893 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 030 | Total loss: 8.877 | Reg loss: 0.017 | Tree loss: 8.877 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 030 | Total loss: 8.852 | Reg loss: 0.017 | Tree loss: 8.852 | Accuracy: 0.593750 | 0.906 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 030 | Total loss: 8.838 | Reg loss: 0.017 | Tree loss: 8.838 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 030 | Total loss: 8.819 | Reg loss: 0.018 | Tree loss: 8.819 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 030 | Total loss: 8.809 | Reg loss: 0.018 | Tree loss: 8.809 | Accuracy: 0.554688 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 030 | Total loss: 8.789 | Reg loss: 0.019 | Tree loss: 8.789 | Accuracy: 0.562500 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 030 | Total loss: 8.769 | Reg loss: 0.019 | Tree loss: 8.769 | Accuracy: 0.574219 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 030 | Total loss: 8.755 | Reg loss: 0.020 | Tree loss: 8.755 | Accuracy: 0.564453 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 030 | Total loss: 8.713 | Reg loss: 0.020 | Tree loss: 8.713 | Accuracy: 0.638672 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 030 | Total loss: 8.715 | Reg loss: 0.021 | Tree loss: 8.715 | Accuracy: 0.566406 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 030 | Total loss: 8.690 | Reg loss: 0.021 | Tree loss: 8.690 | Accuracy: 0.550781 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 030 | Total loss: 8.656 | Reg loss: 0.022 | Tree loss: 8.656 | Accuracy: 0.589844 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 030 | Total loss: 8.652 | Reg loss: 0.022 | Tree loss: 8.652 | Accuracy: 0.548828 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 030 | Total loss: 8.646 | Reg loss: 0.023 | Tree loss: 8.646 | Accuracy: 0.562500 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 030 | Total loss: 8.621 | Reg loss: 0.024 | Tree loss: 8.621 | Accuracy: 0.560547 | 0.905 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 030 | Total loss: 8.593 | Reg loss: 0.024 | Tree loss: 8.593 | Accuracy: 0.570312 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 030 | Total loss: 8.577 | Reg loss: 0.025 | Tree loss: 8.577 | Accuracy: 0.578125 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 030 | Total loss: 8.546 | Reg loss: 0.025 | Tree loss: 8.546 | Accuracy: 0.587891 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 030 | Total loss: 8.529 | Reg loss: 0.026 | Tree loss: 8.529 | Accuracy: 0.609375 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 030 | Total loss: 8.533 | Reg loss: 0.026 | Tree loss: 8.533 | Accuracy: 0.537109 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 030 | Total loss: 8.492 | Reg loss: 0.027 | Tree loss: 8.492 | Accuracy: 0.599609 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 030 | Total loss: 8.492 | Reg loss: 0.027 | Tree loss: 8.492 | Accuracy: 0.562500 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 030 | Total loss: 8.451 | Reg loss: 0.028 | Tree loss: 8.451 | Accuracy: 0.566406 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 029 / 030 | Total loss: 8.418 | Reg loss: 0.028 | Tree loss: 8.418 | Accuracy: 0.647619 | 0.904 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 030 | Total loss: 8.767 | Reg loss: 0.019 | Tree loss: 8.767 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 030 | Total loss: 8.763 | Reg loss: 0.019 | Tree loss: 8.763 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 030 | Total loss: 8.744 | Reg loss: 0.019 | Tree loss: 8.744 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 030 | Total loss: 8.730 | Reg loss: 0.019 | Tree loss: 8.730 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 030 | Total loss: 8.712 | Reg loss: 0.019 | Tree loss: 8.712 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 030 | Total loss: 8.701 | Reg loss: 0.019 | Tree loss: 8.701 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 030 | Total loss: 8.671 | Reg loss: 0.020 | Tree loss: 8.671 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 030 | Total loss: 8.652 | Reg loss: 0.020 | Tree loss: 8.652 | Accuracy: 0.556641 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 030 | Total loss: 8.616 | Reg loss: 0.021 | Tree loss: 8.616 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 030 | Total loss: 8.601 | Reg loss: 0.021 | Tree loss: 8.601 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 030 | Total loss: 8.570 | Reg loss: 0.021 | Tree loss: 8.570 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 030 | Total loss: 8.555 | Reg loss: 0.022 | Tree loss: 8.555 | Accuracy: 0.554688 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 030 | Total loss: 8.515 | Reg loss: 0.022 | Tree loss: 8.515 | Accuracy: 0.607422 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 030 | Total loss: 8.513 | Reg loss: 0.023 | Tree loss: 8.513 | Accuracy: 0.544922 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 030 | Total loss: 8.477 | Reg loss: 0.023 | Tree loss: 8.477 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 030 | Total loss: 8.447 | Reg loss: 0.024 | Tree loss: 8.447 | Accuracy: 0.619141 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 030 | Total loss: 8.428 | Reg loss: 0.024 | Tree loss: 8.428 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 030 | Total loss: 8.396 | Reg loss: 0.025 | Tree loss: 8.396 | Accuracy: 0.583984 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 030 | Total loss: 8.372 | Reg loss: 0.025 | Tree loss: 8.372 | Accuracy: 0.585938 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 030 | Total loss: 8.376 | Reg loss: 0.026 | Tree loss: 8.376 | Accuracy: 0.574219 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 030 | Total loss: 8.340 | Reg loss: 0.026 | Tree loss: 8.340 | Accuracy: 0.582031 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 030 | Total loss: 8.313 | Reg loss: 0.027 | Tree loss: 8.313 | Accuracy: 0.578125 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 030 | Total loss: 8.290 | Reg loss: 0.027 | Tree loss: 8.290 | Accuracy: 0.578125 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 030 | Total loss: 8.272 | Reg loss: 0.028 | Tree loss: 8.272 | Accuracy: 0.548828 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 030 | Total loss: 8.232 | Reg loss: 0.028 | Tree loss: 8.232 | Accuracy: 0.564453 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 030 | Total loss: 8.223 | Reg loss: 0.029 | Tree loss: 8.223 | Accuracy: 0.556641 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 030 | Total loss: 8.191 | Reg loss: 0.029 | Tree loss: 8.191 | Accuracy: 0.597656 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 030 | Total loss: 8.160 | Reg loss: 0.030 | Tree loss: 8.160 | Accuracy: 0.572266 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 030 | Total loss: 8.138 | Reg loss: 0.030 | Tree loss: 8.138 | Accuracy: 0.609375 | 0.905 sec/iter\n",
      "Epoch: 05 | Batch: 029 / 030 | Total loss: 8.154 | Reg loss: 0.031 | Tree loss: 8.154 | Accuracy: 0.533333 | 0.905 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 030 | Total loss: 8.551 | Reg loss: 0.021 | Tree loss: 8.551 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 030 | Total loss: 8.549 | Reg loss: 0.022 | Tree loss: 8.549 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 030 | Total loss: 8.542 | Reg loss: 0.022 | Tree loss: 8.542 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 030 | Total loss: 8.507 | Reg loss: 0.022 | Tree loss: 8.507 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 030 | Total loss: 8.470 | Reg loss: 0.022 | Tree loss: 8.470 | Accuracy: 0.578125 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 005 / 030 | Total loss: 8.455 | Reg loss: 0.022 | Tree loss: 8.455 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 030 | Total loss: 8.424 | Reg loss: 0.023 | Tree loss: 8.424 | Accuracy: 0.566406 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 030 | Total loss: 8.386 | Reg loss: 0.023 | Tree loss: 8.386 | Accuracy: 0.560547 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 030 | Total loss: 8.360 | Reg loss: 0.023 | Tree loss: 8.360 | Accuracy: 0.611328 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 030 | Total loss: 8.345 | Reg loss: 0.024 | Tree loss: 8.345 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 030 | Total loss: 8.310 | Reg loss: 0.024 | Tree loss: 8.310 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 030 | Total loss: 8.291 | Reg loss: 0.025 | Tree loss: 8.291 | Accuracy: 0.583984 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 030 | Total loss: 8.285 | Reg loss: 0.025 | Tree loss: 8.285 | Accuracy: 0.535156 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 030 | Total loss: 8.246 | Reg loss: 0.026 | Tree loss: 8.246 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 030 | Total loss: 8.197 | Reg loss: 0.026 | Tree loss: 8.197 | Accuracy: 0.593750 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 030 | Total loss: 8.172 | Reg loss: 0.027 | Tree loss: 8.172 | Accuracy: 0.595703 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 030 | Total loss: 8.160 | Reg loss: 0.027 | Tree loss: 8.160 | Accuracy: 0.595703 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 030 | Total loss: 8.139 | Reg loss: 0.028 | Tree loss: 8.139 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 030 | Total loss: 8.106 | Reg loss: 0.028 | Tree loss: 8.106 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 030 | Total loss: 8.054 | Reg loss: 0.029 | Tree loss: 8.054 | Accuracy: 0.605469 | 0.906 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 030 | Total loss: 8.029 | Reg loss: 0.029 | Tree loss: 8.029 | Accuracy: 0.574219 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 030 | Total loss: 8.014 | Reg loss: 0.030 | Tree loss: 8.014 | Accuracy: 0.544922 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 030 | Total loss: 8.028 | Reg loss: 0.030 | Tree loss: 8.028 | Accuracy: 0.548828 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 030 | Total loss: 7.950 | Reg loss: 0.031 | Tree loss: 7.950 | Accuracy: 0.580078 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 030 | Total loss: 7.937 | Reg loss: 0.031 | Tree loss: 7.937 | Accuracy: 0.568359 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 030 | Total loss: 7.952 | Reg loss: 0.032 | Tree loss: 7.952 | Accuracy: 0.525391 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 030 | Total loss: 7.902 | Reg loss: 0.032 | Tree loss: 7.902 | Accuracy: 0.562500 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 030 | Total loss: 7.851 | Reg loss: 0.033 | Tree loss: 7.851 | Accuracy: 0.580078 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 030 | Total loss: 7.840 | Reg loss: 0.033 | Tree loss: 7.840 | Accuracy: 0.585938 | 0.905 sec/iter\n",
      "Epoch: 06 | Batch: 029 / 030 | Total loss: 7.808 | Reg loss: 0.034 | Tree loss: 7.808 | Accuracy: 0.600000 | 0.905 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 030 | Total loss: 8.330 | Reg loss: 0.024 | Tree loss: 8.330 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 030 | Total loss: 8.285 | Reg loss: 0.024 | Tree loss: 8.285 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 030 | Total loss: 8.278 | Reg loss: 0.024 | Tree loss: 8.278 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 030 | Total loss: 8.263 | Reg loss: 0.025 | Tree loss: 8.263 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 030 | Total loss: 8.208 | Reg loss: 0.025 | Tree loss: 8.208 | Accuracy: 0.628906 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 030 | Total loss: 8.207 | Reg loss: 0.025 | Tree loss: 8.207 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 030 | Total loss: 8.190 | Reg loss: 0.025 | Tree loss: 8.190 | Accuracy: 0.521484 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 030 | Total loss: 8.149 | Reg loss: 0.026 | Tree loss: 8.149 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 030 | Total loss: 8.096 | Reg loss: 0.026 | Tree loss: 8.096 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 030 | Total loss: 8.067 | Reg loss: 0.026 | Tree loss: 8.067 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 030 | Total loss: 8.032 | Reg loss: 0.027 | Tree loss: 8.032 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 030 | Total loss: 8.029 | Reg loss: 0.027 | Tree loss: 8.029 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 030 | Total loss: 7.996 | Reg loss: 0.028 | Tree loss: 7.996 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 030 | Total loss: 7.977 | Reg loss: 0.028 | Tree loss: 7.977 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 030 | Total loss: 7.925 | Reg loss: 0.029 | Tree loss: 7.925 | Accuracy: 0.591797 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 030 | Total loss: 7.873 | Reg loss: 0.029 | Tree loss: 7.873 | Accuracy: 0.603516 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 030 | Total loss: 7.883 | Reg loss: 0.030 | Tree loss: 7.883 | Accuracy: 0.603516 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 030 | Total loss: 7.834 | Reg loss: 0.030 | Tree loss: 7.834 | Accuracy: 0.556641 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 030 | Total loss: 7.819 | Reg loss: 0.031 | Tree loss: 7.819 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 030 | Total loss: 7.789 | Reg loss: 0.031 | Tree loss: 7.789 | Accuracy: 0.568359 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 030 | Total loss: 7.728 | Reg loss: 0.032 | Tree loss: 7.728 | Accuracy: 0.605469 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 030 | Total loss: 7.746 | Reg loss: 0.032 | Tree loss: 7.746 | Accuracy: 0.591797 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 030 | Total loss: 7.698 | Reg loss: 0.033 | Tree loss: 7.698 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 030 | Total loss: 7.666 | Reg loss: 0.033 | Tree loss: 7.666 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 030 | Total loss: 7.644 | Reg loss: 0.034 | Tree loss: 7.644 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 030 | Total loss: 7.633 | Reg loss: 0.034 | Tree loss: 7.633 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 030 | Total loss: 7.587 | Reg loss: 0.035 | Tree loss: 7.587 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 030 | Total loss: 7.582 | Reg loss: 0.035 | Tree loss: 7.582 | Accuracy: 0.519531 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 030 | Total loss: 7.557 | Reg loss: 0.035 | Tree loss: 7.557 | Accuracy: 0.550781 | 0.906 sec/iter\n",
      "Epoch: 07 | Batch: 029 / 030 | Total loss: 7.481 | Reg loss: 0.036 | Tree loss: 7.481 | Accuracy: 0.580952 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 030 | Total loss: 8.065 | Reg loss: 0.027 | Tree loss: 8.065 | Accuracy: 0.613281 | 0.908 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 030 | Total loss: 8.033 | Reg loss: 0.027 | Tree loss: 8.033 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 030 | Total loss: 8.056 | Reg loss: 0.027 | Tree loss: 8.056 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 030 | Total loss: 8.006 | Reg loss: 0.027 | Tree loss: 8.006 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 030 | Total loss: 7.965 | Reg loss: 0.027 | Tree loss: 7.965 | Accuracy: 0.611328 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 030 | Total loss: 7.936 | Reg loss: 0.028 | Tree loss: 7.936 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 030 | Total loss: 7.940 | Reg loss: 0.028 | Tree loss: 7.940 | Accuracy: 0.539062 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 007 / 030 | Total loss: 7.881 | Reg loss: 0.028 | Tree loss: 7.881 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 030 | Total loss: 7.853 | Reg loss: 0.028 | Tree loss: 7.853 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 030 | Total loss: 7.826 | Reg loss: 0.029 | Tree loss: 7.826 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 030 | Total loss: 7.792 | Reg loss: 0.029 | Tree loss: 7.792 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 030 | Total loss: 7.710 | Reg loss: 0.030 | Tree loss: 7.710 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 030 | Total loss: 7.729 | Reg loss: 0.030 | Tree loss: 7.729 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 030 | Total loss: 7.670 | Reg loss: 0.030 | Tree loss: 7.670 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 030 | Total loss: 7.642 | Reg loss: 0.031 | Tree loss: 7.642 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 030 | Total loss: 7.633 | Reg loss: 0.031 | Tree loss: 7.633 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 030 | Total loss: 7.618 | Reg loss: 0.032 | Tree loss: 7.618 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 030 | Total loss: 7.547 | Reg loss: 0.032 | Tree loss: 7.547 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 030 | Total loss: 7.524 | Reg loss: 0.033 | Tree loss: 7.524 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 030 | Total loss: 7.483 | Reg loss: 0.033 | Tree loss: 7.483 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 030 | Total loss: 7.457 | Reg loss: 0.034 | Tree loss: 7.457 | Accuracy: 0.560547 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 030 | Total loss: 7.416 | Reg loss: 0.034 | Tree loss: 7.416 | Accuracy: 0.607422 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 030 | Total loss: 7.408 | Reg loss: 0.035 | Tree loss: 7.408 | Accuracy: 0.546875 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 030 | Total loss: 7.383 | Reg loss: 0.035 | Tree loss: 7.383 | Accuracy: 0.589844 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 030 | Total loss: 7.358 | Reg loss: 0.036 | Tree loss: 7.358 | Accuracy: 0.558594 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 030 | Total loss: 7.318 | Reg loss: 0.036 | Tree loss: 7.318 | Accuracy: 0.595703 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 030 | Total loss: 7.282 | Reg loss: 0.036 | Tree loss: 7.282 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 030 | Total loss: 7.316 | Reg loss: 0.037 | Tree loss: 7.316 | Accuracy: 0.537109 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 030 | Total loss: 7.234 | Reg loss: 0.037 | Tree loss: 7.234 | Accuracy: 0.558594 | 0.906 sec/iter\n",
      "Epoch: 08 | Batch: 029 / 030 | Total loss: 7.193 | Reg loss: 0.038 | Tree loss: 7.193 | Accuracy: 0.571429 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 030 | Total loss: 7.796 | Reg loss: 0.029 | Tree loss: 7.796 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 030 | Total loss: 7.798 | Reg loss: 0.029 | Tree loss: 7.798 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 030 | Total loss: 7.749 | Reg loss: 0.029 | Tree loss: 7.749 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 030 | Total loss: 7.731 | Reg loss: 0.030 | Tree loss: 7.731 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 030 | Total loss: 7.724 | Reg loss: 0.030 | Tree loss: 7.724 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 030 | Total loss: 7.629 | Reg loss: 0.030 | Tree loss: 7.629 | Accuracy: 0.634766 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 030 | Total loss: 7.617 | Reg loss: 0.030 | Tree loss: 7.617 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 030 | Total loss: 7.620 | Reg loss: 0.030 | Tree loss: 7.620 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 030 | Total loss: 7.567 | Reg loss: 0.031 | Tree loss: 7.567 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 030 | Total loss: 7.549 | Reg loss: 0.031 | Tree loss: 7.549 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 030 | Total loss: 7.509 | Reg loss: 0.031 | Tree loss: 7.509 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 030 | Total loss: 7.482 | Reg loss: 0.032 | Tree loss: 7.482 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 030 | Total loss: 7.458 | Reg loss: 0.032 | Tree loss: 7.458 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 030 | Total loss: 7.417 | Reg loss: 0.033 | Tree loss: 7.417 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 030 | Total loss: 7.357 | Reg loss: 0.033 | Tree loss: 7.357 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 030 | Total loss: 7.368 | Reg loss: 0.033 | Tree loss: 7.368 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 030 | Total loss: 7.286 | Reg loss: 0.034 | Tree loss: 7.286 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 030 | Total loss: 7.291 | Reg loss: 0.034 | Tree loss: 7.291 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 030 | Total loss: 7.233 | Reg loss: 0.035 | Tree loss: 7.233 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 030 | Total loss: 7.177 | Reg loss: 0.035 | Tree loss: 7.177 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 030 | Total loss: 7.203 | Reg loss: 0.036 | Tree loss: 7.203 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 030 | Total loss: 7.149 | Reg loss: 0.036 | Tree loss: 7.149 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 030 | Total loss: 7.164 | Reg loss: 0.036 | Tree loss: 7.164 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 030 | Total loss: 7.116 | Reg loss: 0.037 | Tree loss: 7.116 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 030 | Total loss: 7.107 | Reg loss: 0.037 | Tree loss: 7.107 | Accuracy: 0.593750 | 0.906 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 030 | Total loss: 7.020 | Reg loss: 0.038 | Tree loss: 7.020 | Accuracy: 0.597656 | 0.906 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 030 | Total loss: 7.038 | Reg loss: 0.038 | Tree loss: 7.038 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 030 | Total loss: 6.994 | Reg loss: 0.038 | Tree loss: 6.994 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 030 | Total loss: 6.980 | Reg loss: 0.039 | Tree loss: 6.980 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 09 | Batch: 029 / 030 | Total loss: 7.030 | Reg loss: 0.039 | Tree loss: 7.030 | Accuracy: 0.485714 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 030 | Total loss: 7.589 | Reg loss: 0.032 | Tree loss: 7.589 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 030 | Total loss: 7.531 | Reg loss: 0.032 | Tree loss: 7.531 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 030 | Total loss: 7.499 | Reg loss: 0.032 | Tree loss: 7.499 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 030 | Total loss: 7.485 | Reg loss: 0.032 | Tree loss: 7.485 | Accuracy: 0.525391 | 0.908 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 030 | Total loss: 7.441 | Reg loss: 0.032 | Tree loss: 7.441 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 030 | Total loss: 7.430 | Reg loss: 0.032 | Tree loss: 7.430 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 030 | Total loss: 7.391 | Reg loss: 0.032 | Tree loss: 7.391 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 030 | Total loss: 7.392 | Reg loss: 0.033 | Tree loss: 7.392 | Accuracy: 0.519531 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 030 | Total loss: 7.295 | Reg loss: 0.033 | Tree loss: 7.295 | Accuracy: 0.566406 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 009 / 030 | Total loss: 7.265 | Reg loss: 0.033 | Tree loss: 7.265 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 030 | Total loss: 7.250 | Reg loss: 0.033 | Tree loss: 7.250 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 030 | Total loss: 7.192 | Reg loss: 0.034 | Tree loss: 7.192 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 030 | Total loss: 7.168 | Reg loss: 0.034 | Tree loss: 7.168 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 030 | Total loss: 7.125 | Reg loss: 0.034 | Tree loss: 7.125 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 030 | Total loss: 7.094 | Reg loss: 0.035 | Tree loss: 7.094 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 030 | Total loss: 7.064 | Reg loss: 0.035 | Tree loss: 7.064 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 030 | Total loss: 7.047 | Reg loss: 0.035 | Tree loss: 7.047 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 030 | Total loss: 7.003 | Reg loss: 0.036 | Tree loss: 7.003 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 030 | Total loss: 6.981 | Reg loss: 0.036 | Tree loss: 6.981 | Accuracy: 0.521484 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 030 | Total loss: 6.972 | Reg loss: 0.037 | Tree loss: 6.972 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 030 | Total loss: 6.924 | Reg loss: 0.037 | Tree loss: 6.924 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 030 | Total loss: 6.927 | Reg loss: 0.037 | Tree loss: 6.927 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 030 | Total loss: 6.864 | Reg loss: 0.038 | Tree loss: 6.864 | Accuracy: 0.568359 | 0.906 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 030 | Total loss: 6.835 | Reg loss: 0.038 | Tree loss: 6.835 | Accuracy: 0.599609 | 0.906 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 030 | Total loss: 6.808 | Reg loss: 0.038 | Tree loss: 6.808 | Accuracy: 0.601562 | 0.906 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 030 | Total loss: 6.756 | Reg loss: 0.039 | Tree loss: 6.756 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 030 | Total loss: 6.774 | Reg loss: 0.039 | Tree loss: 6.774 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 030 | Total loss: 6.736 | Reg loss: 0.039 | Tree loss: 6.736 | Accuracy: 0.572266 | 0.906 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 030 | Total loss: 6.722 | Reg loss: 0.040 | Tree loss: 6.722 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 10 | Batch: 029 / 030 | Total loss: 6.632 | Reg loss: 0.040 | Tree loss: 6.632 | Accuracy: 0.590476 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 030 | Total loss: 7.276 | Reg loss: 0.033 | Tree loss: 7.276 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 030 | Total loss: 7.299 | Reg loss: 0.033 | Tree loss: 7.299 | Accuracy: 0.519531 | 0.908 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 030 | Total loss: 7.230 | Reg loss: 0.034 | Tree loss: 7.230 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 030 | Total loss: 7.235 | Reg loss: 0.034 | Tree loss: 7.235 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 030 | Total loss: 7.169 | Reg loss: 0.034 | Tree loss: 7.169 | Accuracy: 0.623047 | 0.908 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 030 | Total loss: 7.156 | Reg loss: 0.034 | Tree loss: 7.156 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 030 | Total loss: 7.110 | Reg loss: 0.034 | Tree loss: 7.110 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 030 | Total loss: 7.107 | Reg loss: 0.034 | Tree loss: 7.107 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 030 | Total loss: 7.056 | Reg loss: 0.034 | Tree loss: 7.056 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 030 | Total loss: 7.026 | Reg loss: 0.035 | Tree loss: 7.026 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 030 | Total loss: 6.983 | Reg loss: 0.035 | Tree loss: 6.983 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 030 | Total loss: 6.921 | Reg loss: 0.035 | Tree loss: 6.921 | Accuracy: 0.617188 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 030 | Total loss: 6.921 | Reg loss: 0.036 | Tree loss: 6.921 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 030 | Total loss: 6.867 | Reg loss: 0.036 | Tree loss: 6.867 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 030 | Total loss: 6.861 | Reg loss: 0.036 | Tree loss: 6.861 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 030 | Total loss: 6.819 | Reg loss: 0.037 | Tree loss: 6.819 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 030 | Total loss: 6.774 | Reg loss: 0.037 | Tree loss: 6.774 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 030 | Total loss: 6.756 | Reg loss: 0.037 | Tree loss: 6.756 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 030 | Total loss: 6.705 | Reg loss: 0.038 | Tree loss: 6.705 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 030 | Total loss: 6.700 | Reg loss: 0.038 | Tree loss: 6.700 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 030 | Total loss: 6.671 | Reg loss: 0.038 | Tree loss: 6.671 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 030 | Total loss: 6.672 | Reg loss: 0.039 | Tree loss: 6.672 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 030 | Total loss: 6.628 | Reg loss: 0.039 | Tree loss: 6.628 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 030 | Total loss: 6.608 | Reg loss: 0.039 | Tree loss: 6.608 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 030 | Total loss: 6.576 | Reg loss: 0.039 | Tree loss: 6.576 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 030 | Total loss: 6.563 | Reg loss: 0.040 | Tree loss: 6.563 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 030 | Total loss: 6.552 | Reg loss: 0.040 | Tree loss: 6.552 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 030 | Total loss: 6.518 | Reg loss: 0.040 | Tree loss: 6.518 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 030 | Total loss: 6.448 | Reg loss: 0.041 | Tree loss: 6.448 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 11 | Batch: 029 / 030 | Total loss: 6.434 | Reg loss: 0.041 | Tree loss: 6.434 | Accuracy: 0.666667 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 030 | Total loss: 7.039 | Reg loss: 0.035 | Tree loss: 7.039 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 030 | Total loss: 7.031 | Reg loss: 0.035 | Tree loss: 7.031 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 030 | Total loss: 6.994 | Reg loss: 0.035 | Tree loss: 6.994 | Accuracy: 0.615234 | 0.908 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 030 | Total loss: 6.959 | Reg loss: 0.035 | Tree loss: 6.959 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 030 | Total loss: 6.908 | Reg loss: 0.035 | Tree loss: 6.908 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 030 | Total loss: 6.885 | Reg loss: 0.035 | Tree loss: 6.885 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 030 | Total loss: 6.883 | Reg loss: 0.036 | Tree loss: 6.883 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 030 | Total loss: 6.893 | Reg loss: 0.036 | Tree loss: 6.893 | Accuracy: 0.513672 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 030 | Total loss: 6.833 | Reg loss: 0.036 | Tree loss: 6.833 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 030 | Total loss: 6.758 | Reg loss: 0.036 | Tree loss: 6.758 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 030 | Total loss: 6.750 | Reg loss: 0.036 | Tree loss: 6.750 | Accuracy: 0.562500 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 011 / 030 | Total loss: 6.677 | Reg loss: 0.037 | Tree loss: 6.677 | Accuracy: 0.619141 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 030 | Total loss: 6.734 | Reg loss: 0.037 | Tree loss: 6.734 | Accuracy: 0.501953 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 030 | Total loss: 6.684 | Reg loss: 0.037 | Tree loss: 6.684 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 030 | Total loss: 6.588 | Reg loss: 0.037 | Tree loss: 6.588 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 030 | Total loss: 6.569 | Reg loss: 0.038 | Tree loss: 6.569 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 030 | Total loss: 6.522 | Reg loss: 0.038 | Tree loss: 6.522 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 030 | Total loss: 6.516 | Reg loss: 0.038 | Tree loss: 6.516 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 030 | Total loss: 6.470 | Reg loss: 0.039 | Tree loss: 6.470 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 030 | Total loss: 6.484 | Reg loss: 0.039 | Tree loss: 6.484 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 030 | Total loss: 6.424 | Reg loss: 0.039 | Tree loss: 6.424 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 030 | Total loss: 6.402 | Reg loss: 0.040 | Tree loss: 6.402 | Accuracy: 0.515625 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 030 | Total loss: 6.337 | Reg loss: 0.040 | Tree loss: 6.337 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 030 | Total loss: 6.344 | Reg loss: 0.040 | Tree loss: 6.344 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 030 | Total loss: 6.299 | Reg loss: 0.040 | Tree loss: 6.299 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 030 | Total loss: 6.315 | Reg loss: 0.041 | Tree loss: 6.315 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 030 | Total loss: 6.265 | Reg loss: 0.041 | Tree loss: 6.265 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 030 | Total loss: 6.231 | Reg loss: 0.041 | Tree loss: 6.231 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 030 | Total loss: 6.222 | Reg loss: 0.042 | Tree loss: 6.222 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 12 | Batch: 029 / 030 | Total loss: 6.125 | Reg loss: 0.042 | Tree loss: 6.125 | Accuracy: 0.580952 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 030 | Total loss: 6.872 | Reg loss: 0.036 | Tree loss: 6.872 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 030 | Total loss: 6.770 | Reg loss: 0.036 | Tree loss: 6.770 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 030 | Total loss: 6.757 | Reg loss: 0.036 | Tree loss: 6.757 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 030 | Total loss: 6.696 | Reg loss: 0.036 | Tree loss: 6.696 | Accuracy: 0.607422 | 0.908 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 030 | Total loss: 6.706 | Reg loss: 0.036 | Tree loss: 6.706 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 030 | Total loss: 6.731 | Reg loss: 0.036 | Tree loss: 6.731 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 030 | Total loss: 6.641 | Reg loss: 0.037 | Tree loss: 6.641 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 030 | Total loss: 6.644 | Reg loss: 0.037 | Tree loss: 6.644 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 030 | Total loss: 6.578 | Reg loss: 0.037 | Tree loss: 6.578 | Accuracy: 0.527344 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 030 | Total loss: 6.503 | Reg loss: 0.037 | Tree loss: 6.503 | Accuracy: 0.636719 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 030 | Total loss: 6.547 | Reg loss: 0.037 | Tree loss: 6.547 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 030 | Total loss: 6.437 | Reg loss: 0.038 | Tree loss: 6.437 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 030 | Total loss: 6.425 | Reg loss: 0.038 | Tree loss: 6.425 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 030 | Total loss: 6.395 | Reg loss: 0.038 | Tree loss: 6.395 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 030 | Total loss: 6.349 | Reg loss: 0.038 | Tree loss: 6.349 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 030 | Total loss: 6.353 | Reg loss: 0.039 | Tree loss: 6.353 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 030 | Total loss: 6.284 | Reg loss: 0.039 | Tree loss: 6.284 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 030 | Total loss: 6.296 | Reg loss: 0.039 | Tree loss: 6.296 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 030 | Total loss: 6.237 | Reg loss: 0.040 | Tree loss: 6.237 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 030 | Total loss: 6.198 | Reg loss: 0.040 | Tree loss: 6.198 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 030 | Total loss: 6.193 | Reg loss: 0.040 | Tree loss: 6.193 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 030 | Total loss: 6.128 | Reg loss: 0.040 | Tree loss: 6.128 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 030 | Total loss: 6.103 | Reg loss: 0.041 | Tree loss: 6.103 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 030 | Total loss: 6.091 | Reg loss: 0.041 | Tree loss: 6.091 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 030 | Total loss: 6.066 | Reg loss: 0.041 | Tree loss: 6.066 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 030 | Total loss: 6.028 | Reg loss: 0.042 | Tree loss: 6.028 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 030 | Total loss: 5.987 | Reg loss: 0.042 | Tree loss: 5.987 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 030 | Total loss: 5.987 | Reg loss: 0.042 | Tree loss: 5.987 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 030 | Total loss: 5.944 | Reg loss: 0.042 | Tree loss: 5.944 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 13 | Batch: 029 / 030 | Total loss: 5.877 | Reg loss: 0.043 | Tree loss: 5.877 | Accuracy: 0.638095 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 030 | Total loss: 6.572 | Reg loss: 0.037 | Tree loss: 6.572 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 030 | Total loss: 6.587 | Reg loss: 0.037 | Tree loss: 6.587 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 030 | Total loss: 6.522 | Reg loss: 0.037 | Tree loss: 6.522 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 030 | Total loss: 6.528 | Reg loss: 0.037 | Tree loss: 6.528 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 030 | Total loss: 6.499 | Reg loss: 0.037 | Tree loss: 6.499 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 030 | Total loss: 6.434 | Reg loss: 0.037 | Tree loss: 6.434 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 030 | Total loss: 6.345 | Reg loss: 0.037 | Tree loss: 6.345 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 030 | Total loss: 6.346 | Reg loss: 0.038 | Tree loss: 6.346 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 030 | Total loss: 6.369 | Reg loss: 0.038 | Tree loss: 6.369 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 030 | Total loss: 6.303 | Reg loss: 0.038 | Tree loss: 6.303 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 030 | Total loss: 6.311 | Reg loss: 0.038 | Tree loss: 6.311 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 030 | Total loss: 6.225 | Reg loss: 0.038 | Tree loss: 6.225 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 030 | Total loss: 6.186 | Reg loss: 0.039 | Tree loss: 6.186 | Accuracy: 0.564453 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 013 / 030 | Total loss: 6.157 | Reg loss: 0.039 | Tree loss: 6.157 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 030 | Total loss: 6.097 | Reg loss: 0.039 | Tree loss: 6.097 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 030 | Total loss: 6.100 | Reg loss: 0.039 | Tree loss: 6.100 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 030 | Total loss: 6.074 | Reg loss: 0.040 | Tree loss: 6.074 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 030 | Total loss: 6.040 | Reg loss: 0.040 | Tree loss: 6.040 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 030 | Total loss: 6.001 | Reg loss: 0.040 | Tree loss: 6.001 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 030 | Total loss: 5.980 | Reg loss: 0.041 | Tree loss: 5.980 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 030 | Total loss: 5.909 | Reg loss: 0.041 | Tree loss: 5.909 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 030 | Total loss: 5.898 | Reg loss: 0.041 | Tree loss: 5.898 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 030 | Total loss: 5.861 | Reg loss: 0.042 | Tree loss: 5.861 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 030 | Total loss: 5.781 | Reg loss: 0.042 | Tree loss: 5.781 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 030 | Total loss: 5.809 | Reg loss: 0.042 | Tree loss: 5.809 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 030 | Total loss: 5.798 | Reg loss: 0.043 | Tree loss: 5.798 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 030 | Total loss: 5.760 | Reg loss: 0.043 | Tree loss: 5.760 | Accuracy: 0.542969 | 0.906 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 030 | Total loss: 5.714 | Reg loss: 0.043 | Tree loss: 5.714 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 030 | Total loss: 5.678 | Reg loss: 0.043 | Tree loss: 5.678 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 14 | Batch: 029 / 030 | Total loss: 5.676 | Reg loss: 0.044 | Tree loss: 5.676 | Accuracy: 0.590476 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 030 | Total loss: 6.381 | Reg loss: 0.038 | Tree loss: 6.381 | Accuracy: 0.525391 | 0.908 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 030 | Total loss: 6.336 | Reg loss: 0.038 | Tree loss: 6.336 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 030 | Total loss: 6.331 | Reg loss: 0.038 | Tree loss: 6.331 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 030 | Total loss: 6.277 | Reg loss: 0.038 | Tree loss: 6.277 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 030 | Total loss: 6.227 | Reg loss: 0.038 | Tree loss: 6.227 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 030 | Total loss: 6.166 | Reg loss: 0.038 | Tree loss: 6.166 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 030 | Total loss: 6.176 | Reg loss: 0.038 | Tree loss: 6.176 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 030 | Total loss: 6.178 | Reg loss: 0.038 | Tree loss: 6.178 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 030 | Total loss: 6.133 | Reg loss: 0.038 | Tree loss: 6.133 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 030 | Total loss: 6.037 | Reg loss: 0.039 | Tree loss: 6.037 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 030 | Total loss: 6.020 | Reg loss: 0.039 | Tree loss: 6.020 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 030 | Total loss: 6.012 | Reg loss: 0.039 | Tree loss: 6.012 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 030 | Total loss: 5.892 | Reg loss: 0.039 | Tree loss: 5.892 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 030 | Total loss: 5.886 | Reg loss: 0.040 | Tree loss: 5.886 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 030 | Total loss: 5.890 | Reg loss: 0.040 | Tree loss: 5.890 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 030 | Total loss: 5.840 | Reg loss: 0.040 | Tree loss: 5.840 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 030 | Total loss: 5.776 | Reg loss: 0.040 | Tree loss: 5.776 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 030 | Total loss: 5.808 | Reg loss: 0.041 | Tree loss: 5.808 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 030 | Total loss: 5.746 | Reg loss: 0.041 | Tree loss: 5.746 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 030 | Total loss: 5.766 | Reg loss: 0.041 | Tree loss: 5.766 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 030 | Total loss: 5.718 | Reg loss: 0.042 | Tree loss: 5.718 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 030 | Total loss: 5.606 | Reg loss: 0.042 | Tree loss: 5.606 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 030 | Total loss: 5.635 | Reg loss: 0.042 | Tree loss: 5.635 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 030 | Total loss: 5.575 | Reg loss: 0.043 | Tree loss: 5.575 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 030 | Total loss: 5.588 | Reg loss: 0.043 | Tree loss: 5.588 | Accuracy: 0.525391 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 030 | Total loss: 5.536 | Reg loss: 0.043 | Tree loss: 5.536 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 030 | Total loss: 5.471 | Reg loss: 0.044 | Tree loss: 5.471 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 030 | Total loss: 5.447 | Reg loss: 0.044 | Tree loss: 5.447 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 030 | Total loss: 5.463 | Reg loss: 0.044 | Tree loss: 5.463 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 15 | Batch: 029 / 030 | Total loss: 5.370 | Reg loss: 0.045 | Tree loss: 5.370 | Accuracy: 0.628571 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 030 | Total loss: 6.139 | Reg loss: 0.038 | Tree loss: 6.139 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 030 | Total loss: 6.099 | Reg loss: 0.038 | Tree loss: 6.099 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 030 | Total loss: 6.046 | Reg loss: 0.038 | Tree loss: 6.046 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 030 | Total loss: 6.030 | Reg loss: 0.038 | Tree loss: 6.030 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 030 | Total loss: 5.979 | Reg loss: 0.038 | Tree loss: 5.979 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 030 | Total loss: 6.011 | Reg loss: 0.038 | Tree loss: 6.011 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 030 | Total loss: 5.922 | Reg loss: 0.039 | Tree loss: 5.922 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 030 | Total loss: 5.876 | Reg loss: 0.039 | Tree loss: 5.876 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 030 | Total loss: 5.843 | Reg loss: 0.039 | Tree loss: 5.843 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 030 | Total loss: 5.771 | Reg loss: 0.039 | Tree loss: 5.771 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 030 | Total loss: 5.776 | Reg loss: 0.039 | Tree loss: 5.776 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 030 | Total loss: 5.769 | Reg loss: 0.040 | Tree loss: 5.769 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 030 | Total loss: 5.702 | Reg loss: 0.040 | Tree loss: 5.702 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 030 | Total loss: 5.661 | Reg loss: 0.040 | Tree loss: 5.661 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 030 | Total loss: 5.615 | Reg loss: 0.041 | Tree loss: 5.615 | Accuracy: 0.605469 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 015 / 030 | Total loss: 5.621 | Reg loss: 0.041 | Tree loss: 5.621 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 030 | Total loss: 5.595 | Reg loss: 0.041 | Tree loss: 5.595 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 030 | Total loss: 5.507 | Reg loss: 0.042 | Tree loss: 5.507 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 030 | Total loss: 5.555 | Reg loss: 0.042 | Tree loss: 5.555 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 030 | Total loss: 5.493 | Reg loss: 0.042 | Tree loss: 5.493 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 030 | Total loss: 5.449 | Reg loss: 0.043 | Tree loss: 5.449 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 030 | Total loss: 5.416 | Reg loss: 0.043 | Tree loss: 5.416 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 030 | Total loss: 5.357 | Reg loss: 0.043 | Tree loss: 5.357 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 030 | Total loss: 5.338 | Reg loss: 0.044 | Tree loss: 5.338 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 030 | Total loss: 5.269 | Reg loss: 0.044 | Tree loss: 5.269 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 030 | Total loss: 5.248 | Reg loss: 0.044 | Tree loss: 5.248 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 030 | Total loss: 5.217 | Reg loss: 0.045 | Tree loss: 5.217 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 030 | Total loss: 5.218 | Reg loss: 0.045 | Tree loss: 5.218 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 030 | Total loss: 5.171 | Reg loss: 0.046 | Tree loss: 5.171 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 16 | Batch: 029 / 030 | Total loss: 5.086 | Reg loss: 0.046 | Tree loss: 5.086 | Accuracy: 0.571429 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 030 | Total loss: 5.902 | Reg loss: 0.039 | Tree loss: 5.902 | Accuracy: 0.630859 | 0.908 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 030 | Total loss: 5.933 | Reg loss: 0.039 | Tree loss: 5.933 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 030 | Total loss: 5.823 | Reg loss: 0.039 | Tree loss: 5.823 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 030 | Total loss: 5.784 | Reg loss: 0.039 | Tree loss: 5.784 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 030 | Total loss: 5.770 | Reg loss: 0.039 | Tree loss: 5.770 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 030 | Total loss: 5.701 | Reg loss: 0.039 | Tree loss: 5.701 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 030 | Total loss: 5.727 | Reg loss: 0.039 | Tree loss: 5.727 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 030 | Total loss: 5.680 | Reg loss: 0.040 | Tree loss: 5.680 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 030 | Total loss: 5.632 | Reg loss: 0.040 | Tree loss: 5.632 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 030 | Total loss: 5.554 | Reg loss: 0.040 | Tree loss: 5.554 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 030 | Total loss: 5.557 | Reg loss: 0.040 | Tree loss: 5.557 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 030 | Total loss: 5.537 | Reg loss: 0.041 | Tree loss: 5.537 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 030 | Total loss: 5.449 | Reg loss: 0.041 | Tree loss: 5.449 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 030 | Total loss: 5.419 | Reg loss: 0.041 | Tree loss: 5.419 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 030 | Total loss: 5.340 | Reg loss: 0.042 | Tree loss: 5.340 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 030 | Total loss: 5.346 | Reg loss: 0.042 | Tree loss: 5.346 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 030 | Total loss: 5.324 | Reg loss: 0.042 | Tree loss: 5.324 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 030 | Total loss: 5.276 | Reg loss: 0.043 | Tree loss: 5.276 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 030 | Total loss: 5.247 | Reg loss: 0.043 | Tree loss: 5.247 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 030 | Total loss: 5.158 | Reg loss: 0.043 | Tree loss: 5.158 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 030 | Total loss: 5.164 | Reg loss: 0.044 | Tree loss: 5.164 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 030 | Total loss: 5.171 | Reg loss: 0.044 | Tree loss: 5.171 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 030 | Total loss: 5.099 | Reg loss: 0.045 | Tree loss: 5.099 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 030 | Total loss: 5.025 | Reg loss: 0.045 | Tree loss: 5.025 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 030 | Total loss: 5.030 | Reg loss: 0.045 | Tree loss: 5.030 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 030 | Total loss: 4.952 | Reg loss: 0.046 | Tree loss: 4.952 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 030 | Total loss: 4.914 | Reg loss: 0.046 | Tree loss: 4.914 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 030 | Total loss: 4.905 | Reg loss: 0.047 | Tree loss: 4.905 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 030 | Total loss: 4.884 | Reg loss: 0.047 | Tree loss: 4.884 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 17 | Batch: 029 / 030 | Total loss: 4.811 | Reg loss: 0.047 | Tree loss: 4.811 | Accuracy: 0.580952 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 030 | Total loss: 5.750 | Reg loss: 0.040 | Tree loss: 5.750 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 030 | Total loss: 5.667 | Reg loss: 0.040 | Tree loss: 5.667 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 030 | Total loss: 5.635 | Reg loss: 0.040 | Tree loss: 5.635 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 030 | Total loss: 5.619 | Reg loss: 0.040 | Tree loss: 5.619 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 030 | Total loss: 5.570 | Reg loss: 0.040 | Tree loss: 5.570 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 030 | Total loss: 5.552 | Reg loss: 0.040 | Tree loss: 5.552 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 030 | Total loss: 5.458 | Reg loss: 0.040 | Tree loss: 5.458 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 030 | Total loss: 5.386 | Reg loss: 0.040 | Tree loss: 5.386 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 030 | Total loss: 5.387 | Reg loss: 0.041 | Tree loss: 5.387 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 030 | Total loss: 5.333 | Reg loss: 0.041 | Tree loss: 5.333 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 030 | Total loss: 5.319 | Reg loss: 0.041 | Tree loss: 5.319 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 030 | Total loss: 5.249 | Reg loss: 0.041 | Tree loss: 5.249 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 030 | Total loss: 5.219 | Reg loss: 0.042 | Tree loss: 5.219 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 030 | Total loss: 5.169 | Reg loss: 0.042 | Tree loss: 5.169 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 030 | Total loss: 5.174 | Reg loss: 0.042 | Tree loss: 5.174 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 030 | Total loss: 5.133 | Reg loss: 0.043 | Tree loss: 5.133 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 030 | Total loss: 5.069 | Reg loss: 0.043 | Tree loss: 5.069 | Accuracy: 0.548828 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 017 / 030 | Total loss: 5.020 | Reg loss: 0.044 | Tree loss: 5.020 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 030 | Total loss: 4.999 | Reg loss: 0.044 | Tree loss: 4.999 | Accuracy: 0.521484 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 030 | Total loss: 4.932 | Reg loss: 0.044 | Tree loss: 4.932 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 030 | Total loss: 4.941 | Reg loss: 0.045 | Tree loss: 4.941 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 030 | Total loss: 4.879 | Reg loss: 0.045 | Tree loss: 4.879 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 030 | Total loss: 4.793 | Reg loss: 0.046 | Tree loss: 4.793 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 030 | Total loss: 4.772 | Reg loss: 0.046 | Tree loss: 4.772 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 030 | Total loss: 4.788 | Reg loss: 0.046 | Tree loss: 4.788 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 030 | Total loss: 4.749 | Reg loss: 0.047 | Tree loss: 4.749 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 030 | Total loss: 4.708 | Reg loss: 0.047 | Tree loss: 4.708 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 030 | Total loss: 4.626 | Reg loss: 0.048 | Tree loss: 4.626 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 030 | Total loss: 4.631 | Reg loss: 0.048 | Tree loss: 4.631 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 18 | Batch: 029 / 030 | Total loss: 4.638 | Reg loss: 0.048 | Tree loss: 4.638 | Accuracy: 0.600000 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 030 | Total loss: 5.548 | Reg loss: 0.040 | Tree loss: 5.548 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 030 | Total loss: 5.469 | Reg loss: 0.040 | Tree loss: 5.469 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 030 | Total loss: 5.465 | Reg loss: 0.040 | Tree loss: 5.465 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 030 | Total loss: 5.422 | Reg loss: 0.041 | Tree loss: 5.422 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 030 | Total loss: 5.327 | Reg loss: 0.041 | Tree loss: 5.327 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 030 | Total loss: 5.349 | Reg loss: 0.041 | Tree loss: 5.349 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 030 | Total loss: 5.268 | Reg loss: 0.041 | Tree loss: 5.268 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 030 | Total loss: 5.241 | Reg loss: 0.041 | Tree loss: 5.241 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 030 | Total loss: 5.201 | Reg loss: 0.041 | Tree loss: 5.201 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 030 | Total loss: 5.130 | Reg loss: 0.042 | Tree loss: 5.130 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 030 | Total loss: 5.075 | Reg loss: 0.042 | Tree loss: 5.075 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 030 | Total loss: 5.031 | Reg loss: 0.042 | Tree loss: 5.031 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 030 | Total loss: 5.024 | Reg loss: 0.042 | Tree loss: 5.024 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 030 | Total loss: 5.004 | Reg loss: 0.043 | Tree loss: 5.004 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 030 | Total loss: 4.904 | Reg loss: 0.043 | Tree loss: 4.904 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 030 | Total loss: 4.877 | Reg loss: 0.043 | Tree loss: 4.877 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 030 | Total loss: 4.838 | Reg loss: 0.044 | Tree loss: 4.838 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 030 | Total loss: 4.830 | Reg loss: 0.044 | Tree loss: 4.830 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 030 | Total loss: 4.763 | Reg loss: 0.045 | Tree loss: 4.763 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 030 | Total loss: 4.734 | Reg loss: 0.045 | Tree loss: 4.734 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 030 | Total loss: 4.654 | Reg loss: 0.045 | Tree loss: 4.654 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 030 | Total loss: 4.620 | Reg loss: 0.046 | Tree loss: 4.620 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 030 | Total loss: 4.600 | Reg loss: 0.046 | Tree loss: 4.600 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 030 | Total loss: 4.583 | Reg loss: 0.047 | Tree loss: 4.583 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 030 | Total loss: 4.532 | Reg loss: 0.047 | Tree loss: 4.532 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 030 | Total loss: 4.523 | Reg loss: 0.047 | Tree loss: 4.523 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 030 | Total loss: 4.504 | Reg loss: 0.048 | Tree loss: 4.504 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 030 | Total loss: 4.444 | Reg loss: 0.048 | Tree loss: 4.444 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 030 | Total loss: 4.384 | Reg loss: 0.048 | Tree loss: 4.384 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 19 | Batch: 029 / 030 | Total loss: 4.379 | Reg loss: 0.049 | Tree loss: 4.379 | Accuracy: 0.533333 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 030 | Total loss: 5.322 | Reg loss: 0.041 | Tree loss: 5.322 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 030 | Total loss: 5.338 | Reg loss: 0.041 | Tree loss: 5.338 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 030 | Total loss: 5.228 | Reg loss: 0.041 | Tree loss: 5.228 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 030 | Total loss: 5.230 | Reg loss: 0.041 | Tree loss: 5.230 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 030 | Total loss: 5.187 | Reg loss: 0.041 | Tree loss: 5.187 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 030 | Total loss: 5.124 | Reg loss: 0.041 | Tree loss: 5.124 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 030 | Total loss: 5.118 | Reg loss: 0.042 | Tree loss: 5.118 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 030 | Total loss: 4.977 | Reg loss: 0.042 | Tree loss: 4.977 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 030 | Total loss: 4.971 | Reg loss: 0.042 | Tree loss: 4.971 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 030 | Total loss: 4.960 | Reg loss: 0.042 | Tree loss: 4.960 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 030 | Total loss: 4.931 | Reg loss: 0.042 | Tree loss: 4.931 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 030 | Total loss: 4.922 | Reg loss: 0.043 | Tree loss: 4.922 | Accuracy: 0.501953 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 030 | Total loss: 4.794 | Reg loss: 0.043 | Tree loss: 4.794 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 030 | Total loss: 4.714 | Reg loss: 0.043 | Tree loss: 4.714 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 030 | Total loss: 4.686 | Reg loss: 0.044 | Tree loss: 4.686 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 030 | Total loss: 4.727 | Reg loss: 0.044 | Tree loss: 4.727 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 030 | Total loss: 4.647 | Reg loss: 0.044 | Tree loss: 4.647 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 030 | Total loss: 4.616 | Reg loss: 0.045 | Tree loss: 4.616 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 030 | Total loss: 4.562 | Reg loss: 0.045 | Tree loss: 4.562 | Accuracy: 0.599609 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 019 / 030 | Total loss: 4.561 | Reg loss: 0.045 | Tree loss: 4.561 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 030 | Total loss: 4.464 | Reg loss: 0.046 | Tree loss: 4.464 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 030 | Total loss: 4.476 | Reg loss: 0.046 | Tree loss: 4.476 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 030 | Total loss: 4.425 | Reg loss: 0.047 | Tree loss: 4.425 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 030 | Total loss: 4.408 | Reg loss: 0.047 | Tree loss: 4.408 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 030 | Total loss: 4.320 | Reg loss: 0.047 | Tree loss: 4.320 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 030 | Total loss: 4.341 | Reg loss: 0.048 | Tree loss: 4.341 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 030 | Total loss: 4.274 | Reg loss: 0.048 | Tree loss: 4.274 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 030 | Total loss: 4.191 | Reg loss: 0.048 | Tree loss: 4.191 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 030 | Total loss: 4.186 | Reg loss: 0.049 | Tree loss: 4.186 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 20 | Batch: 029 / 030 | Total loss: 4.174 | Reg loss: 0.049 | Tree loss: 4.174 | Accuracy: 0.571429 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 030 | Total loss: 5.208 | Reg loss: 0.042 | Tree loss: 5.208 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 030 | Total loss: 5.079 | Reg loss: 0.042 | Tree loss: 5.079 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 030 | Total loss: 5.038 | Reg loss: 0.042 | Tree loss: 5.038 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 030 | Total loss: 4.954 | Reg loss: 0.042 | Tree loss: 4.954 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 030 | Total loss: 5.009 | Reg loss: 0.042 | Tree loss: 5.009 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 030 | Total loss: 4.888 | Reg loss: 0.042 | Tree loss: 4.888 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 030 | Total loss: 4.889 | Reg loss: 0.042 | Tree loss: 4.889 | Accuracy: 0.511719 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 030 | Total loss: 4.832 | Reg loss: 0.042 | Tree loss: 4.832 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 030 | Total loss: 4.782 | Reg loss: 0.043 | Tree loss: 4.782 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 030 | Total loss: 4.759 | Reg loss: 0.043 | Tree loss: 4.759 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 030 | Total loss: 4.720 | Reg loss: 0.043 | Tree loss: 4.720 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 030 | Total loss: 4.683 | Reg loss: 0.043 | Tree loss: 4.683 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 030 | Total loss: 4.659 | Reg loss: 0.044 | Tree loss: 4.659 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 030 | Total loss: 4.540 | Reg loss: 0.044 | Tree loss: 4.540 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 030 | Total loss: 4.530 | Reg loss: 0.044 | Tree loss: 4.530 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 030 | Total loss: 4.493 | Reg loss: 0.045 | Tree loss: 4.493 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 030 | Total loss: 4.473 | Reg loss: 0.045 | Tree loss: 4.473 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 030 | Total loss: 4.463 | Reg loss: 0.045 | Tree loss: 4.463 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 030 | Total loss: 4.443 | Reg loss: 0.046 | Tree loss: 4.443 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 030 | Total loss: 4.343 | Reg loss: 0.046 | Tree loss: 4.343 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 030 | Total loss: 4.307 | Reg loss: 0.046 | Tree loss: 4.307 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 030 | Total loss: 4.240 | Reg loss: 0.047 | Tree loss: 4.240 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 030 | Total loss: 4.259 | Reg loss: 0.047 | Tree loss: 4.259 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 030 | Total loss: 4.178 | Reg loss: 0.047 | Tree loss: 4.178 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 030 | Total loss: 4.187 | Reg loss: 0.048 | Tree loss: 4.187 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 030 | Total loss: 4.113 | Reg loss: 0.048 | Tree loss: 4.113 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 030 | Total loss: 4.099 | Reg loss: 0.049 | Tree loss: 4.099 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 030 | Total loss: 4.055 | Reg loss: 0.049 | Tree loss: 4.055 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 030 | Total loss: 4.024 | Reg loss: 0.049 | Tree loss: 4.024 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 21 | Batch: 029 / 030 | Total loss: 3.917 | Reg loss: 0.050 | Tree loss: 3.917 | Accuracy: 0.571429 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 030 | Total loss: 5.010 | Reg loss: 0.043 | Tree loss: 5.010 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 030 | Total loss: 4.960 | Reg loss: 0.043 | Tree loss: 4.960 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 030 | Total loss: 4.933 | Reg loss: 0.043 | Tree loss: 4.933 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 030 | Total loss: 4.821 | Reg loss: 0.043 | Tree loss: 4.821 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 030 | Total loss: 4.807 | Reg loss: 0.043 | Tree loss: 4.807 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 030 | Total loss: 4.711 | Reg loss: 0.043 | Tree loss: 4.711 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 030 | Total loss: 4.628 | Reg loss: 0.043 | Tree loss: 4.628 | Accuracy: 0.619141 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 030 | Total loss: 4.652 | Reg loss: 0.043 | Tree loss: 4.652 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 030 | Total loss: 4.629 | Reg loss: 0.044 | Tree loss: 4.629 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 030 | Total loss: 4.549 | Reg loss: 0.044 | Tree loss: 4.549 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 030 | Total loss: 4.496 | Reg loss: 0.044 | Tree loss: 4.496 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 030 | Total loss: 4.508 | Reg loss: 0.044 | Tree loss: 4.508 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 030 | Total loss: 4.417 | Reg loss: 0.044 | Tree loss: 4.417 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 030 | Total loss: 4.366 | Reg loss: 0.045 | Tree loss: 4.366 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 030 | Total loss: 4.403 | Reg loss: 0.045 | Tree loss: 4.403 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 030 | Total loss: 4.318 | Reg loss: 0.045 | Tree loss: 4.318 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 030 | Total loss: 4.299 | Reg loss: 0.046 | Tree loss: 4.299 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 030 | Total loss: 4.200 | Reg loss: 0.046 | Tree loss: 4.200 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 030 | Total loss: 4.242 | Reg loss: 0.046 | Tree loss: 4.242 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 030 | Total loss: 4.181 | Reg loss: 0.047 | Tree loss: 4.181 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 030 | Total loss: 4.103 | Reg loss: 0.047 | Tree loss: 4.103 | Accuracy: 0.574219 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batch: 021 / 030 | Total loss: 4.123 | Reg loss: 0.047 | Tree loss: 4.123 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 030 | Total loss: 4.042 | Reg loss: 0.048 | Tree loss: 4.042 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 030 | Total loss: 4.025 | Reg loss: 0.048 | Tree loss: 4.025 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 030 | Total loss: 3.990 | Reg loss: 0.048 | Tree loss: 3.990 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 030 | Total loss: 3.902 | Reg loss: 0.049 | Tree loss: 3.902 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 030 | Total loss: 3.932 | Reg loss: 0.049 | Tree loss: 3.932 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 030 | Total loss: 3.877 | Reg loss: 0.050 | Tree loss: 3.877 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 030 | Total loss: 3.801 | Reg loss: 0.050 | Tree loss: 3.801 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 22 | Batch: 029 / 030 | Total loss: 3.794 | Reg loss: 0.050 | Tree loss: 3.794 | Accuracy: 0.580952 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 030 | Total loss: 4.799 | Reg loss: 0.044 | Tree loss: 4.799 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 030 | Total loss: 4.682 | Reg loss: 0.044 | Tree loss: 4.682 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 030 | Total loss: 4.731 | Reg loss: 0.044 | Tree loss: 4.731 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 030 | Total loss: 4.674 | Reg loss: 0.044 | Tree loss: 4.674 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 030 | Total loss: 4.635 | Reg loss: 0.044 | Tree loss: 4.635 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 030 | Total loss: 4.600 | Reg loss: 0.044 | Tree loss: 4.600 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 030 | Total loss: 4.513 | Reg loss: 0.044 | Tree loss: 4.513 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 030 | Total loss: 4.440 | Reg loss: 0.044 | Tree loss: 4.440 | Accuracy: 0.611328 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 030 | Total loss: 4.394 | Reg loss: 0.044 | Tree loss: 4.394 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 030 | Total loss: 4.406 | Reg loss: 0.045 | Tree loss: 4.406 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 030 | Total loss: 4.374 | Reg loss: 0.045 | Tree loss: 4.374 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 030 | Total loss: 4.308 | Reg loss: 0.045 | Tree loss: 4.308 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 030 | Total loss: 4.313 | Reg loss: 0.045 | Tree loss: 4.313 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 030 | Total loss: 4.227 | Reg loss: 0.046 | Tree loss: 4.227 | Accuracy: 0.529297 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 030 | Total loss: 4.213 | Reg loss: 0.046 | Tree loss: 4.213 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 030 | Total loss: 4.120 | Reg loss: 0.046 | Tree loss: 4.120 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 030 | Total loss: 4.121 | Reg loss: 0.047 | Tree loss: 4.121 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 030 | Total loss: 4.061 | Reg loss: 0.047 | Tree loss: 4.061 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 030 | Total loss: 4.002 | Reg loss: 0.047 | Tree loss: 4.002 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 030 | Total loss: 3.956 | Reg loss: 0.047 | Tree loss: 3.956 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 030 | Total loss: 3.894 | Reg loss: 0.048 | Tree loss: 3.894 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 030 | Total loss: 3.905 | Reg loss: 0.048 | Tree loss: 3.905 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 030 | Total loss: 3.879 | Reg loss: 0.049 | Tree loss: 3.879 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 030 | Total loss: 3.853 | Reg loss: 0.049 | Tree loss: 3.853 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 030 | Total loss: 3.798 | Reg loss: 0.049 | Tree loss: 3.798 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 030 | Total loss: 3.790 | Reg loss: 0.050 | Tree loss: 3.790 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 030 | Total loss: 3.770 | Reg loss: 0.050 | Tree loss: 3.770 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 030 | Total loss: 3.665 | Reg loss: 0.050 | Tree loss: 3.665 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 030 | Total loss: 3.650 | Reg loss: 0.051 | Tree loss: 3.650 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 23 | Batch: 029 / 030 | Total loss: 3.738 | Reg loss: 0.051 | Tree loss: 3.738 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 030 | Total loss: 4.593 | Reg loss: 0.044 | Tree loss: 4.593 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 030 | Total loss: 4.586 | Reg loss: 0.044 | Tree loss: 4.586 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 030 | Total loss: 4.552 | Reg loss: 0.044 | Tree loss: 4.552 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 030 | Total loss: 4.528 | Reg loss: 0.045 | Tree loss: 4.528 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 030 | Total loss: 4.492 | Reg loss: 0.045 | Tree loss: 4.492 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 030 | Total loss: 4.380 | Reg loss: 0.045 | Tree loss: 4.380 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 030 | Total loss: 4.353 | Reg loss: 0.045 | Tree loss: 4.353 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 030 | Total loss: 4.323 | Reg loss: 0.045 | Tree loss: 4.323 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 030 | Total loss: 4.239 | Reg loss: 0.045 | Tree loss: 4.239 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 030 | Total loss: 4.229 | Reg loss: 0.045 | Tree loss: 4.229 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 030 | Total loss: 4.183 | Reg loss: 0.046 | Tree loss: 4.183 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 030 | Total loss: 4.186 | Reg loss: 0.046 | Tree loss: 4.186 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 030 | Total loss: 4.156 | Reg loss: 0.046 | Tree loss: 4.156 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 030 | Total loss: 4.022 | Reg loss: 0.046 | Tree loss: 4.022 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 030 | Total loss: 4.015 | Reg loss: 0.047 | Tree loss: 4.015 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 030 | Total loss: 3.936 | Reg loss: 0.047 | Tree loss: 3.936 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 030 | Total loss: 3.885 | Reg loss: 0.047 | Tree loss: 3.885 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 030 | Total loss: 3.853 | Reg loss: 0.048 | Tree loss: 3.853 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 030 | Total loss: 3.793 | Reg loss: 0.048 | Tree loss: 3.793 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 030 | Total loss: 3.803 | Reg loss: 0.048 | Tree loss: 3.803 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 030 | Total loss: 3.799 | Reg loss: 0.049 | Tree loss: 3.799 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 030 | Total loss: 3.719 | Reg loss: 0.049 | Tree loss: 3.719 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 030 | Total loss: 3.686 | Reg loss: 0.049 | Tree loss: 3.686 | Accuracy: 0.582031 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Batch: 023 / 030 | Total loss: 3.643 | Reg loss: 0.050 | Tree loss: 3.643 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 030 | Total loss: 3.606 | Reg loss: 0.050 | Tree loss: 3.606 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 030 | Total loss: 3.607 | Reg loss: 0.050 | Tree loss: 3.607 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 030 | Total loss: 3.549 | Reg loss: 0.051 | Tree loss: 3.549 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 030 | Total loss: 3.521 | Reg loss: 0.051 | Tree loss: 3.521 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 030 | Total loss: 3.504 | Reg loss: 0.051 | Tree loss: 3.504 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 24 | Batch: 029 / 030 | Total loss: 3.517 | Reg loss: 0.051 | Tree loss: 3.517 | Accuracy: 0.514286 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 030 | Total loss: 4.464 | Reg loss: 0.045 | Tree loss: 4.464 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 030 | Total loss: 4.356 | Reg loss: 0.045 | Tree loss: 4.356 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 030 | Total loss: 4.372 | Reg loss: 0.045 | Tree loss: 4.372 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 030 | Total loss: 4.305 | Reg loss: 0.045 | Tree loss: 4.305 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 030 | Total loss: 4.302 | Reg loss: 0.045 | Tree loss: 4.302 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 030 | Total loss: 4.242 | Reg loss: 0.046 | Tree loss: 4.242 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 030 | Total loss: 4.214 | Reg loss: 0.046 | Tree loss: 4.214 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 030 | Total loss: 4.128 | Reg loss: 0.046 | Tree loss: 4.128 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 030 | Total loss: 4.093 | Reg loss: 0.046 | Tree loss: 4.093 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 030 | Total loss: 4.083 | Reg loss: 0.046 | Tree loss: 4.083 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 030 | Total loss: 3.995 | Reg loss: 0.046 | Tree loss: 3.995 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 030 | Total loss: 3.983 | Reg loss: 0.047 | Tree loss: 3.983 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 030 | Total loss: 3.908 | Reg loss: 0.047 | Tree loss: 3.908 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 030 | Total loss: 3.899 | Reg loss: 0.047 | Tree loss: 3.899 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 030 | Total loss: 3.851 | Reg loss: 0.047 | Tree loss: 3.851 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 030 | Total loss: 3.797 | Reg loss: 0.048 | Tree loss: 3.797 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 030 | Total loss: 3.776 | Reg loss: 0.048 | Tree loss: 3.776 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 030 | Total loss: 3.707 | Reg loss: 0.048 | Tree loss: 3.707 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 030 | Total loss: 3.685 | Reg loss: 0.049 | Tree loss: 3.685 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 030 | Total loss: 3.577 | Reg loss: 0.049 | Tree loss: 3.577 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 030 | Total loss: 3.666 | Reg loss: 0.049 | Tree loss: 3.666 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 030 | Total loss: 3.557 | Reg loss: 0.050 | Tree loss: 3.557 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 030 | Total loss: 3.504 | Reg loss: 0.050 | Tree loss: 3.504 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 030 | Total loss: 3.461 | Reg loss: 0.050 | Tree loss: 3.461 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 030 | Total loss: 3.498 | Reg loss: 0.051 | Tree loss: 3.498 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 030 | Total loss: 3.402 | Reg loss: 0.051 | Tree loss: 3.402 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 030 | Total loss: 3.408 | Reg loss: 0.051 | Tree loss: 3.408 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 030 | Total loss: 3.322 | Reg loss: 0.051 | Tree loss: 3.322 | Accuracy: 0.632812 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 030 | Total loss: 3.303 | Reg loss: 0.052 | Tree loss: 3.303 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 25 | Batch: 029 / 030 | Total loss: 3.333 | Reg loss: 0.052 | Tree loss: 3.333 | Accuracy: 0.523810 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 030 | Total loss: 4.231 | Reg loss: 0.046 | Tree loss: 4.231 | Accuracy: 0.607422 | 0.908 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 030 | Total loss: 4.249 | Reg loss: 0.046 | Tree loss: 4.249 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 030 | Total loss: 4.222 | Reg loss: 0.046 | Tree loss: 4.222 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 030 | Total loss: 4.109 | Reg loss: 0.046 | Tree loss: 4.109 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 030 | Total loss: 4.066 | Reg loss: 0.046 | Tree loss: 4.066 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 030 | Total loss: 4.087 | Reg loss: 0.046 | Tree loss: 4.087 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 030 | Total loss: 4.072 | Reg loss: 0.047 | Tree loss: 4.072 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 030 | Total loss: 4.010 | Reg loss: 0.047 | Tree loss: 4.010 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 030 | Total loss: 3.893 | Reg loss: 0.047 | Tree loss: 3.893 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 030 | Total loss: 3.910 | Reg loss: 0.047 | Tree loss: 3.910 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 030 | Total loss: 3.848 | Reg loss: 0.047 | Tree loss: 3.848 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 030 | Total loss: 3.812 | Reg loss: 0.048 | Tree loss: 3.812 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 030 | Total loss: 3.729 | Reg loss: 0.048 | Tree loss: 3.729 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 030 | Total loss: 3.729 | Reg loss: 0.048 | Tree loss: 3.729 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 030 | Total loss: 3.630 | Reg loss: 0.048 | Tree loss: 3.630 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 030 | Total loss: 3.631 | Reg loss: 0.049 | Tree loss: 3.631 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 030 | Total loss: 3.622 | Reg loss: 0.049 | Tree loss: 3.622 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 030 | Total loss: 3.550 | Reg loss: 0.049 | Tree loss: 3.550 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 030 | Total loss: 3.531 | Reg loss: 0.049 | Tree loss: 3.531 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 030 | Total loss: 3.474 | Reg loss: 0.050 | Tree loss: 3.474 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 030 | Total loss: 3.441 | Reg loss: 0.050 | Tree loss: 3.441 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 030 | Total loss: 3.396 | Reg loss: 0.050 | Tree loss: 3.396 | Accuracy: 0.525391 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 030 | Total loss: 3.380 | Reg loss: 0.051 | Tree loss: 3.380 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 030 | Total loss: 3.356 | Reg loss: 0.051 | Tree loss: 3.356 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 030 | Total loss: 3.288 | Reg loss: 0.051 | Tree loss: 3.288 | Accuracy: 0.609375 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 025 / 030 | Total loss: 3.213 | Reg loss: 0.051 | Tree loss: 3.213 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 030 | Total loss: 3.241 | Reg loss: 0.052 | Tree loss: 3.241 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 030 | Total loss: 3.213 | Reg loss: 0.052 | Tree loss: 3.213 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 030 | Total loss: 3.160 | Reg loss: 0.052 | Tree loss: 3.160 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 26 | Batch: 029 / 030 | Total loss: 3.023 | Reg loss: 0.053 | Tree loss: 3.023 | Accuracy: 0.638095 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 030 | Total loss: 4.138 | Reg loss: 0.047 | Tree loss: 4.138 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 030 | Total loss: 4.021 | Reg loss: 0.047 | Tree loss: 4.021 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 030 | Total loss: 4.014 | Reg loss: 0.047 | Tree loss: 4.014 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 030 | Total loss: 3.974 | Reg loss: 0.047 | Tree loss: 3.974 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 030 | Total loss: 3.972 | Reg loss: 0.047 | Tree loss: 3.972 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 030 | Total loss: 3.863 | Reg loss: 0.047 | Tree loss: 3.863 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 030 | Total loss: 3.846 | Reg loss: 0.047 | Tree loss: 3.846 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 030 | Total loss: 3.864 | Reg loss: 0.048 | Tree loss: 3.864 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 030 | Total loss: 3.767 | Reg loss: 0.048 | Tree loss: 3.767 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 030 | Total loss: 3.740 | Reg loss: 0.048 | Tree loss: 3.740 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 030 | Total loss: 3.659 | Reg loss: 0.048 | Tree loss: 3.659 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 030 | Total loss: 3.674 | Reg loss: 0.048 | Tree loss: 3.674 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 030 | Total loss: 3.620 | Reg loss: 0.049 | Tree loss: 3.620 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 030 | Total loss: 3.549 | Reg loss: 0.049 | Tree loss: 3.549 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 030 | Total loss: 3.444 | Reg loss: 0.049 | Tree loss: 3.444 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 030 | Total loss: 3.421 | Reg loss: 0.049 | Tree loss: 3.421 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 030 | Total loss: 3.448 | Reg loss: 0.050 | Tree loss: 3.448 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 030 | Total loss: 3.368 | Reg loss: 0.050 | Tree loss: 3.368 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 030 | Total loss: 3.388 | Reg loss: 0.050 | Tree loss: 3.388 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 030 | Total loss: 3.302 | Reg loss: 0.050 | Tree loss: 3.302 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 030 | Total loss: 3.229 | Reg loss: 0.051 | Tree loss: 3.229 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 030 | Total loss: 3.201 | Reg loss: 0.051 | Tree loss: 3.201 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 030 | Total loss: 3.192 | Reg loss: 0.051 | Tree loss: 3.192 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 030 | Total loss: 3.159 | Reg loss: 0.052 | Tree loss: 3.159 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 030 | Total loss: 3.143 | Reg loss: 0.052 | Tree loss: 3.143 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 030 | Total loss: 3.133 | Reg loss: 0.052 | Tree loss: 3.133 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 030 | Total loss: 3.105 | Reg loss: 0.052 | Tree loss: 3.105 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 030 | Total loss: 3.038 | Reg loss: 0.053 | Tree loss: 3.038 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 030 | Total loss: 2.992 | Reg loss: 0.053 | Tree loss: 2.992 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 27 | Batch: 029 / 030 | Total loss: 2.993 | Reg loss: 0.053 | Tree loss: 2.993 | Accuracy: 0.628571 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 030 | Total loss: 3.960 | Reg loss: 0.048 | Tree loss: 3.960 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 030 | Total loss: 3.938 | Reg loss: 0.048 | Tree loss: 3.938 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 030 | Total loss: 3.885 | Reg loss: 0.048 | Tree loss: 3.885 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 030 | Total loss: 3.807 | Reg loss: 0.048 | Tree loss: 3.807 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 030 | Total loss: 3.766 | Reg loss: 0.048 | Tree loss: 3.766 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 030 | Total loss: 3.707 | Reg loss: 0.048 | Tree loss: 3.707 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 030 | Total loss: 3.686 | Reg loss: 0.048 | Tree loss: 3.686 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 030 | Total loss: 3.637 | Reg loss: 0.048 | Tree loss: 3.637 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 030 | Total loss: 3.570 | Reg loss: 0.049 | Tree loss: 3.570 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 030 | Total loss: 3.589 | Reg loss: 0.049 | Tree loss: 3.589 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 030 | Total loss: 3.487 | Reg loss: 0.049 | Tree loss: 3.487 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 030 | Total loss: 3.454 | Reg loss: 0.049 | Tree loss: 3.454 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 030 | Total loss: 3.427 | Reg loss: 0.049 | Tree loss: 3.427 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 030 | Total loss: 3.395 | Reg loss: 0.050 | Tree loss: 3.395 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 030 | Total loss: 3.332 | Reg loss: 0.050 | Tree loss: 3.332 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 030 | Total loss: 3.294 | Reg loss: 0.050 | Tree loss: 3.294 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 030 | Total loss: 3.313 | Reg loss: 0.050 | Tree loss: 3.313 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 030 | Total loss: 3.215 | Reg loss: 0.051 | Tree loss: 3.215 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 030 | Total loss: 3.209 | Reg loss: 0.051 | Tree loss: 3.209 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 030 | Total loss: 3.121 | Reg loss: 0.051 | Tree loss: 3.121 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 030 | Total loss: 3.128 | Reg loss: 0.051 | Tree loss: 3.128 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 030 | Total loss: 3.113 | Reg loss: 0.052 | Tree loss: 3.113 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 030 | Total loss: 3.080 | Reg loss: 0.052 | Tree loss: 3.080 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 030 | Total loss: 3.004 | Reg loss: 0.052 | Tree loss: 3.004 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 030 | Total loss: 3.022 | Reg loss: 0.053 | Tree loss: 3.022 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 030 | Total loss: 3.014 | Reg loss: 0.053 | Tree loss: 3.014 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 030 | Total loss: 2.925 | Reg loss: 0.053 | Tree loss: 2.925 | Accuracy: 0.605469 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 027 / 030 | Total loss: 2.909 | Reg loss: 0.053 | Tree loss: 2.909 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 030 | Total loss: 2.820 | Reg loss: 0.054 | Tree loss: 2.820 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 28 | Batch: 029 / 030 | Total loss: 2.838 | Reg loss: 0.054 | Tree loss: 2.838 | Accuracy: 0.619048 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 030 | Total loss: 3.819 | Reg loss: 0.049 | Tree loss: 3.819 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 030 | Total loss: 3.782 | Reg loss: 0.049 | Tree loss: 3.782 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 030 | Total loss: 3.704 | Reg loss: 0.049 | Tree loss: 3.704 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 030 | Total loss: 3.628 | Reg loss: 0.049 | Tree loss: 3.628 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 030 | Total loss: 3.670 | Reg loss: 0.049 | Tree loss: 3.670 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 030 | Total loss: 3.563 | Reg loss: 0.049 | Tree loss: 3.563 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 030 | Total loss: 3.527 | Reg loss: 0.049 | Tree loss: 3.527 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 030 | Total loss: 3.593 | Reg loss: 0.049 | Tree loss: 3.593 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 030 | Total loss: 3.455 | Reg loss: 0.049 | Tree loss: 3.455 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 030 | Total loss: 3.380 | Reg loss: 0.050 | Tree loss: 3.380 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 030 | Total loss: 3.394 | Reg loss: 0.050 | Tree loss: 3.394 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 030 | Total loss: 3.362 | Reg loss: 0.050 | Tree loss: 3.362 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 030 | Total loss: 3.269 | Reg loss: 0.050 | Tree loss: 3.269 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 030 | Total loss: 3.220 | Reg loss: 0.050 | Tree loss: 3.220 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 030 | Total loss: 3.218 | Reg loss: 0.051 | Tree loss: 3.218 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 030 | Total loss: 3.184 | Reg loss: 0.051 | Tree loss: 3.184 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 030 | Total loss: 3.086 | Reg loss: 0.051 | Tree loss: 3.086 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 030 | Total loss: 3.079 | Reg loss: 0.051 | Tree loss: 3.079 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 030 | Total loss: 3.019 | Reg loss: 0.052 | Tree loss: 3.019 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 030 | Total loss: 3.007 | Reg loss: 0.052 | Tree loss: 3.007 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 030 | Total loss: 2.953 | Reg loss: 0.052 | Tree loss: 2.953 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 030 | Total loss: 2.937 | Reg loss: 0.052 | Tree loss: 2.937 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 030 | Total loss: 2.894 | Reg loss: 0.053 | Tree loss: 2.894 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 030 | Total loss: 2.855 | Reg loss: 0.053 | Tree loss: 2.855 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 030 | Total loss: 2.900 | Reg loss: 0.053 | Tree loss: 2.900 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 030 | Total loss: 2.827 | Reg loss: 0.053 | Tree loss: 2.827 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 030 | Total loss: 2.740 | Reg loss: 0.054 | Tree loss: 2.740 | Accuracy: 0.628906 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 030 | Total loss: 2.746 | Reg loss: 0.054 | Tree loss: 2.746 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 030 | Total loss: 2.718 | Reg loss: 0.054 | Tree loss: 2.718 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 29 | Batch: 029 / 030 | Total loss: 2.703 | Reg loss: 0.054 | Tree loss: 2.703 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 030 | Total loss: 3.702 | Reg loss: 0.049 | Tree loss: 3.702 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 030 | Total loss: 3.674 | Reg loss: 0.049 | Tree loss: 3.674 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 030 | Total loss: 3.613 | Reg loss: 0.049 | Tree loss: 3.613 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 030 | Total loss: 3.531 | Reg loss: 0.050 | Tree loss: 3.531 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 030 | Total loss: 3.496 | Reg loss: 0.050 | Tree loss: 3.496 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 030 | Total loss: 3.420 | Reg loss: 0.050 | Tree loss: 3.420 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 030 | Total loss: 3.432 | Reg loss: 0.050 | Tree loss: 3.432 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 030 | Total loss: 3.321 | Reg loss: 0.050 | Tree loss: 3.321 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 030 | Total loss: 3.297 | Reg loss: 0.050 | Tree loss: 3.297 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 030 | Total loss: 3.320 | Reg loss: 0.050 | Tree loss: 3.320 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 030 | Total loss: 3.180 | Reg loss: 0.050 | Tree loss: 3.180 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 030 | Total loss: 3.106 | Reg loss: 0.051 | Tree loss: 3.106 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 030 | Total loss: 3.140 | Reg loss: 0.051 | Tree loss: 3.140 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 030 | Total loss: 3.119 | Reg loss: 0.051 | Tree loss: 3.119 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 030 | Total loss: 3.078 | Reg loss: 0.051 | Tree loss: 3.078 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 030 | Total loss: 3.037 | Reg loss: 0.051 | Tree loss: 3.037 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 030 | Total loss: 2.961 | Reg loss: 0.052 | Tree loss: 2.961 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 030 | Total loss: 2.951 | Reg loss: 0.052 | Tree loss: 2.951 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 030 | Total loss: 2.923 | Reg loss: 0.052 | Tree loss: 2.923 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 030 | Total loss: 2.898 | Reg loss: 0.052 | Tree loss: 2.898 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 030 | Total loss: 2.802 | Reg loss: 0.053 | Tree loss: 2.802 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 030 | Total loss: 2.792 | Reg loss: 0.053 | Tree loss: 2.792 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 030 | Total loss: 2.790 | Reg loss: 0.053 | Tree loss: 2.790 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 030 | Total loss: 2.736 | Reg loss: 0.053 | Tree loss: 2.736 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 030 | Total loss: 2.734 | Reg loss: 0.054 | Tree loss: 2.734 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 030 | Total loss: 2.660 | Reg loss: 0.054 | Tree loss: 2.660 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 030 | Total loss: 2.607 | Reg loss: 0.054 | Tree loss: 2.607 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 030 | Total loss: 2.582 | Reg loss: 0.054 | Tree loss: 2.582 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 030 | Total loss: 2.607 | Reg loss: 0.055 | Tree loss: 2.607 | Accuracy: 0.541016 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 029 / 030 | Total loss: 2.434 | Reg loss: 0.055 | Tree loss: 2.434 | Accuracy: 0.638095 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 030 | Total loss: 3.471 | Reg loss: 0.050 | Tree loss: 3.471 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 030 | Total loss: 3.404 | Reg loss: 0.050 | Tree loss: 3.404 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 030 | Total loss: 3.425 | Reg loss: 0.050 | Tree loss: 3.425 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 030 | Total loss: 3.411 | Reg loss: 0.050 | Tree loss: 3.411 | Accuracy: 0.521484 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 030 | Total loss: 3.396 | Reg loss: 0.050 | Tree loss: 3.396 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 030 | Total loss: 3.292 | Reg loss: 0.050 | Tree loss: 3.292 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 030 | Total loss: 3.289 | Reg loss: 0.050 | Tree loss: 3.289 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 030 | Total loss: 3.315 | Reg loss: 0.051 | Tree loss: 3.315 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 030 | Total loss: 3.165 | Reg loss: 0.051 | Tree loss: 3.165 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 030 | Total loss: 3.186 | Reg loss: 0.051 | Tree loss: 3.186 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 030 | Total loss: 3.119 | Reg loss: 0.051 | Tree loss: 3.119 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 030 | Total loss: 3.021 | Reg loss: 0.051 | Tree loss: 3.021 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 030 | Total loss: 2.991 | Reg loss: 0.051 | Tree loss: 2.991 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 030 | Total loss: 2.957 | Reg loss: 0.052 | Tree loss: 2.957 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 030 | Total loss: 2.946 | Reg loss: 0.052 | Tree loss: 2.946 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 030 | Total loss: 2.869 | Reg loss: 0.052 | Tree loss: 2.869 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 030 | Total loss: 2.820 | Reg loss: 0.052 | Tree loss: 2.820 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 030 | Total loss: 2.799 | Reg loss: 0.052 | Tree loss: 2.799 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 030 | Total loss: 2.762 | Reg loss: 0.053 | Tree loss: 2.762 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 030 | Total loss: 2.717 | Reg loss: 0.053 | Tree loss: 2.717 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 030 | Total loss: 2.743 | Reg loss: 0.053 | Tree loss: 2.743 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 030 | Total loss: 2.670 | Reg loss: 0.053 | Tree loss: 2.670 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 030 | Total loss: 2.645 | Reg loss: 0.054 | Tree loss: 2.645 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 030 | Total loss: 2.625 | Reg loss: 0.054 | Tree loss: 2.625 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 030 | Total loss: 2.574 | Reg loss: 0.054 | Tree loss: 2.574 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 030 | Total loss: 2.505 | Reg loss: 0.054 | Tree loss: 2.505 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 030 | Total loss: 2.498 | Reg loss: 0.055 | Tree loss: 2.498 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 030 | Total loss: 2.515 | Reg loss: 0.055 | Tree loss: 2.515 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 028 / 030 | Total loss: 2.483 | Reg loss: 0.055 | Tree loss: 2.483 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 31 | Batch: 029 / 030 | Total loss: 2.502 | Reg loss: 0.055 | Tree loss: 2.502 | Accuracy: 0.495238 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 030 | Total loss: 3.363 | Reg loss: 0.051 | Tree loss: 3.363 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 030 | Total loss: 3.395 | Reg loss: 0.051 | Tree loss: 3.395 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 030 | Total loss: 3.344 | Reg loss: 0.051 | Tree loss: 3.344 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 030 | Total loss: 3.308 | Reg loss: 0.051 | Tree loss: 3.308 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 030 | Total loss: 3.207 | Reg loss: 0.051 | Tree loss: 3.207 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 030 | Total loss: 3.157 | Reg loss: 0.051 | Tree loss: 3.157 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 030 | Total loss: 3.143 | Reg loss: 0.051 | Tree loss: 3.143 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 030 | Total loss: 2.991 | Reg loss: 0.051 | Tree loss: 2.991 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 030 | Total loss: 3.037 | Reg loss: 0.051 | Tree loss: 3.037 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 030 | Total loss: 3.025 | Reg loss: 0.051 | Tree loss: 3.025 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 030 | Total loss: 2.988 | Reg loss: 0.051 | Tree loss: 2.988 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 030 | Total loss: 2.942 | Reg loss: 0.052 | Tree loss: 2.942 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 030 | Total loss: 2.870 | Reg loss: 0.052 | Tree loss: 2.870 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 030 | Total loss: 2.807 | Reg loss: 0.052 | Tree loss: 2.807 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 030 | Total loss: 2.764 | Reg loss: 0.052 | Tree loss: 2.764 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 030 | Total loss: 2.758 | Reg loss: 0.052 | Tree loss: 2.758 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 030 | Total loss: 2.751 | Reg loss: 0.053 | Tree loss: 2.751 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 030 | Total loss: 2.679 | Reg loss: 0.053 | Tree loss: 2.679 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 030 | Total loss: 2.687 | Reg loss: 0.053 | Tree loss: 2.687 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 030 | Total loss: 2.640 | Reg loss: 0.053 | Tree loss: 2.640 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 030 | Total loss: 2.611 | Reg loss: 0.054 | Tree loss: 2.611 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 030 | Total loss: 2.511 | Reg loss: 0.054 | Tree loss: 2.511 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 030 | Total loss: 2.518 | Reg loss: 0.054 | Tree loss: 2.518 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 030 | Total loss: 2.482 | Reg loss: 0.054 | Tree loss: 2.482 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 030 | Total loss: 2.447 | Reg loss: 0.054 | Tree loss: 2.447 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 030 | Total loss: 2.419 | Reg loss: 0.055 | Tree loss: 2.419 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 030 | Total loss: 2.388 | Reg loss: 0.055 | Tree loss: 2.388 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 030 | Total loss: 2.384 | Reg loss: 0.055 | Tree loss: 2.384 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 030 | Total loss: 2.356 | Reg loss: 0.055 | Tree loss: 2.356 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 32 | Batch: 029 / 030 | Total loss: 2.222 | Reg loss: 0.055 | Tree loss: 2.222 | Accuracy: 0.647619 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 000 / 030 | Total loss: 3.226 | Reg loss: 0.051 | Tree loss: 3.226 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 030 | Total loss: 3.158 | Reg loss: 0.051 | Tree loss: 3.158 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 030 | Total loss: 3.154 | Reg loss: 0.051 | Tree loss: 3.154 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 030 | Total loss: 3.174 | Reg loss: 0.051 | Tree loss: 3.174 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 030 | Total loss: 3.101 | Reg loss: 0.051 | Tree loss: 3.101 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 030 | Total loss: 3.048 | Reg loss: 0.051 | Tree loss: 3.048 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 030 | Total loss: 3.001 | Reg loss: 0.051 | Tree loss: 3.001 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 030 | Total loss: 3.008 | Reg loss: 0.052 | Tree loss: 3.008 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 030 | Total loss: 2.914 | Reg loss: 0.052 | Tree loss: 2.914 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 030 | Total loss: 2.884 | Reg loss: 0.052 | Tree loss: 2.884 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 030 | Total loss: 2.854 | Reg loss: 0.052 | Tree loss: 2.854 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 030 | Total loss: 2.785 | Reg loss: 0.052 | Tree loss: 2.785 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 030 | Total loss: 2.789 | Reg loss: 0.052 | Tree loss: 2.789 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 030 | Total loss: 2.752 | Reg loss: 0.052 | Tree loss: 2.752 | Accuracy: 0.523438 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 030 | Total loss: 2.723 | Reg loss: 0.053 | Tree loss: 2.723 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 030 | Total loss: 2.687 | Reg loss: 0.053 | Tree loss: 2.687 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 030 | Total loss: 2.604 | Reg loss: 0.053 | Tree loss: 2.604 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 030 | Total loss: 2.581 | Reg loss: 0.053 | Tree loss: 2.581 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 030 | Total loss: 2.545 | Reg loss: 0.053 | Tree loss: 2.545 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 030 | Total loss: 2.483 | Reg loss: 0.054 | Tree loss: 2.483 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 030 | Total loss: 2.488 | Reg loss: 0.054 | Tree loss: 2.488 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 030 | Total loss: 2.411 | Reg loss: 0.054 | Tree loss: 2.411 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 030 | Total loss: 2.398 | Reg loss: 0.054 | Tree loss: 2.398 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 030 | Total loss: 2.356 | Reg loss: 0.055 | Tree loss: 2.356 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 030 | Total loss: 2.359 | Reg loss: 0.055 | Tree loss: 2.359 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 030 | Total loss: 2.284 | Reg loss: 0.055 | Tree loss: 2.284 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 030 | Total loss: 2.284 | Reg loss: 0.055 | Tree loss: 2.284 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 030 | Total loss: 2.273 | Reg loss: 0.055 | Tree loss: 2.273 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 030 | Total loss: 2.217 | Reg loss: 0.056 | Tree loss: 2.217 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 33 | Batch: 029 / 030 | Total loss: 2.215 | Reg loss: 0.056 | Tree loss: 2.215 | Accuracy: 0.495238 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 030 | Total loss: 3.108 | Reg loss: 0.052 | Tree loss: 3.108 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 030 | Total loss: 3.034 | Reg loss: 0.052 | Tree loss: 3.034 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 030 | Total loss: 3.042 | Reg loss: 0.052 | Tree loss: 3.042 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 030 | Total loss: 3.135 | Reg loss: 0.052 | Tree loss: 3.135 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 030 | Total loss: 2.926 | Reg loss: 0.052 | Tree loss: 2.926 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 030 | Total loss: 2.976 | Reg loss: 0.052 | Tree loss: 2.976 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 030 | Total loss: 2.893 | Reg loss: 0.052 | Tree loss: 2.893 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 030 | Total loss: 2.782 | Reg loss: 0.052 | Tree loss: 2.782 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 030 | Total loss: 2.787 | Reg loss: 0.052 | Tree loss: 2.787 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 030 | Total loss: 2.764 | Reg loss: 0.052 | Tree loss: 2.764 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 030 | Total loss: 2.755 | Reg loss: 0.052 | Tree loss: 2.755 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 030 | Total loss: 2.727 | Reg loss: 0.052 | Tree loss: 2.727 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 030 | Total loss: 2.657 | Reg loss: 0.053 | Tree loss: 2.657 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 030 | Total loss: 2.644 | Reg loss: 0.053 | Tree loss: 2.644 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 030 | Total loss: 2.592 | Reg loss: 0.053 | Tree loss: 2.592 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 030 | Total loss: 2.511 | Reg loss: 0.053 | Tree loss: 2.511 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 030 | Total loss: 2.510 | Reg loss: 0.053 | Tree loss: 2.510 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 030 | Total loss: 2.472 | Reg loss: 0.053 | Tree loss: 2.472 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 030 | Total loss: 2.462 | Reg loss: 0.054 | Tree loss: 2.462 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 030 | Total loss: 2.435 | Reg loss: 0.054 | Tree loss: 2.435 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 030 | Total loss: 2.404 | Reg loss: 0.054 | Tree loss: 2.404 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 030 | Total loss: 2.320 | Reg loss: 0.054 | Tree loss: 2.320 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 030 | Total loss: 2.302 | Reg loss: 0.054 | Tree loss: 2.302 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 030 | Total loss: 2.228 | Reg loss: 0.055 | Tree loss: 2.228 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 030 | Total loss: 2.239 | Reg loss: 0.055 | Tree loss: 2.239 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 030 | Total loss: 2.235 | Reg loss: 0.055 | Tree loss: 2.235 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 030 | Total loss: 2.187 | Reg loss: 0.055 | Tree loss: 2.187 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 030 | Total loss: 2.159 | Reg loss: 0.055 | Tree loss: 2.159 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 030 | Total loss: 2.149 | Reg loss: 0.056 | Tree loss: 2.149 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 34 | Batch: 029 / 030 | Total loss: 2.110 | Reg loss: 0.056 | Tree loss: 2.110 | Accuracy: 0.590476 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 030 | Total loss: 3.061 | Reg loss: 0.052 | Tree loss: 3.061 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 030 | Total loss: 2.968 | Reg loss: 0.052 | Tree loss: 2.968 | Accuracy: 0.564453 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 002 / 030 | Total loss: 2.971 | Reg loss: 0.052 | Tree loss: 2.971 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 030 | Total loss: 2.923 | Reg loss: 0.052 | Tree loss: 2.923 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 030 | Total loss: 2.863 | Reg loss: 0.052 | Tree loss: 2.863 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 030 | Total loss: 2.857 | Reg loss: 0.052 | Tree loss: 2.857 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 030 | Total loss: 2.783 | Reg loss: 0.052 | Tree loss: 2.783 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 030 | Total loss: 2.786 | Reg loss: 0.052 | Tree loss: 2.786 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 030 | Total loss: 2.692 | Reg loss: 0.052 | Tree loss: 2.692 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 030 | Total loss: 2.635 | Reg loss: 0.052 | Tree loss: 2.635 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 030 | Total loss: 2.590 | Reg loss: 0.053 | Tree loss: 2.590 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 030 | Total loss: 2.601 | Reg loss: 0.053 | Tree loss: 2.601 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 030 | Total loss: 2.545 | Reg loss: 0.053 | Tree loss: 2.545 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 030 | Total loss: 2.521 | Reg loss: 0.053 | Tree loss: 2.521 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 030 | Total loss: 2.470 | Reg loss: 0.053 | Tree loss: 2.470 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 030 | Total loss: 2.472 | Reg loss: 0.053 | Tree loss: 2.472 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 030 | Total loss: 2.410 | Reg loss: 0.054 | Tree loss: 2.410 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 030 | Total loss: 2.363 | Reg loss: 0.054 | Tree loss: 2.363 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 030 | Total loss: 2.310 | Reg loss: 0.054 | Tree loss: 2.310 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 030 | Total loss: 2.313 | Reg loss: 0.054 | Tree loss: 2.313 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 030 | Total loss: 2.254 | Reg loss: 0.054 | Tree loss: 2.254 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 030 | Total loss: 2.288 | Reg loss: 0.055 | Tree loss: 2.288 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 030 | Total loss: 2.229 | Reg loss: 0.055 | Tree loss: 2.229 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 030 | Total loss: 2.199 | Reg loss: 0.055 | Tree loss: 2.199 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 030 | Total loss: 2.154 | Reg loss: 0.055 | Tree loss: 2.154 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 030 | Total loss: 2.113 | Reg loss: 0.055 | Tree loss: 2.113 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 030 | Total loss: 2.074 | Reg loss: 0.055 | Tree loss: 2.074 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 030 | Total loss: 2.022 | Reg loss: 0.056 | Tree loss: 2.022 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 030 | Total loss: 2.069 | Reg loss: 0.056 | Tree loss: 2.069 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 35 | Batch: 029 / 030 | Total loss: 2.043 | Reg loss: 0.056 | Tree loss: 2.043 | Accuracy: 0.504762 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 030 | Total loss: 2.928 | Reg loss: 0.052 | Tree loss: 2.928 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 030 | Total loss: 2.855 | Reg loss: 0.052 | Tree loss: 2.855 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 030 | Total loss: 2.844 | Reg loss: 0.052 | Tree loss: 2.844 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 030 | Total loss: 2.812 | Reg loss: 0.052 | Tree loss: 2.812 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 030 | Total loss: 2.765 | Reg loss: 0.052 | Tree loss: 2.765 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 030 | Total loss: 2.730 | Reg loss: 0.052 | Tree loss: 2.730 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 030 | Total loss: 2.673 | Reg loss: 0.052 | Tree loss: 2.673 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 030 | Total loss: 2.675 | Reg loss: 0.052 | Tree loss: 2.675 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 030 | Total loss: 2.658 | Reg loss: 0.053 | Tree loss: 2.658 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 030 | Total loss: 2.598 | Reg loss: 0.053 | Tree loss: 2.598 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 030 | Total loss: 2.576 | Reg loss: 0.053 | Tree loss: 2.576 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 030 | Total loss: 2.460 | Reg loss: 0.053 | Tree loss: 2.460 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 030 | Total loss: 2.479 | Reg loss: 0.053 | Tree loss: 2.479 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 030 | Total loss: 2.386 | Reg loss: 0.053 | Tree loss: 2.386 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 030 | Total loss: 2.324 | Reg loss: 0.053 | Tree loss: 2.324 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 030 | Total loss: 2.376 | Reg loss: 0.054 | Tree loss: 2.376 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 030 | Total loss: 2.302 | Reg loss: 0.054 | Tree loss: 2.302 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 030 | Total loss: 2.287 | Reg loss: 0.054 | Tree loss: 2.287 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 030 | Total loss: 2.253 | Reg loss: 0.054 | Tree loss: 2.253 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 030 | Total loss: 2.225 | Reg loss: 0.054 | Tree loss: 2.225 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 030 | Total loss: 2.167 | Reg loss: 0.055 | Tree loss: 2.167 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 030 | Total loss: 2.151 | Reg loss: 0.055 | Tree loss: 2.151 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 030 | Total loss: 2.111 | Reg loss: 0.055 | Tree loss: 2.111 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 030 | Total loss: 2.078 | Reg loss: 0.055 | Tree loss: 2.078 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 030 | Total loss: 2.087 | Reg loss: 0.055 | Tree loss: 2.087 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 030 | Total loss: 2.051 | Reg loss: 0.055 | Tree loss: 2.051 | Accuracy: 0.611328 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 030 | Total loss: 2.056 | Reg loss: 0.056 | Tree loss: 2.056 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 030 | Total loss: 1.998 | Reg loss: 0.056 | Tree loss: 1.998 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 030 | Total loss: 1.955 | Reg loss: 0.056 | Tree loss: 1.955 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 36 | Batch: 029 / 030 | Total loss: 2.013 | Reg loss: 0.056 | Tree loss: 2.013 | Accuracy: 0.476190 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 030 | Total loss: 2.844 | Reg loss: 0.052 | Tree loss: 2.844 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 030 | Total loss: 2.798 | Reg loss: 0.052 | Tree loss: 2.798 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 030 | Total loss: 2.790 | Reg loss: 0.052 | Tree loss: 2.790 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 030 | Total loss: 2.723 | Reg loss: 0.052 | Tree loss: 2.723 | Accuracy: 0.593750 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 004 / 030 | Total loss: 2.675 | Reg loss: 0.053 | Tree loss: 2.675 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 030 | Total loss: 2.673 | Reg loss: 0.053 | Tree loss: 2.673 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 030 | Total loss: 2.571 | Reg loss: 0.053 | Tree loss: 2.571 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 030 | Total loss: 2.524 | Reg loss: 0.053 | Tree loss: 2.524 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 030 | Total loss: 2.488 | Reg loss: 0.053 | Tree loss: 2.488 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 030 | Total loss: 2.523 | Reg loss: 0.053 | Tree loss: 2.523 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 030 | Total loss: 2.431 | Reg loss: 0.053 | Tree loss: 2.431 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 030 | Total loss: 2.420 | Reg loss: 0.053 | Tree loss: 2.420 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 030 | Total loss: 2.377 | Reg loss: 0.053 | Tree loss: 2.377 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 030 | Total loss: 2.315 | Reg loss: 0.054 | Tree loss: 2.315 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 030 | Total loss: 2.322 | Reg loss: 0.054 | Tree loss: 2.322 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 030 | Total loss: 2.260 | Reg loss: 0.054 | Tree loss: 2.260 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 030 | Total loss: 2.192 | Reg loss: 0.054 | Tree loss: 2.192 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 030 | Total loss: 2.214 | Reg loss: 0.054 | Tree loss: 2.214 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 030 | Total loss: 2.170 | Reg loss: 0.054 | Tree loss: 2.170 | Accuracy: 0.621094 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 030 | Total loss: 2.074 | Reg loss: 0.055 | Tree loss: 2.074 | Accuracy: 0.625000 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 030 | Total loss: 2.108 | Reg loss: 0.055 | Tree loss: 2.108 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 030 | Total loss: 2.055 | Reg loss: 0.055 | Tree loss: 2.055 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 030 | Total loss: 2.044 | Reg loss: 0.055 | Tree loss: 2.044 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 030 | Total loss: 2.031 | Reg loss: 0.055 | Tree loss: 2.031 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 030 | Total loss: 1.997 | Reg loss: 0.055 | Tree loss: 1.997 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 030 | Total loss: 1.989 | Reg loss: 0.056 | Tree loss: 1.989 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 030 | Total loss: 1.949 | Reg loss: 0.056 | Tree loss: 1.949 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 030 | Total loss: 1.928 | Reg loss: 0.056 | Tree loss: 1.928 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 030 | Total loss: 1.915 | Reg loss: 0.056 | Tree loss: 1.915 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 37 | Batch: 029 / 030 | Total loss: 1.940 | Reg loss: 0.056 | Tree loss: 1.940 | Accuracy: 0.514286 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 030 | Total loss: 2.692 | Reg loss: 0.053 | Tree loss: 2.692 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 030 | Total loss: 2.715 | Reg loss: 0.053 | Tree loss: 2.715 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 030 | Total loss: 2.699 | Reg loss: 0.053 | Tree loss: 2.699 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 030 | Total loss: 2.661 | Reg loss: 0.053 | Tree loss: 2.661 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 030 | Total loss: 2.635 | Reg loss: 0.053 | Tree loss: 2.635 | Accuracy: 0.515625 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 030 | Total loss: 2.512 | Reg loss: 0.053 | Tree loss: 2.512 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 030 | Total loss: 2.585 | Reg loss: 0.053 | Tree loss: 2.585 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 030 | Total loss: 2.485 | Reg loss: 0.053 | Tree loss: 2.485 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 030 | Total loss: 2.483 | Reg loss: 0.053 | Tree loss: 2.483 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 030 | Total loss: 2.441 | Reg loss: 0.053 | Tree loss: 2.441 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 030 | Total loss: 2.299 | Reg loss: 0.053 | Tree loss: 2.299 | Accuracy: 0.632812 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 030 | Total loss: 2.337 | Reg loss: 0.053 | Tree loss: 2.337 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 030 | Total loss: 2.287 | Reg loss: 0.054 | Tree loss: 2.287 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 030 | Total loss: 2.247 | Reg loss: 0.054 | Tree loss: 2.247 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 030 | Total loss: 2.221 | Reg loss: 0.054 | Tree loss: 2.221 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 030 | Total loss: 2.177 | Reg loss: 0.054 | Tree loss: 2.177 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 030 | Total loss: 2.173 | Reg loss: 0.054 | Tree loss: 2.173 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 030 | Total loss: 2.084 | Reg loss: 0.054 | Tree loss: 2.084 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 030 | Total loss: 2.094 | Reg loss: 0.054 | Tree loss: 2.094 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 030 | Total loss: 2.018 | Reg loss: 0.055 | Tree loss: 2.018 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 030 | Total loss: 2.008 | Reg loss: 0.055 | Tree loss: 2.008 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 030 | Total loss: 1.984 | Reg loss: 0.055 | Tree loss: 1.984 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 030 | Total loss: 1.976 | Reg loss: 0.055 | Tree loss: 1.976 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 030 | Total loss: 1.896 | Reg loss: 0.055 | Tree loss: 1.896 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 030 | Total loss: 1.915 | Reg loss: 0.056 | Tree loss: 1.915 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 030 | Total loss: 1.901 | Reg loss: 0.056 | Tree loss: 1.901 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 030 | Total loss: 1.898 | Reg loss: 0.056 | Tree loss: 1.898 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 030 | Total loss: 1.873 | Reg loss: 0.056 | Tree loss: 1.873 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 030 | Total loss: 1.828 | Reg loss: 0.056 | Tree loss: 1.828 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 38 | Batch: 029 / 030 | Total loss: 1.871 | Reg loss: 0.056 | Tree loss: 1.871 | Accuracy: 0.514286 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 030 | Total loss: 2.643 | Reg loss: 0.053 | Tree loss: 2.643 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 030 | Total loss: 2.650 | Reg loss: 0.053 | Tree loss: 2.650 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 030 | Total loss: 2.608 | Reg loss: 0.053 | Tree loss: 2.608 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 030 | Total loss: 2.554 | Reg loss: 0.053 | Tree loss: 2.554 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 030 | Total loss: 2.499 | Reg loss: 0.053 | Tree loss: 2.499 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 030 | Total loss: 2.481 | Reg loss: 0.053 | Tree loss: 2.481 | Accuracy: 0.576172 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 006 / 030 | Total loss: 2.455 | Reg loss: 0.053 | Tree loss: 2.455 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 030 | Total loss: 2.395 | Reg loss: 0.053 | Tree loss: 2.395 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 030 | Total loss: 2.384 | Reg loss: 0.053 | Tree loss: 2.384 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 030 | Total loss: 2.372 | Reg loss: 0.053 | Tree loss: 2.372 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 030 | Total loss: 2.311 | Reg loss: 0.053 | Tree loss: 2.311 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 030 | Total loss: 2.256 | Reg loss: 0.053 | Tree loss: 2.256 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 030 | Total loss: 2.225 | Reg loss: 0.054 | Tree loss: 2.225 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 030 | Total loss: 2.156 | Reg loss: 0.054 | Tree loss: 2.156 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 030 | Total loss: 2.116 | Reg loss: 0.054 | Tree loss: 2.116 | Accuracy: 0.630859 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 030 | Total loss: 2.132 | Reg loss: 0.054 | Tree loss: 2.132 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 030 | Total loss: 2.057 | Reg loss: 0.054 | Tree loss: 2.057 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 030 | Total loss: 2.069 | Reg loss: 0.054 | Tree loss: 2.069 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 030 | Total loss: 1.971 | Reg loss: 0.055 | Tree loss: 1.971 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 030 | Total loss: 1.970 | Reg loss: 0.055 | Tree loss: 1.970 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 030 | Total loss: 1.986 | Reg loss: 0.055 | Tree loss: 1.986 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 030 | Total loss: 1.977 | Reg loss: 0.055 | Tree loss: 1.977 | Accuracy: 0.523438 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 030 | Total loss: 1.940 | Reg loss: 0.055 | Tree loss: 1.940 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 030 | Total loss: 1.883 | Reg loss: 0.055 | Tree loss: 1.883 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 030 | Total loss: 1.830 | Reg loss: 0.056 | Tree loss: 1.830 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 030 | Total loss: 1.836 | Reg loss: 0.056 | Tree loss: 1.836 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 030 | Total loss: 1.835 | Reg loss: 0.056 | Tree loss: 1.835 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 030 | Total loss: 1.771 | Reg loss: 0.056 | Tree loss: 1.771 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 030 | Total loss: 1.735 | Reg loss: 0.056 | Tree loss: 1.735 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 39 | Batch: 029 / 030 | Total loss: 1.675 | Reg loss: 0.056 | Tree loss: 1.675 | Accuracy: 0.619048 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 030 | Total loss: 2.622 | Reg loss: 0.053 | Tree loss: 2.622 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 030 | Total loss: 2.564 | Reg loss: 0.053 | Tree loss: 2.564 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 030 | Total loss: 2.493 | Reg loss: 0.053 | Tree loss: 2.493 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 030 | Total loss: 2.458 | Reg loss: 0.053 | Tree loss: 2.458 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 030 | Total loss: 2.470 | Reg loss: 0.053 | Tree loss: 2.470 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 030 | Total loss: 2.439 | Reg loss: 0.053 | Tree loss: 2.439 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 030 | Total loss: 2.340 | Reg loss: 0.053 | Tree loss: 2.340 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 030 | Total loss: 2.364 | Reg loss: 0.053 | Tree loss: 2.364 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 030 | Total loss: 2.270 | Reg loss: 0.053 | Tree loss: 2.270 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 030 | Total loss: 2.278 | Reg loss: 0.053 | Tree loss: 2.278 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 030 | Total loss: 2.222 | Reg loss: 0.053 | Tree loss: 2.222 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 030 | Total loss: 2.163 | Reg loss: 0.054 | Tree loss: 2.163 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 030 | Total loss: 2.090 | Reg loss: 0.054 | Tree loss: 2.090 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 030 | Total loss: 2.117 | Reg loss: 0.054 | Tree loss: 2.117 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 030 | Total loss: 2.081 | Reg loss: 0.054 | Tree loss: 2.081 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 030 | Total loss: 2.101 | Reg loss: 0.054 | Tree loss: 2.101 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 030 | Total loss: 1.999 | Reg loss: 0.054 | Tree loss: 1.999 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 030 | Total loss: 1.983 | Reg loss: 0.054 | Tree loss: 1.983 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 030 | Total loss: 1.938 | Reg loss: 0.055 | Tree loss: 1.938 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 030 | Total loss: 1.904 | Reg loss: 0.055 | Tree loss: 1.904 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 030 | Total loss: 1.922 | Reg loss: 0.055 | Tree loss: 1.922 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 030 | Total loss: 1.857 | Reg loss: 0.055 | Tree loss: 1.857 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 030 | Total loss: 1.827 | Reg loss: 0.055 | Tree loss: 1.827 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 030 | Total loss: 1.838 | Reg loss: 0.055 | Tree loss: 1.838 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 030 | Total loss: 1.804 | Reg loss: 0.056 | Tree loss: 1.804 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 030 | Total loss: 1.752 | Reg loss: 0.056 | Tree loss: 1.752 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 030 | Total loss: 1.796 | Reg loss: 0.056 | Tree loss: 1.796 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 030 | Total loss: 1.712 | Reg loss: 0.056 | Tree loss: 1.712 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 030 | Total loss: 1.729 | Reg loss: 0.056 | Tree loss: 1.729 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 40 | Batch: 029 / 030 | Total loss: 1.752 | Reg loss: 0.056 | Tree loss: 1.752 | Accuracy: 0.552381 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 030 | Total loss: 2.489 | Reg loss: 0.053 | Tree loss: 2.489 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 030 | Total loss: 2.515 | Reg loss: 0.053 | Tree loss: 2.515 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 030 | Total loss: 2.370 | Reg loss: 0.053 | Tree loss: 2.370 | Accuracy: 0.638672 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 030 | Total loss: 2.438 | Reg loss: 0.053 | Tree loss: 2.438 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 030 | Total loss: 2.372 | Reg loss: 0.053 | Tree loss: 2.372 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 030 | Total loss: 2.284 | Reg loss: 0.053 | Tree loss: 2.284 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 030 | Total loss: 2.344 | Reg loss: 0.053 | Tree loss: 2.344 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 030 | Total loss: 2.262 | Reg loss: 0.053 | Tree loss: 2.262 | Accuracy: 0.585938 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 008 / 030 | Total loss: 2.258 | Reg loss: 0.053 | Tree loss: 2.258 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 030 | Total loss: 2.198 | Reg loss: 0.053 | Tree loss: 2.198 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 030 | Total loss: 2.126 | Reg loss: 0.054 | Tree loss: 2.126 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 030 | Total loss: 2.113 | Reg loss: 0.054 | Tree loss: 2.113 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 030 | Total loss: 2.088 | Reg loss: 0.054 | Tree loss: 2.088 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 030 | Total loss: 2.044 | Reg loss: 0.054 | Tree loss: 2.044 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 030 | Total loss: 2.012 | Reg loss: 0.054 | Tree loss: 2.012 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 030 | Total loss: 1.994 | Reg loss: 0.054 | Tree loss: 1.994 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 030 | Total loss: 1.970 | Reg loss: 0.054 | Tree loss: 1.970 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 030 | Total loss: 1.901 | Reg loss: 0.055 | Tree loss: 1.901 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 030 | Total loss: 1.911 | Reg loss: 0.055 | Tree loss: 1.911 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 030 | Total loss: 1.839 | Reg loss: 0.055 | Tree loss: 1.839 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 030 | Total loss: 1.853 | Reg loss: 0.055 | Tree loss: 1.853 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 030 | Total loss: 1.822 | Reg loss: 0.055 | Tree loss: 1.822 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 030 | Total loss: 1.779 | Reg loss: 0.055 | Tree loss: 1.779 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 030 | Total loss: 1.749 | Reg loss: 0.056 | Tree loss: 1.749 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 030 | Total loss: 1.715 | Reg loss: 0.056 | Tree loss: 1.715 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 030 | Total loss: 1.721 | Reg loss: 0.056 | Tree loss: 1.721 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 030 | Total loss: 1.717 | Reg loss: 0.056 | Tree loss: 1.717 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 030 | Total loss: 1.704 | Reg loss: 0.056 | Tree loss: 1.704 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 030 | Total loss: 1.662 | Reg loss: 0.056 | Tree loss: 1.662 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 41 | Batch: 029 / 030 | Total loss: 1.618 | Reg loss: 0.056 | Tree loss: 1.618 | Accuracy: 0.638095 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 030 | Total loss: 2.451 | Reg loss: 0.053 | Tree loss: 2.451 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 030 | Total loss: 2.424 | Reg loss: 0.053 | Tree loss: 2.424 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 030 | Total loss: 2.374 | Reg loss: 0.053 | Tree loss: 2.374 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 030 | Total loss: 2.347 | Reg loss: 0.053 | Tree loss: 2.347 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 030 | Total loss: 2.330 | Reg loss: 0.053 | Tree loss: 2.330 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 030 | Total loss: 2.294 | Reg loss: 0.053 | Tree loss: 2.294 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 030 | Total loss: 2.242 | Reg loss: 0.053 | Tree loss: 2.242 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 030 | Total loss: 2.244 | Reg loss: 0.053 | Tree loss: 2.244 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 030 | Total loss: 2.102 | Reg loss: 0.053 | Tree loss: 2.102 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 030 | Total loss: 2.115 | Reg loss: 0.053 | Tree loss: 2.115 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 030 | Total loss: 2.053 | Reg loss: 0.054 | Tree loss: 2.053 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 030 | Total loss: 2.065 | Reg loss: 0.054 | Tree loss: 2.065 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 030 | Total loss: 2.056 | Reg loss: 0.054 | Tree loss: 2.056 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 030 | Total loss: 1.992 | Reg loss: 0.054 | Tree loss: 1.992 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 030 | Total loss: 1.976 | Reg loss: 0.054 | Tree loss: 1.976 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 030 | Total loss: 1.906 | Reg loss: 0.054 | Tree loss: 1.906 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 030 | Total loss: 1.872 | Reg loss: 0.054 | Tree loss: 1.872 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 030 | Total loss: 1.859 | Reg loss: 0.055 | Tree loss: 1.859 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 030 | Total loss: 1.829 | Reg loss: 0.055 | Tree loss: 1.829 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 030 | Total loss: 1.807 | Reg loss: 0.055 | Tree loss: 1.807 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 030 | Total loss: 1.791 | Reg loss: 0.055 | Tree loss: 1.791 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 030 | Total loss: 1.750 | Reg loss: 0.055 | Tree loss: 1.750 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 030 | Total loss: 1.727 | Reg loss: 0.055 | Tree loss: 1.727 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 030 | Total loss: 1.664 | Reg loss: 0.055 | Tree loss: 1.664 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 030 | Total loss: 1.666 | Reg loss: 0.056 | Tree loss: 1.666 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 030 | Total loss: 1.647 | Reg loss: 0.056 | Tree loss: 1.647 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 030 | Total loss: 1.643 | Reg loss: 0.056 | Tree loss: 1.643 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 030 | Total loss: 1.648 | Reg loss: 0.056 | Tree loss: 1.648 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 030 | Total loss: 1.649 | Reg loss: 0.056 | Tree loss: 1.649 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 42 | Batch: 029 / 030 | Total loss: 1.593 | Reg loss: 0.056 | Tree loss: 1.593 | Accuracy: 0.590476 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 030 | Total loss: 2.405 | Reg loss: 0.053 | Tree loss: 2.405 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 030 | Total loss: 2.330 | Reg loss: 0.053 | Tree loss: 2.330 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 030 | Total loss: 2.338 | Reg loss: 0.053 | Tree loss: 2.338 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 030 | Total loss: 2.263 | Reg loss: 0.053 | Tree loss: 2.263 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 030 | Total loss: 2.220 | Reg loss: 0.053 | Tree loss: 2.220 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 030 | Total loss: 2.266 | Reg loss: 0.053 | Tree loss: 2.266 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 030 | Total loss: 2.196 | Reg loss: 0.053 | Tree loss: 2.196 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 030 | Total loss: 2.110 | Reg loss: 0.053 | Tree loss: 2.110 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 030 | Total loss: 2.076 | Reg loss: 0.053 | Tree loss: 2.076 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 030 | Total loss: 2.069 | Reg loss: 0.053 | Tree loss: 2.069 | Accuracy: 0.564453 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 010 / 030 | Total loss: 2.003 | Reg loss: 0.054 | Tree loss: 2.003 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 030 | Total loss: 2.001 | Reg loss: 0.054 | Tree loss: 2.001 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 030 | Total loss: 1.945 | Reg loss: 0.054 | Tree loss: 1.945 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 030 | Total loss: 1.978 | Reg loss: 0.054 | Tree loss: 1.978 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 030 | Total loss: 1.905 | Reg loss: 0.054 | Tree loss: 1.905 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 030 | Total loss: 1.882 | Reg loss: 0.054 | Tree loss: 1.882 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 030 | Total loss: 1.830 | Reg loss: 0.054 | Tree loss: 1.830 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 030 | Total loss: 1.777 | Reg loss: 0.055 | Tree loss: 1.777 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 030 | Total loss: 1.767 | Reg loss: 0.055 | Tree loss: 1.767 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 030 | Total loss: 1.731 | Reg loss: 0.055 | Tree loss: 1.731 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 030 | Total loss: 1.741 | Reg loss: 0.055 | Tree loss: 1.741 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 030 | Total loss: 1.692 | Reg loss: 0.055 | Tree loss: 1.692 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 030 | Total loss: 1.691 | Reg loss: 0.055 | Tree loss: 1.691 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 030 | Total loss: 1.659 | Reg loss: 0.055 | Tree loss: 1.659 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 030 | Total loss: 1.674 | Reg loss: 0.056 | Tree loss: 1.674 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 030 | Total loss: 1.627 | Reg loss: 0.056 | Tree loss: 1.627 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 030 | Total loss: 1.594 | Reg loss: 0.056 | Tree loss: 1.594 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 030 | Total loss: 1.588 | Reg loss: 0.056 | Tree loss: 1.588 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 030 | Total loss: 1.542 | Reg loss: 0.056 | Tree loss: 1.542 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 43 | Batch: 029 / 030 | Total loss: 1.609 | Reg loss: 0.056 | Tree loss: 1.609 | Accuracy: 0.504762 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 030 | Total loss: 2.273 | Reg loss: 0.053 | Tree loss: 2.273 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 030 | Total loss: 2.317 | Reg loss: 0.053 | Tree loss: 2.317 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 030 | Total loss: 2.199 | Reg loss: 0.053 | Tree loss: 2.199 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 030 | Total loss: 2.275 | Reg loss: 0.053 | Tree loss: 2.275 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 030 | Total loss: 2.218 | Reg loss: 0.053 | Tree loss: 2.218 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 030 | Total loss: 2.203 | Reg loss: 0.053 | Tree loss: 2.203 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 030 | Total loss: 2.119 | Reg loss: 0.053 | Tree loss: 2.119 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 030 | Total loss: 2.076 | Reg loss: 0.053 | Tree loss: 2.076 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 030 | Total loss: 1.972 | Reg loss: 0.053 | Tree loss: 1.972 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 030 | Total loss: 2.025 | Reg loss: 0.053 | Tree loss: 2.025 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 030 | Total loss: 1.940 | Reg loss: 0.054 | Tree loss: 1.940 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 030 | Total loss: 1.896 | Reg loss: 0.054 | Tree loss: 1.896 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 030 | Total loss: 1.894 | Reg loss: 0.054 | Tree loss: 1.894 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 030 | Total loss: 1.851 | Reg loss: 0.054 | Tree loss: 1.851 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 030 | Total loss: 1.821 | Reg loss: 0.054 | Tree loss: 1.821 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 030 | Total loss: 1.772 | Reg loss: 0.054 | Tree loss: 1.772 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 030 | Total loss: 1.829 | Reg loss: 0.054 | Tree loss: 1.829 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 030 | Total loss: 1.782 | Reg loss: 0.054 | Tree loss: 1.782 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 030 | Total loss: 1.716 | Reg loss: 0.055 | Tree loss: 1.716 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 030 | Total loss: 1.695 | Reg loss: 0.055 | Tree loss: 1.695 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 030 | Total loss: 1.702 | Reg loss: 0.055 | Tree loss: 1.702 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 030 | Total loss: 1.666 | Reg loss: 0.055 | Tree loss: 1.666 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 030 | Total loss: 1.643 | Reg loss: 0.055 | Tree loss: 1.643 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 030 | Total loss: 1.598 | Reg loss: 0.055 | Tree loss: 1.598 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 030 | Total loss: 1.628 | Reg loss: 0.055 | Tree loss: 1.628 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 030 | Total loss: 1.587 | Reg loss: 0.056 | Tree loss: 1.587 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 030 | Total loss: 1.555 | Reg loss: 0.056 | Tree loss: 1.555 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 030 | Total loss: 1.562 | Reg loss: 0.056 | Tree loss: 1.562 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 030 | Total loss: 1.527 | Reg loss: 0.056 | Tree loss: 1.527 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 44 | Batch: 029 / 030 | Total loss: 1.566 | Reg loss: 0.056 | Tree loss: 1.566 | Accuracy: 0.561905 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 030 | Total loss: 2.268 | Reg loss: 0.053 | Tree loss: 2.268 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 030 | Total loss: 2.242 | Reg loss: 0.053 | Tree loss: 2.242 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 030 | Total loss: 2.167 | Reg loss: 0.053 | Tree loss: 2.167 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 030 | Total loss: 2.213 | Reg loss: 0.053 | Tree loss: 2.213 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 030 | Total loss: 2.167 | Reg loss: 0.053 | Tree loss: 2.167 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 030 | Total loss: 2.054 | Reg loss: 0.053 | Tree loss: 2.054 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 030 | Total loss: 2.107 | Reg loss: 0.053 | Tree loss: 2.107 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 030 | Total loss: 2.068 | Reg loss: 0.053 | Tree loss: 2.068 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 030 | Total loss: 1.960 | Reg loss: 0.053 | Tree loss: 1.960 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 030 | Total loss: 1.945 | Reg loss: 0.053 | Tree loss: 1.945 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 030 | Total loss: 1.919 | Reg loss: 0.054 | Tree loss: 1.919 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 030 | Total loss: 1.910 | Reg loss: 0.054 | Tree loss: 1.910 | Accuracy: 0.564453 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Batch: 012 / 030 | Total loss: 1.865 | Reg loss: 0.054 | Tree loss: 1.865 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 030 | Total loss: 1.838 | Reg loss: 0.054 | Tree loss: 1.838 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 030 | Total loss: 1.749 | Reg loss: 0.054 | Tree loss: 1.749 | Accuracy: 0.632812 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 030 | Total loss: 1.749 | Reg loss: 0.054 | Tree loss: 1.749 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 030 | Total loss: 1.737 | Reg loss: 0.054 | Tree loss: 1.737 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 030 | Total loss: 1.689 | Reg loss: 0.054 | Tree loss: 1.689 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 030 | Total loss: 1.658 | Reg loss: 0.055 | Tree loss: 1.658 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 030 | Total loss: 1.667 | Reg loss: 0.055 | Tree loss: 1.667 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 030 | Total loss: 1.655 | Reg loss: 0.055 | Tree loss: 1.655 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 030 | Total loss: 1.593 | Reg loss: 0.055 | Tree loss: 1.593 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 030 | Total loss: 1.562 | Reg loss: 0.055 | Tree loss: 1.562 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 030 | Total loss: 1.563 | Reg loss: 0.055 | Tree loss: 1.563 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 030 | Total loss: 1.524 | Reg loss: 0.055 | Tree loss: 1.524 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 030 | Total loss: 1.514 | Reg loss: 0.056 | Tree loss: 1.514 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 030 | Total loss: 1.495 | Reg loss: 0.056 | Tree loss: 1.495 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 030 | Total loss: 1.513 | Reg loss: 0.056 | Tree loss: 1.513 | Accuracy: 0.523438 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 030 | Total loss: 1.496 | Reg loss: 0.056 | Tree loss: 1.496 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 45 | Batch: 029 / 030 | Total loss: 1.395 | Reg loss: 0.056 | Tree loss: 1.395 | Accuracy: 0.609524 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 030 | Total loss: 2.233 | Reg loss: 0.053 | Tree loss: 2.233 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 030 | Total loss: 2.225 | Reg loss: 0.053 | Tree loss: 2.225 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 030 | Total loss: 2.176 | Reg loss: 0.053 | Tree loss: 2.176 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 030 | Total loss: 2.139 | Reg loss: 0.053 | Tree loss: 2.139 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 030 | Total loss: 2.137 | Reg loss: 0.053 | Tree loss: 2.137 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 030 | Total loss: 2.019 | Reg loss: 0.053 | Tree loss: 2.019 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 030 | Total loss: 1.978 | Reg loss: 0.053 | Tree loss: 1.978 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 030 | Total loss: 1.989 | Reg loss: 0.053 | Tree loss: 1.989 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 030 | Total loss: 1.937 | Reg loss: 0.053 | Tree loss: 1.937 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 030 | Total loss: 1.879 | Reg loss: 0.053 | Tree loss: 1.879 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 030 | Total loss: 1.842 | Reg loss: 0.053 | Tree loss: 1.842 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 030 | Total loss: 1.812 | Reg loss: 0.054 | Tree loss: 1.812 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 030 | Total loss: 1.863 | Reg loss: 0.054 | Tree loss: 1.863 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 030 | Total loss: 1.809 | Reg loss: 0.054 | Tree loss: 1.809 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 030 | Total loss: 1.771 | Reg loss: 0.054 | Tree loss: 1.771 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 030 | Total loss: 1.658 | Reg loss: 0.054 | Tree loss: 1.658 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 030 | Total loss: 1.724 | Reg loss: 0.054 | Tree loss: 1.724 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 030 | Total loss: 1.664 | Reg loss: 0.054 | Tree loss: 1.664 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 030 | Total loss: 1.597 | Reg loss: 0.054 | Tree loss: 1.597 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 030 | Total loss: 1.624 | Reg loss: 0.055 | Tree loss: 1.624 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 030 | Total loss: 1.582 | Reg loss: 0.055 | Tree loss: 1.582 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 030 | Total loss: 1.567 | Reg loss: 0.055 | Tree loss: 1.567 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 030 | Total loss: 1.596 | Reg loss: 0.055 | Tree loss: 1.596 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 030 | Total loss: 1.503 | Reg loss: 0.055 | Tree loss: 1.503 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 030 | Total loss: 1.478 | Reg loss: 0.055 | Tree loss: 1.478 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 030 | Total loss: 1.489 | Reg loss: 0.055 | Tree loss: 1.489 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 030 | Total loss: 1.455 | Reg loss: 0.056 | Tree loss: 1.455 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 030 | Total loss: 1.412 | Reg loss: 0.056 | Tree loss: 1.412 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 030 | Total loss: 1.414 | Reg loss: 0.056 | Tree loss: 1.414 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 46 | Batch: 029 / 030 | Total loss: 1.458 | Reg loss: 0.056 | Tree loss: 1.458 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 030 | Total loss: 2.195 | Reg loss: 0.053 | Tree loss: 2.195 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 030 | Total loss: 2.147 | Reg loss: 0.053 | Tree loss: 2.147 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 030 | Total loss: 2.134 | Reg loss: 0.053 | Tree loss: 2.134 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 030 | Total loss: 2.065 | Reg loss: 0.053 | Tree loss: 2.065 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 030 | Total loss: 2.011 | Reg loss: 0.053 | Tree loss: 2.011 | Accuracy: 0.535156 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 030 | Total loss: 2.017 | Reg loss: 0.053 | Tree loss: 2.017 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 030 | Total loss: 1.953 | Reg loss: 0.053 | Tree loss: 1.953 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 030 | Total loss: 1.941 | Reg loss: 0.053 | Tree loss: 1.941 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 030 | Total loss: 1.871 | Reg loss: 0.053 | Tree loss: 1.871 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 030 | Total loss: 1.866 | Reg loss: 0.053 | Tree loss: 1.866 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 030 | Total loss: 1.800 | Reg loss: 0.053 | Tree loss: 1.800 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 030 | Total loss: 1.808 | Reg loss: 0.053 | Tree loss: 1.808 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 030 | Total loss: 1.798 | Reg loss: 0.054 | Tree loss: 1.798 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 030 | Total loss: 1.698 | Reg loss: 0.054 | Tree loss: 1.698 | Accuracy: 0.599609 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 014 / 030 | Total loss: 1.689 | Reg loss: 0.054 | Tree loss: 1.689 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 030 | Total loss: 1.700 | Reg loss: 0.054 | Tree loss: 1.700 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 030 | Total loss: 1.693 | Reg loss: 0.054 | Tree loss: 1.693 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 030 | Total loss: 1.606 | Reg loss: 0.054 | Tree loss: 1.606 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 030 | Total loss: 1.625 | Reg loss: 0.054 | Tree loss: 1.625 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 030 | Total loss: 1.561 | Reg loss: 0.054 | Tree loss: 1.561 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 030 | Total loss: 1.559 | Reg loss: 0.055 | Tree loss: 1.559 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 030 | Total loss: 1.529 | Reg loss: 0.055 | Tree loss: 1.529 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 030 | Total loss: 1.486 | Reg loss: 0.055 | Tree loss: 1.486 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 030 | Total loss: 1.458 | Reg loss: 0.055 | Tree loss: 1.458 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 030 | Total loss: 1.447 | Reg loss: 0.055 | Tree loss: 1.447 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 030 | Total loss: 1.440 | Reg loss: 0.055 | Tree loss: 1.440 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 030 | Total loss: 1.414 | Reg loss: 0.055 | Tree loss: 1.414 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 030 | Total loss: 1.385 | Reg loss: 0.055 | Tree loss: 1.385 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 030 | Total loss: 1.373 | Reg loss: 0.056 | Tree loss: 1.373 | Accuracy: 0.626953 | 0.907 sec/iter\n",
      "Epoch: 47 | Batch: 029 / 030 | Total loss: 1.365 | Reg loss: 0.056 | Tree loss: 1.365 | Accuracy: 0.580952 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 030 | Total loss: 2.140 | Reg loss: 0.053 | Tree loss: 2.140 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 030 | Total loss: 2.106 | Reg loss: 0.053 | Tree loss: 2.106 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 030 | Total loss: 2.026 | Reg loss: 0.053 | Tree loss: 2.026 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 030 | Total loss: 1.986 | Reg loss: 0.053 | Tree loss: 1.986 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 030 | Total loss: 1.990 | Reg loss: 0.053 | Tree loss: 1.990 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 030 | Total loss: 1.970 | Reg loss: 0.053 | Tree loss: 1.970 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 030 | Total loss: 1.951 | Reg loss: 0.053 | Tree loss: 1.951 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 030 | Total loss: 1.895 | Reg loss: 0.053 | Tree loss: 1.895 | Accuracy: 0.613281 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 030 | Total loss: 1.845 | Reg loss: 0.053 | Tree loss: 1.845 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 030 | Total loss: 1.810 | Reg loss: 0.053 | Tree loss: 1.810 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 030 | Total loss: 1.749 | Reg loss: 0.053 | Tree loss: 1.749 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 030 | Total loss: 1.807 | Reg loss: 0.053 | Tree loss: 1.807 | Accuracy: 0.533203 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 030 | Total loss: 1.737 | Reg loss: 0.053 | Tree loss: 1.737 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 030 | Total loss: 1.676 | Reg loss: 0.053 | Tree loss: 1.676 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 030 | Total loss: 1.656 | Reg loss: 0.054 | Tree loss: 1.656 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 030 | Total loss: 1.635 | Reg loss: 0.054 | Tree loss: 1.635 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 030 | Total loss: 1.611 | Reg loss: 0.054 | Tree loss: 1.611 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 030 | Total loss: 1.610 | Reg loss: 0.054 | Tree loss: 1.610 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 030 | Total loss: 1.521 | Reg loss: 0.054 | Tree loss: 1.521 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 030 | Total loss: 1.580 | Reg loss: 0.054 | Tree loss: 1.580 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 030 | Total loss: 1.508 | Reg loss: 0.054 | Tree loss: 1.508 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 030 | Total loss: 1.484 | Reg loss: 0.055 | Tree loss: 1.484 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 030 | Total loss: 1.480 | Reg loss: 0.055 | Tree loss: 1.480 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 030 | Total loss: 1.440 | Reg loss: 0.055 | Tree loss: 1.440 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 030 | Total loss: 1.448 | Reg loss: 0.055 | Tree loss: 1.448 | Accuracy: 0.500000 | 0.907 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 030 | Total loss: 1.378 | Reg loss: 0.055 | Tree loss: 1.378 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 030 | Total loss: 1.353 | Reg loss: 0.055 | Tree loss: 1.353 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 030 | Total loss: 1.376 | Reg loss: 0.055 | Tree loss: 1.376 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 030 | Total loss: 1.360 | Reg loss: 0.055 | Tree loss: 1.360 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 48 | Batch: 029 / 030 | Total loss: 1.374 | Reg loss: 0.055 | Tree loss: 1.374 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 030 | Total loss: 2.171 | Reg loss: 0.053 | Tree loss: 2.171 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 030 | Total loss: 2.067 | Reg loss: 0.053 | Tree loss: 2.067 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 030 | Total loss: 2.010 | Reg loss: 0.053 | Tree loss: 2.010 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 030 | Total loss: 2.001 | Reg loss: 0.053 | Tree loss: 2.001 | Accuracy: 0.539062 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 030 | Total loss: 1.940 | Reg loss: 0.053 | Tree loss: 1.940 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 030 | Total loss: 1.914 | Reg loss: 0.053 | Tree loss: 1.914 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 030 | Total loss: 1.904 | Reg loss: 0.053 | Tree loss: 1.904 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 030 | Total loss: 1.858 | Reg loss: 0.053 | Tree loss: 1.858 | Accuracy: 0.537109 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 030 | Total loss: 1.822 | Reg loss: 0.053 | Tree loss: 1.822 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 030 | Total loss: 1.793 | Reg loss: 0.053 | Tree loss: 1.793 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 030 | Total loss: 1.753 | Reg loss: 0.053 | Tree loss: 1.753 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 030 | Total loss: 1.645 | Reg loss: 0.053 | Tree loss: 1.645 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 030 | Total loss: 1.698 | Reg loss: 0.053 | Tree loss: 1.698 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 030 | Total loss: 1.668 | Reg loss: 0.053 | Tree loss: 1.668 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 030 | Total loss: 1.608 | Reg loss: 0.053 | Tree loss: 1.608 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 030 | Total loss: 1.541 | Reg loss: 0.054 | Tree loss: 1.541 | Accuracy: 0.617188 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 016 / 030 | Total loss: 1.534 | Reg loss: 0.054 | Tree loss: 1.534 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 030 | Total loss: 1.542 | Reg loss: 0.054 | Tree loss: 1.542 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 030 | Total loss: 1.518 | Reg loss: 0.054 | Tree loss: 1.518 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 030 | Total loss: 1.536 | Reg loss: 0.054 | Tree loss: 1.536 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 030 | Total loss: 1.532 | Reg loss: 0.054 | Tree loss: 1.532 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 030 | Total loss: 1.454 | Reg loss: 0.054 | Tree loss: 1.454 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 030 | Total loss: 1.412 | Reg loss: 0.054 | Tree loss: 1.412 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 030 | Total loss: 1.392 | Reg loss: 0.055 | Tree loss: 1.392 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 030 | Total loss: 1.407 | Reg loss: 0.055 | Tree loss: 1.407 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 030 | Total loss: 1.358 | Reg loss: 0.055 | Tree loss: 1.358 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 030 | Total loss: 1.354 | Reg loss: 0.055 | Tree loss: 1.354 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 030 | Total loss: 1.302 | Reg loss: 0.055 | Tree loss: 1.302 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 030 | Total loss: 1.335 | Reg loss: 0.055 | Tree loss: 1.335 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 49 | Batch: 029 / 030 | Total loss: 1.391 | Reg loss: 0.055 | Tree loss: 1.391 | Accuracy: 0.485714 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 030 | Total loss: 2.029 | Reg loss: 0.052 | Tree loss: 2.029 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 030 | Total loss: 2.042 | Reg loss: 0.052 | Tree loss: 2.042 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 030 | Total loss: 1.954 | Reg loss: 0.052 | Tree loss: 1.954 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 030 | Total loss: 1.993 | Reg loss: 0.052 | Tree loss: 1.993 | Accuracy: 0.531250 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 030 | Total loss: 1.954 | Reg loss: 0.052 | Tree loss: 1.954 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 030 | Total loss: 1.898 | Reg loss: 0.052 | Tree loss: 1.898 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 030 | Total loss: 1.855 | Reg loss: 0.053 | Tree loss: 1.855 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 030 | Total loss: 1.839 | Reg loss: 0.053 | Tree loss: 1.839 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 030 | Total loss: 1.781 | Reg loss: 0.053 | Tree loss: 1.781 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 030 | Total loss: 1.786 | Reg loss: 0.053 | Tree loss: 1.786 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 030 | Total loss: 1.727 | Reg loss: 0.053 | Tree loss: 1.727 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 030 | Total loss: 1.682 | Reg loss: 0.053 | Tree loss: 1.682 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 030 | Total loss: 1.667 | Reg loss: 0.053 | Tree loss: 1.667 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 030 | Total loss: 1.560 | Reg loss: 0.053 | Tree loss: 1.560 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 030 | Total loss: 1.582 | Reg loss: 0.053 | Tree loss: 1.582 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 030 | Total loss: 1.508 | Reg loss: 0.053 | Tree loss: 1.508 | Accuracy: 0.617188 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 030 | Total loss: 1.510 | Reg loss: 0.054 | Tree loss: 1.510 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 030 | Total loss: 1.559 | Reg loss: 0.054 | Tree loss: 1.559 | Accuracy: 0.531250 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 030 | Total loss: 1.510 | Reg loss: 0.054 | Tree loss: 1.510 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 030 | Total loss: 1.500 | Reg loss: 0.054 | Tree loss: 1.500 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 030 | Total loss: 1.437 | Reg loss: 0.054 | Tree loss: 1.437 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 030 | Total loss: 1.395 | Reg loss: 0.054 | Tree loss: 1.395 | Accuracy: 0.625000 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 030 | Total loss: 1.396 | Reg loss: 0.054 | Tree loss: 1.396 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 030 | Total loss: 1.362 | Reg loss: 0.054 | Tree loss: 1.362 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 030 | Total loss: 1.349 | Reg loss: 0.055 | Tree loss: 1.349 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 030 | Total loss: 1.351 | Reg loss: 0.055 | Tree loss: 1.351 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 030 | Total loss: 1.345 | Reg loss: 0.055 | Tree loss: 1.345 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 030 | Total loss: 1.312 | Reg loss: 0.055 | Tree loss: 1.312 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 030 | Total loss: 1.275 | Reg loss: 0.055 | Tree loss: 1.275 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 50 | Batch: 029 / 030 | Total loss: 1.219 | Reg loss: 0.055 | Tree loss: 1.219 | Accuracy: 0.600000 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 030 | Total loss: 2.010 | Reg loss: 0.052 | Tree loss: 2.010 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 030 | Total loss: 1.964 | Reg loss: 0.052 | Tree loss: 1.964 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 030 | Total loss: 1.954 | Reg loss: 0.052 | Tree loss: 1.954 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 030 | Total loss: 1.921 | Reg loss: 0.052 | Tree loss: 1.921 | Accuracy: 0.539062 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 030 | Total loss: 1.897 | Reg loss: 0.052 | Tree loss: 1.897 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 030 | Total loss: 1.855 | Reg loss: 0.052 | Tree loss: 1.855 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 030 | Total loss: 1.809 | Reg loss: 0.052 | Tree loss: 1.809 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 030 | Total loss: 1.822 | Reg loss: 0.052 | Tree loss: 1.822 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 030 | Total loss: 1.763 | Reg loss: 0.052 | Tree loss: 1.763 | Accuracy: 0.515625 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 030 | Total loss: 1.741 | Reg loss: 0.053 | Tree loss: 1.741 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 030 | Total loss: 1.702 | Reg loss: 0.053 | Tree loss: 1.702 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 030 | Total loss: 1.647 | Reg loss: 0.053 | Tree loss: 1.647 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 030 | Total loss: 1.605 | Reg loss: 0.053 | Tree loss: 1.605 | Accuracy: 0.628906 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 030 | Total loss: 1.622 | Reg loss: 0.053 | Tree loss: 1.622 | Accuracy: 0.527344 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 030 | Total loss: 1.575 | Reg loss: 0.053 | Tree loss: 1.575 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 030 | Total loss: 1.529 | Reg loss: 0.053 | Tree loss: 1.529 | Accuracy: 0.613281 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 030 | Total loss: 1.523 | Reg loss: 0.053 | Tree loss: 1.523 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 030 | Total loss: 1.494 | Reg loss: 0.053 | Tree loss: 1.494 | Accuracy: 0.572266 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 018 / 030 | Total loss: 1.492 | Reg loss: 0.054 | Tree loss: 1.492 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 030 | Total loss: 1.441 | Reg loss: 0.054 | Tree loss: 1.441 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 030 | Total loss: 1.350 | Reg loss: 0.054 | Tree loss: 1.350 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 030 | Total loss: 1.396 | Reg loss: 0.054 | Tree loss: 1.396 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 030 | Total loss: 1.405 | Reg loss: 0.054 | Tree loss: 1.405 | Accuracy: 0.517578 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 030 | Total loss: 1.334 | Reg loss: 0.054 | Tree loss: 1.334 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 030 | Total loss: 1.341 | Reg loss: 0.054 | Tree loss: 1.341 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 030 | Total loss: 1.296 | Reg loss: 0.054 | Tree loss: 1.296 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 030 | Total loss: 1.299 | Reg loss: 0.055 | Tree loss: 1.299 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 030 | Total loss: 1.254 | Reg loss: 0.055 | Tree loss: 1.254 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 030 | Total loss: 1.266 | Reg loss: 0.055 | Tree loss: 1.266 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 51 | Batch: 029 / 030 | Total loss: 1.345 | Reg loss: 0.055 | Tree loss: 1.345 | Accuracy: 0.476190 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 030 | Total loss: 1.949 | Reg loss: 0.052 | Tree loss: 1.949 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 030 | Total loss: 1.979 | Reg loss: 0.052 | Tree loss: 1.979 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 030 | Total loss: 1.948 | Reg loss: 0.052 | Tree loss: 1.948 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 030 | Total loss: 1.837 | Reg loss: 0.052 | Tree loss: 1.837 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 030 | Total loss: 1.896 | Reg loss: 0.052 | Tree loss: 1.896 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 030 | Total loss: 1.849 | Reg loss: 0.052 | Tree loss: 1.849 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 030 | Total loss: 1.842 | Reg loss: 0.052 | Tree loss: 1.842 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 030 | Total loss: 1.748 | Reg loss: 0.052 | Tree loss: 1.748 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 030 | Total loss: 1.722 | Reg loss: 0.052 | Tree loss: 1.722 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 030 | Total loss: 1.651 | Reg loss: 0.052 | Tree loss: 1.651 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 030 | Total loss: 1.636 | Reg loss: 0.052 | Tree loss: 1.636 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 030 | Total loss: 1.629 | Reg loss: 0.053 | Tree loss: 1.629 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 030 | Total loss: 1.582 | Reg loss: 0.053 | Tree loss: 1.582 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 030 | Total loss: 1.578 | Reg loss: 0.053 | Tree loss: 1.578 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 030 | Total loss: 1.570 | Reg loss: 0.053 | Tree loss: 1.570 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 030 | Total loss: 1.482 | Reg loss: 0.053 | Tree loss: 1.482 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 030 | Total loss: 1.485 | Reg loss: 0.053 | Tree loss: 1.485 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 030 | Total loss: 1.434 | Reg loss: 0.053 | Tree loss: 1.434 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 030 | Total loss: 1.453 | Reg loss: 0.053 | Tree loss: 1.453 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 030 | Total loss: 1.386 | Reg loss: 0.054 | Tree loss: 1.386 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 030 | Total loss: 1.382 | Reg loss: 0.054 | Tree loss: 1.382 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 030 | Total loss: 1.420 | Reg loss: 0.054 | Tree loss: 1.420 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 030 | Total loss: 1.371 | Reg loss: 0.054 | Tree loss: 1.371 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 030 | Total loss: 1.298 | Reg loss: 0.054 | Tree loss: 1.298 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 030 | Total loss: 1.310 | Reg loss: 0.054 | Tree loss: 1.310 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 030 | Total loss: 1.288 | Reg loss: 0.054 | Tree loss: 1.288 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 030 | Total loss: 1.264 | Reg loss: 0.054 | Tree loss: 1.264 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 030 | Total loss: 1.233 | Reg loss: 0.054 | Tree loss: 1.233 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 030 | Total loss: 1.234 | Reg loss: 0.055 | Tree loss: 1.234 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 52 | Batch: 029 / 030 | Total loss: 1.227 | Reg loss: 0.055 | Tree loss: 1.227 | Accuracy: 0.619048 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 030 | Total loss: 2.004 | Reg loss: 0.052 | Tree loss: 2.004 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 030 | Total loss: 1.926 | Reg loss: 0.052 | Tree loss: 1.926 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 030 | Total loss: 1.879 | Reg loss: 0.052 | Tree loss: 1.879 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 030 | Total loss: 1.894 | Reg loss: 0.052 | Tree loss: 1.894 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 030 | Total loss: 1.795 | Reg loss: 0.052 | Tree loss: 1.795 | Accuracy: 0.613281 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 030 | Total loss: 1.782 | Reg loss: 0.052 | Tree loss: 1.782 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 030 | Total loss: 1.713 | Reg loss: 0.052 | Tree loss: 1.713 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 030 | Total loss: 1.745 | Reg loss: 0.052 | Tree loss: 1.745 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 030 | Total loss: 1.701 | Reg loss: 0.052 | Tree loss: 1.701 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 030 | Total loss: 1.745 | Reg loss: 0.052 | Tree loss: 1.745 | Accuracy: 0.525391 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 030 | Total loss: 1.688 | Reg loss: 0.052 | Tree loss: 1.688 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 030 | Total loss: 1.602 | Reg loss: 0.052 | Tree loss: 1.602 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 030 | Total loss: 1.589 | Reg loss: 0.052 | Tree loss: 1.589 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 030 | Total loss: 1.523 | Reg loss: 0.053 | Tree loss: 1.523 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 030 | Total loss: 1.505 | Reg loss: 0.053 | Tree loss: 1.505 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 030 | Total loss: 1.523 | Reg loss: 0.053 | Tree loss: 1.523 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 030 | Total loss: 1.422 | Reg loss: 0.053 | Tree loss: 1.422 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 030 | Total loss: 1.469 | Reg loss: 0.053 | Tree loss: 1.469 | Accuracy: 0.533203 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 030 | Total loss: 1.406 | Reg loss: 0.053 | Tree loss: 1.406 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 030 | Total loss: 1.392 | Reg loss: 0.053 | Tree loss: 1.392 | Accuracy: 0.558594 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 020 / 030 | Total loss: 1.345 | Reg loss: 0.053 | Tree loss: 1.345 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 030 | Total loss: 1.343 | Reg loss: 0.054 | Tree loss: 1.343 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 030 | Total loss: 1.305 | Reg loss: 0.054 | Tree loss: 1.305 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 030 | Total loss: 1.292 | Reg loss: 0.054 | Tree loss: 1.292 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 030 | Total loss: 1.280 | Reg loss: 0.054 | Tree loss: 1.280 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 030 | Total loss: 1.232 | Reg loss: 0.054 | Tree loss: 1.232 | Accuracy: 0.607422 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 030 | Total loss: 1.240 | Reg loss: 0.054 | Tree loss: 1.240 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 030 | Total loss: 1.186 | Reg loss: 0.054 | Tree loss: 1.186 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 030 | Total loss: 1.234 | Reg loss: 0.054 | Tree loss: 1.234 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 53 | Batch: 029 / 030 | Total loss: 1.228 | Reg loss: 0.054 | Tree loss: 1.228 | Accuracy: 0.561905 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 030 | Total loss: 1.974 | Reg loss: 0.052 | Tree loss: 1.974 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 030 | Total loss: 1.936 | Reg loss: 0.052 | Tree loss: 1.936 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 030 | Total loss: 1.860 | Reg loss: 0.052 | Tree loss: 1.860 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 030 | Total loss: 1.869 | Reg loss: 0.052 | Tree loss: 1.869 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 030 | Total loss: 1.759 | Reg loss: 0.052 | Tree loss: 1.759 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 030 | Total loss: 1.806 | Reg loss: 0.052 | Tree loss: 1.806 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 030 | Total loss: 1.730 | Reg loss: 0.052 | Tree loss: 1.730 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 030 | Total loss: 1.711 | Reg loss: 0.052 | Tree loss: 1.711 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 030 | Total loss: 1.662 | Reg loss: 0.052 | Tree loss: 1.662 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 030 | Total loss: 1.667 | Reg loss: 0.052 | Tree loss: 1.667 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 030 | Total loss: 1.596 | Reg loss: 0.052 | Tree loss: 1.596 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 030 | Total loss: 1.584 | Reg loss: 0.052 | Tree loss: 1.584 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 030 | Total loss: 1.524 | Reg loss: 0.052 | Tree loss: 1.524 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 030 | Total loss: 1.514 | Reg loss: 0.052 | Tree loss: 1.514 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 030 | Total loss: 1.493 | Reg loss: 0.052 | Tree loss: 1.493 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 030 | Total loss: 1.497 | Reg loss: 0.053 | Tree loss: 1.497 | Accuracy: 0.529297 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 030 | Total loss: 1.442 | Reg loss: 0.053 | Tree loss: 1.442 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 030 | Total loss: 1.388 | Reg loss: 0.053 | Tree loss: 1.388 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 030 | Total loss: 1.398 | Reg loss: 0.053 | Tree loss: 1.398 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 030 | Total loss: 1.350 | Reg loss: 0.053 | Tree loss: 1.350 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 030 | Total loss: 1.328 | Reg loss: 0.053 | Tree loss: 1.328 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 030 | Total loss: 1.326 | Reg loss: 0.053 | Tree loss: 1.326 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 030 | Total loss: 1.293 | Reg loss: 0.053 | Tree loss: 1.293 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 030 | Total loss: 1.297 | Reg loss: 0.054 | Tree loss: 1.297 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 030 | Total loss: 1.257 | Reg loss: 0.054 | Tree loss: 1.257 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 030 | Total loss: 1.238 | Reg loss: 0.054 | Tree loss: 1.238 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 030 | Total loss: 1.222 | Reg loss: 0.054 | Tree loss: 1.222 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 030 | Total loss: 1.199 | Reg loss: 0.054 | Tree loss: 1.199 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 030 | Total loss: 1.194 | Reg loss: 0.054 | Tree loss: 1.194 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 54 | Batch: 029 / 030 | Total loss: 1.186 | Reg loss: 0.054 | Tree loss: 1.186 | Accuracy: 0.619048 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 030 | Total loss: 1.893 | Reg loss: 0.051 | Tree loss: 1.893 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 030 | Total loss: 1.926 | Reg loss: 0.051 | Tree loss: 1.926 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 030 | Total loss: 1.808 | Reg loss: 0.051 | Tree loss: 1.808 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 030 | Total loss: 1.838 | Reg loss: 0.051 | Tree loss: 1.838 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 030 | Total loss: 1.829 | Reg loss: 0.051 | Tree loss: 1.829 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 030 | Total loss: 1.733 | Reg loss: 0.051 | Tree loss: 1.733 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 030 | Total loss: 1.768 | Reg loss: 0.052 | Tree loss: 1.768 | Accuracy: 0.529297 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 030 | Total loss: 1.684 | Reg loss: 0.052 | Tree loss: 1.684 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 030 | Total loss: 1.750 | Reg loss: 0.052 | Tree loss: 1.750 | Accuracy: 0.527344 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 030 | Total loss: 1.599 | Reg loss: 0.052 | Tree loss: 1.599 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 030 | Total loss: 1.609 | Reg loss: 0.052 | Tree loss: 1.609 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 030 | Total loss: 1.548 | Reg loss: 0.052 | Tree loss: 1.548 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 030 | Total loss: 1.511 | Reg loss: 0.052 | Tree loss: 1.511 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 030 | Total loss: 1.480 | Reg loss: 0.052 | Tree loss: 1.480 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 030 | Total loss: 1.440 | Reg loss: 0.052 | Tree loss: 1.440 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 030 | Total loss: 1.420 | Reg loss: 0.052 | Tree loss: 1.420 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 030 | Total loss: 1.413 | Reg loss: 0.052 | Tree loss: 1.413 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 030 | Total loss: 1.400 | Reg loss: 0.053 | Tree loss: 1.400 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 030 | Total loss: 1.354 | Reg loss: 0.053 | Tree loss: 1.354 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 030 | Total loss: 1.347 | Reg loss: 0.053 | Tree loss: 1.347 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 030 | Total loss: 1.317 | Reg loss: 0.053 | Tree loss: 1.317 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 030 | Total loss: 1.304 | Reg loss: 0.053 | Tree loss: 1.304 | Accuracy: 0.582031 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 022 / 030 | Total loss: 1.252 | Reg loss: 0.053 | Tree loss: 1.252 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 030 | Total loss: 1.275 | Reg loss: 0.053 | Tree loss: 1.275 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 030 | Total loss: 1.267 | Reg loss: 0.053 | Tree loss: 1.267 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 030 | Total loss: 1.231 | Reg loss: 0.054 | Tree loss: 1.231 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 030 | Total loss: 1.190 | Reg loss: 0.054 | Tree loss: 1.190 | Accuracy: 0.613281 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 030 | Total loss: 1.198 | Reg loss: 0.054 | Tree loss: 1.198 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 030 | Total loss: 1.174 | Reg loss: 0.054 | Tree loss: 1.174 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 55 | Batch: 029 / 030 | Total loss: 1.113 | Reg loss: 0.054 | Tree loss: 1.113 | Accuracy: 0.638095 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 030 | Total loss: 1.944 | Reg loss: 0.051 | Tree loss: 1.944 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 030 | Total loss: 1.842 | Reg loss: 0.051 | Tree loss: 1.842 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 030 | Total loss: 1.837 | Reg loss: 0.051 | Tree loss: 1.837 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 030 | Total loss: 1.787 | Reg loss: 0.051 | Tree loss: 1.787 | Accuracy: 0.537109 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 030 | Total loss: 1.756 | Reg loss: 0.051 | Tree loss: 1.756 | Accuracy: 0.605469 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 030 | Total loss: 1.748 | Reg loss: 0.051 | Tree loss: 1.748 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 030 | Total loss: 1.711 | Reg loss: 0.051 | Tree loss: 1.711 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 030 | Total loss: 1.699 | Reg loss: 0.051 | Tree loss: 1.699 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 030 | Total loss: 1.669 | Reg loss: 0.051 | Tree loss: 1.669 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 030 | Total loss: 1.631 | Reg loss: 0.052 | Tree loss: 1.631 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 030 | Total loss: 1.540 | Reg loss: 0.052 | Tree loss: 1.540 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 030 | Total loss: 1.516 | Reg loss: 0.052 | Tree loss: 1.516 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 030 | Total loss: 1.475 | Reg loss: 0.052 | Tree loss: 1.475 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 030 | Total loss: 1.501 | Reg loss: 0.052 | Tree loss: 1.501 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 030 | Total loss: 1.499 | Reg loss: 0.052 | Tree loss: 1.499 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 030 | Total loss: 1.429 | Reg loss: 0.052 | Tree loss: 1.429 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 030 | Total loss: 1.416 | Reg loss: 0.052 | Tree loss: 1.416 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 030 | Total loss: 1.372 | Reg loss: 0.052 | Tree loss: 1.372 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 030 | Total loss: 1.331 | Reg loss: 0.053 | Tree loss: 1.331 | Accuracy: 0.605469 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 030 | Total loss: 1.321 | Reg loss: 0.053 | Tree loss: 1.321 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 030 | Total loss: 1.310 | Reg loss: 0.053 | Tree loss: 1.310 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 030 | Total loss: 1.312 | Reg loss: 0.053 | Tree loss: 1.312 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 030 | Total loss: 1.231 | Reg loss: 0.053 | Tree loss: 1.231 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 030 | Total loss: 1.248 | Reg loss: 0.053 | Tree loss: 1.248 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 030 | Total loss: 1.188 | Reg loss: 0.053 | Tree loss: 1.188 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 030 | Total loss: 1.215 | Reg loss: 0.053 | Tree loss: 1.215 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 030 | Total loss: 1.207 | Reg loss: 0.053 | Tree loss: 1.207 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 030 | Total loss: 1.155 | Reg loss: 0.054 | Tree loss: 1.155 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 030 | Total loss: 1.171 | Reg loss: 0.054 | Tree loss: 1.171 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 56 | Batch: 029 / 030 | Total loss: 1.140 | Reg loss: 0.054 | Tree loss: 1.140 | Accuracy: 0.552381 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 030 | Total loss: 1.918 | Reg loss: 0.051 | Tree loss: 1.918 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 030 | Total loss: 1.811 | Reg loss: 0.051 | Tree loss: 1.811 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 030 | Total loss: 1.801 | Reg loss: 0.051 | Tree loss: 1.801 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 030 | Total loss: 1.795 | Reg loss: 0.051 | Tree loss: 1.795 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 030 | Total loss: 1.757 | Reg loss: 0.051 | Tree loss: 1.757 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 030 | Total loss: 1.716 | Reg loss: 0.051 | Tree loss: 1.716 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 030 | Total loss: 1.685 | Reg loss: 0.051 | Tree loss: 1.685 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 030 | Total loss: 1.637 | Reg loss: 0.051 | Tree loss: 1.637 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 030 | Total loss: 1.667 | Reg loss: 0.051 | Tree loss: 1.667 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 030 | Total loss: 1.577 | Reg loss: 0.051 | Tree loss: 1.577 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 030 | Total loss: 1.546 | Reg loss: 0.051 | Tree loss: 1.546 | Accuracy: 0.617188 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 030 | Total loss: 1.519 | Reg loss: 0.052 | Tree loss: 1.519 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 030 | Total loss: 1.526 | Reg loss: 0.052 | Tree loss: 1.526 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 030 | Total loss: 1.464 | Reg loss: 0.052 | Tree loss: 1.464 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 030 | Total loss: 1.466 | Reg loss: 0.052 | Tree loss: 1.466 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 030 | Total loss: 1.431 | Reg loss: 0.052 | Tree loss: 1.431 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 030 | Total loss: 1.363 | Reg loss: 0.052 | Tree loss: 1.363 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 030 | Total loss: 1.320 | Reg loss: 0.052 | Tree loss: 1.320 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 030 | Total loss: 1.335 | Reg loss: 0.052 | Tree loss: 1.335 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 030 | Total loss: 1.285 | Reg loss: 0.052 | Tree loss: 1.285 | Accuracy: 0.605469 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 030 | Total loss: 1.280 | Reg loss: 0.053 | Tree loss: 1.280 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 030 | Total loss: 1.261 | Reg loss: 0.053 | Tree loss: 1.261 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 030 | Total loss: 1.278 | Reg loss: 0.053 | Tree loss: 1.278 | Accuracy: 0.535156 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 030 | Total loss: 1.218 | Reg loss: 0.053 | Tree loss: 1.218 | Accuracy: 0.554688 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 024 / 030 | Total loss: 1.234 | Reg loss: 0.053 | Tree loss: 1.234 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 030 | Total loss: 1.198 | Reg loss: 0.053 | Tree loss: 1.198 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 030 | Total loss: 1.209 | Reg loss: 0.053 | Tree loss: 1.209 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 030 | Total loss: 1.151 | Reg loss: 0.053 | Tree loss: 1.151 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 030 | Total loss: 1.167 | Reg loss: 0.053 | Tree loss: 1.167 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 57 | Batch: 029 / 030 | Total loss: 1.072 | Reg loss: 0.053 | Tree loss: 1.072 | Accuracy: 0.685714 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 030 | Total loss: 1.836 | Reg loss: 0.051 | Tree loss: 1.836 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 030 | Total loss: 1.848 | Reg loss: 0.051 | Tree loss: 1.848 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 030 | Total loss: 1.787 | Reg loss: 0.051 | Tree loss: 1.787 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 030 | Total loss: 1.766 | Reg loss: 0.051 | Tree loss: 1.766 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 030 | Total loss: 1.753 | Reg loss: 0.051 | Tree loss: 1.753 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 030 | Total loss: 1.699 | Reg loss: 0.051 | Tree loss: 1.699 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 030 | Total loss: 1.651 | Reg loss: 0.051 | Tree loss: 1.651 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 030 | Total loss: 1.658 | Reg loss: 0.051 | Tree loss: 1.658 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 030 | Total loss: 1.618 | Reg loss: 0.051 | Tree loss: 1.618 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 030 | Total loss: 1.574 | Reg loss: 0.051 | Tree loss: 1.574 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 030 | Total loss: 1.596 | Reg loss: 0.051 | Tree loss: 1.596 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 030 | Total loss: 1.498 | Reg loss: 0.051 | Tree loss: 1.498 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 030 | Total loss: 1.491 | Reg loss: 0.051 | Tree loss: 1.491 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 030 | Total loss: 1.432 | Reg loss: 0.052 | Tree loss: 1.432 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 030 | Total loss: 1.412 | Reg loss: 0.052 | Tree loss: 1.412 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 030 | Total loss: 1.366 | Reg loss: 0.052 | Tree loss: 1.366 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 030 | Total loss: 1.387 | Reg loss: 0.052 | Tree loss: 1.387 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 030 | Total loss: 1.342 | Reg loss: 0.052 | Tree loss: 1.342 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 030 | Total loss: 1.299 | Reg loss: 0.052 | Tree loss: 1.299 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 030 | Total loss: 1.298 | Reg loss: 0.052 | Tree loss: 1.298 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 030 | Total loss: 1.293 | Reg loss: 0.052 | Tree loss: 1.293 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 030 | Total loss: 1.236 | Reg loss: 0.052 | Tree loss: 1.236 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 030 | Total loss: 1.231 | Reg loss: 0.053 | Tree loss: 1.231 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 030 | Total loss: 1.218 | Reg loss: 0.053 | Tree loss: 1.218 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 030 | Total loss: 1.196 | Reg loss: 0.053 | Tree loss: 1.196 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 030 | Total loss: 1.187 | Reg loss: 0.053 | Tree loss: 1.187 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 030 | Total loss: 1.161 | Reg loss: 0.053 | Tree loss: 1.161 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 030 | Total loss: 1.173 | Reg loss: 0.053 | Tree loss: 1.173 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 030 | Total loss: 1.128 | Reg loss: 0.053 | Tree loss: 1.128 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 58 | Batch: 029 / 030 | Total loss: 1.075 | Reg loss: 0.053 | Tree loss: 1.075 | Accuracy: 0.619048 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 030 | Total loss: 1.817 | Reg loss: 0.051 | Tree loss: 1.817 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 030 | Total loss: 1.814 | Reg loss: 0.051 | Tree loss: 1.814 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 030 | Total loss: 1.869 | Reg loss: 0.051 | Tree loss: 1.869 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 030 | Total loss: 1.759 | Reg loss: 0.051 | Tree loss: 1.759 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 030 | Total loss: 1.761 | Reg loss: 0.051 | Tree loss: 1.761 | Accuracy: 0.537109 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 030 | Total loss: 1.667 | Reg loss: 0.051 | Tree loss: 1.667 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 030 | Total loss: 1.655 | Reg loss: 0.051 | Tree loss: 1.655 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 030 | Total loss: 1.629 | Reg loss: 0.051 | Tree loss: 1.629 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 030 | Total loss: 1.613 | Reg loss: 0.051 | Tree loss: 1.613 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 030 | Total loss: 1.564 | Reg loss: 0.051 | Tree loss: 1.564 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 030 | Total loss: 1.499 | Reg loss: 0.051 | Tree loss: 1.499 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 030 | Total loss: 1.491 | Reg loss: 0.051 | Tree loss: 1.491 | Accuracy: 0.615234 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 030 | Total loss: 1.516 | Reg loss: 0.051 | Tree loss: 1.516 | Accuracy: 0.533203 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 030 | Total loss: 1.415 | Reg loss: 0.051 | Tree loss: 1.415 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 030 | Total loss: 1.437 | Reg loss: 0.051 | Tree loss: 1.437 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 030 | Total loss: 1.394 | Reg loss: 0.052 | Tree loss: 1.394 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 030 | Total loss: 1.369 | Reg loss: 0.052 | Tree loss: 1.369 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 030 | Total loss: 1.281 | Reg loss: 0.052 | Tree loss: 1.281 | Accuracy: 0.632812 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 030 | Total loss: 1.321 | Reg loss: 0.052 | Tree loss: 1.321 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 030 | Total loss: 1.254 | Reg loss: 0.052 | Tree loss: 1.254 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 030 | Total loss: 1.273 | Reg loss: 0.052 | Tree loss: 1.273 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 030 | Total loss: 1.247 | Reg loss: 0.052 | Tree loss: 1.247 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 030 | Total loss: 1.228 | Reg loss: 0.052 | Tree loss: 1.228 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 030 | Total loss: 1.198 | Reg loss: 0.052 | Tree loss: 1.198 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 030 | Total loss: 1.167 | Reg loss: 0.053 | Tree loss: 1.167 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 030 | Total loss: 1.187 | Reg loss: 0.053 | Tree loss: 1.187 | Accuracy: 0.554688 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Batch: 026 / 030 | Total loss: 1.141 | Reg loss: 0.053 | Tree loss: 1.141 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 030 | Total loss: 1.121 | Reg loss: 0.053 | Tree loss: 1.121 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 030 | Total loss: 1.106 | Reg loss: 0.053 | Tree loss: 1.106 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 59 | Batch: 029 / 030 | Total loss: 1.083 | Reg loss: 0.053 | Tree loss: 1.083 | Accuracy: 0.666667 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 030 | Total loss: 1.835 | Reg loss: 0.050 | Tree loss: 1.835 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 030 | Total loss: 1.828 | Reg loss: 0.050 | Tree loss: 1.828 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 030 | Total loss: 1.751 | Reg loss: 0.050 | Tree loss: 1.751 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 030 | Total loss: 1.696 | Reg loss: 0.050 | Tree loss: 1.696 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 030 | Total loss: 1.674 | Reg loss: 0.050 | Tree loss: 1.674 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 030 | Total loss: 1.658 | Reg loss: 0.051 | Tree loss: 1.658 | Accuracy: 0.605469 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 030 | Total loss: 1.635 | Reg loss: 0.051 | Tree loss: 1.635 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 030 | Total loss: 1.617 | Reg loss: 0.051 | Tree loss: 1.617 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 030 | Total loss: 1.590 | Reg loss: 0.051 | Tree loss: 1.590 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 030 | Total loss: 1.554 | Reg loss: 0.051 | Tree loss: 1.554 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 030 | Total loss: 1.551 | Reg loss: 0.051 | Tree loss: 1.551 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 030 | Total loss: 1.455 | Reg loss: 0.051 | Tree loss: 1.455 | Accuracy: 0.535156 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 030 | Total loss: 1.465 | Reg loss: 0.051 | Tree loss: 1.465 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 030 | Total loss: 1.436 | Reg loss: 0.051 | Tree loss: 1.436 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 030 | Total loss: 1.390 | Reg loss: 0.051 | Tree loss: 1.390 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 030 | Total loss: 1.374 | Reg loss: 0.051 | Tree loss: 1.374 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 030 | Total loss: 1.363 | Reg loss: 0.051 | Tree loss: 1.363 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 030 | Total loss: 1.329 | Reg loss: 0.052 | Tree loss: 1.329 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 030 | Total loss: 1.307 | Reg loss: 0.052 | Tree loss: 1.307 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 030 | Total loss: 1.293 | Reg loss: 0.052 | Tree loss: 1.293 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 030 | Total loss: 1.286 | Reg loss: 0.052 | Tree loss: 1.286 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 030 | Total loss: 1.244 | Reg loss: 0.052 | Tree loss: 1.244 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 030 | Total loss: 1.229 | Reg loss: 0.052 | Tree loss: 1.229 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 030 | Total loss: 1.197 | Reg loss: 0.052 | Tree loss: 1.197 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 030 | Total loss: 1.121 | Reg loss: 0.052 | Tree loss: 1.121 | Accuracy: 0.628906 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 030 | Total loss: 1.150 | Reg loss: 0.052 | Tree loss: 1.150 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 030 | Total loss: 1.142 | Reg loss: 0.053 | Tree loss: 1.142 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 030 | Total loss: 1.131 | Reg loss: 0.053 | Tree loss: 1.131 | Accuracy: 0.623047 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 030 | Total loss: 1.102 | Reg loss: 0.053 | Tree loss: 1.102 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 60 | Batch: 029 / 030 | Total loss: 1.085 | Reg loss: 0.053 | Tree loss: 1.085 | Accuracy: 0.580952 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 030 | Total loss: 1.853 | Reg loss: 0.050 | Tree loss: 1.853 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 030 | Total loss: 1.827 | Reg loss: 0.050 | Tree loss: 1.827 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 030 | Total loss: 1.793 | Reg loss: 0.050 | Tree loss: 1.793 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 030 | Total loss: 1.694 | Reg loss: 0.050 | Tree loss: 1.694 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 030 | Total loss: 1.730 | Reg loss: 0.050 | Tree loss: 1.730 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 030 | Total loss: 1.708 | Reg loss: 0.050 | Tree loss: 1.708 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 030 | Total loss: 1.625 | Reg loss: 0.050 | Tree loss: 1.625 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 030 | Total loss: 1.586 | Reg loss: 0.050 | Tree loss: 1.586 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 030 | Total loss: 1.520 | Reg loss: 0.050 | Tree loss: 1.520 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 030 | Total loss: 1.554 | Reg loss: 0.051 | Tree loss: 1.554 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 030 | Total loss: 1.540 | Reg loss: 0.051 | Tree loss: 1.540 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 030 | Total loss: 1.477 | Reg loss: 0.051 | Tree loss: 1.477 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 030 | Total loss: 1.399 | Reg loss: 0.051 | Tree loss: 1.399 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 030 | Total loss: 1.418 | Reg loss: 0.051 | Tree loss: 1.418 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 030 | Total loss: 1.390 | Reg loss: 0.051 | Tree loss: 1.390 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 030 | Total loss: 1.362 | Reg loss: 0.051 | Tree loss: 1.362 | Accuracy: 0.615234 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 030 | Total loss: 1.357 | Reg loss: 0.051 | Tree loss: 1.357 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 030 | Total loss: 1.263 | Reg loss: 0.051 | Tree loss: 1.263 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 030 | Total loss: 1.279 | Reg loss: 0.051 | Tree loss: 1.279 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 030 | Total loss: 1.267 | Reg loss: 0.052 | Tree loss: 1.267 | Accuracy: 0.535156 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 030 | Total loss: 1.240 | Reg loss: 0.052 | Tree loss: 1.240 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 030 | Total loss: 1.232 | Reg loss: 0.052 | Tree loss: 1.232 | Accuracy: 0.529297 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 030 | Total loss: 1.209 | Reg loss: 0.052 | Tree loss: 1.209 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 030 | Total loss: 1.171 | Reg loss: 0.052 | Tree loss: 1.171 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 030 | Total loss: 1.132 | Reg loss: 0.052 | Tree loss: 1.132 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 030 | Total loss: 1.159 | Reg loss: 0.052 | Tree loss: 1.159 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 030 | Total loss: 1.144 | Reg loss: 0.052 | Tree loss: 1.144 | Accuracy: 0.529297 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 030 | Total loss: 1.096 | Reg loss: 0.052 | Tree loss: 1.096 | Accuracy: 0.605469 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 028 / 030 | Total loss: 1.076 | Reg loss: 0.053 | Tree loss: 1.076 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 61 | Batch: 029 / 030 | Total loss: 1.064 | Reg loss: 0.053 | Tree loss: 1.064 | Accuracy: 0.628571 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 030 | Total loss: 1.787 | Reg loss: 0.050 | Tree loss: 1.787 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 030 | Total loss: 1.717 | Reg loss: 0.050 | Tree loss: 1.717 | Accuracy: 0.630859 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 030 | Total loss: 1.705 | Reg loss: 0.050 | Tree loss: 1.705 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 030 | Total loss: 1.686 | Reg loss: 0.050 | Tree loss: 1.686 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 030 | Total loss: 1.719 | Reg loss: 0.050 | Tree loss: 1.719 | Accuracy: 0.623047 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 030 | Total loss: 1.611 | Reg loss: 0.050 | Tree loss: 1.611 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 030 | Total loss: 1.632 | Reg loss: 0.050 | Tree loss: 1.632 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 030 | Total loss: 1.599 | Reg loss: 0.050 | Tree loss: 1.599 | Accuracy: 0.607422 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 030 | Total loss: 1.547 | Reg loss: 0.050 | Tree loss: 1.547 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 030 | Total loss: 1.524 | Reg loss: 0.050 | Tree loss: 1.524 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 030 | Total loss: 1.510 | Reg loss: 0.050 | Tree loss: 1.510 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 030 | Total loss: 1.468 | Reg loss: 0.051 | Tree loss: 1.468 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 030 | Total loss: 1.449 | Reg loss: 0.051 | Tree loss: 1.449 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 030 | Total loss: 1.404 | Reg loss: 0.051 | Tree loss: 1.404 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 030 | Total loss: 1.368 | Reg loss: 0.051 | Tree loss: 1.368 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 030 | Total loss: 1.384 | Reg loss: 0.051 | Tree loss: 1.384 | Accuracy: 0.529297 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 030 | Total loss: 1.325 | Reg loss: 0.051 | Tree loss: 1.325 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 030 | Total loss: 1.286 | Reg loss: 0.051 | Tree loss: 1.286 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 030 | Total loss: 1.257 | Reg loss: 0.051 | Tree loss: 1.257 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 030 | Total loss: 1.247 | Reg loss: 0.051 | Tree loss: 1.247 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 030 | Total loss: 1.233 | Reg loss: 0.052 | Tree loss: 1.233 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 030 | Total loss: 1.247 | Reg loss: 0.052 | Tree loss: 1.247 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 030 | Total loss: 1.208 | Reg loss: 0.052 | Tree loss: 1.208 | Accuracy: 0.548828 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 030 | Total loss: 1.198 | Reg loss: 0.052 | Tree loss: 1.198 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 030 | Total loss: 1.135 | Reg loss: 0.052 | Tree loss: 1.135 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 030 | Total loss: 1.150 | Reg loss: 0.052 | Tree loss: 1.150 | Accuracy: 0.535156 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 030 | Total loss: 1.159 | Reg loss: 0.052 | Tree loss: 1.159 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 030 | Total loss: 1.134 | Reg loss: 0.052 | Tree loss: 1.134 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 030 | Total loss: 1.102 | Reg loss: 0.052 | Tree loss: 1.102 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 62 | Batch: 029 / 030 | Total loss: 1.083 | Reg loss: 0.052 | Tree loss: 1.083 | Accuracy: 0.476190 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 030 | Total loss: 1.745 | Reg loss: 0.050 | Tree loss: 1.745 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 030 | Total loss: 1.756 | Reg loss: 0.050 | Tree loss: 1.756 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 030 | Total loss: 1.733 | Reg loss: 0.050 | Tree loss: 1.733 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 030 | Total loss: 1.758 | Reg loss: 0.050 | Tree loss: 1.758 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 030 | Total loss: 1.668 | Reg loss: 0.050 | Tree loss: 1.668 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 030 | Total loss: 1.639 | Reg loss: 0.050 | Tree loss: 1.639 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 030 | Total loss: 1.607 | Reg loss: 0.050 | Tree loss: 1.607 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 030 | Total loss: 1.555 | Reg loss: 0.050 | Tree loss: 1.555 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 030 | Total loss: 1.552 | Reg loss: 0.050 | Tree loss: 1.552 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 030 | Total loss: 1.466 | Reg loss: 0.050 | Tree loss: 1.466 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 030 | Total loss: 1.494 | Reg loss: 0.050 | Tree loss: 1.494 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 030 | Total loss: 1.459 | Reg loss: 0.050 | Tree loss: 1.459 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 030 | Total loss: 1.424 | Reg loss: 0.050 | Tree loss: 1.424 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 030 | Total loss: 1.431 | Reg loss: 0.051 | Tree loss: 1.431 | Accuracy: 0.539062 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 030 | Total loss: 1.374 | Reg loss: 0.051 | Tree loss: 1.374 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 030 | Total loss: 1.364 | Reg loss: 0.051 | Tree loss: 1.364 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 030 | Total loss: 1.308 | Reg loss: 0.051 | Tree loss: 1.308 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 030 | Total loss: 1.279 | Reg loss: 0.051 | Tree loss: 1.279 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 030 | Total loss: 1.207 | Reg loss: 0.051 | Tree loss: 1.207 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 030 | Total loss: 1.260 | Reg loss: 0.051 | Tree loss: 1.260 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 030 | Total loss: 1.187 | Reg loss: 0.051 | Tree loss: 1.187 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 030 | Total loss: 1.210 | Reg loss: 0.051 | Tree loss: 1.210 | Accuracy: 0.537109 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 030 | Total loss: 1.179 | Reg loss: 0.052 | Tree loss: 1.179 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 030 | Total loss: 1.176 | Reg loss: 0.052 | Tree loss: 1.176 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 030 | Total loss: 1.147 | Reg loss: 0.052 | Tree loss: 1.147 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 030 | Total loss: 1.154 | Reg loss: 0.052 | Tree loss: 1.154 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 030 | Total loss: 1.115 | Reg loss: 0.052 | Tree loss: 1.115 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 030 | Total loss: 1.121 | Reg loss: 0.052 | Tree loss: 1.121 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 030 | Total loss: 1.099 | Reg loss: 0.052 | Tree loss: 1.099 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 63 | Batch: 029 / 030 | Total loss: 1.036 | Reg loss: 0.052 | Tree loss: 1.036 | Accuracy: 0.552381 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 030 | Total loss: 1.812 | Reg loss: 0.050 | Tree loss: 1.812 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 030 | Total loss: 1.750 | Reg loss: 0.050 | Tree loss: 1.750 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 030 | Total loss: 1.768 | Reg loss: 0.050 | Tree loss: 1.768 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 030 | Total loss: 1.629 | Reg loss: 0.050 | Tree loss: 1.629 | Accuracy: 0.611328 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 030 | Total loss: 1.677 | Reg loss: 0.050 | Tree loss: 1.677 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 030 | Total loss: 1.645 | Reg loss: 0.050 | Tree loss: 1.645 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 030 | Total loss: 1.657 | Reg loss: 0.050 | Tree loss: 1.657 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 030 | Total loss: 1.586 | Reg loss: 0.050 | Tree loss: 1.586 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 030 | Total loss: 1.506 | Reg loss: 0.050 | Tree loss: 1.506 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 030 | Total loss: 1.520 | Reg loss: 0.050 | Tree loss: 1.520 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 030 | Total loss: 1.486 | Reg loss: 0.050 | Tree loss: 1.486 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 030 | Total loss: 1.394 | Reg loss: 0.050 | Tree loss: 1.394 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 030 | Total loss: 1.395 | Reg loss: 0.050 | Tree loss: 1.395 | Accuracy: 0.535156 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 030 | Total loss: 1.386 | Reg loss: 0.050 | Tree loss: 1.386 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 030 | Total loss: 1.381 | Reg loss: 0.051 | Tree loss: 1.381 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 030 | Total loss: 1.348 | Reg loss: 0.051 | Tree loss: 1.348 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 030 | Total loss: 1.278 | Reg loss: 0.051 | Tree loss: 1.278 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 030 | Total loss: 1.289 | Reg loss: 0.051 | Tree loss: 1.289 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 030 | Total loss: 1.249 | Reg loss: 0.051 | Tree loss: 1.249 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 030 | Total loss: 1.221 | Reg loss: 0.051 | Tree loss: 1.221 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 030 | Total loss: 1.204 | Reg loss: 0.051 | Tree loss: 1.204 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 030 | Total loss: 1.198 | Reg loss: 0.051 | Tree loss: 1.198 | Accuracy: 0.535156 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 030 | Total loss: 1.181 | Reg loss: 0.051 | Tree loss: 1.181 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 030 | Total loss: 1.157 | Reg loss: 0.052 | Tree loss: 1.157 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 030 | Total loss: 1.143 | Reg loss: 0.052 | Tree loss: 1.143 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 030 | Total loss: 1.122 | Reg loss: 0.052 | Tree loss: 1.122 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 030 | Total loss: 1.086 | Reg loss: 0.052 | Tree loss: 1.086 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 030 | Total loss: 1.071 | Reg loss: 0.052 | Tree loss: 1.071 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 030 | Total loss: 1.076 | Reg loss: 0.052 | Tree loss: 1.076 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 64 | Batch: 029 / 030 | Total loss: 1.115 | Reg loss: 0.052 | Tree loss: 1.115 | Accuracy: 0.561905 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 030 | Total loss: 1.780 | Reg loss: 0.050 | Tree loss: 1.780 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 030 | Total loss: 1.743 | Reg loss: 0.050 | Tree loss: 1.743 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 030 | Total loss: 1.751 | Reg loss: 0.050 | Tree loss: 1.751 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 030 | Total loss: 1.690 | Reg loss: 0.050 | Tree loss: 1.690 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 030 | Total loss: 1.673 | Reg loss: 0.050 | Tree loss: 1.673 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 030 | Total loss: 1.697 | Reg loss: 0.050 | Tree loss: 1.697 | Accuracy: 0.531250 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 030 | Total loss: 1.617 | Reg loss: 0.050 | Tree loss: 1.617 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 030 | Total loss: 1.568 | Reg loss: 0.050 | Tree loss: 1.568 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 030 | Total loss: 1.498 | Reg loss: 0.050 | Tree loss: 1.498 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 030 | Total loss: 1.476 | Reg loss: 0.050 | Tree loss: 1.476 | Accuracy: 0.613281 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 030 | Total loss: 1.393 | Reg loss: 0.050 | Tree loss: 1.393 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 030 | Total loss: 1.396 | Reg loss: 0.050 | Tree loss: 1.396 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 030 | Total loss: 1.404 | Reg loss: 0.050 | Tree loss: 1.404 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 030 | Total loss: 1.347 | Reg loss: 0.050 | Tree loss: 1.347 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 030 | Total loss: 1.325 | Reg loss: 0.050 | Tree loss: 1.325 | Accuracy: 0.529297 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 030 | Total loss: 1.341 | Reg loss: 0.050 | Tree loss: 1.341 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 030 | Total loss: 1.295 | Reg loss: 0.051 | Tree loss: 1.295 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 030 | Total loss: 1.251 | Reg loss: 0.051 | Tree loss: 1.251 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 030 | Total loss: 1.236 | Reg loss: 0.051 | Tree loss: 1.236 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 030 | Total loss: 1.228 | Reg loss: 0.051 | Tree loss: 1.228 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 030 | Total loss: 1.222 | Reg loss: 0.051 | Tree loss: 1.222 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 030 | Total loss: 1.186 | Reg loss: 0.051 | Tree loss: 1.186 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 030 | Total loss: 1.141 | Reg loss: 0.051 | Tree loss: 1.141 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 030 | Total loss: 1.140 | Reg loss: 0.051 | Tree loss: 1.140 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 030 | Total loss: 1.144 | Reg loss: 0.051 | Tree loss: 1.144 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 030 | Total loss: 1.145 | Reg loss: 0.052 | Tree loss: 1.145 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 030 | Total loss: 1.106 | Reg loss: 0.052 | Tree loss: 1.106 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 030 | Total loss: 1.096 | Reg loss: 0.052 | Tree loss: 1.096 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 030 | Total loss: 1.061 | Reg loss: 0.052 | Tree loss: 1.061 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 65 | Batch: 029 / 030 | Total loss: 1.083 | Reg loss: 0.052 | Tree loss: 1.083 | Accuracy: 0.571429 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Batch: 000 / 030 | Total loss: 1.753 | Reg loss: 0.049 | Tree loss: 1.753 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 030 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 030 | Total loss: 1.666 | Reg loss: 0.049 | Tree loss: 1.666 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 030 | Total loss: 1.642 | Reg loss: 0.049 | Tree loss: 1.642 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 030 | Total loss: 1.672 | Reg loss: 0.049 | Tree loss: 1.672 | Accuracy: 0.527344 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 030 | Total loss: 1.671 | Reg loss: 0.049 | Tree loss: 1.671 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 030 | Total loss: 1.661 | Reg loss: 0.049 | Tree loss: 1.661 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 030 | Total loss: 1.512 | Reg loss: 0.050 | Tree loss: 1.512 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 030 | Total loss: 1.548 | Reg loss: 0.050 | Tree loss: 1.548 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 030 | Total loss: 1.466 | Reg loss: 0.050 | Tree loss: 1.466 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 030 | Total loss: 1.501 | Reg loss: 0.050 | Tree loss: 1.501 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 030 | Total loss: 1.399 | Reg loss: 0.050 | Tree loss: 1.399 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 030 | Total loss: 1.374 | Reg loss: 0.050 | Tree loss: 1.374 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 030 | Total loss: 1.409 | Reg loss: 0.050 | Tree loss: 1.409 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 030 | Total loss: 1.321 | Reg loss: 0.050 | Tree loss: 1.321 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 030 | Total loss: 1.272 | Reg loss: 0.050 | Tree loss: 1.272 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 030 | Total loss: 1.275 | Reg loss: 0.050 | Tree loss: 1.275 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 030 | Total loss: 1.276 | Reg loss: 0.051 | Tree loss: 1.276 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 030 | Total loss: 1.260 | Reg loss: 0.051 | Tree loss: 1.260 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 030 | Total loss: 1.229 | Reg loss: 0.051 | Tree loss: 1.229 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 030 | Total loss: 1.194 | Reg loss: 0.051 | Tree loss: 1.194 | Accuracy: 0.515625 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 030 | Total loss: 1.155 | Reg loss: 0.051 | Tree loss: 1.155 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 030 | Total loss: 1.162 | Reg loss: 0.051 | Tree loss: 1.162 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 030 | Total loss: 1.140 | Reg loss: 0.051 | Tree loss: 1.140 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 030 | Total loss: 1.120 | Reg loss: 0.051 | Tree loss: 1.120 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 030 | Total loss: 1.099 | Reg loss: 0.051 | Tree loss: 1.099 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 030 | Total loss: 1.071 | Reg loss: 0.051 | Tree loss: 1.071 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 030 | Total loss: 1.103 | Reg loss: 0.052 | Tree loss: 1.103 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 030 | Total loss: 1.066 | Reg loss: 0.052 | Tree loss: 1.066 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 66 | Batch: 029 / 030 | Total loss: 1.044 | Reg loss: 0.052 | Tree loss: 1.044 | Accuracy: 0.580952 | 0.908 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 030 | Total loss: 1.789 | Reg loss: 0.049 | Tree loss: 1.789 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 030 | Total loss: 1.686 | Reg loss: 0.049 | Tree loss: 1.686 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 030 | Total loss: 1.648 | Reg loss: 0.049 | Tree loss: 1.648 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 030 | Total loss: 1.671 | Reg loss: 0.049 | Tree loss: 1.671 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 030 | Total loss: 1.653 | Reg loss: 0.049 | Tree loss: 1.653 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 030 | Total loss: 1.652 | Reg loss: 0.049 | Tree loss: 1.652 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 030 | Total loss: 1.589 | Reg loss: 0.049 | Tree loss: 1.589 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 030 | Total loss: 1.598 | Reg loss: 0.049 | Tree loss: 1.598 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 030 | Total loss: 1.510 | Reg loss: 0.049 | Tree loss: 1.510 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 030 | Total loss: 1.573 | Reg loss: 0.049 | Tree loss: 1.573 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 030 | Total loss: 1.443 | Reg loss: 0.050 | Tree loss: 1.443 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 030 | Total loss: 1.450 | Reg loss: 0.050 | Tree loss: 1.450 | Accuracy: 0.527344 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 030 | Total loss: 1.348 | Reg loss: 0.050 | Tree loss: 1.348 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 030 | Total loss: 1.342 | Reg loss: 0.050 | Tree loss: 1.342 | Accuracy: 0.544922 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 030 | Total loss: 1.296 | Reg loss: 0.050 | Tree loss: 1.296 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 030 | Total loss: 1.288 | Reg loss: 0.050 | Tree loss: 1.288 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 030 | Total loss: 1.267 | Reg loss: 0.050 | Tree loss: 1.267 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 030 | Total loss: 1.204 | Reg loss: 0.050 | Tree loss: 1.204 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 030 | Total loss: 1.234 | Reg loss: 0.050 | Tree loss: 1.234 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 030 | Total loss: 1.216 | Reg loss: 0.051 | Tree loss: 1.216 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 030 | Total loss: 1.210 | Reg loss: 0.051 | Tree loss: 1.210 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 030 | Total loss: 1.166 | Reg loss: 0.051 | Tree loss: 1.166 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 030 | Total loss: 1.161 | Reg loss: 0.051 | Tree loss: 1.161 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 030 | Total loss: 1.165 | Reg loss: 0.051 | Tree loss: 1.165 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 030 | Total loss: 1.083 | Reg loss: 0.051 | Tree loss: 1.083 | Accuracy: 0.623047 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 030 | Total loss: 1.088 | Reg loss: 0.051 | Tree loss: 1.088 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 030 | Total loss: 1.077 | Reg loss: 0.051 | Tree loss: 1.077 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 030 | Total loss: 1.072 | Reg loss: 0.051 | Tree loss: 1.072 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 030 | Total loss: 1.069 | Reg loss: 0.051 | Tree loss: 1.069 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 67 | Batch: 029 / 030 | Total loss: 1.024 | Reg loss: 0.052 | Tree loss: 1.024 | Accuracy: 0.609524 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 030 | Total loss: 1.772 | Reg loss: 0.049 | Tree loss: 1.772 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 030 | Total loss: 1.737 | Reg loss: 0.049 | Tree loss: 1.737 | Accuracy: 0.550781 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 | Batch: 002 / 030 | Total loss: 1.756 | Reg loss: 0.049 | Tree loss: 1.756 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 030 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 030 | Total loss: 1.648 | Reg loss: 0.049 | Tree loss: 1.648 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 030 | Total loss: 1.583 | Reg loss: 0.049 | Tree loss: 1.583 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 030 | Total loss: 1.609 | Reg loss: 0.049 | Tree loss: 1.609 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 030 | Total loss: 1.513 | Reg loss: 0.049 | Tree loss: 1.513 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 030 | Total loss: 1.481 | Reg loss: 0.049 | Tree loss: 1.481 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 030 | Total loss: 1.490 | Reg loss: 0.049 | Tree loss: 1.490 | Accuracy: 0.542969 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 030 | Total loss: 1.437 | Reg loss: 0.049 | Tree loss: 1.437 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 030 | Total loss: 1.439 | Reg loss: 0.050 | Tree loss: 1.439 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 030 | Total loss: 1.349 | Reg loss: 0.050 | Tree loss: 1.349 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 030 | Total loss: 1.355 | Reg loss: 0.050 | Tree loss: 1.355 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 030 | Total loss: 1.320 | Reg loss: 0.050 | Tree loss: 1.320 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 030 | Total loss: 1.294 | Reg loss: 0.050 | Tree loss: 1.294 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 030 | Total loss: 1.243 | Reg loss: 0.050 | Tree loss: 1.243 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 030 | Total loss: 1.205 | Reg loss: 0.050 | Tree loss: 1.205 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 030 | Total loss: 1.195 | Reg loss: 0.050 | Tree loss: 1.195 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 030 | Total loss: 1.224 | Reg loss: 0.050 | Tree loss: 1.224 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 030 | Total loss: 1.158 | Reg loss: 0.050 | Tree loss: 1.158 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 030 | Total loss: 1.172 | Reg loss: 0.051 | Tree loss: 1.172 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 030 | Total loss: 1.126 | Reg loss: 0.051 | Tree loss: 1.126 | Accuracy: 0.617188 | 0.908 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 030 | Total loss: 1.133 | Reg loss: 0.051 | Tree loss: 1.133 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 030 | Total loss: 1.083 | Reg loss: 0.051 | Tree loss: 1.083 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 030 | Total loss: 1.097 | Reg loss: 0.051 | Tree loss: 1.097 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 030 | Total loss: 1.088 | Reg loss: 0.051 | Tree loss: 1.088 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 030 | Total loss: 1.084 | Reg loss: 0.051 | Tree loss: 1.084 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 030 | Total loss: 1.050 | Reg loss: 0.051 | Tree loss: 1.050 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 68 | Batch: 029 / 030 | Total loss: 1.053 | Reg loss: 0.051 | Tree loss: 1.053 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 030 | Total loss: 1.772 | Reg loss: 0.049 | Tree loss: 1.772 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 030 | Total loss: 1.709 | Reg loss: 0.049 | Tree loss: 1.709 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 030 | Total loss: 1.743 | Reg loss: 0.049 | Tree loss: 1.743 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 030 | Total loss: 1.666 | Reg loss: 0.049 | Tree loss: 1.666 | Accuracy: 0.595703 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 030 | Total loss: 1.649 | Reg loss: 0.049 | Tree loss: 1.649 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 030 | Total loss: 1.570 | Reg loss: 0.049 | Tree loss: 1.570 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 030 | Total loss: 1.577 | Reg loss: 0.049 | Tree loss: 1.577 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 030 | Total loss: 1.536 | Reg loss: 0.049 | Tree loss: 1.536 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 030 | Total loss: 1.500 | Reg loss: 0.049 | Tree loss: 1.500 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 030 | Total loss: 1.449 | Reg loss: 0.049 | Tree loss: 1.449 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 030 | Total loss: 1.444 | Reg loss: 0.049 | Tree loss: 1.444 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 030 | Total loss: 1.344 | Reg loss: 0.049 | Tree loss: 1.344 | Accuracy: 0.609375 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 030 | Total loss: 1.386 | Reg loss: 0.049 | Tree loss: 1.386 | Accuracy: 0.605469 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 030 | Total loss: 1.323 | Reg loss: 0.050 | Tree loss: 1.323 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 030 | Total loss: 1.306 | Reg loss: 0.050 | Tree loss: 1.306 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 030 | Total loss: 1.266 | Reg loss: 0.050 | Tree loss: 1.266 | Accuracy: 0.589844 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 030 | Total loss: 1.267 | Reg loss: 0.050 | Tree loss: 1.267 | Accuracy: 0.554688 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 030 | Total loss: 1.234 | Reg loss: 0.050 | Tree loss: 1.234 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 030 | Total loss: 1.226 | Reg loss: 0.050 | Tree loss: 1.226 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 030 | Total loss: 1.206 | Reg loss: 0.050 | Tree loss: 1.206 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 030 | Total loss: 1.151 | Reg loss: 0.050 | Tree loss: 1.151 | Accuracy: 0.625000 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 030 | Total loss: 1.147 | Reg loss: 0.050 | Tree loss: 1.147 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 030 | Total loss: 1.124 | Reg loss: 0.051 | Tree loss: 1.124 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 030 | Total loss: 1.141 | Reg loss: 0.051 | Tree loss: 1.141 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 030 | Total loss: 1.132 | Reg loss: 0.051 | Tree loss: 1.132 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 030 | Total loss: 1.094 | Reg loss: 0.051 | Tree loss: 1.094 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 030 | Total loss: 1.043 | Reg loss: 0.051 | Tree loss: 1.043 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 030 | Total loss: 1.063 | Reg loss: 0.051 | Tree loss: 1.063 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 030 | Total loss: 1.074 | Reg loss: 0.051 | Tree loss: 1.074 | Accuracy: 0.517578 | 0.907 sec/iter\n",
      "Epoch: 69 | Batch: 029 / 030 | Total loss: 1.048 | Reg loss: 0.051 | Tree loss: 1.048 | Accuracy: 0.504762 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 030 | Total loss: 1.732 | Reg loss: 0.049 | Tree loss: 1.732 | Accuracy: 0.617188 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 030 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.619141 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 030 | Total loss: 1.666 | Reg loss: 0.049 | Tree loss: 1.666 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 030 | Total loss: 1.680 | Reg loss: 0.049 | Tree loss: 1.680 | Accuracy: 0.578125 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 004 / 030 | Total loss: 1.672 | Reg loss: 0.049 | Tree loss: 1.672 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 030 | Total loss: 1.552 | Reg loss: 0.049 | Tree loss: 1.552 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 030 | Total loss: 1.528 | Reg loss: 0.049 | Tree loss: 1.528 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 030 | Total loss: 1.501 | Reg loss: 0.049 | Tree loss: 1.501 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 030 | Total loss: 1.555 | Reg loss: 0.049 | Tree loss: 1.555 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 030 | Total loss: 1.434 | Reg loss: 0.049 | Tree loss: 1.434 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 030 | Total loss: 1.446 | Reg loss: 0.049 | Tree loss: 1.446 | Accuracy: 0.568359 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 030 | Total loss: 1.363 | Reg loss: 0.049 | Tree loss: 1.363 | Accuracy: 0.552734 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 030 | Total loss: 1.401 | Reg loss: 0.049 | Tree loss: 1.401 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 030 | Total loss: 1.350 | Reg loss: 0.049 | Tree loss: 1.350 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 030 | Total loss: 1.295 | Reg loss: 0.050 | Tree loss: 1.295 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 030 | Total loss: 1.278 | Reg loss: 0.050 | Tree loss: 1.278 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 030 | Total loss: 1.236 | Reg loss: 0.050 | Tree loss: 1.236 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 030 | Total loss: 1.257 | Reg loss: 0.050 | Tree loss: 1.257 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 030 | Total loss: 1.231 | Reg loss: 0.050 | Tree loss: 1.231 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 030 | Total loss: 1.166 | Reg loss: 0.050 | Tree loss: 1.166 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 030 | Total loss: 1.135 | Reg loss: 0.050 | Tree loss: 1.135 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 030 | Total loss: 1.167 | Reg loss: 0.050 | Tree loss: 1.167 | Accuracy: 0.533203 | 0.908 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 030 | Total loss: 1.131 | Reg loss: 0.050 | Tree loss: 1.131 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 030 | Total loss: 1.124 | Reg loss: 0.051 | Tree loss: 1.124 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 030 | Total loss: 1.096 | Reg loss: 0.051 | Tree loss: 1.096 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 030 | Total loss: 1.097 | Reg loss: 0.051 | Tree loss: 1.097 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 030 | Total loss: 1.067 | Reg loss: 0.051 | Tree loss: 1.067 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 030 | Total loss: 1.062 | Reg loss: 0.051 | Tree loss: 1.062 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 030 | Total loss: 1.038 | Reg loss: 0.051 | Tree loss: 1.038 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 70 | Batch: 029 / 030 | Total loss: 1.076 | Reg loss: 0.051 | Tree loss: 1.076 | Accuracy: 0.580952 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 030 | Total loss: 1.732 | Reg loss: 0.049 | Tree loss: 1.732 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 030 | Total loss: 1.687 | Reg loss: 0.049 | Tree loss: 1.687 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 030 | Total loss: 1.687 | Reg loss: 0.049 | Tree loss: 1.687 | Accuracy: 0.537109 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 030 | Total loss: 1.694 | Reg loss: 0.049 | Tree loss: 1.694 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 030 | Total loss: 1.617 | Reg loss: 0.049 | Tree loss: 1.617 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 030 | Total loss: 1.563 | Reg loss: 0.049 | Tree loss: 1.563 | Accuracy: 0.607422 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 030 | Total loss: 1.548 | Reg loss: 0.049 | Tree loss: 1.548 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 030 | Total loss: 1.567 | Reg loss: 0.049 | Tree loss: 1.567 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 030 | Total loss: 1.498 | Reg loss: 0.049 | Tree loss: 1.498 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 030 | Total loss: 1.460 | Reg loss: 0.049 | Tree loss: 1.460 | Accuracy: 0.515625 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 030 | Total loss: 1.432 | Reg loss: 0.049 | Tree loss: 1.432 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 030 | Total loss: 1.409 | Reg loss: 0.049 | Tree loss: 1.409 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 030 | Total loss: 1.383 | Reg loss: 0.049 | Tree loss: 1.383 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 030 | Total loss: 1.322 | Reg loss: 0.049 | Tree loss: 1.322 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 030 | Total loss: 1.284 | Reg loss: 0.049 | Tree loss: 1.284 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 030 | Total loss: 1.252 | Reg loss: 0.049 | Tree loss: 1.252 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 030 | Total loss: 1.249 | Reg loss: 0.050 | Tree loss: 1.249 | Accuracy: 0.574219 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 030 | Total loss: 1.205 | Reg loss: 0.050 | Tree loss: 1.205 | Accuracy: 0.537109 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 030 | Total loss: 1.192 | Reg loss: 0.050 | Tree loss: 1.192 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 030 | Total loss: 1.198 | Reg loss: 0.050 | Tree loss: 1.198 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 030 | Total loss: 1.148 | Reg loss: 0.050 | Tree loss: 1.148 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 030 | Total loss: 1.121 | Reg loss: 0.050 | Tree loss: 1.121 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 030 | Total loss: 1.121 | Reg loss: 0.050 | Tree loss: 1.121 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 030 | Total loss: 1.079 | Reg loss: 0.050 | Tree loss: 1.079 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 030 | Total loss: 1.103 | Reg loss: 0.050 | Tree loss: 1.103 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 030 | Total loss: 1.068 | Reg loss: 0.051 | Tree loss: 1.068 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 030 | Total loss: 1.066 | Reg loss: 0.051 | Tree loss: 1.066 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 030 | Total loss: 1.052 | Reg loss: 0.051 | Tree loss: 1.052 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 030 | Total loss: 1.034 | Reg loss: 0.051 | Tree loss: 1.034 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 71 | Batch: 029 / 030 | Total loss: 1.043 | Reg loss: 0.051 | Tree loss: 1.043 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 030 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.562500 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 030 | Total loss: 1.637 | Reg loss: 0.048 | Tree loss: 1.637 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 030 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.546875 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 030 | Total loss: 1.628 | Reg loss: 0.048 | Tree loss: 1.628 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 030 | Total loss: 1.543 | Reg loss: 0.048 | Tree loss: 1.543 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 030 | Total loss: 1.551 | Reg loss: 0.048 | Tree loss: 1.551 | Accuracy: 0.574219 | 0.908 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 006 / 030 | Total loss: 1.518 | Reg loss: 0.049 | Tree loss: 1.518 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 030 | Total loss: 1.540 | Reg loss: 0.049 | Tree loss: 1.540 | Accuracy: 0.599609 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 030 | Total loss: 1.540 | Reg loss: 0.049 | Tree loss: 1.540 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 030 | Total loss: 1.432 | Reg loss: 0.049 | Tree loss: 1.432 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 030 | Total loss: 1.454 | Reg loss: 0.049 | Tree loss: 1.454 | Accuracy: 0.570312 | 0.908 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 030 | Total loss: 1.385 | Reg loss: 0.049 | Tree loss: 1.385 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 030 | Total loss: 1.360 | Reg loss: 0.049 | Tree loss: 1.360 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 030 | Total loss: 1.361 | Reg loss: 0.049 | Tree loss: 1.361 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 030 | Total loss: 1.332 | Reg loss: 0.049 | Tree loss: 1.332 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 030 | Total loss: 1.282 | Reg loss: 0.049 | Tree loss: 1.282 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 030 | Total loss: 1.226 | Reg loss: 0.049 | Tree loss: 1.226 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 030 | Total loss: 1.220 | Reg loss: 0.050 | Tree loss: 1.220 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 030 | Total loss: 1.200 | Reg loss: 0.050 | Tree loss: 1.200 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 030 | Total loss: 1.159 | Reg loss: 0.050 | Tree loss: 1.159 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 030 | Total loss: 1.138 | Reg loss: 0.050 | Tree loss: 1.138 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 030 | Total loss: 1.156 | Reg loss: 0.050 | Tree loss: 1.156 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 030 | Total loss: 1.096 | Reg loss: 0.050 | Tree loss: 1.096 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 030 | Total loss: 1.107 | Reg loss: 0.050 | Tree loss: 1.107 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 030 | Total loss: 1.082 | Reg loss: 0.050 | Tree loss: 1.082 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 030 | Total loss: 1.091 | Reg loss: 0.050 | Tree loss: 1.091 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 030 | Total loss: 1.045 | Reg loss: 0.051 | Tree loss: 1.045 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 030 | Total loss: 1.061 | Reg loss: 0.051 | Tree loss: 1.061 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 030 | Total loss: 1.040 | Reg loss: 0.051 | Tree loss: 1.040 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 72 | Batch: 029 / 030 | Total loss: 0.963 | Reg loss: 0.051 | Tree loss: 0.963 | Accuracy: 0.628571 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 030 | Total loss: 1.694 | Reg loss: 0.048 | Tree loss: 1.694 | Accuracy: 0.607422 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 030 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.585938 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 030 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 030 | Total loss: 1.624 | Reg loss: 0.048 | Tree loss: 1.624 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 030 | Total loss: 1.627 | Reg loss: 0.048 | Tree loss: 1.627 | Accuracy: 0.580078 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 030 | Total loss: 1.587 | Reg loss: 0.048 | Tree loss: 1.587 | Accuracy: 0.527344 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 030 | Total loss: 1.591 | Reg loss: 0.048 | Tree loss: 1.591 | Accuracy: 0.576172 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 030 | Total loss: 1.458 | Reg loss: 0.048 | Tree loss: 1.458 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 030 | Total loss: 1.471 | Reg loss: 0.049 | Tree loss: 1.471 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 030 | Total loss: 1.446 | Reg loss: 0.049 | Tree loss: 1.446 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 030 | Total loss: 1.391 | Reg loss: 0.049 | Tree loss: 1.391 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 030 | Total loss: 1.356 | Reg loss: 0.049 | Tree loss: 1.356 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 030 | Total loss: 1.366 | Reg loss: 0.049 | Tree loss: 1.366 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 030 | Total loss: 1.304 | Reg loss: 0.049 | Tree loss: 1.304 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 030 | Total loss: 1.263 | Reg loss: 0.049 | Tree loss: 1.263 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 030 | Total loss: 1.221 | Reg loss: 0.049 | Tree loss: 1.221 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 030 | Total loss: 1.248 | Reg loss: 0.049 | Tree loss: 1.248 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 030 | Total loss: 1.216 | Reg loss: 0.049 | Tree loss: 1.216 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 030 | Total loss: 1.191 | Reg loss: 0.050 | Tree loss: 1.191 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 030 | Total loss: 1.196 | Reg loss: 0.050 | Tree loss: 1.196 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 030 | Total loss: 1.156 | Reg loss: 0.050 | Tree loss: 1.156 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 030 | Total loss: 1.139 | Reg loss: 0.050 | Tree loss: 1.139 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 030 | Total loss: 1.108 | Reg loss: 0.050 | Tree loss: 1.108 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 030 | Total loss: 1.110 | Reg loss: 0.050 | Tree loss: 1.110 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 030 | Total loss: 1.072 | Reg loss: 0.050 | Tree loss: 1.072 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 030 | Total loss: 1.044 | Reg loss: 0.050 | Tree loss: 1.044 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 030 | Total loss: 1.080 | Reg loss: 0.050 | Tree loss: 1.080 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 030 | Total loss: 1.061 | Reg loss: 0.050 | Tree loss: 1.061 | Accuracy: 0.525391 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 030 | Total loss: 1.033 | Reg loss: 0.051 | Tree loss: 1.033 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 73 | Batch: 029 / 030 | Total loss: 1.017 | Reg loss: 0.051 | Tree loss: 1.017 | Accuracy: 0.619048 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 030 | Total loss: 1.803 | Reg loss: 0.048 | Tree loss: 1.803 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 030 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.601562 | 0.908 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 030 | Total loss: 1.676 | Reg loss: 0.048 | Tree loss: 1.676 | Accuracy: 0.603516 | 0.908 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 030 | Total loss: 1.654 | Reg loss: 0.048 | Tree loss: 1.654 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 030 | Total loss: 1.547 | Reg loss: 0.048 | Tree loss: 1.547 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 030 | Total loss: 1.572 | Reg loss: 0.048 | Tree loss: 1.572 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 030 | Total loss: 1.507 | Reg loss: 0.048 | Tree loss: 1.507 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 030 | Total loss: 1.507 | Reg loss: 0.048 | Tree loss: 1.507 | Accuracy: 0.537109 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 | Batch: 008 / 030 | Total loss: 1.473 | Reg loss: 0.048 | Tree loss: 1.473 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 030 | Total loss: 1.408 | Reg loss: 0.048 | Tree loss: 1.408 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 030 | Total loss: 1.323 | Reg loss: 0.049 | Tree loss: 1.323 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 030 | Total loss: 1.352 | Reg loss: 0.049 | Tree loss: 1.352 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 030 | Total loss: 1.353 | Reg loss: 0.049 | Tree loss: 1.353 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 030 | Total loss: 1.296 | Reg loss: 0.049 | Tree loss: 1.296 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 030 | Total loss: 1.305 | Reg loss: 0.049 | Tree loss: 1.305 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 030 | Total loss: 1.220 | Reg loss: 0.049 | Tree loss: 1.220 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 030 | Total loss: 1.218 | Reg loss: 0.049 | Tree loss: 1.218 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 030 | Total loss: 1.214 | Reg loss: 0.049 | Tree loss: 1.214 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 030 | Total loss: 1.211 | Reg loss: 0.049 | Tree loss: 1.211 | Accuracy: 0.517578 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 030 | Total loss: 1.180 | Reg loss: 0.050 | Tree loss: 1.180 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 030 | Total loss: 1.153 | Reg loss: 0.050 | Tree loss: 1.153 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 030 | Total loss: 1.123 | Reg loss: 0.050 | Tree loss: 1.123 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 030 | Total loss: 1.101 | Reg loss: 0.050 | Tree loss: 1.101 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 030 | Total loss: 1.128 | Reg loss: 0.050 | Tree loss: 1.128 | Accuracy: 0.515625 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 030 | Total loss: 1.075 | Reg loss: 0.050 | Tree loss: 1.075 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 030 | Total loss: 1.067 | Reg loss: 0.050 | Tree loss: 1.067 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 030 | Total loss: 1.034 | Reg loss: 0.050 | Tree loss: 1.034 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 030 | Total loss: 1.074 | Reg loss: 0.050 | Tree loss: 1.074 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 030 | Total loss: 1.021 | Reg loss: 0.050 | Tree loss: 1.021 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 74 | Batch: 029 / 030 | Total loss: 1.004 | Reg loss: 0.050 | Tree loss: 1.004 | Accuracy: 0.638095 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 030 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.541016 | 0.908 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 030 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.566406 | 0.908 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 030 | Total loss: 1.674 | Reg loss: 0.048 | Tree loss: 1.674 | Accuracy: 0.556641 | 0.908 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 030 | Total loss: 1.643 | Reg loss: 0.048 | Tree loss: 1.643 | Accuracy: 0.527344 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 030 | Total loss: 1.549 | Reg loss: 0.048 | Tree loss: 1.549 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 030 | Total loss: 1.593 | Reg loss: 0.048 | Tree loss: 1.593 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 030 | Total loss: 1.553 | Reg loss: 0.048 | Tree loss: 1.553 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 030 | Total loss: 1.492 | Reg loss: 0.048 | Tree loss: 1.492 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 030 | Total loss: 1.456 | Reg loss: 0.048 | Tree loss: 1.456 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 030 | Total loss: 1.466 | Reg loss: 0.048 | Tree loss: 1.466 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 030 | Total loss: 1.409 | Reg loss: 0.048 | Tree loss: 1.409 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 030 | Total loss: 1.382 | Reg loss: 0.049 | Tree loss: 1.382 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 030 | Total loss: 1.317 | Reg loss: 0.049 | Tree loss: 1.317 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 030 | Total loss: 1.299 | Reg loss: 0.049 | Tree loss: 1.299 | Accuracy: 0.625000 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 030 | Total loss: 1.261 | Reg loss: 0.049 | Tree loss: 1.261 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 030 | Total loss: 1.289 | Reg loss: 0.049 | Tree loss: 1.289 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 030 | Total loss: 1.240 | Reg loss: 0.049 | Tree loss: 1.240 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 030 | Total loss: 1.207 | Reg loss: 0.049 | Tree loss: 1.207 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 030 | Total loss: 1.188 | Reg loss: 0.049 | Tree loss: 1.188 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 030 | Total loss: 1.143 | Reg loss: 0.049 | Tree loss: 1.143 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 030 | Total loss: 1.121 | Reg loss: 0.050 | Tree loss: 1.121 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 030 | Total loss: 1.107 | Reg loss: 0.050 | Tree loss: 1.107 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 030 | Total loss: 1.072 | Reg loss: 0.050 | Tree loss: 1.072 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 030 | Total loss: 1.066 | Reg loss: 0.050 | Tree loss: 1.066 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 030 | Total loss: 1.057 | Reg loss: 0.050 | Tree loss: 1.057 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 030 | Total loss: 1.034 | Reg loss: 0.050 | Tree loss: 1.034 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 030 | Total loss: 1.049 | Reg loss: 0.050 | Tree loss: 1.049 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 030 | Total loss: 1.049 | Reg loss: 0.050 | Tree loss: 1.049 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 030 | Total loss: 1.031 | Reg loss: 0.050 | Tree loss: 1.031 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 75 | Batch: 029 / 030 | Total loss: 0.989 | Reg loss: 0.050 | Tree loss: 0.989 | Accuracy: 0.647619 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 030 | Total loss: 1.744 | Reg loss: 0.048 | Tree loss: 1.744 | Accuracy: 0.558594 | 0.908 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 030 | Total loss: 1.676 | Reg loss: 0.048 | Tree loss: 1.676 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 030 | Total loss: 1.674 | Reg loss: 0.048 | Tree loss: 1.674 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 030 | Total loss: 1.635 | Reg loss: 0.048 | Tree loss: 1.635 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 030 | Total loss: 1.578 | Reg loss: 0.048 | Tree loss: 1.578 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 030 | Total loss: 1.554 | Reg loss: 0.048 | Tree loss: 1.554 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 030 | Total loss: 1.530 | Reg loss: 0.048 | Tree loss: 1.530 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 030 | Total loss: 1.496 | Reg loss: 0.048 | Tree loss: 1.496 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 030 | Total loss: 1.444 | Reg loss: 0.048 | Tree loss: 1.444 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 030 | Total loss: 1.423 | Reg loss: 0.048 | Tree loss: 1.423 | Accuracy: 0.585938 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 010 / 030 | Total loss: 1.366 | Reg loss: 0.048 | Tree loss: 1.366 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 030 | Total loss: 1.403 | Reg loss: 0.048 | Tree loss: 1.403 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 030 | Total loss: 1.330 | Reg loss: 0.049 | Tree loss: 1.330 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 030 | Total loss: 1.318 | Reg loss: 0.049 | Tree loss: 1.318 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 030 | Total loss: 1.268 | Reg loss: 0.049 | Tree loss: 1.268 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 030 | Total loss: 1.244 | Reg loss: 0.049 | Tree loss: 1.244 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 030 | Total loss: 1.183 | Reg loss: 0.049 | Tree loss: 1.183 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 030 | Total loss: 1.218 | Reg loss: 0.049 | Tree loss: 1.218 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 030 | Total loss: 1.151 | Reg loss: 0.049 | Tree loss: 1.151 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 030 | Total loss: 1.148 | Reg loss: 0.049 | Tree loss: 1.148 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 030 | Total loss: 1.109 | Reg loss: 0.049 | Tree loss: 1.109 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 030 | Total loss: 1.123 | Reg loss: 0.050 | Tree loss: 1.123 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 030 | Total loss: 1.101 | Reg loss: 0.050 | Tree loss: 1.101 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 030 | Total loss: 1.098 | Reg loss: 0.050 | Tree loss: 1.098 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 030 | Total loss: 1.079 | Reg loss: 0.050 | Tree loss: 1.079 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 030 | Total loss: 1.082 | Reg loss: 0.050 | Tree loss: 1.082 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 030 | Total loss: 1.041 | Reg loss: 0.050 | Tree loss: 1.041 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 030 | Total loss: 1.047 | Reg loss: 0.050 | Tree loss: 1.047 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 030 | Total loss: 1.010 | Reg loss: 0.050 | Tree loss: 1.010 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 76 | Batch: 029 / 030 | Total loss: 1.001 | Reg loss: 0.050 | Tree loss: 1.001 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 030 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.583984 | 0.908 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 030 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.582031 | 0.908 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 030 | Total loss: 1.620 | Reg loss: 0.048 | Tree loss: 1.620 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 030 | Total loss: 1.659 | Reg loss: 0.048 | Tree loss: 1.659 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 030 | Total loss: 1.643 | Reg loss: 0.048 | Tree loss: 1.643 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 030 | Total loss: 1.563 | Reg loss: 0.048 | Tree loss: 1.563 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 030 | Total loss: 1.514 | Reg loss: 0.048 | Tree loss: 1.514 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 030 | Total loss: 1.477 | Reg loss: 0.048 | Tree loss: 1.477 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 030 | Total loss: 1.484 | Reg loss: 0.048 | Tree loss: 1.484 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 030 | Total loss: 1.389 | Reg loss: 0.048 | Tree loss: 1.389 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 030 | Total loss: 1.407 | Reg loss: 0.048 | Tree loss: 1.407 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 030 | Total loss: 1.327 | Reg loss: 0.048 | Tree loss: 1.327 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 030 | Total loss: 1.315 | Reg loss: 0.048 | Tree loss: 1.315 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 030 | Total loss: 1.275 | Reg loss: 0.048 | Tree loss: 1.275 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 030 | Total loss: 1.270 | Reg loss: 0.049 | Tree loss: 1.270 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 030 | Total loss: 1.261 | Reg loss: 0.049 | Tree loss: 1.261 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 030 | Total loss: 1.236 | Reg loss: 0.049 | Tree loss: 1.236 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 030 | Total loss: 1.159 | Reg loss: 0.049 | Tree loss: 1.159 | Accuracy: 0.636719 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 030 | Total loss: 1.157 | Reg loss: 0.049 | Tree loss: 1.157 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 030 | Total loss: 1.168 | Reg loss: 0.049 | Tree loss: 1.168 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 030 | Total loss: 1.121 | Reg loss: 0.049 | Tree loss: 1.121 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 030 | Total loss: 1.123 | Reg loss: 0.049 | Tree loss: 1.123 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 030 | Total loss: 1.122 | Reg loss: 0.049 | Tree loss: 1.122 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 030 | Total loss: 1.063 | Reg loss: 0.050 | Tree loss: 1.063 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 030 | Total loss: 1.059 | Reg loss: 0.050 | Tree loss: 1.059 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 030 | Total loss: 1.055 | Reg loss: 0.050 | Tree loss: 1.055 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 030 | Total loss: 1.027 | Reg loss: 0.050 | Tree loss: 1.027 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 030 | Total loss: 1.034 | Reg loss: 0.050 | Tree loss: 1.034 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 030 | Total loss: 1.029 | Reg loss: 0.050 | Tree loss: 1.029 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 77 | Batch: 029 / 030 | Total loss: 1.027 | Reg loss: 0.050 | Tree loss: 1.027 | Accuracy: 0.561905 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 030 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 030 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 030 | Total loss: 1.662 | Reg loss: 0.048 | Tree loss: 1.662 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 030 | Total loss: 1.629 | Reg loss: 0.048 | Tree loss: 1.629 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 030 | Total loss: 1.619 | Reg loss: 0.048 | Tree loss: 1.619 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 030 | Total loss: 1.482 | Reg loss: 0.048 | Tree loss: 1.482 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 030 | Total loss: 1.549 | Reg loss: 0.048 | Tree loss: 1.549 | Accuracy: 0.529297 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 030 | Total loss: 1.494 | Reg loss: 0.048 | Tree loss: 1.494 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 030 | Total loss: 1.477 | Reg loss: 0.048 | Tree loss: 1.477 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 030 | Total loss: 1.387 | Reg loss: 0.048 | Tree loss: 1.387 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 030 | Total loss: 1.380 | Reg loss: 0.048 | Tree loss: 1.380 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 030 | Total loss: 1.327 | Reg loss: 0.048 | Tree loss: 1.327 | Accuracy: 0.605469 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 012 / 030 | Total loss: 1.298 | Reg loss: 0.048 | Tree loss: 1.298 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 030 | Total loss: 1.321 | Reg loss: 0.048 | Tree loss: 1.321 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 030 | Total loss: 1.246 | Reg loss: 0.049 | Tree loss: 1.246 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 030 | Total loss: 1.236 | Reg loss: 0.049 | Tree loss: 1.236 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 030 | Total loss: 1.216 | Reg loss: 0.049 | Tree loss: 1.216 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 030 | Total loss: 1.160 | Reg loss: 0.049 | Tree loss: 1.160 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 030 | Total loss: 1.173 | Reg loss: 0.049 | Tree loss: 1.173 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 030 | Total loss: 1.168 | Reg loss: 0.049 | Tree loss: 1.168 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 030 | Total loss: 1.109 | Reg loss: 0.049 | Tree loss: 1.109 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 030 | Total loss: 1.125 | Reg loss: 0.049 | Tree loss: 1.125 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 030 | Total loss: 1.093 | Reg loss: 0.049 | Tree loss: 1.093 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 030 | Total loss: 1.068 | Reg loss: 0.049 | Tree loss: 1.068 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 030 | Total loss: 1.041 | Reg loss: 0.050 | Tree loss: 1.041 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 030 | Total loss: 1.058 | Reg loss: 0.050 | Tree loss: 1.058 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 030 | Total loss: 1.021 | Reg loss: 0.050 | Tree loss: 1.021 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 030 | Total loss: 1.040 | Reg loss: 0.050 | Tree loss: 1.040 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 030 | Total loss: 1.011 | Reg loss: 0.050 | Tree loss: 1.011 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 78 | Batch: 029 / 030 | Total loss: 1.011 | Reg loss: 0.050 | Tree loss: 1.011 | Accuracy: 0.514286 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 030 | Total loss: 1.774 | Reg loss: 0.048 | Tree loss: 1.774 | Accuracy: 0.587891 | 0.908 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 030 | Total loss: 1.642 | Reg loss: 0.048 | Tree loss: 1.642 | Accuracy: 0.550781 | 0.908 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 030 | Total loss: 1.671 | Reg loss: 0.048 | Tree loss: 1.671 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 030 | Total loss: 1.615 | Reg loss: 0.048 | Tree loss: 1.615 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 030 | Total loss: 1.572 | Reg loss: 0.048 | Tree loss: 1.572 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 030 | Total loss: 1.597 | Reg loss: 0.048 | Tree loss: 1.597 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 030 | Total loss: 1.503 | Reg loss: 0.048 | Tree loss: 1.503 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 030 | Total loss: 1.528 | Reg loss: 0.048 | Tree loss: 1.528 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 030 | Total loss: 1.480 | Reg loss: 0.048 | Tree loss: 1.480 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 030 | Total loss: 1.423 | Reg loss: 0.048 | Tree loss: 1.423 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 030 | Total loss: 1.338 | Reg loss: 0.048 | Tree loss: 1.338 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 030 | Total loss: 1.336 | Reg loss: 0.048 | Tree loss: 1.336 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 030 | Total loss: 1.341 | Reg loss: 0.048 | Tree loss: 1.341 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 030 | Total loss: 1.265 | Reg loss: 0.048 | Tree loss: 1.265 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 030 | Total loss: 1.264 | Reg loss: 0.048 | Tree loss: 1.264 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 030 | Total loss: 1.198 | Reg loss: 0.048 | Tree loss: 1.198 | Accuracy: 0.626953 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 030 | Total loss: 1.208 | Reg loss: 0.049 | Tree loss: 1.208 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 030 | Total loss: 1.191 | Reg loss: 0.049 | Tree loss: 1.191 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 030 | Total loss: 1.170 | Reg loss: 0.049 | Tree loss: 1.170 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 030 | Total loss: 1.173 | Reg loss: 0.049 | Tree loss: 1.173 | Accuracy: 0.515625 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 030 | Total loss: 1.112 | Reg loss: 0.049 | Tree loss: 1.112 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 030 | Total loss: 1.076 | Reg loss: 0.049 | Tree loss: 1.076 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 030 | Total loss: 1.073 | Reg loss: 0.049 | Tree loss: 1.073 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 030 | Total loss: 1.087 | Reg loss: 0.049 | Tree loss: 1.087 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 030 | Total loss: 1.049 | Reg loss: 0.049 | Tree loss: 1.049 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 030 | Total loss: 1.032 | Reg loss: 0.050 | Tree loss: 1.032 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 030 | Total loss: 1.004 | Reg loss: 0.050 | Tree loss: 1.004 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 030 | Total loss: 1.014 | Reg loss: 0.050 | Tree loss: 1.014 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 030 | Total loss: 1.003 | Reg loss: 0.050 | Tree loss: 1.003 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 79 | Batch: 029 / 030 | Total loss: 1.022 | Reg loss: 0.050 | Tree loss: 1.022 | Accuracy: 0.561905 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 030 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.572266 | 0.908 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 030 | Total loss: 1.710 | Reg loss: 0.047 | Tree loss: 1.710 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 030 | Total loss: 1.659 | Reg loss: 0.047 | Tree loss: 1.659 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 030 | Total loss: 1.591 | Reg loss: 0.047 | Tree loss: 1.591 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 030 | Total loss: 1.577 | Reg loss: 0.047 | Tree loss: 1.577 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 030 | Total loss: 1.523 | Reg loss: 0.048 | Tree loss: 1.523 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 030 | Total loss: 1.515 | Reg loss: 0.048 | Tree loss: 1.515 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 030 | Total loss: 1.483 | Reg loss: 0.048 | Tree loss: 1.483 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 030 | Total loss: 1.436 | Reg loss: 0.048 | Tree loss: 1.436 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 030 | Total loss: 1.422 | Reg loss: 0.048 | Tree loss: 1.422 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 030 | Total loss: 1.410 | Reg loss: 0.048 | Tree loss: 1.410 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 030 | Total loss: 1.331 | Reg loss: 0.048 | Tree loss: 1.331 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 030 | Total loss: 1.285 | Reg loss: 0.048 | Tree loss: 1.285 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 030 | Total loss: 1.245 | Reg loss: 0.048 | Tree loss: 1.245 | Accuracy: 0.593750 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 014 / 030 | Total loss: 1.225 | Reg loss: 0.048 | Tree loss: 1.225 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 030 | Total loss: 1.229 | Reg loss: 0.048 | Tree loss: 1.229 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 030 | Total loss: 1.209 | Reg loss: 0.049 | Tree loss: 1.209 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 030 | Total loss: 1.200 | Reg loss: 0.049 | Tree loss: 1.200 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 030 | Total loss: 1.152 | Reg loss: 0.049 | Tree loss: 1.152 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 030 | Total loss: 1.129 | Reg loss: 0.049 | Tree loss: 1.129 | Accuracy: 0.615234 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 030 | Total loss: 1.126 | Reg loss: 0.049 | Tree loss: 1.126 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 030 | Total loss: 1.114 | Reg loss: 0.049 | Tree loss: 1.114 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 030 | Total loss: 1.068 | Reg loss: 0.049 | Tree loss: 1.068 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 030 | Total loss: 1.078 | Reg loss: 0.049 | Tree loss: 1.078 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 030 | Total loss: 1.045 | Reg loss: 0.049 | Tree loss: 1.045 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 030 | Total loss: 1.017 | Reg loss: 0.049 | Tree loss: 1.017 | Accuracy: 0.617188 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 030 | Total loss: 1.045 | Reg loss: 0.050 | Tree loss: 1.045 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 030 | Total loss: 1.044 | Reg loss: 0.050 | Tree loss: 1.044 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 028 / 030 | Total loss: 1.010 | Reg loss: 0.050 | Tree loss: 1.010 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 80 | Batch: 029 / 030 | Total loss: 0.977 | Reg loss: 0.050 | Tree loss: 0.977 | Accuracy: 0.600000 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 030 | Total loss: 1.658 | Reg loss: 0.047 | Tree loss: 1.658 | Accuracy: 0.597656 | 0.908 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 030 | Total loss: 1.699 | Reg loss: 0.047 | Tree loss: 1.699 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 030 | Total loss: 1.676 | Reg loss: 0.047 | Tree loss: 1.676 | Accuracy: 0.591797 | 0.908 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 030 | Total loss: 1.606 | Reg loss: 0.047 | Tree loss: 1.606 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 030 | Total loss: 1.586 | Reg loss: 0.047 | Tree loss: 1.586 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 030 | Total loss: 1.560 | Reg loss: 0.047 | Tree loss: 1.560 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 030 | Total loss: 1.510 | Reg loss: 0.047 | Tree loss: 1.510 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 030 | Total loss: 1.479 | Reg loss: 0.048 | Tree loss: 1.479 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 030 | Total loss: 1.462 | Reg loss: 0.048 | Tree loss: 1.462 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 030 | Total loss: 1.458 | Reg loss: 0.048 | Tree loss: 1.458 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 030 | Total loss: 1.361 | Reg loss: 0.048 | Tree loss: 1.361 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 030 | Total loss: 1.318 | Reg loss: 0.048 | Tree loss: 1.318 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 030 | Total loss: 1.312 | Reg loss: 0.048 | Tree loss: 1.312 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 030 | Total loss: 1.296 | Reg loss: 0.048 | Tree loss: 1.296 | Accuracy: 0.529297 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 030 | Total loss: 1.238 | Reg loss: 0.048 | Tree loss: 1.238 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 030 | Total loss: 1.215 | Reg loss: 0.048 | Tree loss: 1.215 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 030 | Total loss: 1.174 | Reg loss: 0.048 | Tree loss: 1.174 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 030 | Total loss: 1.137 | Reg loss: 0.049 | Tree loss: 1.137 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 030 | Total loss: 1.126 | Reg loss: 0.049 | Tree loss: 1.126 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 030 | Total loss: 1.139 | Reg loss: 0.049 | Tree loss: 1.139 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 030 | Total loss: 1.085 | Reg loss: 0.049 | Tree loss: 1.085 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 030 | Total loss: 1.116 | Reg loss: 0.049 | Tree loss: 1.116 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 030 | Total loss: 1.054 | Reg loss: 0.049 | Tree loss: 1.054 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 030 | Total loss: 1.079 | Reg loss: 0.049 | Tree loss: 1.079 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 030 | Total loss: 1.051 | Reg loss: 0.049 | Tree loss: 1.051 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 030 | Total loss: 1.053 | Reg loss: 0.049 | Tree loss: 1.053 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 030 | Total loss: 1.050 | Reg loss: 0.049 | Tree loss: 1.050 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 030 | Total loss: 1.005 | Reg loss: 0.050 | Tree loss: 1.005 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 030 | Total loss: 1.006 | Reg loss: 0.050 | Tree loss: 1.006 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 81 | Batch: 029 / 030 | Total loss: 0.964 | Reg loss: 0.050 | Tree loss: 0.964 | Accuracy: 0.609524 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 030 | Total loss: 1.692 | Reg loss: 0.047 | Tree loss: 1.692 | Accuracy: 0.578125 | 0.908 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 030 | Total loss: 1.681 | Reg loss: 0.047 | Tree loss: 1.681 | Accuracy: 0.564453 | 0.908 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 030 | Total loss: 1.677 | Reg loss: 0.047 | Tree loss: 1.677 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 030 | Total loss: 1.611 | Reg loss: 0.047 | Tree loss: 1.611 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 030 | Total loss: 1.533 | Reg loss: 0.047 | Tree loss: 1.533 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 030 | Total loss: 1.525 | Reg loss: 0.047 | Tree loss: 1.525 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 030 | Total loss: 1.525 | Reg loss: 0.047 | Tree loss: 1.525 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 030 | Total loss: 1.452 | Reg loss: 0.047 | Tree loss: 1.452 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 030 | Total loss: 1.463 | Reg loss: 0.048 | Tree loss: 1.463 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 030 | Total loss: 1.393 | Reg loss: 0.048 | Tree loss: 1.393 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 030 | Total loss: 1.394 | Reg loss: 0.048 | Tree loss: 1.394 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 030 | Total loss: 1.317 | Reg loss: 0.048 | Tree loss: 1.317 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 030 | Total loss: 1.360 | Reg loss: 0.048 | Tree loss: 1.360 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 030 | Total loss: 1.290 | Reg loss: 0.048 | Tree loss: 1.290 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 030 | Total loss: 1.222 | Reg loss: 0.048 | Tree loss: 1.222 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 030 | Total loss: 1.193 | Reg loss: 0.048 | Tree loss: 1.193 | Accuracy: 0.619141 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 016 / 030 | Total loss: 1.189 | Reg loss: 0.048 | Tree loss: 1.189 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 030 | Total loss: 1.197 | Reg loss: 0.048 | Tree loss: 1.197 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 030 | Total loss: 1.169 | Reg loss: 0.049 | Tree loss: 1.169 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 030 | Total loss: 1.130 | Reg loss: 0.049 | Tree loss: 1.130 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 030 | Total loss: 1.097 | Reg loss: 0.049 | Tree loss: 1.097 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 030 | Total loss: 1.094 | Reg loss: 0.049 | Tree loss: 1.094 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 030 | Total loss: 1.098 | Reg loss: 0.049 | Tree loss: 1.098 | Accuracy: 0.527344 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 030 | Total loss: 1.054 | Reg loss: 0.049 | Tree loss: 1.054 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 030 | Total loss: 1.031 | Reg loss: 0.049 | Tree loss: 1.031 | Accuracy: 0.611328 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 030 | Total loss: 1.019 | Reg loss: 0.049 | Tree loss: 1.019 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 030 | Total loss: 1.013 | Reg loss: 0.049 | Tree loss: 1.013 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 030 | Total loss: 1.004 | Reg loss: 0.049 | Tree loss: 1.004 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 030 | Total loss: 0.995 | Reg loss: 0.050 | Tree loss: 0.995 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 82 | Batch: 029 / 030 | Total loss: 0.998 | Reg loss: 0.050 | Tree loss: 0.998 | Accuracy: 0.600000 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 030 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.560547 | 0.908 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 030 | Total loss: 1.620 | Reg loss: 0.047 | Tree loss: 1.620 | Accuracy: 0.593750 | 0.908 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 030 | Total loss: 1.659 | Reg loss: 0.047 | Tree loss: 1.659 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 030 | Total loss: 1.614 | Reg loss: 0.047 | Tree loss: 1.614 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 030 | Total loss: 1.581 | Reg loss: 0.047 | Tree loss: 1.581 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 030 | Total loss: 1.517 | Reg loss: 0.047 | Tree loss: 1.517 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 030 | Total loss: 1.594 | Reg loss: 0.047 | Tree loss: 1.594 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 030 | Total loss: 1.485 | Reg loss: 0.047 | Tree loss: 1.485 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 030 | Total loss: 1.473 | Reg loss: 0.047 | Tree loss: 1.473 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 030 | Total loss: 1.392 | Reg loss: 0.048 | Tree loss: 1.392 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 030 | Total loss: 1.314 | Reg loss: 0.048 | Tree loss: 1.314 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 030 | Total loss: 1.333 | Reg loss: 0.048 | Tree loss: 1.333 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 030 | Total loss: 1.308 | Reg loss: 0.048 | Tree loss: 1.308 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 030 | Total loss: 1.244 | Reg loss: 0.048 | Tree loss: 1.244 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 030 | Total loss: 1.271 | Reg loss: 0.048 | Tree loss: 1.271 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 030 | Total loss: 1.159 | Reg loss: 0.048 | Tree loss: 1.159 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 030 | Total loss: 1.180 | Reg loss: 0.048 | Tree loss: 1.180 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 030 | Total loss: 1.160 | Reg loss: 0.048 | Tree loss: 1.160 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 030 | Total loss: 1.153 | Reg loss: 0.048 | Tree loss: 1.153 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 030 | Total loss: 1.146 | Reg loss: 0.049 | Tree loss: 1.146 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 030 | Total loss: 1.080 | Reg loss: 0.049 | Tree loss: 1.080 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 030 | Total loss: 1.117 | Reg loss: 0.049 | Tree loss: 1.117 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 030 | Total loss: 1.070 | Reg loss: 0.049 | Tree loss: 1.070 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 030 | Total loss: 1.053 | Reg loss: 0.049 | Tree loss: 1.053 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 030 | Total loss: 1.061 | Reg loss: 0.049 | Tree loss: 1.061 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 030 | Total loss: 1.020 | Reg loss: 0.049 | Tree loss: 1.020 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 030 | Total loss: 1.014 | Reg loss: 0.049 | Tree loss: 1.014 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 030 | Total loss: 0.992 | Reg loss: 0.049 | Tree loss: 0.992 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 030 | Total loss: 0.987 | Reg loss: 0.049 | Tree loss: 0.987 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 83 | Batch: 029 / 030 | Total loss: 1.020 | Reg loss: 0.050 | Tree loss: 1.020 | Accuracy: 0.600000 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 030 | Total loss: 1.708 | Reg loss: 0.047 | Tree loss: 1.708 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 030 | Total loss: 1.656 | Reg loss: 0.047 | Tree loss: 1.656 | Accuracy: 0.509766 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 030 | Total loss: 1.652 | Reg loss: 0.047 | Tree loss: 1.652 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 030 | Total loss: 1.618 | Reg loss: 0.047 | Tree loss: 1.618 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 030 | Total loss: 1.620 | Reg loss: 0.047 | Tree loss: 1.620 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 030 | Total loss: 1.573 | Reg loss: 0.047 | Tree loss: 1.573 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 030 | Total loss: 1.473 | Reg loss: 0.047 | Tree loss: 1.473 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 030 | Total loss: 1.482 | Reg loss: 0.047 | Tree loss: 1.482 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 030 | Total loss: 1.413 | Reg loss: 0.047 | Tree loss: 1.413 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 030 | Total loss: 1.358 | Reg loss: 0.047 | Tree loss: 1.358 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 030 | Total loss: 1.315 | Reg loss: 0.047 | Tree loss: 1.315 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 030 | Total loss: 1.369 | Reg loss: 0.048 | Tree loss: 1.369 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 030 | Total loss: 1.300 | Reg loss: 0.048 | Tree loss: 1.300 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 030 | Total loss: 1.250 | Reg loss: 0.048 | Tree loss: 1.250 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 030 | Total loss: 1.222 | Reg loss: 0.048 | Tree loss: 1.222 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 030 | Total loss: 1.212 | Reg loss: 0.048 | Tree loss: 1.212 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 030 | Total loss: 1.182 | Reg loss: 0.048 | Tree loss: 1.182 | Accuracy: 0.617188 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 030 | Total loss: 1.174 | Reg loss: 0.048 | Tree loss: 1.174 | Accuracy: 0.595703 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 018 / 030 | Total loss: 1.133 | Reg loss: 0.048 | Tree loss: 1.133 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 030 | Total loss: 1.139 | Reg loss: 0.048 | Tree loss: 1.139 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 030 | Total loss: 1.136 | Reg loss: 0.049 | Tree loss: 1.136 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 030 | Total loss: 1.100 | Reg loss: 0.049 | Tree loss: 1.100 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 030 | Total loss: 1.075 | Reg loss: 0.049 | Tree loss: 1.075 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 030 | Total loss: 1.040 | Reg loss: 0.049 | Tree loss: 1.040 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 030 | Total loss: 1.068 | Reg loss: 0.049 | Tree loss: 1.068 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 030 | Total loss: 1.007 | Reg loss: 0.049 | Tree loss: 1.007 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 030 | Total loss: 1.000 | Reg loss: 0.049 | Tree loss: 1.000 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 030 | Total loss: 0.982 | Reg loss: 0.049 | Tree loss: 0.982 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 030 | Total loss: 0.970 | Reg loss: 0.049 | Tree loss: 0.970 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 84 | Batch: 029 / 030 | Total loss: 0.918 | Reg loss: 0.049 | Tree loss: 0.918 | Accuracy: 0.628571 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 030 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 030 | Total loss: 1.696 | Reg loss: 0.047 | Tree loss: 1.696 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 030 | Total loss: 1.594 | Reg loss: 0.047 | Tree loss: 1.594 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 030 | Total loss: 1.633 | Reg loss: 0.047 | Tree loss: 1.633 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 030 | Total loss: 1.569 | Reg loss: 0.047 | Tree loss: 1.569 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 030 | Total loss: 1.557 | Reg loss: 0.047 | Tree loss: 1.557 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 030 | Total loss: 1.481 | Reg loss: 0.047 | Tree loss: 1.481 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 030 | Total loss: 1.418 | Reg loss: 0.047 | Tree loss: 1.418 | Accuracy: 0.527344 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 030 | Total loss: 1.435 | Reg loss: 0.047 | Tree loss: 1.435 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 030 | Total loss: 1.429 | Reg loss: 0.047 | Tree loss: 1.429 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 030 | Total loss: 1.359 | Reg loss: 0.047 | Tree loss: 1.359 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 030 | Total loss: 1.310 | Reg loss: 0.047 | Tree loss: 1.310 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 030 | Total loss: 1.326 | Reg loss: 0.048 | Tree loss: 1.326 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 030 | Total loss: 1.264 | Reg loss: 0.048 | Tree loss: 1.264 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 030 | Total loss: 1.259 | Reg loss: 0.048 | Tree loss: 1.259 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 030 | Total loss: 1.186 | Reg loss: 0.048 | Tree loss: 1.186 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 030 | Total loss: 1.180 | Reg loss: 0.048 | Tree loss: 1.180 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 030 | Total loss: 1.139 | Reg loss: 0.048 | Tree loss: 1.139 | Accuracy: 0.636719 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 030 | Total loss: 1.166 | Reg loss: 0.048 | Tree loss: 1.166 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 030 | Total loss: 1.133 | Reg loss: 0.048 | Tree loss: 1.133 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 030 | Total loss: 1.115 | Reg loss: 0.048 | Tree loss: 1.115 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 030 | Total loss: 1.089 | Reg loss: 0.049 | Tree loss: 1.089 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 030 | Total loss: 1.087 | Reg loss: 0.049 | Tree loss: 1.087 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 030 | Total loss: 1.036 | Reg loss: 0.049 | Tree loss: 1.036 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 030 | Total loss: 1.026 | Reg loss: 0.049 | Tree loss: 1.026 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 030 | Total loss: 1.021 | Reg loss: 0.049 | Tree loss: 1.021 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 030 | Total loss: 0.987 | Reg loss: 0.049 | Tree loss: 0.987 | Accuracy: 0.613281 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 030 | Total loss: 0.990 | Reg loss: 0.049 | Tree loss: 0.990 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 030 | Total loss: 0.997 | Reg loss: 0.049 | Tree loss: 0.997 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 85 | Batch: 029 / 030 | Total loss: 0.910 | Reg loss: 0.049 | Tree loss: 0.910 | Accuracy: 0.638095 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 030 | Total loss: 1.700 | Reg loss: 0.047 | Tree loss: 1.700 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 030 | Total loss: 1.631 | Reg loss: 0.047 | Tree loss: 1.631 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 030 | Total loss: 1.638 | Reg loss: 0.047 | Tree loss: 1.638 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 030 | Total loss: 1.585 | Reg loss: 0.047 | Tree loss: 1.585 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 030 | Total loss: 1.578 | Reg loss: 0.047 | Tree loss: 1.578 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 030 | Total loss: 1.477 | Reg loss: 0.047 | Tree loss: 1.477 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 030 | Total loss: 1.479 | Reg loss: 0.047 | Tree loss: 1.479 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 030 | Total loss: 1.461 | Reg loss: 0.047 | Tree loss: 1.461 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 030 | Total loss: 1.429 | Reg loss: 0.047 | Tree loss: 1.429 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 030 | Total loss: 1.388 | Reg loss: 0.047 | Tree loss: 1.388 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 030 | Total loss: 1.377 | Reg loss: 0.047 | Tree loss: 1.377 | Accuracy: 0.523438 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 030 | Total loss: 1.310 | Reg loss: 0.047 | Tree loss: 1.310 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 030 | Total loss: 1.325 | Reg loss: 0.047 | Tree loss: 1.325 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 030 | Total loss: 1.280 | Reg loss: 0.048 | Tree loss: 1.280 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 030 | Total loss: 1.271 | Reg loss: 0.048 | Tree loss: 1.271 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 030 | Total loss: 1.247 | Reg loss: 0.048 | Tree loss: 1.247 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 030 | Total loss: 1.176 | Reg loss: 0.048 | Tree loss: 1.176 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 030 | Total loss: 1.154 | Reg loss: 0.048 | Tree loss: 1.154 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 030 | Total loss: 1.145 | Reg loss: 0.048 | Tree loss: 1.145 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 030 | Total loss: 1.135 | Reg loss: 0.048 | Tree loss: 1.135 | Accuracy: 0.572266 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 020 / 030 | Total loss: 1.112 | Reg loss: 0.048 | Tree loss: 1.112 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 030 | Total loss: 1.102 | Reg loss: 0.049 | Tree loss: 1.102 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 030 | Total loss: 1.094 | Reg loss: 0.049 | Tree loss: 1.094 | Accuracy: 0.539062 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 030 | Total loss: 1.052 | Reg loss: 0.049 | Tree loss: 1.052 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 030 | Total loss: 1.009 | Reg loss: 0.049 | Tree loss: 1.009 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 030 | Total loss: 1.018 | Reg loss: 0.049 | Tree loss: 1.018 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 030 | Total loss: 0.994 | Reg loss: 0.049 | Tree loss: 0.994 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 030 | Total loss: 0.979 | Reg loss: 0.049 | Tree loss: 0.979 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 030 | Total loss: 0.972 | Reg loss: 0.049 | Tree loss: 0.972 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 86 | Batch: 029 / 030 | Total loss: 0.897 | Reg loss: 0.049 | Tree loss: 0.897 | Accuracy: 0.571429 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 030 | Total loss: 1.749 | Reg loss: 0.047 | Tree loss: 1.749 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 030 | Total loss: 1.675 | Reg loss: 0.047 | Tree loss: 1.675 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 030 | Total loss: 1.608 | Reg loss: 0.047 | Tree loss: 1.608 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 030 | Total loss: 1.559 | Reg loss: 0.047 | Tree loss: 1.559 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 030 | Total loss: 1.600 | Reg loss: 0.047 | Tree loss: 1.600 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 030 | Total loss: 1.537 | Reg loss: 0.047 | Tree loss: 1.537 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 030 | Total loss: 1.473 | Reg loss: 0.047 | Tree loss: 1.473 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 030 | Total loss: 1.471 | Reg loss: 0.047 | Tree loss: 1.471 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 030 | Total loss: 1.426 | Reg loss: 0.047 | Tree loss: 1.426 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 030 | Total loss: 1.441 | Reg loss: 0.047 | Tree loss: 1.441 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 030 | Total loss: 1.331 | Reg loss: 0.047 | Tree loss: 1.331 | Accuracy: 0.621094 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 030 | Total loss: 1.347 | Reg loss: 0.047 | Tree loss: 1.347 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 030 | Total loss: 1.269 | Reg loss: 0.047 | Tree loss: 1.269 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 030 | Total loss: 1.303 | Reg loss: 0.048 | Tree loss: 1.303 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 030 | Total loss: 1.234 | Reg loss: 0.048 | Tree loss: 1.234 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 030 | Total loss: 1.187 | Reg loss: 0.048 | Tree loss: 1.187 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 030 | Total loss: 1.132 | Reg loss: 0.048 | Tree loss: 1.132 | Accuracy: 0.611328 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 030 | Total loss: 1.148 | Reg loss: 0.048 | Tree loss: 1.148 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 030 | Total loss: 1.124 | Reg loss: 0.048 | Tree loss: 1.124 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 030 | Total loss: 1.138 | Reg loss: 0.048 | Tree loss: 1.138 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 030 | Total loss: 1.097 | Reg loss: 0.048 | Tree loss: 1.097 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 030 | Total loss: 1.075 | Reg loss: 0.048 | Tree loss: 1.075 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 030 | Total loss: 1.049 | Reg loss: 0.049 | Tree loss: 1.049 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 030 | Total loss: 1.062 | Reg loss: 0.049 | Tree loss: 1.062 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 030 | Total loss: 1.032 | Reg loss: 0.049 | Tree loss: 1.032 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 030 | Total loss: 1.018 | Reg loss: 0.049 | Tree loss: 1.018 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 030 | Total loss: 1.026 | Reg loss: 0.049 | Tree loss: 1.026 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 030 | Total loss: 0.991 | Reg loss: 0.049 | Tree loss: 0.991 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 030 | Total loss: 0.947 | Reg loss: 0.049 | Tree loss: 0.947 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 87 | Batch: 029 / 030 | Total loss: 0.988 | Reg loss: 0.049 | Tree loss: 0.988 | Accuracy: 0.590476 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 030 | Total loss: 1.654 | Reg loss: 0.047 | Tree loss: 1.654 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 030 | Total loss: 1.643 | Reg loss: 0.047 | Tree loss: 1.643 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 030 | Total loss: 1.650 | Reg loss: 0.047 | Tree loss: 1.650 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 030 | Total loss: 1.640 | Reg loss: 0.047 | Tree loss: 1.640 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 030 | Total loss: 1.549 | Reg loss: 0.047 | Tree loss: 1.549 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 030 | Total loss: 1.542 | Reg loss: 0.047 | Tree loss: 1.542 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 030 | Total loss: 1.481 | Reg loss: 0.047 | Tree loss: 1.481 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 030 | Total loss: 1.446 | Reg loss: 0.047 | Tree loss: 1.446 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 030 | Total loss: 1.409 | Reg loss: 0.047 | Tree loss: 1.409 | Accuracy: 0.611328 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 030 | Total loss: 1.352 | Reg loss: 0.047 | Tree loss: 1.352 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 030 | Total loss: 1.336 | Reg loss: 0.047 | Tree loss: 1.336 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 030 | Total loss: 1.311 | Reg loss: 0.047 | Tree loss: 1.311 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 030 | Total loss: 1.333 | Reg loss: 0.047 | Tree loss: 1.333 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 030 | Total loss: 1.320 | Reg loss: 0.047 | Tree loss: 1.320 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 030 | Total loss: 1.276 | Reg loss: 0.048 | Tree loss: 1.276 | Accuracy: 0.531250 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 030 | Total loss: 1.194 | Reg loss: 0.048 | Tree loss: 1.194 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 030 | Total loss: 1.188 | Reg loss: 0.048 | Tree loss: 1.188 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 030 | Total loss: 1.166 | Reg loss: 0.048 | Tree loss: 1.166 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 030 | Total loss: 1.094 | Reg loss: 0.048 | Tree loss: 1.094 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 030 | Total loss: 1.119 | Reg loss: 0.048 | Tree loss: 1.119 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 030 | Total loss: 1.075 | Reg loss: 0.048 | Tree loss: 1.075 | Accuracy: 0.625000 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 030 | Total loss: 1.051 | Reg loss: 0.048 | Tree loss: 1.051 | Accuracy: 0.607422 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 | Batch: 022 / 030 | Total loss: 1.076 | Reg loss: 0.048 | Tree loss: 1.076 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 030 | Total loss: 1.035 | Reg loss: 0.049 | Tree loss: 1.035 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 030 | Total loss: 1.016 | Reg loss: 0.049 | Tree loss: 1.016 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 030 | Total loss: 1.010 | Reg loss: 0.049 | Tree loss: 1.010 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 030 | Total loss: 0.993 | Reg loss: 0.049 | Tree loss: 0.993 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 030 | Total loss: 1.009 | Reg loss: 0.049 | Tree loss: 1.009 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 030 | Total loss: 0.968 | Reg loss: 0.049 | Tree loss: 0.968 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 88 | Batch: 029 / 030 | Total loss: 0.962 | Reg loss: 0.049 | Tree loss: 0.962 | Accuracy: 0.533333 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 030 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 030 | Total loss: 1.691 | Reg loss: 0.047 | Tree loss: 1.691 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 030 | Total loss: 1.642 | Reg loss: 0.047 | Tree loss: 1.642 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 030 | Total loss: 1.576 | Reg loss: 0.047 | Tree loss: 1.576 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 030 | Total loss: 1.524 | Reg loss: 0.047 | Tree loss: 1.524 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 030 | Total loss: 1.497 | Reg loss: 0.047 | Tree loss: 1.497 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 030 | Total loss: 1.485 | Reg loss: 0.047 | Tree loss: 1.485 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 030 | Total loss: 1.502 | Reg loss: 0.047 | Tree loss: 1.502 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 030 | Total loss: 1.369 | Reg loss: 0.047 | Tree loss: 1.369 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 030 | Total loss: 1.407 | Reg loss: 0.047 | Tree loss: 1.407 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 030 | Total loss: 1.355 | Reg loss: 0.047 | Tree loss: 1.355 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 030 | Total loss: 1.302 | Reg loss: 0.047 | Tree loss: 1.302 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 030 | Total loss: 1.314 | Reg loss: 0.047 | Tree loss: 1.314 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 030 | Total loss: 1.270 | Reg loss: 0.047 | Tree loss: 1.270 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 030 | Total loss: 1.217 | Reg loss: 0.047 | Tree loss: 1.217 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 030 | Total loss: 1.175 | Reg loss: 0.048 | Tree loss: 1.175 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 030 | Total loss: 1.156 | Reg loss: 0.048 | Tree loss: 1.156 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 030 | Total loss: 1.168 | Reg loss: 0.048 | Tree loss: 1.168 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 030 | Total loss: 1.145 | Reg loss: 0.048 | Tree loss: 1.145 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 030 | Total loss: 1.119 | Reg loss: 0.048 | Tree loss: 1.119 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 030 | Total loss: 1.079 | Reg loss: 0.048 | Tree loss: 1.079 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 030 | Total loss: 1.029 | Reg loss: 0.048 | Tree loss: 1.029 | Accuracy: 0.617188 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 030 | Total loss: 1.061 | Reg loss: 0.048 | Tree loss: 1.061 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 030 | Total loss: 1.025 | Reg loss: 0.048 | Tree loss: 1.025 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 030 | Total loss: 1.028 | Reg loss: 0.049 | Tree loss: 1.028 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 030 | Total loss: 1.034 | Reg loss: 0.049 | Tree loss: 1.034 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 030 | Total loss: 1.017 | Reg loss: 0.049 | Tree loss: 1.017 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 030 | Total loss: 0.972 | Reg loss: 0.049 | Tree loss: 0.972 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 030 | Total loss: 0.979 | Reg loss: 0.049 | Tree loss: 0.979 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 89 | Batch: 029 / 030 | Total loss: 1.001 | Reg loss: 0.049 | Tree loss: 1.001 | Accuracy: 0.533333 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 030 | Total loss: 1.638 | Reg loss: 0.047 | Tree loss: 1.638 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 030 | Total loss: 1.647 | Reg loss: 0.047 | Tree loss: 1.647 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 030 | Total loss: 1.586 | Reg loss: 0.047 | Tree loss: 1.586 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 030 | Total loss: 1.616 | Reg loss: 0.047 | Tree loss: 1.616 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 030 | Total loss: 1.534 | Reg loss: 0.047 | Tree loss: 1.534 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 030 | Total loss: 1.565 | Reg loss: 0.047 | Tree loss: 1.565 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 030 | Total loss: 1.486 | Reg loss: 0.047 | Tree loss: 1.486 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 030 | Total loss: 1.432 | Reg loss: 0.047 | Tree loss: 1.432 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 030 | Total loss: 1.403 | Reg loss: 0.047 | Tree loss: 1.403 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 030 | Total loss: 1.367 | Reg loss: 0.047 | Tree loss: 1.367 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 030 | Total loss: 1.359 | Reg loss: 0.047 | Tree loss: 1.359 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 030 | Total loss: 1.277 | Reg loss: 0.047 | Tree loss: 1.277 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 030 | Total loss: 1.279 | Reg loss: 0.047 | Tree loss: 1.279 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 030 | Total loss: 1.240 | Reg loss: 0.047 | Tree loss: 1.240 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 030 | Total loss: 1.258 | Reg loss: 0.047 | Tree loss: 1.258 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 030 | Total loss: 1.231 | Reg loss: 0.048 | Tree loss: 1.231 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 030 | Total loss: 1.155 | Reg loss: 0.048 | Tree loss: 1.155 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 030 | Total loss: 1.171 | Reg loss: 0.048 | Tree loss: 1.171 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 030 | Total loss: 1.127 | Reg loss: 0.048 | Tree loss: 1.127 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 030 | Total loss: 1.115 | Reg loss: 0.048 | Tree loss: 1.115 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 030 | Total loss: 1.100 | Reg loss: 0.048 | Tree loss: 1.100 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 030 | Total loss: 1.095 | Reg loss: 0.048 | Tree loss: 1.095 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 030 | Total loss: 1.040 | Reg loss: 0.048 | Tree loss: 1.040 | Accuracy: 0.535156 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 030 | Total loss: 1.048 | Reg loss: 0.048 | Tree loss: 1.048 | Accuracy: 0.556641 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 024 / 030 | Total loss: 1.019 | Reg loss: 0.049 | Tree loss: 1.019 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 030 | Total loss: 1.017 | Reg loss: 0.049 | Tree loss: 1.017 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 030 | Total loss: 1.012 | Reg loss: 0.049 | Tree loss: 1.012 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 030 | Total loss: 0.967 | Reg loss: 0.049 | Tree loss: 0.967 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 030 | Total loss: 0.970 | Reg loss: 0.049 | Tree loss: 0.970 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 90 | Batch: 029 / 030 | Total loss: 0.966 | Reg loss: 0.049 | Tree loss: 0.966 | Accuracy: 0.571429 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 030 | Total loss: 1.664 | Reg loss: 0.046 | Tree loss: 1.664 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 030 | Total loss: 1.660 | Reg loss: 0.046 | Tree loss: 1.660 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 030 | Total loss: 1.678 | Reg loss: 0.046 | Tree loss: 1.678 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 030 | Total loss: 1.598 | Reg loss: 0.046 | Tree loss: 1.598 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 030 | Total loss: 1.556 | Reg loss: 0.046 | Tree loss: 1.556 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 030 | Total loss: 1.581 | Reg loss: 0.047 | Tree loss: 1.581 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 030 | Total loss: 1.515 | Reg loss: 0.047 | Tree loss: 1.515 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 030 | Total loss: 1.464 | Reg loss: 0.047 | Tree loss: 1.464 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 030 | Total loss: 1.376 | Reg loss: 0.047 | Tree loss: 1.376 | Accuracy: 0.605469 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 030 | Total loss: 1.365 | Reg loss: 0.047 | Tree loss: 1.365 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 030 | Total loss: 1.313 | Reg loss: 0.047 | Tree loss: 1.313 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 030 | Total loss: 1.318 | Reg loss: 0.047 | Tree loss: 1.318 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 030 | Total loss: 1.288 | Reg loss: 0.047 | Tree loss: 1.288 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 030 | Total loss: 1.233 | Reg loss: 0.047 | Tree loss: 1.233 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 030 | Total loss: 1.200 | Reg loss: 0.047 | Tree loss: 1.200 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 030 | Total loss: 1.196 | Reg loss: 0.047 | Tree loss: 1.196 | Accuracy: 0.589844 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 030 | Total loss: 1.169 | Reg loss: 0.048 | Tree loss: 1.169 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 030 | Total loss: 1.135 | Reg loss: 0.048 | Tree loss: 1.135 | Accuracy: 0.587891 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 030 | Total loss: 1.117 | Reg loss: 0.048 | Tree loss: 1.117 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 030 | Total loss: 1.101 | Reg loss: 0.048 | Tree loss: 1.101 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 030 | Total loss: 1.080 | Reg loss: 0.048 | Tree loss: 1.080 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 030 | Total loss: 1.027 | Reg loss: 0.048 | Tree loss: 1.027 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 030 | Total loss: 1.024 | Reg loss: 0.048 | Tree loss: 1.024 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 030 | Total loss: 1.049 | Reg loss: 0.048 | Tree loss: 1.049 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 030 | Total loss: 1.031 | Reg loss: 0.048 | Tree loss: 1.031 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 030 | Total loss: 1.031 | Reg loss: 0.049 | Tree loss: 1.031 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 030 | Total loss: 1.012 | Reg loss: 0.049 | Tree loss: 1.012 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 030 | Total loss: 0.974 | Reg loss: 0.049 | Tree loss: 0.974 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 030 | Total loss: 0.943 | Reg loss: 0.049 | Tree loss: 0.943 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 91 | Batch: 029 / 030 | Total loss: 1.028 | Reg loss: 0.049 | Tree loss: 1.028 | Accuracy: 0.542857 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 030 | Total loss: 1.647 | Reg loss: 0.046 | Tree loss: 1.647 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 030 | Total loss: 1.651 | Reg loss: 0.046 | Tree loss: 1.651 | Accuracy: 0.546875 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 030 | Total loss: 1.599 | Reg loss: 0.046 | Tree loss: 1.599 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 030 | Total loss: 1.611 | Reg loss: 0.046 | Tree loss: 1.611 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 030 | Total loss: 1.512 | Reg loss: 0.046 | Tree loss: 1.512 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 030 | Total loss: 1.504 | Reg loss: 0.046 | Tree loss: 1.504 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 030 | Total loss: 1.499 | Reg loss: 0.046 | Tree loss: 1.499 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 030 | Total loss: 1.468 | Reg loss: 0.047 | Tree loss: 1.468 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 030 | Total loss: 1.348 | Reg loss: 0.047 | Tree loss: 1.348 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 030 | Total loss: 1.408 | Reg loss: 0.047 | Tree loss: 1.408 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 030 | Total loss: 1.348 | Reg loss: 0.047 | Tree loss: 1.348 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 030 | Total loss: 1.350 | Reg loss: 0.047 | Tree loss: 1.350 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 030 | Total loss: 1.295 | Reg loss: 0.047 | Tree loss: 1.295 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 030 | Total loss: 1.260 | Reg loss: 0.047 | Tree loss: 1.260 | Accuracy: 0.599609 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 030 | Total loss: 1.232 | Reg loss: 0.047 | Tree loss: 1.232 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 030 | Total loss: 1.196 | Reg loss: 0.047 | Tree loss: 1.196 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 030 | Total loss: 1.160 | Reg loss: 0.047 | Tree loss: 1.160 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 030 | Total loss: 1.163 | Reg loss: 0.048 | Tree loss: 1.163 | Accuracy: 0.548828 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 030 | Total loss: 1.110 | Reg loss: 0.048 | Tree loss: 1.110 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 030 | Total loss: 1.094 | Reg loss: 0.048 | Tree loss: 1.094 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 030 | Total loss: 1.069 | Reg loss: 0.048 | Tree loss: 1.069 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 030 | Total loss: 1.055 | Reg loss: 0.048 | Tree loss: 1.055 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 030 | Total loss: 1.045 | Reg loss: 0.048 | Tree loss: 1.045 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 030 | Total loss: 1.018 | Reg loss: 0.048 | Tree loss: 1.018 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 030 | Total loss: 1.028 | Reg loss: 0.048 | Tree loss: 1.028 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 030 | Total loss: 1.025 | Reg loss: 0.048 | Tree loss: 1.025 | Accuracy: 0.535156 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 026 / 030 | Total loss: 0.975 | Reg loss: 0.049 | Tree loss: 0.975 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 030 | Total loss: 0.984 | Reg loss: 0.049 | Tree loss: 0.984 | Accuracy: 0.568359 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 030 | Total loss: 0.953 | Reg loss: 0.049 | Tree loss: 0.953 | Accuracy: 0.607422 | 0.907 sec/iter\n",
      "Epoch: 92 | Batch: 029 / 030 | Total loss: 0.929 | Reg loss: 0.049 | Tree loss: 0.929 | Accuracy: 0.580952 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 030 | Total loss: 1.690 | Reg loss: 0.046 | Tree loss: 1.690 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 030 | Total loss: 1.675 | Reg loss: 0.046 | Tree loss: 1.675 | Accuracy: 0.578125 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 030 | Total loss: 1.568 | Reg loss: 0.046 | Tree loss: 1.568 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 030 | Total loss: 1.582 | Reg loss: 0.046 | Tree loss: 1.582 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 030 | Total loss: 1.545 | Reg loss: 0.046 | Tree loss: 1.545 | Accuracy: 0.609375 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 030 | Total loss: 1.509 | Reg loss: 0.046 | Tree loss: 1.509 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 030 | Total loss: 1.502 | Reg loss: 0.046 | Tree loss: 1.502 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 030 | Total loss: 1.460 | Reg loss: 0.046 | Tree loss: 1.460 | Accuracy: 0.554688 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 030 | Total loss: 1.385 | Reg loss: 0.047 | Tree loss: 1.385 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 030 | Total loss: 1.360 | Reg loss: 0.047 | Tree loss: 1.360 | Accuracy: 0.558594 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 030 | Total loss: 1.331 | Reg loss: 0.047 | Tree loss: 1.331 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 030 | Total loss: 1.338 | Reg loss: 0.047 | Tree loss: 1.338 | Accuracy: 0.537109 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 030 | Total loss: 1.280 | Reg loss: 0.047 | Tree loss: 1.280 | Accuracy: 0.619141 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 030 | Total loss: 1.279 | Reg loss: 0.047 | Tree loss: 1.279 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 030 | Total loss: 1.211 | Reg loss: 0.047 | Tree loss: 1.211 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 030 | Total loss: 1.213 | Reg loss: 0.047 | Tree loss: 1.213 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 030 | Total loss: 1.150 | Reg loss: 0.047 | Tree loss: 1.150 | Accuracy: 0.621094 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 030 | Total loss: 1.145 | Reg loss: 0.047 | Tree loss: 1.145 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 030 | Total loss: 1.066 | Reg loss: 0.048 | Tree loss: 1.066 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 030 | Total loss: 1.078 | Reg loss: 0.048 | Tree loss: 1.078 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 030 | Total loss: 1.087 | Reg loss: 0.048 | Tree loss: 1.087 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 030 | Total loss: 1.053 | Reg loss: 0.048 | Tree loss: 1.053 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 030 | Total loss: 1.056 | Reg loss: 0.048 | Tree loss: 1.056 | Accuracy: 0.566406 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 030 | Total loss: 1.018 | Reg loss: 0.048 | Tree loss: 1.018 | Accuracy: 0.593750 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 030 | Total loss: 1.035 | Reg loss: 0.048 | Tree loss: 1.035 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 030 | Total loss: 1.000 | Reg loss: 0.048 | Tree loss: 1.000 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 030 | Total loss: 0.984 | Reg loss: 0.048 | Tree loss: 0.984 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 030 | Total loss: 1.011 | Reg loss: 0.049 | Tree loss: 1.011 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 030 | Total loss: 0.961 | Reg loss: 0.049 | Tree loss: 0.961 | Accuracy: 0.595703 | 0.907 sec/iter\n",
      "Epoch: 93 | Batch: 029 / 030 | Total loss: 0.943 | Reg loss: 0.049 | Tree loss: 0.943 | Accuracy: 0.552381 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 030 | Total loss: 1.697 | Reg loss: 0.046 | Tree loss: 1.697 | Accuracy: 0.533203 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 030 | Total loss: 1.649 | Reg loss: 0.046 | Tree loss: 1.649 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 030 | Total loss: 1.619 | Reg loss: 0.046 | Tree loss: 1.619 | Accuracy: 0.623047 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 030 | Total loss: 1.590 | Reg loss: 0.046 | Tree loss: 1.590 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 030 | Total loss: 1.540 | Reg loss: 0.046 | Tree loss: 1.540 | Accuracy: 0.611328 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 030 | Total loss: 1.491 | Reg loss: 0.046 | Tree loss: 1.491 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 030 | Total loss: 1.495 | Reg loss: 0.046 | Tree loss: 1.495 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 030 | Total loss: 1.423 | Reg loss: 0.046 | Tree loss: 1.423 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 030 | Total loss: 1.402 | Reg loss: 0.046 | Tree loss: 1.402 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 030 | Total loss: 1.341 | Reg loss: 0.047 | Tree loss: 1.341 | Accuracy: 0.597656 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 030 | Total loss: 1.325 | Reg loss: 0.047 | Tree loss: 1.325 | Accuracy: 0.603516 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 030 | Total loss: 1.315 | Reg loss: 0.047 | Tree loss: 1.315 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 030 | Total loss: 1.265 | Reg loss: 0.047 | Tree loss: 1.265 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 030 | Total loss: 1.227 | Reg loss: 0.047 | Tree loss: 1.227 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 030 | Total loss: 1.253 | Reg loss: 0.047 | Tree loss: 1.253 | Accuracy: 0.576172 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 030 | Total loss: 1.185 | Reg loss: 0.047 | Tree loss: 1.185 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 030 | Total loss: 1.187 | Reg loss: 0.047 | Tree loss: 1.187 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 030 | Total loss: 1.132 | Reg loss: 0.047 | Tree loss: 1.132 | Accuracy: 0.564453 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 030 | Total loss: 1.122 | Reg loss: 0.048 | Tree loss: 1.122 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 030 | Total loss: 1.098 | Reg loss: 0.048 | Tree loss: 1.098 | Accuracy: 0.550781 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 030 | Total loss: 1.094 | Reg loss: 0.048 | Tree loss: 1.094 | Accuracy: 0.601562 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 030 | Total loss: 1.046 | Reg loss: 0.048 | Tree loss: 1.046 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 030 | Total loss: 1.024 | Reg loss: 0.048 | Tree loss: 1.024 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 030 | Total loss: 1.021 | Reg loss: 0.048 | Tree loss: 1.021 | Accuracy: 0.560547 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 030 | Total loss: 1.004 | Reg loss: 0.048 | Tree loss: 1.004 | Accuracy: 0.552734 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 030 | Total loss: 0.991 | Reg loss: 0.048 | Tree loss: 0.991 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 030 | Total loss: 1.005 | Reg loss: 0.048 | Tree loss: 1.005 | Accuracy: 0.542969 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 030 | Total loss: 0.981 | Reg loss: 0.048 | Tree loss: 0.981 | Accuracy: 0.568359 | 0.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 028 / 030 | Total loss: 0.965 | Reg loss: 0.049 | Tree loss: 0.965 | Accuracy: 0.580078 | 0.907 sec/iter\n",
      "Epoch: 94 | Batch: 029 / 030 | Total loss: 0.962 | Reg loss: 0.049 | Tree loss: 0.962 | Accuracy: 0.561905 | 0.907 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 030 | Total loss: 1.688 | Reg loss: 0.046 | Tree loss: 1.688 | Accuracy: 0.583984 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 030 | Total loss: 1.598 | Reg loss: 0.046 | Tree loss: 1.598 | Accuracy: 0.572266 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 030 | Total loss: 1.625 | Reg loss: 0.046 | Tree loss: 1.625 | Accuracy: 0.562500 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 030 | Total loss: 1.628 | Reg loss: 0.046 | Tree loss: 1.628 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 030 | Total loss: 1.506 | Reg loss: 0.046 | Tree loss: 1.506 | Accuracy: 0.591797 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 030 | Total loss: 1.533 | Reg loss: 0.046 | Tree loss: 1.533 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 030 | Total loss: 1.514 | Reg loss: 0.046 | Tree loss: 1.514 | Accuracy: 0.574219 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 030 | Total loss: 1.421 | Reg loss: 0.046 | Tree loss: 1.421 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 030 | Total loss: 1.386 | Reg loss: 0.046 | Tree loss: 1.386 | Accuracy: 0.585938 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 030 | Total loss: 1.398 | Reg loss: 0.046 | Tree loss: 1.398 | Accuracy: 0.582031 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 030 | Total loss: 1.324 | Reg loss: 0.047 | Tree loss: 1.324 | Accuracy: 0.541016 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 030 | Total loss: 1.318 | Reg loss: 0.047 | Tree loss: 1.318 | Accuracy: 0.570312 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 030 | Total loss: 1.279 | Reg loss: 0.047 | Tree loss: 1.279 | Accuracy: 0.556641 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 030 | Total loss: 1.258 | Reg loss: 0.047 | Tree loss: 1.258 | Accuracy: 0.544922 | 0.907 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 030 | Total loss: 1.236 | Reg loss: 0.047 | Tree loss: 1.236 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 030 | Total loss: 1.181 | Reg loss: 0.047 | Tree loss: 1.181 | Accuracy: 0.527344 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 030 | Total loss: 1.187 | Reg loss: 0.047 | Tree loss: 1.187 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 030 | Total loss: 1.095 | Reg loss: 0.047 | Tree loss: 1.095 | Accuracy: 0.611328 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 030 | Total loss: 1.147 | Reg loss: 0.047 | Tree loss: 1.147 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 030 | Total loss: 1.074 | Reg loss: 0.048 | Tree loss: 1.074 | Accuracy: 0.550781 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 030 | Total loss: 1.032 | Reg loss: 0.048 | Tree loss: 1.032 | Accuracy: 0.613281 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 030 | Total loss: 1.058 | Reg loss: 0.048 | Tree loss: 1.058 | Accuracy: 0.572266 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 030 | Total loss: 1.065 | Reg loss: 0.048 | Tree loss: 1.065 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 030 | Total loss: 1.004 | Reg loss: 0.048 | Tree loss: 1.004 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 030 | Total loss: 1.000 | Reg loss: 0.048 | Tree loss: 1.000 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 030 | Total loss: 0.988 | Reg loss: 0.048 | Tree loss: 0.988 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 030 | Total loss: 1.002 | Reg loss: 0.048 | Tree loss: 1.002 | Accuracy: 0.566406 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 030 | Total loss: 0.951 | Reg loss: 0.048 | Tree loss: 0.951 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 030 | Total loss: 0.946 | Reg loss: 0.048 | Tree loss: 0.946 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 95 | Batch: 029 / 030 | Total loss: 0.975 | Reg loss: 0.049 | Tree loss: 0.975 | Accuracy: 0.580952 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 030 | Total loss: 1.688 | Reg loss: 0.046 | Tree loss: 1.688 | Accuracy: 0.515625 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 030 | Total loss: 1.657 | Reg loss: 0.046 | Tree loss: 1.657 | Accuracy: 0.589844 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 030 | Total loss: 1.633 | Reg loss: 0.046 | Tree loss: 1.633 | Accuracy: 0.576172 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 030 | Total loss: 1.630 | Reg loss: 0.046 | Tree loss: 1.630 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 030 | Total loss: 1.517 | Reg loss: 0.046 | Tree loss: 1.517 | Accuracy: 0.617188 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 030 | Total loss: 1.519 | Reg loss: 0.046 | Tree loss: 1.519 | Accuracy: 0.525391 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 030 | Total loss: 1.486 | Reg loss: 0.046 | Tree loss: 1.486 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 030 | Total loss: 1.397 | Reg loss: 0.046 | Tree loss: 1.397 | Accuracy: 0.568359 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 030 | Total loss: 1.371 | Reg loss: 0.046 | Tree loss: 1.371 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 030 | Total loss: 1.350 | Reg loss: 0.046 | Tree loss: 1.350 | Accuracy: 0.591797 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 030 | Total loss: 1.308 | Reg loss: 0.047 | Tree loss: 1.308 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 030 | Total loss: 1.301 | Reg loss: 0.047 | Tree loss: 1.301 | Accuracy: 0.595703 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 030 | Total loss: 1.286 | Reg loss: 0.047 | Tree loss: 1.286 | Accuracy: 0.593750 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 030 | Total loss: 1.232 | Reg loss: 0.047 | Tree loss: 1.232 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 030 | Total loss: 1.218 | Reg loss: 0.047 | Tree loss: 1.218 | Accuracy: 0.556641 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 030 | Total loss: 1.185 | Reg loss: 0.047 | Tree loss: 1.185 | Accuracy: 0.566406 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 030 | Total loss: 1.182 | Reg loss: 0.047 | Tree loss: 1.182 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 030 | Total loss: 1.128 | Reg loss: 0.047 | Tree loss: 1.128 | Accuracy: 0.589844 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 030 | Total loss: 1.110 | Reg loss: 0.047 | Tree loss: 1.110 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 030 | Total loss: 1.093 | Reg loss: 0.048 | Tree loss: 1.093 | Accuracy: 0.576172 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 030 | Total loss: 1.090 | Reg loss: 0.048 | Tree loss: 1.090 | Accuracy: 0.539062 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 030 | Total loss: 1.023 | Reg loss: 0.048 | Tree loss: 1.023 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 030 | Total loss: 1.030 | Reg loss: 0.048 | Tree loss: 1.030 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 030 | Total loss: 0.995 | Reg loss: 0.048 | Tree loss: 0.995 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 030 | Total loss: 1.015 | Reg loss: 0.048 | Tree loss: 1.015 | Accuracy: 0.568359 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 030 | Total loss: 0.999 | Reg loss: 0.048 | Tree loss: 0.999 | Accuracy: 0.589844 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 030 | Total loss: 0.991 | Reg loss: 0.048 | Tree loss: 0.991 | Accuracy: 0.548828 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 030 | Total loss: 0.968 | Reg loss: 0.048 | Tree loss: 0.968 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 030 | Total loss: 0.947 | Reg loss: 0.048 | Tree loss: 0.947 | Accuracy: 0.585938 | 0.906 sec/iter\n",
      "Epoch: 96 | Batch: 029 / 030 | Total loss: 0.971 | Reg loss: 0.048 | Tree loss: 0.971 | Accuracy: 0.542857 | 0.906 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 030 | Total loss: 1.613 | Reg loss: 0.046 | Tree loss: 1.613 | Accuracy: 0.554688 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 030 | Total loss: 1.663 | Reg loss: 0.046 | Tree loss: 1.663 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 030 | Total loss: 1.588 | Reg loss: 0.046 | Tree loss: 1.588 | Accuracy: 0.566406 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 030 | Total loss: 1.571 | Reg loss: 0.046 | Tree loss: 1.571 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 030 | Total loss: 1.510 | Reg loss: 0.046 | Tree loss: 1.510 | Accuracy: 0.572266 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 030 | Total loss: 1.469 | Reg loss: 0.046 | Tree loss: 1.469 | Accuracy: 0.583984 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 030 | Total loss: 1.446 | Reg loss: 0.046 | Tree loss: 1.446 | Accuracy: 0.603516 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 030 | Total loss: 1.427 | Reg loss: 0.046 | Tree loss: 1.427 | Accuracy: 0.593750 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 030 | Total loss: 1.398 | Reg loss: 0.046 | Tree loss: 1.398 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 030 | Total loss: 1.344 | Reg loss: 0.046 | Tree loss: 1.344 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 030 | Total loss: 1.389 | Reg loss: 0.046 | Tree loss: 1.389 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 030 | Total loss: 1.274 | Reg loss: 0.047 | Tree loss: 1.274 | Accuracy: 0.597656 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 030 | Total loss: 1.247 | Reg loss: 0.047 | Tree loss: 1.247 | Accuracy: 0.599609 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 030 | Total loss: 1.235 | Reg loss: 0.047 | Tree loss: 1.235 | Accuracy: 0.548828 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 030 | Total loss: 1.269 | Reg loss: 0.047 | Tree loss: 1.269 | Accuracy: 0.539062 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 030 | Total loss: 1.210 | Reg loss: 0.047 | Tree loss: 1.210 | Accuracy: 0.531250 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 030 | Total loss: 1.166 | Reg loss: 0.047 | Tree loss: 1.166 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 030 | Total loss: 1.154 | Reg loss: 0.047 | Tree loss: 1.154 | Accuracy: 0.597656 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 030 | Total loss: 1.102 | Reg loss: 0.047 | Tree loss: 1.102 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 030 | Total loss: 1.085 | Reg loss: 0.047 | Tree loss: 1.085 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 030 | Total loss: 1.105 | Reg loss: 0.048 | Tree loss: 1.105 | Accuracy: 0.554688 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 030 | Total loss: 1.068 | Reg loss: 0.048 | Tree loss: 1.068 | Accuracy: 0.564453 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 030 | Total loss: 1.028 | Reg loss: 0.048 | Tree loss: 1.028 | Accuracy: 0.583984 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 030 | Total loss: 1.012 | Reg loss: 0.048 | Tree loss: 1.012 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 030 | Total loss: 1.013 | Reg loss: 0.048 | Tree loss: 1.013 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 030 | Total loss: 1.000 | Reg loss: 0.048 | Tree loss: 1.000 | Accuracy: 0.558594 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 030 | Total loss: 0.977 | Reg loss: 0.048 | Tree loss: 0.977 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 030 | Total loss: 0.959 | Reg loss: 0.048 | Tree loss: 0.959 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 030 | Total loss: 0.962 | Reg loss: 0.048 | Tree loss: 0.962 | Accuracy: 0.576172 | 0.906 sec/iter\n",
      "Epoch: 97 | Batch: 029 / 030 | Total loss: 0.949 | Reg loss: 0.048 | Tree loss: 0.949 | Accuracy: 0.590476 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 030 | Total loss: 1.674 | Reg loss: 0.046 | Tree loss: 1.674 | Accuracy: 0.595703 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 030 | Total loss: 1.689 | Reg loss: 0.046 | Tree loss: 1.689 | Accuracy: 0.548828 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 030 | Total loss: 1.578 | Reg loss: 0.046 | Tree loss: 1.578 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 030 | Total loss: 1.572 | Reg loss: 0.046 | Tree loss: 1.572 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 030 | Total loss: 1.567 | Reg loss: 0.046 | Tree loss: 1.567 | Accuracy: 0.550781 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 030 | Total loss: 1.532 | Reg loss: 0.046 | Tree loss: 1.532 | Accuracy: 0.539062 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 030 | Total loss: 1.501 | Reg loss: 0.046 | Tree loss: 1.501 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 030 | Total loss: 1.411 | Reg loss: 0.046 | Tree loss: 1.411 | Accuracy: 0.572266 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 030 | Total loss: 1.406 | Reg loss: 0.046 | Tree loss: 1.406 | Accuracy: 0.560547 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 030 | Total loss: 1.344 | Reg loss: 0.046 | Tree loss: 1.344 | Accuracy: 0.558594 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 030 | Total loss: 1.303 | Reg loss: 0.046 | Tree loss: 1.303 | Accuracy: 0.601562 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 030 | Total loss: 1.286 | Reg loss: 0.047 | Tree loss: 1.286 | Accuracy: 0.583984 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 030 | Total loss: 1.236 | Reg loss: 0.047 | Tree loss: 1.236 | Accuracy: 0.597656 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 030 | Total loss: 1.228 | Reg loss: 0.047 | Tree loss: 1.228 | Accuracy: 0.601562 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 030 | Total loss: 1.153 | Reg loss: 0.047 | Tree loss: 1.153 | Accuracy: 0.580078 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 030 | Total loss: 1.161 | Reg loss: 0.047 | Tree loss: 1.161 | Accuracy: 0.583984 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 030 | Total loss: 1.157 | Reg loss: 0.047 | Tree loss: 1.157 | Accuracy: 0.597656 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 030 | Total loss: 1.114 | Reg loss: 0.047 | Tree loss: 1.114 | Accuracy: 0.587891 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 030 | Total loss: 1.098 | Reg loss: 0.047 | Tree loss: 1.098 | Accuracy: 0.572266 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 030 | Total loss: 1.104 | Reg loss: 0.047 | Tree loss: 1.104 | Accuracy: 0.591797 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 030 | Total loss: 1.060 | Reg loss: 0.048 | Tree loss: 1.060 | Accuracy: 0.558594 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 030 | Total loss: 1.038 | Reg loss: 0.048 | Tree loss: 1.038 | Accuracy: 0.564453 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 030 | Total loss: 1.003 | Reg loss: 0.048 | Tree loss: 1.003 | Accuracy: 0.595703 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 030 | Total loss: 1.011 | Reg loss: 0.048 | Tree loss: 1.011 | Accuracy: 0.570312 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 030 | Total loss: 1.008 | Reg loss: 0.048 | Tree loss: 1.008 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 030 | Total loss: 1.009 | Reg loss: 0.048 | Tree loss: 1.009 | Accuracy: 0.566406 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 030 | Total loss: 0.983 | Reg loss: 0.048 | Tree loss: 0.983 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 030 | Total loss: 0.965 | Reg loss: 0.048 | Tree loss: 0.965 | Accuracy: 0.560547 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 030 | Total loss: 0.980 | Reg loss: 0.048 | Tree loss: 0.980 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 98 | Batch: 029 / 030 | Total loss: 0.940 | Reg loss: 0.048 | Tree loss: 0.940 | Accuracy: 0.571429 | 0.906 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 000 / 030 | Total loss: 1.669 | Reg loss: 0.046 | Tree loss: 1.669 | Accuracy: 0.583984 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 030 | Total loss: 1.619 | Reg loss: 0.046 | Tree loss: 1.619 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 030 | Total loss: 1.657 | Reg loss: 0.046 | Tree loss: 1.657 | Accuracy: 0.576172 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 030 | Total loss: 1.579 | Reg loss: 0.046 | Tree loss: 1.579 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 030 | Total loss: 1.505 | Reg loss: 0.046 | Tree loss: 1.505 | Accuracy: 0.568359 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 030 | Total loss: 1.545 | Reg loss: 0.046 | Tree loss: 1.545 | Accuracy: 0.562500 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 030 | Total loss: 1.448 | Reg loss: 0.046 | Tree loss: 1.448 | Accuracy: 0.601562 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 030 | Total loss: 1.429 | Reg loss: 0.046 | Tree loss: 1.429 | Accuracy: 0.578125 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 030 | Total loss: 1.431 | Reg loss: 0.046 | Tree loss: 1.431 | Accuracy: 0.552734 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 030 | Total loss: 1.413 | Reg loss: 0.046 | Tree loss: 1.413 | Accuracy: 0.572266 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 030 | Total loss: 1.322 | Reg loss: 0.046 | Tree loss: 1.322 | Accuracy: 0.544922 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 030 | Total loss: 1.339 | Reg loss: 0.046 | Tree loss: 1.339 | Accuracy: 0.548828 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 030 | Total loss: 1.240 | Reg loss: 0.047 | Tree loss: 1.240 | Accuracy: 0.554688 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 030 | Total loss: 1.237 | Reg loss: 0.047 | Tree loss: 1.237 | Accuracy: 0.574219 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 030 | Total loss: 1.219 | Reg loss: 0.047 | Tree loss: 1.219 | Accuracy: 0.613281 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 030 | Total loss: 1.196 | Reg loss: 0.047 | Tree loss: 1.196 | Accuracy: 0.568359 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 030 | Total loss: 1.123 | Reg loss: 0.047 | Tree loss: 1.123 | Accuracy: 0.556641 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 030 | Total loss: 1.103 | Reg loss: 0.047 | Tree loss: 1.103 | Accuracy: 0.582031 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 030 | Total loss: 1.071 | Reg loss: 0.047 | Tree loss: 1.071 | Accuracy: 0.615234 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 030 | Total loss: 1.094 | Reg loss: 0.047 | Tree loss: 1.094 | Accuracy: 0.554688 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 030 | Total loss: 1.002 | Reg loss: 0.047 | Tree loss: 1.002 | Accuracy: 0.605469 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 030 | Total loss: 1.007 | Reg loss: 0.048 | Tree loss: 1.007 | Accuracy: 0.609375 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 030 | Total loss: 1.041 | Reg loss: 0.048 | Tree loss: 1.041 | Accuracy: 0.539062 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 030 | Total loss: 0.992 | Reg loss: 0.048 | Tree loss: 0.992 | Accuracy: 0.603516 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 030 | Total loss: 1.013 | Reg loss: 0.048 | Tree loss: 1.013 | Accuracy: 0.568359 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 030 | Total loss: 0.976 | Reg loss: 0.048 | Tree loss: 0.976 | Accuracy: 0.564453 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 030 | Total loss: 0.987 | Reg loss: 0.048 | Tree loss: 0.987 | Accuracy: 0.535156 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 030 | Total loss: 0.969 | Reg loss: 0.048 | Tree loss: 0.969 | Accuracy: 0.609375 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 030 | Total loss: 0.942 | Reg loss: 0.048 | Tree loss: 0.942 | Accuracy: 0.554688 | 0.906 sec/iter\n",
      "Epoch: 99 | Batch: 029 / 030 | Total loss: 0.943 | Reg loss: 0.048 | Tree loss: 0.943 | Accuracy: 0.571429 | 0.906 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb97bd4a40e84fd1a6be326a75089c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b1d918b9254db084d13f09e180bbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc2defd74ca4ed4855144e378f1dd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aa395b5acd4455b1af798ffd8f4d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 9.191358024691358\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 324\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9468\n",
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "3127\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "1188\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "103\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "120\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "947\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "Average comprehensibility: 48.18518518518518\n",
      "std comprehensibility: 6.052468348851946\n",
      "var comprehensibility: 36.6323731138546\n",
      "minimum comprehensibility: 24\n",
      "maximum comprehensibility: 58\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
