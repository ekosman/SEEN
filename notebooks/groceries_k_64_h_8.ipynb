{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "tree_depth = 8\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.174827575683594 | KNN Loss: 6.228297710418701 | BCE Loss: 1.9465301036834717\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.177190780639648 | KNN Loss: 6.22810173034668 | BCE Loss: 1.9490892887115479\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.218077659606934 | KNN Loss: 6.228068828582764 | BCE Loss: 1.99000883102417\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.136442184448242 | KNN Loss: 6.2280378341674805 | BCE Loss: 1.90840482711792\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.170071601867676 | KNN Loss: 6.2273850440979 | BCE Loss: 1.9426862001419067\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.154052734375 | KNN Loss: 6.227167129516602 | BCE Loss: 1.9268858432769775\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.193419456481934 | KNN Loss: 6.226790428161621 | BCE Loss: 1.9666287899017334\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.15809440612793 | KNN Loss: 6.226618766784668 | BCE Loss: 1.93147611618042\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.144584655761719 | KNN Loss: 6.226370811462402 | BCE Loss: 1.9182143211364746\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.11990737915039 | KNN Loss: 6.226385593414307 | BCE Loss: 1.893521785736084\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.128690719604492 | KNN Loss: 6.225912094116211 | BCE Loss: 1.9027786254882812\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.116644859313965 | KNN Loss: 6.225866317749023 | BCE Loss: 1.8907785415649414\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.06295394897461 | KNN Loss: 6.224963665008545 | BCE Loss: 1.8379898071289062\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.071043968200684 | KNN Loss: 6.224791526794434 | BCE Loss: 1.846252202987671\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.072734832763672 | KNN Loss: 6.224795341491699 | BCE Loss: 1.8479398488998413\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.073017120361328 | KNN Loss: 6.224441051483154 | BCE Loss: 1.8485758304595947\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.027189254760742 | KNN Loss: 6.224087238311768 | BCE Loss: 1.8031023740768433\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.085235595703125 | KNN Loss: 6.222684383392334 | BCE Loss: 1.8625514507293701\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.040090560913086 | KNN Loss: 6.2225728034973145 | BCE Loss: 1.8175181150436401\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.02126407623291 | KNN Loss: 6.222300052642822 | BCE Loss: 1.7989636659622192\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.010437965393066 | KNN Loss: 6.22132682800293 | BCE Loss: 1.7891110181808472\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.037164688110352 | KNN Loss: 6.22023868560791 | BCE Loss: 1.816926121711731\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.03837776184082 | KNN Loss: 6.220053672790527 | BCE Loss: 1.8183236122131348\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.968201637268066 | KNN Loss: 6.218094348907471 | BCE Loss: 1.7501075267791748\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.949274063110352 | KNN Loss: 6.218448638916016 | BCE Loss: 1.7308251857757568\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 8.036657333374023 | KNN Loss: 6.216679096221924 | BCE Loss: 1.819977879524231\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.961927890777588 | KNN Loss: 6.215662002563477 | BCE Loss: 1.7462658882141113\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.978688716888428 | KNN Loss: 6.212665557861328 | BCE Loss: 1.76602303981781\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.965714454650879 | KNN Loss: 6.212574005126953 | BCE Loss: 1.7531402111053467\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.980661392211914 | KNN Loss: 6.210562705993652 | BCE Loss: 1.7700984477996826\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.89418363571167 | KNN Loss: 6.208312034606934 | BCE Loss: 1.6858714818954468\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.940404415130615 | KNN Loss: 6.20561408996582 | BCE Loss: 1.7347904443740845\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.937551498413086 | KNN Loss: 6.20241117477417 | BCE Loss: 1.7351402044296265\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.878780364990234 | KNN Loss: 6.198817729949951 | BCE Loss: 1.6799628734588623\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.823324680328369 | KNN Loss: 6.194851875305176 | BCE Loss: 1.6284726858139038\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.791406631469727 | KNN Loss: 6.1922221183776855 | BCE Loss: 1.599184274673462\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.799546241760254 | KNN Loss: 6.191719055175781 | BCE Loss: 1.6078271865844727\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.787913799285889 | KNN Loss: 6.187680721282959 | BCE Loss: 1.6002331972122192\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.786144256591797 | KNN Loss: 6.17584753036499 | BCE Loss: 1.6102964878082275\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.750083923339844 | KNN Loss: 6.175801753997803 | BCE Loss: 1.5742824077606201\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.683643341064453 | KNN Loss: 6.166003704071045 | BCE Loss: 1.5176398754119873\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.694906711578369 | KNN Loss: 6.149523735046387 | BCE Loss: 1.5453829765319824\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.648647308349609 | KNN Loss: 6.149331569671631 | BCE Loss: 1.499315619468689\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.6268229484558105 | KNN Loss: 6.126992702484131 | BCE Loss: 1.4998303651809692\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.6401214599609375 | KNN Loss: 6.121335029602051 | BCE Loss: 1.5187865495681763\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.572418689727783 | KNN Loss: 6.099302768707275 | BCE Loss: 1.4731158018112183\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.577380180358887 | KNN Loss: 6.096850395202637 | BCE Loss: 1.480529546737671\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.486198902130127 | KNN Loss: 6.059030055999756 | BCE Loss: 1.4271687269210815\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.437618255615234 | KNN Loss: 6.041534423828125 | BCE Loss: 1.396083950996399\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.385781288146973 | KNN Loss: 6.028055667877197 | BCE Loss: 1.3577258586883545\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.344298362731934 | KNN Loss: 5.9954094886779785 | BCE Loss: 1.348888635635376\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.278655052185059 | KNN Loss: 5.949435234069824 | BCE Loss: 1.3292196989059448\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.1904826164245605 | KNN Loss: 5.919674873352051 | BCE Loss: 1.2708077430725098\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.172009468078613 | KNN Loss: 5.875847339630127 | BCE Loss: 1.2961618900299072\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.075320243835449 | KNN Loss: 5.8124823570251465 | BCE Loss: 1.2628381252288818\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 6.959005355834961 | KNN Loss: 5.7218732833862305 | BCE Loss: 1.2371320724487305\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.839597702026367 | KNN Loss: 5.663217544555664 | BCE Loss: 1.1763802766799927\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.756455421447754 | KNN Loss: 5.530677795410156 | BCE Loss: 1.2257778644561768\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.643074035644531 | KNN Loss: 5.460158824920654 | BCE Loss: 1.182915210723877\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.526225566864014 | KNN Loss: 5.347084045410156 | BCE Loss: 1.179141640663147\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.368968963623047 | KNN Loss: 5.260873794555664 | BCE Loss: 1.1080951690673828\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.333261013031006 | KNN Loss: 5.205174446105957 | BCE Loss: 1.1280865669250488\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.180516242980957 | KNN Loss: 5.047307968139648 | BCE Loss: 1.1332085132598877\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.146728992462158 | KNN Loss: 4.9934186935424805 | BCE Loss: 1.1533104181289673\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.07170295715332 | KNN Loss: 4.923910140991211 | BCE Loss: 1.1477930545806885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 5.972324371337891 | KNN Loss: 4.83497428894043 | BCE Loss: 1.1373498439788818\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.987344741821289 | KNN Loss: 4.841643810272217 | BCE Loss: 1.1457008123397827\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.96442174911499 | KNN Loss: 4.795790195465088 | BCE Loss: 1.1686314344406128\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.8169708251953125 | KNN Loss: 4.7224884033203125 | BCE Loss: 1.094482421875\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.812456130981445 | KNN Loss: 4.7007646560668945 | BCE Loss: 1.1116917133331299\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.810124397277832 | KNN Loss: 4.6933417320251465 | BCE Loss: 1.116782546043396\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.793524265289307 | KNN Loss: 4.673505783081055 | BCE Loss: 1.1200186014175415\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.723931789398193 | KNN Loss: 4.6097893714904785 | BCE Loss: 1.1141424179077148\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.665378570556641 | KNN Loss: 4.60558557510376 | BCE Loss: 1.0597927570343018\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.664296627044678 | KNN Loss: 4.580555438995361 | BCE Loss: 1.0837410688400269\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.703315258026123 | KNN Loss: 4.5832061767578125 | BCE Loss: 1.1201092004776\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.739133358001709 | KNN Loss: 4.62274694442749 | BCE Loss: 1.1163864135742188\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.696047782897949 | KNN Loss: 4.581066131591797 | BCE Loss: 1.1149816513061523\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.690338134765625 | KNN Loss: 4.565760612487793 | BCE Loss: 1.1245774030685425\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.67179012298584 | KNN Loss: 4.57805871963501 | BCE Loss: 1.09373140335083\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.638324737548828 | KNN Loss: 4.53067684173584 | BCE Loss: 1.1076480150222778\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.644401550292969 | KNN Loss: 4.544426918029785 | BCE Loss: 1.0999747514724731\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.656365394592285 | KNN Loss: 4.566087245941162 | BCE Loss: 1.0902783870697021\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 5.6695427894592285 | KNN Loss: 4.581275463104248 | BCE Loss: 1.0882673263549805\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 5.56053352355957 | KNN Loss: 4.505931377410889 | BCE Loss: 1.054602026939392\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 5.677325248718262 | KNN Loss: 4.574408054351807 | BCE Loss: 1.102916955947876\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 5.64312744140625 | KNN Loss: 4.553594589233398 | BCE Loss: 1.0895328521728516\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 5.6280670166015625 | KNN Loss: 4.532703399658203 | BCE Loss: 1.0953638553619385\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 5.570503234863281 | KNN Loss: 4.508301258087158 | BCE Loss: 1.062201976776123\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 5.595351696014404 | KNN Loss: 4.501283645629883 | BCE Loss: 1.0940680503845215\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.596832752227783 | KNN Loss: 4.5050811767578125 | BCE Loss: 1.0917515754699707\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 5.578044891357422 | KNN Loss: 4.486876964569092 | BCE Loss: 1.09116792678833\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 5.604946136474609 | KNN Loss: 4.534512996673584 | BCE Loss: 1.0704333782196045\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 5.565979957580566 | KNN Loss: 4.491527080535889 | BCE Loss: 1.0744531154632568\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 5.6209259033203125 | KNN Loss: 4.513991355895996 | BCE Loss: 1.1069343090057373\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 5.576638698577881 | KNN Loss: 4.48585844039917 | BCE Loss: 1.0907803773880005\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 5.575325012207031 | KNN Loss: 4.504374980926514 | BCE Loss: 1.0709497928619385\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 5.58103084564209 | KNN Loss: 4.495138168334961 | BCE Loss: 1.0858924388885498\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 5.588977813720703 | KNN Loss: 4.4927849769592285 | BCE Loss: 1.0961928367614746\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 5.517253875732422 | KNN Loss: 4.477812767028809 | BCE Loss: 1.0394413471221924\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 5.555951118469238 | KNN Loss: 4.467649459838867 | BCE Loss: 1.0883018970489502\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 5.529455661773682 | KNN Loss: 4.455259799957275 | BCE Loss: 1.0741958618164062\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 5.607260704040527 | KNN Loss: 4.516147136688232 | BCE Loss: 1.0911136865615845\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 5.535194396972656 | KNN Loss: 4.472337245941162 | BCE Loss: 1.0628573894500732\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 5.572329998016357 | KNN Loss: 4.48224401473999 | BCE Loss: 1.0900859832763672\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 5.504770278930664 | KNN Loss: 4.464434623718262 | BCE Loss: 1.0403354167938232\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 5.552340507507324 | KNN Loss: 4.491305351257324 | BCE Loss: 1.06103515625\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 5.547414779663086 | KNN Loss: 4.48023796081543 | BCE Loss: 1.0671770572662354\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 5.538246154785156 | KNN Loss: 4.477506637573242 | BCE Loss: 1.060739517211914\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 5.538896560668945 | KNN Loss: 4.474279403686523 | BCE Loss: 1.0646171569824219\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 5.5108184814453125 | KNN Loss: 4.466132164001465 | BCE Loss: 1.0446860790252686\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 5.587002754211426 | KNN Loss: 4.49972677230835 | BCE Loss: 1.0872761011123657\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 5.551154136657715 | KNN Loss: 4.47109317779541 | BCE Loss: 1.0800609588623047\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 5.564691543579102 | KNN Loss: 4.486955165863037 | BCE Loss: 1.0777361392974854\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 5.556615829467773 | KNN Loss: 4.494020462036133 | BCE Loss: 1.0625953674316406\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 5.560459136962891 | KNN Loss: 4.491922378540039 | BCE Loss: 1.0685365200042725\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 5.4986677169799805 | KNN Loss: 4.452057361602783 | BCE Loss: 1.0466103553771973\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 5.486800670623779 | KNN Loss: 4.457296371459961 | BCE Loss: 1.0295042991638184\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 5.518146514892578 | KNN Loss: 4.447412490844727 | BCE Loss: 1.070733904838562\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 5.5428314208984375 | KNN Loss: 4.475179195404053 | BCE Loss: 1.0676519870758057\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 5.542779922485352 | KNN Loss: 4.468116283416748 | BCE Loss: 1.0746638774871826\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 5.531678676605225 | KNN Loss: 4.4740095138549805 | BCE Loss: 1.0576690435409546\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 5.504218101501465 | KNN Loss: 4.46909761428833 | BCE Loss: 1.0351204872131348\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 5.526149749755859 | KNN Loss: 4.460022926330566 | BCE Loss: 1.0661269426345825\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 5.496248245239258 | KNN Loss: 4.44450044631958 | BCE Loss: 1.0517479181289673\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 5.523201942443848 | KNN Loss: 4.445375442504883 | BCE Loss: 1.0778264999389648\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 5.481440544128418 | KNN Loss: 4.428497791290283 | BCE Loss: 1.0529427528381348\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 5.5100297927856445 | KNN Loss: 4.452127456665039 | BCE Loss: 1.0579020977020264\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 5.529019355773926 | KNN Loss: 4.4745402336120605 | BCE Loss: 1.0544793605804443\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 5.502432346343994 | KNN Loss: 4.455521106719971 | BCE Loss: 1.0469111204147339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 5.5076117515563965 | KNN Loss: 4.457967281341553 | BCE Loss: 1.0496445894241333\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 5.538027763366699 | KNN Loss: 4.485864639282227 | BCE Loss: 1.0521628856658936\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 5.51624870300293 | KNN Loss: 4.441437244415283 | BCE Loss: 1.0748112201690674\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 5.4546966552734375 | KNN Loss: 4.411798000335693 | BCE Loss: 1.042898416519165\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 5.493555068969727 | KNN Loss: 4.437297821044922 | BCE Loss: 1.0562574863433838\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 5.537965774536133 | KNN Loss: 4.481687545776367 | BCE Loss: 1.0562784671783447\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 5.524158000946045 | KNN Loss: 4.479379177093506 | BCE Loss: 1.044778823852539\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 5.468611717224121 | KNN Loss: 4.432379722595215 | BCE Loss: 1.0362317562103271\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 5.535665512084961 | KNN Loss: 4.480636119842529 | BCE Loss: 1.0550293922424316\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 5.519570827484131 | KNN Loss: 4.458310604095459 | BCE Loss: 1.0612602233886719\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 5.5400872230529785 | KNN Loss: 4.4887590408325195 | BCE Loss: 1.0513283014297485\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 5.5040693283081055 | KNN Loss: 4.445257663726807 | BCE Loss: 1.0588114261627197\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 5.522294044494629 | KNN Loss: 4.454002857208252 | BCE Loss: 1.068291187286377\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 5.449462890625 | KNN Loss: 4.423703193664551 | BCE Loss: 1.0257599353790283\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 5.472702980041504 | KNN Loss: 4.425995349884033 | BCE Loss: 1.0467073917388916\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 5.505299091339111 | KNN Loss: 4.429941654205322 | BCE Loss: 1.0753575563430786\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 5.522946834564209 | KNN Loss: 4.466957092285156 | BCE Loss: 1.0559896230697632\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 5.449261665344238 | KNN Loss: 4.4246110916137695 | BCE Loss: 1.0246508121490479\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 5.472251892089844 | KNN Loss: 4.4245924949646 | BCE Loss: 1.0476593971252441\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 5.471401214599609 | KNN Loss: 4.426621437072754 | BCE Loss: 1.0447800159454346\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 5.467703819274902 | KNN Loss: 4.4300618171691895 | BCE Loss: 1.0376418828964233\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 5.5162787437438965 | KNN Loss: 4.451018333435059 | BCE Loss: 1.0652605295181274\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 5.533576965332031 | KNN Loss: 4.4604926109313965 | BCE Loss: 1.0730842351913452\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 5.498134613037109 | KNN Loss: 4.465039253234863 | BCE Loss: 1.0330955982208252\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 5.512751579284668 | KNN Loss: 4.456404685974121 | BCE Loss: 1.0563467741012573\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 5.456780910491943 | KNN Loss: 4.407558441162109 | BCE Loss: 1.049222469329834\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 5.44665002822876 | KNN Loss: 4.403890609741211 | BCE Loss: 1.0427592992782593\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 5.482722282409668 | KNN Loss: 4.429149627685547 | BCE Loss: 1.053572416305542\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 5.51900053024292 | KNN Loss: 4.444880962371826 | BCE Loss: 1.0741195678710938\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 5.491057395935059 | KNN Loss: 4.452402591705322 | BCE Loss: 1.0386545658111572\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 5.464036464691162 | KNN Loss: 4.425617218017578 | BCE Loss: 1.038419246673584\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 5.524179458618164 | KNN Loss: 4.470163345336914 | BCE Loss: 1.054015874862671\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 5.45188570022583 | KNN Loss: 4.425689220428467 | BCE Loss: 1.0261964797973633\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 5.543881416320801 | KNN Loss: 4.472024440765381 | BCE Loss: 1.0718567371368408\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 5.507579326629639 | KNN Loss: 4.446492671966553 | BCE Loss: 1.061086654663086\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 5.496509075164795 | KNN Loss: 4.4605793952941895 | BCE Loss: 1.0359296798706055\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 5.500249862670898 | KNN Loss: 4.436464309692383 | BCE Loss: 1.0637855529785156\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 5.530320167541504 | KNN Loss: 4.458163261413574 | BCE Loss: 1.0721569061279297\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 5.45305061340332 | KNN Loss: 4.406371116638184 | BCE Loss: 1.0466793775558472\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 5.419224739074707 | KNN Loss: 4.397833347320557 | BCE Loss: 1.0213916301727295\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 5.484401702880859 | KNN Loss: 4.441329479217529 | BCE Loss: 1.043071985244751\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 5.486757278442383 | KNN Loss: 4.431722164154053 | BCE Loss: 1.0550353527069092\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 5.523401260375977 | KNN Loss: 4.474623203277588 | BCE Loss: 1.0487778186798096\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 5.468317985534668 | KNN Loss: 4.4383087158203125 | BCE Loss: 1.0300095081329346\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 5.441267013549805 | KNN Loss: 4.401863098144531 | BCE Loss: 1.0394036769866943\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 5.465999603271484 | KNN Loss: 4.418417930603027 | BCE Loss: 1.047581672668457\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 5.461109161376953 | KNN Loss: 4.4106903076171875 | BCE Loss: 1.0504188537597656\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 5.470454692840576 | KNN Loss: 4.413811206817627 | BCE Loss: 1.0566434860229492\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 5.517258644104004 | KNN Loss: 4.459019184112549 | BCE Loss: 1.0582396984100342\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 5.473308086395264 | KNN Loss: 4.422455787658691 | BCE Loss: 1.0508522987365723\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 5.461156845092773 | KNN Loss: 4.420445919036865 | BCE Loss: 1.0407110452651978\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 5.4593915939331055 | KNN Loss: 4.4132981300354 | BCE Loss: 1.0460937023162842\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 5.487321376800537 | KNN Loss: 4.4728264808654785 | BCE Loss: 1.0144948959350586\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 5.485964775085449 | KNN Loss: 4.443329811096191 | BCE Loss: 1.0426350831985474\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 5.466733932495117 | KNN Loss: 4.409173011779785 | BCE Loss: 1.0575611591339111\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 5.45285177230835 | KNN Loss: 4.4066901206970215 | BCE Loss: 1.0461616516113281\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 5.411722183227539 | KNN Loss: 4.397708415985107 | BCE Loss: 1.0140135288238525\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 5.525363922119141 | KNN Loss: 4.451615333557129 | BCE Loss: 1.0737485885620117\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 5.4690961837768555 | KNN Loss: 4.42538595199585 | BCE Loss: 1.0437102317810059\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 5.517884731292725 | KNN Loss: 4.448652267456055 | BCE Loss: 1.06923246383667\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 5.446839809417725 | KNN Loss: 4.414768695831299 | BCE Loss: 1.0320711135864258\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 5.462420463562012 | KNN Loss: 4.4123921394348145 | BCE Loss: 1.0500283241271973\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 5.436237335205078 | KNN Loss: 4.384895324707031 | BCE Loss: 1.051342248916626\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 5.426183223724365 | KNN Loss: 4.389487266540527 | BCE Loss: 1.036695957183838\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 5.451810836791992 | KNN Loss: 4.413931846618652 | BCE Loss: 1.0378788709640503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 5.4424638748168945 | KNN Loss: 4.4166083335876465 | BCE Loss: 1.0258557796478271\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 5.4416279792785645 | KNN Loss: 4.402205467224121 | BCE Loss: 1.0394225120544434\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 5.496085166931152 | KNN Loss: 4.437416076660156 | BCE Loss: 1.058669090270996\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 5.45173454284668 | KNN Loss: 4.408928394317627 | BCE Loss: 1.0428060293197632\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 5.485454082489014 | KNN Loss: 4.417438983917236 | BCE Loss: 1.068015217781067\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 5.485142230987549 | KNN Loss: 4.42959451675415 | BCE Loss: 1.055547833442688\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 5.494961738586426 | KNN Loss: 4.429432392120361 | BCE Loss: 1.0655295848846436\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 5.482602119445801 | KNN Loss: 4.441196918487549 | BCE Loss: 1.041405439376831\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 5.480774402618408 | KNN Loss: 4.409117698669434 | BCE Loss: 1.0716568231582642\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 5.437067985534668 | KNN Loss: 4.415114879608154 | BCE Loss: 1.0219531059265137\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 5.459201812744141 | KNN Loss: 4.423744201660156 | BCE Loss: 1.035457730293274\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 5.425448417663574 | KNN Loss: 4.39536190032959 | BCE Loss: 1.0300863981246948\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 5.454042434692383 | KNN Loss: 4.395679473876953 | BCE Loss: 1.0583629608154297\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 5.453859329223633 | KNN Loss: 4.427119255065918 | BCE Loss: 1.0267398357391357\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 5.475669860839844 | KNN Loss: 4.422660827636719 | BCE Loss: 1.053009033203125\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 5.477522373199463 | KNN Loss: 4.432664394378662 | BCE Loss: 1.0448580980300903\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 5.454720497131348 | KNN Loss: 4.427571773529053 | BCE Loss: 1.027148723602295\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 5.47927188873291 | KNN Loss: 4.423155784606934 | BCE Loss: 1.0561163425445557\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 5.444951057434082 | KNN Loss: 4.394524574279785 | BCE Loss: 1.0504262447357178\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 5.45481538772583 | KNN Loss: 4.4111762046813965 | BCE Loss: 1.0436393022537231\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 5.451243877410889 | KNN Loss: 4.414956092834473 | BCE Loss: 1.036287784576416\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 5.455732345581055 | KNN Loss: 4.4171342849731445 | BCE Loss: 1.038597822189331\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 5.487081050872803 | KNN Loss: 4.427691459655762 | BCE Loss: 1.059389591217041\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 5.470931053161621 | KNN Loss: 4.442014217376709 | BCE Loss: 1.0289167165756226\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 5.452277183532715 | KNN Loss: 4.40031623840332 | BCE Loss: 1.0519609451293945\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 5.464130401611328 | KNN Loss: 4.411208152770996 | BCE Loss: 1.052922248840332\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 5.441417694091797 | KNN Loss: 4.387675762176514 | BCE Loss: 1.0537420511245728\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 5.428462505340576 | KNN Loss: 4.399329662322998 | BCE Loss: 1.0291328430175781\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 5.483342170715332 | KNN Loss: 4.420807361602783 | BCE Loss: 1.0625345706939697\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 5.441269397735596 | KNN Loss: 4.430557727813721 | BCE Loss: 1.010711669921875\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 5.433150291442871 | KNN Loss: 4.416879177093506 | BCE Loss: 1.0162711143493652\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 5.42803955078125 | KNN Loss: 4.385293483734131 | BCE Loss: 1.0427463054656982\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 5.464681148529053 | KNN Loss: 4.413639545440674 | BCE Loss: 1.0510417222976685\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 5.479609966278076 | KNN Loss: 4.415017604827881 | BCE Loss: 1.0645922422409058\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 5.468610763549805 | KNN Loss: 4.4124040603637695 | BCE Loss: 1.0562069416046143\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 5.499863147735596 | KNN Loss: 4.443913459777832 | BCE Loss: 1.0559496879577637\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 5.445040702819824 | KNN Loss: 4.418194770812988 | BCE Loss: 1.0268460512161255\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 5.461702823638916 | KNN Loss: 4.442890644073486 | BCE Loss: 1.0188120603561401\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 5.476736068725586 | KNN Loss: 4.38844633102417 | BCE Loss: 1.088289499282837\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 5.4449462890625 | KNN Loss: 4.432492733001709 | BCE Loss: 1.0124537944793701\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 5.432088851928711 | KNN Loss: 4.397226333618164 | BCE Loss: 1.0348623991012573\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 5.485467910766602 | KNN Loss: 4.417516231536865 | BCE Loss: 1.0679519176483154\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 5.4549455642700195 | KNN Loss: 4.407046794891357 | BCE Loss: 1.0478990077972412\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 5.484801292419434 | KNN Loss: 4.4233598709106445 | BCE Loss: 1.0614416599273682\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 5.450052261352539 | KNN Loss: 4.413928031921387 | BCE Loss: 1.0361241102218628\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 5.437246799468994 | KNN Loss: 4.3842453956604 | BCE Loss: 1.0530014038085938\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 5.4456048011779785 | KNN Loss: 4.412285327911377 | BCE Loss: 1.033319354057312\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 5.427828788757324 | KNN Loss: 4.385572910308838 | BCE Loss: 1.0422558784484863\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 5.423158168792725 | KNN Loss: 4.381022930145264 | BCE Loss: 1.0421351194381714\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 5.504707336425781 | KNN Loss: 4.451009273529053 | BCE Loss: 1.053697943687439\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 5.453109264373779 | KNN Loss: 4.422410488128662 | BCE Loss: 1.0306987762451172\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 5.432577133178711 | KNN Loss: 4.406135082244873 | BCE Loss: 1.026442050933838\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 5.472099304199219 | KNN Loss: 4.409486770629883 | BCE Loss: 1.062612533569336\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 5.447666168212891 | KNN Loss: 4.395076751708984 | BCE Loss: 1.0525892972946167\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 5.470457553863525 | KNN Loss: 4.424465179443359 | BCE Loss: 1.0459922552108765\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 5.426741123199463 | KNN Loss: 4.401691913604736 | BCE Loss: 1.0250492095947266\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 5.513427734375 | KNN Loss: 4.448911666870117 | BCE Loss: 1.064516305923462\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 5.462319850921631 | KNN Loss: 4.400693416595459 | BCE Loss: 1.0616265535354614\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 5.431268215179443 | KNN Loss: 4.393986225128174 | BCE Loss: 1.0372819900512695\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 5.430455207824707 | KNN Loss: 4.406304359436035 | BCE Loss: 1.0241507291793823\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 5.450936317443848 | KNN Loss: 4.413928985595703 | BCE Loss: 1.037007451057434\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 5.462350368499756 | KNN Loss: 4.424276828765869 | BCE Loss: 1.0380735397338867\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 5.459433555603027 | KNN Loss: 4.412204265594482 | BCE Loss: 1.047229290008545\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 5.451348304748535 | KNN Loss: 4.413633823394775 | BCE Loss: 1.0377143621444702\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 5.474590301513672 | KNN Loss: 4.416388988494873 | BCE Loss: 1.058201551437378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 5.439749717712402 | KNN Loss: 4.4092912673950195 | BCE Loss: 1.0304585695266724\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 5.3913493156433105 | KNN Loss: 4.368643283843994 | BCE Loss: 1.0227059125900269\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 5.417226791381836 | KNN Loss: 4.386479377746582 | BCE Loss: 1.0307471752166748\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 5.456398963928223 | KNN Loss: 4.393693447113037 | BCE Loss: 1.062705397605896\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 5.4906840324401855 | KNN Loss: 4.430553436279297 | BCE Loss: 1.0601304769515991\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 5.418916702270508 | KNN Loss: 4.407166957855225 | BCE Loss: 1.0117496252059937\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 5.456377983093262 | KNN Loss: 4.4151201248168945 | BCE Loss: 1.0412578582763672\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 5.4453582763671875 | KNN Loss: 4.38686990737915 | BCE Loss: 1.0584886074066162\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 5.433802127838135 | KNN Loss: 4.374600410461426 | BCE Loss: 1.0592018365859985\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 5.4244232177734375 | KNN Loss: 4.378419399261475 | BCE Loss: 1.046003818511963\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 5.47314977645874 | KNN Loss: 4.434966087341309 | BCE Loss: 1.0381836891174316\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 5.398266792297363 | KNN Loss: 4.367073059082031 | BCE Loss: 1.0311938524246216\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 5.394218921661377 | KNN Loss: 4.363154411315918 | BCE Loss: 1.031064510345459\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 5.432806968688965 | KNN Loss: 4.391946792602539 | BCE Loss: 1.0408604145050049\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 5.407125473022461 | KNN Loss: 4.378451347351074 | BCE Loss: 1.0286738872528076\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 5.479917049407959 | KNN Loss: 4.427928447723389 | BCE Loss: 1.0519886016845703\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 5.435380458831787 | KNN Loss: 4.375677585601807 | BCE Loss: 1.0597028732299805\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 5.428117275238037 | KNN Loss: 4.400346755981445 | BCE Loss: 1.0277705192565918\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 5.526587009429932 | KNN Loss: 4.455920219421387 | BCE Loss: 1.0706669092178345\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 5.430707931518555 | KNN Loss: 4.3929009437561035 | BCE Loss: 1.037806749343872\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 5.484686851501465 | KNN Loss: 4.405858516693115 | BCE Loss: 1.0788283348083496\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 5.470293998718262 | KNN Loss: 4.411557674407959 | BCE Loss: 1.0587363243103027\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 5.401710033416748 | KNN Loss: 4.366191864013672 | BCE Loss: 1.0355182886123657\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 5.434924125671387 | KNN Loss: 4.407270431518555 | BCE Loss: 1.0276535749435425\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 5.429722785949707 | KNN Loss: 4.374561309814453 | BCE Loss: 1.0551615953445435\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 5.405264854431152 | KNN Loss: 4.377882957458496 | BCE Loss: 1.0273816585540771\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 5.474106788635254 | KNN Loss: 4.394168376922607 | BCE Loss: 1.0799386501312256\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 5.410721778869629 | KNN Loss: 4.3987836837768555 | BCE Loss: 1.0119379758834839\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 5.427469253540039 | KNN Loss: 4.371303558349609 | BCE Loss: 1.0561654567718506\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 5.473262786865234 | KNN Loss: 4.406444072723389 | BCE Loss: 1.0668187141418457\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 5.4086689949035645 | KNN Loss: 4.3847503662109375 | BCE Loss: 1.023918628692627\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 5.483049392700195 | KNN Loss: 4.451351642608643 | BCE Loss: 1.0316979885101318\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 5.444144248962402 | KNN Loss: 4.3852858543396 | BCE Loss: 1.0588583946228027\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 5.423028945922852 | KNN Loss: 4.387324333190918 | BCE Loss: 1.0357043743133545\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 5.377501010894775 | KNN Loss: 4.349282264709473 | BCE Loss: 1.0282188653945923\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 5.474987506866455 | KNN Loss: 4.420138359069824 | BCE Loss: 1.0548491477966309\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 5.441919803619385 | KNN Loss: 4.400843620300293 | BCE Loss: 1.0410761833190918\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 5.472343444824219 | KNN Loss: 4.412566661834717 | BCE Loss: 1.059777021408081\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 5.442526340484619 | KNN Loss: 4.4084672927856445 | BCE Loss: 1.0340590476989746\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 5.413113594055176 | KNN Loss: 4.391892910003662 | BCE Loss: 1.0212209224700928\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 5.4366326332092285 | KNN Loss: 4.4061598777771 | BCE Loss: 1.0304726362228394\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 5.484482288360596 | KNN Loss: 4.43275785446167 | BCE Loss: 1.0517244338989258\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 5.433443069458008 | KNN Loss: 4.38500452041626 | BCE Loss: 1.0484386682510376\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 5.408059120178223 | KNN Loss: 4.374324798583984 | BCE Loss: 1.0337340831756592\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 5.473756313323975 | KNN Loss: 4.440840721130371 | BCE Loss: 1.0329155921936035\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 5.433589458465576 | KNN Loss: 4.3989410400390625 | BCE Loss: 1.0346484184265137\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 5.399499893188477 | KNN Loss: 4.386712074279785 | BCE Loss: 1.0127878189086914\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 5.417391300201416 | KNN Loss: 4.3852691650390625 | BCE Loss: 1.0321221351623535\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 5.4842681884765625 | KNN Loss: 4.432432651519775 | BCE Loss: 1.0518357753753662\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 5.474425315856934 | KNN Loss: 4.414093494415283 | BCE Loss: 1.0603315830230713\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 5.4292449951171875 | KNN Loss: 4.3927531242370605 | BCE Loss: 1.036491870880127\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 5.442076683044434 | KNN Loss: 4.401144504547119 | BCE Loss: 1.0409321784973145\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 5.390013694763184 | KNN Loss: 4.368150234222412 | BCE Loss: 1.0218634605407715\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 5.424286842346191 | KNN Loss: 4.3840436935424805 | BCE Loss: 1.0402430295944214\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 5.3929290771484375 | KNN Loss: 4.3930206298828125 | BCE Loss: 0.9999082088470459\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 5.44446325302124 | KNN Loss: 4.386053562164307 | BCE Loss: 1.0584096908569336\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 5.484890460968018 | KNN Loss: 4.439209461212158 | BCE Loss: 1.0456809997558594\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 5.414709091186523 | KNN Loss: 4.369790077209473 | BCE Loss: 1.0449187755584717\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 5.481527328491211 | KNN Loss: 4.414266109466553 | BCE Loss: 1.0672612190246582\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 5.432111740112305 | KNN Loss: 4.401808261871338 | BCE Loss: 1.0303032398223877\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 5.492197036743164 | KNN Loss: 4.413807392120361 | BCE Loss: 1.0783898830413818\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 5.390695571899414 | KNN Loss: 4.370502948760986 | BCE Loss: 1.0201926231384277\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 5.41356086730957 | KNN Loss: 4.383385181427002 | BCE Loss: 1.0301754474639893\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 5.393500804901123 | KNN Loss: 4.371279716491699 | BCE Loss: 1.0222209692001343\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 5.455384254455566 | KNN Loss: 4.422999858856201 | BCE Loss: 1.0323842763900757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 5.387113571166992 | KNN Loss: 4.374667167663574 | BCE Loss: 1.012446641921997\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 5.446811676025391 | KNN Loss: 4.403135299682617 | BCE Loss: 1.0436766147613525\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 5.404544830322266 | KNN Loss: 4.384265899658203 | BCE Loss: 1.0202791690826416\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 5.426520347595215 | KNN Loss: 4.360771656036377 | BCE Loss: 1.0657484531402588\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 5.397165298461914 | KNN Loss: 4.364058971405029 | BCE Loss: 1.0331063270568848\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 5.428272247314453 | KNN Loss: 4.379443645477295 | BCE Loss: 1.0488286018371582\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 5.4153242111206055 | KNN Loss: 4.3819260597229 | BCE Loss: 1.0333982706069946\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 5.497959613800049 | KNN Loss: 4.4260993003845215 | BCE Loss: 1.0718603134155273\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 5.43057918548584 | KNN Loss: 4.381109237670898 | BCE Loss: 1.0494698286056519\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 5.434457302093506 | KNN Loss: 4.36300802230835 | BCE Loss: 1.0714492797851562\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 5.4457197189331055 | KNN Loss: 4.419921875 | BCE Loss: 1.0257980823516846\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 5.420830726623535 | KNN Loss: 4.3610663414001465 | BCE Loss: 1.0597641468048096\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 5.463338375091553 | KNN Loss: 4.44391393661499 | BCE Loss: 1.0194244384765625\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 5.417729377746582 | KNN Loss: 4.382758617401123 | BCE Loss: 1.0349708795547485\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 5.371702194213867 | KNN Loss: 4.3789286613464355 | BCE Loss: 0.9927733540534973\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 5.3927226066589355 | KNN Loss: 4.357892990112305 | BCE Loss: 1.0348297357559204\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 5.411888122558594 | KNN Loss: 4.379086971282959 | BCE Loss: 1.0328011512756348\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 5.391280174255371 | KNN Loss: 4.3816118240356445 | BCE Loss: 1.0096684694290161\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 5.373629093170166 | KNN Loss: 4.367532253265381 | BCE Loss: 1.0060968399047852\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 5.4604291915893555 | KNN Loss: 4.433587551116943 | BCE Loss: 1.026841402053833\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 5.4179816246032715 | KNN Loss: 4.3696980476379395 | BCE Loss: 1.048283576965332\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 5.428950786590576 | KNN Loss: 4.383089065551758 | BCE Loss: 1.0458616018295288\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 5.3903961181640625 | KNN Loss: 4.374026298522949 | BCE Loss: 1.0163698196411133\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 5.415180206298828 | KNN Loss: 4.409241676330566 | BCE Loss: 1.0059387683868408\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 5.463024139404297 | KNN Loss: 4.39343786239624 | BCE Loss: 1.0695865154266357\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 5.447447776794434 | KNN Loss: 4.4021100997924805 | BCE Loss: 1.0453379154205322\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 5.4531450271606445 | KNN Loss: 4.418419361114502 | BCE Loss: 1.0347259044647217\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 5.416841506958008 | KNN Loss: 4.393497467041016 | BCE Loss: 1.023343801498413\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 5.410146713256836 | KNN Loss: 4.377358913421631 | BCE Loss: 1.0327879190444946\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 5.49691915512085 | KNN Loss: 4.422144412994385 | BCE Loss: 1.0747747421264648\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 5.412045478820801 | KNN Loss: 4.384325981140137 | BCE Loss: 1.0277193784713745\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 5.414691925048828 | KNN Loss: 4.399356365203857 | BCE Loss: 1.0153353214263916\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 5.429884910583496 | KNN Loss: 4.391329765319824 | BCE Loss: 1.0385550260543823\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 5.444674968719482 | KNN Loss: 4.3837385177612305 | BCE Loss: 1.060936450958252\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 5.437288761138916 | KNN Loss: 4.408276081085205 | BCE Loss: 1.0290127992630005\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 5.476587295532227 | KNN Loss: 4.420229911804199 | BCE Loss: 1.056357502937317\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 5.43117094039917 | KNN Loss: 4.382161617279053 | BCE Loss: 1.0490093231201172\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 5.476596832275391 | KNN Loss: 4.4349870681762695 | BCE Loss: 1.0416100025177002\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 5.407731056213379 | KNN Loss: 4.383996486663818 | BCE Loss: 1.0237348079681396\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 5.415855407714844 | KNN Loss: 4.4130449295043945 | BCE Loss: 1.0028105974197388\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 5.419004440307617 | KNN Loss: 4.377199649810791 | BCE Loss: 1.0418050289154053\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 5.379518508911133 | KNN Loss: 4.362268447875977 | BCE Loss: 1.0172502994537354\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 5.446918487548828 | KNN Loss: 4.377731800079346 | BCE Loss: 1.0691866874694824\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 5.390505790710449 | KNN Loss: 4.36351203918457 | BCE Loss: 1.026993751525879\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 5.388179302215576 | KNN Loss: 4.367110729217529 | BCE Loss: 1.0210685729980469\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 5.395497798919678 | KNN Loss: 4.358219623565674 | BCE Loss: 1.037278175354004\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 5.41274356842041 | KNN Loss: 4.365652084350586 | BCE Loss: 1.0470917224884033\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 5.441521644592285 | KNN Loss: 4.400424480438232 | BCE Loss: 1.0410969257354736\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 5.412738800048828 | KNN Loss: 4.401125907897949 | BCE Loss: 1.011612892150879\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 5.419441223144531 | KNN Loss: 4.385112762451172 | BCE Loss: 1.0343282222747803\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 5.475596904754639 | KNN Loss: 4.417045593261719 | BCE Loss: 1.05855131149292\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 5.417825698852539 | KNN Loss: 4.398155212402344 | BCE Loss: 1.0196703672409058\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 5.4038262367248535 | KNN Loss: 4.360373497009277 | BCE Loss: 1.0434527397155762\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 5.413332939147949 | KNN Loss: 4.382552146911621 | BCE Loss: 1.030780553817749\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 5.434318542480469 | KNN Loss: 4.38377571105957 | BCE Loss: 1.0505430698394775\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 5.456363201141357 | KNN Loss: 4.428518295288086 | BCE Loss: 1.0278449058532715\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 5.376598358154297 | KNN Loss: 4.377020359039307 | BCE Loss: 0.999578058719635\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 5.407593727111816 | KNN Loss: 4.388956069946289 | BCE Loss: 1.0186374187469482\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 5.421834945678711 | KNN Loss: 4.384066581726074 | BCE Loss: 1.0377686023712158\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 5.3894853591918945 | KNN Loss: 4.373756408691406 | BCE Loss: 1.0157291889190674\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 5.367166519165039 | KNN Loss: 4.3566155433654785 | BCE Loss: 1.01055109500885\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 5.432234287261963 | KNN Loss: 4.389866352081299 | BCE Loss: 1.0423678159713745\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 5.442704677581787 | KNN Loss: 4.408594608306885 | BCE Loss: 1.0341100692749023\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 5.452430725097656 | KNN Loss: 4.392155170440674 | BCE Loss: 1.0602757930755615\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 5.3865861892700195 | KNN Loss: 4.367762088775635 | BCE Loss: 1.0188238620758057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 5.398621082305908 | KNN Loss: 4.346743583679199 | BCE Loss: 1.051877498626709\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 5.383538246154785 | KNN Loss: 4.367569923400879 | BCE Loss: 1.0159683227539062\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 5.487311363220215 | KNN Loss: 4.44534158706665 | BCE Loss: 1.0419695377349854\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 5.4748735427856445 | KNN Loss: 4.421506881713867 | BCE Loss: 1.0533666610717773\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 5.366055488586426 | KNN Loss: 4.353030204772949 | BCE Loss: 1.0130252838134766\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 5.407138347625732 | KNN Loss: 4.385196208953857 | BCE Loss: 1.021942138671875\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 5.4341230392456055 | KNN Loss: 4.402965068817139 | BCE Loss: 1.0311577320098877\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 5.397789001464844 | KNN Loss: 4.380267143249512 | BCE Loss: 1.0175217390060425\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 5.449315547943115 | KNN Loss: 4.402113914489746 | BCE Loss: 1.0472016334533691\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 5.410153388977051 | KNN Loss: 4.396450042724609 | BCE Loss: 1.0137033462524414\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 5.430628299713135 | KNN Loss: 4.416520595550537 | BCE Loss: 1.0141077041625977\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 5.3933186531066895 | KNN Loss: 4.3530707359313965 | BCE Loss: 1.040247917175293\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 5.407595157623291 | KNN Loss: 4.356922149658203 | BCE Loss: 1.050673007965088\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 5.404592990875244 | KNN Loss: 4.391816139221191 | BCE Loss: 1.0127768516540527\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 5.4215593338012695 | KNN Loss: 4.389774799346924 | BCE Loss: 1.0317846536636353\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 5.420836448669434 | KNN Loss: 4.3762922286987305 | BCE Loss: 1.0445444583892822\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 5.399313449859619 | KNN Loss: 4.35694694519043 | BCE Loss: 1.0423665046691895\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 5.403877258300781 | KNN Loss: 4.36498498916626 | BCE Loss: 1.0388920307159424\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 5.412016868591309 | KNN Loss: 4.361751556396484 | BCE Loss: 1.0502655506134033\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 5.441683769226074 | KNN Loss: 4.396407127380371 | BCE Loss: 1.045276403427124\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 5.459029197692871 | KNN Loss: 4.412841320037842 | BCE Loss: 1.0461877584457397\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 5.427042484283447 | KNN Loss: 4.388012409210205 | BCE Loss: 1.0390299558639526\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 5.462499618530273 | KNN Loss: 4.403581619262695 | BCE Loss: 1.0589178800582886\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 5.447243690490723 | KNN Loss: 4.407515048980713 | BCE Loss: 1.0397285223007202\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 5.405065059661865 | KNN Loss: 4.394160270690918 | BCE Loss: 1.0109047889709473\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 5.47063684463501 | KNN Loss: 4.43895959854126 | BCE Loss: 1.0316771268844604\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 5.387029647827148 | KNN Loss: 4.371319770812988 | BCE Loss: 1.0157101154327393\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 5.404931545257568 | KNN Loss: 4.390729904174805 | BCE Loss: 1.0142015218734741\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 5.440481662750244 | KNN Loss: 4.418982982635498 | BCE Loss: 1.021498680114746\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 5.460094451904297 | KNN Loss: 4.415596961975098 | BCE Loss: 1.0444976091384888\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 5.429286003112793 | KNN Loss: 4.387950420379639 | BCE Loss: 1.0413358211517334\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 5.399077892303467 | KNN Loss: 4.359710216522217 | BCE Loss: 1.03936767578125\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 5.421413421630859 | KNN Loss: 4.379674434661865 | BCE Loss: 1.0417391061782837\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 5.37841272354126 | KNN Loss: 4.358185768127441 | BCE Loss: 1.0202269554138184\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 5.393383979797363 | KNN Loss: 4.352731227874756 | BCE Loss: 1.040652871131897\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 5.422844886779785 | KNN Loss: 4.38184928894043 | BCE Loss: 1.040995717048645\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 5.388768196105957 | KNN Loss: 4.3635663986206055 | BCE Loss: 1.025201678276062\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 5.37453556060791 | KNN Loss: 4.354357719421387 | BCE Loss: 1.0201776027679443\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 5.419268608093262 | KNN Loss: 4.371428489685059 | BCE Loss: 1.047839879989624\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 5.416889667510986 | KNN Loss: 4.392904281616211 | BCE Loss: 1.0239853858947754\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 5.408342361450195 | KNN Loss: 4.3858842849731445 | BCE Loss: 1.0224583148956299\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 5.413348197937012 | KNN Loss: 4.35872220993042 | BCE Loss: 1.0546259880065918\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 5.437784671783447 | KNN Loss: 4.371942520141602 | BCE Loss: 1.0658421516418457\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 5.414529323577881 | KNN Loss: 4.380277633666992 | BCE Loss: 1.0342518091201782\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 5.420547008514404 | KNN Loss: 4.366566181182861 | BCE Loss: 1.0539807081222534\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 5.407723426818848 | KNN Loss: 4.388707160949707 | BCE Loss: 1.0190163850784302\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 5.457640647888184 | KNN Loss: 4.39649772644043 | BCE Loss: 1.061143159866333\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 5.39644718170166 | KNN Loss: 4.356868743896484 | BCE Loss: 1.0395786762237549\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 5.450435638427734 | KNN Loss: 4.424337387084961 | BCE Loss: 1.0260984897613525\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 5.43159294128418 | KNN Loss: 4.400191783905029 | BCE Loss: 1.0314011573791504\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 5.440578937530518 | KNN Loss: 4.42451810836792 | BCE Loss: 1.0160608291625977\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 5.37384033203125 | KNN Loss: 4.357367992401123 | BCE Loss: 1.016472578048706\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 5.387912750244141 | KNN Loss: 4.356432914733887 | BCE Loss: 1.031479835510254\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 5.450419902801514 | KNN Loss: 4.414524555206299 | BCE Loss: 1.0358953475952148\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 5.453573226928711 | KNN Loss: 4.405050277709961 | BCE Loss: 1.04852294921875\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 5.429253578186035 | KNN Loss: 4.41525936126709 | BCE Loss: 1.0139939785003662\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 5.437524318695068 | KNN Loss: 4.414616584777832 | BCE Loss: 1.0229078531265259\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 5.400063991546631 | KNN Loss: 4.360049247741699 | BCE Loss: 1.0400147438049316\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 5.422491550445557 | KNN Loss: 4.371904373168945 | BCE Loss: 1.0505872964859009\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 5.3733625411987305 | KNN Loss: 4.350627899169922 | BCE Loss: 1.0227344036102295\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 5.368436336517334 | KNN Loss: 4.345812797546387 | BCE Loss: 1.0226235389709473\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 5.416682243347168 | KNN Loss: 4.365201950073242 | BCE Loss: 1.0514805316925049\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 5.453760623931885 | KNN Loss: 4.386506080627441 | BCE Loss: 1.0672545433044434\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 5.415688514709473 | KNN Loss: 4.387898921966553 | BCE Loss: 1.0277897119522095\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 5.435598850250244 | KNN Loss: 4.402670383453369 | BCE Loss: 1.032928466796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 5.3887505531311035 | KNN Loss: 4.3645405769348145 | BCE Loss: 1.024209976196289\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 5.381153106689453 | KNN Loss: 4.365494728088379 | BCE Loss: 1.0156583786010742\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 5.454333305358887 | KNN Loss: 4.397769927978516 | BCE Loss: 1.056563138961792\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 5.467281818389893 | KNN Loss: 4.417895317077637 | BCE Loss: 1.0493865013122559\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 5.420456886291504 | KNN Loss: 4.390741348266602 | BCE Loss: 1.0297152996063232\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 5.444135665893555 | KNN Loss: 4.409120559692383 | BCE Loss: 1.0350151062011719\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 5.411436557769775 | KNN Loss: 4.382104396820068 | BCE Loss: 1.0293322801589966\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 5.400620460510254 | KNN Loss: 4.336050510406494 | BCE Loss: 1.0645700693130493\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 5.438697814941406 | KNN Loss: 4.383053302764893 | BCE Loss: 1.0556446313858032\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 5.443305492401123 | KNN Loss: 4.437917232513428 | BCE Loss: 1.0053881406784058\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 5.401393890380859 | KNN Loss: 4.359076499938965 | BCE Loss: 1.0423173904418945\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 5.41474723815918 | KNN Loss: 4.387126922607422 | BCE Loss: 1.027620553970337\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 5.408275604248047 | KNN Loss: 4.390601634979248 | BCE Loss: 1.017674207687378\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 5.445530891418457 | KNN Loss: 4.395379543304443 | BCE Loss: 1.0501513481140137\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 5.430729389190674 | KNN Loss: 4.383968353271484 | BCE Loss: 1.0467610359191895\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 5.396213531494141 | KNN Loss: 4.374256610870361 | BCE Loss: 1.0219568014144897\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 5.404376029968262 | KNN Loss: 4.388176918029785 | BCE Loss: 1.0161988735198975\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 5.392235279083252 | KNN Loss: 4.370524883270264 | BCE Loss: 1.0217102766036987\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 5.405344486236572 | KNN Loss: 4.403237342834473 | BCE Loss: 1.0021071434020996\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 5.437485218048096 | KNN Loss: 4.385882377624512 | BCE Loss: 1.0516027212142944\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 5.3778228759765625 | KNN Loss: 4.365118503570557 | BCE Loss: 1.012704610824585\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 5.39035701751709 | KNN Loss: 4.358014106750488 | BCE Loss: 1.0323430299758911\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 5.407544136047363 | KNN Loss: 4.394559860229492 | BCE Loss: 1.0129841566085815\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 5.417374610900879 | KNN Loss: 4.405065059661865 | BCE Loss: 1.0123093128204346\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 5.468899726867676 | KNN Loss: 4.405569553375244 | BCE Loss: 1.0633301734924316\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 5.392977237701416 | KNN Loss: 4.380013465881348 | BCE Loss: 1.0129637718200684\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 5.460314750671387 | KNN Loss: 4.408186912536621 | BCE Loss: 1.0521275997161865\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 5.361920356750488 | KNN Loss: 4.360106945037842 | BCE Loss: 1.0018131732940674\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 5.399428844451904 | KNN Loss: 4.390781402587891 | BCE Loss: 1.0086474418640137\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 5.371029853820801 | KNN Loss: 4.380134582519531 | BCE Loss: 0.9908950328826904\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 5.410488128662109 | KNN Loss: 4.377449989318848 | BCE Loss: 1.0330383777618408\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 5.432182312011719 | KNN Loss: 4.386786460876465 | BCE Loss: 1.045395851135254\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 5.408380031585693 | KNN Loss: 4.365306854248047 | BCE Loss: 1.0430731773376465\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 5.410850524902344 | KNN Loss: 4.369211673736572 | BCE Loss: 1.0416390895843506\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 5.406670570373535 | KNN Loss: 4.377613544464111 | BCE Loss: 1.0290567874908447\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 5.389011383056641 | KNN Loss: 4.361741542816162 | BCE Loss: 1.0272696018218994\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 5.39465856552124 | KNN Loss: 4.363729953765869 | BCE Loss: 1.030928611755371\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 5.424415111541748 | KNN Loss: 4.384707927703857 | BCE Loss: 1.039707064628601\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 5.380321502685547 | KNN Loss: 4.371079444885254 | BCE Loss: 1.009242057800293\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 5.408865451812744 | KNN Loss: 4.393997669219971 | BCE Loss: 1.0148676633834839\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 5.393709182739258 | KNN Loss: 4.373985290527344 | BCE Loss: 1.019723653793335\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 5.424245834350586 | KNN Loss: 4.411390781402588 | BCE Loss: 1.0128552913665771\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 5.439975738525391 | KNN Loss: 4.4199323654174805 | BCE Loss: 1.0200433731079102\n",
      "Epoch    83: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 5.403955936431885 | KNN Loss: 4.388935565948486 | BCE Loss: 1.0150202512741089\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 5.402836799621582 | KNN Loss: 4.355224132537842 | BCE Loss: 1.0476124286651611\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 5.386111259460449 | KNN Loss: 4.375914573669434 | BCE Loss: 1.010196566581726\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 5.42840576171875 | KNN Loss: 4.386251449584961 | BCE Loss: 1.0421544313430786\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 5.462310791015625 | KNN Loss: 4.4329376220703125 | BCE Loss: 1.029373049736023\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 5.3942766189575195 | KNN Loss: 4.360957145690918 | BCE Loss: 1.0333195924758911\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 5.405413627624512 | KNN Loss: 4.390899181365967 | BCE Loss: 1.0145142078399658\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 5.397464752197266 | KNN Loss: 4.395893573760986 | BCE Loss: 1.0015711784362793\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 5.4290571212768555 | KNN Loss: 4.406772613525391 | BCE Loss: 1.0222845077514648\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 5.387055397033691 | KNN Loss: 4.357087135314941 | BCE Loss: 1.029968500137329\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 5.372550964355469 | KNN Loss: 4.35377311706543 | BCE Loss: 1.0187777280807495\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 5.377565383911133 | KNN Loss: 4.357405185699463 | BCE Loss: 1.0201603174209595\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 5.334654331207275 | KNN Loss: 4.337956428527832 | BCE Loss: 0.9966980218887329\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 5.3906707763671875 | KNN Loss: 4.350225925445557 | BCE Loss: 1.0404446125030518\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 5.376704216003418 | KNN Loss: 4.3683695793151855 | BCE Loss: 1.0083343982696533\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 5.407489776611328 | KNN Loss: 4.358466625213623 | BCE Loss: 1.0490233898162842\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 5.398340225219727 | KNN Loss: 4.363543510437012 | BCE Loss: 1.0347965955734253\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 5.40626335144043 | KNN Loss: 4.372628211975098 | BCE Loss: 1.033634901046753\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 5.465989112854004 | KNN Loss: 4.41461706161499 | BCE Loss: 1.0513722896575928\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 5.397884845733643 | KNN Loss: 4.385822296142578 | BCE Loss: 1.012062430381775\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 5.379514694213867 | KNN Loss: 4.350808143615723 | BCE Loss: 1.0287067890167236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 5.3990888595581055 | KNN Loss: 4.374813556671143 | BCE Loss: 1.024275541305542\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 5.4385786056518555 | KNN Loss: 4.387181282043457 | BCE Loss: 1.0513975620269775\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 5.411357402801514 | KNN Loss: 4.365033149719238 | BCE Loss: 1.046324372291565\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 5.385058403015137 | KNN Loss: 4.356067657470703 | BCE Loss: 1.0289907455444336\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 5.423992156982422 | KNN Loss: 4.40303897857666 | BCE Loss: 1.0209534168243408\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 5.400088787078857 | KNN Loss: 4.374690055847168 | BCE Loss: 1.025398850440979\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 5.42087459564209 | KNN Loss: 4.387348175048828 | BCE Loss: 1.0335263013839722\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 5.422621250152588 | KNN Loss: 4.409318923950195 | BCE Loss: 1.0133023262023926\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 5.3686652183532715 | KNN Loss: 4.355364799499512 | BCE Loss: 1.0133004188537598\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 5.403487682342529 | KNN Loss: 4.355589389801025 | BCE Loss: 1.047898292541504\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 5.388409614562988 | KNN Loss: 4.370180606842041 | BCE Loss: 1.0182290077209473\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 5.3960676193237305 | KNN Loss: 4.370855331420898 | BCE Loss: 1.025212287902832\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 5.4124369621276855 | KNN Loss: 4.386973857879639 | BCE Loss: 1.0254631042480469\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 5.374024391174316 | KNN Loss: 4.351188659667969 | BCE Loss: 1.0228359699249268\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 5.387458801269531 | KNN Loss: 4.3589067459106445 | BCE Loss: 1.0285518169403076\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 5.419142723083496 | KNN Loss: 4.381762504577637 | BCE Loss: 1.0373802185058594\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 5.366353988647461 | KNN Loss: 4.351603984832764 | BCE Loss: 1.0147502422332764\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 5.411270618438721 | KNN Loss: 4.372446537017822 | BCE Loss: 1.0388240814208984\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 5.418423652648926 | KNN Loss: 4.40765905380249 | BCE Loss: 1.0107643604278564\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 5.436765670776367 | KNN Loss: 4.390979290008545 | BCE Loss: 1.0457866191864014\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 5.431013107299805 | KNN Loss: 4.399041175842285 | BCE Loss: 1.0319716930389404\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 5.367225646972656 | KNN Loss: 4.350815773010254 | BCE Loss: 1.0164101123809814\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 5.407711029052734 | KNN Loss: 4.3654561042785645 | BCE Loss: 1.04225492477417\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 5.362435340881348 | KNN Loss: 4.348462104797363 | BCE Loss: 1.0139731168746948\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 5.375917434692383 | KNN Loss: 4.363515853881836 | BCE Loss: 1.012401819229126\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 5.401625633239746 | KNN Loss: 4.3954644203186035 | BCE Loss: 1.0061612129211426\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 5.457067489624023 | KNN Loss: 4.395948886871338 | BCE Loss: 1.0611183643341064\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 5.4346513748168945 | KNN Loss: 4.401474475860596 | BCE Loss: 1.033177137374878\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 5.378524303436279 | KNN Loss: 4.370814800262451 | BCE Loss: 1.0077095031738281\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 5.415247917175293 | KNN Loss: 4.394773960113525 | BCE Loss: 1.0204737186431885\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 5.407933235168457 | KNN Loss: 4.373143672943115 | BCE Loss: 1.0347895622253418\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 5.373595237731934 | KNN Loss: 4.37520694732666 | BCE Loss: 0.9983883500099182\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 5.440271377563477 | KNN Loss: 4.376914024353027 | BCE Loss: 1.0633572340011597\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 5.367690086364746 | KNN Loss: 4.348350524902344 | BCE Loss: 1.019339680671692\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 5.394322872161865 | KNN Loss: 4.375733852386475 | BCE Loss: 1.0185890197753906\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 5.4051713943481445 | KNN Loss: 4.358238697052002 | BCE Loss: 1.0469329357147217\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 5.502535820007324 | KNN Loss: 4.46555233001709 | BCE Loss: 1.0369837284088135\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 5.404634475708008 | KNN Loss: 4.363560199737549 | BCE Loss: 1.041074275970459\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 5.40691614151001 | KNN Loss: 4.379978656768799 | BCE Loss: 1.0269373655319214\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 5.442675590515137 | KNN Loss: 4.411503791809082 | BCE Loss: 1.0311716794967651\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 5.413407325744629 | KNN Loss: 4.363375663757324 | BCE Loss: 1.0500317811965942\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 5.3896260261535645 | KNN Loss: 4.357024669647217 | BCE Loss: 1.0326013565063477\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 5.472922325134277 | KNN Loss: 4.4538397789001465 | BCE Loss: 1.01908278465271\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 5.400157451629639 | KNN Loss: 4.373981475830078 | BCE Loss: 1.026175856590271\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 5.406718730926514 | KNN Loss: 4.378708362579346 | BCE Loss: 1.028010368347168\n",
      "Epoch    94: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 5.408082008361816 | KNN Loss: 4.3805832862854 | BCE Loss: 1.0274989604949951\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 5.417966842651367 | KNN Loss: 4.360586643218994 | BCE Loss: 1.057380199432373\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 5.39586877822876 | KNN Loss: 4.361049175262451 | BCE Loss: 1.034819483757019\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 5.383088111877441 | KNN Loss: 4.349745750427246 | BCE Loss: 1.0333422422409058\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 5.417205810546875 | KNN Loss: 4.397996425628662 | BCE Loss: 1.0192095041275024\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 5.386375427246094 | KNN Loss: 4.363394737243652 | BCE Loss: 1.0229804515838623\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 5.4057416915893555 | KNN Loss: 4.387902736663818 | BCE Loss: 1.017838716506958\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 5.426042079925537 | KNN Loss: 4.380624294281006 | BCE Loss: 1.0454177856445312\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 5.418617248535156 | KNN Loss: 4.375800609588623 | BCE Loss: 1.0428166389465332\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 5.415557861328125 | KNN Loss: 4.380503177642822 | BCE Loss: 1.0350548028945923\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 5.3840484619140625 | KNN Loss: 4.373868465423584 | BCE Loss: 1.010180115699768\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 5.420433521270752 | KNN Loss: 4.378538608551025 | BCE Loss: 1.0418950319290161\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 5.390842437744141 | KNN Loss: 4.379199981689453 | BCE Loss: 1.0116426944732666\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 5.402312278747559 | KNN Loss: 4.399662971496582 | BCE Loss: 1.0026493072509766\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 5.4324049949646 | KNN Loss: 4.380786418914795 | BCE Loss: 1.0516184568405151\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 5.429587364196777 | KNN Loss: 4.3876237869262695 | BCE Loss: 1.0419635772705078\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 5.422706604003906 | KNN Loss: 4.413902759552002 | BCE Loss: 1.0088040828704834\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 5.39847993850708 | KNN Loss: 4.37348747253418 | BCE Loss: 1.0249923467636108\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 5.421693801879883 | KNN Loss: 4.37174129486084 | BCE Loss: 1.0499522686004639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 5.412693023681641 | KNN Loss: 4.365521430969238 | BCE Loss: 1.0471715927124023\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 5.388228416442871 | KNN Loss: 4.382907390594482 | BCE Loss: 1.0053210258483887\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 5.412227153778076 | KNN Loss: 4.390500545501709 | BCE Loss: 1.0217267274856567\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 5.362514972686768 | KNN Loss: 4.355202674865723 | BCE Loss: 1.007312297821045\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 5.40207052230835 | KNN Loss: 4.37186861038208 | BCE Loss: 1.030202031135559\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 5.390743732452393 | KNN Loss: 4.364981651306152 | BCE Loss: 1.0257620811462402\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 5.395610809326172 | KNN Loss: 4.366288661956787 | BCE Loss: 1.0293219089508057\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 5.432631969451904 | KNN Loss: 4.399496555328369 | BCE Loss: 1.0331354141235352\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 5.3761138916015625 | KNN Loss: 4.348504543304443 | BCE Loss: 1.0276093482971191\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 5.4370317459106445 | KNN Loss: 4.364984512329102 | BCE Loss: 1.072047233581543\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 5.381964206695557 | KNN Loss: 4.366492748260498 | BCE Loss: 1.0154714584350586\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 5.4147233963012695 | KNN Loss: 4.362514972686768 | BCE Loss: 1.052208423614502\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 5.407400131225586 | KNN Loss: 4.389514446258545 | BCE Loss: 1.017885684967041\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 5.433791160583496 | KNN Loss: 4.402039527893066 | BCE Loss: 1.0317516326904297\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 5.391082286834717 | KNN Loss: 4.376309871673584 | BCE Loss: 1.0147725343704224\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 5.378586292266846 | KNN Loss: 4.371957778930664 | BCE Loss: 1.0066285133361816\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 5.443594932556152 | KNN Loss: 4.411491394042969 | BCE Loss: 1.0321033000946045\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 5.407045364379883 | KNN Loss: 4.357713222503662 | BCE Loss: 1.0493319034576416\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 5.372267723083496 | KNN Loss: 4.355045318603516 | BCE Loss: 1.017222285270691\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 5.405874252319336 | KNN Loss: 4.376475811004639 | BCE Loss: 1.0293982028961182\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 5.462809085845947 | KNN Loss: 4.413263320922852 | BCE Loss: 1.0495457649230957\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 5.429135322570801 | KNN Loss: 4.377833843231201 | BCE Loss: 1.0513012409210205\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 5.393833160400391 | KNN Loss: 4.393609523773193 | BCE Loss: 1.0002238750457764\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 5.380975246429443 | KNN Loss: 4.364626407623291 | BCE Loss: 1.0163488388061523\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 5.4535112380981445 | KNN Loss: 4.395415782928467 | BCE Loss: 1.0580952167510986\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 5.407272815704346 | KNN Loss: 4.367125988006592 | BCE Loss: 1.040146827697754\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 5.392702102661133 | KNN Loss: 4.374129772186279 | BCE Loss: 1.0185720920562744\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 5.394140720367432 | KNN Loss: 4.367491245269775 | BCE Loss: 1.0266494750976562\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 5.429036617279053 | KNN Loss: 4.390811443328857 | BCE Loss: 1.0382252931594849\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 5.4225616455078125 | KNN Loss: 4.375340938568115 | BCE Loss: 1.0472209453582764\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 5.416247844696045 | KNN Loss: 4.423318862915039 | BCE Loss: 0.9929291605949402\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 5.424816608428955 | KNN Loss: 4.373509407043457 | BCE Loss: 1.051307201385498\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 5.44149923324585 | KNN Loss: 4.404994964599609 | BCE Loss: 1.0365043878555298\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 5.439535140991211 | KNN Loss: 4.408942699432373 | BCE Loss: 1.0305922031402588\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 5.407835960388184 | KNN Loss: 4.372213363647461 | BCE Loss: 1.0356223583221436\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 5.395370960235596 | KNN Loss: 4.357897758483887 | BCE Loss: 1.037473201751709\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 5.431153297424316 | KNN Loss: 4.3858866691589355 | BCE Loss: 1.0452667474746704\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 5.455308437347412 | KNN Loss: 4.408575057983398 | BCE Loss: 1.0467333793640137\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 5.40728759765625 | KNN Loss: 4.366365909576416 | BCE Loss: 1.040921688079834\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 5.349916934967041 | KNN Loss: 4.350320339202881 | BCE Loss: 0.9995964169502258\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 5.368294715881348 | KNN Loss: 4.351032733917236 | BCE Loss: 1.0172618627548218\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 5.432323455810547 | KNN Loss: 4.387560844421387 | BCE Loss: 1.0447627305984497\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 5.424971580505371 | KNN Loss: 4.391983509063721 | BCE Loss: 1.0329879522323608\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 5.377903461456299 | KNN Loss: 4.3556318283081055 | BCE Loss: 1.0222716331481934\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 5.459296226501465 | KNN Loss: 4.422948360443115 | BCE Loss: 1.0363478660583496\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 5.377485752105713 | KNN Loss: 4.3735551834106445 | BCE Loss: 1.003930687904358\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 5.396937847137451 | KNN Loss: 4.35689640045166 | BCE Loss: 1.040041446685791\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 5.436748504638672 | KNN Loss: 4.388293266296387 | BCE Loss: 1.0484554767608643\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 5.424172401428223 | KNN Loss: 4.380257606506348 | BCE Loss: 1.043914556503296\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 5.4360761642456055 | KNN Loss: 4.397701263427734 | BCE Loss: 1.038374900817871\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 5.442519187927246 | KNN Loss: 4.391094207763672 | BCE Loss: 1.0514247417449951\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 5.414430141448975 | KNN Loss: 4.390357971191406 | BCE Loss: 1.0240721702575684\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 5.363760948181152 | KNN Loss: 4.3639235496521 | BCE Loss: 0.9998375177383423\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 5.366268157958984 | KNN Loss: 4.364334583282471 | BCE Loss: 1.0019335746765137\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 5.405834674835205 | KNN Loss: 4.367254257202148 | BCE Loss: 1.0385804176330566\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 5.4136505126953125 | KNN Loss: 4.359386444091797 | BCE Loss: 1.0542643070220947\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 5.463354110717773 | KNN Loss: 4.404284954071045 | BCE Loss: 1.0590691566467285\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 5.346022605895996 | KNN Loss: 4.334678649902344 | BCE Loss: 1.0113437175750732\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 5.393822193145752 | KNN Loss: 4.352063179016113 | BCE Loss: 1.0417591333389282\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 5.403486728668213 | KNN Loss: 4.372470378875732 | BCE Loss: 1.0310163497924805\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 5.415679454803467 | KNN Loss: 4.348989486694336 | BCE Loss: 1.0666898488998413\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 5.386711120605469 | KNN Loss: 4.359460353851318 | BCE Loss: 1.0272506475448608\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 5.40522575378418 | KNN Loss: 4.374189376831055 | BCE Loss: 1.031036376953125\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 5.445454120635986 | KNN Loss: 4.384557723999023 | BCE Loss: 1.060896396636963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 5.489634990692139 | KNN Loss: 4.466172218322754 | BCE Loss: 1.0234627723693848\n",
      "Epoch   108: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 5.396200180053711 | KNN Loss: 4.382324695587158 | BCE Loss: 1.0138752460479736\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 5.420331954956055 | KNN Loss: 4.367659568786621 | BCE Loss: 1.0526723861694336\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 5.431967735290527 | KNN Loss: 4.3840179443359375 | BCE Loss: 1.047950029373169\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 5.4168901443481445 | KNN Loss: 4.37197208404541 | BCE Loss: 1.0449178218841553\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 5.420536994934082 | KNN Loss: 4.394333839416504 | BCE Loss: 1.0262031555175781\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 5.393564701080322 | KNN Loss: 4.366558074951172 | BCE Loss: 1.0270065069198608\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 5.4001336097717285 | KNN Loss: 4.381633281707764 | BCE Loss: 1.0185002088546753\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 5.408843517303467 | KNN Loss: 4.343529224395752 | BCE Loss: 1.0653144121170044\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 5.418332099914551 | KNN Loss: 4.410670280456543 | BCE Loss: 1.0076615810394287\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 5.384909629821777 | KNN Loss: 4.366177558898926 | BCE Loss: 1.0187320709228516\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 5.379837989807129 | KNN Loss: 4.344079494476318 | BCE Loss: 1.0357586145401\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 5.418013572692871 | KNN Loss: 4.40022611618042 | BCE Loss: 1.0177876949310303\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 5.419783592224121 | KNN Loss: 4.378098487854004 | BCE Loss: 1.041684865951538\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 5.41429328918457 | KNN Loss: 4.38748025894165 | BCE Loss: 1.0268127918243408\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 5.415810585021973 | KNN Loss: 4.376520156860352 | BCE Loss: 1.039290428161621\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 5.417828559875488 | KNN Loss: 4.3715643882751465 | BCE Loss: 1.046264410018921\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 5.380022048950195 | KNN Loss: 4.346851825714111 | BCE Loss: 1.033170223236084\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 5.425253868103027 | KNN Loss: 4.392109394073486 | BCE Loss: 1.033144235610962\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 5.418612957000732 | KNN Loss: 4.365900039672852 | BCE Loss: 1.0527127981185913\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 5.4092326164245605 | KNN Loss: 4.379095077514648 | BCE Loss: 1.030137538909912\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 5.401949405670166 | KNN Loss: 4.391260623931885 | BCE Loss: 1.0106887817382812\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 5.43753719329834 | KNN Loss: 4.402482509613037 | BCE Loss: 1.0350544452667236\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 5.401951789855957 | KNN Loss: 4.3721442222595215 | BCE Loss: 1.0298073291778564\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 5.437067031860352 | KNN Loss: 4.37398624420166 | BCE Loss: 1.0630807876586914\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 5.425411701202393 | KNN Loss: 4.367443561553955 | BCE Loss: 1.0579681396484375\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 5.3844404220581055 | KNN Loss: 4.372884750366211 | BCE Loss: 1.011555552482605\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 5.401317596435547 | KNN Loss: 4.372406005859375 | BCE Loss: 1.028911828994751\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 5.417562484741211 | KNN Loss: 4.387609958648682 | BCE Loss: 1.0299525260925293\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 5.387470722198486 | KNN Loss: 4.366272926330566 | BCE Loss: 1.0211979150772095\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 5.403900623321533 | KNN Loss: 4.3501081466674805 | BCE Loss: 1.0537925958633423\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 5.377403259277344 | KNN Loss: 4.358265399932861 | BCE Loss: 1.0191380977630615\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 5.363046169281006 | KNN Loss: 4.3418288230896 | BCE Loss: 1.0212172269821167\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 5.393181800842285 | KNN Loss: 4.350552082061768 | BCE Loss: 1.0426299571990967\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 5.393082618713379 | KNN Loss: 4.3546366691589355 | BCE Loss: 1.038446068763733\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 5.38446044921875 | KNN Loss: 4.352972984313965 | BCE Loss: 1.031487226486206\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 5.3685760498046875 | KNN Loss: 4.35822868347168 | BCE Loss: 1.0103472471237183\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 5.3916802406311035 | KNN Loss: 4.3828253746032715 | BCE Loss: 1.008854866027832\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 5.395809173583984 | KNN Loss: 4.364738941192627 | BCE Loss: 1.0310702323913574\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 5.437357425689697 | KNN Loss: 4.403692722320557 | BCE Loss: 1.0336647033691406\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 5.42529296875 | KNN Loss: 4.389166831970215 | BCE Loss: 1.036125898361206\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 5.409787178039551 | KNN Loss: 4.365126132965088 | BCE Loss: 1.0446608066558838\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 5.442182540893555 | KNN Loss: 4.413997173309326 | BCE Loss: 1.0281851291656494\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 5.410842418670654 | KNN Loss: 4.3776726722717285 | BCE Loss: 1.0331697463989258\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 5.416749477386475 | KNN Loss: 4.379847049713135 | BCE Loss: 1.0369024276733398\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 5.410184860229492 | KNN Loss: 4.363696098327637 | BCE Loss: 1.0464887619018555\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 5.434203147888184 | KNN Loss: 4.402434349060059 | BCE Loss: 1.0317686796188354\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 5.417956352233887 | KNN Loss: 4.382137775421143 | BCE Loss: 1.035818338394165\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 5.396596908569336 | KNN Loss: 4.358588218688965 | BCE Loss: 1.0380089282989502\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 5.3968610763549805 | KNN Loss: 4.365583419799805 | BCE Loss: 1.0312774181365967\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 5.412357330322266 | KNN Loss: 4.408449172973633 | BCE Loss: 1.0039079189300537\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 5.450830936431885 | KNN Loss: 4.396335124969482 | BCE Loss: 1.054495930671692\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 5.437119960784912 | KNN Loss: 4.38733434677124 | BCE Loss: 1.0497856140136719\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 5.3710150718688965 | KNN Loss: 4.357545852661133 | BCE Loss: 1.0134693384170532\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 5.381890296936035 | KNN Loss: 4.363949775695801 | BCE Loss: 1.0179407596588135\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 5.399084091186523 | KNN Loss: 4.365147113800049 | BCE Loss: 1.0339370965957642\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 5.416367053985596 | KNN Loss: 4.3748273849487305 | BCE Loss: 1.0415395498275757\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 5.4218363761901855 | KNN Loss: 4.36920690536499 | BCE Loss: 1.0526294708251953\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 5.362222671508789 | KNN Loss: 4.360419750213623 | BCE Loss: 1.001802682876587\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 5.397000312805176 | KNN Loss: 4.370619297027588 | BCE Loss: 1.0263811349868774\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 5.36911678314209 | KNN Loss: 4.34859561920166 | BCE Loss: 1.0205211639404297\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 5.373462677001953 | KNN Loss: 4.35905647277832 | BCE Loss: 1.0144059658050537\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 5.459134101867676 | KNN Loss: 4.418145179748535 | BCE Loss: 1.0409886837005615\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 5.405854225158691 | KNN Loss: 4.382440567016602 | BCE Loss: 1.0234134197235107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 5.4399824142456055 | KNN Loss: 4.39484167098999 | BCE Loss: 1.0451407432556152\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 5.3714189529418945 | KNN Loss: 4.349240779876709 | BCE Loss: 1.0221779346466064\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 5.427596569061279 | KNN Loss: 4.382777214050293 | BCE Loss: 1.0448194742202759\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 5.366668701171875 | KNN Loss: 4.378049850463867 | BCE Loss: 0.9886189103126526\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 5.370730400085449 | KNN Loss: 4.3556342124938965 | BCE Loss: 1.0150964260101318\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 5.422802925109863 | KNN Loss: 4.372765064239502 | BCE Loss: 1.0500377416610718\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 5.4230241775512695 | KNN Loss: 4.3888983726501465 | BCE Loss: 1.034125804901123\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 5.433114528656006 | KNN Loss: 4.404086589813232 | BCE Loss: 1.029028058052063\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 5.392946720123291 | KNN Loss: 4.379800319671631 | BCE Loss: 1.0131462812423706\n",
      "Epoch   120: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 5.420276641845703 | KNN Loss: 4.367298126220703 | BCE Loss: 1.052978515625\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 5.392543315887451 | KNN Loss: 4.380183219909668 | BCE Loss: 1.0123599767684937\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 5.435039043426514 | KNN Loss: 4.407301902770996 | BCE Loss: 1.0277371406555176\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 5.42006254196167 | KNN Loss: 4.3935017585754395 | BCE Loss: 1.0265607833862305\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 5.360273838043213 | KNN Loss: 4.344629287719727 | BCE Loss: 1.0156446695327759\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 5.411006450653076 | KNN Loss: 4.411413192749023 | BCE Loss: 0.999593198299408\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 5.419936180114746 | KNN Loss: 4.380921840667725 | BCE Loss: 1.0390145778656006\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 5.406533241271973 | KNN Loss: 4.364126682281494 | BCE Loss: 1.0424065589904785\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 5.383927345275879 | KNN Loss: 4.352141857147217 | BCE Loss: 1.0317853689193726\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 5.377206802368164 | KNN Loss: 4.3675665855407715 | BCE Loss: 1.009640097618103\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 5.412722587585449 | KNN Loss: 4.389071464538574 | BCE Loss: 1.023651361465454\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 5.418059349060059 | KNN Loss: 4.410591125488281 | BCE Loss: 1.0074682235717773\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 5.369157791137695 | KNN Loss: 4.352931499481201 | BCE Loss: 1.016226053237915\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 5.4263176918029785 | KNN Loss: 4.383144378662109 | BCE Loss: 1.0431734323501587\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 5.432893753051758 | KNN Loss: 4.406566143035889 | BCE Loss: 1.0263276100158691\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 5.3817243576049805 | KNN Loss: 4.370092868804932 | BCE Loss: 1.0116314888000488\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 5.441483497619629 | KNN Loss: 4.3960466384887695 | BCE Loss: 1.0454366207122803\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 5.408814907073975 | KNN Loss: 4.377239227294922 | BCE Loss: 1.0315756797790527\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 5.389039039611816 | KNN Loss: 4.376014232635498 | BCE Loss: 1.0130250453948975\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 5.403885841369629 | KNN Loss: 4.362118721008301 | BCE Loss: 1.041766881942749\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 5.406071662902832 | KNN Loss: 4.377854347229004 | BCE Loss: 1.028217077255249\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 5.422094345092773 | KNN Loss: 4.367853164672852 | BCE Loss: 1.0542411804199219\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 5.371287822723389 | KNN Loss: 4.363338947296143 | BCE Loss: 1.007948875427246\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 5.401939868927002 | KNN Loss: 4.375430107116699 | BCE Loss: 1.0265097618103027\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 5.4054274559021 | KNN Loss: 4.385487079620361 | BCE Loss: 1.0199403762817383\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 5.404243469238281 | KNN Loss: 4.384546756744385 | BCE Loss: 1.0196967124938965\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 5.431267738342285 | KNN Loss: 4.3943562507629395 | BCE Loss: 1.0369113683700562\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 5.437930107116699 | KNN Loss: 4.426461696624756 | BCE Loss: 1.0114681720733643\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 5.405114650726318 | KNN Loss: 4.378046035766602 | BCE Loss: 1.0270686149597168\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 5.445012092590332 | KNN Loss: 4.3896636962890625 | BCE Loss: 1.05534827709198\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 5.394954681396484 | KNN Loss: 4.373518943786621 | BCE Loss: 1.0214357376098633\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 5.420995712280273 | KNN Loss: 4.3938798904418945 | BCE Loss: 1.027116060256958\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 5.423399448394775 | KNN Loss: 4.387436389923096 | BCE Loss: 1.0359631776809692\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 5.406637191772461 | KNN Loss: 4.374868392944336 | BCE Loss: 1.0317689180374146\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 5.3673095703125 | KNN Loss: 4.354676246643066 | BCE Loss: 1.012633204460144\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 5.401800155639648 | KNN Loss: 4.3703694343566895 | BCE Loss: 1.0314308404922485\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 5.420578956604004 | KNN Loss: 4.374593257904053 | BCE Loss: 1.045985460281372\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 5.371191024780273 | KNN Loss: 4.354492664337158 | BCE Loss: 1.0166981220245361\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 5.360208988189697 | KNN Loss: 4.344677448272705 | BCE Loss: 1.0155315399169922\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 5.40538215637207 | KNN Loss: 4.388260364532471 | BCE Loss: 1.0171217918395996\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 5.337708950042725 | KNN Loss: 4.339685440063477 | BCE Loss: 0.9980236291885376\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 5.392284393310547 | KNN Loss: 4.3528289794921875 | BCE Loss: 1.0394554138183594\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 5.396934986114502 | KNN Loss: 4.373416423797607 | BCE Loss: 1.0235185623168945\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 5.443426132202148 | KNN Loss: 4.407806873321533 | BCE Loss: 1.0356194972991943\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 5.389472961425781 | KNN Loss: 4.357260704040527 | BCE Loss: 1.0322120189666748\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 5.359530448913574 | KNN Loss: 4.355196475982666 | BCE Loss: 1.0043339729309082\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 5.40034294128418 | KNN Loss: 4.372463226318359 | BCE Loss: 1.0278797149658203\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 5.4336700439453125 | KNN Loss: 4.364173889160156 | BCE Loss: 1.0694963932037354\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 5.430048942565918 | KNN Loss: 4.3872480392456055 | BCE Loss: 1.0428011417388916\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 5.416121482849121 | KNN Loss: 4.37095832824707 | BCE Loss: 1.0451632738113403\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 5.398343086242676 | KNN Loss: 4.380162239074707 | BCE Loss: 1.0181808471679688\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 5.392884254455566 | KNN Loss: 4.37730598449707 | BCE Loss: 1.0155785083770752\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 5.390539169311523 | KNN Loss: 4.3553032875061035 | BCE Loss: 1.035236120223999\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 5.384775161743164 | KNN Loss: 4.378740310668945 | BCE Loss: 1.0060348510742188\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 5.412144184112549 | KNN Loss: 4.3828325271606445 | BCE Loss: 1.0293116569519043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 5.401525020599365 | KNN Loss: 4.372458457946777 | BCE Loss: 1.029066562652588\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 5.400982856750488 | KNN Loss: 4.3750529289245605 | BCE Loss: 1.0259300470352173\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 5.39568567276001 | KNN Loss: 4.378140449523926 | BCE Loss: 1.0175453424453735\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 5.411736488342285 | KNN Loss: 4.376201629638672 | BCE Loss: 1.0355347394943237\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 5.5009050369262695 | KNN Loss: 4.433978080749512 | BCE Loss: 1.066927194595337\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 5.404448509216309 | KNN Loss: 4.362146377563477 | BCE Loss: 1.042301893234253\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 5.410538196563721 | KNN Loss: 4.364859580993652 | BCE Loss: 1.0456786155700684\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 5.415460109710693 | KNN Loss: 4.373363971710205 | BCE Loss: 1.0420961380004883\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 5.360787391662598 | KNN Loss: 4.342895984649658 | BCE Loss: 1.0178916454315186\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 5.418218612670898 | KNN Loss: 4.405728340148926 | BCE Loss: 1.0124902725219727\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 5.393179416656494 | KNN Loss: 4.370258808135986 | BCE Loss: 1.0229204893112183\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 5.3858561515808105 | KNN Loss: 4.35612678527832 | BCE Loss: 1.0297293663024902\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 5.4460129737854 | KNN Loss: 4.381831169128418 | BCE Loss: 1.0641818046569824\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 5.411665439605713 | KNN Loss: 4.398541450500488 | BCE Loss: 1.0131239891052246\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 5.426800727844238 | KNN Loss: 4.404206275939941 | BCE Loss: 1.0225943326950073\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 5.412033557891846 | KNN Loss: 4.383261203765869 | BCE Loss: 1.0287723541259766\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 5.403648376464844 | KNN Loss: 4.380926609039307 | BCE Loss: 1.0227220058441162\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 5.391656875610352 | KNN Loss: 4.3679423332214355 | BCE Loss: 1.023714303970337\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 5.427244663238525 | KNN Loss: 4.38088321685791 | BCE Loss: 1.0463615655899048\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 5.402431488037109 | KNN Loss: 4.36029052734375 | BCE Loss: 1.042141079902649\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 5.362075328826904 | KNN Loss: 4.34804105758667 | BCE Loss: 1.0140342712402344\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 5.414811134338379 | KNN Loss: 4.378754138946533 | BCE Loss: 1.0360567569732666\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 5.409143447875977 | KNN Loss: 4.385863780975342 | BCE Loss: 1.0232799053192139\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 5.362464427947998 | KNN Loss: 4.359257698059082 | BCE Loss: 1.003206729888916\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 5.427529811859131 | KNN Loss: 4.397303104400635 | BCE Loss: 1.0302265882492065\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 5.445477485656738 | KNN Loss: 4.381453037261963 | BCE Loss: 1.0640246868133545\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 5.394400596618652 | KNN Loss: 4.383655548095703 | BCE Loss: 1.0107451677322388\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 5.374167442321777 | KNN Loss: 4.367222785949707 | BCE Loss: 1.0069444179534912\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 5.380342960357666 | KNN Loss: 4.359328269958496 | BCE Loss: 1.02101469039917\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 5.376676559448242 | KNN Loss: 4.357698440551758 | BCE Loss: 1.0189778804779053\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 5.431077480316162 | KNN Loss: 4.387484073638916 | BCE Loss: 1.043593406677246\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 5.441312789916992 | KNN Loss: 4.390286445617676 | BCE Loss: 1.0510265827178955\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 5.380314826965332 | KNN Loss: 4.364996433258057 | BCE Loss: 1.0153182744979858\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 5.41057014465332 | KNN Loss: 4.387509346008301 | BCE Loss: 1.02306067943573\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 5.38233757019043 | KNN Loss: 4.369771480560303 | BCE Loss: 1.012566089630127\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 5.37783145904541 | KNN Loss: 4.3359599113464355 | BCE Loss: 1.0418713092803955\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 5.3809309005737305 | KNN Loss: 4.379859924316406 | BCE Loss: 1.0010707378387451\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 5.381560325622559 | KNN Loss: 4.3498029708862305 | BCE Loss: 1.0317572355270386\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 5.442441940307617 | KNN Loss: 4.388602256774902 | BCE Loss: 1.053839921951294\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 5.386825084686279 | KNN Loss: 4.354161262512207 | BCE Loss: 1.0326638221740723\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 5.372354507446289 | KNN Loss: 4.347225189208984 | BCE Loss: 1.0251291990280151\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 5.441815376281738 | KNN Loss: 4.381763935089111 | BCE Loss: 1.0600513219833374\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 5.428842067718506 | KNN Loss: 4.397050380706787 | BCE Loss: 1.0317915678024292\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 5.349803924560547 | KNN Loss: 4.362962245941162 | BCE Loss: 0.9868419170379639\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 5.405815124511719 | KNN Loss: 4.3727264404296875 | BCE Loss: 1.0330885648727417\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 5.372676372528076 | KNN Loss: 4.361658573150635 | BCE Loss: 1.0110177993774414\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 5.4346394538879395 | KNN Loss: 4.399045944213867 | BCE Loss: 1.0355936288833618\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 5.398563385009766 | KNN Loss: 4.356388568878174 | BCE Loss: 1.042175054550171\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 5.390773773193359 | KNN Loss: 4.378907203674316 | BCE Loss: 1.0118666887283325\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 5.399771690368652 | KNN Loss: 4.374163627624512 | BCE Loss: 1.0256083011627197\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 5.358649253845215 | KNN Loss: 4.362658500671387 | BCE Loss: 0.9959907531738281\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 5.426032066345215 | KNN Loss: 4.376612186431885 | BCE Loss: 1.04941987991333\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 5.415583610534668 | KNN Loss: 4.365713119506836 | BCE Loss: 1.0498703718185425\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 5.37209415435791 | KNN Loss: 4.355591773986816 | BCE Loss: 1.0165021419525146\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 5.392355918884277 | KNN Loss: 4.386219501495361 | BCE Loss: 1.006136417388916\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 5.421638011932373 | KNN Loss: 4.391885757446289 | BCE Loss: 1.029752254486084\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 5.418200492858887 | KNN Loss: 4.406761169433594 | BCE Loss: 1.0114390850067139\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 5.430922508239746 | KNN Loss: 4.394320487976074 | BCE Loss: 1.0366017818450928\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 5.394763469696045 | KNN Loss: 4.374962329864502 | BCE Loss: 1.0198010206222534\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 5.431269645690918 | KNN Loss: 4.4389166831970215 | BCE Loss: 0.9923532009124756\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 5.422246932983398 | KNN Loss: 4.387323379516602 | BCE Loss: 1.0349233150482178\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 5.438231468200684 | KNN Loss: 4.398782730102539 | BCE Loss: 1.0394487380981445\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 5.391401290893555 | KNN Loss: 4.365891456604004 | BCE Loss: 1.0255100727081299\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 5.41474723815918 | KNN Loss: 4.371384143829346 | BCE Loss: 1.043363094329834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 5.46508264541626 | KNN Loss: 4.4123358726501465 | BCE Loss: 1.0527468919754028\n",
      "Epoch   140: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 5.3770575523376465 | KNN Loss: 4.357994079589844 | BCE Loss: 1.0190633535385132\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 5.377534866333008 | KNN Loss: 4.362887382507324 | BCE Loss: 1.0146477222442627\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 5.402493953704834 | KNN Loss: 4.35179328918457 | BCE Loss: 1.0507006645202637\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 5.417993068695068 | KNN Loss: 4.349936008453369 | BCE Loss: 1.0680571794509888\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 5.434355735778809 | KNN Loss: 4.37747049331665 | BCE Loss: 1.0568854808807373\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 5.364978313446045 | KNN Loss: 4.356512546539307 | BCE Loss: 1.0084656476974487\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 5.404054641723633 | KNN Loss: 4.383326530456543 | BCE Loss: 1.020728349685669\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 5.4494309425354 | KNN Loss: 4.359196186065674 | BCE Loss: 1.0902347564697266\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 5.421478271484375 | KNN Loss: 4.396204471588135 | BCE Loss: 1.0252736806869507\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 5.4552507400512695 | KNN Loss: 4.393676280975342 | BCE Loss: 1.0615742206573486\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 5.441656112670898 | KNN Loss: 4.402512550354004 | BCE Loss: 1.039143443107605\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 5.430216312408447 | KNN Loss: 4.37803840637207 | BCE Loss: 1.052177906036377\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 5.379237651824951 | KNN Loss: 4.380401134490967 | BCE Loss: 0.9988364577293396\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 5.4623565673828125 | KNN Loss: 4.443302154541016 | BCE Loss: 1.0190541744232178\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 5.4053754806518555 | KNN Loss: 4.397270679473877 | BCE Loss: 1.0081050395965576\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 5.394204139709473 | KNN Loss: 4.388383865356445 | BCE Loss: 1.0058202743530273\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 5.439730167388916 | KNN Loss: 4.365503311157227 | BCE Loss: 1.074226975440979\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 5.380248546600342 | KNN Loss: 4.337639808654785 | BCE Loss: 1.042608618736267\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 5.392762184143066 | KNN Loss: 4.382319927215576 | BCE Loss: 1.0104422569274902\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 5.379520893096924 | KNN Loss: 4.372368335723877 | BCE Loss: 1.0071525573730469\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 5.383837699890137 | KNN Loss: 4.333480358123779 | BCE Loss: 1.0503575801849365\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 5.389269828796387 | KNN Loss: 4.354582786560059 | BCE Loss: 1.0346872806549072\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 5.400599479675293 | KNN Loss: 4.387434959411621 | BCE Loss: 1.013164758682251\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 5.3806352615356445 | KNN Loss: 4.372280120849609 | BCE Loss: 1.0083551406860352\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 5.387821197509766 | KNN Loss: 4.37686824798584 | BCE Loss: 1.0109531879425049\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 5.4210896492004395 | KNN Loss: 4.369901657104492 | BCE Loss: 1.0511879920959473\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 5.405890464782715 | KNN Loss: 4.407020092010498 | BCE Loss: 0.9988704323768616\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 5.392009735107422 | KNN Loss: 4.3697710037231445 | BCE Loss: 1.0222387313842773\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 5.385115623474121 | KNN Loss: 4.367718696594238 | BCE Loss: 1.017397165298462\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 5.391139984130859 | KNN Loss: 4.364391326904297 | BCE Loss: 1.0267488956451416\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 5.425302982330322 | KNN Loss: 4.390167236328125 | BCE Loss: 1.0351357460021973\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 5.392524719238281 | KNN Loss: 4.3804450035095215 | BCE Loss: 1.0120794773101807\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 5.409566879272461 | KNN Loss: 4.393154621124268 | BCE Loss: 1.0164122581481934\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 5.385329723358154 | KNN Loss: 4.36496114730835 | BCE Loss: 1.0203685760498047\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 5.411228179931641 | KNN Loss: 4.401909828186035 | BCE Loss: 1.0093181133270264\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 5.408766269683838 | KNN Loss: 4.383420467376709 | BCE Loss: 1.0253459215164185\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 5.440493583679199 | KNN Loss: 4.3754496574401855 | BCE Loss: 1.0650441646575928\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 5.389208793640137 | KNN Loss: 4.357795238494873 | BCE Loss: 1.0314135551452637\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 5.3767547607421875 | KNN Loss: 4.379812717437744 | BCE Loss: 0.9969418048858643\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 5.46811580657959 | KNN Loss: 4.446224689483643 | BCE Loss: 1.0218908786773682\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 5.373462677001953 | KNN Loss: 4.376859664916992 | BCE Loss: 0.9966031908988953\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 5.395997524261475 | KNN Loss: 4.361706256866455 | BCE Loss: 1.0342912673950195\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 5.4097514152526855 | KNN Loss: 4.362622261047363 | BCE Loss: 1.0471292734146118\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 5.426307678222656 | KNN Loss: 4.416247367858887 | BCE Loss: 1.0100605487823486\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 5.362776279449463 | KNN Loss: 4.357856750488281 | BCE Loss: 1.0049195289611816\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 5.367337703704834 | KNN Loss: 4.347982406616211 | BCE Loss: 1.0193551778793335\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 5.40403938293457 | KNN Loss: 4.380877494812012 | BCE Loss: 1.0231618881225586\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 5.3962602615356445 | KNN Loss: 4.378093242645264 | BCE Loss: 1.0181667804718018\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 5.361392021179199 | KNN Loss: 4.355673789978027 | BCE Loss: 1.0057181119918823\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 5.390030384063721 | KNN Loss: 4.382918357849121 | BCE Loss: 1.0071121454238892\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 5.409543037414551 | KNN Loss: 4.423179626464844 | BCE Loss: 0.986363410949707\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 5.419685363769531 | KNN Loss: 4.392237663269043 | BCE Loss: 1.0274479389190674\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 5.435092926025391 | KNN Loss: 4.38293981552124 | BCE Loss: 1.0521528720855713\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 5.388666152954102 | KNN Loss: 4.388979434967041 | BCE Loss: 0.9996869564056396\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 5.436285495758057 | KNN Loss: 4.4115824699401855 | BCE Loss: 1.024703025817871\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 5.41172456741333 | KNN Loss: 4.3930158615112305 | BCE Loss: 1.0187088251113892\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 5.464999198913574 | KNN Loss: 4.4085516929626465 | BCE Loss: 1.0564472675323486\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 5.416445732116699 | KNN Loss: 4.359477996826172 | BCE Loss: 1.0569677352905273\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 5.389632701873779 | KNN Loss: 4.362936019897461 | BCE Loss: 1.0266966819763184\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 5.390405654907227 | KNN Loss: 4.367476940155029 | BCE Loss: 1.0229287147521973\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 5.362656116485596 | KNN Loss: 4.35300874710083 | BCE Loss: 1.0096473693847656\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 5.4124755859375 | KNN Loss: 4.372785568237305 | BCE Loss: 1.0396897792816162\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 5.3586273193359375 | KNN Loss: 4.359142780303955 | BCE Loss: 0.9994847178459167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 5.381557464599609 | KNN Loss: 4.361090660095215 | BCE Loss: 1.0204668045043945\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 5.441189765930176 | KNN Loss: 4.418916702270508 | BCE Loss: 1.0222729444503784\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 5.433194160461426 | KNN Loss: 4.379369258880615 | BCE Loss: 1.0538249015808105\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 5.396218299865723 | KNN Loss: 4.360690593719482 | BCE Loss: 1.0355274677276611\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 5.433990478515625 | KNN Loss: 4.390612602233887 | BCE Loss: 1.0433779954910278\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 5.35537576675415 | KNN Loss: 4.355792045593262 | BCE Loss: 0.9995837211608887\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 5.406432151794434 | KNN Loss: 4.374722003936768 | BCE Loss: 1.031709909439087\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 5.4137773513793945 | KNN Loss: 4.35959529876709 | BCE Loss: 1.0541819334030151\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 5.337468147277832 | KNN Loss: 4.349059581756592 | BCE Loss: 0.9884085655212402\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 5.414093017578125 | KNN Loss: 4.386187553405762 | BCE Loss: 1.0279055833816528\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 5.352651596069336 | KNN Loss: 4.3424201011657715 | BCE Loss: 1.010231614112854\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 5.474268913269043 | KNN Loss: 4.418903350830078 | BCE Loss: 1.0553653240203857\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 5.44099235534668 | KNN Loss: 4.399288654327393 | BCE Loss: 1.0417039394378662\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 5.35552453994751 | KNN Loss: 4.336057186126709 | BCE Loss: 1.0194673538208008\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 5.383935451507568 | KNN Loss: 4.356383323669434 | BCE Loss: 1.0275522470474243\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 5.421144485473633 | KNN Loss: 4.368326187133789 | BCE Loss: 1.0528182983398438\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 5.441557884216309 | KNN Loss: 4.434233665466309 | BCE Loss: 1.007323980331421\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 5.4180097579956055 | KNN Loss: 4.3736252784729 | BCE Loss: 1.044384241104126\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 5.401872158050537 | KNN Loss: 4.372534275054932 | BCE Loss: 1.029338002204895\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 5.374514579772949 | KNN Loss: 4.354442596435547 | BCE Loss: 1.0200717449188232\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 5.483671188354492 | KNN Loss: 4.439834117889404 | BCE Loss: 1.043837308883667\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 5.4389753341674805 | KNN Loss: 4.418914318084717 | BCE Loss: 1.0200612545013428\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 5.4085845947265625 | KNN Loss: 4.378716468811035 | BCE Loss: 1.0298681259155273\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 5.39139986038208 | KNN Loss: 4.383001804351807 | BCE Loss: 1.0083979368209839\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 5.4200639724731445 | KNN Loss: 4.416073322296143 | BCE Loss: 1.0039905309677124\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 5.430063247680664 | KNN Loss: 4.39495849609375 | BCE Loss: 1.035104513168335\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 5.430177211761475 | KNN Loss: 4.3804731369018555 | BCE Loss: 1.0497040748596191\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 5.427848815917969 | KNN Loss: 4.383294105529785 | BCE Loss: 1.0445547103881836\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 5.351083278656006 | KNN Loss: 4.343058109283447 | BCE Loss: 1.0080251693725586\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 5.44720458984375 | KNN Loss: 4.414062976837158 | BCE Loss: 1.0331417322158813\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 5.409582138061523 | KNN Loss: 4.356454372406006 | BCE Loss: 1.0531277656555176\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 5.409859657287598 | KNN Loss: 4.347437381744385 | BCE Loss: 1.0624223947525024\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 5.357426643371582 | KNN Loss: 4.360297203063965 | BCE Loss: 0.9971296191215515\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 5.390743255615234 | KNN Loss: 4.382211208343506 | BCE Loss: 1.0085318088531494\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 5.451673984527588 | KNN Loss: 4.381307125091553 | BCE Loss: 1.0703669786453247\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 5.362240791320801 | KNN Loss: 4.343569278717041 | BCE Loss: 1.0186713933944702\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 5.40233039855957 | KNN Loss: 4.362113952636719 | BCE Loss: 1.040216326713562\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 5.448554039001465 | KNN Loss: 4.37822961807251 | BCE Loss: 1.070324182510376\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 5.441812515258789 | KNN Loss: 4.404024600982666 | BCE Loss: 1.0377881526947021\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 5.435023307800293 | KNN Loss: 4.416760444641113 | BCE Loss: 1.0182631015777588\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 5.421817779541016 | KNN Loss: 4.4136152267456055 | BCE Loss: 1.0082027912139893\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 5.4409332275390625 | KNN Loss: 4.3964948654174805 | BCE Loss: 1.044438362121582\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 5.425105094909668 | KNN Loss: 4.4026408195495605 | BCE Loss: 1.0224642753601074\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 5.429729461669922 | KNN Loss: 4.382805347442627 | BCE Loss: 1.046924114227295\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 5.452795505523682 | KNN Loss: 4.409631729125977 | BCE Loss: 1.0431636571884155\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 5.379485607147217 | KNN Loss: 4.359679698944092 | BCE Loss: 1.019805908203125\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 5.393098831176758 | KNN Loss: 4.348743438720703 | BCE Loss: 1.0443553924560547\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 5.416790008544922 | KNN Loss: 4.374978065490723 | BCE Loss: 1.0418121814727783\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 5.435606956481934 | KNN Loss: 4.3700714111328125 | BCE Loss: 1.065535306930542\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 5.447185039520264 | KNN Loss: 4.396336078643799 | BCE Loss: 1.0508488416671753\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 5.431488990783691 | KNN Loss: 4.414211750030518 | BCE Loss: 1.017277479171753\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 5.394839286804199 | KNN Loss: 4.385084629058838 | BCE Loss: 1.0097545385360718\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 5.387178421020508 | KNN Loss: 4.355427265167236 | BCE Loss: 1.0317509174346924\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 5.337883472442627 | KNN Loss: 4.338394641876221 | BCE Loss: 0.9994888305664062\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 5.41683292388916 | KNN Loss: 4.3881049156188965 | BCE Loss: 1.0287282466888428\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 5.413342475891113 | KNN Loss: 4.372276306152344 | BCE Loss: 1.041066288948059\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 5.393880367279053 | KNN Loss: 4.35526180267334 | BCE Loss: 1.0386186838150024\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 5.435351371765137 | KNN Loss: 4.420311450958252 | BCE Loss: 1.0150401592254639\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 5.40624475479126 | KNN Loss: 4.36805534362793 | BCE Loss: 1.03818941116333\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 5.368558406829834 | KNN Loss: 4.373887538909912 | BCE Loss: 0.9946706891059875\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 5.432718753814697 | KNN Loss: 4.392004013061523 | BCE Loss: 1.0407147407531738\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 5.388012409210205 | KNN Loss: 4.37111234664917 | BCE Loss: 1.0168999433517456\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 5.414196014404297 | KNN Loss: 4.369944095611572 | BCE Loss: 1.0442519187927246\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 5.444744110107422 | KNN Loss: 4.393184185028076 | BCE Loss: 1.0515599250793457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 5.374227523803711 | KNN Loss: 4.332513332366943 | BCE Loss: 1.0417141914367676\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 5.466549873352051 | KNN Loss: 4.419331073760986 | BCE Loss: 1.0472185611724854\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 5.41642427444458 | KNN Loss: 4.367187023162842 | BCE Loss: 1.0492371320724487\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 5.372176647186279 | KNN Loss: 4.364610195159912 | BCE Loss: 1.0075664520263672\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 5.442203521728516 | KNN Loss: 4.395930290222168 | BCE Loss: 1.0462734699249268\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 5.4208831787109375 | KNN Loss: 4.380406379699707 | BCE Loss: 1.04047691822052\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 5.370200157165527 | KNN Loss: 4.351572036743164 | BCE Loss: 1.0186280012130737\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 5.415696144104004 | KNN Loss: 4.398961544036865 | BCE Loss: 1.0167348384857178\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 5.473824501037598 | KNN Loss: 4.448003768920898 | BCE Loss: 1.0258209705352783\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 5.411529541015625 | KNN Loss: 4.394659042358398 | BCE Loss: 1.0168707370758057\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 5.370542526245117 | KNN Loss: 4.364976406097412 | BCE Loss: 1.0055663585662842\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 5.381365776062012 | KNN Loss: 4.353791236877441 | BCE Loss: 1.0275743007659912\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 5.356791019439697 | KNN Loss: 4.342653751373291 | BCE Loss: 1.0141373872756958\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 5.424499034881592 | KNN Loss: 4.387156963348389 | BCE Loss: 1.0373421907424927\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 5.375819206237793 | KNN Loss: 4.361813068389893 | BCE Loss: 1.0140061378479004\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 5.418491840362549 | KNN Loss: 4.38617467880249 | BCE Loss: 1.032317042350769\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 5.373846054077148 | KNN Loss: 4.358520030975342 | BCE Loss: 1.0153262615203857\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 5.464034080505371 | KNN Loss: 4.41636848449707 | BCE Loss: 1.0476657152175903\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 5.3904805183410645 | KNN Loss: 4.377808094024658 | BCE Loss: 1.0126724243164062\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 5.381418704986572 | KNN Loss: 4.360387325286865 | BCE Loss: 1.021031379699707\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 5.409103870391846 | KNN Loss: 4.378903388977051 | BCE Loss: 1.0302006006240845\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 5.419857978820801 | KNN Loss: 4.393946647644043 | BCE Loss: 1.025911569595337\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 5.4648895263671875 | KNN Loss: 4.39688777923584 | BCE Loss: 1.0680015087127686\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 5.362239837646484 | KNN Loss: 4.354791164398193 | BCE Loss: 1.007448673248291\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 5.349682807922363 | KNN Loss: 4.3400959968566895 | BCE Loss: 1.0095865726470947\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 5.387516021728516 | KNN Loss: 4.379681587219238 | BCE Loss: 1.0078341960906982\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 5.390974521636963 | KNN Loss: 4.355236053466797 | BCE Loss: 1.035738468170166\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 5.395960807800293 | KNN Loss: 4.356992244720459 | BCE Loss: 1.038968563079834\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 5.362430095672607 | KNN Loss: 4.36237907409668 | BCE Loss: 1.0000511407852173\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 5.392639636993408 | KNN Loss: 4.354424476623535 | BCE Loss: 1.038215160369873\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 5.430054187774658 | KNN Loss: 4.407922744750977 | BCE Loss: 1.0221315622329712\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 5.428777694702148 | KNN Loss: 4.3991522789001465 | BCE Loss: 1.029625415802002\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 5.4135637283325195 | KNN Loss: 4.375515937805176 | BCE Loss: 1.0380475521087646\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 5.400707244873047 | KNN Loss: 4.358504772186279 | BCE Loss: 1.0422027111053467\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 5.362155437469482 | KNN Loss: 4.363186836242676 | BCE Loss: 0.998968780040741\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 5.365638256072998 | KNN Loss: 4.345485687255859 | BCE Loss: 1.0201525688171387\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 5.36232852935791 | KNN Loss: 4.336257457733154 | BCE Loss: 1.0260709524154663\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 5.394916534423828 | KNN Loss: 4.377481937408447 | BCE Loss: 1.0174344778060913\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 5.376358985900879 | KNN Loss: 4.358389854431152 | BCE Loss: 1.0179688930511475\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 5.398090362548828 | KNN Loss: 4.3600006103515625 | BCE Loss: 1.038089632987976\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 5.395888328552246 | KNN Loss: 4.358695030212402 | BCE Loss: 1.0371932983398438\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 5.447782516479492 | KNN Loss: 4.425161361694336 | BCE Loss: 1.0226211547851562\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 5.389280319213867 | KNN Loss: 4.365654468536377 | BCE Loss: 1.0236258506774902\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 5.358309745788574 | KNN Loss: 4.368210315704346 | BCE Loss: 0.9900994300842285\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 5.437664031982422 | KNN Loss: 4.413765907287598 | BCE Loss: 1.0238980054855347\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 5.425819396972656 | KNN Loss: 4.396896839141846 | BCE Loss: 1.0289227962493896\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 5.402022361755371 | KNN Loss: 4.370625019073486 | BCE Loss: 1.0313973426818848\n",
      "Epoch   169: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 5.385623455047607 | KNN Loss: 4.368902206420898 | BCE Loss: 1.016721248626709\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 5.410770416259766 | KNN Loss: 4.378662109375 | BCE Loss: 1.0321085453033447\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 5.465456962585449 | KNN Loss: 4.403493404388428 | BCE Loss: 1.0619637966156006\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 5.4020466804504395 | KNN Loss: 4.398734092712402 | BCE Loss: 1.0033127069473267\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 5.415372848510742 | KNN Loss: 4.383235931396484 | BCE Loss: 1.0321369171142578\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 5.4471001625061035 | KNN Loss: 4.43004035949707 | BCE Loss: 1.0170598030090332\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 5.3975982666015625 | KNN Loss: 4.357544422149658 | BCE Loss: 1.0400540828704834\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 5.410707473754883 | KNN Loss: 4.396152019500732 | BCE Loss: 1.0145556926727295\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 5.4390974044799805 | KNN Loss: 4.402410984039307 | BCE Loss: 1.0366864204406738\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 5.402800559997559 | KNN Loss: 4.412169933319092 | BCE Loss: 0.9906307458877563\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 5.367812156677246 | KNN Loss: 4.347842216491699 | BCE Loss: 1.0199697017669678\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 5.375149726867676 | KNN Loss: 4.355334758758545 | BCE Loss: 1.01981520652771\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 5.399494171142578 | KNN Loss: 4.387628555297852 | BCE Loss: 1.0118653774261475\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 5.412078857421875 | KNN Loss: 4.383693218231201 | BCE Loss: 1.0283854007720947\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 5.416006088256836 | KNN Loss: 4.384854793548584 | BCE Loss: 1.031151294708252\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 5.446717262268066 | KNN Loss: 4.430434226989746 | BCE Loss: 1.0162827968597412\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 5.433372974395752 | KNN Loss: 4.402599334716797 | BCE Loss: 1.0307735204696655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 5.36831521987915 | KNN Loss: 4.35055685043335 | BCE Loss: 1.0177583694458008\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 5.39324426651001 | KNN Loss: 4.377485275268555 | BCE Loss: 1.015758991241455\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 5.434553146362305 | KNN Loss: 4.412121295928955 | BCE Loss: 1.0224320888519287\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 5.408450603485107 | KNN Loss: 4.386533737182617 | BCE Loss: 1.0219169855117798\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 5.4300312995910645 | KNN Loss: 4.383147716522217 | BCE Loss: 1.046883463859558\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 5.394927978515625 | KNN Loss: 4.348963260650635 | BCE Loss: 1.0459644794464111\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 5.398402214050293 | KNN Loss: 4.359979152679443 | BCE Loss: 1.0384230613708496\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 5.427352428436279 | KNN Loss: 4.399730682373047 | BCE Loss: 1.0276217460632324\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 5.4060282707214355 | KNN Loss: 4.383419513702393 | BCE Loss: 1.0226088762283325\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 5.359567165374756 | KNN Loss: 4.3543524742126465 | BCE Loss: 1.0052145719528198\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 5.468036651611328 | KNN Loss: 4.415375232696533 | BCE Loss: 1.0526611804962158\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 5.361266613006592 | KNN Loss: 4.347268581390381 | BCE Loss: 1.013998031616211\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 5.446846961975098 | KNN Loss: 4.420854091644287 | BCE Loss: 1.0259929895401\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 5.3880181312561035 | KNN Loss: 4.36073637008667 | BCE Loss: 1.0272818803787231\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 5.382186412811279 | KNN Loss: 4.368347644805908 | BCE Loss: 1.013838768005371\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 5.41473388671875 | KNN Loss: 4.384193420410156 | BCE Loss: 1.0305403470993042\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 5.455927848815918 | KNN Loss: 4.402738094329834 | BCE Loss: 1.0531895160675049\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 5.419751167297363 | KNN Loss: 4.364504814147949 | BCE Loss: 1.055246114730835\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 5.38065242767334 | KNN Loss: 4.350675582885742 | BCE Loss: 1.0299766063690186\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 5.368175029754639 | KNN Loss: 4.341599464416504 | BCE Loss: 1.0265756845474243\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 5.404829025268555 | KNN Loss: 4.397324085235596 | BCE Loss: 1.007504940032959\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 5.3661370277404785 | KNN Loss: 4.337437152862549 | BCE Loss: 1.0286998748779297\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 5.467485427856445 | KNN Loss: 4.424511909484863 | BCE Loss: 1.0429737567901611\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 5.470987319946289 | KNN Loss: 4.419474124908447 | BCE Loss: 1.0515129566192627\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 5.411969184875488 | KNN Loss: 4.374301433563232 | BCE Loss: 1.0376677513122559\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 5.4281158447265625 | KNN Loss: 4.3570556640625 | BCE Loss: 1.0710601806640625\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 5.373600006103516 | KNN Loss: 4.3607563972473145 | BCE Loss: 1.0128436088562012\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 5.395063877105713 | KNN Loss: 4.373601913452148 | BCE Loss: 1.021461844444275\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 5.353013515472412 | KNN Loss: 4.3304667472839355 | BCE Loss: 1.0225467681884766\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 5.387656211853027 | KNN Loss: 4.371161460876465 | BCE Loss: 1.0164949893951416\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 5.4123945236206055 | KNN Loss: 4.379959583282471 | BCE Loss: 1.0324349403381348\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 5.44785213470459 | KNN Loss: 4.396722316741943 | BCE Loss: 1.0511298179626465\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 5.385236740112305 | KNN Loss: 4.350952625274658 | BCE Loss: 1.0342843532562256\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 5.3712592124938965 | KNN Loss: 4.362174987792969 | BCE Loss: 1.0090843439102173\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 5.364789009094238 | KNN Loss: 4.34810209274292 | BCE Loss: 1.0166869163513184\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 5.343034744262695 | KNN Loss: 4.351613521575928 | BCE Loss: 0.991421103477478\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 5.357889175415039 | KNN Loss: 4.370366096496582 | BCE Loss: 0.9875233173370361\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 5.397607803344727 | KNN Loss: 4.378281593322754 | BCE Loss: 1.0193259716033936\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 5.419562816619873 | KNN Loss: 4.385442733764648 | BCE Loss: 1.0341200828552246\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 5.385207176208496 | KNN Loss: 4.3568620681762695 | BCE Loss: 1.028344988822937\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 5.394948482513428 | KNN Loss: 4.387238025665283 | BCE Loss: 1.007710576057434\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 5.414045333862305 | KNN Loss: 4.403329849243164 | BCE Loss: 1.0107154846191406\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 5.379484176635742 | KNN Loss: 4.354382514953613 | BCE Loss: 1.0251017808914185\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 5.382994651794434 | KNN Loss: 4.349799156188965 | BCE Loss: 1.0331952571868896\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 5.433356285095215 | KNN Loss: 4.386423587799072 | BCE Loss: 1.0469326972961426\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 5.430539131164551 | KNN Loss: 4.383451461791992 | BCE Loss: 1.0470874309539795\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 5.366544246673584 | KNN Loss: 4.345423698425293 | BCE Loss: 1.0211204290390015\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 5.406519889831543 | KNN Loss: 4.380937576293945 | BCE Loss: 1.0255824327468872\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 5.418709754943848 | KNN Loss: 4.397854328155518 | BCE Loss: 1.02085542678833\n",
      "Epoch   180: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 5.3690185546875 | KNN Loss: 4.369017124176025 | BCE Loss: 1.000001311302185\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 5.381877422332764 | KNN Loss: 4.351315975189209 | BCE Loss: 1.0305613279342651\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 5.43553352355957 | KNN Loss: 4.369631290435791 | BCE Loss: 1.0659023523330688\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 5.401066780090332 | KNN Loss: 4.362029552459717 | BCE Loss: 1.0390374660491943\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 5.416276454925537 | KNN Loss: 4.374884605407715 | BCE Loss: 1.0413918495178223\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 5.408402919769287 | KNN Loss: 4.384278297424316 | BCE Loss: 1.0241246223449707\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 5.359503746032715 | KNN Loss: 4.340182304382324 | BCE Loss: 1.0193215608596802\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 5.3801984786987305 | KNN Loss: 4.354780197143555 | BCE Loss: 1.0254185199737549\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 5.391379356384277 | KNN Loss: 4.379859447479248 | BCE Loss: 1.0115200281143188\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 5.438154220581055 | KNN Loss: 4.38895320892334 | BCE Loss: 1.0492010116577148\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 5.40986442565918 | KNN Loss: 4.381864547729492 | BCE Loss: 1.027999758720398\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 5.4281005859375 | KNN Loss: 4.37909460067749 | BCE Loss: 1.0490057468414307\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 5.349142074584961 | KNN Loss: 4.354086399078369 | BCE Loss: 0.9950554966926575\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 5.358980655670166 | KNN Loss: 4.362545013427734 | BCE Loss: 0.9964357614517212\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 5.4099321365356445 | KNN Loss: 4.386894702911377 | BCE Loss: 1.0230374336242676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 5.415480613708496 | KNN Loss: 4.400487422943115 | BCE Loss: 1.0149930715560913\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 5.410183906555176 | KNN Loss: 4.366849422454834 | BCE Loss: 1.0433344841003418\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 5.380431652069092 | KNN Loss: 4.378454685211182 | BCE Loss: 1.0019769668579102\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 5.374417304992676 | KNN Loss: 4.356521129608154 | BCE Loss: 1.0178959369659424\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 5.435103893280029 | KNN Loss: 4.4098992347717285 | BCE Loss: 1.0252045392990112\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 5.423579216003418 | KNN Loss: 4.38866662979126 | BCE Loss: 1.0349128246307373\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 5.356777191162109 | KNN Loss: 4.344326496124268 | BCE Loss: 1.0124504566192627\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 5.420635223388672 | KNN Loss: 4.357544898986816 | BCE Loss: 1.0630900859832764\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 5.399151802062988 | KNN Loss: 4.398204326629639 | BCE Loss: 1.0009477138519287\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 5.427252769470215 | KNN Loss: 4.374877452850342 | BCE Loss: 1.0523755550384521\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 5.437055587768555 | KNN Loss: 4.392159461975098 | BCE Loss: 1.044896125793457\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 5.401524543762207 | KNN Loss: 4.395692348480225 | BCE Loss: 1.0058324337005615\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 5.399940490722656 | KNN Loss: 4.36956787109375 | BCE Loss: 1.0303723812103271\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 5.357234954833984 | KNN Loss: 4.360416889190674 | BCE Loss: 0.9968181848526001\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 5.426481246948242 | KNN Loss: 4.396408557891846 | BCE Loss: 1.0300729274749756\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 5.352555751800537 | KNN Loss: 4.34758186340332 | BCE Loss: 1.0049738883972168\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 5.452873706817627 | KNN Loss: 4.391685962677002 | BCE Loss: 1.0611876249313354\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 5.4610772132873535 | KNN Loss: 4.418821811676025 | BCE Loss: 1.0422554016113281\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 5.407079219818115 | KNN Loss: 4.368886947631836 | BCE Loss: 1.0381921529769897\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 5.391608238220215 | KNN Loss: 4.379305362701416 | BCE Loss: 1.012303113937378\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 5.351698398590088 | KNN Loss: 4.339818000793457 | BCE Loss: 1.0118803977966309\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 5.405666351318359 | KNN Loss: 4.369307994842529 | BCE Loss: 1.03635835647583\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 5.414972305297852 | KNN Loss: 4.402531623840332 | BCE Loss: 1.0124406814575195\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 5.375152587890625 | KNN Loss: 4.372225761413574 | BCE Loss: 1.0029268264770508\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 5.435358047485352 | KNN Loss: 4.397621154785156 | BCE Loss: 1.0377370119094849\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 5.394829750061035 | KNN Loss: 4.340850830078125 | BCE Loss: 1.0539789199829102\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 5.386501789093018 | KNN Loss: 4.374242305755615 | BCE Loss: 1.0122594833374023\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 5.395349502563477 | KNN Loss: 4.358102798461914 | BCE Loss: 1.0372469425201416\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 5.45961856842041 | KNN Loss: 4.414913177490234 | BCE Loss: 1.0447056293487549\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 5.409065246582031 | KNN Loss: 4.356906414031982 | BCE Loss: 1.0521588325500488\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 5.397915840148926 | KNN Loss: 4.382144451141357 | BCE Loss: 1.0157711505889893\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 5.4225640296936035 | KNN Loss: 4.412330627441406 | BCE Loss: 1.0102334022521973\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 5.398096561431885 | KNN Loss: 4.370683193206787 | BCE Loss: 1.0274134874343872\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 5.386622905731201 | KNN Loss: 4.3422532081604 | BCE Loss: 1.0443696975708008\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 5.403176307678223 | KNN Loss: 4.364325523376465 | BCE Loss: 1.0388507843017578\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 5.364991188049316 | KNN Loss: 4.349951267242432 | BCE Loss: 1.0150399208068848\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 5.444913864135742 | KNN Loss: 4.380571365356445 | BCE Loss: 1.0643422603607178\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 5.401786804199219 | KNN Loss: 4.372853755950928 | BCE Loss: 1.0289332866668701\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 5.34013032913208 | KNN Loss: 4.330501079559326 | BCE Loss: 1.0096293687820435\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 5.422328472137451 | KNN Loss: 4.393308162689209 | BCE Loss: 1.0290203094482422\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 5.427223205566406 | KNN Loss: 4.389536380767822 | BCE Loss: 1.0376869440078735\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 5.420952320098877 | KNN Loss: 4.399175643920898 | BCE Loss: 1.021776795387268\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 5.399237632751465 | KNN Loss: 4.360105037689209 | BCE Loss: 1.0391323566436768\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 5.369199275970459 | KNN Loss: 4.369566440582275 | BCE Loss: 0.9996328353881836\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 5.383903980255127 | KNN Loss: 4.374659061431885 | BCE Loss: 1.0092449188232422\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 5.444031715393066 | KNN Loss: 4.432051658630371 | BCE Loss: 1.0119802951812744\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 5.395355224609375 | KNN Loss: 4.373203277587891 | BCE Loss: 1.0221521854400635\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 5.430772304534912 | KNN Loss: 4.401506423950195 | BCE Loss: 1.0292657613754272\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 5.381952285766602 | KNN Loss: 4.34481143951416 | BCE Loss: 1.0371408462524414\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 5.430775165557861 | KNN Loss: 4.363102912902832 | BCE Loss: 1.0676722526550293\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 5.380474090576172 | KNN Loss: 4.35162878036499 | BCE Loss: 1.0288454294204712\n",
      "Epoch   191: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 5.3929443359375 | KNN Loss: 4.370151519775391 | BCE Loss: 1.0227925777435303\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 5.426503658294678 | KNN Loss: 4.392754554748535 | BCE Loss: 1.0337491035461426\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 5.410833358764648 | KNN Loss: 4.378086566925049 | BCE Loss: 1.0327467918395996\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 5.435666084289551 | KNN Loss: 4.405484199523926 | BCE Loss: 1.0301817655563354\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 5.416893005371094 | KNN Loss: 4.3732500076293945 | BCE Loss: 1.0436427593231201\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 5.372056007385254 | KNN Loss: 4.339909553527832 | BCE Loss: 1.032146692276001\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 5.3676981925964355 | KNN Loss: 4.35241174697876 | BCE Loss: 1.0152865648269653\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 5.373259544372559 | KNN Loss: 4.344816207885742 | BCE Loss: 1.0284435749053955\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 5.435911178588867 | KNN Loss: 4.409788608551025 | BCE Loss: 1.0261223316192627\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 5.405209541320801 | KNN Loss: 4.364155292510986 | BCE Loss: 1.0410544872283936\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 5.420831680297852 | KNN Loss: 4.359428405761719 | BCE Loss: 1.0614033937454224\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 5.412012100219727 | KNN Loss: 4.367058753967285 | BCE Loss: 1.0449535846710205\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 5.340778350830078 | KNN Loss: 4.342644691467285 | BCE Loss: 0.9981338977813721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 5.371336460113525 | KNN Loss: 4.355814456939697 | BCE Loss: 1.0155220031738281\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 5.439828395843506 | KNN Loss: 4.3843488693237305 | BCE Loss: 1.0554795265197754\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 5.3485426902771 | KNN Loss: 4.339293956756592 | BCE Loss: 1.0092486143112183\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 5.382338523864746 | KNN Loss: 4.373823165893555 | BCE Loss: 1.0085151195526123\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 5.3646440505981445 | KNN Loss: 4.35214376449585 | BCE Loss: 1.012500286102295\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 5.423469543457031 | KNN Loss: 4.366580963134766 | BCE Loss: 1.0568888187408447\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 5.3954315185546875 | KNN Loss: 4.371198654174805 | BCE Loss: 1.0242326259613037\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 5.379033088684082 | KNN Loss: 4.36260461807251 | BCE Loss: 1.0164282321929932\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 5.412993431091309 | KNN Loss: 4.350124359130859 | BCE Loss: 1.0628691911697388\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 5.345829010009766 | KNN Loss: 4.333777904510498 | BCE Loss: 1.012050986289978\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 5.431396007537842 | KNN Loss: 4.380218505859375 | BCE Loss: 1.0511775016784668\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 5.418493747711182 | KNN Loss: 4.365505218505859 | BCE Loss: 1.0529886484146118\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 5.385565757751465 | KNN Loss: 4.353729248046875 | BCE Loss: 1.0318365097045898\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 5.440858840942383 | KNN Loss: 4.409584045410156 | BCE Loss: 1.0312745571136475\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 5.420906066894531 | KNN Loss: 4.398357391357422 | BCE Loss: 1.0225486755371094\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 5.405841827392578 | KNN Loss: 4.364725589752197 | BCE Loss: 1.0411161184310913\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 5.4098801612854 | KNN Loss: 4.37033224105835 | BCE Loss: 1.0395479202270508\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 5.408914566040039 | KNN Loss: 4.373536586761475 | BCE Loss: 1.0353777408599854\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 5.425004005432129 | KNN Loss: 4.404099464416504 | BCE Loss: 1.020904302597046\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 5.381463050842285 | KNN Loss: 4.352512359619141 | BCE Loss: 1.0289504528045654\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 5.419098854064941 | KNN Loss: 4.400516510009766 | BCE Loss: 1.0185824632644653\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 5.420853137969971 | KNN Loss: 4.370613098144531 | BCE Loss: 1.0502400398254395\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 5.401668548583984 | KNN Loss: 4.366388320922852 | BCE Loss: 1.0352802276611328\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 5.345099449157715 | KNN Loss: 4.322456359863281 | BCE Loss: 1.0226433277130127\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 5.405686378479004 | KNN Loss: 4.372501373291016 | BCE Loss: 1.0331852436065674\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 5.37732458114624 | KNN Loss: 4.3679399490356445 | BCE Loss: 1.0093846321105957\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 5.4182915687561035 | KNN Loss: 4.3683762550354 | BCE Loss: 1.0499151945114136\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 5.351404666900635 | KNN Loss: 4.349810600280762 | BCE Loss: 1.001594066619873\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 5.3714799880981445 | KNN Loss: 4.3638482093811035 | BCE Loss: 1.0076320171356201\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 5.401132583618164 | KNN Loss: 4.385584831237793 | BCE Loss: 1.0155476331710815\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 5.384488582611084 | KNN Loss: 4.377498149871826 | BCE Loss: 1.0069904327392578\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 5.4075727462768555 | KNN Loss: 4.370606899261475 | BCE Loss: 1.0369656085968018\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 5.364267826080322 | KNN Loss: 4.363342761993408 | BCE Loss: 1.000925064086914\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 5.399653434753418 | KNN Loss: 4.355952262878418 | BCE Loss: 1.043700933456421\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 5.380012512207031 | KNN Loss: 4.375690460205078 | BCE Loss: 1.0043219327926636\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 5.3509297370910645 | KNN Loss: 4.3669633865356445 | BCE Loss: 0.9839664101600647\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 5.358905792236328 | KNN Loss: 4.358366966247559 | BCE Loss: 1.0005388259887695\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 5.370722770690918 | KNN Loss: 4.337437629699707 | BCE Loss: 1.0332850217819214\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 5.381564140319824 | KNN Loss: 4.360775470733643 | BCE Loss: 1.0207884311676025\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 5.410717487335205 | KNN Loss: 4.387866020202637 | BCE Loss: 1.0228513479232788\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 5.4264607429504395 | KNN Loss: 4.391181945800781 | BCE Loss: 1.0352789163589478\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 5.352564334869385 | KNN Loss: 4.353994846343994 | BCE Loss: 0.9985696077346802\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 5.382081031799316 | KNN Loss: 4.368040561676025 | BCE Loss: 1.0140407085418701\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 5.417494773864746 | KNN Loss: 4.368793487548828 | BCE Loss: 1.048701286315918\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 5.401639461517334 | KNN Loss: 4.379397392272949 | BCE Loss: 1.0222420692443848\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 5.376298904418945 | KNN Loss: 4.374843120574951 | BCE Loss: 1.0014557838439941\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 5.392151832580566 | KNN Loss: 4.381328582763672 | BCE Loss: 1.0108234882354736\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 5.4374589920043945 | KNN Loss: 4.367218971252441 | BCE Loss: 1.0702400207519531\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 5.460993766784668 | KNN Loss: 4.4127373695373535 | BCE Loss: 1.0482563972473145\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 5.406179428100586 | KNN Loss: 4.380164623260498 | BCE Loss: 1.026014804840088\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 5.43863582611084 | KNN Loss: 4.39909553527832 | BCE Loss: 1.0395402908325195\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 5.394748687744141 | KNN Loss: 4.372317314147949 | BCE Loss: 1.0224316120147705\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 5.384246826171875 | KNN Loss: 4.3746490478515625 | BCE Loss: 1.0095980167388916\n",
      "Epoch   202: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 5.397948741912842 | KNN Loss: 4.378492832183838 | BCE Loss: 1.0194560289382935\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 5.416554927825928 | KNN Loss: 4.386171817779541 | BCE Loss: 1.0303832292556763\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 5.426372528076172 | KNN Loss: 4.370802402496338 | BCE Loss: 1.055570363998413\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 5.396185874938965 | KNN Loss: 4.383715629577637 | BCE Loss: 1.0124704837799072\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 5.420645713806152 | KNN Loss: 4.396124362945557 | BCE Loss: 1.0245213508605957\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 5.44378662109375 | KNN Loss: 4.389743804931641 | BCE Loss: 1.0540430545806885\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 5.367774963378906 | KNN Loss: 4.341274261474609 | BCE Loss: 1.0265004634857178\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 5.352745056152344 | KNN Loss: 4.354364395141602 | BCE Loss: 0.9983804821968079\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 5.4227705001831055 | KNN Loss: 4.368983268737793 | BCE Loss: 1.0537869930267334\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 5.430536270141602 | KNN Loss: 4.363466262817383 | BCE Loss: 1.0670697689056396\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 5.447445869445801 | KNN Loss: 4.426687717437744 | BCE Loss: 1.0207579135894775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 5.44553279876709 | KNN Loss: 4.397899150848389 | BCE Loss: 1.0476338863372803\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 5.368404388427734 | KNN Loss: 4.355591773986816 | BCE Loss: 1.012812852859497\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 5.454561233520508 | KNN Loss: 4.416686534881592 | BCE Loss: 1.037874698638916\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 5.42636775970459 | KNN Loss: 4.403219223022461 | BCE Loss: 1.023148536682129\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 5.3892083168029785 | KNN Loss: 4.387063980102539 | BCE Loss: 1.00214421749115\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 5.385799884796143 | KNN Loss: 4.385186195373535 | BCE Loss: 1.0006136894226074\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 5.41087532043457 | KNN Loss: 4.355223655700684 | BCE Loss: 1.0556514263153076\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 5.432422637939453 | KNN Loss: 4.38545036315918 | BCE Loss: 1.046972393989563\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 5.471959114074707 | KNN Loss: 4.433572292327881 | BCE Loss: 1.038386583328247\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 5.364295482635498 | KNN Loss: 4.349871635437012 | BCE Loss: 1.0144237279891968\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 5.396441459655762 | KNN Loss: 4.3997321128845215 | BCE Loss: 0.9967094659805298\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 5.447569370269775 | KNN Loss: 4.398813247680664 | BCE Loss: 1.0487561225891113\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 5.429015159606934 | KNN Loss: 4.403297424316406 | BCE Loss: 1.0257177352905273\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 5.326967239379883 | KNN Loss: 4.33912992477417 | BCE Loss: 0.9878374338150024\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 5.366860389709473 | KNN Loss: 4.340426445007324 | BCE Loss: 1.0264337062835693\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 5.383549213409424 | KNN Loss: 4.361434459686279 | BCE Loss: 1.022114634513855\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 5.416139602661133 | KNN Loss: 4.388662338256836 | BCE Loss: 1.0274773836135864\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 5.431748867034912 | KNN Loss: 4.396242618560791 | BCE Loss: 1.0355061292648315\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 5.418569087982178 | KNN Loss: 4.376921653747559 | BCE Loss: 1.0416474342346191\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 5.386223793029785 | KNN Loss: 4.347586154937744 | BCE Loss: 1.0386378765106201\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 5.385342121124268 | KNN Loss: 4.357450485229492 | BCE Loss: 1.0278916358947754\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 5.406187534332275 | KNN Loss: 4.384820461273193 | BCE Loss: 1.021367073059082\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 5.376861572265625 | KNN Loss: 4.363500118255615 | BCE Loss: 1.0133614540100098\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 5.393918037414551 | KNN Loss: 4.362949371337891 | BCE Loss: 1.0309687852859497\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 5.374477386474609 | KNN Loss: 4.343891620635986 | BCE Loss: 1.0305860042572021\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 5.420872688293457 | KNN Loss: 4.377870082855225 | BCE Loss: 1.0430028438568115\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 5.362825870513916 | KNN Loss: 4.339279651641846 | BCE Loss: 1.0235462188720703\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 5.424612045288086 | KNN Loss: 4.37903356552124 | BCE Loss: 1.0455782413482666\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 5.416431427001953 | KNN Loss: 4.375103950500488 | BCE Loss: 1.0413272380828857\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 5.38681697845459 | KNN Loss: 4.363994121551514 | BCE Loss: 1.0228228569030762\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 5.405282020568848 | KNN Loss: 4.364231586456299 | BCE Loss: 1.0410503149032593\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 5.384055137634277 | KNN Loss: 4.351065158843994 | BCE Loss: 1.032989740371704\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 5.401554107666016 | KNN Loss: 4.358879089355469 | BCE Loss: 1.0426751375198364\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 5.410257339477539 | KNN Loss: 4.376039981842041 | BCE Loss: 1.034217119216919\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 5.424874305725098 | KNN Loss: 4.342792987823486 | BCE Loss: 1.0820815563201904\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 5.426546096801758 | KNN Loss: 4.383269786834717 | BCE Loss: 1.0432765483856201\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 5.471744060516357 | KNN Loss: 4.40871000289917 | BCE Loss: 1.0630340576171875\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 5.446595191955566 | KNN Loss: 4.424890041351318 | BCE Loss: 1.021704912185669\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 5.419792175292969 | KNN Loss: 4.391357898712158 | BCE Loss: 1.0284340381622314\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 5.437175273895264 | KNN Loss: 4.392319202423096 | BCE Loss: 1.044856071472168\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 5.387948036193848 | KNN Loss: 4.3565673828125 | BCE Loss: 1.031380534172058\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 5.370826721191406 | KNN Loss: 4.3543524742126465 | BCE Loss: 1.0164740085601807\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 5.37885856628418 | KNN Loss: 4.337613105773926 | BCE Loss: 1.0412452220916748\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 5.4238080978393555 | KNN Loss: 4.389007568359375 | BCE Loss: 1.0348002910614014\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 5.364225387573242 | KNN Loss: 4.353084087371826 | BCE Loss: 1.011141300201416\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 5.410392761230469 | KNN Loss: 4.373136043548584 | BCE Loss: 1.0372564792633057\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 5.433999061584473 | KNN Loss: 4.396407127380371 | BCE Loss: 1.0375916957855225\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 5.428861618041992 | KNN Loss: 4.4026994705200195 | BCE Loss: 1.0261623859405518\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 5.477765083312988 | KNN Loss: 4.435421943664551 | BCE Loss: 1.042343258857727\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 5.391046524047852 | KNN Loss: 4.3549299240112305 | BCE Loss: 1.0361168384552002\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 5.411599159240723 | KNN Loss: 4.386068344116211 | BCE Loss: 1.0255305767059326\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 5.4055914878845215 | KNN Loss: 4.359912395477295 | BCE Loss: 1.045678973197937\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 5.409910678863525 | KNN Loss: 4.387117862701416 | BCE Loss: 1.022792935371399\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 5.420242786407471 | KNN Loss: 4.377042770385742 | BCE Loss: 1.0432000160217285\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 5.3893046379089355 | KNN Loss: 4.3798394203186035 | BCE Loss: 1.009465217590332\n",
      "Epoch   213: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 5.432427406311035 | KNN Loss: 4.4049458503723145 | BCE Loss: 1.0274817943572998\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 5.438542366027832 | KNN Loss: 4.397714614868164 | BCE Loss: 1.040827751159668\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 5.396815299987793 | KNN Loss: 4.367743015289307 | BCE Loss: 1.0290724039077759\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 5.3782196044921875 | KNN Loss: 4.362042427062988 | BCE Loss: 1.0161771774291992\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 5.403253078460693 | KNN Loss: 4.36695671081543 | BCE Loss: 1.0362964868545532\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 5.378170967102051 | KNN Loss: 4.34441614151001 | BCE Loss: 1.033754825592041\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 5.420185089111328 | KNN Loss: 4.3905510902404785 | BCE Loss: 1.0296341180801392\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 5.429486274719238 | KNN Loss: 4.3741888999938965 | BCE Loss: 1.0552972555160522\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 5.444521903991699 | KNN Loss: 4.4085211753845215 | BCE Loss: 1.0360009670257568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 5.376770973205566 | KNN Loss: 4.347792148590088 | BCE Loss: 1.0289790630340576\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 5.424764633178711 | KNN Loss: 4.390745639801025 | BCE Loss: 1.0340189933776855\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 5.350327968597412 | KNN Loss: 4.352342128753662 | BCE Loss: 0.9979859590530396\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 5.405364513397217 | KNN Loss: 4.3691182136535645 | BCE Loss: 1.0362461805343628\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 5.40249490737915 | KNN Loss: 4.342508792877197 | BCE Loss: 1.0599861145019531\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 5.360642433166504 | KNN Loss: 4.352747917175293 | BCE Loss: 1.00789475440979\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 5.38388204574585 | KNN Loss: 4.366787910461426 | BCE Loss: 1.0170940160751343\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 5.415053367614746 | KNN Loss: 4.366507053375244 | BCE Loss: 1.048546552658081\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 5.442501068115234 | KNN Loss: 4.412511348724365 | BCE Loss: 1.02998948097229\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 5.4233012199401855 | KNN Loss: 4.382411479949951 | BCE Loss: 1.040889859199524\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 5.391385078430176 | KNN Loss: 4.3716020584106445 | BCE Loss: 1.0197831392288208\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 5.393305778503418 | KNN Loss: 4.374623775482178 | BCE Loss: 1.0186822414398193\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 5.396486282348633 | KNN Loss: 4.349570274353027 | BCE Loss: 1.0469157695770264\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 5.44198751449585 | KNN Loss: 4.412285327911377 | BCE Loss: 1.0297021865844727\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 5.378559589385986 | KNN Loss: 4.365190505981445 | BCE Loss: 1.0133689641952515\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 5.432032108306885 | KNN Loss: 4.381810188293457 | BCE Loss: 1.0502219200134277\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 5.360536575317383 | KNN Loss: 4.356869697570801 | BCE Loss: 1.0036669969558716\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 5.382839202880859 | KNN Loss: 4.363430976867676 | BCE Loss: 1.0194084644317627\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 5.398026466369629 | KNN Loss: 4.35717248916626 | BCE Loss: 1.0408542156219482\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 5.3773016929626465 | KNN Loss: 4.364143371582031 | BCE Loss: 1.0131582021713257\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 5.377082824707031 | KNN Loss: 4.350530624389648 | BCE Loss: 1.0265522003173828\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 5.360573768615723 | KNN Loss: 4.33321475982666 | BCE Loss: 1.0273587703704834\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 5.413994789123535 | KNN Loss: 4.402841567993164 | BCE Loss: 1.011153221130371\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 5.423916816711426 | KNN Loss: 4.376748085021973 | BCE Loss: 1.0471689701080322\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 5.376548767089844 | KNN Loss: 4.371973991394043 | BCE Loss: 1.0045750141143799\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 5.436278343200684 | KNN Loss: 4.390058994293213 | BCE Loss: 1.0462192296981812\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 5.397918224334717 | KNN Loss: 4.368555068969727 | BCE Loss: 1.0293632745742798\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 5.366211891174316 | KNN Loss: 4.350627422332764 | BCE Loss: 1.0155844688415527\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 5.357115745544434 | KNN Loss: 4.351479530334473 | BCE Loss: 1.0056359767913818\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 5.362762451171875 | KNN Loss: 4.351266384124756 | BCE Loss: 1.0114960670471191\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 5.393949508666992 | KNN Loss: 4.365521430969238 | BCE Loss: 1.028428316116333\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 5.389403343200684 | KNN Loss: 4.368750095367432 | BCE Loss: 1.0206533670425415\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 5.373073101043701 | KNN Loss: 4.350874900817871 | BCE Loss: 1.02219820022583\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 5.395997524261475 | KNN Loss: 4.358335971832275 | BCE Loss: 1.0376616716384888\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 5.402767181396484 | KNN Loss: 4.3838348388671875 | BCE Loss: 1.0189323425292969\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 5.424095153808594 | KNN Loss: 4.388647556304932 | BCE Loss: 1.035447597503662\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 5.383337020874023 | KNN Loss: 4.352282524108887 | BCE Loss: 1.0310547351837158\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 5.434206008911133 | KNN Loss: 4.369500637054443 | BCE Loss: 1.0647051334381104\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 5.410895347595215 | KNN Loss: 4.3864617347717285 | BCE Loss: 1.0244336128234863\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 5.429015159606934 | KNN Loss: 4.382955074310303 | BCE Loss: 1.04606032371521\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 5.433645248413086 | KNN Loss: 4.3858489990234375 | BCE Loss: 1.0477964878082275\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 5.430959224700928 | KNN Loss: 4.368378639221191 | BCE Loss: 1.0625805854797363\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 5.40092134475708 | KNN Loss: 4.371153831481934 | BCE Loss: 1.029767632484436\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 5.419039726257324 | KNN Loss: 4.3842082023620605 | BCE Loss: 1.0348316431045532\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 5.379993438720703 | KNN Loss: 4.348254203796387 | BCE Loss: 1.031739354133606\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 5.364297866821289 | KNN Loss: 4.355456352233887 | BCE Loss: 1.0088417530059814\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 5.404565334320068 | KNN Loss: 4.354957103729248 | BCE Loss: 1.0496081113815308\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 5.428659439086914 | KNN Loss: 4.426690578460693 | BCE Loss: 1.0019687414169312\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 5.40451192855835 | KNN Loss: 4.385074138641357 | BCE Loss: 1.0194377899169922\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 5.386064529418945 | KNN Loss: 4.354574680328369 | BCE Loss: 1.0314898490905762\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 5.41928768157959 | KNN Loss: 4.396129608154297 | BCE Loss: 1.023158311843872\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 5.384469032287598 | KNN Loss: 4.360527038574219 | BCE Loss: 1.0239417552947998\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 5.414703369140625 | KNN Loss: 4.383376598358154 | BCE Loss: 1.0313265323638916\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 5.397561550140381 | KNN Loss: 4.362969875335693 | BCE Loss: 1.034591555595398\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 5.46112060546875 | KNN Loss: 4.3806681632995605 | BCE Loss: 1.080452561378479\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 5.4079270362854 | KNN Loss: 4.407334327697754 | BCE Loss: 1.000592589378357\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 5.406903266906738 | KNN Loss: 4.359235763549805 | BCE Loss: 1.0476672649383545\n",
      "Epoch   224: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 5.422191143035889 | KNN Loss: 4.3722825050354 | BCE Loss: 1.0499087572097778\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 5.385274887084961 | KNN Loss: 4.349872589111328 | BCE Loss: 1.035402536392212\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 5.387239456176758 | KNN Loss: 4.371707439422607 | BCE Loss: 1.01553213596344\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 5.4075398445129395 | KNN Loss: 4.369172096252441 | BCE Loss: 1.038367748260498\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 5.401485919952393 | KNN Loss: 4.373466491699219 | BCE Loss: 1.0280194282531738\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 5.42039155960083 | KNN Loss: 4.391282081604004 | BCE Loss: 1.0291094779968262\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 5.435476303100586 | KNN Loss: 4.387065410614014 | BCE Loss: 1.0484111309051514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 5.358302593231201 | KNN Loss: 4.346256256103516 | BCE Loss: 1.012046217918396\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 5.388148307800293 | KNN Loss: 4.3635382652282715 | BCE Loss: 1.0246098041534424\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 5.390926361083984 | KNN Loss: 4.373022079467773 | BCE Loss: 1.0179040431976318\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 5.3877034187316895 | KNN Loss: 4.3659443855285645 | BCE Loss: 1.021759033203125\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 5.373426914215088 | KNN Loss: 4.356732368469238 | BCE Loss: 1.0166945457458496\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 5.404093265533447 | KNN Loss: 4.3650898933410645 | BCE Loss: 1.0390032529830933\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 5.36570930480957 | KNN Loss: 4.37091588973999 | BCE Loss: 0.9947935342788696\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 5.3829545974731445 | KNN Loss: 4.365495204925537 | BCE Loss: 1.0174591541290283\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 5.424656867980957 | KNN Loss: 4.390167713165283 | BCE Loss: 1.0344892740249634\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 5.377091884613037 | KNN Loss: 4.350387096405029 | BCE Loss: 1.0267046689987183\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 5.4011006355285645 | KNN Loss: 4.36573600769043 | BCE Loss: 1.0353646278381348\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 5.383134365081787 | KNN Loss: 4.359071731567383 | BCE Loss: 1.0240625143051147\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 5.419003486633301 | KNN Loss: 4.372338771820068 | BCE Loss: 1.0466647148132324\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 5.359705924987793 | KNN Loss: 4.348198890686035 | BCE Loss: 1.0115067958831787\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 5.440047264099121 | KNN Loss: 4.3844428062438965 | BCE Loss: 1.0556046962738037\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 5.401711463928223 | KNN Loss: 4.373880863189697 | BCE Loss: 1.0278308391571045\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 5.4089484214782715 | KNN Loss: 4.36485481262207 | BCE Loss: 1.0440936088562012\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 5.398996353149414 | KNN Loss: 4.3660783767700195 | BCE Loss: 1.032917857170105\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 5.3388776779174805 | KNN Loss: 4.345439434051514 | BCE Loss: 0.9934383034706116\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 5.4158830642700195 | KNN Loss: 4.375432014465332 | BCE Loss: 1.0404508113861084\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 5.377103805541992 | KNN Loss: 4.379525184631348 | BCE Loss: 0.9975787997245789\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 5.384884834289551 | KNN Loss: 4.338744640350342 | BCE Loss: 1.046140193939209\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 5.412205219268799 | KNN Loss: 4.386508464813232 | BCE Loss: 1.0256966352462769\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 5.422029495239258 | KNN Loss: 4.391772270202637 | BCE Loss: 1.0302574634552002\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 5.367016792297363 | KNN Loss: 4.352982044219971 | BCE Loss: 1.0140347480773926\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 5.374507904052734 | KNN Loss: 4.35749626159668 | BCE Loss: 1.0170118808746338\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 5.385708808898926 | KNN Loss: 4.363229751586914 | BCE Loss: 1.0224792957305908\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 5.428086280822754 | KNN Loss: 4.404400825500488 | BCE Loss: 1.0236854553222656\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 5.44840145111084 | KNN Loss: 4.404516220092773 | BCE Loss: 1.0438849925994873\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 5.419575214385986 | KNN Loss: 4.374085426330566 | BCE Loss: 1.04548978805542\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 5.414620399475098 | KNN Loss: 4.387420654296875 | BCE Loss: 1.027199625968933\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 5.390087127685547 | KNN Loss: 4.355159759521484 | BCE Loss: 1.034927487373352\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 5.432529449462891 | KNN Loss: 4.37178373336792 | BCE Loss: 1.0607459545135498\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 5.4300761222839355 | KNN Loss: 4.400503635406494 | BCE Loss: 1.0295723676681519\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 5.3899359703063965 | KNN Loss: 4.364604949951172 | BCE Loss: 1.025330901145935\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 5.35426139831543 | KNN Loss: 4.355630397796631 | BCE Loss: 0.9986310005187988\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 5.394899845123291 | KNN Loss: 4.358462810516357 | BCE Loss: 1.0364371538162231\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 5.40708065032959 | KNN Loss: 4.3875885009765625 | BCE Loss: 1.0194921493530273\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 5.393664360046387 | KNN Loss: 4.369963645935059 | BCE Loss: 1.0237009525299072\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 5.40906286239624 | KNN Loss: 4.379422664642334 | BCE Loss: 1.0296401977539062\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 5.415246486663818 | KNN Loss: 4.390879154205322 | BCE Loss: 1.0243674516677856\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 5.463138580322266 | KNN Loss: 4.4064483642578125 | BCE Loss: 1.0566903352737427\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 5.448753356933594 | KNN Loss: 4.410569667816162 | BCE Loss: 1.0381834506988525\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 5.37452507019043 | KNN Loss: 4.350678443908691 | BCE Loss: 1.0238463878631592\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 5.375410556793213 | KNN Loss: 4.359169960021973 | BCE Loss: 1.0162405967712402\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 5.418083190917969 | KNN Loss: 4.3775248527526855 | BCE Loss: 1.040558099746704\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 5.449225425720215 | KNN Loss: 4.435069561004639 | BCE Loss: 1.0141558647155762\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 5.385437965393066 | KNN Loss: 4.380220413208008 | BCE Loss: 1.005217432975769\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 5.386933326721191 | KNN Loss: 4.349173545837402 | BCE Loss: 1.0377596616744995\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 5.329982280731201 | KNN Loss: 4.341355323791504 | BCE Loss: 0.9886268377304077\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 5.520594596862793 | KNN Loss: 4.44708251953125 | BCE Loss: 1.073512077331543\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 5.43621826171875 | KNN Loss: 4.37697696685791 | BCE Loss: 1.059241533279419\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 5.365723609924316 | KNN Loss: 4.342226505279541 | BCE Loss: 1.0234968662261963\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 5.382358551025391 | KNN Loss: 4.357085704803467 | BCE Loss: 1.0252729654312134\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 5.421106815338135 | KNN Loss: 4.419392108917236 | BCE Loss: 1.0017147064208984\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 5.448077201843262 | KNN Loss: 4.374697208404541 | BCE Loss: 1.0733798742294312\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 5.384718894958496 | KNN Loss: 4.3435139656066895 | BCE Loss: 1.0412049293518066\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 5.390963554382324 | KNN Loss: 4.371854305267334 | BCE Loss: 1.0191094875335693\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 5.3542985916137695 | KNN Loss: 4.3517351150512695 | BCE Loss: 1.0025634765625\n",
      "Epoch   235: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 5.434860706329346 | KNN Loss: 4.375632286071777 | BCE Loss: 1.0592284202575684\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 5.390414237976074 | KNN Loss: 4.3799519538879395 | BCE Loss: 1.0104625225067139\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 5.428586483001709 | KNN Loss: 4.404768466949463 | BCE Loss: 1.023818016052246\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 5.383066654205322 | KNN Loss: 4.349900722503662 | BCE Loss: 1.0331658124923706\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 5.399539947509766 | KNN Loss: 4.367997646331787 | BCE Loss: 1.031542181968689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 5.4504804611206055 | KNN Loss: 4.406428813934326 | BCE Loss: 1.0440518856048584\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 5.372734546661377 | KNN Loss: 4.341066360473633 | BCE Loss: 1.0316680669784546\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 5.429498672485352 | KNN Loss: 4.371689319610596 | BCE Loss: 1.057809591293335\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 5.393152713775635 | KNN Loss: 4.396761417388916 | BCE Loss: 0.9963914155960083\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 5.451716899871826 | KNN Loss: 4.389090538024902 | BCE Loss: 1.0626263618469238\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 5.348982334136963 | KNN Loss: 4.354413032531738 | BCE Loss: 0.9945691823959351\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 5.354949474334717 | KNN Loss: 4.348024845123291 | BCE Loss: 1.0069246292114258\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 5.377147674560547 | KNN Loss: 4.344461917877197 | BCE Loss: 1.0326857566833496\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 5.442562103271484 | KNN Loss: 4.405186176300049 | BCE Loss: 1.037375807762146\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 5.385274887084961 | KNN Loss: 4.360776901245117 | BCE Loss: 1.0244977474212646\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 5.349215507507324 | KNN Loss: 4.333907604217529 | BCE Loss: 1.0153080224990845\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 5.391211986541748 | KNN Loss: 4.36752986907959 | BCE Loss: 1.0236821174621582\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 5.420463562011719 | KNN Loss: 4.385100841522217 | BCE Loss: 1.0353624820709229\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 5.388929843902588 | KNN Loss: 4.370302200317383 | BCE Loss: 1.0186275243759155\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 5.402576923370361 | KNN Loss: 4.395834922790527 | BCE Loss: 1.0067418813705444\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 5.388261318206787 | KNN Loss: 4.362426280975342 | BCE Loss: 1.0258349180221558\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 5.406473159790039 | KNN Loss: 4.380575180053711 | BCE Loss: 1.0258980989456177\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 5.409526824951172 | KNN Loss: 4.3769001960754395 | BCE Loss: 1.0326266288757324\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 5.414125442504883 | KNN Loss: 4.390895366668701 | BCE Loss: 1.0232303142547607\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 5.402872562408447 | KNN Loss: 4.382343292236328 | BCE Loss: 1.0205293893814087\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 5.387800216674805 | KNN Loss: 4.36744499206543 | BCE Loss: 1.020354986190796\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 5.421168327331543 | KNN Loss: 4.38875675201416 | BCE Loss: 1.0324113368988037\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 5.436777591705322 | KNN Loss: 4.401050567626953 | BCE Loss: 1.0357269048690796\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 5.399174213409424 | KNN Loss: 4.386608123779297 | BCE Loss: 1.012566089630127\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 5.411822319030762 | KNN Loss: 4.385657787322998 | BCE Loss: 1.0261644124984741\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 5.40974235534668 | KNN Loss: 4.38456392288208 | BCE Loss: 1.0251784324645996\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 5.398287296295166 | KNN Loss: 4.366476535797119 | BCE Loss: 1.0318107604980469\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 5.4430646896362305 | KNN Loss: 4.3796281814575195 | BCE Loss: 1.06343674659729\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 5.406516075134277 | KNN Loss: 4.348665237426758 | BCE Loss: 1.0578505992889404\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 5.377197265625 | KNN Loss: 4.356996536254883 | BCE Loss: 1.020200490951538\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 5.406075477600098 | KNN Loss: 4.379868030548096 | BCE Loss: 1.026207447052002\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 5.412509918212891 | KNN Loss: 4.372633934020996 | BCE Loss: 1.0398762226104736\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 5.3919219970703125 | KNN Loss: 4.402878284454346 | BCE Loss: 0.9890435934066772\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 5.397705554962158 | KNN Loss: 4.379913806915283 | BCE Loss: 1.0177918672561646\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 5.393396854400635 | KNN Loss: 4.3574538230896 | BCE Loss: 1.0359429121017456\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 5.391802787780762 | KNN Loss: 4.366817474365234 | BCE Loss: 1.0249851942062378\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 5.381083011627197 | KNN Loss: 4.3615288734436035 | BCE Loss: 1.0195540189743042\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 5.411330699920654 | KNN Loss: 4.387124061584473 | BCE Loss: 1.024206519126892\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 5.411093711853027 | KNN Loss: 4.3812971115112305 | BCE Loss: 1.0297966003417969\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 5.3661394119262695 | KNN Loss: 4.350346565246582 | BCE Loss: 1.0157926082611084\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 5.467321395874023 | KNN Loss: 4.424883842468262 | BCE Loss: 1.0424375534057617\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 5.448370456695557 | KNN Loss: 4.421075820922852 | BCE Loss: 1.027294635772705\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 5.409430503845215 | KNN Loss: 4.369298934936523 | BCE Loss: 1.0401318073272705\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 5.363400459289551 | KNN Loss: 4.366700172424316 | BCE Loss: 0.9967001676559448\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 5.3820013999938965 | KNN Loss: 4.3703083992004395 | BCE Loss: 1.011693000793457\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 5.406562328338623 | KNN Loss: 4.38072395324707 | BCE Loss: 1.0258382558822632\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 5.403907775878906 | KNN Loss: 4.359979629516602 | BCE Loss: 1.0439281463623047\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 5.430706977844238 | KNN Loss: 4.383434295654297 | BCE Loss: 1.0472729206085205\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 5.414340019226074 | KNN Loss: 4.3818793296813965 | BCE Loss: 1.0324609279632568\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 5.425373077392578 | KNN Loss: 4.3772430419921875 | BCE Loss: 1.0481297969818115\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 5.423298358917236 | KNN Loss: 4.407107830047607 | BCE Loss: 1.016190528869629\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 5.375024318695068 | KNN Loss: 4.372169017791748 | BCE Loss: 1.0028551816940308\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 5.420890808105469 | KNN Loss: 4.393545150756836 | BCE Loss: 1.027345895767212\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 5.421226978302002 | KNN Loss: 4.3790507316589355 | BCE Loss: 1.0421762466430664\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 5.4111528396606445 | KNN Loss: 4.365588665008545 | BCE Loss: 1.04556405544281\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 5.404054641723633 | KNN Loss: 4.368844509124756 | BCE Loss: 1.035210132598877\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 5.424068450927734 | KNN Loss: 4.385725498199463 | BCE Loss: 1.0383429527282715\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 5.455007553100586 | KNN Loss: 4.408486366271973 | BCE Loss: 1.0465210676193237\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 5.3863420486450195 | KNN Loss: 4.358268737792969 | BCE Loss: 1.0280733108520508\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 5.423196792602539 | KNN Loss: 4.387722969055176 | BCE Loss: 1.0354737043380737\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 5.44791316986084 | KNN Loss: 4.422810077667236 | BCE Loss: 1.025102972984314\n",
      "Epoch   246: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 5.338374614715576 | KNN Loss: 4.323244094848633 | BCE Loss: 1.015130639076233\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 5.422207832336426 | KNN Loss: 4.392302989959717 | BCE Loss: 1.0299046039581299\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 5.356570720672607 | KNN Loss: 4.350430965423584 | BCE Loss: 1.0061396360397339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 5.405891418457031 | KNN Loss: 4.342324256896973 | BCE Loss: 1.0635673999786377\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 5.397876739501953 | KNN Loss: 4.367661476135254 | BCE Loss: 1.0302150249481201\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 5.434875965118408 | KNN Loss: 4.390585422515869 | BCE Loss: 1.0442904233932495\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 5.432035446166992 | KNN Loss: 4.413301467895508 | BCE Loss: 1.0187337398529053\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 5.403598785400391 | KNN Loss: 4.38935661315918 | BCE Loss: 1.01424241065979\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 5.3720831871032715 | KNN Loss: 4.34710168838501 | BCE Loss: 1.0249816179275513\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 5.411288738250732 | KNN Loss: 4.359678268432617 | BCE Loss: 1.0516104698181152\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 5.4093523025512695 | KNN Loss: 4.368225574493408 | BCE Loss: 1.0411267280578613\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 5.377666473388672 | KNN Loss: 4.342083930969238 | BCE Loss: 1.0355823040008545\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 5.476424694061279 | KNN Loss: 4.4276909828186035 | BCE Loss: 1.0487337112426758\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 5.418997764587402 | KNN Loss: 4.385068893432617 | BCE Loss: 1.0339288711547852\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 5.395496368408203 | KNN Loss: 4.344574451446533 | BCE Loss: 1.0509216785430908\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 5.425858497619629 | KNN Loss: 4.399302005767822 | BCE Loss: 1.0265567302703857\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 5.41495418548584 | KNN Loss: 4.387503147125244 | BCE Loss: 1.0274507999420166\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 5.40300178527832 | KNN Loss: 4.351535320281982 | BCE Loss: 1.051466703414917\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 5.44162130355835 | KNN Loss: 4.380006313323975 | BCE Loss: 1.0616151094436646\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 5.426557540893555 | KNN Loss: 4.394474983215332 | BCE Loss: 1.0320827960968018\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 5.39603328704834 | KNN Loss: 4.3596649169921875 | BCE Loss: 1.0363686084747314\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 5.362671852111816 | KNN Loss: 4.34023904800415 | BCE Loss: 1.0224330425262451\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 5.4045729637146 | KNN Loss: 4.361004829406738 | BCE Loss: 1.0435681343078613\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 5.384059429168701 | KNN Loss: 4.389288902282715 | BCE Loss: 0.9947704672813416\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 5.395727157592773 | KNN Loss: 4.358880043029785 | BCE Loss: 1.0368473529815674\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 5.404473304748535 | KNN Loss: 4.365314960479736 | BCE Loss: 1.039158582687378\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 5.451654434204102 | KNN Loss: 4.375657558441162 | BCE Loss: 1.0759966373443604\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 5.361441612243652 | KNN Loss: 4.370765209197998 | BCE Loss: 0.9906762838363647\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 5.386281490325928 | KNN Loss: 4.358709335327148 | BCE Loss: 1.0275720357894897\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 5.416351318359375 | KNN Loss: 4.365133285522461 | BCE Loss: 1.051218032836914\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 5.369351387023926 | KNN Loss: 4.363387584686279 | BCE Loss: 1.0059640407562256\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 5.417801380157471 | KNN Loss: 4.374419689178467 | BCE Loss: 1.0433815717697144\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 5.444055080413818 | KNN Loss: 4.389202117919922 | BCE Loss: 1.054853081703186\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 5.397336959838867 | KNN Loss: 4.358114719390869 | BCE Loss: 1.039222002029419\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 5.385904312133789 | KNN Loss: 4.348608016967773 | BCE Loss: 1.037296175956726\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 5.398224830627441 | KNN Loss: 4.359799385070801 | BCE Loss: 1.0384252071380615\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 5.394757270812988 | KNN Loss: 4.357442378997803 | BCE Loss: 1.0373151302337646\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 5.4381208419799805 | KNN Loss: 4.406126499176025 | BCE Loss: 1.031994104385376\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 5.419118881225586 | KNN Loss: 4.377624988555908 | BCE Loss: 1.0414938926696777\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 5.3693084716796875 | KNN Loss: 4.371936798095703 | BCE Loss: 0.9973716139793396\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 5.41197395324707 | KNN Loss: 4.374282360076904 | BCE Loss: 1.0376917123794556\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 5.42874002456665 | KNN Loss: 4.411067485809326 | BCE Loss: 1.0176726579666138\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 5.407431602478027 | KNN Loss: 4.36746072769165 | BCE Loss: 1.039970874786377\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 5.395903587341309 | KNN Loss: 4.365804672241211 | BCE Loss: 1.0300991535186768\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 5.38105583190918 | KNN Loss: 4.353961944580078 | BCE Loss: 1.0270941257476807\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 5.453837871551514 | KNN Loss: 4.40895414352417 | BCE Loss: 1.0448838472366333\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 5.34401273727417 | KNN Loss: 4.34393835067749 | BCE Loss: 1.0000743865966797\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 5.446033000946045 | KNN Loss: 4.44573450088501 | BCE Loss: 1.0002985000610352\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 5.377560615539551 | KNN Loss: 4.36317777633667 | BCE Loss: 1.01438307762146\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 5.419426918029785 | KNN Loss: 4.376002311706543 | BCE Loss: 1.0434248447418213\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 5.395598888397217 | KNN Loss: 4.359935760498047 | BCE Loss: 1.03566312789917\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 5.441623687744141 | KNN Loss: 4.397748947143555 | BCE Loss: 1.0438748598098755\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 5.423767566680908 | KNN Loss: 4.39227294921875 | BCE Loss: 1.0314946174621582\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 5.402039051055908 | KNN Loss: 4.3426642417907715 | BCE Loss: 1.0593749284744263\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 5.4159674644470215 | KNN Loss: 4.361889839172363 | BCE Loss: 1.0540776252746582\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 5.426032066345215 | KNN Loss: 4.386322021484375 | BCE Loss: 1.0397100448608398\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 5.417655944824219 | KNN Loss: 4.363422393798828 | BCE Loss: 1.0542335510253906\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 5.428999423980713 | KNN Loss: 4.392606258392334 | BCE Loss: 1.0363930463790894\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 5.465497016906738 | KNN Loss: 4.42954158782959 | BCE Loss: 1.0359554290771484\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 5.40892219543457 | KNN Loss: 4.363974571228027 | BCE Loss: 1.044947624206543\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 5.408565044403076 | KNN Loss: 4.371240139007568 | BCE Loss: 1.0373249053955078\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 5.430563449859619 | KNN Loss: 4.4018073081970215 | BCE Loss: 1.0287562608718872\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 5.393701553344727 | KNN Loss: 4.361594200134277 | BCE Loss: 1.0321071147918701\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 5.409266471862793 | KNN Loss: 4.388297080993652 | BCE Loss: 1.0209693908691406\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 5.413444519042969 | KNN Loss: 4.393024444580078 | BCE Loss: 1.020419955253601\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 5.405158519744873 | KNN Loss: 4.3828020095825195 | BCE Loss: 1.0223565101623535\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 5.36448860168457 | KNN Loss: 4.350856304168701 | BCE Loss: 1.01363205909729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 5.399740219116211 | KNN Loss: 4.375702381134033 | BCE Loss: 1.0240377187728882\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 5.42772102355957 | KNN Loss: 4.405219078063965 | BCE Loss: 1.0225019454956055\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 5.3969316482543945 | KNN Loss: 4.373394966125488 | BCE Loss: 1.0235364437103271\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 5.416792392730713 | KNN Loss: 4.379599571228027 | BCE Loss: 1.037192702293396\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 5.404210090637207 | KNN Loss: 4.389542579650879 | BCE Loss: 1.0146675109863281\n",
      "Epoch   258: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 5.468334674835205 | KNN Loss: 4.430617332458496 | BCE Loss: 1.0377172231674194\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 5.462325096130371 | KNN Loss: 4.402691841125488 | BCE Loss: 1.059633493423462\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 5.374025344848633 | KNN Loss: 4.355278491973877 | BCE Loss: 1.0187466144561768\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 5.4102559089660645 | KNN Loss: 4.367211818695068 | BCE Loss: 1.043044090270996\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 5.383326053619385 | KNN Loss: 4.363839149475098 | BCE Loss: 1.0194870233535767\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 5.353411674499512 | KNN Loss: 4.344731330871582 | BCE Loss: 1.0086802244186401\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 5.36021089553833 | KNN Loss: 4.33916711807251 | BCE Loss: 1.0210436582565308\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 5.381752014160156 | KNN Loss: 4.361217021942139 | BCE Loss: 1.0205351114273071\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 5.357102394104004 | KNN Loss: 4.332422256469727 | BCE Loss: 1.0246803760528564\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 5.390511989593506 | KNN Loss: 4.349756717681885 | BCE Loss: 1.0407551527023315\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 5.446043491363525 | KNN Loss: 4.393121242523193 | BCE Loss: 1.052922248840332\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 5.397534370422363 | KNN Loss: 4.371127128601074 | BCE Loss: 1.026407241821289\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 5.404383659362793 | KNN Loss: 4.385052680969238 | BCE Loss: 1.0193307399749756\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 5.431545257568359 | KNN Loss: 4.389509677886963 | BCE Loss: 1.0420355796813965\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 5.399839401245117 | KNN Loss: 4.365140914916992 | BCE Loss: 1.0346986055374146\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 5.390598297119141 | KNN Loss: 4.359684467315674 | BCE Loss: 1.0309139490127563\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 5.3634867668151855 | KNN Loss: 4.343578815460205 | BCE Loss: 1.01990807056427\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 5.432513236999512 | KNN Loss: 4.401511192321777 | BCE Loss: 1.0310018062591553\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 5.378129482269287 | KNN Loss: 4.363791465759277 | BCE Loss: 1.0143380165100098\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 5.412265300750732 | KNN Loss: 4.372559070587158 | BCE Loss: 1.0397063493728638\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 5.44580078125 | KNN Loss: 4.39982795715332 | BCE Loss: 1.0459729433059692\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 5.34031867980957 | KNN Loss: 4.352276802062988 | BCE Loss: 0.9880421161651611\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 5.432660102844238 | KNN Loss: 4.383113861083984 | BCE Loss: 1.0495460033416748\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 5.3675384521484375 | KNN Loss: 4.346853256225586 | BCE Loss: 1.0206851959228516\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 5.377423286437988 | KNN Loss: 4.362610816955566 | BCE Loss: 1.0148124694824219\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 5.376096248626709 | KNN Loss: 4.358100891113281 | BCE Loss: 1.0179952383041382\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 5.379629611968994 | KNN Loss: 4.353132247924805 | BCE Loss: 1.0264973640441895\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 5.4311442375183105 | KNN Loss: 4.378913879394531 | BCE Loss: 1.0522303581237793\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 5.414299964904785 | KNN Loss: 4.403414249420166 | BCE Loss: 1.0108855962753296\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 5.362709045410156 | KNN Loss: 4.351683616638184 | BCE Loss: 1.0110254287719727\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 5.369932174682617 | KNN Loss: 4.358285427093506 | BCE Loss: 1.0116465091705322\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 5.391602516174316 | KNN Loss: 4.370113849639893 | BCE Loss: 1.0214886665344238\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 5.3755412101745605 | KNN Loss: 4.348898887634277 | BCE Loss: 1.0266423225402832\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 5.395104885101318 | KNN Loss: 4.362248420715332 | BCE Loss: 1.0328564643859863\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 5.385578632354736 | KNN Loss: 4.348974227905273 | BCE Loss: 1.036604404449463\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 5.398077487945557 | KNN Loss: 4.3541340827941895 | BCE Loss: 1.0439435243606567\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 5.400756359100342 | KNN Loss: 4.3612565994262695 | BCE Loss: 1.0394997596740723\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 5.386178970336914 | KNN Loss: 4.365562438964844 | BCE Loss: 1.0206167697906494\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 5.3919525146484375 | KNN Loss: 4.375024795532227 | BCE Loss: 1.016927719116211\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 5.3749308586120605 | KNN Loss: 4.34751558303833 | BCE Loss: 1.0274152755737305\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 5.360212802886963 | KNN Loss: 4.358463764190674 | BCE Loss: 1.001749038696289\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 5.344486236572266 | KNN Loss: 4.347931861877441 | BCE Loss: 0.9965542554855347\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 5.4342241287231445 | KNN Loss: 4.412116050720215 | BCE Loss: 1.0221080780029297\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 5.40474271774292 | KNN Loss: 4.360378265380859 | BCE Loss: 1.044364333152771\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 5.366872787475586 | KNN Loss: 4.348585605621338 | BCE Loss: 1.0182874202728271\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 5.411266326904297 | KNN Loss: 4.3895263671875 | BCE Loss: 1.0217397212982178\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 5.403156280517578 | KNN Loss: 4.344544410705566 | BCE Loss: 1.0586121082305908\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 5.401926040649414 | KNN Loss: 4.38221549987793 | BCE Loss: 1.0197105407714844\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 5.390732765197754 | KNN Loss: 4.349950313568115 | BCE Loss: 1.0407824516296387\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 5.360014915466309 | KNN Loss: 4.338336944580078 | BCE Loss: 1.0216777324676514\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 5.406452655792236 | KNN Loss: 4.413705825805664 | BCE Loss: 0.9927468299865723\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 5.375827789306641 | KNN Loss: 4.3505449295043945 | BCE Loss: 1.0252829790115356\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 5.377810001373291 | KNN Loss: 4.345034599304199 | BCE Loss: 1.0327754020690918\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 5.439912796020508 | KNN Loss: 4.3955535888671875 | BCE Loss: 1.0443589687347412\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 5.416302680969238 | KNN Loss: 4.362210750579834 | BCE Loss: 1.0540918111801147\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 5.398494720458984 | KNN Loss: 4.359538555145264 | BCE Loss: 1.0389560461044312\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 5.412629127502441 | KNN Loss: 4.386505126953125 | BCE Loss: 1.026124119758606\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 5.410402774810791 | KNN Loss: 4.381067752838135 | BCE Loss: 1.0293351411819458\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 5.420197486877441 | KNN Loss: 4.397586345672607 | BCE Loss: 1.0226110219955444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 5.400324821472168 | KNN Loss: 4.360675811767578 | BCE Loss: 1.039649248123169\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 5.376794815063477 | KNN Loss: 4.35478401184082 | BCE Loss: 1.0220110416412354\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 5.384860038757324 | KNN Loss: 4.344861030578613 | BCE Loss: 1.0399987697601318\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 5.397288799285889 | KNN Loss: 4.365914821624756 | BCE Loss: 1.0313740968704224\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 5.394014835357666 | KNN Loss: 4.353979587554932 | BCE Loss: 1.0400352478027344\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 5.36430549621582 | KNN Loss: 4.345435619354248 | BCE Loss: 1.0188698768615723\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 5.384111404418945 | KNN Loss: 4.359631061553955 | BCE Loss: 1.0244803428649902\n",
      "Epoch   269: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 5.415848731994629 | KNN Loss: 4.371531963348389 | BCE Loss: 1.0443170070648193\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 5.386214256286621 | KNN Loss: 4.367987632751465 | BCE Loss: 1.0182268619537354\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 5.396452903747559 | KNN Loss: 4.35863733291626 | BCE Loss: 1.0378153324127197\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 5.40249490737915 | KNN Loss: 4.400757312774658 | BCE Loss: 1.0017374753952026\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 5.379157543182373 | KNN Loss: 4.352006912231445 | BCE Loss: 1.0271505117416382\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 5.4163103103637695 | KNN Loss: 4.3842973709106445 | BCE Loss: 1.032012701034546\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 5.46357536315918 | KNN Loss: 4.396830081939697 | BCE Loss: 1.0667455196380615\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 5.396587371826172 | KNN Loss: 4.375391960144043 | BCE Loss: 1.021195650100708\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 5.3843889236450195 | KNN Loss: 4.362675666809082 | BCE Loss: 1.0217134952545166\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 5.375979423522949 | KNN Loss: 4.3531904220581055 | BCE Loss: 1.0227890014648438\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 5.4723052978515625 | KNN Loss: 4.395514965057373 | BCE Loss: 1.0767902135849\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 5.333101272583008 | KNN Loss: 4.3293023109436035 | BCE Loss: 1.0037992000579834\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 5.4383015632629395 | KNN Loss: 4.380976676940918 | BCE Loss: 1.0573248863220215\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 5.378164291381836 | KNN Loss: 4.35244083404541 | BCE Loss: 1.0257233381271362\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 5.439123630523682 | KNN Loss: 4.382194519042969 | BCE Loss: 1.0569289922714233\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 5.433108329772949 | KNN Loss: 4.414149761199951 | BCE Loss: 1.018958568572998\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 5.3733625411987305 | KNN Loss: 4.356420040130615 | BCE Loss: 1.0169425010681152\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 5.409395217895508 | KNN Loss: 4.376357555389404 | BCE Loss: 1.0330374240875244\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 5.388299942016602 | KNN Loss: 4.366455554962158 | BCE Loss: 1.0218441486358643\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 5.3886847496032715 | KNN Loss: 4.375998497009277 | BCE Loss: 1.0126862525939941\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 5.405586242675781 | KNN Loss: 4.355166435241699 | BCE Loss: 1.050419569015503\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 5.382203578948975 | KNN Loss: 4.3532633781433105 | BCE Loss: 1.028940200805664\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 5.461483955383301 | KNN Loss: 4.39742374420166 | BCE Loss: 1.0640599727630615\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 5.410923004150391 | KNN Loss: 4.385009765625 | BCE Loss: 1.025913119316101\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 5.409502029418945 | KNN Loss: 4.387357234954834 | BCE Loss: 1.0221445560455322\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 5.428651332855225 | KNN Loss: 4.398203372955322 | BCE Loss: 1.0304478406906128\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 5.381507873535156 | KNN Loss: 4.344173908233643 | BCE Loss: 1.0373339653015137\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 5.407218933105469 | KNN Loss: 4.399388790130615 | BCE Loss: 1.0078299045562744\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 5.418147563934326 | KNN Loss: 4.389275550842285 | BCE Loss: 1.028872013092041\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 5.378226280212402 | KNN Loss: 4.3972907066345215 | BCE Loss: 0.9809353351593018\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 5.403045654296875 | KNN Loss: 4.367859363555908 | BCE Loss: 1.0351860523223877\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 5.339660167694092 | KNN Loss: 4.34177827835083 | BCE Loss: 0.9978820085525513\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 5.450708389282227 | KNN Loss: 4.4116530418396 | BCE Loss: 1.039055347442627\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 5.421027183532715 | KNN Loss: 4.389078617095947 | BCE Loss: 1.0319483280181885\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 5.4140520095825195 | KNN Loss: 4.415049076080322 | BCE Loss: 0.999002993106842\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 5.395318508148193 | KNN Loss: 4.360706806182861 | BCE Loss: 1.0346118211746216\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 5.383390426635742 | KNN Loss: 4.395340919494629 | BCE Loss: 0.9880495667457581\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 5.430231094360352 | KNN Loss: 4.377426624298096 | BCE Loss: 1.0528045892715454\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 5.415653228759766 | KNN Loss: 4.388034820556641 | BCE Loss: 1.0276185274124146\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 5.394664764404297 | KNN Loss: 4.3727240562438965 | BCE Loss: 1.0219404697418213\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 5.364333152770996 | KNN Loss: 4.353631019592285 | BCE Loss: 1.01070237159729\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 5.412629127502441 | KNN Loss: 4.372827529907227 | BCE Loss: 1.0398013591766357\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 5.450634956359863 | KNN Loss: 4.421487808227539 | BCE Loss: 1.0291473865509033\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 5.369434833526611 | KNN Loss: 4.3498101234436035 | BCE Loss: 1.0196248292922974\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 5.3873395919799805 | KNN Loss: 4.357228755950928 | BCE Loss: 1.0301110744476318\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 5.390568733215332 | KNN Loss: 4.355147361755371 | BCE Loss: 1.035421371459961\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 5.3883056640625 | KNN Loss: 4.369505882263184 | BCE Loss: 1.0187996625900269\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 5.442707538604736 | KNN Loss: 4.3857903480529785 | BCE Loss: 1.0569171905517578\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 5.379458427429199 | KNN Loss: 4.378167152404785 | BCE Loss: 1.001291036605835\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 5.40521764755249 | KNN Loss: 4.375485897064209 | BCE Loss: 1.0297316312789917\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 5.438791751861572 | KNN Loss: 4.419850826263428 | BCE Loss: 1.018941044807434\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 5.424817085266113 | KNN Loss: 4.371550559997559 | BCE Loss: 1.0532664060592651\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 5.359719276428223 | KNN Loss: 4.343071937561035 | BCE Loss: 1.0166475772857666\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 5.353311061859131 | KNN Loss: 4.345814228057861 | BCE Loss: 1.007496953010559\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 5.422100067138672 | KNN Loss: 4.393181800842285 | BCE Loss: 1.0289182662963867\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 5.363165378570557 | KNN Loss: 4.350218772888184 | BCE Loss: 1.012946605682373\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 5.4001874923706055 | KNN Loss: 4.369374752044678 | BCE Loss: 1.0308129787445068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 5.388853549957275 | KNN Loss: 4.373793125152588 | BCE Loss: 1.015060544013977\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 5.3996429443359375 | KNN Loss: 4.396242141723633 | BCE Loss: 1.0034005641937256\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 5.392556190490723 | KNN Loss: 4.384540557861328 | BCE Loss: 1.0080156326293945\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 5.398913383483887 | KNN Loss: 4.3583784103393555 | BCE Loss: 1.0405352115631104\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 5.405203819274902 | KNN Loss: 4.370213031768799 | BCE Loss: 1.0349905490875244\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 5.397258758544922 | KNN Loss: 4.376405239105225 | BCE Loss: 1.0208537578582764\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 5.428540229797363 | KNN Loss: 4.3618316650390625 | BCE Loss: 1.0667083263397217\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 5.369275093078613 | KNN Loss: 4.341807842254639 | BCE Loss: 1.027467131614685\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 5.395418167114258 | KNN Loss: 4.367641448974609 | BCE Loss: 1.0277764797210693\n",
      "Epoch   280: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 5.408383369445801 | KNN Loss: 4.373341083526611 | BCE Loss: 1.0350420475006104\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 5.423348426818848 | KNN Loss: 4.424497127532959 | BCE Loss: 0.9988511800765991\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 5.381735324859619 | KNN Loss: 4.364055156707764 | BCE Loss: 1.017680287361145\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 5.408727169036865 | KNN Loss: 4.389119625091553 | BCE Loss: 1.0196075439453125\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 5.363483428955078 | KNN Loss: 4.342179298400879 | BCE Loss: 1.0213041305541992\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 5.360276699066162 | KNN Loss: 4.363800525665283 | BCE Loss: 0.9964760541915894\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 5.430305480957031 | KNN Loss: 4.39999532699585 | BCE Loss: 1.0303103923797607\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 5.394989967346191 | KNN Loss: 4.355922222137451 | BCE Loss: 1.0390675067901611\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 5.426333427429199 | KNN Loss: 4.406186103820801 | BCE Loss: 1.020147442817688\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 5.382096767425537 | KNN Loss: 4.36959981918335 | BCE Loss: 1.0124969482421875\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 5.384686470031738 | KNN Loss: 4.358268737792969 | BCE Loss: 1.0264177322387695\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 5.396031856536865 | KNN Loss: 4.361644744873047 | BCE Loss: 1.034387230873108\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 5.354613780975342 | KNN Loss: 4.354429244995117 | BCE Loss: 1.000184416770935\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 5.4258575439453125 | KNN Loss: 4.3624186515808105 | BCE Loss: 1.063439130783081\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 5.4078145027160645 | KNN Loss: 4.403582572937012 | BCE Loss: 1.0042320489883423\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 5.3792877197265625 | KNN Loss: 4.337401390075684 | BCE Loss: 1.0418860912322998\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 5.383316516876221 | KNN Loss: 4.344108581542969 | BCE Loss: 1.039207935333252\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 5.423862457275391 | KNN Loss: 4.40101432800293 | BCE Loss: 1.02284836769104\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 5.421289443969727 | KNN Loss: 4.362486839294434 | BCE Loss: 1.0588024854660034\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 5.423232078552246 | KNN Loss: 4.389659404754639 | BCE Loss: 1.0335729122161865\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 5.422511100769043 | KNN Loss: 4.403404235839844 | BCE Loss: 1.0191068649291992\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 5.415885925292969 | KNN Loss: 4.3691558837890625 | BCE Loss: 1.0467299222946167\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 5.40203857421875 | KNN Loss: 4.371512413024902 | BCE Loss: 1.0305259227752686\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 5.4174652099609375 | KNN Loss: 4.420313358306885 | BCE Loss: 0.9971516728401184\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 5.389948844909668 | KNN Loss: 4.3584303855896 | BCE Loss: 1.0315182209014893\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 5.3623270988464355 | KNN Loss: 4.3453545570373535 | BCE Loss: 1.016972541809082\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 5.39897346496582 | KNN Loss: 4.355833053588867 | BCE Loss: 1.0431404113769531\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 5.379082679748535 | KNN Loss: 4.354419231414795 | BCE Loss: 1.0246634483337402\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 5.396698474884033 | KNN Loss: 4.351675510406494 | BCE Loss: 1.045022964477539\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 5.383221626281738 | KNN Loss: 4.356719970703125 | BCE Loss: 1.0265016555786133\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 5.463794708251953 | KNN Loss: 4.4012932777404785 | BCE Loss: 1.0625015497207642\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 5.383623123168945 | KNN Loss: 4.350703239440918 | BCE Loss: 1.0329198837280273\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 5.357386589050293 | KNN Loss: 4.34682035446167 | BCE Loss: 1.0105664730072021\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 5.398131847381592 | KNN Loss: 4.370352268218994 | BCE Loss: 1.0277795791625977\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 5.406682968139648 | KNN Loss: 4.365530967712402 | BCE Loss: 1.0411522388458252\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 5.3408708572387695 | KNN Loss: 4.352168560028076 | BCE Loss: 0.9887020587921143\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 5.3913397789001465 | KNN Loss: 4.3984479904174805 | BCE Loss: 0.992891788482666\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 5.355119705200195 | KNN Loss: 4.333776473999023 | BCE Loss: 1.021343469619751\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 5.374826431274414 | KNN Loss: 4.352787971496582 | BCE Loss: 1.022038221359253\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 5.4178242683410645 | KNN Loss: 4.365876197814941 | BCE Loss: 1.051948070526123\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 5.455543518066406 | KNN Loss: 4.415657043457031 | BCE Loss: 1.039886713027954\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 5.369166851043701 | KNN Loss: 4.352358341217041 | BCE Loss: 1.0168086290359497\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 5.369945526123047 | KNN Loss: 4.351719379425049 | BCE Loss: 1.0182262659072876\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 5.419660568237305 | KNN Loss: 4.398486137390137 | BCE Loss: 1.0211741924285889\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 5.391604900360107 | KNN Loss: 4.361313343048096 | BCE Loss: 1.0302915573120117\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 5.407329082489014 | KNN Loss: 4.37752103805542 | BCE Loss: 1.0298080444335938\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 5.3877787590026855 | KNN Loss: 4.372992992401123 | BCE Loss: 1.014785647392273\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 5.363026142120361 | KNN Loss: 4.3498735427856445 | BCE Loss: 1.0131525993347168\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 5.408076286315918 | KNN Loss: 4.36147403717041 | BCE Loss: 1.0466022491455078\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 5.418692588806152 | KNN Loss: 4.374485015869141 | BCE Loss: 1.0442073345184326\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 5.432039260864258 | KNN Loss: 4.370362281799316 | BCE Loss: 1.0616767406463623\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 5.476812362670898 | KNN Loss: 4.4278244972229 | BCE Loss: 1.048987627029419\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 5.439576148986816 | KNN Loss: 4.394012451171875 | BCE Loss: 1.045563817024231\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 5.399215221405029 | KNN Loss: 4.369656562805176 | BCE Loss: 1.029558539390564\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 5.404850959777832 | KNN Loss: 4.370916843414307 | BCE Loss: 1.0339341163635254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 5.380566596984863 | KNN Loss: 4.344812870025635 | BCE Loss: 1.0357539653778076\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 5.433135032653809 | KNN Loss: 4.394646167755127 | BCE Loss: 1.0384891033172607\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 5.355498790740967 | KNN Loss: 4.341590404510498 | BCE Loss: 1.0139082670211792\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 5.3794026374816895 | KNN Loss: 4.360113620758057 | BCE Loss: 1.0192891359329224\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 5.427642822265625 | KNN Loss: 4.418810844421387 | BCE Loss: 1.0088319778442383\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 5.415714263916016 | KNN Loss: 4.382504463195801 | BCE Loss: 1.0332099199295044\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 5.489725112915039 | KNN Loss: 4.473855972290039 | BCE Loss: 1.0158692598342896\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 5.429081916809082 | KNN Loss: 4.40468692779541 | BCE Loss: 1.0243949890136719\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 5.425624370574951 | KNN Loss: 4.359025478363037 | BCE Loss: 1.066598892211914\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 5.40036678314209 | KNN Loss: 4.365218639373779 | BCE Loss: 1.0351479053497314\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 5.451279163360596 | KNN Loss: 4.425168991088867 | BCE Loss: 1.0261101722717285\n",
      "Epoch   291: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 5.4050774574279785 | KNN Loss: 4.358094692230225 | BCE Loss: 1.046982765197754\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 5.35291051864624 | KNN Loss: 4.337081432342529 | BCE Loss: 1.015829086303711\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 5.420581817626953 | KNN Loss: 4.380659103393555 | BCE Loss: 1.0399227142333984\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 5.388980388641357 | KNN Loss: 4.368554592132568 | BCE Loss: 1.020425796508789\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 5.401976585388184 | KNN Loss: 4.387958526611328 | BCE Loss: 1.014017939567566\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 5.404545783996582 | KNN Loss: 4.35262393951416 | BCE Loss: 1.0519216060638428\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 5.405068874359131 | KNN Loss: 4.390480995178223 | BCE Loss: 1.0145878791809082\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 5.401378154754639 | KNN Loss: 4.346546173095703 | BCE Loss: 1.0548319816589355\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 5.394003391265869 | KNN Loss: 4.362837791442871 | BCE Loss: 1.031165599822998\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 5.378646373748779 | KNN Loss: 4.364592552185059 | BCE Loss: 1.0140538215637207\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 5.395265579223633 | KNN Loss: 4.372963905334473 | BCE Loss: 1.0223016738891602\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 5.385195255279541 | KNN Loss: 4.3348917961120605 | BCE Loss: 1.0503034591674805\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 5.378479957580566 | KNN Loss: 4.360804557800293 | BCE Loss: 1.017675518989563\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 5.430746555328369 | KNN Loss: 4.4063801765441895 | BCE Loss: 1.0243663787841797\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 5.3439106941223145 | KNN Loss: 4.344546794891357 | BCE Loss: 0.9993640184402466\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 5.408913612365723 | KNN Loss: 4.356072425842285 | BCE Loss: 1.0528414249420166\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 5.429150581359863 | KNN Loss: 4.385126113891602 | BCE Loss: 1.0440242290496826\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 5.362276077270508 | KNN Loss: 4.366272449493408 | BCE Loss: 0.9960033893585205\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 5.459743499755859 | KNN Loss: 4.397841453552246 | BCE Loss: 1.0619020462036133\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 5.414825439453125 | KNN Loss: 4.384349346160889 | BCE Loss: 1.0304762125015259\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 5.422337532043457 | KNN Loss: 4.396656513214111 | BCE Loss: 1.0256811380386353\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 5.400073051452637 | KNN Loss: 4.360706329345703 | BCE Loss: 1.0393669605255127\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 5.385912895202637 | KNN Loss: 4.360690593719482 | BCE Loss: 1.0252220630645752\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 5.374142646789551 | KNN Loss: 4.364499568939209 | BCE Loss: 1.0096430778503418\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 5.443559646606445 | KNN Loss: 4.407278060913086 | BCE Loss: 1.0362815856933594\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 5.407538414001465 | KNN Loss: 4.369084358215332 | BCE Loss: 1.0384538173675537\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 5.361764907836914 | KNN Loss: 4.358876705169678 | BCE Loss: 1.0028883218765259\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 5.402681350708008 | KNN Loss: 4.378876686096191 | BCE Loss: 1.0238046646118164\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 5.370188236236572 | KNN Loss: 4.367756366729736 | BCE Loss: 1.002431869506836\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 5.4682440757751465 | KNN Loss: 4.4000372886657715 | BCE Loss: 1.0682066679000854\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 5.428476333618164 | KNN Loss: 4.398256301879883 | BCE Loss: 1.0302200317382812\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 5.371797561645508 | KNN Loss: 4.359800815582275 | BCE Loss: 1.0119969844818115\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 5.449493408203125 | KNN Loss: 4.403614044189453 | BCE Loss: 1.045879602432251\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 5.39864444732666 | KNN Loss: 4.350739479064941 | BCE Loss: 1.0479048490524292\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 5.470215797424316 | KNN Loss: 4.4090375900268555 | BCE Loss: 1.0611779689788818\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 5.385488510131836 | KNN Loss: 4.350040912628174 | BCE Loss: 1.035447597503662\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 5.372262001037598 | KNN Loss: 4.35650110244751 | BCE Loss: 1.0157606601715088\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 5.379227161407471 | KNN Loss: 4.3410162925720215 | BCE Loss: 1.0382108688354492\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 5.42769718170166 | KNN Loss: 4.415826320648193 | BCE Loss: 1.011871099472046\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 5.394430637359619 | KNN Loss: 4.3506760597229 | BCE Loss: 1.0437544584274292\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 5.387083053588867 | KNN Loss: 4.35114860534668 | BCE Loss: 1.0359342098236084\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 5.44117546081543 | KNN Loss: 4.399389266967773 | BCE Loss: 1.0417860746383667\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 5.376311302185059 | KNN Loss: 4.353641033172607 | BCE Loss: 1.022670030593872\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 5.418540000915527 | KNN Loss: 4.407315731048584 | BCE Loss: 1.0112242698669434\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 5.363396167755127 | KNN Loss: 4.349211692810059 | BCE Loss: 1.0141844749450684\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 5.427386283874512 | KNN Loss: 4.398399829864502 | BCE Loss: 1.0289866924285889\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 5.388154983520508 | KNN Loss: 4.341198921203613 | BCE Loss: 1.046956181526184\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 5.426438808441162 | KNN Loss: 4.407531261444092 | BCE Loss: 1.0189075469970703\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 5.369734764099121 | KNN Loss: 4.340665340423584 | BCE Loss: 1.029069423675537\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 5.352443695068359 | KNN Loss: 4.354917049407959 | BCE Loss: 0.9975265264511108\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 5.464897155761719 | KNN Loss: 4.391218185424805 | BCE Loss: 1.073678970336914\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 5.374256610870361 | KNN Loss: 4.350617408752441 | BCE Loss: 1.0236390829086304\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 5.421460151672363 | KNN Loss: 4.364624977111816 | BCE Loss: 1.056835412979126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 5.402722358703613 | KNN Loss: 4.363831043243408 | BCE Loss: 1.038891315460205\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 5.426999568939209 | KNN Loss: 4.386746406555176 | BCE Loss: 1.0402531623840332\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 5.384426593780518 | KNN Loss: 4.363600254058838 | BCE Loss: 1.0208264589309692\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 5.3980207443237305 | KNN Loss: 4.37890625 | BCE Loss: 1.0191142559051514\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 5.495565414428711 | KNN Loss: 4.469158172607422 | BCE Loss: 1.026407241821289\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 5.411613941192627 | KNN Loss: 4.373678684234619 | BCE Loss: 1.0379351377487183\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 5.397150039672852 | KNN Loss: 4.370487689971924 | BCE Loss: 1.0266621112823486\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 5.426206111907959 | KNN Loss: 4.3838911056518555 | BCE Loss: 1.0423150062561035\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 5.38101863861084 | KNN Loss: 4.352344989776611 | BCE Loss: 1.028673768043518\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 5.416255474090576 | KNN Loss: 4.380763530731201 | BCE Loss: 1.0354920625686646\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 5.416180610656738 | KNN Loss: 4.374411582946777 | BCE Loss: 1.0417687892913818\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 5.363184928894043 | KNN Loss: 4.351931571960449 | BCE Loss: 1.0112531185150146\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 5.42304801940918 | KNN Loss: 4.3935227394104 | BCE Loss: 1.0295252799987793\n",
      "Epoch   302: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 5.395211219787598 | KNN Loss: 4.351799964904785 | BCE Loss: 1.0434112548828125\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 5.436621189117432 | KNN Loss: 4.4031805992126465 | BCE Loss: 1.0334404706954956\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 5.3840532302856445 | KNN Loss: 4.359885215759277 | BCE Loss: 1.0241682529449463\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 5.383758068084717 | KNN Loss: 4.382175445556641 | BCE Loss: 1.0015826225280762\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 5.497701644897461 | KNN Loss: 4.470304489135742 | BCE Loss: 1.0273971557617188\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 5.415841579437256 | KNN Loss: 4.37408971786499 | BCE Loss: 1.041751742362976\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 5.393019676208496 | KNN Loss: 4.365077018737793 | BCE Loss: 1.0279426574707031\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 5.417067050933838 | KNN Loss: 4.3822340965271 | BCE Loss: 1.0348329544067383\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 5.421364784240723 | KNN Loss: 4.369081974029541 | BCE Loss: 1.052282691001892\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 5.396066665649414 | KNN Loss: 4.389954090118408 | BCE Loss: 1.0061125755310059\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 5.391646385192871 | KNN Loss: 4.354954242706299 | BCE Loss: 1.0366921424865723\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 5.390023231506348 | KNN Loss: 4.37652587890625 | BCE Loss: 1.0134973526000977\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 5.393802642822266 | KNN Loss: 4.345427989959717 | BCE Loss: 1.048374891281128\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 5.399388790130615 | KNN Loss: 4.353446960449219 | BCE Loss: 1.045941948890686\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 5.39754581451416 | KNN Loss: 4.371073246002197 | BCE Loss: 1.026472806930542\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 5.438065052032471 | KNN Loss: 4.3814215660095215 | BCE Loss: 1.0566434860229492\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 5.382743835449219 | KNN Loss: 4.3445916175842285 | BCE Loss: 1.0381519794464111\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 5.434682369232178 | KNN Loss: 4.408796787261963 | BCE Loss: 1.0258854627609253\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 5.387592315673828 | KNN Loss: 4.352563858032227 | BCE Loss: 1.0350282192230225\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 5.410229682922363 | KNN Loss: 4.353569984436035 | BCE Loss: 1.0566599369049072\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 5.405653953552246 | KNN Loss: 4.36007833480835 | BCE Loss: 1.0455753803253174\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 5.353817939758301 | KNN Loss: 4.362151622772217 | BCE Loss: 0.9916661381721497\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 5.405284881591797 | KNN Loss: 4.372365474700928 | BCE Loss: 1.0329194068908691\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 5.449918746948242 | KNN Loss: 4.412693023681641 | BCE Loss: 1.0372259616851807\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 5.387059211730957 | KNN Loss: 4.339533805847168 | BCE Loss: 1.0475252866744995\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 5.385836601257324 | KNN Loss: 4.365975856781006 | BCE Loss: 1.0198605060577393\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 5.418122291564941 | KNN Loss: 4.393368721008301 | BCE Loss: 1.0247538089752197\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 5.395723819732666 | KNN Loss: 4.359516143798828 | BCE Loss: 1.036207675933838\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 5.41124153137207 | KNN Loss: 4.376389503479004 | BCE Loss: 1.0348517894744873\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 5.436578273773193 | KNN Loss: 4.397209167480469 | BCE Loss: 1.039368987083435\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 5.421065807342529 | KNN Loss: 4.382615089416504 | BCE Loss: 1.0384505987167358\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 5.468155384063721 | KNN Loss: 4.445117473602295 | BCE Loss: 1.0230379104614258\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 5.395023345947266 | KNN Loss: 4.360540390014648 | BCE Loss: 1.0344831943511963\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 5.382105827331543 | KNN Loss: 4.369906425476074 | BCE Loss: 1.0121994018554688\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 5.419837474822998 | KNN Loss: 4.382420539855957 | BCE Loss: 1.0374168157577515\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 5.361370086669922 | KNN Loss: 4.348531246185303 | BCE Loss: 1.01283860206604\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 5.397279262542725 | KNN Loss: 4.348560333251953 | BCE Loss: 1.048719048500061\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 5.399794578552246 | KNN Loss: 4.363366603851318 | BCE Loss: 1.0364279747009277\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 5.43414306640625 | KNN Loss: 4.410401344299316 | BCE Loss: 1.0237418413162231\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 5.412267208099365 | KNN Loss: 4.384732246398926 | BCE Loss: 1.0275349617004395\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 5.364898681640625 | KNN Loss: 4.344770908355713 | BCE Loss: 1.020127534866333\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 5.336222171783447 | KNN Loss: 4.348581314086914 | BCE Loss: 0.9876410365104675\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 5.386979579925537 | KNN Loss: 4.352573394775391 | BCE Loss: 1.034406304359436\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 5.397554397583008 | KNN Loss: 4.345125198364258 | BCE Loss: 1.052428960800171\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 5.364875793457031 | KNN Loss: 4.329537868499756 | BCE Loss: 1.0353376865386963\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 5.392850399017334 | KNN Loss: 4.387357711791992 | BCE Loss: 1.0054926872253418\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 5.3811821937561035 | KNN Loss: 4.365312576293945 | BCE Loss: 1.0158696174621582\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 5.387233734130859 | KNN Loss: 4.354134559631348 | BCE Loss: 1.0330994129180908\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 5.378978729248047 | KNN Loss: 4.348055362701416 | BCE Loss: 1.0309233665466309\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 5.353140354156494 | KNN Loss: 4.362246036529541 | BCE Loss: 0.9908941388130188\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 5.421751022338867 | KNN Loss: 4.385900974273682 | BCE Loss: 1.0358500480651855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 5.4004292488098145 | KNN Loss: 4.34157133102417 | BCE Loss: 1.0588579177856445\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 5.428223609924316 | KNN Loss: 4.386580944061279 | BCE Loss: 1.0416425466537476\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 5.385236740112305 | KNN Loss: 4.357962608337402 | BCE Loss: 1.027274250984192\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 5.369210243225098 | KNN Loss: 4.35513973236084 | BCE Loss: 1.0140706300735474\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 5.376352310180664 | KNN Loss: 4.364897727966309 | BCE Loss: 1.0114543437957764\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 5.383723258972168 | KNN Loss: 4.367844581604004 | BCE Loss: 1.015878438949585\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 5.401627540588379 | KNN Loss: 4.374452114105225 | BCE Loss: 1.0271755456924438\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 5.420899868011475 | KNN Loss: 4.371690273284912 | BCE Loss: 1.0492095947265625\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 5.402491569519043 | KNN Loss: 4.372148513793945 | BCE Loss: 1.030342936515808\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 5.36751127243042 | KNN Loss: 4.354341983795166 | BCE Loss: 1.013169288635254\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 5.404816627502441 | KNN Loss: 4.356811046600342 | BCE Loss: 1.0480055809020996\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 5.375421047210693 | KNN Loss: 4.377863883972168 | BCE Loss: 0.9975571632385254\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 5.5049285888671875 | KNN Loss: 4.446308135986328 | BCE Loss: 1.0586202144622803\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 5.42996883392334 | KNN Loss: 4.382556438446045 | BCE Loss: 1.0474121570587158\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 5.337898254394531 | KNN Loss: 4.336097717285156 | BCE Loss: 1.001800298690796\n",
      "Epoch   313: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 5.402479648590088 | KNN Loss: 4.369365215301514 | BCE Loss: 1.0331144332885742\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 5.375129699707031 | KNN Loss: 4.3805460929870605 | BCE Loss: 0.9945834279060364\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 5.379969120025635 | KNN Loss: 4.367154121398926 | BCE Loss: 1.0128148794174194\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 5.386185646057129 | KNN Loss: 4.371713638305664 | BCE Loss: 1.0144717693328857\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 5.369447708129883 | KNN Loss: 4.344602584838867 | BCE Loss: 1.0248451232910156\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 5.456223011016846 | KNN Loss: 4.411849021911621 | BCE Loss: 1.044373869895935\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 5.427182674407959 | KNN Loss: 4.404842853546143 | BCE Loss: 1.0223397016525269\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 5.4873576164245605 | KNN Loss: 4.412166118621826 | BCE Loss: 1.0751913785934448\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 5.44052791595459 | KNN Loss: 4.391427993774414 | BCE Loss: 1.0491001605987549\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 5.384545803070068 | KNN Loss: 4.360849857330322 | BCE Loss: 1.023695945739746\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 5.390110969543457 | KNN Loss: 4.36382532119751 | BCE Loss: 1.0262854099273682\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 5.390519618988037 | KNN Loss: 4.369554042816162 | BCE Loss: 1.0209654569625854\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 5.433380126953125 | KNN Loss: 4.374229431152344 | BCE Loss: 1.0591509342193604\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 5.397464752197266 | KNN Loss: 4.350704669952393 | BCE Loss: 1.0467603206634521\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 5.383173942565918 | KNN Loss: 4.376514911651611 | BCE Loss: 1.0066590309143066\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 5.382579803466797 | KNN Loss: 4.351418495178223 | BCE Loss: 1.0311613082885742\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 5.397708892822266 | KNN Loss: 4.383853912353516 | BCE Loss: 1.01385498046875\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 5.432839393615723 | KNN Loss: 4.391697406768799 | BCE Loss: 1.0411419868469238\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 5.387009620666504 | KNN Loss: 4.371683597564697 | BCE Loss: 1.0153260231018066\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 5.403145790100098 | KNN Loss: 4.401113033294678 | BCE Loss: 1.002032995223999\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 5.411823272705078 | KNN Loss: 4.356068134307861 | BCE Loss: 1.0557548999786377\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 5.368844509124756 | KNN Loss: 4.376185417175293 | BCE Loss: 0.9926590919494629\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 5.407050132751465 | KNN Loss: 4.359713554382324 | BCE Loss: 1.0473363399505615\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 5.433075904846191 | KNN Loss: 4.385936260223389 | BCE Loss: 1.0471398830413818\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 5.422529220581055 | KNN Loss: 4.395434856414795 | BCE Loss: 1.0270946025848389\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 5.386283874511719 | KNN Loss: 4.352881908416748 | BCE Loss: 1.0334022045135498\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 5.381103992462158 | KNN Loss: 4.363052845001221 | BCE Loss: 1.0180511474609375\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 5.373589038848877 | KNN Loss: 4.372049808502197 | BCE Loss: 1.0015392303466797\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 5.4226789474487305 | KNN Loss: 4.42157506942749 | BCE Loss: 1.0011041164398193\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 5.407203674316406 | KNN Loss: 4.3752288818359375 | BCE Loss: 1.0319747924804688\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 5.402617454528809 | KNN Loss: 4.355532169342041 | BCE Loss: 1.0470855236053467\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 5.388118267059326 | KNN Loss: 4.343339920043945 | BCE Loss: 1.0447784662246704\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 5.425107955932617 | KNN Loss: 4.3739776611328125 | BCE Loss: 1.0511300563812256\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 5.361137390136719 | KNN Loss: 4.334676265716553 | BCE Loss: 1.0264610052108765\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 5.4415082931518555 | KNN Loss: 4.388785362243652 | BCE Loss: 1.0527231693267822\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 5.448123931884766 | KNN Loss: 4.425373077392578 | BCE Loss: 1.0227510929107666\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 5.367821216583252 | KNN Loss: 4.345556259155273 | BCE Loss: 1.0222649574279785\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 5.388568878173828 | KNN Loss: 4.368429183959961 | BCE Loss: 1.020139455795288\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 5.349067211151123 | KNN Loss: 4.337038040161133 | BCE Loss: 1.0120291709899902\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 5.394401550292969 | KNN Loss: 4.37703800201416 | BCE Loss: 1.0173633098602295\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 5.363246917724609 | KNN Loss: 4.359846115112305 | BCE Loss: 1.0034010410308838\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 5.409502029418945 | KNN Loss: 4.371827602386475 | BCE Loss: 1.0376746654510498\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 5.400576591491699 | KNN Loss: 4.363804817199707 | BCE Loss: 1.0367717742919922\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 5.4022297859191895 | KNN Loss: 4.3930230140686035 | BCE Loss: 1.0092066526412964\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 5.427774429321289 | KNN Loss: 4.403136730194092 | BCE Loss: 1.0246378183364868\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 5.356435298919678 | KNN Loss: 4.341766834259033 | BCE Loss: 1.014668583869934\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 5.43023681640625 | KNN Loss: 4.393561840057373 | BCE Loss: 1.0366748571395874\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 5.358942985534668 | KNN Loss: 4.346316814422607 | BCE Loss: 1.0126261711120605\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 5.461719512939453 | KNN Loss: 4.432620525360107 | BCE Loss: 1.0290992259979248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 5.38386344909668 | KNN Loss: 4.342007637023926 | BCE Loss: 1.041855812072754\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 5.460561275482178 | KNN Loss: 4.411808490753174 | BCE Loss: 1.0487526655197144\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 5.411027431488037 | KNN Loss: 4.394240379333496 | BCE Loss: 1.016787052154541\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 5.392977237701416 | KNN Loss: 4.3748884201049805 | BCE Loss: 1.0180888175964355\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 5.395218372344971 | KNN Loss: 4.3823347091674805 | BCE Loss: 1.0128835439682007\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 5.418330192565918 | KNN Loss: 4.40716552734375 | BCE Loss: 1.0111644268035889\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 5.407203674316406 | KNN Loss: 4.37639045715332 | BCE Loss: 1.030813217163086\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 5.383775234222412 | KNN Loss: 4.348777770996094 | BCE Loss: 1.0349973440170288\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 5.396859169006348 | KNN Loss: 4.38330602645874 | BCE Loss: 1.0135529041290283\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 5.462172508239746 | KNN Loss: 4.416389465332031 | BCE Loss: 1.045783281326294\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 5.4145097732543945 | KNN Loss: 4.374845027923584 | BCE Loss: 1.0396647453308105\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 5.429409503936768 | KNN Loss: 4.398462295532227 | BCE Loss: 1.030947208404541\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 5.445903301239014 | KNN Loss: 4.402979850769043 | BCE Loss: 1.0429233312606812\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 5.3654398918151855 | KNN Loss: 4.369865417480469 | BCE Loss: 0.9955745339393616\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 5.406002044677734 | KNN Loss: 4.357330322265625 | BCE Loss: 1.0486714839935303\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 5.405496597290039 | KNN Loss: 4.370084762573242 | BCE Loss: 1.0354117155075073\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 5.388615608215332 | KNN Loss: 4.379179000854492 | BCE Loss: 1.0094364881515503\n",
      "Epoch   324: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 5.39476203918457 | KNN Loss: 4.367799758911133 | BCE Loss: 1.0269620418548584\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 5.429856777191162 | KNN Loss: 4.371646881103516 | BCE Loss: 1.058209776878357\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 5.410160541534424 | KNN Loss: 4.371295928955078 | BCE Loss: 1.0388646125793457\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 5.425361633300781 | KNN Loss: 4.393124103546143 | BCE Loss: 1.0322372913360596\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 5.44342041015625 | KNN Loss: 4.403798580169678 | BCE Loss: 1.0396215915679932\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 5.371809005737305 | KNN Loss: 4.359895706176758 | BCE Loss: 1.0119130611419678\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 5.405898094177246 | KNN Loss: 4.356256008148193 | BCE Loss: 1.0496420860290527\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 5.4209394454956055 | KNN Loss: 4.425695419311523 | BCE Loss: 0.9952441453933716\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 5.423474311828613 | KNN Loss: 4.387486934661865 | BCE Loss: 1.0359876155853271\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 5.388967037200928 | KNN Loss: 4.3682661056518555 | BCE Loss: 1.0207009315490723\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 5.377535820007324 | KNN Loss: 4.359187602996826 | BCE Loss: 1.0183484554290771\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 5.390355587005615 | KNN Loss: 4.355953216552734 | BCE Loss: 1.0344022512435913\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 5.383654594421387 | KNN Loss: 4.3575968742370605 | BCE Loss: 1.0260579586029053\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 5.379146575927734 | KNN Loss: 4.348970890045166 | BCE Loss: 1.0301755666732788\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 5.431082725524902 | KNN Loss: 4.398402214050293 | BCE Loss: 1.0326802730560303\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 5.39180850982666 | KNN Loss: 4.364379405975342 | BCE Loss: 1.0274291038513184\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 5.464015960693359 | KNN Loss: 4.425695419311523 | BCE Loss: 1.038320541381836\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 5.377220153808594 | KNN Loss: 4.367345809936523 | BCE Loss: 1.0098741054534912\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 5.439809322357178 | KNN Loss: 4.382999420166016 | BCE Loss: 1.056809902191162\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 5.440520286560059 | KNN Loss: 4.3941874504089355 | BCE Loss: 1.046332836151123\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 5.401199817657471 | KNN Loss: 4.36717414855957 | BCE Loss: 1.03402578830719\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 5.378355026245117 | KNN Loss: 4.355275630950928 | BCE Loss: 1.0230796337127686\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 5.396332263946533 | KNN Loss: 4.368042945861816 | BCE Loss: 1.0282891988754272\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 5.425124645233154 | KNN Loss: 4.385392665863037 | BCE Loss: 1.0397318601608276\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 5.404295921325684 | KNN Loss: 4.374973297119141 | BCE Loss: 1.0293223857879639\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 5.405035018920898 | KNN Loss: 4.37497615814209 | BCE Loss: 1.0300589799880981\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 5.447434902191162 | KNN Loss: 4.4053120613098145 | BCE Loss: 1.0421229600906372\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 5.395807266235352 | KNN Loss: 4.35582971572876 | BCE Loss: 1.039977788925171\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 5.416696548461914 | KNN Loss: 4.384254455566406 | BCE Loss: 1.0324418544769287\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 5.414074420928955 | KNN Loss: 4.3815836906433105 | BCE Loss: 1.0324907302856445\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 5.386384963989258 | KNN Loss: 4.359639644622803 | BCE Loss: 1.0267452001571655\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 5.355350017547607 | KNN Loss: 4.370335102081299 | BCE Loss: 0.9850150346755981\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 5.418054580688477 | KNN Loss: 4.3579840660095215 | BCE Loss: 1.060070514678955\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 5.384322643280029 | KNN Loss: 4.35281229019165 | BCE Loss: 1.031510353088379\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 5.437262058258057 | KNN Loss: 4.4080705642700195 | BCE Loss: 1.0291916131973267\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 5.423072814941406 | KNN Loss: 4.400792121887207 | BCE Loss: 1.0222808122634888\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 5.41120719909668 | KNN Loss: 4.37630558013916 | BCE Loss: 1.0349018573760986\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 5.397609233856201 | KNN Loss: 4.388552665710449 | BCE Loss: 1.009056568145752\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 5.435659885406494 | KNN Loss: 4.390225410461426 | BCE Loss: 1.0454344749450684\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 5.408601760864258 | KNN Loss: 4.384454727172852 | BCE Loss: 1.0241470336914062\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 5.366135120391846 | KNN Loss: 4.359624862670898 | BCE Loss: 1.0065102577209473\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 5.40091609954834 | KNN Loss: 4.348170757293701 | BCE Loss: 1.0527454614639282\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 5.363861083984375 | KNN Loss: 4.336729526519775 | BCE Loss: 1.0271315574645996\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 5.360676288604736 | KNN Loss: 4.3491692543029785 | BCE Loss: 1.0115070343017578\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 5.355791091918945 | KNN Loss: 4.341558933258057 | BCE Loss: 1.0142323970794678\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 5.422172546386719 | KNN Loss: 4.3660888671875 | BCE Loss: 1.0560836791992188\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 5.364498615264893 | KNN Loss: 4.353572845458984 | BCE Loss: 1.0109257698059082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 5.446134090423584 | KNN Loss: 4.411021709442139 | BCE Loss: 1.0351123809814453\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 5.407476902008057 | KNN Loss: 4.366114139556885 | BCE Loss: 1.0413627624511719\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 5.422477722167969 | KNN Loss: 4.3732194900512695 | BCE Loss: 1.0492579936981201\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 5.433860778808594 | KNN Loss: 4.405467987060547 | BCE Loss: 1.028393030166626\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 5.423696517944336 | KNN Loss: 4.408357620239258 | BCE Loss: 1.015338659286499\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 5.436310291290283 | KNN Loss: 4.393581867218018 | BCE Loss: 1.0427284240722656\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 5.403205394744873 | KNN Loss: 4.362895488739014 | BCE Loss: 1.0403097867965698\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 5.359805107116699 | KNN Loss: 4.340354919433594 | BCE Loss: 1.019450306892395\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 5.35953426361084 | KNN Loss: 4.356800079345703 | BCE Loss: 1.0027344226837158\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 5.445201873779297 | KNN Loss: 4.427577972412109 | BCE Loss: 1.0176239013671875\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 5.3968329429626465 | KNN Loss: 4.378823280334473 | BCE Loss: 1.0180095434188843\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 5.441524028778076 | KNN Loss: 4.372629642486572 | BCE Loss: 1.068894386291504\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 5.409050941467285 | KNN Loss: 4.3879265785217285 | BCE Loss: 1.0211241245269775\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 5.384457588195801 | KNN Loss: 4.362198352813721 | BCE Loss: 1.0222594738006592\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 5.357967853546143 | KNN Loss: 4.340468406677246 | BCE Loss: 1.017499327659607\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 5.414026260375977 | KNN Loss: 4.3751397132873535 | BCE Loss: 1.038886308670044\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 5.414407730102539 | KNN Loss: 4.366068363189697 | BCE Loss: 1.048339605331421\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 5.404595851898193 | KNN Loss: 4.377559185028076 | BCE Loss: 1.0270366668701172\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 5.435572624206543 | KNN Loss: 4.382319927215576 | BCE Loss: 1.0532526969909668\n",
      "Epoch   335: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 5.3776350021362305 | KNN Loss: 4.364867210388184 | BCE Loss: 1.012768030166626\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 5.420493125915527 | KNN Loss: 4.38545560836792 | BCE Loss: 1.0350377559661865\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 5.445815563201904 | KNN Loss: 4.396168231964111 | BCE Loss: 1.0496472120285034\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 5.4058427810668945 | KNN Loss: 4.370611190795898 | BCE Loss: 1.035231351852417\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 5.4368672370910645 | KNN Loss: 4.391620635986328 | BCE Loss: 1.0452464818954468\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 5.407123565673828 | KNN Loss: 4.365212917327881 | BCE Loss: 1.0419106483459473\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 5.3982977867126465 | KNN Loss: 4.356551170349121 | BCE Loss: 1.0417464971542358\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 5.411805152893066 | KNN Loss: 4.3667449951171875 | BCE Loss: 1.045060157775879\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 5.359930515289307 | KNN Loss: 4.3634138107299805 | BCE Loss: 0.9965168237686157\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 5.38201379776001 | KNN Loss: 4.338655471801758 | BCE Loss: 1.043358325958252\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 5.386363506317139 | KNN Loss: 4.359501838684082 | BCE Loss: 1.0268616676330566\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 5.383655548095703 | KNN Loss: 4.357568740844727 | BCE Loss: 1.0260870456695557\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 5.392719745635986 | KNN Loss: 4.357667922973633 | BCE Loss: 1.0350518226623535\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 5.440093040466309 | KNN Loss: 4.402982234954834 | BCE Loss: 1.0371110439300537\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 5.360084533691406 | KNN Loss: 4.358011722564697 | BCE Loss: 1.0020726919174194\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 5.413778305053711 | KNN Loss: 4.372503280639648 | BCE Loss: 1.041274905204773\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 5.379064559936523 | KNN Loss: 4.364708423614502 | BCE Loss: 1.014356255531311\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 5.397417068481445 | KNN Loss: 4.362436771392822 | BCE Loss: 1.034980297088623\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 5.373884201049805 | KNN Loss: 4.363580703735352 | BCE Loss: 1.0103036165237427\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 5.373884677886963 | KNN Loss: 4.3522257804870605 | BCE Loss: 1.0216588973999023\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 5.351159572601318 | KNN Loss: 4.3362627029418945 | BCE Loss: 1.0148968696594238\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 5.410707473754883 | KNN Loss: 4.390954494476318 | BCE Loss: 1.0197529792785645\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 5.380467414855957 | KNN Loss: 4.367708683013916 | BCE Loss: 1.012758493423462\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 5.380242347717285 | KNN Loss: 4.3475823402404785 | BCE Loss: 1.0326597690582275\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 5.331167697906494 | KNN Loss: 4.337912559509277 | BCE Loss: 0.9932552576065063\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 5.417024612426758 | KNN Loss: 4.394437313079834 | BCE Loss: 1.0225870609283447\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 5.4508891105651855 | KNN Loss: 4.4013285636901855 | BCE Loss: 1.049560546875\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 5.408402442932129 | KNN Loss: 4.376766204833984 | BCE Loss: 1.031636118888855\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 5.446255207061768 | KNN Loss: 4.393103122711182 | BCE Loss: 1.053152084350586\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 5.388870716094971 | KNN Loss: 4.362602233886719 | BCE Loss: 1.0262683629989624\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 5.385255813598633 | KNN Loss: 4.358891487121582 | BCE Loss: 1.0263644456863403\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 5.400918960571289 | KNN Loss: 4.388876438140869 | BCE Loss: 1.0120426416397095\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 5.438541889190674 | KNN Loss: 4.389225959777832 | BCE Loss: 1.0493159294128418\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 5.445076942443848 | KNN Loss: 4.400730609893799 | BCE Loss: 1.0443463325500488\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 5.419964790344238 | KNN Loss: 4.408463478088379 | BCE Loss: 1.0115013122558594\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 5.375609874725342 | KNN Loss: 4.363842964172363 | BCE Loss: 1.0117669105529785\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 5.359447479248047 | KNN Loss: 4.340899467468262 | BCE Loss: 1.0185480117797852\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 5.369476318359375 | KNN Loss: 4.343911170959473 | BCE Loss: 1.0255649089813232\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 5.443209171295166 | KNN Loss: 4.395912170410156 | BCE Loss: 1.0472970008850098\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 5.452977657318115 | KNN Loss: 4.3904218673706055 | BCE Loss: 1.0625559091567993\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 5.399876594543457 | KNN Loss: 4.353130340576172 | BCE Loss: 1.046746015548706\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 5.411749839782715 | KNN Loss: 4.389896392822266 | BCE Loss: 1.0218534469604492\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 5.486763954162598 | KNN Loss: 4.42442512512207 | BCE Loss: 1.0623390674591064\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 5.430423259735107 | KNN Loss: 4.396174907684326 | BCE Loss: 1.0342483520507812\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 5.344516754150391 | KNN Loss: 4.351118087768555 | BCE Loss: 0.9933984875679016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 5.449846267700195 | KNN Loss: 4.4107160568237305 | BCE Loss: 1.0391300916671753\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 5.355711460113525 | KNN Loss: 4.359804153442383 | BCE Loss: 0.9959074854850769\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 5.4524431228637695 | KNN Loss: 4.402338027954102 | BCE Loss: 1.050105094909668\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 5.374826908111572 | KNN Loss: 4.371763229370117 | BCE Loss: 1.003063678741455\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 5.434188365936279 | KNN Loss: 4.382929801940918 | BCE Loss: 1.0512584447860718\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 5.376378059387207 | KNN Loss: 4.356935024261475 | BCE Loss: 1.0194430351257324\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 5.396095275878906 | KNN Loss: 4.36132287979126 | BCE Loss: 1.0347723960876465\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 5.379488945007324 | KNN Loss: 4.367842197418213 | BCE Loss: 1.0116467475891113\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 5.422818183898926 | KNN Loss: 4.382806777954102 | BCE Loss: 1.0400111675262451\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 5.395281791687012 | KNN Loss: 4.376576900482178 | BCE Loss: 1.0187047719955444\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 5.427321434020996 | KNN Loss: 4.380311012268066 | BCE Loss: 1.0470104217529297\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 5.403546333312988 | KNN Loss: 4.390293121337891 | BCE Loss: 1.0132534503936768\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 5.3526482582092285 | KNN Loss: 4.347936630249023 | BCE Loss: 1.004711627960205\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 5.405205726623535 | KNN Loss: 4.361873149871826 | BCE Loss: 1.0433323383331299\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 5.413686752319336 | KNN Loss: 4.406942844390869 | BCE Loss: 1.0067440271377563\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 5.377002716064453 | KNN Loss: 4.348775386810303 | BCE Loss: 1.0282273292541504\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 5.426680088043213 | KNN Loss: 4.4092512130737305 | BCE Loss: 1.017428994178772\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 5.424172401428223 | KNN Loss: 4.392310619354248 | BCE Loss: 1.0318615436553955\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 5.417189598083496 | KNN Loss: 4.386396408081055 | BCE Loss: 1.0307931900024414\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 5.394063472747803 | KNN Loss: 4.367273807525635 | BCE Loss: 1.0267895460128784\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 5.410358905792236 | KNN Loss: 4.390816688537598 | BCE Loss: 1.0195422172546387\n",
      "Epoch   346: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 5.364233493804932 | KNN Loss: 4.342133522033691 | BCE Loss: 1.0220998525619507\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 5.439365386962891 | KNN Loss: 4.401224136352539 | BCE Loss: 1.0381414890289307\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 5.455728530883789 | KNN Loss: 4.4099249839782715 | BCE Loss: 1.0458037853240967\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 5.385530948638916 | KNN Loss: 4.392167091369629 | BCE Loss: 0.9933637380599976\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 5.381296157836914 | KNN Loss: 4.335554599761963 | BCE Loss: 1.0457414388656616\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 5.420066833496094 | KNN Loss: 4.39427375793457 | BCE Loss: 1.0257928371429443\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 5.3783111572265625 | KNN Loss: 4.346296787261963 | BCE Loss: 1.0320144891738892\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 5.4104485511779785 | KNN Loss: 4.390998363494873 | BCE Loss: 1.019450068473816\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 5.3580002784729 | KNN Loss: 4.359219074249268 | BCE Loss: 0.9987810850143433\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 5.3809661865234375 | KNN Loss: 4.353253364562988 | BCE Loss: 1.0277125835418701\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 5.432251930236816 | KNN Loss: 4.369360446929932 | BCE Loss: 1.0628917217254639\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 5.358221054077148 | KNN Loss: 4.343514442443848 | BCE Loss: 1.0147067308425903\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 5.370583534240723 | KNN Loss: 4.358957767486572 | BCE Loss: 1.0116255283355713\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 5.397041320800781 | KNN Loss: 4.381163120269775 | BCE Loss: 1.015878438949585\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 5.4360809326171875 | KNN Loss: 4.40828275680542 | BCE Loss: 1.0277982950210571\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 5.433213710784912 | KNN Loss: 4.4125237464904785 | BCE Loss: 1.0206899642944336\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 5.439576148986816 | KNN Loss: 4.37529993057251 | BCE Loss: 1.0642759799957275\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 5.351503372192383 | KNN Loss: 4.351009845733643 | BCE Loss: 1.0004937648773193\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 5.399487495422363 | KNN Loss: 4.368110179901123 | BCE Loss: 1.0313773155212402\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 5.373575210571289 | KNN Loss: 4.351879119873047 | BCE Loss: 1.0216960906982422\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 5.441702842712402 | KNN Loss: 4.380190849304199 | BCE Loss: 1.0615122318267822\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 5.404096603393555 | KNN Loss: 4.3708672523498535 | BCE Loss: 1.0332295894622803\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 5.405684471130371 | KNN Loss: 4.38444185256958 | BCE Loss: 1.021242618560791\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 5.403629302978516 | KNN Loss: 4.363898754119873 | BCE Loss: 1.0397303104400635\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 5.396385192871094 | KNN Loss: 4.379316806793213 | BCE Loss: 1.0170685052871704\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 5.420454025268555 | KNN Loss: 4.426412105560303 | BCE Loss: 0.9940416812896729\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 5.352517127990723 | KNN Loss: 4.357056617736816 | BCE Loss: 0.9954606294631958\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 5.402523040771484 | KNN Loss: 4.365983963012695 | BCE Loss: 1.0365389585494995\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 5.359510898590088 | KNN Loss: 4.345463275909424 | BCE Loss: 1.0140475034713745\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 5.348101615905762 | KNN Loss: 4.34111213684082 | BCE Loss: 1.0069892406463623\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 5.402409553527832 | KNN Loss: 4.371771335601807 | BCE Loss: 1.0306384563446045\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 5.376229763031006 | KNN Loss: 4.363674163818359 | BCE Loss: 1.012555718421936\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 5.375794410705566 | KNN Loss: 4.3570876121521 | BCE Loss: 1.0187065601348877\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 5.369424819946289 | KNN Loss: 4.350674152374268 | BCE Loss: 1.0187506675720215\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 5.377929210662842 | KNN Loss: 4.344427108764648 | BCE Loss: 1.0335021018981934\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 5.393723487854004 | KNN Loss: 4.362021446228027 | BCE Loss: 1.0317022800445557\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 5.4157562255859375 | KNN Loss: 4.397121906280518 | BCE Loss: 1.0186340808868408\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 5.4107561111450195 | KNN Loss: 4.363384246826172 | BCE Loss: 1.0473719835281372\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 5.411625385284424 | KNN Loss: 4.373602390289307 | BCE Loss: 1.0380229949951172\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 5.443609237670898 | KNN Loss: 4.40045166015625 | BCE Loss: 1.0431575775146484\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 5.412564277648926 | KNN Loss: 4.356567859649658 | BCE Loss: 1.0559961795806885\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 5.414272308349609 | KNN Loss: 4.372834205627441 | BCE Loss: 1.0414379835128784\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 5.382193565368652 | KNN Loss: 4.348537445068359 | BCE Loss: 1.033656120300293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 5.343733787536621 | KNN Loss: 4.328978061676025 | BCE Loss: 1.0147559642791748\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 5.3775739669799805 | KNN Loss: 4.347911834716797 | BCE Loss: 1.0296621322631836\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 5.395534038543701 | KNN Loss: 4.375138759613037 | BCE Loss: 1.0203951597213745\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 5.37571382522583 | KNN Loss: 4.348917484283447 | BCE Loss: 1.0267962217330933\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 5.369720458984375 | KNN Loss: 4.368724346160889 | BCE Loss: 1.0009958744049072\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 5.381543159484863 | KNN Loss: 4.357430458068848 | BCE Loss: 1.0241127014160156\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 5.3741254806518555 | KNN Loss: 4.366258144378662 | BCE Loss: 1.0078675746917725\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 5.385378360748291 | KNN Loss: 4.367142677307129 | BCE Loss: 1.018235683441162\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 5.365257263183594 | KNN Loss: 4.34724235534668 | BCE Loss: 1.018014907836914\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 5.400395393371582 | KNN Loss: 4.3558244705200195 | BCE Loss: 1.0445711612701416\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 5.416946887969971 | KNN Loss: 4.396740913391113 | BCE Loss: 1.0202059745788574\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 5.3890461921691895 | KNN Loss: 4.379372596740723 | BCE Loss: 1.0096737146377563\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 5.34222936630249 | KNN Loss: 4.3427534103393555 | BCE Loss: 0.9994759559631348\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 5.409348487854004 | KNN Loss: 4.354773998260498 | BCE Loss: 1.0545742511749268\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 5.411197185516357 | KNN Loss: 4.373446464538574 | BCE Loss: 1.0377506017684937\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 5.415280342102051 | KNN Loss: 4.409310817718506 | BCE Loss: 1.0059696435928345\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 5.400331497192383 | KNN Loss: 4.385015487670898 | BCE Loss: 1.0153160095214844\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 5.361470699310303 | KNN Loss: 4.361684799194336 | BCE Loss: 0.9997860193252563\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 5.436182975769043 | KNN Loss: 4.382812976837158 | BCE Loss: 1.0533701181411743\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 5.405654430389404 | KNN Loss: 4.344788074493408 | BCE Loss: 1.0608662366867065\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 5.41916561126709 | KNN Loss: 4.3695387840271 | BCE Loss: 1.0496268272399902\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 5.395699501037598 | KNN Loss: 4.388548374176025 | BCE Loss: 1.0071511268615723\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 5.407053470611572 | KNN Loss: 4.369380950927734 | BCE Loss: 1.037672519683838\n",
      "Epoch   357: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 5.383821487426758 | KNN Loss: 4.363218784332275 | BCE Loss: 1.0206025838851929\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 5.396416664123535 | KNN Loss: 4.369436740875244 | BCE Loss: 1.0269801616668701\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 5.403124809265137 | KNN Loss: 4.3706793785095215 | BCE Loss: 1.0324454307556152\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 5.393503189086914 | KNN Loss: 4.374451160430908 | BCE Loss: 1.0190517902374268\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 5.469100475311279 | KNN Loss: 4.423574924468994 | BCE Loss: 1.0455254316329956\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 5.392083168029785 | KNN Loss: 4.369199275970459 | BCE Loss: 1.022883653640747\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 5.3834123611450195 | KNN Loss: 4.371463298797607 | BCE Loss: 1.0119493007659912\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 5.430019378662109 | KNN Loss: 4.37868070602417 | BCE Loss: 1.0513389110565186\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 5.45440673828125 | KNN Loss: 4.415506839752197 | BCE Loss: 1.0388998985290527\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 5.382927894592285 | KNN Loss: 4.362884044647217 | BCE Loss: 1.0200436115264893\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 5.410726547241211 | KNN Loss: 4.365777015686035 | BCE Loss: 1.0449492931365967\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 5.401540756225586 | KNN Loss: 4.362175941467285 | BCE Loss: 1.0393648147583008\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 5.395140647888184 | KNN Loss: 4.37238073348999 | BCE Loss: 1.0227601528167725\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 5.406188488006592 | KNN Loss: 4.388019561767578 | BCE Loss: 1.0181689262390137\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 5.439211845397949 | KNN Loss: 4.398223876953125 | BCE Loss: 1.0409877300262451\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 5.428188800811768 | KNN Loss: 4.404572010040283 | BCE Loss: 1.0236166715621948\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 5.422743797302246 | KNN Loss: 4.3718390464782715 | BCE Loss: 1.0509049892425537\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 5.407483100891113 | KNN Loss: 4.39211893081665 | BCE Loss: 1.0153639316558838\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 5.449616432189941 | KNN Loss: 4.406528472900391 | BCE Loss: 1.0430879592895508\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 5.4357452392578125 | KNN Loss: 4.40195369720459 | BCE Loss: 1.0337915420532227\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 5.373856544494629 | KNN Loss: 4.361325263977051 | BCE Loss: 1.012531042098999\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 5.398180961608887 | KNN Loss: 4.356732368469238 | BCE Loss: 1.0414488315582275\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 5.365854263305664 | KNN Loss: 4.344225883483887 | BCE Loss: 1.0216282606124878\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 5.3690056800842285 | KNN Loss: 4.379015922546387 | BCE Loss: 0.9899897575378418\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 5.3383026123046875 | KNN Loss: 4.352278232574463 | BCE Loss: 0.9860244393348694\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 5.414109706878662 | KNN Loss: 4.3662824630737305 | BCE Loss: 1.0478273630142212\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 5.372108459472656 | KNN Loss: 4.359127998352051 | BCE Loss: 1.0129806995391846\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 5.377153396606445 | KNN Loss: 4.346797943115234 | BCE Loss: 1.0303552150726318\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 5.439168453216553 | KNN Loss: 4.386366844177246 | BCE Loss: 1.052801489830017\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 5.406650543212891 | KNN Loss: 4.366840839385986 | BCE Loss: 1.0398094654083252\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 5.395749568939209 | KNN Loss: 4.3694376945495605 | BCE Loss: 1.0263118743896484\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 5.3921918869018555 | KNN Loss: 4.35231351852417 | BCE Loss: 1.0398783683776855\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 5.352989673614502 | KNN Loss: 4.35087776184082 | BCE Loss: 1.0021120309829712\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 5.371034622192383 | KNN Loss: 4.341832160949707 | BCE Loss: 1.0292022228240967\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 5.4588623046875 | KNN Loss: 4.390866279602051 | BCE Loss: 1.0679960250854492\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 5.489666938781738 | KNN Loss: 4.435994625091553 | BCE Loss: 1.0536725521087646\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 5.420176029205322 | KNN Loss: 4.357358932495117 | BCE Loss: 1.0628169775009155\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 5.395969390869141 | KNN Loss: 4.368264675140381 | BCE Loss: 1.0277045965194702\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 5.422314167022705 | KNN Loss: 4.376157760620117 | BCE Loss: 1.0461565256118774\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 5.425698280334473 | KNN Loss: 4.373689651489258 | BCE Loss: 1.052008867263794\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 5.444826126098633 | KNN Loss: 4.403317928314209 | BCE Loss: 1.041508436203003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 5.407011985778809 | KNN Loss: 4.392210483551025 | BCE Loss: 1.0148017406463623\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 5.409515380859375 | KNN Loss: 4.377837657928467 | BCE Loss: 1.031677484512329\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 5.403809070587158 | KNN Loss: 4.39387845993042 | BCE Loss: 1.0099307298660278\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 5.474316596984863 | KNN Loss: 4.43327522277832 | BCE Loss: 1.0410414934158325\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 5.4332756996154785 | KNN Loss: 4.3868865966796875 | BCE Loss: 1.046389102935791\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 5.426518440246582 | KNN Loss: 4.395773410797119 | BCE Loss: 1.030745029449463\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 5.413646697998047 | KNN Loss: 4.368694305419922 | BCE Loss: 1.044952392578125\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 5.385837078094482 | KNN Loss: 4.37705135345459 | BCE Loss: 1.0087857246398926\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 5.442676544189453 | KNN Loss: 4.396982669830322 | BCE Loss: 1.0456936359405518\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 5.3887224197387695 | KNN Loss: 4.367983341217041 | BCE Loss: 1.0207390785217285\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 5.434528350830078 | KNN Loss: 4.401348114013672 | BCE Loss: 1.0331804752349854\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 5.415592193603516 | KNN Loss: 4.39424991607666 | BCE Loss: 1.021342158317566\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 5.4307708740234375 | KNN Loss: 4.373685836791992 | BCE Loss: 1.0570852756500244\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 5.394295692443848 | KNN Loss: 4.38938045501709 | BCE Loss: 1.004915475845337\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 5.413309097290039 | KNN Loss: 4.3559250831604 | BCE Loss: 1.0573842525482178\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 5.425121307373047 | KNN Loss: 4.400771141052246 | BCE Loss: 1.0243504047393799\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 5.3556647300720215 | KNN Loss: 4.353659152984619 | BCE Loss: 1.0020055770874023\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 5.41447639465332 | KNN Loss: 4.374964237213135 | BCE Loss: 1.0395119190216064\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 5.441155433654785 | KNN Loss: 4.39277982711792 | BCE Loss: 1.0483757257461548\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 5.4164628982543945 | KNN Loss: 4.371493339538574 | BCE Loss: 1.0449693202972412\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 5.439864158630371 | KNN Loss: 4.385493278503418 | BCE Loss: 1.0543711185455322\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 5.39992094039917 | KNN Loss: 4.397754669189453 | BCE Loss: 1.0021663904190063\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 5.424300193786621 | KNN Loss: 4.403075695037842 | BCE Loss: 1.0212243795394897\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 5.397321701049805 | KNN Loss: 4.35355281829834 | BCE Loss: 1.0437686443328857\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 5.408273696899414 | KNN Loss: 4.372822284698486 | BCE Loss: 1.0354514122009277\n",
      "Epoch   368: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 5.37720251083374 | KNN Loss: 4.364606857299805 | BCE Loss: 1.012595534324646\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 5.413443088531494 | KNN Loss: 4.388154983520508 | BCE Loss: 1.0252879858016968\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 5.369544982910156 | KNN Loss: 4.34446907043457 | BCE Loss: 1.025076150894165\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 5.4210028648376465 | KNN Loss: 4.370933532714844 | BCE Loss: 1.0500693321228027\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 5.376910209655762 | KNN Loss: 4.364207744598389 | BCE Loss: 1.012702226638794\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 5.409636497497559 | KNN Loss: 4.355869293212891 | BCE Loss: 1.0537669658660889\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 5.3968682289123535 | KNN Loss: 4.373249053955078 | BCE Loss: 1.023619294166565\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 5.400042533874512 | KNN Loss: 4.36616849899292 | BCE Loss: 1.0338737964630127\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 5.417621612548828 | KNN Loss: 4.368919372558594 | BCE Loss: 1.0487022399902344\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 5.391879081726074 | KNN Loss: 4.398543834686279 | BCE Loss: 0.9933353662490845\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 5.359185218811035 | KNN Loss: 4.33917760848999 | BCE Loss: 1.020007848739624\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 5.411236763000488 | KNN Loss: 4.3719048500061035 | BCE Loss: 1.0393319129943848\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 5.342922210693359 | KNN Loss: 4.339048385620117 | BCE Loss: 1.0038738250732422\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 5.444275856018066 | KNN Loss: 4.415703773498535 | BCE Loss: 1.0285720825195312\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 5.435859680175781 | KNN Loss: 4.3909478187561035 | BCE Loss: 1.0449116230010986\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 5.467660903930664 | KNN Loss: 4.419690132141113 | BCE Loss: 1.0479707717895508\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 5.40449333190918 | KNN Loss: 4.36522912979126 | BCE Loss: 1.0392640829086304\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 5.452549934387207 | KNN Loss: 4.41144323348999 | BCE Loss: 1.041106939315796\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 5.425339221954346 | KNN Loss: 4.387566566467285 | BCE Loss: 1.0377726554870605\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 5.386816024780273 | KNN Loss: 4.36376428604126 | BCE Loss: 1.0230518579483032\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 5.361626625061035 | KNN Loss: 4.352461338043213 | BCE Loss: 1.0091651678085327\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 5.4215850830078125 | KNN Loss: 4.365058422088623 | BCE Loss: 1.0565265417099\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 5.441959381103516 | KNN Loss: 4.407841682434082 | BCE Loss: 1.0341178178787231\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 5.417053699493408 | KNN Loss: 4.383167266845703 | BCE Loss: 1.0338865518569946\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 5.446328163146973 | KNN Loss: 4.414832592010498 | BCE Loss: 1.0314955711364746\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 5.396050930023193 | KNN Loss: 4.367593288421631 | BCE Loss: 1.0284576416015625\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 5.3953471183776855 | KNN Loss: 4.373277187347412 | BCE Loss: 1.0220698118209839\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 5.367539405822754 | KNN Loss: 4.362565517425537 | BCE Loss: 1.0049737691879272\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 5.357547760009766 | KNN Loss: 4.350292205810547 | BCE Loss: 1.0072553157806396\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 5.434108734130859 | KNN Loss: 4.3900346755981445 | BCE Loss: 1.0440738201141357\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 5.421435356140137 | KNN Loss: 4.402828693389893 | BCE Loss: 1.0186069011688232\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 5.394105911254883 | KNN Loss: 4.370179653167725 | BCE Loss: 1.0239264965057373\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 5.37997579574585 | KNN Loss: 4.35477876663208 | BCE Loss: 1.0251970291137695\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 5.359657287597656 | KNN Loss: 4.349260330200195 | BCE Loss: 1.01039719581604\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 5.385501861572266 | KNN Loss: 4.374026775360107 | BCE Loss: 1.0114750862121582\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 5.446011543273926 | KNN Loss: 4.406768798828125 | BCE Loss: 1.0392425060272217\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 5.437314987182617 | KNN Loss: 4.385895729064941 | BCE Loss: 1.0514193773269653\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 5.374267578125 | KNN Loss: 4.3488640785217285 | BCE Loss: 1.0254032611846924\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 5.454537868499756 | KNN Loss: 4.40134334564209 | BCE Loss: 1.0531946420669556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 5.411194801330566 | KNN Loss: 4.385404109954834 | BCE Loss: 1.0257906913757324\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 5.441565990447998 | KNN Loss: 4.388049602508545 | BCE Loss: 1.0535163879394531\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 5.418768882751465 | KNN Loss: 4.385362148284912 | BCE Loss: 1.0334067344665527\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 5.374481201171875 | KNN Loss: 4.347359657287598 | BCE Loss: 1.027121663093567\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 5.363791465759277 | KNN Loss: 4.356779098510742 | BCE Loss: 1.0070123672485352\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 5.434025287628174 | KNN Loss: 4.372797966003418 | BCE Loss: 1.0612273216247559\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 5.363168239593506 | KNN Loss: 4.351267337799072 | BCE Loss: 1.0119010210037231\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 5.3906354904174805 | KNN Loss: 4.375050067901611 | BCE Loss: 1.0155854225158691\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 5.423846244812012 | KNN Loss: 4.395659446716309 | BCE Loss: 1.0281867980957031\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 5.433295249938965 | KNN Loss: 4.359862804412842 | BCE Loss: 1.073432207107544\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 5.377832889556885 | KNN Loss: 4.343873500823975 | BCE Loss: 1.0339595079421997\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 5.371298789978027 | KNN Loss: 4.358445167541504 | BCE Loss: 1.0128536224365234\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 5.387365341186523 | KNN Loss: 4.3451008796691895 | BCE Loss: 1.0422643423080444\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 5.397686958312988 | KNN Loss: 4.384078025817871 | BCE Loss: 1.013608694076538\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 5.426885604858398 | KNN Loss: 4.398597717285156 | BCE Loss: 1.0282877683639526\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 5.387289047241211 | KNN Loss: 4.355870246887207 | BCE Loss: 1.031419038772583\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 5.412144660949707 | KNN Loss: 4.366591453552246 | BCE Loss: 1.04555344581604\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 5.4025115966796875 | KNN Loss: 4.390785217285156 | BCE Loss: 1.0117266178131104\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 5.416445255279541 | KNN Loss: 4.3710408210754395 | BCE Loss: 1.0454045534133911\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 5.378195762634277 | KNN Loss: 4.350146293640137 | BCE Loss: 1.0280497074127197\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 5.398591041564941 | KNN Loss: 4.392179012298584 | BCE Loss: 1.0064120292663574\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 5.40852689743042 | KNN Loss: 4.37221622467041 | BCE Loss: 1.0363106727600098\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 5.3929009437561035 | KNN Loss: 4.39501428604126 | BCE Loss: 0.9978867769241333\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 5.395209312438965 | KNN Loss: 4.379374980926514 | BCE Loss: 1.0158342123031616\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 5.3644490242004395 | KNN Loss: 4.336135387420654 | BCE Loss: 1.0283135175704956\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 5.422140598297119 | KNN Loss: 4.391758441925049 | BCE Loss: 1.0303820371627808\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 5.408479690551758 | KNN Loss: 4.399264812469482 | BCE Loss: 1.0092146396636963\n",
      "Epoch   379: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 5.384099960327148 | KNN Loss: 4.347594738006592 | BCE Loss: 1.036505103111267\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 5.3609619140625 | KNN Loss: 4.337935447692871 | BCE Loss: 1.0230262279510498\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 5.371788501739502 | KNN Loss: 4.342541217803955 | BCE Loss: 1.0292474031448364\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 5.426085948944092 | KNN Loss: 4.397124290466309 | BCE Loss: 1.0289615392684937\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 5.408941268920898 | KNN Loss: 4.386665344238281 | BCE Loss: 1.0222760438919067\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 5.412833213806152 | KNN Loss: 4.3553972244262695 | BCE Loss: 1.057436227798462\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 5.4000396728515625 | KNN Loss: 4.373843193054199 | BCE Loss: 1.0261965990066528\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 5.337347030639648 | KNN Loss: 4.354682922363281 | BCE Loss: 0.9826643466949463\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 5.449801445007324 | KNN Loss: 4.384054183959961 | BCE Loss: 1.0657473802566528\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 5.423709869384766 | KNN Loss: 4.4063720703125 | BCE Loss: 1.017337679862976\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 5.386280536651611 | KNN Loss: 4.37357234954834 | BCE Loss: 1.012708306312561\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 5.427717685699463 | KNN Loss: 4.378767967224121 | BCE Loss: 1.0489498376846313\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 5.406152725219727 | KNN Loss: 4.363488674163818 | BCE Loss: 1.0426641702651978\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 5.420216083526611 | KNN Loss: 4.384621620178223 | BCE Loss: 1.0355944633483887\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 5.429487705230713 | KNN Loss: 4.373386859893799 | BCE Loss: 1.0561007261276245\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 5.408343315124512 | KNN Loss: 4.374014854431152 | BCE Loss: 1.0343282222747803\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 5.359401702880859 | KNN Loss: 4.365137100219727 | BCE Loss: 0.9942644834518433\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 5.397342681884766 | KNN Loss: 4.363870143890381 | BCE Loss: 1.0334725379943848\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 5.363124370574951 | KNN Loss: 4.346704006195068 | BCE Loss: 1.0164204835891724\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 5.404901504516602 | KNN Loss: 4.3765549659729 | BCE Loss: 1.028346300125122\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 5.432616710662842 | KNN Loss: 4.393007755279541 | BCE Loss: 1.0396089553833008\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 5.467951774597168 | KNN Loss: 4.386448860168457 | BCE Loss: 1.0815030336380005\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 5.3748273849487305 | KNN Loss: 4.360973358154297 | BCE Loss: 1.0138542652130127\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 5.382513999938965 | KNN Loss: 4.373674392700195 | BCE Loss: 1.0088396072387695\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 5.393551826477051 | KNN Loss: 4.367407321929932 | BCE Loss: 1.0261446237564087\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 5.399055480957031 | KNN Loss: 4.354769229888916 | BCE Loss: 1.0442860126495361\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 5.415297985076904 | KNN Loss: 4.358928680419922 | BCE Loss: 1.0563693046569824\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 5.412827491760254 | KNN Loss: 4.375826358795166 | BCE Loss: 1.037001132965088\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 5.4564738273620605 | KNN Loss: 4.397740840911865 | BCE Loss: 1.0587331056594849\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 5.374695301055908 | KNN Loss: 4.356691837310791 | BCE Loss: 1.0180034637451172\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 5.361246109008789 | KNN Loss: 4.355666637420654 | BCE Loss: 1.0055793523788452\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 5.399249076843262 | KNN Loss: 4.394928932189941 | BCE Loss: 1.0043200254440308\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 5.404651641845703 | KNN Loss: 4.396475791931152 | BCE Loss: 1.0081757307052612\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 5.4034013748168945 | KNN Loss: 4.3559722900390625 | BCE Loss: 1.047429084777832\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 5.39398193359375 | KNN Loss: 4.378275394439697 | BCE Loss: 1.0157063007354736\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 5.402529716491699 | KNN Loss: 4.378133773803711 | BCE Loss: 1.0243960618972778\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 5.385770320892334 | KNN Loss: 4.351229190826416 | BCE Loss: 1.034541130065918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 5.421492099761963 | KNN Loss: 4.417176723480225 | BCE Loss: 1.0043152570724487\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 5.388345718383789 | KNN Loss: 4.371342182159424 | BCE Loss: 1.0170034170150757\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 5.433860778808594 | KNN Loss: 4.3962507247924805 | BCE Loss: 1.0376101732254028\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 5.438559055328369 | KNN Loss: 4.406874179840088 | BCE Loss: 1.0316849946975708\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 5.422153472900391 | KNN Loss: 4.391071319580078 | BCE Loss: 1.0310821533203125\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 5.433164596557617 | KNN Loss: 4.383158206939697 | BCE Loss: 1.05000638961792\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 5.4013752937316895 | KNN Loss: 4.374415397644043 | BCE Loss: 1.0269598960876465\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 5.420682907104492 | KNN Loss: 4.400352954864502 | BCE Loss: 1.0203298330307007\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 5.3665008544921875 | KNN Loss: 4.347463607788086 | BCE Loss: 1.0190370082855225\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 5.386277198791504 | KNN Loss: 4.360140323638916 | BCE Loss: 1.026136875152588\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 5.459292411804199 | KNN Loss: 4.390259742736816 | BCE Loss: 1.0690324306488037\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 5.407701015472412 | KNN Loss: 4.394437789916992 | BCE Loss: 1.01326322555542\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 5.436930179595947 | KNN Loss: 4.385243892669678 | BCE Loss: 1.0516862869262695\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 5.463254928588867 | KNN Loss: 4.403153896331787 | BCE Loss: 1.060100793838501\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 5.395310878753662 | KNN Loss: 4.352004528045654 | BCE Loss: 1.0433063507080078\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 5.408978462219238 | KNN Loss: 4.356919288635254 | BCE Loss: 1.0520594120025635\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 5.372785568237305 | KNN Loss: 4.336678504943848 | BCE Loss: 1.0361071825027466\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 5.3858842849731445 | KNN Loss: 4.359773635864258 | BCE Loss: 1.0261105298995972\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 5.407121181488037 | KNN Loss: 4.366119861602783 | BCE Loss: 1.041001319885254\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 5.391390800476074 | KNN Loss: 4.363585948944092 | BCE Loss: 1.0278046131134033\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 5.425098419189453 | KNN Loss: 4.3842082023620605 | BCE Loss: 1.0408899784088135\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 5.40082311630249 | KNN Loss: 4.365847110748291 | BCE Loss: 1.0349760055541992\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 5.4153971672058105 | KNN Loss: 4.384990692138672 | BCE Loss: 1.0304064750671387\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 5.3566412925720215 | KNN Loss: 4.344955921173096 | BCE Loss: 1.0116854906082153\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 5.462967872619629 | KNN Loss: 4.44464635848999 | BCE Loss: 1.0183215141296387\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 5.396129131317139 | KNN Loss: 4.356917858123779 | BCE Loss: 1.039211392402649\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 5.365535259246826 | KNN Loss: 4.334589004516602 | BCE Loss: 1.0309462547302246\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 5.393615245819092 | KNN Loss: 4.385575294494629 | BCE Loss: 1.008039951324463\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 5.414403915405273 | KNN Loss: 4.393650054931641 | BCE Loss: 1.020754098892212\n",
      "Epoch   390: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 5.381805419921875 | KNN Loss: 4.361905574798584 | BCE Loss: 1.019899845123291\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 5.40316104888916 | KNN Loss: 4.369384288787842 | BCE Loss: 1.0337767601013184\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 5.422298431396484 | KNN Loss: 4.37955379486084 | BCE Loss: 1.0427446365356445\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 5.389819145202637 | KNN Loss: 4.362837791442871 | BCE Loss: 1.0269815921783447\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 5.424149513244629 | KNN Loss: 4.374783515930176 | BCE Loss: 1.0493662357330322\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 5.362680435180664 | KNN Loss: 4.351543426513672 | BCE Loss: 1.0111368894577026\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 5.391263961791992 | KNN Loss: 4.383784770965576 | BCE Loss: 1.007478952407837\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 5.415679931640625 | KNN Loss: 4.36899471282959 | BCE Loss: 1.046684980392456\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 5.429734706878662 | KNN Loss: 4.39133358001709 | BCE Loss: 1.0384010076522827\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 5.363015651702881 | KNN Loss: 4.358198642730713 | BCE Loss: 1.004817008972168\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 5.403164386749268 | KNN Loss: 4.38690710067749 | BCE Loss: 1.016257405281067\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 5.3921709060668945 | KNN Loss: 4.354511260986328 | BCE Loss: 1.0376594066619873\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 5.39370059967041 | KNN Loss: 4.351997375488281 | BCE Loss: 1.041703462600708\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 5.369956970214844 | KNN Loss: 4.352363586425781 | BCE Loss: 1.0175933837890625\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 5.385226249694824 | KNN Loss: 4.385680675506592 | BCE Loss: 0.9995455741882324\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 5.382396697998047 | KNN Loss: 4.369481086730957 | BCE Loss: 1.012915849685669\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 5.3905534744262695 | KNN Loss: 4.384804725646973 | BCE Loss: 1.005748987197876\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 5.41696834564209 | KNN Loss: 4.397832870483398 | BCE Loss: 1.0191352367401123\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 5.3797478675842285 | KNN Loss: 4.3368611335754395 | BCE Loss: 1.042886734008789\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 5.36904239654541 | KNN Loss: 4.351944923400879 | BCE Loss: 1.0170974731445312\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 5.362910270690918 | KNN Loss: 4.345776557922363 | BCE Loss: 1.0171334743499756\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 5.4224700927734375 | KNN Loss: 4.382045269012451 | BCE Loss: 1.0404245853424072\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 5.380171298980713 | KNN Loss: 4.342429161071777 | BCE Loss: 1.0377421379089355\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 5.405381202697754 | KNN Loss: 4.371373176574707 | BCE Loss: 1.034008264541626\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 5.362826347351074 | KNN Loss: 4.349729061126709 | BCE Loss: 1.0130975246429443\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 5.434070110321045 | KNN Loss: 4.412810802459717 | BCE Loss: 1.0212591886520386\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 5.362318992614746 | KNN Loss: 4.345465183258057 | BCE Loss: 1.0168538093566895\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 5.385904312133789 | KNN Loss: 4.355353832244873 | BCE Loss: 1.030550479888916\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 5.353740692138672 | KNN Loss: 4.3438310623168945 | BCE Loss: 1.009909749031067\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 5.388935565948486 | KNN Loss: 4.363976001739502 | BCE Loss: 1.0249595642089844\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 5.460452556610107 | KNN Loss: 4.426206111907959 | BCE Loss: 1.0342464447021484\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 5.385650634765625 | KNN Loss: 4.36021089553833 | BCE Loss: 1.0254396200180054\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 5.347637176513672 | KNN Loss: 4.337042808532715 | BCE Loss: 1.0105946063995361\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 5.412450313568115 | KNN Loss: 4.385797500610352 | BCE Loss: 1.0266528129577637\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 5.392144203186035 | KNN Loss: 4.360931873321533 | BCE Loss: 1.0312122106552124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 5.360018253326416 | KNN Loss: 4.376079559326172 | BCE Loss: 0.9839386343955994\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 5.405110836029053 | KNN Loss: 4.389258861541748 | BCE Loss: 1.0158518552780151\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 5.391532897949219 | KNN Loss: 4.373805522918701 | BCE Loss: 1.0177271366119385\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 5.367646217346191 | KNN Loss: 4.3445892333984375 | BCE Loss: 1.0230567455291748\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 5.360358238220215 | KNN Loss: 4.332310199737549 | BCE Loss: 1.028047800064087\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 5.413618087768555 | KNN Loss: 4.380745887756348 | BCE Loss: 1.0328723192214966\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 5.445387840270996 | KNN Loss: 4.381087779998779 | BCE Loss: 1.0643000602722168\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 5.389957904815674 | KNN Loss: 4.355424404144287 | BCE Loss: 1.0345335006713867\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 5.433375358581543 | KNN Loss: 4.394677639007568 | BCE Loss: 1.0386974811553955\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 5.483398914337158 | KNN Loss: 4.433864116668701 | BCE Loss: 1.0495349168777466\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 5.371621608734131 | KNN Loss: 4.362988471984863 | BCE Loss: 1.008633017539978\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 5.437190055847168 | KNN Loss: 4.386448383331299 | BCE Loss: 1.0507416725158691\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 5.374540328979492 | KNN Loss: 4.336103439331055 | BCE Loss: 1.0384366512298584\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 5.374150276184082 | KNN Loss: 4.367586135864258 | BCE Loss: 1.0065642595291138\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 5.407625675201416 | KNN Loss: 4.356415271759033 | BCE Loss: 1.0512105226516724\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 5.427994728088379 | KNN Loss: 4.355152130126953 | BCE Loss: 1.0728423595428467\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 5.427203178405762 | KNN Loss: 4.408156871795654 | BCE Loss: 1.0190465450286865\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 5.4049906730651855 | KNN Loss: 4.3806586265563965 | BCE Loss: 1.0243321657180786\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 5.380191802978516 | KNN Loss: 4.366913318634033 | BCE Loss: 1.0132784843444824\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 5.465136528015137 | KNN Loss: 4.407005786895752 | BCE Loss: 1.0581309795379639\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 5.390265941619873 | KNN Loss: 4.373531818389893 | BCE Loss: 1.0167341232299805\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 5.450206756591797 | KNN Loss: 4.381960391998291 | BCE Loss: 1.0682462453842163\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 5.405447483062744 | KNN Loss: 4.389618873596191 | BCE Loss: 1.0158286094665527\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 5.369504928588867 | KNN Loss: 4.359076499938965 | BCE Loss: 1.0104281902313232\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 5.376273155212402 | KNN Loss: 4.342808723449707 | BCE Loss: 1.0334646701812744\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 5.386050224304199 | KNN Loss: 4.365144729614258 | BCE Loss: 1.0209054946899414\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 5.4646830558776855 | KNN Loss: 4.386041164398193 | BCE Loss: 1.0786418914794922\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 5.397662162780762 | KNN Loss: 4.356535911560059 | BCE Loss: 1.041126012802124\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 5.420733451843262 | KNN Loss: 4.397547245025635 | BCE Loss: 1.0231860876083374\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 5.378586769104004 | KNN Loss: 4.362166404724121 | BCE Loss: 1.016420602798462\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 5.405480861663818 | KNN Loss: 4.361494541168213 | BCE Loss: 1.0439863204956055\n",
      "Epoch   401: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 5.399523735046387 | KNN Loss: 4.373476505279541 | BCE Loss: 1.0260472297668457\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 5.365437030792236 | KNN Loss: 4.335471153259277 | BCE Loss: 1.029965877532959\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 5.370622158050537 | KNN Loss: 4.355294704437256 | BCE Loss: 1.0153275728225708\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 5.416807651519775 | KNN Loss: 4.382885932922363 | BCE Loss: 1.033921718597412\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 5.430962562561035 | KNN Loss: 4.3953328132629395 | BCE Loss: 1.0356295108795166\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 5.405086994171143 | KNN Loss: 4.389080047607422 | BCE Loss: 1.0160070657730103\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 5.388030052185059 | KNN Loss: 4.347000598907471 | BCE Loss: 1.041029691696167\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 5.388031959533691 | KNN Loss: 4.374492168426514 | BCE Loss: 1.0135397911071777\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 5.342159748077393 | KNN Loss: 4.341721534729004 | BCE Loss: 1.0004383325576782\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 5.408560752868652 | KNN Loss: 4.389787197113037 | BCE Loss: 1.0187737941741943\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 5.432432174682617 | KNN Loss: 4.380363464355469 | BCE Loss: 1.0520689487457275\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 5.461580753326416 | KNN Loss: 4.442117691040039 | BCE Loss: 1.019463062286377\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 5.392419815063477 | KNN Loss: 4.367560863494873 | BCE Loss: 1.0248591899871826\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 5.393361568450928 | KNN Loss: 4.39057731628418 | BCE Loss: 1.0027841329574585\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 5.435159683227539 | KNN Loss: 4.379895210266113 | BCE Loss: 1.0552642345428467\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 5.432546615600586 | KNN Loss: 4.406378746032715 | BCE Loss: 1.026167869567871\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 5.339070796966553 | KNN Loss: 4.337096214294434 | BCE Loss: 1.0019745826721191\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 5.441808700561523 | KNN Loss: 4.402275562286377 | BCE Loss: 1.0395331382751465\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 5.439137935638428 | KNN Loss: 4.408010959625244 | BCE Loss: 1.031126856803894\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 5.393197059631348 | KNN Loss: 4.35054349899292 | BCE Loss: 1.0426536798477173\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 5.36492919921875 | KNN Loss: 4.337120532989502 | BCE Loss: 1.027808666229248\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 5.4237565994262695 | KNN Loss: 4.371816635131836 | BCE Loss: 1.0519402027130127\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 5.422839164733887 | KNN Loss: 4.405326843261719 | BCE Loss: 1.0175120830535889\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 5.401942253112793 | KNN Loss: 4.3713788986206055 | BCE Loss: 1.0305633544921875\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 5.394007205963135 | KNN Loss: 4.362488269805908 | BCE Loss: 1.0315190553665161\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 5.393374443054199 | KNN Loss: 4.352458953857422 | BCE Loss: 1.0409152507781982\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 5.380549430847168 | KNN Loss: 4.342793941497803 | BCE Loss: 1.0377556085586548\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 5.440594673156738 | KNN Loss: 4.429117202758789 | BCE Loss: 1.0114774703979492\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 5.360747814178467 | KNN Loss: 4.340157508850098 | BCE Loss: 1.0205904245376587\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 5.378016471862793 | KNN Loss: 4.383544445037842 | BCE Loss: 0.9944719076156616\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 5.394044399261475 | KNN Loss: 4.355981349945068 | BCE Loss: 1.0380630493164062\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 5.423709392547607 | KNN Loss: 4.392452716827393 | BCE Loss: 1.0312566757202148\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 5.372973442077637 | KNN Loss: 4.3382887840271 | BCE Loss: 1.0346847772598267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 5.4188551902771 | KNN Loss: 4.404438495635986 | BCE Loss: 1.0144168138504028\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 5.377321243286133 | KNN Loss: 4.359106540679932 | BCE Loss: 1.018214464187622\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 5.39042854309082 | KNN Loss: 4.364990234375 | BCE Loss: 1.0254380702972412\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 5.430169105529785 | KNN Loss: 4.408502578735352 | BCE Loss: 1.0216665267944336\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 5.390746116638184 | KNN Loss: 4.359445571899414 | BCE Loss: 1.0313005447387695\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 5.367217063903809 | KNN Loss: 4.368931770324707 | BCE Loss: 0.9982852339744568\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 5.391040325164795 | KNN Loss: 4.3637375831604 | BCE Loss: 1.0273027420043945\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 5.412813186645508 | KNN Loss: 4.372248649597168 | BCE Loss: 1.0405646562576294\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 5.4169816970825195 | KNN Loss: 4.374812602996826 | BCE Loss: 1.0421690940856934\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 5.435230255126953 | KNN Loss: 4.385735034942627 | BCE Loss: 1.0494954586029053\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 5.440023422241211 | KNN Loss: 4.3706512451171875 | BCE Loss: 1.0693719387054443\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 5.429355621337891 | KNN Loss: 4.393756866455078 | BCE Loss: 1.035598635673523\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 5.365096092224121 | KNN Loss: 4.329532623291016 | BCE Loss: 1.035563588142395\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 5.407760143280029 | KNN Loss: 4.377479553222656 | BCE Loss: 1.0302804708480835\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 5.408435344696045 | KNN Loss: 4.394731044769287 | BCE Loss: 1.0137042999267578\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 5.41132926940918 | KNN Loss: 4.391639232635498 | BCE Loss: 1.0196897983551025\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 5.4343695640563965 | KNN Loss: 4.409948348999023 | BCE Loss: 1.024421215057373\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 5.403985023498535 | KNN Loss: 4.370331287384033 | BCE Loss: 1.033653974533081\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 5.39833402633667 | KNN Loss: 4.377890586853027 | BCE Loss: 1.0204435586929321\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 5.3476057052612305 | KNN Loss: 4.347909450531006 | BCE Loss: 0.9996963739395142\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 5.3839111328125 | KNN Loss: 4.363943099975586 | BCE Loss: 1.0199682712554932\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 5.4442291259765625 | KNN Loss: 4.3743085861206055 | BCE Loss: 1.0699206590652466\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 5.443925857543945 | KNN Loss: 4.420833587646484 | BCE Loss: 1.023092269897461\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 5.429128646850586 | KNN Loss: 4.372422695159912 | BCE Loss: 1.0567057132720947\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 5.432185173034668 | KNN Loss: 4.385198593139648 | BCE Loss: 1.0469863414764404\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 5.454955101013184 | KNN Loss: 4.416987895965576 | BCE Loss: 1.037967324256897\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 5.398972511291504 | KNN Loss: 4.36181640625 | BCE Loss: 1.037156105041504\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 5.481566429138184 | KNN Loss: 4.425358295440674 | BCE Loss: 1.0562081336975098\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 5.4163408279418945 | KNN Loss: 4.3681793212890625 | BCE Loss: 1.048161506652832\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 5.436819076538086 | KNN Loss: 4.385905742645264 | BCE Loss: 1.0509135723114014\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 5.429339408874512 | KNN Loss: 4.3762311935424805 | BCE Loss: 1.0531079769134521\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 5.41640043258667 | KNN Loss: 4.3622846603393555 | BCE Loss: 1.0541157722473145\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 5.449258327484131 | KNN Loss: 4.403140068054199 | BCE Loss: 1.0461183786392212\n",
      "Epoch   412: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 5.396986961364746 | KNN Loss: 4.365686893463135 | BCE Loss: 1.0312998294830322\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 5.388786315917969 | KNN Loss: 4.3461198806762695 | BCE Loss: 1.0426666736602783\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 5.438350677490234 | KNN Loss: 4.409126281738281 | BCE Loss: 1.029224157333374\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 5.3976898193359375 | KNN Loss: 4.3739118576049805 | BCE Loss: 1.023777723312378\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 5.458769798278809 | KNN Loss: 4.423330307006836 | BCE Loss: 1.0354394912719727\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 5.402797698974609 | KNN Loss: 4.392148017883301 | BCE Loss: 1.0106498003005981\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 5.3872222900390625 | KNN Loss: 4.375409126281738 | BCE Loss: 1.0118129253387451\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 5.35803747177124 | KNN Loss: 4.3791117668151855 | BCE Loss: 0.9789255857467651\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 5.470003128051758 | KNN Loss: 4.411910533905029 | BCE Loss: 1.0580923557281494\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 5.46577262878418 | KNN Loss: 4.428072452545166 | BCE Loss: 1.0377001762390137\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 5.370744705200195 | KNN Loss: 4.349201679229736 | BCE Loss: 1.0215427875518799\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 5.412173271179199 | KNN Loss: 4.371496677398682 | BCE Loss: 1.0406765937805176\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 5.378368377685547 | KNN Loss: 4.363440990447998 | BCE Loss: 1.014927625656128\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 5.370786190032959 | KNN Loss: 4.35537576675415 | BCE Loss: 1.0154104232788086\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 5.42429256439209 | KNN Loss: 4.390627861022949 | BCE Loss: 1.0336649417877197\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 5.391427516937256 | KNN Loss: 4.374268054962158 | BCE Loss: 1.0171594619750977\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 5.414008140563965 | KNN Loss: 4.365428924560547 | BCE Loss: 1.048579454421997\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 5.373761177062988 | KNN Loss: 4.343258857727051 | BCE Loss: 1.0305023193359375\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 5.3842010498046875 | KNN Loss: 4.351505756378174 | BCE Loss: 1.0326952934265137\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 5.378711700439453 | KNN Loss: 4.373164176940918 | BCE Loss: 1.0055477619171143\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 5.395100116729736 | KNN Loss: 4.363320350646973 | BCE Loss: 1.0317797660827637\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 5.371222972869873 | KNN Loss: 4.371423721313477 | BCE Loss: 0.9997994303703308\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 5.403415203094482 | KNN Loss: 4.369731903076172 | BCE Loss: 1.0336833000183105\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 5.3672966957092285 | KNN Loss: 4.339554309844971 | BCE Loss: 1.0277422666549683\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 5.3905181884765625 | KNN Loss: 4.364907264709473 | BCE Loss: 1.0256109237670898\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 5.371707439422607 | KNN Loss: 4.354851722717285 | BCE Loss: 1.0168558359146118\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 5.422493934631348 | KNN Loss: 4.404329776763916 | BCE Loss: 1.0181641578674316\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 5.381427764892578 | KNN Loss: 4.353762626647949 | BCE Loss: 1.027665138244629\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 5.376869201660156 | KNN Loss: 4.344468593597412 | BCE Loss: 1.0324008464813232\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 5.385281562805176 | KNN Loss: 4.370873928070068 | BCE Loss: 1.014407753944397\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 5.391745567321777 | KNN Loss: 4.3531060218811035 | BCE Loss: 1.0386396646499634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 5.375885009765625 | KNN Loss: 4.356439113616943 | BCE Loss: 1.0194460153579712\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 5.4187331199646 | KNN Loss: 4.389726638793945 | BCE Loss: 1.0290064811706543\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 5.416496276855469 | KNN Loss: 4.386774063110352 | BCE Loss: 1.0297220945358276\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 5.368431091308594 | KNN Loss: 4.361526012420654 | BCE Loss: 1.0069053173065186\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 5.48036003112793 | KNN Loss: 4.4274702072143555 | BCE Loss: 1.0528895854949951\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 5.371793270111084 | KNN Loss: 4.346191883087158 | BCE Loss: 1.0256013870239258\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 5.397812843322754 | KNN Loss: 4.396267890930176 | BCE Loss: 1.001544713973999\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 5.39389181137085 | KNN Loss: 4.367108345031738 | BCE Loss: 1.0267834663391113\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 5.382403373718262 | KNN Loss: 4.362349033355713 | BCE Loss: 1.0200541019439697\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 5.377180099487305 | KNN Loss: 4.350677013397217 | BCE Loss: 1.026503086090088\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 5.35818338394165 | KNN Loss: 4.3573503494262695 | BCE Loss: 1.0008331537246704\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 5.406083583831787 | KNN Loss: 4.378499984741211 | BCE Loss: 1.0275834798812866\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 5.436724662780762 | KNN Loss: 4.398246765136719 | BCE Loss: 1.038478136062622\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 5.409358978271484 | KNN Loss: 4.380772590637207 | BCE Loss: 1.0285861492156982\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 5.34782600402832 | KNN Loss: 4.352956295013428 | BCE Loss: 0.994869589805603\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 5.351243019104004 | KNN Loss: 4.348145961761475 | BCE Loss: 1.0030971765518188\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 5.381476402282715 | KNN Loss: 4.34891414642334 | BCE Loss: 1.0325623750686646\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 5.404422283172607 | KNN Loss: 4.360595703125 | BCE Loss: 1.0438265800476074\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 5.456984519958496 | KNN Loss: 4.424368858337402 | BCE Loss: 1.0326156616210938\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 5.376834869384766 | KNN Loss: 4.38217306137085 | BCE Loss: 0.9946615695953369\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 5.372470855712891 | KNN Loss: 4.355061054229736 | BCE Loss: 1.0174098014831543\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 5.470511436462402 | KNN Loss: 4.427455902099609 | BCE Loss: 1.0430552959442139\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 5.414299488067627 | KNN Loss: 4.381555080413818 | BCE Loss: 1.032744288444519\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 5.39933967590332 | KNN Loss: 4.365799903869629 | BCE Loss: 1.0335396528244019\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 5.41379976272583 | KNN Loss: 4.392124652862549 | BCE Loss: 1.0216751098632812\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 5.393895149230957 | KNN Loss: 4.361418724060059 | BCE Loss: 1.0324766635894775\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 5.424758434295654 | KNN Loss: 4.38007116317749 | BCE Loss: 1.044687271118164\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 5.428631782531738 | KNN Loss: 4.384768009185791 | BCE Loss: 1.0438635349273682\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 5.390623569488525 | KNN Loss: 4.360412120819092 | BCE Loss: 1.0302114486694336\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 5.445898056030273 | KNN Loss: 4.379344940185547 | BCE Loss: 1.0665533542633057\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 5.421660900115967 | KNN Loss: 4.374516487121582 | BCE Loss: 1.0471444129943848\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 5.443103313446045 | KNN Loss: 4.38906192779541 | BCE Loss: 1.0540415048599243\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 5.41259765625 | KNN Loss: 4.378078937530518 | BCE Loss: 1.0345184803009033\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 5.374739646911621 | KNN Loss: 4.353532314300537 | BCE Loss: 1.0212070941925049\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 5.389589309692383 | KNN Loss: 4.370315074920654 | BCE Loss: 1.019274353981018\n",
      "Epoch   423: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 5.40958309173584 | KNN Loss: 4.381953239440918 | BCE Loss: 1.0276297330856323\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 5.439699649810791 | KNN Loss: 4.383784294128418 | BCE Loss: 1.055915355682373\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 5.424073219299316 | KNN Loss: 4.38261079788208 | BCE Loss: 1.0414623022079468\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 5.384654998779297 | KNN Loss: 4.345162868499756 | BCE Loss: 1.0394923686981201\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 5.377746105194092 | KNN Loss: 4.341969013214111 | BCE Loss: 1.0357770919799805\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 5.422500133514404 | KNN Loss: 4.384587287902832 | BCE Loss: 1.0379127264022827\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 5.354210376739502 | KNN Loss: 4.345765590667725 | BCE Loss: 1.008444905281067\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 5.465322494506836 | KNN Loss: 4.409387588500977 | BCE Loss: 1.0559351444244385\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 5.410214424133301 | KNN Loss: 4.379124164581299 | BCE Loss: 1.0310900211334229\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 5.4170026779174805 | KNN Loss: 4.3785295486450195 | BCE Loss: 1.0384732484817505\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 5.405553817749023 | KNN Loss: 4.364137172698975 | BCE Loss: 1.0414164066314697\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 5.407184600830078 | KNN Loss: 4.366732597351074 | BCE Loss: 1.0404517650604248\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 5.4399518966674805 | KNN Loss: 4.366970539093018 | BCE Loss: 1.0729811191558838\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 5.409937858581543 | KNN Loss: 4.361041069030762 | BCE Loss: 1.0488967895507812\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 5.466723442077637 | KNN Loss: 4.4541215896606445 | BCE Loss: 1.012601613998413\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 5.394051551818848 | KNN Loss: 4.377470016479492 | BCE Loss: 1.016581654548645\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 5.430112838745117 | KNN Loss: 4.389003753662109 | BCE Loss: 1.0411090850830078\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 5.498729705810547 | KNN Loss: 4.496023654937744 | BCE Loss: 1.0027060508728027\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 5.428771018981934 | KNN Loss: 4.390426158905029 | BCE Loss: 1.0383450984954834\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 5.393862247467041 | KNN Loss: 4.364238262176514 | BCE Loss: 1.0296239852905273\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 5.407227516174316 | KNN Loss: 4.3686394691467285 | BCE Loss: 1.0385881662368774\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 5.361148834228516 | KNN Loss: 4.340686321258545 | BCE Loss: 1.0204622745513916\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 5.400951862335205 | KNN Loss: 4.384365558624268 | BCE Loss: 1.016586422920227\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 5.368158340454102 | KNN Loss: 4.3694353103637695 | BCE Loss: 0.9987232685089111\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 5.457736492156982 | KNN Loss: 4.4180192947387695 | BCE Loss: 1.0397170782089233\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 5.37714147567749 | KNN Loss: 4.371044635772705 | BCE Loss: 1.0060968399047852\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 5.377612113952637 | KNN Loss: 4.356616497039795 | BCE Loss: 1.0209957361221313\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 5.4074249267578125 | KNN Loss: 4.369617462158203 | BCE Loss: 1.0378074645996094\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 5.387853622436523 | KNN Loss: 4.371887683868408 | BCE Loss: 1.0159657001495361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 5.397862434387207 | KNN Loss: 4.37176513671875 | BCE Loss: 1.026097059249878\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 5.4377665519714355 | KNN Loss: 4.401102542877197 | BCE Loss: 1.0366640090942383\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 5.402417182922363 | KNN Loss: 4.383434772491455 | BCE Loss: 1.0189826488494873\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 5.340904235839844 | KNN Loss: 4.329569339752197 | BCE Loss: 1.011334776878357\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 5.431185245513916 | KNN Loss: 4.398580074310303 | BCE Loss: 1.0326051712036133\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 5.5016703605651855 | KNN Loss: 4.452282905578613 | BCE Loss: 1.0493875741958618\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 5.378060340881348 | KNN Loss: 4.3455400466918945 | BCE Loss: 1.0325204133987427\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 5.3853440284729 | KNN Loss: 4.387049674987793 | BCE Loss: 0.9982942342758179\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 5.386885166168213 | KNN Loss: 4.364817142486572 | BCE Loss: 1.0220680236816406\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 5.392033576965332 | KNN Loss: 4.355468273162842 | BCE Loss: 1.0365651845932007\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 5.386017799377441 | KNN Loss: 4.362998962402344 | BCE Loss: 1.023018717765808\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 5.37375545501709 | KNN Loss: 4.360457420349121 | BCE Loss: 1.0132977962493896\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 5.4233503341674805 | KNN Loss: 4.39729642868042 | BCE Loss: 1.0260539054870605\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 5.434317588806152 | KNN Loss: 4.407556533813477 | BCE Loss: 1.0267610549926758\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 5.340991973876953 | KNN Loss: 4.34342622756958 | BCE Loss: 0.9975656270980835\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 5.406200408935547 | KNN Loss: 4.40895414352417 | BCE Loss: 0.9972463846206665\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 5.414217948913574 | KNN Loss: 4.394389629364014 | BCE Loss: 1.0198280811309814\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 5.380313396453857 | KNN Loss: 4.357054710388184 | BCE Loss: 1.0232586860656738\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 5.3644514083862305 | KNN Loss: 4.340645790100098 | BCE Loss: 1.0238054990768433\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 5.373421669006348 | KNN Loss: 4.352712154388428 | BCE Loss: 1.02070951461792\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 5.388828754425049 | KNN Loss: 4.351999282836914 | BCE Loss: 1.0368294715881348\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 5.383600234985352 | KNN Loss: 4.356534004211426 | BCE Loss: 1.0270661115646362\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 5.463202476501465 | KNN Loss: 4.4237799644470215 | BCE Loss: 1.039422631263733\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 5.424916744232178 | KNN Loss: 4.411890506744385 | BCE Loss: 1.0130261182785034\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 5.405646324157715 | KNN Loss: 4.372990131378174 | BCE Loss: 1.0326564311981201\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 5.387275218963623 | KNN Loss: 4.368644714355469 | BCE Loss: 1.0186305046081543\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 5.394995212554932 | KNN Loss: 4.357186317443848 | BCE Loss: 1.0378087759017944\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 5.446974754333496 | KNN Loss: 4.390233039855957 | BCE Loss: 1.0567419528961182\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 5.393302917480469 | KNN Loss: 4.336073875427246 | BCE Loss: 1.0572288036346436\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 5.375741004943848 | KNN Loss: 4.349045276641846 | BCE Loss: 1.026695966720581\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 5.373605728149414 | KNN Loss: 4.350992202758789 | BCE Loss: 1.022613525390625\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 5.357257843017578 | KNN Loss: 4.3556084632873535 | BCE Loss: 1.0016496181488037\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 5.427806854248047 | KNN Loss: 4.376873016357422 | BCE Loss: 1.050934076309204\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 5.398143768310547 | KNN Loss: 4.369243621826172 | BCE Loss: 1.028900146484375\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 5.380046844482422 | KNN Loss: 4.354396343231201 | BCE Loss: 1.0256507396697998\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 5.445173740386963 | KNN Loss: 4.417774200439453 | BCE Loss: 1.0273995399475098\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 5.38372802734375 | KNN Loss: 4.355190753936768 | BCE Loss: 1.0285375118255615\n",
      "Epoch   434: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 5.42214298248291 | KNN Loss: 4.403573513031006 | BCE Loss: 1.0185697078704834\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 5.3994035720825195 | KNN Loss: 4.37520694732666 | BCE Loss: 1.0241963863372803\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 5.3763017654418945 | KNN Loss: 4.364681720733643 | BCE Loss: 1.011620283126831\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 5.419956207275391 | KNN Loss: 4.3831071853637695 | BCE Loss: 1.036849021911621\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 5.384068012237549 | KNN Loss: 4.367297172546387 | BCE Loss: 1.0167707204818726\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 5.375578880310059 | KNN Loss: 4.3526105880737305 | BCE Loss: 1.0229682922363281\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 5.355302810668945 | KNN Loss: 4.362179279327393 | BCE Loss: 0.9931232929229736\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 5.430901050567627 | KNN Loss: 4.397505283355713 | BCE Loss: 1.033395767211914\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 5.3657636642456055 | KNN Loss: 4.371401309967041 | BCE Loss: 0.9943622946739197\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 5.418927192687988 | KNN Loss: 4.383188247680664 | BCE Loss: 1.0357391834259033\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 5.418680667877197 | KNN Loss: 4.390442371368408 | BCE Loss: 1.0282381772994995\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 5.400864601135254 | KNN Loss: 4.378096103668213 | BCE Loss: 1.022768497467041\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 5.454748153686523 | KNN Loss: 4.415194511413574 | BCE Loss: 1.0395534038543701\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 5.408533573150635 | KNN Loss: 4.369309902191162 | BCE Loss: 1.0392236709594727\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 5.419329643249512 | KNN Loss: 4.387388706207275 | BCE Loss: 1.0319409370422363\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 5.44796085357666 | KNN Loss: 4.437530517578125 | BCE Loss: 1.010430097579956\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 5.383435249328613 | KNN Loss: 4.370993137359619 | BCE Loss: 1.0124423503875732\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 5.400430679321289 | KNN Loss: 4.388307094573975 | BCE Loss: 1.0121233463287354\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 5.357548713684082 | KNN Loss: 4.337861061096191 | BCE Loss: 1.0196874141693115\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 5.423729419708252 | KNN Loss: 4.365461826324463 | BCE Loss: 1.058267593383789\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 5.4068827629089355 | KNN Loss: 4.398284912109375 | BCE Loss: 1.0085978507995605\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 5.384472370147705 | KNN Loss: 4.358412742614746 | BCE Loss: 1.0260595083236694\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 5.369128704071045 | KNN Loss: 4.360248565673828 | BCE Loss: 1.0088801383972168\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 5.417934417724609 | KNN Loss: 4.3695878982543945 | BCE Loss: 1.048346757888794\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 5.3925604820251465 | KNN Loss: 4.380827903747559 | BCE Loss: 1.0117324590682983\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 5.383725166320801 | KNN Loss: 4.36204195022583 | BCE Loss: 1.0216834545135498\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 5.471615791320801 | KNN Loss: 4.419284820556641 | BCE Loss: 1.0523309707641602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 5.367556571960449 | KNN Loss: 4.350622177124023 | BCE Loss: 1.0169346332550049\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 5.426031589508057 | KNN Loss: 4.388668537139893 | BCE Loss: 1.037363052368164\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 5.385848045349121 | KNN Loss: 4.380543231964111 | BCE Loss: 1.0053046941757202\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 5.348920822143555 | KNN Loss: 4.359360694885254 | BCE Loss: 0.9895601272583008\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 5.410243034362793 | KNN Loss: 4.376736640930176 | BCE Loss: 1.0335063934326172\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 5.3865861892700195 | KNN Loss: 4.375328540802002 | BCE Loss: 1.0112576484680176\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 5.378180980682373 | KNN Loss: 4.356457233428955 | BCE Loss: 1.021723747253418\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 5.376687049865723 | KNN Loss: 4.3452959060668945 | BCE Loss: 1.0313913822174072\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 5.413727283477783 | KNN Loss: 4.370585918426514 | BCE Loss: 1.04314124584198\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 5.350863933563232 | KNN Loss: 4.34133768081665 | BCE Loss: 1.009526252746582\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 5.419608116149902 | KNN Loss: 4.394820213317871 | BCE Loss: 1.0247876644134521\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 5.429163932800293 | KNN Loss: 4.358471393585205 | BCE Loss: 1.0706926584243774\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 5.456218242645264 | KNN Loss: 4.420071601867676 | BCE Loss: 1.036146640777588\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 5.3491082191467285 | KNN Loss: 4.351357460021973 | BCE Loss: 0.9977508783340454\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 5.389852046966553 | KNN Loss: 4.376201152801514 | BCE Loss: 1.0136510133743286\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 5.430668830871582 | KNN Loss: 4.375426769256592 | BCE Loss: 1.0552420616149902\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 5.412191867828369 | KNN Loss: 4.390481472015381 | BCE Loss: 1.0217102766036987\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 5.359562873840332 | KNN Loss: 4.3719072341918945 | BCE Loss: 0.9876558780670166\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 5.37183952331543 | KNN Loss: 4.355588436126709 | BCE Loss: 1.0162508487701416\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 5.353721618652344 | KNN Loss: 4.347604274749756 | BCE Loss: 1.006117582321167\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 5.415869235992432 | KNN Loss: 4.386018753051758 | BCE Loss: 1.0298504829406738\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 5.389807224273682 | KNN Loss: 4.361438751220703 | BCE Loss: 1.028368592262268\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 5.377793788909912 | KNN Loss: 4.3825154304504395 | BCE Loss: 0.9952782392501831\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 5.407794952392578 | KNN Loss: 4.365005970001221 | BCE Loss: 1.0427888631820679\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 5.3413591384887695 | KNN Loss: 4.3412370681762695 | BCE Loss: 1.0001219511032104\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 5.422428131103516 | KNN Loss: 4.404773235321045 | BCE Loss: 1.0176550149917603\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 5.400371551513672 | KNN Loss: 4.357665061950684 | BCE Loss: 1.0427064895629883\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 5.37376594543457 | KNN Loss: 4.36552619934082 | BCE Loss: 1.00823974609375\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 5.4400177001953125 | KNN Loss: 4.376518726348877 | BCE Loss: 1.0634992122650146\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 5.4417009353637695 | KNN Loss: 4.388803958892822 | BCE Loss: 1.0528970956802368\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 5.366243839263916 | KNN Loss: 4.350080966949463 | BCE Loss: 1.0161627531051636\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 5.396258354187012 | KNN Loss: 4.369258403778076 | BCE Loss: 1.0269999504089355\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 5.432674407958984 | KNN Loss: 4.403186798095703 | BCE Loss: 1.0294877290725708\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 5.438610076904297 | KNN Loss: 4.408735275268555 | BCE Loss: 1.0298749208450317\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 5.397862911224365 | KNN Loss: 4.383691310882568 | BCE Loss: 1.0141717195510864\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 5.406182289123535 | KNN Loss: 4.342994213104248 | BCE Loss: 1.0631883144378662\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 5.395108699798584 | KNN Loss: 4.36246919631958 | BCE Loss: 1.032639503479004\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 5.4038801193237305 | KNN Loss: 4.379096508026123 | BCE Loss: 1.0247836112976074\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 5.426619529724121 | KNN Loss: 4.394782066345215 | BCE Loss: 1.0318375825881958\n",
      "Epoch   445: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 5.4435648918151855 | KNN Loss: 4.402436256408691 | BCE Loss: 1.0411285161972046\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 5.3515305519104 | KNN Loss: 4.334934234619141 | BCE Loss: 1.0165961980819702\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 5.3633317947387695 | KNN Loss: 4.351312160491943 | BCE Loss: 1.0120198726654053\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 5.404778003692627 | KNN Loss: 4.3599534034729 | BCE Loss: 1.0448246002197266\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 5.4308013916015625 | KNN Loss: 4.376774787902832 | BCE Loss: 1.0540263652801514\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 5.38101863861084 | KNN Loss: 4.367526054382324 | BCE Loss: 1.0134925842285156\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 5.398006439208984 | KNN Loss: 4.384891986846924 | BCE Loss: 1.0131144523620605\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 5.463719367980957 | KNN Loss: 4.444061279296875 | BCE Loss: 1.019657850265503\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 5.468340873718262 | KNN Loss: 4.395049095153809 | BCE Loss: 1.073291540145874\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 5.3979597091674805 | KNN Loss: 4.394887924194336 | BCE Loss: 1.0030715465545654\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 5.396515846252441 | KNN Loss: 4.371945858001709 | BCE Loss: 1.024570107460022\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 5.362260341644287 | KNN Loss: 4.37728214263916 | BCE Loss: 0.9849780797958374\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 5.456295013427734 | KNN Loss: 4.4177446365356445 | BCE Loss: 1.0385501384735107\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 5.422275543212891 | KNN Loss: 4.35807991027832 | BCE Loss: 1.0641956329345703\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 5.450484275817871 | KNN Loss: 4.409256935119629 | BCE Loss: 1.0412273406982422\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 5.384266376495361 | KNN Loss: 4.375671863555908 | BCE Loss: 1.0085945129394531\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 5.4228363037109375 | KNN Loss: 4.382048606872559 | BCE Loss: 1.040787696838379\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 5.380879878997803 | KNN Loss: 4.363936901092529 | BCE Loss: 1.0169428586959839\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 5.3988118171691895 | KNN Loss: 4.366892337799072 | BCE Loss: 1.0319194793701172\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 5.436591625213623 | KNN Loss: 4.368373870849609 | BCE Loss: 1.0682176351547241\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 5.372052192687988 | KNN Loss: 4.3425397872924805 | BCE Loss: 1.029512643814087\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 5.365788459777832 | KNN Loss: 4.342781066894531 | BCE Loss: 1.0230076313018799\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 5.430008888244629 | KNN Loss: 4.388739585876465 | BCE Loss: 1.041269302368164\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 5.407248497009277 | KNN Loss: 4.379693984985352 | BCE Loss: 1.0275546312332153\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 5.400816917419434 | KNN Loss: 4.374956130981445 | BCE Loss: 1.0258607864379883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 5.415876388549805 | KNN Loss: 4.390794277191162 | BCE Loss: 1.025081992149353\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 5.352718353271484 | KNN Loss: 4.345103740692139 | BCE Loss: 1.0076146125793457\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 5.429523944854736 | KNN Loss: 4.387258529663086 | BCE Loss: 1.0422654151916504\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 5.395442962646484 | KNN Loss: 4.361802577972412 | BCE Loss: 1.0336403846740723\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 5.387103080749512 | KNN Loss: 4.352215766906738 | BCE Loss: 1.034887433052063\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 5.407593727111816 | KNN Loss: 4.380495548248291 | BCE Loss: 1.0270979404449463\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 5.409254550933838 | KNN Loss: 4.3655805587768555 | BCE Loss: 1.043674111366272\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 5.382393836975098 | KNN Loss: 4.349435806274414 | BCE Loss: 1.0329582691192627\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 5.401621341705322 | KNN Loss: 4.351383209228516 | BCE Loss: 1.0502381324768066\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 5.389606952667236 | KNN Loss: 4.373688220977783 | BCE Loss: 1.0159187316894531\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 5.37187385559082 | KNN Loss: 4.345754146575928 | BCE Loss: 1.0261199474334717\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 5.392736434936523 | KNN Loss: 4.366720199584961 | BCE Loss: 1.0260164737701416\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 5.398656845092773 | KNN Loss: 4.34092903137207 | BCE Loss: 1.057727575302124\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 5.374115943908691 | KNN Loss: 4.341277599334717 | BCE Loss: 1.0328381061553955\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 5.468837261199951 | KNN Loss: 4.419220447540283 | BCE Loss: 1.0496166944503784\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 5.370240211486816 | KNN Loss: 4.354475498199463 | BCE Loss: 1.0157649517059326\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 5.459922790527344 | KNN Loss: 4.434845924377441 | BCE Loss: 1.0250768661499023\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 5.402836799621582 | KNN Loss: 4.372796058654785 | BCE Loss: 1.0300405025482178\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 5.385056495666504 | KNN Loss: 4.369792461395264 | BCE Loss: 1.0152640342712402\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 5.416886329650879 | KNN Loss: 4.367132186889648 | BCE Loss: 1.0497543811798096\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 5.402603626251221 | KNN Loss: 4.378809928894043 | BCE Loss: 1.0237936973571777\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 5.364455699920654 | KNN Loss: 4.346407890319824 | BCE Loss: 1.0180476903915405\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 5.430200576782227 | KNN Loss: 4.3774333000183105 | BCE Loss: 1.0527675151824951\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 5.373322486877441 | KNN Loss: 4.37779426574707 | BCE Loss: 0.9955281019210815\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 5.420525550842285 | KNN Loss: 4.3951544761657715 | BCE Loss: 1.0253708362579346\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 5.4152679443359375 | KNN Loss: 4.375617504119873 | BCE Loss: 1.039650321006775\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 5.379298210144043 | KNN Loss: 4.3501200675964355 | BCE Loss: 1.0291779041290283\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 5.378230094909668 | KNN Loss: 4.351871490478516 | BCE Loss: 1.0263588428497314\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 5.399271011352539 | KNN Loss: 4.3564839363098145 | BCE Loss: 1.0427873134613037\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 5.409768581390381 | KNN Loss: 4.3756303787231445 | BCE Loss: 1.0341383218765259\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 5.392010688781738 | KNN Loss: 4.369268417358398 | BCE Loss: 1.0227423906326294\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 5.418284893035889 | KNN Loss: 4.37725830078125 | BCE Loss: 1.0410265922546387\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 5.379517078399658 | KNN Loss: 4.368186950683594 | BCE Loss: 1.0113301277160645\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 5.40477180480957 | KNN Loss: 4.3699631690979 | BCE Loss: 1.0348083972930908\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 5.43532133102417 | KNN Loss: 4.408304691314697 | BCE Loss: 1.0270167589187622\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 5.411479949951172 | KNN Loss: 4.3675665855407715 | BCE Loss: 1.0439136028289795\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 5.387319564819336 | KNN Loss: 4.374350070953369 | BCE Loss: 1.0129694938659668\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 5.4031219482421875 | KNN Loss: 4.388085842132568 | BCE Loss: 1.0150361061096191\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 5.439186096191406 | KNN Loss: 4.387289047241211 | BCE Loss: 1.0518972873687744\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 5.409946441650391 | KNN Loss: 4.394762992858887 | BCE Loss: 1.0151832103729248\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 5.410375595092773 | KNN Loss: 4.384379863739014 | BCE Loss: 1.0259959697723389\n",
      "Epoch   456: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 5.395218372344971 | KNN Loss: 4.358841896057129 | BCE Loss: 1.0363764762878418\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 5.405580043792725 | KNN Loss: 4.378788471221924 | BCE Loss: 1.0267915725708008\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 5.454453945159912 | KNN Loss: 4.414881706237793 | BCE Loss: 1.0395723581314087\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 5.419399261474609 | KNN Loss: 4.379944324493408 | BCE Loss: 1.0394549369812012\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 5.4459333419799805 | KNN Loss: 4.416942119598389 | BCE Loss: 1.0289912223815918\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 5.440308570861816 | KNN Loss: 4.407122611999512 | BCE Loss: 1.0331860780715942\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 5.3518290519714355 | KNN Loss: 4.343143463134766 | BCE Loss: 1.00868558883667\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 5.398160457611084 | KNN Loss: 4.374791622161865 | BCE Loss: 1.0233688354492188\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 5.439691543579102 | KNN Loss: 4.403257846832275 | BCE Loss: 1.0364335775375366\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 5.338719367980957 | KNN Loss: 4.349362850189209 | BCE Loss: 0.9893563985824585\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 5.438543796539307 | KNN Loss: 4.4160661697387695 | BCE Loss: 1.0224777460098267\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 5.435080528259277 | KNN Loss: 4.381406307220459 | BCE Loss: 1.0536739826202393\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 5.398294925689697 | KNN Loss: 4.379756450653076 | BCE Loss: 1.0185385942459106\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 5.377793312072754 | KNN Loss: 4.364179611206055 | BCE Loss: 1.0136139392852783\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 5.442654609680176 | KNN Loss: 4.426259994506836 | BCE Loss: 1.0163946151733398\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 5.439587593078613 | KNN Loss: 4.391418933868408 | BCE Loss: 1.0481687784194946\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 5.384479522705078 | KNN Loss: 4.38065242767334 | BCE Loss: 1.0038270950317383\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 5.371402740478516 | KNN Loss: 4.3598809242248535 | BCE Loss: 1.011521577835083\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 5.403537750244141 | KNN Loss: 4.364541530609131 | BCE Loss: 1.0389959812164307\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 5.436013221740723 | KNN Loss: 4.388458728790283 | BCE Loss: 1.0475542545318604\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 5.425368785858154 | KNN Loss: 4.370207786560059 | BCE Loss: 1.0551611185073853\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 5.336714267730713 | KNN Loss: 4.346714019775391 | BCE Loss: 0.9900003671646118\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 5.433856964111328 | KNN Loss: 4.407812595367432 | BCE Loss: 1.0260443687438965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 5.3846659660339355 | KNN Loss: 4.376435279846191 | BCE Loss: 1.0082306861877441\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 5.380387306213379 | KNN Loss: 4.3619232177734375 | BCE Loss: 1.0184638500213623\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 5.428438186645508 | KNN Loss: 4.376776218414307 | BCE Loss: 1.0516619682312012\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 5.414963722229004 | KNN Loss: 4.393837928771973 | BCE Loss: 1.0211255550384521\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 5.447108268737793 | KNN Loss: 4.378981590270996 | BCE Loss: 1.0681264400482178\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 5.400277614593506 | KNN Loss: 4.39302396774292 | BCE Loss: 1.0072537660598755\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 5.373232841491699 | KNN Loss: 4.364997863769531 | BCE Loss: 1.0082347393035889\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 5.353822708129883 | KNN Loss: 4.343193054199219 | BCE Loss: 1.0106298923492432\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 5.387935161590576 | KNN Loss: 4.3482441902160645 | BCE Loss: 1.0396909713745117\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 5.442765235900879 | KNN Loss: 4.405671119689941 | BCE Loss: 1.0370943546295166\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 5.374405384063721 | KNN Loss: 4.370744705200195 | BCE Loss: 1.0036606788635254\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 5.3730788230896 | KNN Loss: 4.357083320617676 | BCE Loss: 1.0159956216812134\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 5.430331230163574 | KNN Loss: 4.413877010345459 | BCE Loss: 1.0164544582366943\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 5.37476921081543 | KNN Loss: 4.353239059448242 | BCE Loss: 1.0215301513671875\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 5.450537204742432 | KNN Loss: 4.3740410804748535 | BCE Loss: 1.0764961242675781\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 5.410670757293701 | KNN Loss: 4.361302852630615 | BCE Loss: 1.049367904663086\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 5.45081901550293 | KNN Loss: 4.392068862915039 | BCE Loss: 1.0587501525878906\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 5.420234680175781 | KNN Loss: 4.374532699584961 | BCE Loss: 1.0457020998001099\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 5.427857875823975 | KNN Loss: 4.37125825881958 | BCE Loss: 1.0565996170043945\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 5.371965408325195 | KNN Loss: 4.370628356933594 | BCE Loss: 1.0013370513916016\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 5.416215896606445 | KNN Loss: 4.3917388916015625 | BCE Loss: 1.024477243423462\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 5.445618152618408 | KNN Loss: 4.40318489074707 | BCE Loss: 1.042433261871338\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 5.424609661102295 | KNN Loss: 4.370273590087891 | BCE Loss: 1.0543359518051147\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 5.414449214935303 | KNN Loss: 4.378878116607666 | BCE Loss: 1.0355709791183472\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 5.406197547912598 | KNN Loss: 4.388881206512451 | BCE Loss: 1.017316222190857\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 5.362877368927002 | KNN Loss: 4.350772857666016 | BCE Loss: 1.0121045112609863\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 5.372766017913818 | KNN Loss: 4.37204647064209 | BCE Loss: 1.000719428062439\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 5.448625564575195 | KNN Loss: 4.3833723068237305 | BCE Loss: 1.0652531385421753\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 5.3852152824401855 | KNN Loss: 4.351613998413086 | BCE Loss: 1.0336012840270996\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 5.414085388183594 | KNN Loss: 4.394357204437256 | BCE Loss: 1.019728422164917\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 5.396143436431885 | KNN Loss: 4.369399070739746 | BCE Loss: 1.0267444849014282\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 5.441422462463379 | KNN Loss: 4.3766069412231445 | BCE Loss: 1.0648157596588135\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 5.379265308380127 | KNN Loss: 4.374648571014404 | BCE Loss: 1.0046167373657227\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 5.419315338134766 | KNN Loss: 4.370903015136719 | BCE Loss: 1.0484120845794678\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 5.381422519683838 | KNN Loss: 4.363829135894775 | BCE Loss: 1.0175933837890625\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 5.366386413574219 | KNN Loss: 4.336729049682617 | BCE Loss: 1.0296576023101807\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 5.419994354248047 | KNN Loss: 4.379058837890625 | BCE Loss: 1.0409353971481323\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 5.395296096801758 | KNN Loss: 4.383993148803711 | BCE Loss: 1.011303186416626\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 5.416048049926758 | KNN Loss: 4.365744113922119 | BCE Loss: 1.0503039360046387\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 5.41691780090332 | KNN Loss: 4.367459774017334 | BCE Loss: 1.0494582653045654\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 5.452293395996094 | KNN Loss: 4.412116527557373 | BCE Loss: 1.0401766300201416\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 5.4109787940979 | KNN Loss: 4.375838756561279 | BCE Loss: 1.0351399183273315\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 5.403883457183838 | KNN Loss: 4.395162582397461 | BCE Loss: 1.008720874786377\n",
      "Epoch   467: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 5.3942975997924805 | KNN Loss: 4.381301403045654 | BCE Loss: 1.012995958328247\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 5.412371635437012 | KNN Loss: 4.374017238616943 | BCE Loss: 1.0383541584014893\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 5.403234958648682 | KNN Loss: 4.381037712097168 | BCE Loss: 1.0221972465515137\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 5.396188259124756 | KNN Loss: 4.352267265319824 | BCE Loss: 1.0439211130142212\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 5.486881256103516 | KNN Loss: 4.443312644958496 | BCE Loss: 1.0435686111450195\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 5.420734405517578 | KNN Loss: 4.3810319900512695 | BCE Loss: 1.0397021770477295\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 5.377037525177002 | KNN Loss: 4.34950590133667 | BCE Loss: 1.027531623840332\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 5.3815765380859375 | KNN Loss: 4.355425834655762 | BCE Loss: 1.0261509418487549\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 5.440906524658203 | KNN Loss: 4.3785719871521 | BCE Loss: 1.0623347759246826\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 5.431962490081787 | KNN Loss: 4.390167236328125 | BCE Loss: 1.041795253753662\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 5.430593490600586 | KNN Loss: 4.394999027252197 | BCE Loss: 1.0355942249298096\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 5.415096282958984 | KNN Loss: 4.400106906890869 | BCE Loss: 1.0149896144866943\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 5.4472575187683105 | KNN Loss: 4.411117076873779 | BCE Loss: 1.0361403226852417\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 5.395439147949219 | KNN Loss: 4.373007774353027 | BCE Loss: 1.0224313735961914\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 5.427228927612305 | KNN Loss: 4.3917646408081055 | BCE Loss: 1.0354642868041992\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 5.385045051574707 | KNN Loss: 4.369686603546143 | BCE Loss: 1.0153586864471436\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 5.396415710449219 | KNN Loss: 4.357969760894775 | BCE Loss: 1.0384457111358643\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 5.425477027893066 | KNN Loss: 4.406137466430664 | BCE Loss: 1.0193393230438232\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 5.404868125915527 | KNN Loss: 4.355614185333252 | BCE Loss: 1.0492537021636963\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 5.381070137023926 | KNN Loss: 4.3665924072265625 | BCE Loss: 1.0144774913787842\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 5.390532493591309 | KNN Loss: 4.3740973472595215 | BCE Loss: 1.016434907913208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 5.379047393798828 | KNN Loss: 4.343362808227539 | BCE Loss: 1.0356848239898682\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 5.382643699645996 | KNN Loss: 4.343164920806885 | BCE Loss: 1.0394785404205322\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 5.374365329742432 | KNN Loss: 4.361779689788818 | BCE Loss: 1.0125857591629028\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 5.399739742279053 | KNN Loss: 4.361929416656494 | BCE Loss: 1.037810206413269\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 5.390772819519043 | KNN Loss: 4.3784918785095215 | BCE Loss: 1.012281060218811\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 5.411612510681152 | KNN Loss: 4.391783714294434 | BCE Loss: 1.0198289155960083\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 5.435630798339844 | KNN Loss: 4.365993499755859 | BCE Loss: 1.0696371793746948\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 5.437350273132324 | KNN Loss: 4.367086887359619 | BCE Loss: 1.0702636241912842\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 5.3775739669799805 | KNN Loss: 4.365750312805176 | BCE Loss: 1.0118236541748047\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 5.413757801055908 | KNN Loss: 4.384561061859131 | BCE Loss: 1.0291967391967773\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 5.406888961791992 | KNN Loss: 4.3518147468566895 | BCE Loss: 1.0550743341445923\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 5.388825416564941 | KNN Loss: 4.359323501586914 | BCE Loss: 1.0295016765594482\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 5.4194416999816895 | KNN Loss: 4.352961540222168 | BCE Loss: 1.066480278968811\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 5.383759498596191 | KNN Loss: 4.371404647827148 | BCE Loss: 1.0123546123504639\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 5.4010090827941895 | KNN Loss: 4.359655857086182 | BCE Loss: 1.0413533449172974\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 5.405929088592529 | KNN Loss: 4.368170261383057 | BCE Loss: 1.0377589464187622\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 5.367300033569336 | KNN Loss: 4.351739406585693 | BCE Loss: 1.015560507774353\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 5.445046901702881 | KNN Loss: 4.413285732269287 | BCE Loss: 1.0317611694335938\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 5.399904251098633 | KNN Loss: 4.353751182556152 | BCE Loss: 1.0461528301239014\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 5.399718284606934 | KNN Loss: 4.365616798400879 | BCE Loss: 1.0341014862060547\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 5.382901668548584 | KNN Loss: 4.380895614624023 | BCE Loss: 1.0020060539245605\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 5.428948879241943 | KNN Loss: 4.390496253967285 | BCE Loss: 1.0384525060653687\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 5.435141563415527 | KNN Loss: 4.356103420257568 | BCE Loss: 1.079038381576538\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 5.398041725158691 | KNN Loss: 4.351904392242432 | BCE Loss: 1.0461373329162598\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 5.419119358062744 | KNN Loss: 4.387770175933838 | BCE Loss: 1.0313491821289062\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 5.375570774078369 | KNN Loss: 4.353178024291992 | BCE Loss: 1.0223928689956665\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 5.393902778625488 | KNN Loss: 4.365261554718018 | BCE Loss: 1.0286412239074707\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 5.42252254486084 | KNN Loss: 4.382798671722412 | BCE Loss: 1.0397238731384277\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 5.463146686553955 | KNN Loss: 4.40031099319458 | BCE Loss: 1.062835693359375\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 5.514674663543701 | KNN Loss: 4.4858808517456055 | BCE Loss: 1.0287936925888062\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 5.4289703369140625 | KNN Loss: 4.392786026000977 | BCE Loss: 1.036184549331665\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 5.415962219238281 | KNN Loss: 4.389954566955566 | BCE Loss: 1.026007890701294\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 5.404268741607666 | KNN Loss: 4.395538806915283 | BCE Loss: 1.0087299346923828\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 5.38975191116333 | KNN Loss: 4.3731584548950195 | BCE Loss: 1.0165934562683105\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 5.430415153503418 | KNN Loss: 4.412153720855713 | BCE Loss: 1.0182616710662842\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 5.4063334465026855 | KNN Loss: 4.35258674621582 | BCE Loss: 1.0537468194961548\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 5.420898914337158 | KNN Loss: 4.393078804016113 | BCE Loss: 1.027820110321045\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 5.355772018432617 | KNN Loss: 4.340407848358154 | BCE Loss: 1.0153642892837524\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 5.428605079650879 | KNN Loss: 4.421846866607666 | BCE Loss: 1.0067579746246338\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 5.451779842376709 | KNN Loss: 4.402576923370361 | BCE Loss: 1.0492029190063477\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 5.3824687004089355 | KNN Loss: 4.38623046875 | BCE Loss: 0.996238112449646\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 5.414455413818359 | KNN Loss: 4.3634934425354 | BCE Loss: 1.050962209701538\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 5.426575660705566 | KNN Loss: 4.398833751678467 | BCE Loss: 1.0277419090270996\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 5.438563823699951 | KNN Loss: 4.4179534912109375 | BCE Loss: 1.0206103324890137\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 5.408557891845703 | KNN Loss: 4.361120700836182 | BCE Loss: 1.0474369525909424\n",
      "Epoch   478: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 5.450284004211426 | KNN Loss: 4.4079766273498535 | BCE Loss: 1.0423076152801514\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 5.400022983551025 | KNN Loss: 4.391345024108887 | BCE Loss: 1.0086780786514282\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 5.405339241027832 | KNN Loss: 4.362082481384277 | BCE Loss: 1.0432567596435547\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 5.42168664932251 | KNN Loss: 4.399327754974365 | BCE Loss: 1.022359013557434\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 5.411940574645996 | KNN Loss: 4.382264614105225 | BCE Loss: 1.0296759605407715\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 5.401362895965576 | KNN Loss: 4.3609747886657715 | BCE Loss: 1.0403882265090942\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 5.462319374084473 | KNN Loss: 4.444964408874512 | BCE Loss: 1.0173548460006714\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 5.402553558349609 | KNN Loss: 4.370506763458252 | BCE Loss: 1.0320470333099365\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 5.380833148956299 | KNN Loss: 4.369869709014893 | BCE Loss: 1.0109633207321167\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 5.389496326446533 | KNN Loss: 4.3466596603393555 | BCE Loss: 1.0428367853164673\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 5.390749454498291 | KNN Loss: 4.364905834197998 | BCE Loss: 1.0258437395095825\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 5.41500186920166 | KNN Loss: 4.363211631774902 | BCE Loss: 1.0517903566360474\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 5.404351234436035 | KNN Loss: 4.381060600280762 | BCE Loss: 1.023290753364563\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 5.372463226318359 | KNN Loss: 4.368139743804932 | BCE Loss: 1.0043234825134277\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 5.449183940887451 | KNN Loss: 4.428335666656494 | BCE Loss: 1.020848274230957\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 5.36812686920166 | KNN Loss: 4.358078956604004 | BCE Loss: 1.0100479125976562\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 5.446604251861572 | KNN Loss: 4.408962726593018 | BCE Loss: 1.0376415252685547\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 5.443514823913574 | KNN Loss: 4.389811038970947 | BCE Loss: 1.053703784942627\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 5.377845764160156 | KNN Loss: 4.348750591278076 | BCE Loss: 1.0290954113006592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 5.416067123413086 | KNN Loss: 4.398866176605225 | BCE Loss: 1.0172009468078613\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 5.411890029907227 | KNN Loss: 4.358218193054199 | BCE Loss: 1.0536715984344482\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 5.356133460998535 | KNN Loss: 4.349209785461426 | BCE Loss: 1.0069239139556885\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 5.366866111755371 | KNN Loss: 4.337185859680176 | BCE Loss: 1.0296804904937744\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 5.434194564819336 | KNN Loss: 4.369448184967041 | BCE Loss: 1.0647461414337158\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 5.406996250152588 | KNN Loss: 4.394435405731201 | BCE Loss: 1.0125607252120972\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 5.428384780883789 | KNN Loss: 4.412336826324463 | BCE Loss: 1.0160481929779053\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 5.390324592590332 | KNN Loss: 4.360025405883789 | BCE Loss: 1.030299425125122\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 5.426936149597168 | KNN Loss: 4.367991924285889 | BCE Loss: 1.0589443445205688\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 5.385473251342773 | KNN Loss: 4.37533712387085 | BCE Loss: 1.010136365890503\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 5.370644569396973 | KNN Loss: 4.3463521003723145 | BCE Loss: 1.0242925882339478\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 5.381106853485107 | KNN Loss: 4.349637508392334 | BCE Loss: 1.031469464302063\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 5.465287685394287 | KNN Loss: 4.4324631690979 | BCE Loss: 1.0328246355056763\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 5.393540382385254 | KNN Loss: 4.395957946777344 | BCE Loss: 0.9975824952125549\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 5.4235639572143555 | KNN Loss: 4.369096755981445 | BCE Loss: 1.0544673204421997\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 5.479862213134766 | KNN Loss: 4.431600570678711 | BCE Loss: 1.0482614040374756\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 5.417682647705078 | KNN Loss: 4.394759178161621 | BCE Loss: 1.022923231124878\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 5.458318710327148 | KNN Loss: 4.443688869476318 | BCE Loss: 1.0146299600601196\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 5.44592809677124 | KNN Loss: 4.394787788391113 | BCE Loss: 1.0511404275894165\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 5.4255852699279785 | KNN Loss: 4.376807689666748 | BCE Loss: 1.0487775802612305\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 5.376343727111816 | KNN Loss: 4.371045112609863 | BCE Loss: 1.0052988529205322\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 5.364121437072754 | KNN Loss: 4.351577281951904 | BCE Loss: 1.0125443935394287\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 5.378674030303955 | KNN Loss: 4.346195697784424 | BCE Loss: 1.0324783325195312\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 5.394077301025391 | KNN Loss: 4.373108386993408 | BCE Loss: 1.0209689140319824\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 5.412492275238037 | KNN Loss: 4.3753461837768555 | BCE Loss: 1.0371460914611816\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 5.389745712280273 | KNN Loss: 4.366289138793945 | BCE Loss: 1.023456335067749\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 5.397790431976318 | KNN Loss: 4.3837571144104 | BCE Loss: 1.014033317565918\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 5.383816242218018 | KNN Loss: 4.376715660095215 | BCE Loss: 1.0071005821228027\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 5.422513484954834 | KNN Loss: 4.378710746765137 | BCE Loss: 1.0438027381896973\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 5.372935771942139 | KNN Loss: 4.356107234954834 | BCE Loss: 1.0168285369873047\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 5.3952789306640625 | KNN Loss: 4.367020130157471 | BCE Loss: 1.0282585620880127\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 5.470137119293213 | KNN Loss: 4.435415267944336 | BCE Loss: 1.0347217321395874\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 5.404858589172363 | KNN Loss: 4.368597030639648 | BCE Loss: 1.0362615585327148\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 5.404365062713623 | KNN Loss: 4.367961406707764 | BCE Loss: 1.0364036560058594\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 5.401585102081299 | KNN Loss: 4.372427940368652 | BCE Loss: 1.029157280921936\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 5.357632637023926 | KNN Loss: 4.35042667388916 | BCE Loss: 1.007205843925476\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 5.4257659912109375 | KNN Loss: 4.38892126083374 | BCE Loss: 1.0368444919586182\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 5.358585834503174 | KNN Loss: 4.341151714324951 | BCE Loss: 1.0174342393875122\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 5.417664051055908 | KNN Loss: 4.409700393676758 | BCE Loss: 1.0079636573791504\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 5.39593505859375 | KNN Loss: 4.354383945465088 | BCE Loss: 1.0415513515472412\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 5.366150856018066 | KNN Loss: 4.353139400482178 | BCE Loss: 1.0130114555358887\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 5.384197235107422 | KNN Loss: 4.370023250579834 | BCE Loss: 1.0141737461090088\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 5.394276142120361 | KNN Loss: 4.369847297668457 | BCE Loss: 1.0244287252426147\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 5.365304470062256 | KNN Loss: 4.345265865325928 | BCE Loss: 1.0200387239456177\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 5.387146949768066 | KNN Loss: 4.34491491317749 | BCE Loss: 1.0422320365905762\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 5.456869602203369 | KNN Loss: 4.398187160491943 | BCE Loss: 1.0586825609207153\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 5.432061195373535 | KNN Loss: 4.412440299987793 | BCE Loss: 1.0196208953857422\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 5.352611541748047 | KNN Loss: 4.340941429138184 | BCE Loss: 1.0116702318191528\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 5.447500228881836 | KNN Loss: 4.4107537269592285 | BCE Loss: 1.0367467403411865\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 5.405345916748047 | KNN Loss: 4.384258270263672 | BCE Loss: 1.0210877656936646\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 5.439577102661133 | KNN Loss: 4.389935493469238 | BCE Loss: 1.0496416091918945\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 5.3989644050598145 | KNN Loss: 4.369136333465576 | BCE Loss: 1.0298281908035278\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 5.351780414581299 | KNN Loss: 4.345222473144531 | BCE Loss: 1.0065580606460571\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 5.413944244384766 | KNN Loss: 4.37906551361084 | BCE Loss: 1.0348787307739258\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 5.381244659423828 | KNN Loss: 4.349507808685303 | BCE Loss: 1.0317368507385254\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 5.395752906799316 | KNN Loss: 4.363969802856445 | BCE Loss: 1.0317833423614502\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 5.4287028312683105 | KNN Loss: 4.381192207336426 | BCE Loss: 1.0475106239318848\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 5.379841327667236 | KNN Loss: 4.342757225036621 | BCE Loss: 1.0370841026306152\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 5.399404048919678 | KNN Loss: 4.353395462036133 | BCE Loss: 1.046008586883545\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 5.4103827476501465 | KNN Loss: 4.391193389892578 | BCE Loss: 1.0191893577575684\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 5.3690595626831055 | KNN Loss: 4.359167098999023 | BCE Loss: 1.0098927021026611\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 5.441704273223877 | KNN Loss: 4.43056058883667 | BCE Loss: 1.011143684387207\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 5.431539535522461 | KNN Loss: 4.38043212890625 | BCE Loss: 1.0511075258255005\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 5.422856330871582 | KNN Loss: 4.401992321014404 | BCE Loss: 1.0208637714385986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 5.432872772216797 | KNN Loss: 4.369320869445801 | BCE Loss: 1.0635517835617065\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 5.410745143890381 | KNN Loss: 4.365313529968262 | BCE Loss: 1.0454316139221191\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 5.4033637046813965 | KNN Loss: 4.357863426208496 | BCE Loss: 1.0455002784729004\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 5.490099906921387 | KNN Loss: 4.437799453735352 | BCE Loss: 1.052300214767456\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 5.450606346130371 | KNN Loss: 4.41609525680542 | BCE Loss: 1.0345113277435303\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 5.399035930633545 | KNN Loss: 4.3543572425842285 | BCE Loss: 1.044678807258606\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 5.3918023109436035 | KNN Loss: 4.384982585906982 | BCE Loss: 1.006819725036621\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 5.442098617553711 | KNN Loss: 4.405357360839844 | BCE Loss: 1.0367413759231567\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 5.422088623046875 | KNN Loss: 4.392343044281006 | BCE Loss: 1.0297456979751587\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 5.403036594390869 | KNN Loss: 4.366491317749023 | BCE Loss: 1.0365453958511353\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 5.439441204071045 | KNN Loss: 4.423876762390137 | BCE Loss: 1.0155644416809082\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 5.401525497436523 | KNN Loss: 4.381256580352783 | BCE Loss: 1.0202691555023193\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 5.421152591705322 | KNN Loss: 4.382345676422119 | BCE Loss: 1.0388069152832031\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 5.433232307434082 | KNN Loss: 4.388637065887451 | BCE Loss: 1.0445950031280518\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 5.421239852905273 | KNN Loss: 4.403185844421387 | BCE Loss: 1.0180541276931763\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 5.405158996582031 | KNN Loss: 4.361556053161621 | BCE Loss: 1.0436030626296997\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 5.419285774230957 | KNN Loss: 4.388576984405518 | BCE Loss: 1.030708909034729\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 5.39199161529541 | KNN Loss: 4.365772247314453 | BCE Loss: 1.026219129562378\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 5.465692520141602 | KNN Loss: 4.44582462310791 | BCE Loss: 1.0198681354522705\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 5.408138275146484 | KNN Loss: 4.365225791931152 | BCE Loss: 1.0429126024246216\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 5.398497581481934 | KNN Loss: 4.372446537017822 | BCE Loss: 1.0260511636734009\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 5.442933559417725 | KNN Loss: 4.418290615081787 | BCE Loss: 1.024643063545227\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 5.38478946685791 | KNN Loss: 4.388254165649414 | BCE Loss: 0.9965354204177856\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 5.471179962158203 | KNN Loss: 4.43362283706665 | BCE Loss: 1.0375568866729736\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 5.414347171783447 | KNN Loss: 4.400408744812012 | BCE Loss: 1.0139384269714355\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 5.351776123046875 | KNN Loss: 4.346593856811523 | BCE Loss: 1.0051823854446411\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 5.408919334411621 | KNN Loss: 4.3714141845703125 | BCE Loss: 1.0375051498413086\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 5.408462047576904 | KNN Loss: 4.413602352142334 | BCE Loss: 0.994859516620636\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 5.398003101348877 | KNN Loss: 4.351246356964111 | BCE Loss: 1.0467567443847656\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 5.36721658706665 | KNN Loss: 4.372143268585205 | BCE Loss: 0.9950734376907349\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 5.3957366943359375 | KNN Loss: 4.381837844848633 | BCE Loss: 1.0138989686965942\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 5.435959815979004 | KNN Loss: 4.396761417388916 | BCE Loss: 1.0391981601715088\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 5.42953634262085 | KNN Loss: 4.376980304718018 | BCE Loss: 1.052556037902832\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 5.431363105773926 | KNN Loss: 4.401946544647217 | BCE Loss: 1.029416561126709\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 5.410933971405029 | KNN Loss: 4.365524768829346 | BCE Loss: 1.0454092025756836\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 5.350894927978516 | KNN Loss: 4.339109420776367 | BCE Loss: 1.0117857456207275\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 5.39200496673584 | KNN Loss: 4.354554653167725 | BCE Loss: 1.0374504327774048\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 5.405279636383057 | KNN Loss: 4.363376140594482 | BCE Loss: 1.0419034957885742\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 5.365597248077393 | KNN Loss: 4.369575500488281 | BCE Loss: 0.9960216283798218\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 5.393856048583984 | KNN Loss: 4.351464748382568 | BCE Loss: 1.0423915386199951\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 5.416725158691406 | KNN Loss: 4.365349769592285 | BCE Loss: 1.051375150680542\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 5.3917927742004395 | KNN Loss: 4.357515335083008 | BCE Loss: 1.0342774391174316\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 5.3862080574035645 | KNN Loss: 4.3765034675598145 | BCE Loss: 1.00970458984375\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 5.338769912719727 | KNN Loss: 4.339158535003662 | BCE Loss: 0.9996113777160645\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 5.3932318687438965 | KNN Loss: 4.36822509765625 | BCE Loss: 1.025006890296936\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 5.403142929077148 | KNN Loss: 4.3618669509887695 | BCE Loss: 1.041275978088379\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 5.4337663650512695 | KNN Loss: 4.358251094818115 | BCE Loss: 1.0755155086517334\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 5.40887975692749 | KNN Loss: 4.371769428253174 | BCE Loss: 1.037110447883606\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 5.3728132247924805 | KNN Loss: 4.355672836303711 | BCE Loss: 1.0171403884887695\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.4010e+00,  2.2324e+00,  1.9259e+00,  4.4305e+00,  4.3083e+00,\n",
      "          8.2646e-01,  2.8776e+00,  2.2525e+00,  1.6196e+00,  2.5330e+00,\n",
      "          2.7708e+00,  2.3513e+00,  8.7957e-01,  1.9581e+00,  1.4848e+00,\n",
      "          1.2184e+00,  3.0277e+00,  1.7433e+00,  2.0688e+00,  1.9962e+00,\n",
      "          1.8550e+00,  1.6882e+00,  2.8604e+00,  3.1876e+00,  1.7777e+00,\n",
      "          1.9339e+00,  1.3438e+00,  9.6325e-01,  1.2171e+00,  3.6746e-01,\n",
      "          7.2058e-02,  8.8119e-01,  2.8268e-01,  1.0370e+00,  1.0608e+00,\n",
      "          1.9995e+00,  1.0214e+00,  4.1382e+00,  4.7325e-01,  1.4965e+00,\n",
      "          1.0061e+00, -6.7459e-01, -6.6089e-01,  2.7963e+00,  1.9189e+00,\n",
      "          3.0279e-01, -2.4250e-02,  2.4552e-02,  1.9926e+00,  1.7274e+00,\n",
      "          1.0519e+00, -2.2927e-02,  1.2995e+00,  7.2269e-01, -4.8415e-01,\n",
      "          1.5939e+00,  1.9762e+00,  1.1411e+00,  1.4182e+00,  1.0440e+00,\n",
      "          4.4690e-01,  9.6211e-01,  2.4799e-01,  1.5313e+00,  1.4831e+00,\n",
      "          1.7758e+00, -1.8357e+00,  5.8302e-01,  2.4601e+00,  1.4655e+00,\n",
      "          2.7618e+00,  1.1135e-01,  1.6022e+00,  2.2866e+00,  2.4406e+00,\n",
      "          1.2909e+00,  2.4865e-01,  1.1696e+00,  4.8819e-01,  1.8435e+00,\n",
      "         -2.0335e-01,  4.6302e-01,  2.3767e+00, -6.0679e-01,  3.5944e-01,\n",
      "         -1.1088e+00, -2.2428e+00, -5.3268e-01,  5.6952e-01, -1.7098e+00,\n",
      "          4.8803e-01,  2.6183e-01, -5.9443e-01, -9.4463e-01,  6.8592e-01,\n",
      "          1.4690e+00, -7.5474e-01, -7.5788e-01,  3.2281e-01,  1.2302e+00,\n",
      "          8.1894e-01, -1.1783e+00,  9.4169e-01,  8.4840e-01, -1.1659e+00,\n",
      "         -1.0749e+00, -1.5800e-01,  1.4696e-01, -1.3350e+00, -1.5098e+00,\n",
      "         -7.9593e-01, -2.9051e+00, -2.1505e-01,  1.9294e+00,  1.1939e+00,\n",
      "         -4.6798e-01, -9.7290e-01,  1.3226e-01,  1.2748e+00, -2.6190e+00,\n",
      "         -3.9222e-03, -2.7669e-01,  5.7525e-01, -5.9270e-01,  1.8766e-02,\n",
      "         -6.1924e-01, -6.9105e-01,  1.4289e+00,  5.4229e-01, -4.1545e-01,\n",
      "          3.4427e-01, -4.5541e-01, -1.3848e+00, -1.1232e-02, -5.1948e-01,\n",
      "          9.5104e-01, -4.4482e-01,  6.4808e-02, -1.8113e+00, -8.8574e-01,\n",
      "         -1.1895e+00,  7.5206e-01, -1.8372e+00, -9.3500e-01, -1.1205e+00,\n",
      "         -5.2461e-01, -1.6898e+00, -9.1442e-01, -2.4092e+00, -1.1335e+00,\n",
      "         -1.3425e+00, -4.5057e-01, -1.7227e+00,  2.0550e-01, -1.6577e+00,\n",
      "         -5.8621e-01, -3.4129e+00,  3.3701e-01, -1.2809e-01, -6.7373e-01,\n",
      "         -2.4252e+00, -1.5116e+00, -1.1779e+00, -1.0639e+00, -2.1546e+00,\n",
      "         -2.4899e+00, -3.2305e+00]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.4129, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.4305, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e336b7048d77480b880987ab3f569945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 76.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8359e2b54046b1b63892ec05d46a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b2b3a1a3aa416f948184945edff0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eca66c31c7d4ababd099af0eddc1e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "Epoch: 00 | Batch: 000 / 029 | Total loss: 9.632 | Reg loss: 0.009 | Tree loss: 9.632 | Accuracy: 0.000000 | 0.279 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 029 | Total loss: 9.619 | Reg loss: 0.009 | Tree loss: 9.619 | Accuracy: 0.000000 | 0.254 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 029 | Total loss: 9.609 | Reg loss: 0.008 | Tree loss: 9.609 | Accuracy: 0.000000 | 0.247 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 029 | Total loss: 9.603 | Reg loss: 0.008 | Tree loss: 9.603 | Accuracy: 0.000000 | 0.243 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 029 | Total loss: 9.588 | Reg loss: 0.008 | Tree loss: 9.588 | Accuracy: 0.000000 | 0.243 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 029 | Total loss: 9.586 | Reg loss: 0.008 | Tree loss: 9.586 | Accuracy: 0.000000 | 0.242 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 029 | Total loss: 9.570 | Reg loss: 0.008 | Tree loss: 9.570 | Accuracy: 0.003906 | 0.241 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 029 | Total loss: 9.567 | Reg loss: 0.007 | Tree loss: 9.567 | Accuracy: 0.001953 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 029 | Total loss: 9.567 | Reg loss: 0.007 | Tree loss: 9.567 | Accuracy: 0.007812 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 029 | Total loss: 9.542 | Reg loss: 0.007 | Tree loss: 9.542 | Accuracy: 0.011719 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 029 | Total loss: 9.538 | Reg loss: 0.007 | Tree loss: 9.538 | Accuracy: 0.035156 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 029 | Total loss: 9.534 | Reg loss: 0.007 | Tree loss: 9.534 | Accuracy: 0.064453 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 029 | Total loss: 9.519 | Reg loss: 0.008 | Tree loss: 9.519 | Accuracy: 0.136719 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 029 | Total loss: 9.513 | Reg loss: 0.008 | Tree loss: 9.513 | Accuracy: 0.205078 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 029 | Total loss: 9.501 | Reg loss: 0.008 | Tree loss: 9.501 | Accuracy: 0.273438 | 0.241 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 029 | Total loss: 9.505 | Reg loss: 0.008 | Tree loss: 9.505 | Accuracy: 0.240234 | 0.241 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 029 | Total loss: 9.500 | Reg loss: 0.008 | Tree loss: 9.500 | Accuracy: 0.259766 | 0.241 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 029 | Total loss: 9.483 | Reg loss: 0.008 | Tree loss: 9.483 | Accuracy: 0.267578 | 0.241 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 029 | Total loss: 9.480 | Reg loss: 0.009 | Tree loss: 9.480 | Accuracy: 0.259766 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 029 | Total loss: 9.464 | Reg loss: 0.009 | Tree loss: 9.464 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 029 | Total loss: 9.454 | Reg loss: 0.009 | Tree loss: 9.454 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 029 | Total loss: 9.452 | Reg loss: 0.009 | Tree loss: 9.452 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 029 | Total loss: 9.433 | Reg loss: 0.009 | Tree loss: 9.433 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 029 | Total loss: 9.432 | Reg loss: 0.010 | Tree loss: 9.432 | Accuracy: 0.277344 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 029 | Total loss: 9.427 | Reg loss: 0.010 | Tree loss: 9.427 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 029 | Total loss: 9.407 | Reg loss: 0.010 | Tree loss: 9.407 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 029 | Total loss: 9.406 | Reg loss: 0.010 | Tree loss: 9.406 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 029 | Total loss: 9.422 | Reg loss: 0.011 | Tree loss: 9.422 | Accuracy: 0.234375 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 029 | Total loss: 9.416 | Reg loss: 0.011 | Tree loss: 9.416 | Accuracy: 0.230769 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 01 | Batch: 000 / 029 | Total loss: 9.470 | Reg loss: 0.004 | Tree loss: 9.470 | Accuracy: 0.291016 | 0.241 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 029 | Total loss: 9.472 | Reg loss: 0.004 | Tree loss: 9.472 | Accuracy: 0.298828 | 0.241 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 029 | Total loss: 9.455 | Reg loss: 0.004 | Tree loss: 9.455 | Accuracy: 0.318359 | 0.241 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 029 | Total loss: 9.451 | Reg loss: 0.005 | Tree loss: 9.451 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 029 | Total loss: 9.439 | Reg loss: 0.005 | Tree loss: 9.439 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 029 | Total loss: 9.436 | Reg loss: 0.005 | Tree loss: 9.436 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 029 | Total loss: 9.444 | Reg loss: 0.005 | Tree loss: 9.444 | Accuracy: 0.226562 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 029 | Total loss: 9.419 | Reg loss: 0.005 | Tree loss: 9.419 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 029 | Total loss: 9.424 | Reg loss: 0.006 | Tree loss: 9.424 | Accuracy: 0.250000 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 029 | Total loss: 9.414 | Reg loss: 0.006 | Tree loss: 9.414 | Accuracy: 0.253906 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 029 | Total loss: 9.392 | Reg loss: 0.006 | Tree loss: 9.392 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 029 | Total loss: 9.392 | Reg loss: 0.007 | Tree loss: 9.392 | Accuracy: 0.281250 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 029 | Total loss: 9.394 | Reg loss: 0.007 | Tree loss: 9.394 | Accuracy: 0.248047 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 029 | Total loss: 9.368 | Reg loss: 0.007 | Tree loss: 9.368 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 029 | Total loss: 9.378 | Reg loss: 0.007 | Tree loss: 9.378 | Accuracy: 0.248047 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 029 | Total loss: 9.350 | Reg loss: 0.008 | Tree loss: 9.350 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 029 | Total loss: 9.356 | Reg loss: 0.008 | Tree loss: 9.356 | Accuracy: 0.265625 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 029 | Total loss: 9.351 | Reg loss: 0.008 | Tree loss: 9.351 | Accuracy: 0.242188 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 029 | Total loss: 9.321 | Reg loss: 0.009 | Tree loss: 9.321 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 029 | Total loss: 9.312 | Reg loss: 0.009 | Tree loss: 9.312 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 029 | Total loss: 9.318 | Reg loss: 0.010 | Tree loss: 9.318 | Accuracy: 0.271484 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 029 | Total loss: 9.313 | Reg loss: 0.010 | Tree loss: 9.313 | Accuracy: 0.259766 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 029 | Total loss: 9.298 | Reg loss: 0.010 | Tree loss: 9.298 | Accuracy: 0.267578 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 029 | Total loss: 9.290 | Reg loss: 0.011 | Tree loss: 9.290 | Accuracy: 0.267578 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 029 | Total loss: 9.284 | Reg loss: 0.011 | Tree loss: 9.284 | Accuracy: 0.265625 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 029 | Total loss: 9.268 | Reg loss: 0.011 | Tree loss: 9.268 | Accuracy: 0.277344 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 029 | Total loss: 9.259 | Reg loss: 0.012 | Tree loss: 9.259 | Accuracy: 0.275391 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 029 | Total loss: 9.248 | Reg loss: 0.012 | Tree loss: 9.248 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 029 | Total loss: 9.210 | Reg loss: 0.012 | Tree loss: 9.210 | Accuracy: 0.307692 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 02 | Batch: 000 / 029 | Total loss: 9.346 | Reg loss: 0.006 | Tree loss: 9.346 | Accuracy: 0.279297 | 0.241 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 029 | Total loss: 9.343 | Reg loss: 0.007 | Tree loss: 9.343 | Accuracy: 0.253906 | 0.241 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 029 | Total loss: 9.340 | Reg loss: 0.007 | Tree loss: 9.340 | Accuracy: 0.244141 | 0.241 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 029 | Total loss: 9.299 | Reg loss: 0.007 | Tree loss: 9.299 | Accuracy: 0.332031 | 0.241 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 004 / 029 | Total loss: 9.314 | Reg loss: 0.007 | Tree loss: 9.314 | Accuracy: 0.275391 | 0.241 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 029 | Total loss: 9.293 | Reg loss: 0.007 | Tree loss: 9.293 | Accuracy: 0.302734 | 0.241 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 029 | Total loss: 9.302 | Reg loss: 0.007 | Tree loss: 9.302 | Accuracy: 0.253906 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 029 | Total loss: 9.295 | Reg loss: 0.008 | Tree loss: 9.295 | Accuracy: 0.257812 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 029 | Total loss: 9.274 | Reg loss: 0.008 | Tree loss: 9.274 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 029 | Total loss: 9.279 | Reg loss: 0.008 | Tree loss: 9.279 | Accuracy: 0.250000 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 029 | Total loss: 9.252 | Reg loss: 0.009 | Tree loss: 9.252 | Accuracy: 0.269531 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 029 | Total loss: 9.251 | Reg loss: 0.009 | Tree loss: 9.251 | Accuracy: 0.259766 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 029 | Total loss: 9.257 | Reg loss: 0.009 | Tree loss: 9.257 | Accuracy: 0.242188 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 029 | Total loss: 9.224 | Reg loss: 0.010 | Tree loss: 9.224 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 029 | Total loss: 9.224 | Reg loss: 0.010 | Tree loss: 9.224 | Accuracy: 0.263672 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 029 | Total loss: 9.198 | Reg loss: 0.010 | Tree loss: 9.198 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 029 | Total loss: 9.202 | Reg loss: 0.011 | Tree loss: 9.202 | Accuracy: 0.271484 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 029 | Total loss: 9.184 | Reg loss: 0.011 | Tree loss: 9.184 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 029 | Total loss: 9.167 | Reg loss: 0.012 | Tree loss: 9.167 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 029 | Total loss: 9.177 | Reg loss: 0.012 | Tree loss: 9.177 | Accuracy: 0.269531 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 029 | Total loss: 9.160 | Reg loss: 0.012 | Tree loss: 9.160 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 029 | Total loss: 9.139 | Reg loss: 0.013 | Tree loss: 9.139 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 029 | Total loss: 9.139 | Reg loss: 0.013 | Tree loss: 9.139 | Accuracy: 0.269531 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 029 | Total loss: 9.113 | Reg loss: 0.014 | Tree loss: 9.113 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 029 | Total loss: 9.105 | Reg loss: 0.014 | Tree loss: 9.105 | Accuracy: 0.279297 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 029 | Total loss: 9.101 | Reg loss: 0.014 | Tree loss: 9.101 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 029 | Total loss: 9.092 | Reg loss: 0.015 | Tree loss: 9.092 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 029 | Total loss: 9.071 | Reg loss: 0.015 | Tree loss: 9.071 | Accuracy: 0.292969 | 0.241 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 029 | Total loss: 9.125 | Reg loss: 0.016 | Tree loss: 9.125 | Accuracy: 0.307692 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 03 | Batch: 000 / 029 | Total loss: 9.209 | Reg loss: 0.009 | Tree loss: 9.209 | Accuracy: 0.273438 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 029 | Total loss: 9.203 | Reg loss: 0.009 | Tree loss: 9.203 | Accuracy: 0.255859 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 029 | Total loss: 9.197 | Reg loss: 0.009 | Tree loss: 9.197 | Accuracy: 0.269531 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 029 | Total loss: 9.166 | Reg loss: 0.009 | Tree loss: 9.166 | Accuracy: 0.302734 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 029 | Total loss: 9.146 | Reg loss: 0.010 | Tree loss: 9.146 | Accuracy: 0.314453 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 029 | Total loss: 9.153 | Reg loss: 0.010 | Tree loss: 9.153 | Accuracy: 0.291016 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 029 | Total loss: 9.133 | Reg loss: 0.010 | Tree loss: 9.133 | Accuracy: 0.300781 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 029 | Total loss: 9.140 | Reg loss: 0.010 | Tree loss: 9.140 | Accuracy: 0.267578 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 029 | Total loss: 9.132 | Reg loss: 0.011 | Tree loss: 9.132 | Accuracy: 0.255859 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 029 | Total loss: 9.111 | Reg loss: 0.011 | Tree loss: 9.111 | Accuracy: 0.283203 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 029 | Total loss: 9.108 | Reg loss: 0.011 | Tree loss: 9.108 | Accuracy: 0.261719 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 029 | Total loss: 9.083 | Reg loss: 0.012 | Tree loss: 9.083 | Accuracy: 0.291016 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 029 | Total loss: 9.070 | Reg loss: 0.012 | Tree loss: 9.070 | Accuracy: 0.289062 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 029 | Total loss: 9.062 | Reg loss: 0.012 | Tree loss: 9.062 | Accuracy: 0.287109 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 029 | Total loss: 9.048 | Reg loss: 0.013 | Tree loss: 9.048 | Accuracy: 0.298828 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 029 | Total loss: 9.058 | Reg loss: 0.013 | Tree loss: 9.058 | Accuracy: 0.253906 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 029 | Total loss: 9.039 | Reg loss: 0.014 | Tree loss: 9.039 | Accuracy: 0.263672 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 029 | Total loss: 9.032 | Reg loss: 0.014 | Tree loss: 9.032 | Accuracy: 0.265625 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 029 | Total loss: 9.004 | Reg loss: 0.015 | Tree loss: 9.004 | Accuracy: 0.281250 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 029 | Total loss: 9.002 | Reg loss: 0.015 | Tree loss: 9.002 | Accuracy: 0.277344 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 029 | Total loss: 8.976 | Reg loss: 0.015 | Tree loss: 8.976 | Accuracy: 0.281250 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 029 | Total loss: 8.961 | Reg loss: 0.016 | Tree loss: 8.961 | Accuracy: 0.302734 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 029 | Total loss: 8.961 | Reg loss: 0.016 | Tree loss: 8.961 | Accuracy: 0.279297 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 029 | Total loss: 8.927 | Reg loss: 0.017 | Tree loss: 8.927 | Accuracy: 0.279297 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 029 | Total loss: 8.935 | Reg loss: 0.017 | Tree loss: 8.935 | Accuracy: 0.250000 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 029 | Total loss: 8.931 | Reg loss: 0.018 | Tree loss: 8.931 | Accuracy: 0.257812 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 029 | Total loss: 8.897 | Reg loss: 0.018 | Tree loss: 8.897 | Accuracy: 0.292969 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 029 | Total loss: 8.898 | Reg loss: 0.018 | Tree loss: 8.898 | Accuracy: 0.277344 | 0.241 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 029 | Total loss: 8.912 | Reg loss: 0.019 | Tree loss: 8.912 | Accuracy: 0.153846 | 0.241 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 04 | Batch: 000 / 029 | Total loss: 9.046 | Reg loss: 0.012 | Tree loss: 9.046 | Accuracy: 0.291016 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 029 | Total loss: 9.048 | Reg loss: 0.012 | Tree loss: 9.048 | Accuracy: 0.265625 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 029 | Total loss: 9.029 | Reg loss: 0.012 | Tree loss: 9.029 | Accuracy: 0.287109 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 029 | Total loss: 9.035 | Reg loss: 0.012 | Tree loss: 9.035 | Accuracy: 0.242188 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 029 | Total loss: 9.011 | Reg loss: 0.012 | Tree loss: 9.011 | Accuracy: 0.279297 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 029 | Total loss: 8.980 | Reg loss: 0.012 | Tree loss: 8.980 | Accuracy: 0.296875 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 029 | Total loss: 8.995 | Reg loss: 0.013 | Tree loss: 8.995 | Accuracy: 0.263672 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 029 | Total loss: 8.995 | Reg loss: 0.013 | Tree loss: 8.995 | Accuracy: 0.242188 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 029 | Total loss: 8.956 | Reg loss: 0.013 | Tree loss: 8.956 | Accuracy: 0.259766 | 0.241 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 009 / 029 | Total loss: 8.926 | Reg loss: 0.013 | Tree loss: 8.926 | Accuracy: 0.306641 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 029 | Total loss: 8.932 | Reg loss: 0.014 | Tree loss: 8.932 | Accuracy: 0.271484 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 029 | Total loss: 8.923 | Reg loss: 0.014 | Tree loss: 8.923 | Accuracy: 0.273438 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 029 | Total loss: 8.900 | Reg loss: 0.014 | Tree loss: 8.900 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 029 | Total loss: 8.894 | Reg loss: 0.015 | Tree loss: 8.894 | Accuracy: 0.279297 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 029 | Total loss: 8.882 | Reg loss: 0.015 | Tree loss: 8.882 | Accuracy: 0.269531 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 029 | Total loss: 8.828 | Reg loss: 0.016 | Tree loss: 8.828 | Accuracy: 0.316406 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 029 | Total loss: 8.831 | Reg loss: 0.016 | Tree loss: 8.831 | Accuracy: 0.289062 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 029 | Total loss: 8.829 | Reg loss: 0.016 | Tree loss: 8.829 | Accuracy: 0.265625 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 029 | Total loss: 8.815 | Reg loss: 0.017 | Tree loss: 8.815 | Accuracy: 0.277344 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 029 | Total loss: 8.796 | Reg loss: 0.017 | Tree loss: 8.796 | Accuracy: 0.281250 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 029 | Total loss: 8.783 | Reg loss: 0.018 | Tree loss: 8.783 | Accuracy: 0.281250 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 029 | Total loss: 8.765 | Reg loss: 0.018 | Tree loss: 8.765 | Accuracy: 0.273438 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 029 | Total loss: 8.761 | Reg loss: 0.019 | Tree loss: 8.761 | Accuracy: 0.287109 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 029 | Total loss: 8.751 | Reg loss: 0.019 | Tree loss: 8.751 | Accuracy: 0.251953 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 029 | Total loss: 8.724 | Reg loss: 0.020 | Tree loss: 8.724 | Accuracy: 0.279297 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 029 | Total loss: 8.681 | Reg loss: 0.020 | Tree loss: 8.681 | Accuracy: 0.308594 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 029 | Total loss: 8.682 | Reg loss: 0.020 | Tree loss: 8.682 | Accuracy: 0.302734 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 029 | Total loss: 8.667 | Reg loss: 0.021 | Tree loss: 8.667 | Accuracy: 0.289062 | 0.241 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 029 | Total loss: 8.790 | Reg loss: 0.021 | Tree loss: 8.790 | Accuracy: 0.000000 | 0.241 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 05 | Batch: 000 / 029 | Total loss: 8.867 | Reg loss: 0.014 | Tree loss: 8.867 | Accuracy: 0.302734 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 029 | Total loss: 8.851 | Reg loss: 0.014 | Tree loss: 8.851 | Accuracy: 0.320312 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 029 | Total loss: 8.831 | Reg loss: 0.014 | Tree loss: 8.831 | Accuracy: 0.320312 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 029 | Total loss: 8.861 | Reg loss: 0.014 | Tree loss: 8.861 | Accuracy: 0.253906 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 029 | Total loss: 8.844 | Reg loss: 0.015 | Tree loss: 8.844 | Accuracy: 0.255859 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 029 | Total loss: 8.818 | Reg loss: 0.015 | Tree loss: 8.818 | Accuracy: 0.281250 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 029 | Total loss: 8.787 | Reg loss: 0.015 | Tree loss: 8.787 | Accuracy: 0.267578 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 029 | Total loss: 8.755 | Reg loss: 0.015 | Tree loss: 8.755 | Accuracy: 0.314453 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 029 | Total loss: 8.773 | Reg loss: 0.015 | Tree loss: 8.773 | Accuracy: 0.259766 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 029 | Total loss: 8.756 | Reg loss: 0.016 | Tree loss: 8.756 | Accuracy: 0.271484 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 029 | Total loss: 8.750 | Reg loss: 0.016 | Tree loss: 8.750 | Accuracy: 0.257812 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 029 | Total loss: 8.721 | Reg loss: 0.016 | Tree loss: 8.721 | Accuracy: 0.273438 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 029 | Total loss: 8.725 | Reg loss: 0.017 | Tree loss: 8.725 | Accuracy: 0.250000 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 029 | Total loss: 8.692 | Reg loss: 0.017 | Tree loss: 8.692 | Accuracy: 0.289062 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 029 | Total loss: 8.674 | Reg loss: 0.017 | Tree loss: 8.674 | Accuracy: 0.292969 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 029 | Total loss: 8.662 | Reg loss: 0.018 | Tree loss: 8.662 | Accuracy: 0.269531 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 029 | Total loss: 8.661 | Reg loss: 0.018 | Tree loss: 8.661 | Accuracy: 0.236328 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 029 | Total loss: 8.587 | Reg loss: 0.019 | Tree loss: 8.587 | Accuracy: 0.308594 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 029 | Total loss: 8.587 | Reg loss: 0.019 | Tree loss: 8.587 | Accuracy: 0.291016 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 029 | Total loss: 8.574 | Reg loss: 0.019 | Tree loss: 8.574 | Accuracy: 0.285156 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 029 | Total loss: 8.537 | Reg loss: 0.020 | Tree loss: 8.537 | Accuracy: 0.300781 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 029 | Total loss: 8.546 | Reg loss: 0.020 | Tree loss: 8.546 | Accuracy: 0.271484 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 029 | Total loss: 8.549 | Reg loss: 0.021 | Tree loss: 8.549 | Accuracy: 0.271484 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 029 | Total loss: 8.522 | Reg loss: 0.021 | Tree loss: 8.522 | Accuracy: 0.265625 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 029 | Total loss: 8.515 | Reg loss: 0.021 | Tree loss: 8.515 | Accuracy: 0.244141 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 029 | Total loss: 8.479 | Reg loss: 0.022 | Tree loss: 8.479 | Accuracy: 0.285156 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 029 | Total loss: 8.481 | Reg loss: 0.022 | Tree loss: 8.481 | Accuracy: 0.261719 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 029 | Total loss: 8.446 | Reg loss: 0.022 | Tree loss: 8.446 | Accuracy: 0.300781 | 0.241 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 029 | Total loss: 8.587 | Reg loss: 0.023 | Tree loss: 8.587 | Accuracy: 0.153846 | 0.241 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 06 | Batch: 000 / 029 | Total loss: 8.667 | Reg loss: 0.016 | Tree loss: 8.667 | Accuracy: 0.316406 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 029 | Total loss: 8.688 | Reg loss: 0.016 | Tree loss: 8.688 | Accuracy: 0.234375 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 029 | Total loss: 8.645 | Reg loss: 0.016 | Tree loss: 8.645 | Accuracy: 0.275391 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 029 | Total loss: 8.660 | Reg loss: 0.017 | Tree loss: 8.660 | Accuracy: 0.248047 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 029 | Total loss: 8.614 | Reg loss: 0.017 | Tree loss: 8.614 | Accuracy: 0.291016 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 029 | Total loss: 8.623 | Reg loss: 0.017 | Tree loss: 8.623 | Accuracy: 0.253906 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 029 | Total loss: 8.616 | Reg loss: 0.017 | Tree loss: 8.616 | Accuracy: 0.238281 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 029 | Total loss: 8.579 | Reg loss: 0.017 | Tree loss: 8.579 | Accuracy: 0.279297 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 029 | Total loss: 8.575 | Reg loss: 0.017 | Tree loss: 8.575 | Accuracy: 0.265625 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 029 | Total loss: 8.554 | Reg loss: 0.018 | Tree loss: 8.554 | Accuracy: 0.273438 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 029 | Total loss: 8.525 | Reg loss: 0.018 | Tree loss: 8.525 | Accuracy: 0.273438 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 029 | Total loss: 8.501 | Reg loss: 0.018 | Tree loss: 8.501 | Accuracy: 0.281250 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 029 | Total loss: 8.488 | Reg loss: 0.019 | Tree loss: 8.488 | Accuracy: 0.273438 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 029 | Total loss: 8.463 | Reg loss: 0.019 | Tree loss: 8.463 | Accuracy: 0.302734 | 0.242 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 014 / 029 | Total loss: 8.419 | Reg loss: 0.019 | Tree loss: 8.419 | Accuracy: 0.320312 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 029 | Total loss: 8.414 | Reg loss: 0.019 | Tree loss: 8.414 | Accuracy: 0.312500 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 029 | Total loss: 8.423 | Reg loss: 0.020 | Tree loss: 8.423 | Accuracy: 0.287109 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 029 | Total loss: 8.404 | Reg loss: 0.020 | Tree loss: 8.404 | Accuracy: 0.261719 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 029 | Total loss: 8.401 | Reg loss: 0.020 | Tree loss: 8.401 | Accuracy: 0.232422 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 029 | Total loss: 8.357 | Reg loss: 0.021 | Tree loss: 8.357 | Accuracy: 0.267578 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 029 | Total loss: 8.367 | Reg loss: 0.021 | Tree loss: 8.367 | Accuracy: 0.248047 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 029 | Total loss: 8.320 | Reg loss: 0.022 | Tree loss: 8.320 | Accuracy: 0.285156 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 029 | Total loss: 8.293 | Reg loss: 0.022 | Tree loss: 8.293 | Accuracy: 0.310547 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 029 | Total loss: 8.259 | Reg loss: 0.022 | Tree loss: 8.259 | Accuracy: 0.300781 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 029 | Total loss: 8.258 | Reg loss: 0.023 | Tree loss: 8.258 | Accuracy: 0.312500 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 029 | Total loss: 8.244 | Reg loss: 0.023 | Tree loss: 8.244 | Accuracy: 0.275391 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 029 | Total loss: 8.226 | Reg loss: 0.023 | Tree loss: 8.226 | Accuracy: 0.300781 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 029 | Total loss: 8.223 | Reg loss: 0.024 | Tree loss: 8.223 | Accuracy: 0.273438 | 0.242 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 029 | Total loss: 8.265 | Reg loss: 0.024 | Tree loss: 8.265 | Accuracy: 0.153846 | 0.242 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 07 | Batch: 000 / 029 | Total loss: 8.465 | Reg loss: 0.018 | Tree loss: 8.465 | Accuracy: 0.273438 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 029 | Total loss: 8.476 | Reg loss: 0.018 | Tree loss: 8.476 | Accuracy: 0.277344 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 029 | Total loss: 8.448 | Reg loss: 0.018 | Tree loss: 8.448 | Accuracy: 0.265625 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 029 | Total loss: 8.411 | Reg loss: 0.018 | Tree loss: 8.411 | Accuracy: 0.287109 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 029 | Total loss: 8.424 | Reg loss: 0.019 | Tree loss: 8.424 | Accuracy: 0.279297 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 029 | Total loss: 8.370 | Reg loss: 0.019 | Tree loss: 8.370 | Accuracy: 0.306641 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 029 | Total loss: 8.385 | Reg loss: 0.019 | Tree loss: 8.385 | Accuracy: 0.253906 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 029 | Total loss: 8.342 | Reg loss: 0.019 | Tree loss: 8.342 | Accuracy: 0.267578 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 029 | Total loss: 8.313 | Reg loss: 0.019 | Tree loss: 8.313 | Accuracy: 0.294922 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 029 | Total loss: 8.313 | Reg loss: 0.019 | Tree loss: 8.313 | Accuracy: 0.298828 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 029 | Total loss: 8.325 | Reg loss: 0.020 | Tree loss: 8.325 | Accuracy: 0.248047 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 029 | Total loss: 8.263 | Reg loss: 0.020 | Tree loss: 8.263 | Accuracy: 0.302734 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 029 | Total loss: 8.284 | Reg loss: 0.020 | Tree loss: 8.284 | Accuracy: 0.263672 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 029 | Total loss: 8.241 | Reg loss: 0.020 | Tree loss: 8.241 | Accuracy: 0.279297 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 029 | Total loss: 8.238 | Reg loss: 0.021 | Tree loss: 8.238 | Accuracy: 0.263672 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 029 | Total loss: 8.186 | Reg loss: 0.021 | Tree loss: 8.186 | Accuracy: 0.294922 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 029 | Total loss: 8.176 | Reg loss: 0.021 | Tree loss: 8.176 | Accuracy: 0.273438 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 029 | Total loss: 8.177 | Reg loss: 0.022 | Tree loss: 8.177 | Accuracy: 0.228516 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 029 | Total loss: 8.122 | Reg loss: 0.022 | Tree loss: 8.122 | Accuracy: 0.312500 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 029 | Total loss: 8.106 | Reg loss: 0.022 | Tree loss: 8.106 | Accuracy: 0.283203 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 029 | Total loss: 8.104 | Reg loss: 0.022 | Tree loss: 8.104 | Accuracy: 0.298828 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 029 | Total loss: 8.068 | Reg loss: 0.023 | Tree loss: 8.068 | Accuracy: 0.279297 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 029 | Total loss: 8.086 | Reg loss: 0.023 | Tree loss: 8.086 | Accuracy: 0.259766 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 029 | Total loss: 8.044 | Reg loss: 0.023 | Tree loss: 8.044 | Accuracy: 0.287109 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 029 | Total loss: 8.048 | Reg loss: 0.024 | Tree loss: 8.048 | Accuracy: 0.251953 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 029 | Total loss: 8.005 | Reg loss: 0.024 | Tree loss: 8.005 | Accuracy: 0.265625 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 029 | Total loss: 8.015 | Reg loss: 0.024 | Tree loss: 8.015 | Accuracy: 0.292969 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 029 | Total loss: 7.963 | Reg loss: 0.025 | Tree loss: 7.963 | Accuracy: 0.308594 | 0.242 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 029 | Total loss: 7.874 | Reg loss: 0.025 | Tree loss: 7.874 | Accuracy: 0.384615 | 0.242 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 08 | Batch: 000 / 029 | Total loss: 8.246 | Reg loss: 0.020 | Tree loss: 8.246 | Accuracy: 0.285156 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 029 | Total loss: 8.262 | Reg loss: 0.020 | Tree loss: 8.262 | Accuracy: 0.255859 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 029 | Total loss: 8.192 | Reg loss: 0.020 | Tree loss: 8.192 | Accuracy: 0.306641 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 029 | Total loss: 8.206 | Reg loss: 0.020 | Tree loss: 8.206 | Accuracy: 0.279297 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 029 | Total loss: 8.173 | Reg loss: 0.020 | Tree loss: 8.173 | Accuracy: 0.291016 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 029 | Total loss: 8.200 | Reg loss: 0.020 | Tree loss: 8.200 | Accuracy: 0.246094 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 029 | Total loss: 8.131 | Reg loss: 0.020 | Tree loss: 8.131 | Accuracy: 0.265625 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 029 | Total loss: 8.132 | Reg loss: 0.020 | Tree loss: 8.132 | Accuracy: 0.253906 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 029 | Total loss: 8.106 | Reg loss: 0.021 | Tree loss: 8.106 | Accuracy: 0.275391 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 029 | Total loss: 8.065 | Reg loss: 0.021 | Tree loss: 8.065 | Accuracy: 0.310547 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 029 | Total loss: 8.031 | Reg loss: 0.021 | Tree loss: 8.031 | Accuracy: 0.316406 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 029 | Total loss: 7.999 | Reg loss: 0.021 | Tree loss: 7.999 | Accuracy: 0.308594 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 029 | Total loss: 8.028 | Reg loss: 0.021 | Tree loss: 8.028 | Accuracy: 0.257812 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 029 | Total loss: 7.993 | Reg loss: 0.022 | Tree loss: 7.993 | Accuracy: 0.287109 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 029 | Total loss: 7.986 | Reg loss: 0.022 | Tree loss: 7.986 | Accuracy: 0.265625 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 029 | Total loss: 7.960 | Reg loss: 0.022 | Tree loss: 7.960 | Accuracy: 0.287109 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 029 | Total loss: 7.933 | Reg loss: 0.023 | Tree loss: 7.933 | Accuracy: 0.281250 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 029 | Total loss: 7.916 | Reg loss: 0.023 | Tree loss: 7.916 | Accuracy: 0.310547 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 029 | Total loss: 7.903 | Reg loss: 0.023 | Tree loss: 7.903 | Accuracy: 0.281250 | 0.243 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 019 / 029 | Total loss: 7.875 | Reg loss: 0.023 | Tree loss: 7.875 | Accuracy: 0.277344 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 029 | Total loss: 7.883 | Reg loss: 0.024 | Tree loss: 7.883 | Accuracy: 0.250000 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 029 | Total loss: 7.838 | Reg loss: 0.024 | Tree loss: 7.838 | Accuracy: 0.251953 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 029 | Total loss: 7.825 | Reg loss: 0.024 | Tree loss: 7.825 | Accuracy: 0.263672 | 0.243 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 029 | Total loss: 7.785 | Reg loss: 0.025 | Tree loss: 7.785 | Accuracy: 0.292969 | 0.242 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 029 | Total loss: 7.783 | Reg loss: 0.025 | Tree loss: 7.783 | Accuracy: 0.257812 | 0.242 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 029 | Total loss: 7.721 | Reg loss: 0.025 | Tree loss: 7.721 | Accuracy: 0.292969 | 0.242 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 029 | Total loss: 7.718 | Reg loss: 0.025 | Tree loss: 7.718 | Accuracy: 0.296875 | 0.242 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 029 | Total loss: 7.748 | Reg loss: 0.026 | Tree loss: 7.748 | Accuracy: 0.251953 | 0.242 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 029 | Total loss: 7.628 | Reg loss: 0.026 | Tree loss: 7.628 | Accuracy: 0.461538 | 0.242 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 09 | Batch: 000 / 029 | Total loss: 8.035 | Reg loss: 0.021 | Tree loss: 8.035 | Accuracy: 0.251953 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 029 | Total loss: 8.013 | Reg loss: 0.021 | Tree loss: 8.013 | Accuracy: 0.273438 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 029 | Total loss: 7.990 | Reg loss: 0.021 | Tree loss: 7.990 | Accuracy: 0.261719 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 029 | Total loss: 7.954 | Reg loss: 0.021 | Tree loss: 7.954 | Accuracy: 0.287109 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 029 | Total loss: 7.909 | Reg loss: 0.021 | Tree loss: 7.909 | Accuracy: 0.296875 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 029 | Total loss: 7.896 | Reg loss: 0.022 | Tree loss: 7.896 | Accuracy: 0.308594 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 029 | Total loss: 7.882 | Reg loss: 0.022 | Tree loss: 7.882 | Accuracy: 0.289062 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 029 | Total loss: 7.856 | Reg loss: 0.022 | Tree loss: 7.856 | Accuracy: 0.273438 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 029 | Total loss: 7.847 | Reg loss: 0.022 | Tree loss: 7.847 | Accuracy: 0.287109 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 029 | Total loss: 7.831 | Reg loss: 0.022 | Tree loss: 7.831 | Accuracy: 0.269531 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 029 | Total loss: 7.824 | Reg loss: 0.022 | Tree loss: 7.824 | Accuracy: 0.261719 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 029 | Total loss: 7.789 | Reg loss: 0.023 | Tree loss: 7.789 | Accuracy: 0.250000 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 029 | Total loss: 7.761 | Reg loss: 0.023 | Tree loss: 7.761 | Accuracy: 0.281250 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 029 | Total loss: 7.742 | Reg loss: 0.023 | Tree loss: 7.742 | Accuracy: 0.291016 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 029 | Total loss: 7.723 | Reg loss: 0.023 | Tree loss: 7.723 | Accuracy: 0.285156 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 029 | Total loss: 7.690 | Reg loss: 0.024 | Tree loss: 7.690 | Accuracy: 0.273438 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 029 | Total loss: 7.653 | Reg loss: 0.024 | Tree loss: 7.653 | Accuracy: 0.310547 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 029 | Total loss: 7.637 | Reg loss: 0.024 | Tree loss: 7.637 | Accuracy: 0.287109 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 029 | Total loss: 7.597 | Reg loss: 0.024 | Tree loss: 7.597 | Accuracy: 0.302734 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 029 | Total loss: 7.594 | Reg loss: 0.025 | Tree loss: 7.594 | Accuracy: 0.294922 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 029 | Total loss: 7.585 | Reg loss: 0.025 | Tree loss: 7.585 | Accuracy: 0.283203 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 029 | Total loss: 7.593 | Reg loss: 0.025 | Tree loss: 7.593 | Accuracy: 0.263672 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 029 | Total loss: 7.538 | Reg loss: 0.025 | Tree loss: 7.538 | Accuracy: 0.275391 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 029 | Total loss: 7.519 | Reg loss: 0.026 | Tree loss: 7.519 | Accuracy: 0.277344 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 029 | Total loss: 7.482 | Reg loss: 0.026 | Tree loss: 7.482 | Accuracy: 0.277344 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 029 | Total loss: 7.502 | Reg loss: 0.026 | Tree loss: 7.502 | Accuracy: 0.263672 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 029 | Total loss: 7.473 | Reg loss: 0.026 | Tree loss: 7.473 | Accuracy: 0.265625 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 029 | Total loss: 7.463 | Reg loss: 0.027 | Tree loss: 7.463 | Accuracy: 0.257812 | 0.243 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 029 | Total loss: 7.345 | Reg loss: 0.027 | Tree loss: 7.345 | Accuracy: 0.307692 | 0.244 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 10 | Batch: 000 / 029 | Total loss: 7.757 | Reg loss: 0.023 | Tree loss: 7.757 | Accuracy: 0.232422 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 029 | Total loss: 7.751 | Reg loss: 0.023 | Tree loss: 7.751 | Accuracy: 0.261719 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 029 | Total loss: 7.733 | Reg loss: 0.023 | Tree loss: 7.733 | Accuracy: 0.263672 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 029 | Total loss: 7.645 | Reg loss: 0.023 | Tree loss: 7.645 | Accuracy: 0.292969 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 029 | Total loss: 7.632 | Reg loss: 0.023 | Tree loss: 7.632 | Accuracy: 0.281250 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 029 | Total loss: 7.644 | Reg loss: 0.023 | Tree loss: 7.644 | Accuracy: 0.269531 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 029 | Total loss: 7.597 | Reg loss: 0.023 | Tree loss: 7.597 | Accuracy: 0.287109 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 029 | Total loss: 7.580 | Reg loss: 0.023 | Tree loss: 7.580 | Accuracy: 0.271484 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 029 | Total loss: 7.573 | Reg loss: 0.023 | Tree loss: 7.573 | Accuracy: 0.271484 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 029 | Total loss: 7.557 | Reg loss: 0.023 | Tree loss: 7.557 | Accuracy: 0.250000 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 029 | Total loss: 7.530 | Reg loss: 0.024 | Tree loss: 7.530 | Accuracy: 0.279297 | 0.245 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 029 | Total loss: 7.514 | Reg loss: 0.024 | Tree loss: 7.514 | Accuracy: 0.257812 | 0.246 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 029 | Total loss: 7.482 | Reg loss: 0.024 | Tree loss: 7.482 | Accuracy: 0.269531 | 0.246 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 029 | Total loss: 7.454 | Reg loss: 0.024 | Tree loss: 7.454 | Accuracy: 0.281250 | 0.246 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 029 | Total loss: 7.443 | Reg loss: 0.024 | Tree loss: 7.443 | Accuracy: 0.269531 | 0.246 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 029 | Total loss: 7.393 | Reg loss: 0.025 | Tree loss: 7.393 | Accuracy: 0.291016 | 0.246 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 029 | Total loss: 7.380 | Reg loss: 0.025 | Tree loss: 7.380 | Accuracy: 0.310547 | 0.246 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 029 | Total loss: 7.352 | Reg loss: 0.025 | Tree loss: 7.352 | Accuracy: 0.306641 | 0.246 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 029 | Total loss: 7.331 | Reg loss: 0.025 | Tree loss: 7.331 | Accuracy: 0.281250 | 0.247 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 029 | Total loss: 7.309 | Reg loss: 0.026 | Tree loss: 7.309 | Accuracy: 0.291016 | 0.248 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 029 | Total loss: 7.315 | Reg loss: 0.026 | Tree loss: 7.315 | Accuracy: 0.271484 | 0.249 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 029 | Total loss: 7.276 | Reg loss: 0.026 | Tree loss: 7.276 | Accuracy: 0.291016 | 0.249 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 029 | Total loss: 7.244 | Reg loss: 0.026 | Tree loss: 7.244 | Accuracy: 0.289062 | 0.249 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 029 | Total loss: 7.225 | Reg loss: 0.027 | Tree loss: 7.225 | Accuracy: 0.294922 | 0.25 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 024 / 029 | Total loss: 7.196 | Reg loss: 0.027 | Tree loss: 7.196 | Accuracy: 0.304688 | 0.25 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 029 | Total loss: 7.207 | Reg loss: 0.027 | Tree loss: 7.207 | Accuracy: 0.265625 | 0.25 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 029 | Total loss: 7.158 | Reg loss: 0.027 | Tree loss: 7.158 | Accuracy: 0.287109 | 0.251 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 029 | Total loss: 7.162 | Reg loss: 0.028 | Tree loss: 7.162 | Accuracy: 0.279297 | 0.251 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 029 | Total loss: 7.358 | Reg loss: 0.028 | Tree loss: 7.358 | Accuracy: 0.230769 | 0.251 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 11 | Batch: 000 / 029 | Total loss: 7.446 | Reg loss: 0.024 | Tree loss: 7.446 | Accuracy: 0.298828 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 029 | Total loss: 7.433 | Reg loss: 0.024 | Tree loss: 7.433 | Accuracy: 0.277344 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 029 | Total loss: 7.425 | Reg loss: 0.024 | Tree loss: 7.425 | Accuracy: 0.287109 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 029 | Total loss: 7.364 | Reg loss: 0.024 | Tree loss: 7.364 | Accuracy: 0.281250 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 029 | Total loss: 7.355 | Reg loss: 0.024 | Tree loss: 7.355 | Accuracy: 0.277344 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 029 | Total loss: 7.326 | Reg loss: 0.024 | Tree loss: 7.326 | Accuracy: 0.275391 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 029 | Total loss: 7.301 | Reg loss: 0.024 | Tree loss: 7.301 | Accuracy: 0.304688 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 029 | Total loss: 7.296 | Reg loss: 0.024 | Tree loss: 7.296 | Accuracy: 0.287109 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 029 | Total loss: 7.268 | Reg loss: 0.024 | Tree loss: 7.268 | Accuracy: 0.263672 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 029 | Total loss: 7.205 | Reg loss: 0.024 | Tree loss: 7.205 | Accuracy: 0.292969 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 029 | Total loss: 7.216 | Reg loss: 0.025 | Tree loss: 7.216 | Accuracy: 0.312500 | 0.251 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 029 | Total loss: 7.208 | Reg loss: 0.025 | Tree loss: 7.208 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 029 | Total loss: 7.188 | Reg loss: 0.025 | Tree loss: 7.188 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 029 | Total loss: 7.186 | Reg loss: 0.025 | Tree loss: 7.186 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 029 | Total loss: 7.136 | Reg loss: 0.025 | Tree loss: 7.136 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 029 | Total loss: 7.129 | Reg loss: 0.026 | Tree loss: 7.129 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 029 | Total loss: 7.087 | Reg loss: 0.026 | Tree loss: 7.087 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 029 | Total loss: 7.070 | Reg loss: 0.026 | Tree loss: 7.070 | Accuracy: 0.306641 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 029 | Total loss: 7.035 | Reg loss: 0.026 | Tree loss: 7.035 | Accuracy: 0.308594 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 029 | Total loss: 7.067 | Reg loss: 0.026 | Tree loss: 7.067 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 029 | Total loss: 6.995 | Reg loss: 0.027 | Tree loss: 6.995 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 029 | Total loss: 6.980 | Reg loss: 0.027 | Tree loss: 6.980 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 029 | Total loss: 6.967 | Reg loss: 0.027 | Tree loss: 6.967 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 029 | Total loss: 6.983 | Reg loss: 0.027 | Tree loss: 6.983 | Accuracy: 0.238281 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 029 | Total loss: 6.927 | Reg loss: 0.027 | Tree loss: 6.927 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 029 | Total loss: 6.897 | Reg loss: 0.028 | Tree loss: 6.897 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 029 | Total loss: 6.866 | Reg loss: 0.028 | Tree loss: 6.866 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 029 | Total loss: 6.893 | Reg loss: 0.028 | Tree loss: 6.893 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 029 | Total loss: 6.943 | Reg loss: 0.028 | Tree loss: 6.943 | Accuracy: 0.153846 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 12 | Batch: 000 / 029 | Total loss: 7.144 | Reg loss: 0.025 | Tree loss: 7.144 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 029 | Total loss: 7.166 | Reg loss: 0.025 | Tree loss: 7.166 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 029 | Total loss: 7.124 | Reg loss: 0.025 | Tree loss: 7.124 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 029 | Total loss: 7.095 | Reg loss: 0.025 | Tree loss: 7.095 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 029 | Total loss: 7.070 | Reg loss: 0.025 | Tree loss: 7.070 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 029 | Total loss: 7.041 | Reg loss: 0.025 | Tree loss: 7.041 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 029 | Total loss: 7.021 | Reg loss: 0.025 | Tree loss: 7.021 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 029 | Total loss: 6.988 | Reg loss: 0.025 | Tree loss: 6.988 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 029 | Total loss: 6.960 | Reg loss: 0.025 | Tree loss: 6.960 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 029 | Total loss: 6.984 | Reg loss: 0.025 | Tree loss: 6.984 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 029 | Total loss: 6.920 | Reg loss: 0.025 | Tree loss: 6.920 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 029 | Total loss: 6.897 | Reg loss: 0.026 | Tree loss: 6.897 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 029 | Total loss: 6.904 | Reg loss: 0.026 | Tree loss: 6.904 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 029 | Total loss: 6.878 | Reg loss: 0.026 | Tree loss: 6.878 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 029 | Total loss: 6.810 | Reg loss: 0.026 | Tree loss: 6.810 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 029 | Total loss: 6.850 | Reg loss: 0.026 | Tree loss: 6.850 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 029 | Total loss: 6.790 | Reg loss: 0.026 | Tree loss: 6.790 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 029 | Total loss: 6.760 | Reg loss: 0.027 | Tree loss: 6.760 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 029 | Total loss: 6.761 | Reg loss: 0.027 | Tree loss: 6.761 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 029 | Total loss: 6.704 | Reg loss: 0.027 | Tree loss: 6.704 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 029 | Total loss: 6.687 | Reg loss: 0.027 | Tree loss: 6.687 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 029 | Total loss: 6.720 | Reg loss: 0.027 | Tree loss: 6.720 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 029 | Total loss: 6.665 | Reg loss: 0.027 | Tree loss: 6.665 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 029 | Total loss: 6.688 | Reg loss: 0.028 | Tree loss: 6.688 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 029 | Total loss: 6.660 | Reg loss: 0.028 | Tree loss: 6.660 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 029 | Total loss: 6.588 | Reg loss: 0.028 | Tree loss: 6.588 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 029 | Total loss: 6.612 | Reg loss: 0.028 | Tree loss: 6.612 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 029 | Total loss: 6.563 | Reg loss: 0.028 | Tree loss: 6.563 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 029 | Total loss: 6.700 | Reg loss: 0.028 | Tree loss: 6.700 | Accuracy: 0.153846 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 000 / 029 | Total loss: 6.866 | Reg loss: 0.025 | Tree loss: 6.866 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 029 | Total loss: 6.854 | Reg loss: 0.025 | Tree loss: 6.854 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 029 | Total loss: 6.848 | Reg loss: 0.025 | Tree loss: 6.848 | Accuracy: 0.246094 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 029 | Total loss: 6.777 | Reg loss: 0.025 | Tree loss: 6.777 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 029 | Total loss: 6.775 | Reg loss: 0.026 | Tree loss: 6.775 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 029 | Total loss: 6.733 | Reg loss: 0.026 | Tree loss: 6.733 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 029 | Total loss: 6.725 | Reg loss: 0.026 | Tree loss: 6.725 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 029 | Total loss: 6.737 | Reg loss: 0.026 | Tree loss: 6.737 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 029 | Total loss: 6.714 | Reg loss: 0.026 | Tree loss: 6.714 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 029 | Total loss: 6.663 | Reg loss: 0.026 | Tree loss: 6.663 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 029 | Total loss: 6.672 | Reg loss: 0.026 | Tree loss: 6.672 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 029 | Total loss: 6.628 | Reg loss: 0.026 | Tree loss: 6.628 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 029 | Total loss: 6.621 | Reg loss: 0.026 | Tree loss: 6.621 | Accuracy: 0.242188 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 029 | Total loss: 6.548 | Reg loss: 0.026 | Tree loss: 6.548 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 029 | Total loss: 6.551 | Reg loss: 0.026 | Tree loss: 6.551 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 029 | Total loss: 6.511 | Reg loss: 0.027 | Tree loss: 6.511 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 029 | Total loss: 6.500 | Reg loss: 0.027 | Tree loss: 6.500 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 029 | Total loss: 6.490 | Reg loss: 0.027 | Tree loss: 6.490 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 029 | Total loss: 6.448 | Reg loss: 0.027 | Tree loss: 6.448 | Accuracy: 0.324219 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 029 | Total loss: 6.452 | Reg loss: 0.027 | Tree loss: 6.452 | Accuracy: 0.310547 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 029 | Total loss: 6.434 | Reg loss: 0.027 | Tree loss: 6.434 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 029 | Total loss: 6.414 | Reg loss: 0.027 | Tree loss: 6.414 | Accuracy: 0.312500 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 029 | Total loss: 6.377 | Reg loss: 0.028 | Tree loss: 6.377 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 029 | Total loss: 6.366 | Reg loss: 0.028 | Tree loss: 6.366 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 029 | Total loss: 6.376 | Reg loss: 0.028 | Tree loss: 6.376 | Accuracy: 0.242188 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 029 | Total loss: 6.316 | Reg loss: 0.028 | Tree loss: 6.316 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 029 | Total loss: 6.293 | Reg loss: 0.028 | Tree loss: 6.293 | Accuracy: 0.302734 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 029 | Total loss: 6.341 | Reg loss: 0.028 | Tree loss: 6.341 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 029 | Total loss: 6.320 | Reg loss: 0.028 | Tree loss: 6.320 | Accuracy: 0.384615 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 14 | Batch: 000 / 029 | Total loss: 6.564 | Reg loss: 0.026 | Tree loss: 6.564 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 029 | Total loss: 6.548 | Reg loss: 0.026 | Tree loss: 6.548 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 029 | Total loss: 6.503 | Reg loss: 0.026 | Tree loss: 6.503 | Accuracy: 0.320312 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 029 | Total loss: 6.509 | Reg loss: 0.026 | Tree loss: 6.509 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 029 | Total loss: 6.475 | Reg loss: 0.026 | Tree loss: 6.475 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 029 | Total loss: 6.486 | Reg loss: 0.026 | Tree loss: 6.486 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 029 | Total loss: 6.464 | Reg loss: 0.026 | Tree loss: 6.464 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 029 | Total loss: 6.427 | Reg loss: 0.026 | Tree loss: 6.427 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 029 | Total loss: 6.439 | Reg loss: 0.026 | Tree loss: 6.439 | Accuracy: 0.248047 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 029 | Total loss: 6.394 | Reg loss: 0.026 | Tree loss: 6.394 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 029 | Total loss: 6.337 | Reg loss: 0.026 | Tree loss: 6.337 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 029 | Total loss: 6.327 | Reg loss: 0.026 | Tree loss: 6.327 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 029 | Total loss: 6.323 | Reg loss: 0.027 | Tree loss: 6.323 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 029 | Total loss: 6.322 | Reg loss: 0.027 | Tree loss: 6.322 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 029 | Total loss: 6.293 | Reg loss: 0.027 | Tree loss: 6.293 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 029 | Total loss: 6.262 | Reg loss: 0.027 | Tree loss: 6.262 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 029 | Total loss: 6.257 | Reg loss: 0.027 | Tree loss: 6.257 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 029 | Total loss: 6.196 | Reg loss: 0.027 | Tree loss: 6.196 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 029 | Total loss: 6.201 | Reg loss: 0.027 | Tree loss: 6.201 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 029 | Total loss: 6.171 | Reg loss: 0.027 | Tree loss: 6.171 | Accuracy: 0.291016 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 029 | Total loss: 6.180 | Reg loss: 0.027 | Tree loss: 6.180 | Accuracy: 0.259766 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 029 | Total loss: 6.131 | Reg loss: 0.028 | Tree loss: 6.131 | Accuracy: 0.302734 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 029 | Total loss: 6.123 | Reg loss: 0.028 | Tree loss: 6.123 | Accuracy: 0.285156 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 029 | Total loss: 6.117 | Reg loss: 0.028 | Tree loss: 6.117 | Accuracy: 0.259766 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 029 | Total loss: 6.093 | Reg loss: 0.028 | Tree loss: 6.093 | Accuracy: 0.267578 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 029 | Total loss: 6.042 | Reg loss: 0.028 | Tree loss: 6.042 | Accuracy: 0.275391 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 029 | Total loss: 6.053 | Reg loss: 0.028 | Tree loss: 6.053 | Accuracy: 0.292969 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 029 | Total loss: 6.028 | Reg loss: 0.028 | Tree loss: 6.028 | Accuracy: 0.267578 | 0.255 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 029 | Total loss: 5.879 | Reg loss: 0.028 | Tree loss: 5.879 | Accuracy: 0.461538 | 0.255 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 15 | Batch: 000 / 029 | Total loss: 6.274 | Reg loss: 0.026 | Tree loss: 6.274 | Accuracy: 0.277344 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 029 | Total loss: 6.258 | Reg loss: 0.026 | Tree loss: 6.258 | Accuracy: 0.265625 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 029 | Total loss: 6.260 | Reg loss: 0.026 | Tree loss: 6.260 | Accuracy: 0.265625 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 029 | Total loss: 6.265 | Reg loss: 0.026 | Tree loss: 6.265 | Accuracy: 0.240234 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 029 | Total loss: 6.191 | Reg loss: 0.026 | Tree loss: 6.191 | Accuracy: 0.263672 | 0.255 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 005 / 029 | Total loss: 6.191 | Reg loss: 0.026 | Tree loss: 6.191 | Accuracy: 0.291016 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 029 | Total loss: 6.181 | Reg loss: 0.026 | Tree loss: 6.181 | Accuracy: 0.269531 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 029 | Total loss: 6.135 | Reg loss: 0.026 | Tree loss: 6.135 | Accuracy: 0.277344 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 029 | Total loss: 6.109 | Reg loss: 0.026 | Tree loss: 6.109 | Accuracy: 0.310547 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 029 | Total loss: 6.097 | Reg loss: 0.026 | Tree loss: 6.097 | Accuracy: 0.283203 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 029 | Total loss: 6.081 | Reg loss: 0.027 | Tree loss: 6.081 | Accuracy: 0.291016 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 029 | Total loss: 6.075 | Reg loss: 0.027 | Tree loss: 6.075 | Accuracy: 0.287109 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 029 | Total loss: 6.054 | Reg loss: 0.027 | Tree loss: 6.054 | Accuracy: 0.255859 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 029 | Total loss: 6.060 | Reg loss: 0.027 | Tree loss: 6.060 | Accuracy: 0.248047 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 029 | Total loss: 6.026 | Reg loss: 0.027 | Tree loss: 6.026 | Accuracy: 0.277344 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 029 | Total loss: 5.977 | Reg loss: 0.027 | Tree loss: 5.977 | Accuracy: 0.277344 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 029 | Total loss: 5.984 | Reg loss: 0.027 | Tree loss: 5.984 | Accuracy: 0.251953 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 029 | Total loss: 5.930 | Reg loss: 0.027 | Tree loss: 5.930 | Accuracy: 0.291016 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 029 | Total loss: 5.951 | Reg loss: 0.027 | Tree loss: 5.951 | Accuracy: 0.232422 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 029 | Total loss: 5.939 | Reg loss: 0.027 | Tree loss: 5.939 | Accuracy: 0.269531 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 029 | Total loss: 5.871 | Reg loss: 0.027 | Tree loss: 5.871 | Accuracy: 0.275391 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 029 | Total loss: 5.900 | Reg loss: 0.027 | Tree loss: 5.900 | Accuracy: 0.242188 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 029 | Total loss: 5.845 | Reg loss: 0.028 | Tree loss: 5.845 | Accuracy: 0.289062 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 029 | Total loss: 5.900 | Reg loss: 0.028 | Tree loss: 5.900 | Accuracy: 0.250000 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 029 | Total loss: 5.843 | Reg loss: 0.028 | Tree loss: 5.843 | Accuracy: 0.271484 | 0.255 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 029 | Total loss: 5.785 | Reg loss: 0.028 | Tree loss: 5.785 | Accuracy: 0.263672 | 0.256 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 029 | Total loss: 5.792 | Reg loss: 0.028 | Tree loss: 5.792 | Accuracy: 0.275391 | 0.256 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 029 | Total loss: 5.795 | Reg loss: 0.028 | Tree loss: 5.795 | Accuracy: 0.257812 | 0.256 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 029 | Total loss: 5.835 | Reg loss: 0.028 | Tree loss: 5.835 | Accuracy: 0.153846 | 0.256 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 16 | Batch: 000 / 029 | Total loss: 6.050 | Reg loss: 0.026 | Tree loss: 6.050 | Accuracy: 0.246094 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 029 | Total loss: 6.005 | Reg loss: 0.026 | Tree loss: 6.005 | Accuracy: 0.283203 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 029 | Total loss: 5.988 | Reg loss: 0.026 | Tree loss: 5.988 | Accuracy: 0.289062 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 029 | Total loss: 6.006 | Reg loss: 0.026 | Tree loss: 6.006 | Accuracy: 0.275391 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 029 | Total loss: 5.945 | Reg loss: 0.026 | Tree loss: 5.945 | Accuracy: 0.279297 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 029 | Total loss: 5.941 | Reg loss: 0.026 | Tree loss: 5.941 | Accuracy: 0.281250 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 029 | Total loss: 5.936 | Reg loss: 0.026 | Tree loss: 5.936 | Accuracy: 0.238281 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 029 | Total loss: 5.892 | Reg loss: 0.026 | Tree loss: 5.892 | Accuracy: 0.275391 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 029 | Total loss: 5.862 | Reg loss: 0.026 | Tree loss: 5.862 | Accuracy: 0.267578 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 029 | Total loss: 5.849 | Reg loss: 0.027 | Tree loss: 5.849 | Accuracy: 0.253906 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 029 | Total loss: 5.804 | Reg loss: 0.027 | Tree loss: 5.804 | Accuracy: 0.255859 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 029 | Total loss: 5.823 | Reg loss: 0.027 | Tree loss: 5.823 | Accuracy: 0.259766 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 029 | Total loss: 5.827 | Reg loss: 0.027 | Tree loss: 5.827 | Accuracy: 0.224609 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 029 | Total loss: 5.769 | Reg loss: 0.027 | Tree loss: 5.769 | Accuracy: 0.253906 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 029 | Total loss: 5.746 | Reg loss: 0.027 | Tree loss: 5.746 | Accuracy: 0.234375 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 029 | Total loss: 5.724 | Reg loss: 0.027 | Tree loss: 5.724 | Accuracy: 0.281250 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 029 | Total loss: 5.725 | Reg loss: 0.027 | Tree loss: 5.725 | Accuracy: 0.273438 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 029 | Total loss: 5.709 | Reg loss: 0.027 | Tree loss: 5.709 | Accuracy: 0.236328 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 029 | Total loss: 5.676 | Reg loss: 0.027 | Tree loss: 5.676 | Accuracy: 0.255859 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 029 | Total loss: 5.668 | Reg loss: 0.027 | Tree loss: 5.668 | Accuracy: 0.265625 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 029 | Total loss: 5.612 | Reg loss: 0.027 | Tree loss: 5.612 | Accuracy: 0.279297 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 029 | Total loss: 5.597 | Reg loss: 0.027 | Tree loss: 5.597 | Accuracy: 0.240234 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 029 | Total loss: 5.614 | Reg loss: 0.027 | Tree loss: 5.614 | Accuracy: 0.279297 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 029 | Total loss: 5.554 | Reg loss: 0.027 | Tree loss: 5.554 | Accuracy: 0.253906 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 029 | Total loss: 5.574 | Reg loss: 0.028 | Tree loss: 5.574 | Accuracy: 0.287109 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 029 | Total loss: 5.548 | Reg loss: 0.028 | Tree loss: 5.548 | Accuracy: 0.244141 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 029 | Total loss: 5.571 | Reg loss: 0.028 | Tree loss: 5.571 | Accuracy: 0.250000 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 029 | Total loss: 5.524 | Reg loss: 0.028 | Tree loss: 5.524 | Accuracy: 0.259766 | 0.256 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 029 | Total loss: 5.365 | Reg loss: 0.028 | Tree loss: 5.365 | Accuracy: 0.307692 | 0.256 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 17 | Batch: 000 / 029 | Total loss: 5.749 | Reg loss: 0.026 | Tree loss: 5.749 | Accuracy: 0.250000 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 029 | Total loss: 5.757 | Reg loss: 0.026 | Tree loss: 5.757 | Accuracy: 0.250000 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 029 | Total loss: 5.741 | Reg loss: 0.026 | Tree loss: 5.741 | Accuracy: 0.248047 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 029 | Total loss: 5.695 | Reg loss: 0.026 | Tree loss: 5.695 | Accuracy: 0.257812 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 029 | Total loss: 5.692 | Reg loss: 0.026 | Tree loss: 5.692 | Accuracy: 0.271484 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 029 | Total loss: 5.657 | Reg loss: 0.026 | Tree loss: 5.657 | Accuracy: 0.273438 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 029 | Total loss: 5.644 | Reg loss: 0.026 | Tree loss: 5.644 | Accuracy: 0.275391 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 029 | Total loss: 5.617 | Reg loss: 0.026 | Tree loss: 5.617 | Accuracy: 0.267578 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 029 | Total loss: 5.624 | Reg loss: 0.026 | Tree loss: 5.624 | Accuracy: 0.271484 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 029 | Total loss: 5.610 | Reg loss: 0.026 | Tree loss: 5.610 | Accuracy: 0.257812 | 0.256 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 010 / 029 | Total loss: 5.596 | Reg loss: 0.026 | Tree loss: 5.596 | Accuracy: 0.244141 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 029 | Total loss: 5.587 | Reg loss: 0.027 | Tree loss: 5.587 | Accuracy: 0.248047 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 029 | Total loss: 5.539 | Reg loss: 0.027 | Tree loss: 5.539 | Accuracy: 0.240234 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 029 | Total loss: 5.539 | Reg loss: 0.027 | Tree loss: 5.539 | Accuracy: 0.220703 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 029 | Total loss: 5.485 | Reg loss: 0.027 | Tree loss: 5.485 | Accuracy: 0.236328 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 029 | Total loss: 5.473 | Reg loss: 0.027 | Tree loss: 5.473 | Accuracy: 0.257812 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 029 | Total loss: 5.449 | Reg loss: 0.027 | Tree loss: 5.449 | Accuracy: 0.220703 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 029 | Total loss: 5.442 | Reg loss: 0.027 | Tree loss: 5.442 | Accuracy: 0.246094 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 029 | Total loss: 5.426 | Reg loss: 0.027 | Tree loss: 5.426 | Accuracy: 0.199219 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 029 | Total loss: 5.409 | Reg loss: 0.027 | Tree loss: 5.409 | Accuracy: 0.253906 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 029 | Total loss: 5.418 | Reg loss: 0.027 | Tree loss: 5.418 | Accuracy: 0.263672 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 029 | Total loss: 5.374 | Reg loss: 0.027 | Tree loss: 5.374 | Accuracy: 0.234375 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 029 | Total loss: 5.388 | Reg loss: 0.027 | Tree loss: 5.388 | Accuracy: 0.195312 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 029 | Total loss: 5.368 | Reg loss: 0.027 | Tree loss: 5.368 | Accuracy: 0.187500 | 0.256 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 029 | Total loss: 5.349 | Reg loss: 0.027 | Tree loss: 5.349 | Accuracy: 0.191406 | 0.257 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 029 | Total loss: 5.331 | Reg loss: 0.027 | Tree loss: 5.331 | Accuracy: 0.195312 | 0.257 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 029 | Total loss: 5.286 | Reg loss: 0.027 | Tree loss: 5.286 | Accuracy: 0.197266 | 0.257 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 029 | Total loss: 5.304 | Reg loss: 0.028 | Tree loss: 5.304 | Accuracy: 0.193359 | 0.257 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 029 | Total loss: 5.416 | Reg loss: 0.028 | Tree loss: 5.416 | Accuracy: 0.000000 | 0.257 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 18 | Batch: 000 / 029 | Total loss: 5.528 | Reg loss: 0.026 | Tree loss: 5.528 | Accuracy: 0.261719 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 029 | Total loss: 5.510 | Reg loss: 0.026 | Tree loss: 5.510 | Accuracy: 0.228516 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 029 | Total loss: 5.467 | Reg loss: 0.026 | Tree loss: 5.467 | Accuracy: 0.273438 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 029 | Total loss: 5.472 | Reg loss: 0.026 | Tree loss: 5.472 | Accuracy: 0.242188 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 029 | Total loss: 5.446 | Reg loss: 0.026 | Tree loss: 5.446 | Accuracy: 0.273438 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 029 | Total loss: 5.428 | Reg loss: 0.026 | Tree loss: 5.428 | Accuracy: 0.230469 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 029 | Total loss: 5.427 | Reg loss: 0.026 | Tree loss: 5.427 | Accuracy: 0.228516 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 029 | Total loss: 5.372 | Reg loss: 0.026 | Tree loss: 5.372 | Accuracy: 0.234375 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 029 | Total loss: 5.367 | Reg loss: 0.026 | Tree loss: 5.367 | Accuracy: 0.234375 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 029 | Total loss: 5.343 | Reg loss: 0.026 | Tree loss: 5.343 | Accuracy: 0.218750 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 029 | Total loss: 5.294 | Reg loss: 0.026 | Tree loss: 5.294 | Accuracy: 0.251953 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 029 | Total loss: 5.337 | Reg loss: 0.026 | Tree loss: 5.337 | Accuracy: 0.230469 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 029 | Total loss: 5.335 | Reg loss: 0.026 | Tree loss: 5.335 | Accuracy: 0.193359 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 029 | Total loss: 5.249 | Reg loss: 0.026 | Tree loss: 5.249 | Accuracy: 0.248047 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 029 | Total loss: 5.299 | Reg loss: 0.026 | Tree loss: 5.299 | Accuracy: 0.212891 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 029 | Total loss: 5.209 | Reg loss: 0.026 | Tree loss: 5.209 | Accuracy: 0.236328 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 029 | Total loss: 5.217 | Reg loss: 0.027 | Tree loss: 5.217 | Accuracy: 0.208984 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 029 | Total loss: 5.200 | Reg loss: 0.027 | Tree loss: 5.200 | Accuracy: 0.187500 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 029 | Total loss: 5.195 | Reg loss: 0.027 | Tree loss: 5.195 | Accuracy: 0.181641 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 029 | Total loss: 5.193 | Reg loss: 0.027 | Tree loss: 5.193 | Accuracy: 0.158203 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 029 | Total loss: 5.150 | Reg loss: 0.027 | Tree loss: 5.150 | Accuracy: 0.164062 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 029 | Total loss: 5.177 | Reg loss: 0.027 | Tree loss: 5.177 | Accuracy: 0.152344 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 029 | Total loss: 5.133 | Reg loss: 0.027 | Tree loss: 5.133 | Accuracy: 0.144531 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 029 | Total loss: 5.119 | Reg loss: 0.027 | Tree loss: 5.119 | Accuracy: 0.166016 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 029 | Total loss: 5.108 | Reg loss: 0.027 | Tree loss: 5.108 | Accuracy: 0.160156 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 029 | Total loss: 5.084 | Reg loss: 0.027 | Tree loss: 5.084 | Accuracy: 0.160156 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 029 | Total loss: 5.050 | Reg loss: 0.027 | Tree loss: 5.050 | Accuracy: 0.173828 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 029 | Total loss: 5.061 | Reg loss: 0.027 | Tree loss: 5.061 | Accuracy: 0.140625 | 0.257 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 029 | Total loss: 4.949 | Reg loss: 0.027 | Tree loss: 4.949 | Accuracy: 0.384615 | 0.257 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 19 | Batch: 000 / 029 | Total loss: 5.275 | Reg loss: 0.026 | Tree loss: 5.275 | Accuracy: 0.257812 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 029 | Total loss: 5.241 | Reg loss: 0.026 | Tree loss: 5.241 | Accuracy: 0.226562 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 029 | Total loss: 5.265 | Reg loss: 0.026 | Tree loss: 5.265 | Accuracy: 0.216797 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 029 | Total loss: 5.218 | Reg loss: 0.026 | Tree loss: 5.218 | Accuracy: 0.222656 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 029 | Total loss: 5.169 | Reg loss: 0.026 | Tree loss: 5.169 | Accuracy: 0.234375 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 029 | Total loss: 5.187 | Reg loss: 0.026 | Tree loss: 5.187 | Accuracy: 0.214844 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 029 | Total loss: 5.200 | Reg loss: 0.026 | Tree loss: 5.200 | Accuracy: 0.224609 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 029 | Total loss: 5.123 | Reg loss: 0.026 | Tree loss: 5.123 | Accuracy: 0.253906 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 029 | Total loss: 5.129 | Reg loss: 0.026 | Tree loss: 5.129 | Accuracy: 0.203125 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 029 | Total loss: 5.119 | Reg loss: 0.026 | Tree loss: 5.119 | Accuracy: 0.205078 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 029 | Total loss: 5.094 | Reg loss: 0.026 | Tree loss: 5.094 | Accuracy: 0.230469 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 029 | Total loss: 5.079 | Reg loss: 0.026 | Tree loss: 5.079 | Accuracy: 0.228516 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 029 | Total loss: 5.065 | Reg loss: 0.026 | Tree loss: 5.065 | Accuracy: 0.214844 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 029 | Total loss: 5.063 | Reg loss: 0.026 | Tree loss: 5.063 | Accuracy: 0.214844 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 029 | Total loss: 5.053 | Reg loss: 0.026 | Tree loss: 5.053 | Accuracy: 0.173828 | 0.257 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 015 / 029 | Total loss: 5.012 | Reg loss: 0.026 | Tree loss: 5.012 | Accuracy: 0.181641 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 029 | Total loss: 4.997 | Reg loss: 0.026 | Tree loss: 4.997 | Accuracy: 0.144531 | 0.257 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 029 | Total loss: 4.967 | Reg loss: 0.026 | Tree loss: 4.967 | Accuracy: 0.160156 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 029 | Total loss: 4.945 | Reg loss: 0.026 | Tree loss: 4.945 | Accuracy: 0.160156 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 029 | Total loss: 4.933 | Reg loss: 0.026 | Tree loss: 4.933 | Accuracy: 0.150391 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 029 | Total loss: 4.940 | Reg loss: 0.026 | Tree loss: 4.940 | Accuracy: 0.166016 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 029 | Total loss: 4.904 | Reg loss: 0.027 | Tree loss: 4.904 | Accuracy: 0.160156 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 029 | Total loss: 4.876 | Reg loss: 0.027 | Tree loss: 4.876 | Accuracy: 0.166016 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 029 | Total loss: 4.939 | Reg loss: 0.027 | Tree loss: 4.939 | Accuracy: 0.150391 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 029 | Total loss: 4.850 | Reg loss: 0.027 | Tree loss: 4.850 | Accuracy: 0.148438 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 029 | Total loss: 4.862 | Reg loss: 0.027 | Tree loss: 4.862 | Accuracy: 0.164062 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 029 | Total loss: 4.831 | Reg loss: 0.027 | Tree loss: 4.831 | Accuracy: 0.156250 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 029 | Total loss: 4.826 | Reg loss: 0.027 | Tree loss: 4.826 | Accuracy: 0.162109 | 0.258 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 029 | Total loss: 4.721 | Reg loss: 0.027 | Tree loss: 4.721 | Accuracy: 0.153846 | 0.258 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 20 | Batch: 000 / 029 | Total loss: 5.046 | Reg loss: 0.025 | Tree loss: 5.046 | Accuracy: 0.300781 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 029 | Total loss: 5.019 | Reg loss: 0.025 | Tree loss: 5.019 | Accuracy: 0.232422 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 029 | Total loss: 5.023 | Reg loss: 0.025 | Tree loss: 5.023 | Accuracy: 0.210938 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 029 | Total loss: 4.974 | Reg loss: 0.025 | Tree loss: 4.974 | Accuracy: 0.226562 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 029 | Total loss: 4.955 | Reg loss: 0.025 | Tree loss: 4.955 | Accuracy: 0.226562 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 029 | Total loss: 4.936 | Reg loss: 0.025 | Tree loss: 4.936 | Accuracy: 0.248047 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 029 | Total loss: 4.947 | Reg loss: 0.025 | Tree loss: 4.947 | Accuracy: 0.224609 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 029 | Total loss: 4.918 | Reg loss: 0.025 | Tree loss: 4.918 | Accuracy: 0.208984 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 029 | Total loss: 4.959 | Reg loss: 0.025 | Tree loss: 4.959 | Accuracy: 0.197266 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 029 | Total loss: 4.863 | Reg loss: 0.025 | Tree loss: 4.863 | Accuracy: 0.238281 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 029 | Total loss: 4.868 | Reg loss: 0.026 | Tree loss: 4.868 | Accuracy: 0.207031 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 029 | Total loss: 4.845 | Reg loss: 0.026 | Tree loss: 4.845 | Accuracy: 0.195312 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 029 | Total loss: 4.831 | Reg loss: 0.026 | Tree loss: 4.831 | Accuracy: 0.175781 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 029 | Total loss: 4.811 | Reg loss: 0.026 | Tree loss: 4.811 | Accuracy: 0.136719 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 029 | Total loss: 4.793 | Reg loss: 0.026 | Tree loss: 4.793 | Accuracy: 0.130859 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 029 | Total loss: 4.741 | Reg loss: 0.026 | Tree loss: 4.741 | Accuracy: 0.173828 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 029 | Total loss: 4.796 | Reg loss: 0.026 | Tree loss: 4.796 | Accuracy: 0.144531 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 029 | Total loss: 4.712 | Reg loss: 0.026 | Tree loss: 4.712 | Accuracy: 0.140625 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 029 | Total loss: 4.693 | Reg loss: 0.026 | Tree loss: 4.693 | Accuracy: 0.175781 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 029 | Total loss: 4.756 | Reg loss: 0.026 | Tree loss: 4.756 | Accuracy: 0.132812 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 029 | Total loss: 4.715 | Reg loss: 0.026 | Tree loss: 4.715 | Accuracy: 0.158203 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 029 | Total loss: 4.710 | Reg loss: 0.026 | Tree loss: 4.710 | Accuracy: 0.130859 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 029 | Total loss: 4.680 | Reg loss: 0.026 | Tree loss: 4.680 | Accuracy: 0.134766 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 029 | Total loss: 4.654 | Reg loss: 0.026 | Tree loss: 4.654 | Accuracy: 0.132812 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 029 | Total loss: 4.639 | Reg loss: 0.027 | Tree loss: 4.639 | Accuracy: 0.164062 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 029 | Total loss: 4.659 | Reg loss: 0.027 | Tree loss: 4.659 | Accuracy: 0.136719 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 029 | Total loss: 4.620 | Reg loss: 0.027 | Tree loss: 4.620 | Accuracy: 0.156250 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 029 | Total loss: 4.582 | Reg loss: 0.027 | Tree loss: 4.582 | Accuracy: 0.162109 | 0.258 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 029 | Total loss: 4.538 | Reg loss: 0.027 | Tree loss: 4.538 | Accuracy: 0.000000 | 0.258 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 21 | Batch: 000 / 029 | Total loss: 4.787 | Reg loss: 0.025 | Tree loss: 4.787 | Accuracy: 0.148438 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 029 | Total loss: 4.782 | Reg loss: 0.025 | Tree loss: 4.782 | Accuracy: 0.134766 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 029 | Total loss: 4.741 | Reg loss: 0.025 | Tree loss: 4.741 | Accuracy: 0.146484 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 029 | Total loss: 4.682 | Reg loss: 0.025 | Tree loss: 4.682 | Accuracy: 0.167969 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 029 | Total loss: 4.712 | Reg loss: 0.025 | Tree loss: 4.712 | Accuracy: 0.148438 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 029 | Total loss: 4.723 | Reg loss: 0.025 | Tree loss: 4.723 | Accuracy: 0.138672 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 029 | Total loss: 4.660 | Reg loss: 0.025 | Tree loss: 4.660 | Accuracy: 0.130859 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 029 | Total loss: 4.659 | Reg loss: 0.025 | Tree loss: 4.659 | Accuracy: 0.148438 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 029 | Total loss: 4.650 | Reg loss: 0.025 | Tree loss: 4.650 | Accuracy: 0.152344 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 029 | Total loss: 4.616 | Reg loss: 0.025 | Tree loss: 4.616 | Accuracy: 0.158203 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 029 | Total loss: 4.571 | Reg loss: 0.025 | Tree loss: 4.571 | Accuracy: 0.179688 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 029 | Total loss: 4.585 | Reg loss: 0.025 | Tree loss: 4.585 | Accuracy: 0.140625 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 029 | Total loss: 4.581 | Reg loss: 0.025 | Tree loss: 4.581 | Accuracy: 0.152344 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 029 | Total loss: 4.541 | Reg loss: 0.025 | Tree loss: 4.541 | Accuracy: 0.134766 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 029 | Total loss: 4.558 | Reg loss: 0.026 | Tree loss: 4.558 | Accuracy: 0.136719 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 029 | Total loss: 4.508 | Reg loss: 0.026 | Tree loss: 4.508 | Accuracy: 0.150391 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 029 | Total loss: 4.487 | Reg loss: 0.026 | Tree loss: 4.487 | Accuracy: 0.162109 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 029 | Total loss: 4.503 | Reg loss: 0.026 | Tree loss: 4.503 | Accuracy: 0.123047 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 029 | Total loss: 4.507 | Reg loss: 0.026 | Tree loss: 4.507 | Accuracy: 0.140625 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 029 | Total loss: 4.443 | Reg loss: 0.026 | Tree loss: 4.443 | Accuracy: 0.126953 | 0.258 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 020 / 029 | Total loss: 4.416 | Reg loss: 0.026 | Tree loss: 4.416 | Accuracy: 0.130859 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 029 | Total loss: 4.455 | Reg loss: 0.026 | Tree loss: 4.455 | Accuracy: 0.115234 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 029 | Total loss: 4.412 | Reg loss: 0.026 | Tree loss: 4.412 | Accuracy: 0.119141 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 029 | Total loss: 4.389 | Reg loss: 0.026 | Tree loss: 4.389 | Accuracy: 0.128906 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 029 | Total loss: 4.386 | Reg loss: 0.027 | Tree loss: 4.386 | Accuracy: 0.126953 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 029 | Total loss: 4.343 | Reg loss: 0.027 | Tree loss: 4.343 | Accuracy: 0.117188 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 029 | Total loss: 4.320 | Reg loss: 0.027 | Tree loss: 4.320 | Accuracy: 0.166016 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 029 | Total loss: 4.360 | Reg loss: 0.027 | Tree loss: 4.360 | Accuracy: 0.111328 | 0.258 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 029 | Total loss: 3.988 | Reg loss: 0.027 | Tree loss: 3.988 | Accuracy: 0.153846 | 0.258 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 22 | Batch: 000 / 029 | Total loss: 4.544 | Reg loss: 0.025 | Tree loss: 4.544 | Accuracy: 0.128906 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 029 | Total loss: 4.459 | Reg loss: 0.025 | Tree loss: 4.459 | Accuracy: 0.150391 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 029 | Total loss: 4.495 | Reg loss: 0.025 | Tree loss: 4.495 | Accuracy: 0.109375 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 029 | Total loss: 4.416 | Reg loss: 0.025 | Tree loss: 4.416 | Accuracy: 0.158203 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 029 | Total loss: 4.411 | Reg loss: 0.025 | Tree loss: 4.411 | Accuracy: 0.158203 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 029 | Total loss: 4.419 | Reg loss: 0.025 | Tree loss: 4.419 | Accuracy: 0.128906 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 029 | Total loss: 4.401 | Reg loss: 0.025 | Tree loss: 4.401 | Accuracy: 0.123047 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 029 | Total loss: 4.364 | Reg loss: 0.025 | Tree loss: 4.364 | Accuracy: 0.130859 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 029 | Total loss: 4.386 | Reg loss: 0.025 | Tree loss: 4.386 | Accuracy: 0.123047 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 029 | Total loss: 4.340 | Reg loss: 0.025 | Tree loss: 4.340 | Accuracy: 0.132812 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 029 | Total loss: 4.358 | Reg loss: 0.025 | Tree loss: 4.358 | Accuracy: 0.142578 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 029 | Total loss: 4.280 | Reg loss: 0.025 | Tree loss: 4.280 | Accuracy: 0.119141 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 029 | Total loss: 4.331 | Reg loss: 0.025 | Tree loss: 4.331 | Accuracy: 0.130859 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 029 | Total loss: 4.263 | Reg loss: 0.025 | Tree loss: 4.263 | Accuracy: 0.136719 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 029 | Total loss: 4.341 | Reg loss: 0.026 | Tree loss: 4.341 | Accuracy: 0.113281 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 029 | Total loss: 4.267 | Reg loss: 0.026 | Tree loss: 4.267 | Accuracy: 0.121094 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 029 | Total loss: 4.274 | Reg loss: 0.026 | Tree loss: 4.274 | Accuracy: 0.121094 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 029 | Total loss: 4.195 | Reg loss: 0.026 | Tree loss: 4.195 | Accuracy: 0.125000 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 029 | Total loss: 4.186 | Reg loss: 0.026 | Tree loss: 4.186 | Accuracy: 0.138672 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 029 | Total loss: 4.151 | Reg loss: 0.026 | Tree loss: 4.151 | Accuracy: 0.154297 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 029 | Total loss: 4.162 | Reg loss: 0.026 | Tree loss: 4.162 | Accuracy: 0.160156 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 029 | Total loss: 4.191 | Reg loss: 0.026 | Tree loss: 4.191 | Accuracy: 0.111328 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 029 | Total loss: 4.149 | Reg loss: 0.026 | Tree loss: 4.149 | Accuracy: 0.136719 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 029 | Total loss: 4.115 | Reg loss: 0.026 | Tree loss: 4.115 | Accuracy: 0.148438 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 029 | Total loss: 4.132 | Reg loss: 0.026 | Tree loss: 4.132 | Accuracy: 0.125000 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 029 | Total loss: 4.120 | Reg loss: 0.027 | Tree loss: 4.120 | Accuracy: 0.132812 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 029 | Total loss: 4.035 | Reg loss: 0.027 | Tree loss: 4.035 | Accuracy: 0.126953 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 029 | Total loss: 4.082 | Reg loss: 0.027 | Tree loss: 4.082 | Accuracy: 0.128906 | 0.258 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 029 | Total loss: 4.065 | Reg loss: 0.027 | Tree loss: 4.065 | Accuracy: 0.076923 | 0.257 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 23 | Batch: 000 / 029 | Total loss: 4.236 | Reg loss: 0.025 | Tree loss: 4.236 | Accuracy: 0.107422 | 0.258 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 029 | Total loss: 4.238 | Reg loss: 0.025 | Tree loss: 4.238 | Accuracy: 0.126953 | 0.258 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 029 | Total loss: 4.194 | Reg loss: 0.025 | Tree loss: 4.194 | Accuracy: 0.142578 | 0.258 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 029 | Total loss: 4.165 | Reg loss: 0.025 | Tree loss: 4.165 | Accuracy: 0.142578 | 0.258 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 029 | Total loss: 4.214 | Reg loss: 0.025 | Tree loss: 4.214 | Accuracy: 0.123047 | 0.258 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 029 | Total loss: 4.162 | Reg loss: 0.025 | Tree loss: 4.162 | Accuracy: 0.146484 | 0.258 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 029 | Total loss: 4.141 | Reg loss: 0.025 | Tree loss: 4.141 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 029 | Total loss: 4.102 | Reg loss: 0.025 | Tree loss: 4.102 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 029 | Total loss: 4.128 | Reg loss: 0.025 | Tree loss: 4.128 | Accuracy: 0.099609 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 029 | Total loss: 4.077 | Reg loss: 0.025 | Tree loss: 4.077 | Accuracy: 0.154297 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 029 | Total loss: 4.060 | Reg loss: 0.025 | Tree loss: 4.060 | Accuracy: 0.140625 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 029 | Total loss: 4.049 | Reg loss: 0.025 | Tree loss: 4.049 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 029 | Total loss: 4.005 | Reg loss: 0.026 | Tree loss: 4.005 | Accuracy: 0.105469 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 029 | Total loss: 4.027 | Reg loss: 0.026 | Tree loss: 4.027 | Accuracy: 0.111328 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 029 | Total loss: 3.968 | Reg loss: 0.026 | Tree loss: 3.968 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 029 | Total loss: 3.979 | Reg loss: 0.026 | Tree loss: 3.979 | Accuracy: 0.119141 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 029 | Total loss: 3.970 | Reg loss: 0.026 | Tree loss: 3.970 | Accuracy: 0.146484 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 029 | Total loss: 3.960 | Reg loss: 0.026 | Tree loss: 3.960 | Accuracy: 0.105469 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 029 | Total loss: 3.941 | Reg loss: 0.026 | Tree loss: 3.941 | Accuracy: 0.152344 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 029 | Total loss: 3.894 | Reg loss: 0.026 | Tree loss: 3.894 | Accuracy: 0.167969 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 029 | Total loss: 3.942 | Reg loss: 0.026 | Tree loss: 3.942 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 029 | Total loss: 3.958 | Reg loss: 0.026 | Tree loss: 3.958 | Accuracy: 0.119141 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 029 | Total loss: 3.902 | Reg loss: 0.026 | Tree loss: 3.902 | Accuracy: 0.142578 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 029 | Total loss: 3.893 | Reg loss: 0.026 | Tree loss: 3.893 | Accuracy: 0.115234 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 029 | Total loss: 3.846 | Reg loss: 0.026 | Tree loss: 3.846 | Accuracy: 0.150391 | 0.257 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 025 / 029 | Total loss: 3.854 | Reg loss: 0.027 | Tree loss: 3.854 | Accuracy: 0.099609 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 029 | Total loss: 3.845 | Reg loss: 0.027 | Tree loss: 3.845 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 029 | Total loss: 3.807 | Reg loss: 0.027 | Tree loss: 3.807 | Accuracy: 0.126953 | 0.257 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 029 | Total loss: 3.850 | Reg loss: 0.027 | Tree loss: 3.850 | Accuracy: 0.076923 | 0.257 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 24 | Batch: 000 / 029 | Total loss: 3.935 | Reg loss: 0.025 | Tree loss: 3.935 | Accuracy: 0.128906 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 029 | Total loss: 3.907 | Reg loss: 0.025 | Tree loss: 3.907 | Accuracy: 0.128906 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 029 | Total loss: 3.942 | Reg loss: 0.025 | Tree loss: 3.942 | Accuracy: 0.123047 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 029 | Total loss: 3.944 | Reg loss: 0.025 | Tree loss: 3.944 | Accuracy: 0.144531 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 029 | Total loss: 3.933 | Reg loss: 0.025 | Tree loss: 3.933 | Accuracy: 0.144531 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 029 | Total loss: 3.862 | Reg loss: 0.025 | Tree loss: 3.862 | Accuracy: 0.125000 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 029 | Total loss: 3.885 | Reg loss: 0.025 | Tree loss: 3.885 | Accuracy: 0.136719 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 029 | Total loss: 3.859 | Reg loss: 0.025 | Tree loss: 3.859 | Accuracy: 0.152344 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 029 | Total loss: 3.816 | Reg loss: 0.025 | Tree loss: 3.816 | Accuracy: 0.130859 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 029 | Total loss: 3.875 | Reg loss: 0.025 | Tree loss: 3.875 | Accuracy: 0.111328 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 029 | Total loss: 3.795 | Reg loss: 0.026 | Tree loss: 3.795 | Accuracy: 0.128906 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 029 | Total loss: 3.781 | Reg loss: 0.026 | Tree loss: 3.781 | Accuracy: 0.154297 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 029 | Total loss: 3.802 | Reg loss: 0.026 | Tree loss: 3.802 | Accuracy: 0.140625 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 029 | Total loss: 3.805 | Reg loss: 0.026 | Tree loss: 3.805 | Accuracy: 0.140625 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 029 | Total loss: 3.796 | Reg loss: 0.026 | Tree loss: 3.796 | Accuracy: 0.126953 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 029 | Total loss: 3.750 | Reg loss: 0.026 | Tree loss: 3.750 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 029 | Total loss: 3.787 | Reg loss: 0.026 | Tree loss: 3.787 | Accuracy: 0.107422 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 029 | Total loss: 3.696 | Reg loss: 0.026 | Tree loss: 3.696 | Accuracy: 0.140625 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 029 | Total loss: 3.684 | Reg loss: 0.026 | Tree loss: 3.684 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 029 | Total loss: 3.736 | Reg loss: 0.026 | Tree loss: 3.736 | Accuracy: 0.117188 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 029 | Total loss: 3.677 | Reg loss: 0.026 | Tree loss: 3.677 | Accuracy: 0.130859 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 029 | Total loss: 3.686 | Reg loss: 0.026 | Tree loss: 3.686 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 029 | Total loss: 3.677 | Reg loss: 0.026 | Tree loss: 3.677 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 029 | Total loss: 3.668 | Reg loss: 0.026 | Tree loss: 3.668 | Accuracy: 0.136719 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 029 | Total loss: 3.581 | Reg loss: 0.026 | Tree loss: 3.581 | Accuracy: 0.146484 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 029 | Total loss: 3.674 | Reg loss: 0.026 | Tree loss: 3.674 | Accuracy: 0.123047 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 029 | Total loss: 3.631 | Reg loss: 0.027 | Tree loss: 3.631 | Accuracy: 0.121094 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 029 | Total loss: 3.629 | Reg loss: 0.027 | Tree loss: 3.629 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 029 | Total loss: 3.637 | Reg loss: 0.027 | Tree loss: 3.637 | Accuracy: 0.076923 | 0.257 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 25 | Batch: 000 / 029 | Total loss: 3.736 | Reg loss: 0.025 | Tree loss: 3.736 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 029 | Total loss: 3.800 | Reg loss: 0.025 | Tree loss: 3.800 | Accuracy: 0.109375 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 029 | Total loss: 3.731 | Reg loss: 0.025 | Tree loss: 3.731 | Accuracy: 0.115234 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 029 | Total loss: 3.715 | Reg loss: 0.025 | Tree loss: 3.715 | Accuracy: 0.105469 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 029 | Total loss: 3.667 | Reg loss: 0.025 | Tree loss: 3.667 | Accuracy: 0.125000 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 029 | Total loss: 3.656 | Reg loss: 0.025 | Tree loss: 3.656 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 029 | Total loss: 3.687 | Reg loss: 0.026 | Tree loss: 3.687 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 029 | Total loss: 3.646 | Reg loss: 0.026 | Tree loss: 3.646 | Accuracy: 0.144531 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 029 | Total loss: 3.592 | Reg loss: 0.026 | Tree loss: 3.592 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 029 | Total loss: 3.619 | Reg loss: 0.026 | Tree loss: 3.619 | Accuracy: 0.123047 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 029 | Total loss: 3.587 | Reg loss: 0.026 | Tree loss: 3.587 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 029 | Total loss: 3.571 | Reg loss: 0.026 | Tree loss: 3.571 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 029 | Total loss: 3.607 | Reg loss: 0.026 | Tree loss: 3.607 | Accuracy: 0.103516 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 029 | Total loss: 3.554 | Reg loss: 0.026 | Tree loss: 3.554 | Accuracy: 0.150391 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 029 | Total loss: 3.543 | Reg loss: 0.026 | Tree loss: 3.543 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 029 | Total loss: 3.520 | Reg loss: 0.026 | Tree loss: 3.520 | Accuracy: 0.166016 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 029 | Total loss: 3.499 | Reg loss: 0.026 | Tree loss: 3.499 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 029 | Total loss: 3.501 | Reg loss: 0.026 | Tree loss: 3.501 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 029 | Total loss: 3.533 | Reg loss: 0.026 | Tree loss: 3.533 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 029 | Total loss: 3.519 | Reg loss: 0.026 | Tree loss: 3.519 | Accuracy: 0.140625 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 029 | Total loss: 3.495 | Reg loss: 0.026 | Tree loss: 3.495 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 029 | Total loss: 3.461 | Reg loss: 0.026 | Tree loss: 3.461 | Accuracy: 0.150391 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 029 | Total loss: 3.422 | Reg loss: 0.026 | Tree loss: 3.422 | Accuracy: 0.123047 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 029 | Total loss: 3.492 | Reg loss: 0.026 | Tree loss: 3.492 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 029 | Total loss: 3.434 | Reg loss: 0.026 | Tree loss: 3.434 | Accuracy: 0.154297 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 029 | Total loss: 3.466 | Reg loss: 0.026 | Tree loss: 3.466 | Accuracy: 0.126953 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 029 | Total loss: 3.390 | Reg loss: 0.026 | Tree loss: 3.390 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 029 | Total loss: 3.457 | Reg loss: 0.027 | Tree loss: 3.457 | Accuracy: 0.125000 | 0.257 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 029 | Total loss: 3.460 | Reg loss: 0.027 | Tree loss: 3.460 | Accuracy: 0.076923 | 0.257 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 000 / 029 | Total loss: 3.518 | Reg loss: 0.026 | Tree loss: 3.518 | Accuracy: 0.105469 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 029 | Total loss: 3.536 | Reg loss: 0.026 | Tree loss: 3.536 | Accuracy: 0.115234 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 029 | Total loss: 3.473 | Reg loss: 0.026 | Tree loss: 3.473 | Accuracy: 0.117188 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 029 | Total loss: 3.499 | Reg loss: 0.026 | Tree loss: 3.499 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 029 | Total loss: 3.525 | Reg loss: 0.026 | Tree loss: 3.525 | Accuracy: 0.136719 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 029 | Total loss: 3.478 | Reg loss: 0.026 | Tree loss: 3.478 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 029 | Total loss: 3.451 | Reg loss: 0.026 | Tree loss: 3.451 | Accuracy: 0.105469 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 029 | Total loss: 3.477 | Reg loss: 0.026 | Tree loss: 3.477 | Accuracy: 0.101562 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 029 | Total loss: 3.422 | Reg loss: 0.026 | Tree loss: 3.422 | Accuracy: 0.121094 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 029 | Total loss: 3.398 | Reg loss: 0.026 | Tree loss: 3.398 | Accuracy: 0.132812 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 029 | Total loss: 3.404 | Reg loss: 0.026 | Tree loss: 3.404 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 029 | Total loss: 3.414 | Reg loss: 0.026 | Tree loss: 3.414 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 029 | Total loss: 3.380 | Reg loss: 0.026 | Tree loss: 3.380 | Accuracy: 0.193359 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 029 | Total loss: 3.417 | Reg loss: 0.026 | Tree loss: 3.417 | Accuracy: 0.099609 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 029 | Total loss: 3.383 | Reg loss: 0.026 | Tree loss: 3.383 | Accuracy: 0.140625 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 029 | Total loss: 3.349 | Reg loss: 0.026 | Tree loss: 3.349 | Accuracy: 0.146484 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 029 | Total loss: 3.394 | Reg loss: 0.026 | Tree loss: 3.394 | Accuracy: 0.121094 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 029 | Total loss: 3.323 | Reg loss: 0.026 | Tree loss: 3.323 | Accuracy: 0.125000 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 029 | Total loss: 3.362 | Reg loss: 0.026 | Tree loss: 3.362 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 029 | Total loss: 3.295 | Reg loss: 0.026 | Tree loss: 3.295 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 029 | Total loss: 3.288 | Reg loss: 0.026 | Tree loss: 3.288 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 029 | Total loss: 3.259 | Reg loss: 0.026 | Tree loss: 3.259 | Accuracy: 0.187500 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 029 | Total loss: 3.314 | Reg loss: 0.026 | Tree loss: 3.314 | Accuracy: 0.125000 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 029 | Total loss: 3.273 | Reg loss: 0.026 | Tree loss: 3.273 | Accuracy: 0.146484 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 029 | Total loss: 3.291 | Reg loss: 0.026 | Tree loss: 3.291 | Accuracy: 0.150391 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 029 | Total loss: 3.297 | Reg loss: 0.026 | Tree loss: 3.297 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 029 | Total loss: 3.280 | Reg loss: 0.026 | Tree loss: 3.280 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 029 | Total loss: 3.236 | Reg loss: 0.026 | Tree loss: 3.236 | Accuracy: 0.144531 | 0.257 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 029 | Total loss: 3.279 | Reg loss: 0.026 | Tree loss: 3.279 | Accuracy: 0.153846 | 0.257 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 27 | Batch: 000 / 029 | Total loss: 3.317 | Reg loss: 0.026 | Tree loss: 3.317 | Accuracy: 0.152344 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 029 | Total loss: 3.336 | Reg loss: 0.026 | Tree loss: 3.336 | Accuracy: 0.154297 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 029 | Total loss: 3.312 | Reg loss: 0.026 | Tree loss: 3.312 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 029 | Total loss: 3.277 | Reg loss: 0.026 | Tree loss: 3.277 | Accuracy: 0.175781 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 029 | Total loss: 3.310 | Reg loss: 0.026 | Tree loss: 3.310 | Accuracy: 0.109375 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 029 | Total loss: 3.248 | Reg loss: 0.026 | Tree loss: 3.248 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 029 | Total loss: 3.324 | Reg loss: 0.026 | Tree loss: 3.324 | Accuracy: 0.103516 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 029 | Total loss: 3.232 | Reg loss: 0.026 | Tree loss: 3.232 | Accuracy: 0.130859 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 029 | Total loss: 3.303 | Reg loss: 0.026 | Tree loss: 3.303 | Accuracy: 0.148438 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 029 | Total loss: 3.246 | Reg loss: 0.026 | Tree loss: 3.246 | Accuracy: 0.152344 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 029 | Total loss: 3.267 | Reg loss: 0.026 | Tree loss: 3.267 | Accuracy: 0.119141 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 029 | Total loss: 3.227 | Reg loss: 0.026 | Tree loss: 3.227 | Accuracy: 0.113281 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 029 | Total loss: 3.203 | Reg loss: 0.026 | Tree loss: 3.203 | Accuracy: 0.146484 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 029 | Total loss: 3.223 | Reg loss: 0.026 | Tree loss: 3.223 | Accuracy: 0.160156 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 029 | Total loss: 3.197 | Reg loss: 0.026 | Tree loss: 3.197 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 029 | Total loss: 3.215 | Reg loss: 0.026 | Tree loss: 3.215 | Accuracy: 0.142578 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 029 | Total loss: 3.244 | Reg loss: 0.026 | Tree loss: 3.244 | Accuracy: 0.130859 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 029 | Total loss: 3.134 | Reg loss: 0.026 | Tree loss: 3.134 | Accuracy: 0.138672 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 029 | Total loss: 3.187 | Reg loss: 0.026 | Tree loss: 3.187 | Accuracy: 0.136719 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 029 | Total loss: 3.205 | Reg loss: 0.026 | Tree loss: 3.205 | Accuracy: 0.134766 | 0.257 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 029 | Total loss: 3.166 | Reg loss: 0.026 | Tree loss: 3.166 | Accuracy: 0.126953 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 029 | Total loss: 3.178 | Reg loss: 0.026 | Tree loss: 3.178 | Accuracy: 0.142578 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 029 | Total loss: 3.179 | Reg loss: 0.026 | Tree loss: 3.179 | Accuracy: 0.134766 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 029 | Total loss: 3.214 | Reg loss: 0.026 | Tree loss: 3.214 | Accuracy: 0.113281 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 029 | Total loss: 3.117 | Reg loss: 0.026 | Tree loss: 3.117 | Accuracy: 0.154297 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 029 | Total loss: 3.168 | Reg loss: 0.026 | Tree loss: 3.168 | Accuracy: 0.136719 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 029 | Total loss: 3.122 | Reg loss: 0.026 | Tree loss: 3.122 | Accuracy: 0.123047 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 029 | Total loss: 3.136 | Reg loss: 0.026 | Tree loss: 3.136 | Accuracy: 0.123047 | 0.256 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 029 | Total loss: 3.044 | Reg loss: 0.026 | Tree loss: 3.044 | Accuracy: 0.000000 | 0.256 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 28 | Batch: 000 / 029 | Total loss: 3.152 | Reg loss: 0.026 | Tree loss: 3.152 | Accuracy: 0.154297 | 0.257 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 029 | Total loss: 3.226 | Reg loss: 0.026 | Tree loss: 3.226 | Accuracy: 0.105469 | 0.257 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 029 | Total loss: 3.190 | Reg loss: 0.026 | Tree loss: 3.190 | Accuracy: 0.119141 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 029 | Total loss: 3.155 | Reg loss: 0.026 | Tree loss: 3.155 | Accuracy: 0.128906 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 029 | Total loss: 3.151 | Reg loss: 0.026 | Tree loss: 3.151 | Accuracy: 0.152344 | 0.256 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 005 / 029 | Total loss: 3.129 | Reg loss: 0.026 | Tree loss: 3.129 | Accuracy: 0.144531 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 029 | Total loss: 3.136 | Reg loss: 0.026 | Tree loss: 3.136 | Accuracy: 0.128906 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 029 | Total loss: 3.067 | Reg loss: 0.026 | Tree loss: 3.067 | Accuracy: 0.142578 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 029 | Total loss: 3.147 | Reg loss: 0.026 | Tree loss: 3.147 | Accuracy: 0.126953 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 029 | Total loss: 3.106 | Reg loss: 0.026 | Tree loss: 3.106 | Accuracy: 0.144531 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 029 | Total loss: 3.133 | Reg loss: 0.026 | Tree loss: 3.133 | Accuracy: 0.113281 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 029 | Total loss: 3.092 | Reg loss: 0.026 | Tree loss: 3.092 | Accuracy: 0.148438 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 029 | Total loss: 3.139 | Reg loss: 0.026 | Tree loss: 3.139 | Accuracy: 0.123047 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 029 | Total loss: 3.107 | Reg loss: 0.026 | Tree loss: 3.107 | Accuracy: 0.134766 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 029 | Total loss: 3.083 | Reg loss: 0.026 | Tree loss: 3.083 | Accuracy: 0.111328 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 029 | Total loss: 3.075 | Reg loss: 0.026 | Tree loss: 3.075 | Accuracy: 0.123047 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 029 | Total loss: 3.066 | Reg loss: 0.026 | Tree loss: 3.066 | Accuracy: 0.144531 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 029 | Total loss: 3.045 | Reg loss: 0.026 | Tree loss: 3.045 | Accuracy: 0.148438 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 029 | Total loss: 3.057 | Reg loss: 0.026 | Tree loss: 3.057 | Accuracy: 0.128906 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 029 | Total loss: 3.070 | Reg loss: 0.026 | Tree loss: 3.070 | Accuracy: 0.138672 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 029 | Total loss: 3.078 | Reg loss: 0.026 | Tree loss: 3.078 | Accuracy: 0.140625 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 029 | Total loss: 3.012 | Reg loss: 0.026 | Tree loss: 3.012 | Accuracy: 0.152344 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 029 | Total loss: 3.001 | Reg loss: 0.026 | Tree loss: 3.001 | Accuracy: 0.142578 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 029 | Total loss: 3.018 | Reg loss: 0.026 | Tree loss: 3.018 | Accuracy: 0.162109 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 029 | Total loss: 3.020 | Reg loss: 0.026 | Tree loss: 3.020 | Accuracy: 0.150391 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 029 | Total loss: 3.005 | Reg loss: 0.026 | Tree loss: 3.005 | Accuracy: 0.136719 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 029 | Total loss: 3.005 | Reg loss: 0.026 | Tree loss: 3.005 | Accuracy: 0.111328 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 029 | Total loss: 2.995 | Reg loss: 0.026 | Tree loss: 2.995 | Accuracy: 0.152344 | 0.256 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 029 | Total loss: 2.927 | Reg loss: 0.026 | Tree loss: 2.927 | Accuracy: 0.230769 | 0.256 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 29 | Batch: 000 / 029 | Total loss: 3.054 | Reg loss: 0.026 | Tree loss: 3.054 | Accuracy: 0.138672 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 029 | Total loss: 3.032 | Reg loss: 0.026 | Tree loss: 3.032 | Accuracy: 0.164062 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 029 | Total loss: 3.033 | Reg loss: 0.026 | Tree loss: 3.033 | Accuracy: 0.125000 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 029 | Total loss: 3.018 | Reg loss: 0.026 | Tree loss: 3.018 | Accuracy: 0.123047 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 029 | Total loss: 3.023 | Reg loss: 0.026 | Tree loss: 3.023 | Accuracy: 0.119141 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 029 | Total loss: 2.998 | Reg loss: 0.026 | Tree loss: 2.998 | Accuracy: 0.142578 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 029 | Total loss: 2.994 | Reg loss: 0.026 | Tree loss: 2.994 | Accuracy: 0.130859 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 029 | Total loss: 2.986 | Reg loss: 0.026 | Tree loss: 2.986 | Accuracy: 0.134766 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 029 | Total loss: 2.992 | Reg loss: 0.026 | Tree loss: 2.992 | Accuracy: 0.132812 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 029 | Total loss: 3.026 | Reg loss: 0.026 | Tree loss: 3.026 | Accuracy: 0.119141 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 029 | Total loss: 2.998 | Reg loss: 0.026 | Tree loss: 2.998 | Accuracy: 0.138672 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 029 | Total loss: 2.986 | Reg loss: 0.026 | Tree loss: 2.986 | Accuracy: 0.119141 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 029 | Total loss: 3.006 | Reg loss: 0.026 | Tree loss: 3.006 | Accuracy: 0.101562 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 029 | Total loss: 2.946 | Reg loss: 0.026 | Tree loss: 2.946 | Accuracy: 0.146484 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 029 | Total loss: 2.957 | Reg loss: 0.026 | Tree loss: 2.957 | Accuracy: 0.136719 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 029 | Total loss: 2.968 | Reg loss: 0.026 | Tree loss: 2.968 | Accuracy: 0.140625 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 029 | Total loss: 2.963 | Reg loss: 0.026 | Tree loss: 2.963 | Accuracy: 0.142578 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 029 | Total loss: 2.928 | Reg loss: 0.026 | Tree loss: 2.928 | Accuracy: 0.146484 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 029 | Total loss: 2.961 | Reg loss: 0.026 | Tree loss: 2.961 | Accuracy: 0.140625 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 029 | Total loss: 2.965 | Reg loss: 0.026 | Tree loss: 2.965 | Accuracy: 0.113281 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 029 | Total loss: 2.974 | Reg loss: 0.026 | Tree loss: 2.974 | Accuracy: 0.105469 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 029 | Total loss: 2.904 | Reg loss: 0.026 | Tree loss: 2.904 | Accuracy: 0.164062 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 029 | Total loss: 2.886 | Reg loss: 0.026 | Tree loss: 2.886 | Accuracy: 0.146484 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 029 | Total loss: 2.907 | Reg loss: 0.026 | Tree loss: 2.907 | Accuracy: 0.148438 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 029 | Total loss: 2.909 | Reg loss: 0.026 | Tree loss: 2.909 | Accuracy: 0.167969 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 029 | Total loss: 2.890 | Reg loss: 0.026 | Tree loss: 2.890 | Accuracy: 0.142578 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 029 | Total loss: 2.943 | Reg loss: 0.026 | Tree loss: 2.943 | Accuracy: 0.140625 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 029 | Total loss: 2.896 | Reg loss: 0.026 | Tree loss: 2.896 | Accuracy: 0.142578 | 0.256 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 029 | Total loss: 2.906 | Reg loss: 0.026 | Tree loss: 2.906 | Accuracy: 0.076923 | 0.256 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 30 | Batch: 000 / 029 | Total loss: 2.947 | Reg loss: 0.026 | Tree loss: 2.947 | Accuracy: 0.148438 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 029 | Total loss: 2.935 | Reg loss: 0.026 | Tree loss: 2.935 | Accuracy: 0.132812 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 029 | Total loss: 2.921 | Reg loss: 0.026 | Tree loss: 2.921 | Accuracy: 0.140625 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 029 | Total loss: 2.873 | Reg loss: 0.026 | Tree loss: 2.873 | Accuracy: 0.148438 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 029 | Total loss: 2.919 | Reg loss: 0.026 | Tree loss: 2.919 | Accuracy: 0.134766 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 029 | Total loss: 2.865 | Reg loss: 0.026 | Tree loss: 2.865 | Accuracy: 0.134766 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 029 | Total loss: 2.953 | Reg loss: 0.026 | Tree loss: 2.953 | Accuracy: 0.144531 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 029 | Total loss: 2.880 | Reg loss: 0.026 | Tree loss: 2.880 | Accuracy: 0.128906 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 029 | Total loss: 2.887 | Reg loss: 0.026 | Tree loss: 2.887 | Accuracy: 0.128906 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 029 | Total loss: 2.894 | Reg loss: 0.026 | Tree loss: 2.894 | Accuracy: 0.130859 | 0.256 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 010 / 029 | Total loss: 2.859 | Reg loss: 0.026 | Tree loss: 2.859 | Accuracy: 0.152344 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 029 | Total loss: 2.861 | Reg loss: 0.026 | Tree loss: 2.861 | Accuracy: 0.136719 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 029 | Total loss: 2.847 | Reg loss: 0.026 | Tree loss: 2.847 | Accuracy: 0.154297 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 029 | Total loss: 2.914 | Reg loss: 0.026 | Tree loss: 2.914 | Accuracy: 0.121094 | 0.256 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 029 | Total loss: 2.871 | Reg loss: 0.026 | Tree loss: 2.871 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 029 | Total loss: 2.867 | Reg loss: 0.026 | Tree loss: 2.867 | Accuracy: 0.125000 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 029 | Total loss: 2.852 | Reg loss: 0.026 | Tree loss: 2.852 | Accuracy: 0.148438 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 029 | Total loss: 2.851 | Reg loss: 0.026 | Tree loss: 2.851 | Accuracy: 0.150391 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 029 | Total loss: 2.866 | Reg loss: 0.026 | Tree loss: 2.866 | Accuracy: 0.128906 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 029 | Total loss: 2.839 | Reg loss: 0.026 | Tree loss: 2.839 | Accuracy: 0.126953 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 029 | Total loss: 2.828 | Reg loss: 0.026 | Tree loss: 2.828 | Accuracy: 0.152344 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 029 | Total loss: 2.816 | Reg loss: 0.026 | Tree loss: 2.816 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 029 | Total loss: 2.818 | Reg loss: 0.026 | Tree loss: 2.818 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 029 | Total loss: 2.847 | Reg loss: 0.026 | Tree loss: 2.847 | Accuracy: 0.148438 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 029 | Total loss: 2.840 | Reg loss: 0.026 | Tree loss: 2.840 | Accuracy: 0.115234 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 029 | Total loss: 2.816 | Reg loss: 0.026 | Tree loss: 2.816 | Accuracy: 0.115234 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 029 | Total loss: 2.784 | Reg loss: 0.026 | Tree loss: 2.784 | Accuracy: 0.123047 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 029 | Total loss: 2.807 | Reg loss: 0.026 | Tree loss: 2.807 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 029 | Total loss: 3.006 | Reg loss: 0.026 | Tree loss: 3.006 | Accuracy: 0.153846 | 0.255 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 31 | Batch: 000 / 029 | Total loss: 2.852 | Reg loss: 0.026 | Tree loss: 2.852 | Accuracy: 0.123047 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 029 | Total loss: 2.828 | Reg loss: 0.026 | Tree loss: 2.828 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 029 | Total loss: 2.817 | Reg loss: 0.026 | Tree loss: 2.817 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 029 | Total loss: 2.805 | Reg loss: 0.026 | Tree loss: 2.805 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 029 | Total loss: 2.823 | Reg loss: 0.026 | Tree loss: 2.823 | Accuracy: 0.121094 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 029 | Total loss: 2.844 | Reg loss: 0.026 | Tree loss: 2.844 | Accuracy: 0.132812 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 029 | Total loss: 2.802 | Reg loss: 0.026 | Tree loss: 2.802 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 029 | Total loss: 2.759 | Reg loss: 0.026 | Tree loss: 2.759 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 029 | Total loss: 2.836 | Reg loss: 0.026 | Tree loss: 2.836 | Accuracy: 0.119141 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 029 | Total loss: 2.827 | Reg loss: 0.026 | Tree loss: 2.827 | Accuracy: 0.150391 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 029 | Total loss: 2.784 | Reg loss: 0.026 | Tree loss: 2.784 | Accuracy: 0.128906 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 029 | Total loss: 2.813 | Reg loss: 0.026 | Tree loss: 2.813 | Accuracy: 0.148438 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 029 | Total loss: 2.803 | Reg loss: 0.026 | Tree loss: 2.803 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 029 | Total loss: 2.755 | Reg loss: 0.026 | Tree loss: 2.755 | Accuracy: 0.162109 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 029 | Total loss: 2.802 | Reg loss: 0.026 | Tree loss: 2.802 | Accuracy: 0.125000 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 029 | Total loss: 2.778 | Reg loss: 0.026 | Tree loss: 2.778 | Accuracy: 0.158203 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 029 | Total loss: 2.750 | Reg loss: 0.026 | Tree loss: 2.750 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 029 | Total loss: 2.747 | Reg loss: 0.026 | Tree loss: 2.747 | Accuracy: 0.142578 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 029 | Total loss: 2.768 | Reg loss: 0.026 | Tree loss: 2.768 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 029 | Total loss: 2.752 | Reg loss: 0.026 | Tree loss: 2.752 | Accuracy: 0.128906 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 029 | Total loss: 2.753 | Reg loss: 0.026 | Tree loss: 2.753 | Accuracy: 0.111328 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 029 | Total loss: 2.706 | Reg loss: 0.026 | Tree loss: 2.706 | Accuracy: 0.164062 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 029 | Total loss: 2.727 | Reg loss: 0.026 | Tree loss: 2.727 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 029 | Total loss: 2.711 | Reg loss: 0.026 | Tree loss: 2.711 | Accuracy: 0.128906 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 029 | Total loss: 2.717 | Reg loss: 0.026 | Tree loss: 2.717 | Accuracy: 0.140625 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 029 | Total loss: 2.728 | Reg loss: 0.026 | Tree loss: 2.728 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 029 | Total loss: 2.756 | Reg loss: 0.026 | Tree loss: 2.756 | Accuracy: 0.097656 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 029 | Total loss: 2.702 | Reg loss: 0.026 | Tree loss: 2.702 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 31 | Batch: 028 / 029 | Total loss: 2.748 | Reg loss: 0.026 | Tree loss: 2.748 | Accuracy: 0.076923 | 0.255 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 32 | Batch: 000 / 029 | Total loss: 2.743 | Reg loss: 0.026 | Tree loss: 2.743 | Accuracy: 0.115234 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 029 | Total loss: 2.739 | Reg loss: 0.026 | Tree loss: 2.739 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 029 | Total loss: 2.782 | Reg loss: 0.026 | Tree loss: 2.782 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 029 | Total loss: 2.703 | Reg loss: 0.026 | Tree loss: 2.703 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 029 | Total loss: 2.735 | Reg loss: 0.026 | Tree loss: 2.735 | Accuracy: 0.125000 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 029 | Total loss: 2.765 | Reg loss: 0.026 | Tree loss: 2.765 | Accuracy: 0.113281 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 029 | Total loss: 2.721 | Reg loss: 0.026 | Tree loss: 2.721 | Accuracy: 0.107422 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 029 | Total loss: 2.725 | Reg loss: 0.026 | Tree loss: 2.725 | Accuracy: 0.125000 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 029 | Total loss: 2.716 | Reg loss: 0.026 | Tree loss: 2.716 | Accuracy: 0.144531 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 029 | Total loss: 2.698 | Reg loss: 0.026 | Tree loss: 2.698 | Accuracy: 0.154297 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 029 | Total loss: 2.725 | Reg loss: 0.026 | Tree loss: 2.725 | Accuracy: 0.121094 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 029 | Total loss: 2.698 | Reg loss: 0.026 | Tree loss: 2.698 | Accuracy: 0.150391 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 029 | Total loss: 2.687 | Reg loss: 0.026 | Tree loss: 2.687 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 029 | Total loss: 2.703 | Reg loss: 0.026 | Tree loss: 2.703 | Accuracy: 0.144531 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 029 | Total loss: 2.692 | Reg loss: 0.026 | Tree loss: 2.692 | Accuracy: 0.144531 | 0.255 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 015 / 029 | Total loss: 2.671 | Reg loss: 0.026 | Tree loss: 2.671 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 029 | Total loss: 2.680 | Reg loss: 0.026 | Tree loss: 2.680 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 029 | Total loss: 2.681 | Reg loss: 0.026 | Tree loss: 2.681 | Accuracy: 0.125000 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 029 | Total loss: 2.698 | Reg loss: 0.026 | Tree loss: 2.698 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 029 | Total loss: 2.648 | Reg loss: 0.026 | Tree loss: 2.648 | Accuracy: 0.152344 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 029 | Total loss: 2.665 | Reg loss: 0.026 | Tree loss: 2.665 | Accuracy: 0.160156 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 029 | Total loss: 2.724 | Reg loss: 0.026 | Tree loss: 2.724 | Accuracy: 0.095703 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 029 | Total loss: 2.657 | Reg loss: 0.026 | Tree loss: 2.657 | Accuracy: 0.171875 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 029 | Total loss: 2.675 | Reg loss: 0.026 | Tree loss: 2.675 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 029 | Total loss: 2.643 | Reg loss: 0.026 | Tree loss: 2.643 | Accuracy: 0.132812 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 029 | Total loss: 2.655 | Reg loss: 0.026 | Tree loss: 2.655 | Accuracy: 0.166016 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 029 | Total loss: 2.653 | Reg loss: 0.026 | Tree loss: 2.653 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 029 | Total loss: 2.642 | Reg loss: 0.026 | Tree loss: 2.642 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 029 | Total loss: 2.841 | Reg loss: 0.026 | Tree loss: 2.841 | Accuracy: 0.000000 | 0.255 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 33 | Batch: 000 / 029 | Total loss: 2.680 | Reg loss: 0.026 | Tree loss: 2.680 | Accuracy: 0.144531 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 029 | Total loss: 2.689 | Reg loss: 0.026 | Tree loss: 2.689 | Accuracy: 0.132812 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 029 | Total loss: 2.663 | Reg loss: 0.026 | Tree loss: 2.663 | Accuracy: 0.148438 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 029 | Total loss: 2.714 | Reg loss: 0.026 | Tree loss: 2.714 | Accuracy: 0.126953 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 029 | Total loss: 2.671 | Reg loss: 0.026 | Tree loss: 2.671 | Accuracy: 0.154297 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 029 | Total loss: 2.647 | Reg loss: 0.026 | Tree loss: 2.647 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 029 | Total loss: 2.663 | Reg loss: 0.026 | Tree loss: 2.663 | Accuracy: 0.113281 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 029 | Total loss: 2.602 | Reg loss: 0.026 | Tree loss: 2.602 | Accuracy: 0.152344 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 029 | Total loss: 2.649 | Reg loss: 0.026 | Tree loss: 2.649 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 029 | Total loss: 2.598 | Reg loss: 0.026 | Tree loss: 2.598 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 029 | Total loss: 2.681 | Reg loss: 0.026 | Tree loss: 2.681 | Accuracy: 0.121094 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 029 | Total loss: 2.618 | Reg loss: 0.026 | Tree loss: 2.618 | Accuracy: 0.128906 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 029 | Total loss: 2.633 | Reg loss: 0.026 | Tree loss: 2.633 | Accuracy: 0.140625 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 029 | Total loss: 2.676 | Reg loss: 0.026 | Tree loss: 2.676 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 029 | Total loss: 2.629 | Reg loss: 0.026 | Tree loss: 2.629 | Accuracy: 0.152344 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 029 | Total loss: 2.596 | Reg loss: 0.026 | Tree loss: 2.596 | Accuracy: 0.154297 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 029 | Total loss: 2.575 | Reg loss: 0.026 | Tree loss: 2.575 | Accuracy: 0.167969 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 029 | Total loss: 2.614 | Reg loss: 0.026 | Tree loss: 2.614 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 029 | Total loss: 2.589 | Reg loss: 0.026 | Tree loss: 2.589 | Accuracy: 0.166016 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 029 | Total loss: 2.634 | Reg loss: 0.026 | Tree loss: 2.634 | Accuracy: 0.152344 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 029 | Total loss: 2.614 | Reg loss: 0.026 | Tree loss: 2.614 | Accuracy: 0.115234 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 029 | Total loss: 2.606 | Reg loss: 0.026 | Tree loss: 2.606 | Accuracy: 0.103516 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 029 | Total loss: 2.597 | Reg loss: 0.026 | Tree loss: 2.597 | Accuracy: 0.115234 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 029 | Total loss: 2.617 | Reg loss: 0.026 | Tree loss: 2.617 | Accuracy: 0.105469 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 029 | Total loss: 2.610 | Reg loss: 0.026 | Tree loss: 2.610 | Accuracy: 0.123047 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 029 | Total loss: 2.591 | Reg loss: 0.026 | Tree loss: 2.591 | Accuracy: 0.162109 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 029 | Total loss: 2.548 | Reg loss: 0.026 | Tree loss: 2.548 | Accuracy: 0.128906 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 029 | Total loss: 2.582 | Reg loss: 0.026 | Tree loss: 2.582 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 029 | Total loss: 2.682 | Reg loss: 0.026 | Tree loss: 2.682 | Accuracy: 0.000000 | 0.255 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 34 | Batch: 000 / 029 | Total loss: 2.568 | Reg loss: 0.026 | Tree loss: 2.568 | Accuracy: 0.140625 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 029 | Total loss: 2.619 | Reg loss: 0.026 | Tree loss: 2.619 | Accuracy: 0.152344 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 029 | Total loss: 2.612 | Reg loss: 0.026 | Tree loss: 2.612 | Accuracy: 0.148438 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 029 | Total loss: 2.615 | Reg loss: 0.026 | Tree loss: 2.615 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 029 | Total loss: 2.580 | Reg loss: 0.026 | Tree loss: 2.580 | Accuracy: 0.148438 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 029 | Total loss: 2.565 | Reg loss: 0.026 | Tree loss: 2.565 | Accuracy: 0.140625 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 029 | Total loss: 2.602 | Reg loss: 0.026 | Tree loss: 2.602 | Accuracy: 0.132812 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 029 | Total loss: 2.580 | Reg loss: 0.026 | Tree loss: 2.580 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 029 | Total loss: 2.601 | Reg loss: 0.026 | Tree loss: 2.601 | Accuracy: 0.142578 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 029 | Total loss: 2.606 | Reg loss: 0.026 | Tree loss: 2.606 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 029 | Total loss: 2.543 | Reg loss: 0.026 | Tree loss: 2.543 | Accuracy: 0.160156 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 029 | Total loss: 2.545 | Reg loss: 0.026 | Tree loss: 2.545 | Accuracy: 0.162109 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 029 | Total loss: 2.547 | Reg loss: 0.026 | Tree loss: 2.547 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 029 | Total loss: 2.578 | Reg loss: 0.026 | Tree loss: 2.578 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 029 | Total loss: 2.584 | Reg loss: 0.026 | Tree loss: 2.584 | Accuracy: 0.119141 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 029 | Total loss: 2.564 | Reg loss: 0.026 | Tree loss: 2.564 | Accuracy: 0.109375 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 029 | Total loss: 2.553 | Reg loss: 0.026 | Tree loss: 2.553 | Accuracy: 0.156250 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 029 | Total loss: 2.547 | Reg loss: 0.026 | Tree loss: 2.547 | Accuracy: 0.158203 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 029 | Total loss: 2.565 | Reg loss: 0.026 | Tree loss: 2.565 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 029 | Total loss: 2.538 | Reg loss: 0.026 | Tree loss: 2.538 | Accuracy: 0.130859 | 0.255 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 020 / 029 | Total loss: 2.547 | Reg loss: 0.026 | Tree loss: 2.547 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 029 | Total loss: 2.541 | Reg loss: 0.026 | Tree loss: 2.541 | Accuracy: 0.119141 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 029 | Total loss: 2.527 | Reg loss: 0.026 | Tree loss: 2.527 | Accuracy: 0.111328 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 029 | Total loss: 2.528 | Reg loss: 0.026 | Tree loss: 2.528 | Accuracy: 0.111328 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 029 | Total loss: 2.548 | Reg loss: 0.026 | Tree loss: 2.548 | Accuracy: 0.136719 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 029 | Total loss: 2.556 | Reg loss: 0.026 | Tree loss: 2.556 | Accuracy: 0.119141 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 029 | Total loss: 2.559 | Reg loss: 0.026 | Tree loss: 2.559 | Accuracy: 0.119141 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 029 | Total loss: 2.562 | Reg loss: 0.026 | Tree loss: 2.562 | Accuracy: 0.132812 | 0.255 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 029 | Total loss: 2.655 | Reg loss: 0.026 | Tree loss: 2.655 | Accuracy: 0.000000 | 0.255 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 35 | Batch: 000 / 029 | Total loss: 2.536 | Reg loss: 0.026 | Tree loss: 2.536 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 029 | Total loss: 2.528 | Reg loss: 0.026 | Tree loss: 2.528 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 029 | Total loss: 2.531 | Reg loss: 0.026 | Tree loss: 2.531 | Accuracy: 0.156250 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 029 | Total loss: 2.531 | Reg loss: 0.026 | Tree loss: 2.531 | Accuracy: 0.095703 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 029 | Total loss: 2.507 | Reg loss: 0.026 | Tree loss: 2.507 | Accuracy: 0.144531 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 029 | Total loss: 2.546 | Reg loss: 0.026 | Tree loss: 2.546 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 029 | Total loss: 2.551 | Reg loss: 0.026 | Tree loss: 2.551 | Accuracy: 0.128906 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 029 | Total loss: 2.530 | Reg loss: 0.026 | Tree loss: 2.530 | Accuracy: 0.160156 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 029 | Total loss: 2.540 | Reg loss: 0.026 | Tree loss: 2.540 | Accuracy: 0.144531 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 029 | Total loss: 2.530 | Reg loss: 0.026 | Tree loss: 2.530 | Accuracy: 0.132812 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 029 | Total loss: 2.523 | Reg loss: 0.026 | Tree loss: 2.523 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 029 | Total loss: 2.499 | Reg loss: 0.026 | Tree loss: 2.499 | Accuracy: 0.142578 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 029 | Total loss: 2.511 | Reg loss: 0.026 | Tree loss: 2.511 | Accuracy: 0.166016 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 029 | Total loss: 2.564 | Reg loss: 0.026 | Tree loss: 2.564 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 029 | Total loss: 2.530 | Reg loss: 0.026 | Tree loss: 2.530 | Accuracy: 0.169922 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 029 | Total loss: 2.485 | Reg loss: 0.026 | Tree loss: 2.485 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 029 | Total loss: 2.506 | Reg loss: 0.026 | Tree loss: 2.506 | Accuracy: 0.126953 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 029 | Total loss: 2.519 | Reg loss: 0.026 | Tree loss: 2.519 | Accuracy: 0.105469 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 029 | Total loss: 2.466 | Reg loss: 0.026 | Tree loss: 2.466 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 029 | Total loss: 2.523 | Reg loss: 0.026 | Tree loss: 2.523 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 029 | Total loss: 2.491 | Reg loss: 0.026 | Tree loss: 2.491 | Accuracy: 0.123047 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 029 | Total loss: 2.507 | Reg loss: 0.026 | Tree loss: 2.507 | Accuracy: 0.115234 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 029 | Total loss: 2.500 | Reg loss: 0.026 | Tree loss: 2.500 | Accuracy: 0.125000 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 029 | Total loss: 2.480 | Reg loss: 0.026 | Tree loss: 2.480 | Accuracy: 0.140625 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 029 | Total loss: 2.480 | Reg loss: 0.026 | Tree loss: 2.480 | Accuracy: 0.115234 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 029 | Total loss: 2.467 | Reg loss: 0.026 | Tree loss: 2.467 | Accuracy: 0.130859 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 029 | Total loss: 2.519 | Reg loss: 0.026 | Tree loss: 2.519 | Accuracy: 0.123047 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 029 | Total loss: 2.468 | Reg loss: 0.026 | Tree loss: 2.468 | Accuracy: 0.164062 | 0.255 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 029 | Total loss: 2.657 | Reg loss: 0.026 | Tree loss: 2.657 | Accuracy: 0.000000 | 0.255 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 36 | Batch: 000 / 029 | Total loss: 2.491 | Reg loss: 0.026 | Tree loss: 2.491 | Accuracy: 0.154297 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 029 | Total loss: 2.490 | Reg loss: 0.026 | Tree loss: 2.490 | Accuracy: 0.125000 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 029 | Total loss: 2.473 | Reg loss: 0.026 | Tree loss: 2.473 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 029 | Total loss: 2.495 | Reg loss: 0.026 | Tree loss: 2.495 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 029 | Total loss: 2.484 | Reg loss: 0.026 | Tree loss: 2.484 | Accuracy: 0.150391 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 029 | Total loss: 2.516 | Reg loss: 0.026 | Tree loss: 2.516 | Accuracy: 0.138672 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 029 | Total loss: 2.481 | Reg loss: 0.026 | Tree loss: 2.481 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 029 | Total loss: 2.462 | Reg loss: 0.026 | Tree loss: 2.462 | Accuracy: 0.146484 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 029 | Total loss: 2.469 | Reg loss: 0.026 | Tree loss: 2.469 | Accuracy: 0.144531 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 029 | Total loss: 2.506 | Reg loss: 0.026 | Tree loss: 2.506 | Accuracy: 0.119141 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 029 | Total loss: 2.450 | Reg loss: 0.026 | Tree loss: 2.450 | Accuracy: 0.152344 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 029 | Total loss: 2.465 | Reg loss: 0.026 | Tree loss: 2.465 | Accuracy: 0.134766 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 029 | Total loss: 2.451 | Reg loss: 0.026 | Tree loss: 2.451 | Accuracy: 0.142578 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 029 | Total loss: 2.441 | Reg loss: 0.026 | Tree loss: 2.441 | Accuracy: 0.208984 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 029 | Total loss: 2.435 | Reg loss: 0.026 | Tree loss: 2.435 | Accuracy: 0.207031 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 029 | Total loss: 2.467 | Reg loss: 0.026 | Tree loss: 2.467 | Accuracy: 0.205078 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 029 | Total loss: 2.489 | Reg loss: 0.026 | Tree loss: 2.489 | Accuracy: 0.218750 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 029 | Total loss: 2.445 | Reg loss: 0.026 | Tree loss: 2.445 | Accuracy: 0.244141 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 029 | Total loss: 2.466 | Reg loss: 0.026 | Tree loss: 2.466 | Accuracy: 0.246094 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 029 | Total loss: 2.482 | Reg loss: 0.026 | Tree loss: 2.482 | Accuracy: 0.222656 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 029 | Total loss: 2.452 | Reg loss: 0.026 | Tree loss: 2.452 | Accuracy: 0.238281 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 029 | Total loss: 2.454 | Reg loss: 0.026 | Tree loss: 2.454 | Accuracy: 0.250000 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 029 | Total loss: 2.457 | Reg loss: 0.026 | Tree loss: 2.457 | Accuracy: 0.222656 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 029 | Total loss: 2.468 | Reg loss: 0.026 | Tree loss: 2.468 | Accuracy: 0.255859 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 029 | Total loss: 2.430 | Reg loss: 0.026 | Tree loss: 2.430 | Accuracy: 0.246094 | 0.255 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 025 / 029 | Total loss: 2.434 | Reg loss: 0.026 | Tree loss: 2.434 | Accuracy: 0.275391 | 0.255 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 029 | Total loss: 2.464 | Reg loss: 0.026 | Tree loss: 2.464 | Accuracy: 0.259766 | 0.254 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 029 | Total loss: 2.443 | Reg loss: 0.026 | Tree loss: 2.443 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 029 | Total loss: 2.345 | Reg loss: 0.026 | Tree loss: 2.345 | Accuracy: 0.538462 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 37 | Batch: 000 / 029 | Total loss: 2.493 | Reg loss: 0.025 | Tree loss: 2.493 | Accuracy: 0.236328 | 0.255 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 029 | Total loss: 2.452 | Reg loss: 0.025 | Tree loss: 2.452 | Accuracy: 0.257812 | 0.255 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 029 | Total loss: 2.434 | Reg loss: 0.025 | Tree loss: 2.434 | Accuracy: 0.265625 | 0.255 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 029 | Total loss: 2.491 | Reg loss: 0.025 | Tree loss: 2.491 | Accuracy: 0.271484 | 0.255 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 029 | Total loss: 2.477 | Reg loss: 0.025 | Tree loss: 2.477 | Accuracy: 0.242188 | 0.255 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 029 | Total loss: 2.429 | Reg loss: 0.025 | Tree loss: 2.429 | Accuracy: 0.253906 | 0.255 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 029 | Total loss: 2.436 | Reg loss: 0.025 | Tree loss: 2.436 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 029 | Total loss: 2.433 | Reg loss: 0.025 | Tree loss: 2.433 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 029 | Total loss: 2.411 | Reg loss: 0.025 | Tree loss: 2.411 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 029 | Total loss: 2.401 | Reg loss: 0.025 | Tree loss: 2.401 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 029 | Total loss: 2.419 | Reg loss: 0.025 | Tree loss: 2.419 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 029 | Total loss: 2.409 | Reg loss: 0.025 | Tree loss: 2.409 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 029 | Total loss: 2.446 | Reg loss: 0.025 | Tree loss: 2.446 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 029 | Total loss: 2.424 | Reg loss: 0.025 | Tree loss: 2.424 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 029 | Total loss: 2.457 | Reg loss: 0.026 | Tree loss: 2.457 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 029 | Total loss: 2.421 | Reg loss: 0.026 | Tree loss: 2.421 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 029 | Total loss: 2.429 | Reg loss: 0.026 | Tree loss: 2.429 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 029 | Total loss: 2.398 | Reg loss: 0.026 | Tree loss: 2.398 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 029 | Total loss: 2.394 | Reg loss: 0.026 | Tree loss: 2.394 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 029 | Total loss: 2.404 | Reg loss: 0.026 | Tree loss: 2.404 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 029 | Total loss: 2.409 | Reg loss: 0.026 | Tree loss: 2.409 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 029 | Total loss: 2.434 | Reg loss: 0.026 | Tree loss: 2.434 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 029 | Total loss: 2.388 | Reg loss: 0.026 | Tree loss: 2.388 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 029 | Total loss: 2.418 | Reg loss: 0.026 | Tree loss: 2.418 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 029 | Total loss: 2.422 | Reg loss: 0.026 | Tree loss: 2.422 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 029 | Total loss: 2.375 | Reg loss: 0.026 | Tree loss: 2.375 | Accuracy: 0.312500 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 029 | Total loss: 2.381 | Reg loss: 0.026 | Tree loss: 2.381 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 029 | Total loss: 2.405 | Reg loss: 0.026 | Tree loss: 2.405 | Accuracy: 0.244141 | 0.254 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 029 | Total loss: 2.481 | Reg loss: 0.026 | Tree loss: 2.481 | Accuracy: 0.153846 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 38 | Batch: 000 / 029 | Total loss: 2.421 | Reg loss: 0.025 | Tree loss: 2.421 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 029 | Total loss: 2.407 | Reg loss: 0.025 | Tree loss: 2.407 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 029 | Total loss: 2.409 | Reg loss: 0.025 | Tree loss: 2.409 | Accuracy: 0.308594 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 029 | Total loss: 2.407 | Reg loss: 0.025 | Tree loss: 2.407 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 029 | Total loss: 2.367 | Reg loss: 0.025 | Tree loss: 2.367 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 029 | Total loss: 2.381 | Reg loss: 0.025 | Tree loss: 2.381 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 029 | Total loss: 2.375 | Reg loss: 0.025 | Tree loss: 2.375 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 029 | Total loss: 2.397 | Reg loss: 0.025 | Tree loss: 2.397 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 029 | Total loss: 2.411 | Reg loss: 0.025 | Tree loss: 2.411 | Accuracy: 0.246094 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 029 | Total loss: 2.408 | Reg loss: 0.025 | Tree loss: 2.408 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 029 | Total loss: 2.349 | Reg loss: 0.025 | Tree loss: 2.349 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 029 | Total loss: 2.412 | Reg loss: 0.025 | Tree loss: 2.412 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 029 | Total loss: 2.369 | Reg loss: 0.025 | Tree loss: 2.369 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 029 | Total loss: 2.389 | Reg loss: 0.025 | Tree loss: 2.389 | Accuracy: 0.302734 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 029 | Total loss: 2.397 | Reg loss: 0.025 | Tree loss: 2.397 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 029 | Total loss: 2.387 | Reg loss: 0.025 | Tree loss: 2.387 | Accuracy: 0.251953 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 029 | Total loss: 2.423 | Reg loss: 0.025 | Tree loss: 2.423 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 029 | Total loss: 2.414 | Reg loss: 0.025 | Tree loss: 2.414 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 029 | Total loss: 2.360 | Reg loss: 0.025 | Tree loss: 2.360 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 029 | Total loss: 2.372 | Reg loss: 0.025 | Tree loss: 2.372 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 029 | Total loss: 2.333 | Reg loss: 0.025 | Tree loss: 2.333 | Accuracy: 0.332031 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 029 | Total loss: 2.440 | Reg loss: 0.025 | Tree loss: 2.440 | Accuracy: 0.240234 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 029 | Total loss: 2.372 | Reg loss: 0.025 | Tree loss: 2.372 | Accuracy: 0.302734 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 029 | Total loss: 2.364 | Reg loss: 0.025 | Tree loss: 2.364 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 029 | Total loss: 2.376 | Reg loss: 0.025 | Tree loss: 2.376 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 029 | Total loss: 2.383 | Reg loss: 0.025 | Tree loss: 2.383 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 029 | Total loss: 2.377 | Reg loss: 0.025 | Tree loss: 2.377 | Accuracy: 0.244141 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 029 | Total loss: 2.366 | Reg loss: 0.026 | Tree loss: 2.366 | Accuracy: 0.226562 | 0.254 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 029 | Total loss: 2.525 | Reg loss: 0.026 | Tree loss: 2.525 | Accuracy: 0.153846 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 000 / 029 | Total loss: 2.333 | Reg loss: 0.025 | Tree loss: 2.333 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 029 | Total loss: 2.347 | Reg loss: 0.025 | Tree loss: 2.347 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 029 | Total loss: 2.386 | Reg loss: 0.025 | Tree loss: 2.386 | Accuracy: 0.234375 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 029 | Total loss: 2.344 | Reg loss: 0.025 | Tree loss: 2.344 | Accuracy: 0.306641 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 029 | Total loss: 2.362 | Reg loss: 0.025 | Tree loss: 2.362 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 029 | Total loss: 2.371 | Reg loss: 0.025 | Tree loss: 2.371 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 029 | Total loss: 2.349 | Reg loss: 0.025 | Tree loss: 2.349 | Accuracy: 0.304688 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 029 | Total loss: 2.362 | Reg loss: 0.025 | Tree loss: 2.362 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 029 | Total loss: 2.363 | Reg loss: 0.025 | Tree loss: 2.363 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 029 | Total loss: 2.385 | Reg loss: 0.025 | Tree loss: 2.385 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 029 | Total loss: 2.372 | Reg loss: 0.025 | Tree loss: 2.372 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 029 | Total loss: 2.343 | Reg loss: 0.025 | Tree loss: 2.343 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 029 | Total loss: 2.362 | Reg loss: 0.025 | Tree loss: 2.362 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 029 | Total loss: 2.394 | Reg loss: 0.025 | Tree loss: 2.394 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 029 | Total loss: 2.382 | Reg loss: 0.025 | Tree loss: 2.382 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 029 | Total loss: 2.359 | Reg loss: 0.025 | Tree loss: 2.359 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 029 | Total loss: 2.351 | Reg loss: 0.025 | Tree loss: 2.351 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 029 | Total loss: 2.314 | Reg loss: 0.025 | Tree loss: 2.314 | Accuracy: 0.333984 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 029 | Total loss: 2.355 | Reg loss: 0.025 | Tree loss: 2.355 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 029 | Total loss: 2.382 | Reg loss: 0.025 | Tree loss: 2.382 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 029 | Total loss: 2.347 | Reg loss: 0.025 | Tree loss: 2.347 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 029 | Total loss: 2.333 | Reg loss: 0.025 | Tree loss: 2.333 | Accuracy: 0.304688 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 029 | Total loss: 2.373 | Reg loss: 0.025 | Tree loss: 2.373 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 029 | Total loss: 2.303 | Reg loss: 0.025 | Tree loss: 2.303 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 029 | Total loss: 2.342 | Reg loss: 0.025 | Tree loss: 2.342 | Accuracy: 0.242188 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 029 | Total loss: 2.358 | Reg loss: 0.025 | Tree loss: 2.358 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 029 | Total loss: 2.330 | Reg loss: 0.025 | Tree loss: 2.330 | Accuracy: 0.259766 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 029 | Total loss: 2.361 | Reg loss: 0.025 | Tree loss: 2.361 | Accuracy: 0.306641 | 0.254 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 029 | Total loss: 2.223 | Reg loss: 0.025 | Tree loss: 2.223 | Accuracy: 0.230769 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 40 | Batch: 000 / 029 | Total loss: 2.306 | Reg loss: 0.025 | Tree loss: 2.306 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 029 | Total loss: 2.354 | Reg loss: 0.025 | Tree loss: 2.354 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 029 | Total loss: 2.394 | Reg loss: 0.025 | Tree loss: 2.394 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 029 | Total loss: 2.372 | Reg loss: 0.025 | Tree loss: 2.372 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 029 | Total loss: 2.331 | Reg loss: 0.025 | Tree loss: 2.331 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 029 | Total loss: 2.312 | Reg loss: 0.025 | Tree loss: 2.312 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 029 | Total loss: 2.310 | Reg loss: 0.025 | Tree loss: 2.310 | Accuracy: 0.312500 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 029 | Total loss: 2.344 | Reg loss: 0.025 | Tree loss: 2.344 | Accuracy: 0.244141 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 029 | Total loss: 2.334 | Reg loss: 0.025 | Tree loss: 2.334 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 029 | Total loss: 2.334 | Reg loss: 0.025 | Tree loss: 2.334 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 029 | Total loss: 2.301 | Reg loss: 0.025 | Tree loss: 2.301 | Accuracy: 0.320312 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 029 | Total loss: 2.331 | Reg loss: 0.025 | Tree loss: 2.331 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 029 | Total loss: 2.344 | Reg loss: 0.025 | Tree loss: 2.344 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 029 | Total loss: 2.326 | Reg loss: 0.025 | Tree loss: 2.326 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 029 | Total loss: 2.316 | Reg loss: 0.025 | Tree loss: 2.316 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 029 | Total loss: 2.321 | Reg loss: 0.025 | Tree loss: 2.321 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 029 | Total loss: 2.311 | Reg loss: 0.025 | Tree loss: 2.311 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 029 | Total loss: 2.350 | Reg loss: 0.025 | Tree loss: 2.350 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 029 | Total loss: 2.333 | Reg loss: 0.025 | Tree loss: 2.333 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 029 | Total loss: 2.312 | Reg loss: 0.025 | Tree loss: 2.312 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 029 | Total loss: 2.335 | Reg loss: 0.025 | Tree loss: 2.335 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 029 | Total loss: 2.340 | Reg loss: 0.025 | Tree loss: 2.340 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 029 | Total loss: 2.303 | Reg loss: 0.025 | Tree loss: 2.303 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 029 | Total loss: 2.327 | Reg loss: 0.025 | Tree loss: 2.327 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 029 | Total loss: 2.360 | Reg loss: 0.025 | Tree loss: 2.360 | Accuracy: 0.244141 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 029 | Total loss: 2.291 | Reg loss: 0.025 | Tree loss: 2.291 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 029 | Total loss: 2.300 | Reg loss: 0.025 | Tree loss: 2.300 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 029 | Total loss: 2.270 | Reg loss: 0.025 | Tree loss: 2.270 | Accuracy: 0.308594 | 0.254 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 029 | Total loss: 2.261 | Reg loss: 0.025 | Tree loss: 2.261 | Accuracy: 0.153846 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 41 | Batch: 000 / 029 | Total loss: 2.346 | Reg loss: 0.025 | Tree loss: 2.346 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 029 | Total loss: 2.309 | Reg loss: 0.025 | Tree loss: 2.309 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 029 | Total loss: 2.338 | Reg loss: 0.025 | Tree loss: 2.338 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 029 | Total loss: 2.288 | Reg loss: 0.025 | Tree loss: 2.288 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 029 | Total loss: 2.344 | Reg loss: 0.025 | Tree loss: 2.344 | Accuracy: 0.242188 | 0.254 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 005 / 029 | Total loss: 2.363 | Reg loss: 0.025 | Tree loss: 2.363 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 029 | Total loss: 2.291 | Reg loss: 0.025 | Tree loss: 2.291 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 029 | Total loss: 2.340 | Reg loss: 0.025 | Tree loss: 2.340 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 029 | Total loss: 2.280 | Reg loss: 0.025 | Tree loss: 2.280 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 029 | Total loss: 2.281 | Reg loss: 0.025 | Tree loss: 2.281 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 029 | Total loss: 2.333 | Reg loss: 0.025 | Tree loss: 2.333 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 029 | Total loss: 2.274 | Reg loss: 0.025 | Tree loss: 2.274 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 029 | Total loss: 2.291 | Reg loss: 0.025 | Tree loss: 2.291 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 029 | Total loss: 2.317 | Reg loss: 0.025 | Tree loss: 2.317 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 029 | Total loss: 2.332 | Reg loss: 0.025 | Tree loss: 2.332 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 029 | Total loss: 2.321 | Reg loss: 0.025 | Tree loss: 2.321 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 029 | Total loss: 2.273 | Reg loss: 0.025 | Tree loss: 2.273 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 029 | Total loss: 2.271 | Reg loss: 0.025 | Tree loss: 2.271 | Accuracy: 0.322266 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 029 | Total loss: 2.332 | Reg loss: 0.025 | Tree loss: 2.332 | Accuracy: 0.251953 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 029 | Total loss: 2.294 | Reg loss: 0.025 | Tree loss: 2.294 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 029 | Total loss: 2.250 | Reg loss: 0.025 | Tree loss: 2.250 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 029 | Total loss: 2.260 | Reg loss: 0.025 | Tree loss: 2.260 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 029 | Total loss: 2.265 | Reg loss: 0.025 | Tree loss: 2.265 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 029 | Total loss: 2.293 | Reg loss: 0.025 | Tree loss: 2.293 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 029 | Total loss: 2.309 | Reg loss: 0.025 | Tree loss: 2.309 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 029 | Total loss: 2.263 | Reg loss: 0.025 | Tree loss: 2.263 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 029 | Total loss: 2.305 | Reg loss: 0.025 | Tree loss: 2.305 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 029 | Total loss: 2.284 | Reg loss: 0.025 | Tree loss: 2.284 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 029 | Total loss: 2.137 | Reg loss: 0.025 | Tree loss: 2.137 | Accuracy: 0.307692 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 42 | Batch: 000 / 029 | Total loss: 2.277 | Reg loss: 0.025 | Tree loss: 2.277 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 029 | Total loss: 2.312 | Reg loss: 0.025 | Tree loss: 2.312 | Accuracy: 0.251953 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 029 | Total loss: 2.294 | Reg loss: 0.025 | Tree loss: 2.294 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 029 | Total loss: 2.295 | Reg loss: 0.025 | Tree loss: 2.295 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 029 | Total loss: 2.273 | Reg loss: 0.025 | Tree loss: 2.273 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 029 | Total loss: 2.272 | Reg loss: 0.025 | Tree loss: 2.272 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 029 | Total loss: 2.271 | Reg loss: 0.025 | Tree loss: 2.271 | Accuracy: 0.326172 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 029 | Total loss: 2.375 | Reg loss: 0.025 | Tree loss: 2.375 | Accuracy: 0.246094 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 029 | Total loss: 2.288 | Reg loss: 0.025 | Tree loss: 2.288 | Accuracy: 0.302734 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 029 | Total loss: 2.273 | Reg loss: 0.025 | Tree loss: 2.273 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 029 | Total loss: 2.312 | Reg loss: 0.025 | Tree loss: 2.312 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 029 | Total loss: 2.294 | Reg loss: 0.025 | Tree loss: 2.294 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 029 | Total loss: 2.306 | Reg loss: 0.025 | Tree loss: 2.306 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 029 | Total loss: 2.309 | Reg loss: 0.025 | Tree loss: 2.309 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 029 | Total loss: 2.268 | Reg loss: 0.025 | Tree loss: 2.268 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 029 | Total loss: 2.232 | Reg loss: 0.025 | Tree loss: 2.232 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 029 | Total loss: 2.259 | Reg loss: 0.025 | Tree loss: 2.259 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 029 | Total loss: 2.289 | Reg loss: 0.025 | Tree loss: 2.289 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 029 | Total loss: 2.275 | Reg loss: 0.025 | Tree loss: 2.275 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 029 | Total loss: 2.278 | Reg loss: 0.025 | Tree loss: 2.278 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 029 | Total loss: 2.284 | Reg loss: 0.025 | Tree loss: 2.284 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 029 | Total loss: 2.245 | Reg loss: 0.025 | Tree loss: 2.245 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 029 | Total loss: 2.246 | Reg loss: 0.025 | Tree loss: 2.246 | Accuracy: 0.248047 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 029 | Total loss: 2.250 | Reg loss: 0.025 | Tree loss: 2.250 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 029 | Total loss: 2.260 | Reg loss: 0.025 | Tree loss: 2.260 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 029 | Total loss: 2.258 | Reg loss: 0.025 | Tree loss: 2.258 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 029 | Total loss: 2.281 | Reg loss: 0.025 | Tree loss: 2.281 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 029 | Total loss: 2.236 | Reg loss: 0.025 | Tree loss: 2.236 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 029 | Total loss: 2.297 | Reg loss: 0.025 | Tree loss: 2.297 | Accuracy: 0.153846 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 43 | Batch: 000 / 029 | Total loss: 2.292 | Reg loss: 0.025 | Tree loss: 2.292 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 029 | Total loss: 2.285 | Reg loss: 0.025 | Tree loss: 2.285 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 029 | Total loss: 2.288 | Reg loss: 0.025 | Tree loss: 2.288 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 029 | Total loss: 2.271 | Reg loss: 0.025 | Tree loss: 2.271 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 029 | Total loss: 2.260 | Reg loss: 0.025 | Tree loss: 2.260 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 029 | Total loss: 2.263 | Reg loss: 0.025 | Tree loss: 2.263 | Accuracy: 0.310547 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 029 | Total loss: 2.267 | Reg loss: 0.025 | Tree loss: 2.267 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 029 | Total loss: 2.296 | Reg loss: 0.025 | Tree loss: 2.296 | Accuracy: 0.234375 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 029 | Total loss: 2.255 | Reg loss: 0.025 | Tree loss: 2.255 | Accuracy: 0.304688 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 029 | Total loss: 2.262 | Reg loss: 0.025 | Tree loss: 2.262 | Accuracy: 0.285156 | 0.254 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 010 / 029 | Total loss: 2.262 | Reg loss: 0.025 | Tree loss: 2.262 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 029 | Total loss: 2.240 | Reg loss: 0.025 | Tree loss: 2.240 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 029 | Total loss: 2.256 | Reg loss: 0.025 | Tree loss: 2.256 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 029 | Total loss: 2.211 | Reg loss: 0.025 | Tree loss: 2.211 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 029 | Total loss: 2.241 | Reg loss: 0.025 | Tree loss: 2.241 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 029 | Total loss: 2.261 | Reg loss: 0.025 | Tree loss: 2.261 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 029 | Total loss: 2.236 | Reg loss: 0.025 | Tree loss: 2.236 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 029 | Total loss: 2.279 | Reg loss: 0.025 | Tree loss: 2.279 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 029 | Total loss: 2.242 | Reg loss: 0.025 | Tree loss: 2.242 | Accuracy: 0.310547 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 029 | Total loss: 2.228 | Reg loss: 0.025 | Tree loss: 2.228 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 029 | Total loss: 2.263 | Reg loss: 0.025 | Tree loss: 2.263 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 029 | Total loss: 2.257 | Reg loss: 0.025 | Tree loss: 2.257 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 029 | Total loss: 2.306 | Reg loss: 0.025 | Tree loss: 2.306 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 029 | Total loss: 2.199 | Reg loss: 0.025 | Tree loss: 2.199 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 029 | Total loss: 2.219 | Reg loss: 0.025 | Tree loss: 2.219 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 029 | Total loss: 2.246 | Reg loss: 0.025 | Tree loss: 2.246 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 029 | Total loss: 2.290 | Reg loss: 0.025 | Tree loss: 2.290 | Accuracy: 0.251953 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 029 | Total loss: 2.271 | Reg loss: 0.025 | Tree loss: 2.271 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 029 | Total loss: 2.239 | Reg loss: 0.025 | Tree loss: 2.239 | Accuracy: 0.230769 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 44 | Batch: 000 / 029 | Total loss: 2.204 | Reg loss: 0.025 | Tree loss: 2.204 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 029 | Total loss: 2.262 | Reg loss: 0.025 | Tree loss: 2.262 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 029 | Total loss: 2.258 | Reg loss: 0.025 | Tree loss: 2.258 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 029 | Total loss: 2.244 | Reg loss: 0.025 | Tree loss: 2.244 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 029 | Total loss: 2.240 | Reg loss: 0.025 | Tree loss: 2.240 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 029 | Total loss: 2.272 | Reg loss: 0.025 | Tree loss: 2.272 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 029 | Total loss: 2.235 | Reg loss: 0.025 | Tree loss: 2.235 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 029 | Total loss: 2.229 | Reg loss: 0.025 | Tree loss: 2.229 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 029 | Total loss: 2.214 | Reg loss: 0.025 | Tree loss: 2.214 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 029 | Total loss: 2.258 | Reg loss: 0.025 | Tree loss: 2.258 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 029 | Total loss: 2.275 | Reg loss: 0.025 | Tree loss: 2.275 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 029 | Total loss: 2.275 | Reg loss: 0.025 | Tree loss: 2.275 | Accuracy: 0.230469 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 029 | Total loss: 2.235 | Reg loss: 0.025 | Tree loss: 2.235 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 029 | Total loss: 2.247 | Reg loss: 0.025 | Tree loss: 2.247 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 029 | Total loss: 2.268 | Reg loss: 0.025 | Tree loss: 2.268 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 029 | Total loss: 2.230 | Reg loss: 0.025 | Tree loss: 2.230 | Accuracy: 0.302734 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 029 | Total loss: 2.224 | Reg loss: 0.025 | Tree loss: 2.224 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 029 | Total loss: 2.246 | Reg loss: 0.025 | Tree loss: 2.246 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 029 | Total loss: 2.221 | Reg loss: 0.025 | Tree loss: 2.221 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 029 | Total loss: 2.254 | Reg loss: 0.025 | Tree loss: 2.254 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 029 | Total loss: 2.238 | Reg loss: 0.025 | Tree loss: 2.238 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 029 | Total loss: 2.237 | Reg loss: 0.025 | Tree loss: 2.237 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 029 | Total loss: 2.217 | Reg loss: 0.025 | Tree loss: 2.217 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 029 | Total loss: 2.233 | Reg loss: 0.025 | Tree loss: 2.233 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 029 | Total loss: 2.208 | Reg loss: 0.025 | Tree loss: 2.208 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 029 | Total loss: 2.195 | Reg loss: 0.025 | Tree loss: 2.195 | Accuracy: 0.306641 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 029 | Total loss: 2.264 | Reg loss: 0.025 | Tree loss: 2.264 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 029 | Total loss: 2.250 | Reg loss: 0.025 | Tree loss: 2.250 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 029 | Total loss: 2.257 | Reg loss: 0.025 | Tree loss: 2.257 | Accuracy: 0.230769 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 45 | Batch: 000 / 029 | Total loss: 2.251 | Reg loss: 0.025 | Tree loss: 2.251 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 029 | Total loss: 2.232 | Reg loss: 0.025 | Tree loss: 2.232 | Accuracy: 0.304688 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 029 | Total loss: 2.195 | Reg loss: 0.025 | Tree loss: 2.195 | Accuracy: 0.324219 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 029 | Total loss: 2.257 | Reg loss: 0.025 | Tree loss: 2.257 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 029 | Total loss: 2.242 | Reg loss: 0.025 | Tree loss: 2.242 | Accuracy: 0.236328 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 029 | Total loss: 2.232 | Reg loss: 0.025 | Tree loss: 2.232 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 029 | Total loss: 2.263 | Reg loss: 0.025 | Tree loss: 2.263 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 029 | Total loss: 2.221 | Reg loss: 0.025 | Tree loss: 2.221 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 029 | Total loss: 2.206 | Reg loss: 0.025 | Tree loss: 2.206 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 029 | Total loss: 2.181 | Reg loss: 0.025 | Tree loss: 2.181 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 029 | Total loss: 2.241 | Reg loss: 0.025 | Tree loss: 2.241 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 029 | Total loss: 2.180 | Reg loss: 0.025 | Tree loss: 2.180 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 029 | Total loss: 2.246 | Reg loss: 0.025 | Tree loss: 2.246 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 029 | Total loss: 2.229 | Reg loss: 0.025 | Tree loss: 2.229 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 029 | Total loss: 2.300 | Reg loss: 0.025 | Tree loss: 2.300 | Accuracy: 0.224609 | 0.254 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Batch: 015 / 029 | Total loss: 2.235 | Reg loss: 0.025 | Tree loss: 2.235 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 029 | Total loss: 2.224 | Reg loss: 0.025 | Tree loss: 2.224 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 029 | Total loss: 2.207 | Reg loss: 0.025 | Tree loss: 2.207 | Accuracy: 0.318359 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 029 | Total loss: 2.221 | Reg loss: 0.025 | Tree loss: 2.221 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 029 | Total loss: 2.216 | Reg loss: 0.025 | Tree loss: 2.216 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 029 | Total loss: 2.160 | Reg loss: 0.025 | Tree loss: 2.160 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 029 | Total loss: 2.209 | Reg loss: 0.025 | Tree loss: 2.209 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 029 | Total loss: 2.230 | Reg loss: 0.025 | Tree loss: 2.230 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 029 | Total loss: 2.224 | Reg loss: 0.025 | Tree loss: 2.224 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 029 | Total loss: 2.218 | Reg loss: 0.025 | Tree loss: 2.218 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 029 | Total loss: 2.244 | Reg loss: 0.025 | Tree loss: 2.244 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 029 | Total loss: 2.221 | Reg loss: 0.025 | Tree loss: 2.221 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 029 | Total loss: 2.192 | Reg loss: 0.025 | Tree loss: 2.192 | Accuracy: 0.316406 | 0.254 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 029 | Total loss: 2.138 | Reg loss: 0.025 | Tree loss: 2.138 | Accuracy: 0.153846 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 46 | Batch: 000 / 029 | Total loss: 2.220 | Reg loss: 0.025 | Tree loss: 2.220 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 029 | Total loss: 2.239 | Reg loss: 0.025 | Tree loss: 2.239 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 029 | Total loss: 2.189 | Reg loss: 0.025 | Tree loss: 2.189 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 029 | Total loss: 2.220 | Reg loss: 0.025 | Tree loss: 2.220 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 029 | Total loss: 2.161 | Reg loss: 0.025 | Tree loss: 2.161 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 029 | Total loss: 2.201 | Reg loss: 0.025 | Tree loss: 2.201 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 029 | Total loss: 2.213 | Reg loss: 0.025 | Tree loss: 2.213 | Accuracy: 0.324219 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 029 | Total loss: 2.215 | Reg loss: 0.025 | Tree loss: 2.215 | Accuracy: 0.251953 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 029 | Total loss: 2.217 | Reg loss: 0.025 | Tree loss: 2.217 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 029 | Total loss: 2.246 | Reg loss: 0.025 | Tree loss: 2.246 | Accuracy: 0.259766 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 029 | Total loss: 2.188 | Reg loss: 0.025 | Tree loss: 2.188 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 029 | Total loss: 2.211 | Reg loss: 0.025 | Tree loss: 2.211 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 029 | Total loss: 2.227 | Reg loss: 0.025 | Tree loss: 2.227 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 029 | Total loss: 2.177 | Reg loss: 0.025 | Tree loss: 2.177 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 029 | Total loss: 2.180 | Reg loss: 0.025 | Tree loss: 2.180 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 029 | Total loss: 2.186 | Reg loss: 0.025 | Tree loss: 2.186 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 029 | Total loss: 2.198 | Reg loss: 0.025 | Tree loss: 2.198 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 029 | Total loss: 2.208 | Reg loss: 0.025 | Tree loss: 2.208 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 029 | Total loss: 2.207 | Reg loss: 0.025 | Tree loss: 2.207 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 029 | Total loss: 2.247 | Reg loss: 0.025 | Tree loss: 2.247 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 029 | Total loss: 2.199 | Reg loss: 0.025 | Tree loss: 2.199 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 029 | Total loss: 2.202 | Reg loss: 0.025 | Tree loss: 2.202 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 029 | Total loss: 2.201 | Reg loss: 0.025 | Tree loss: 2.201 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 029 | Total loss: 2.252 | Reg loss: 0.025 | Tree loss: 2.252 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 029 | Total loss: 2.189 | Reg loss: 0.025 | Tree loss: 2.189 | Accuracy: 0.308594 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 029 | Total loss: 2.209 | Reg loss: 0.025 | Tree loss: 2.209 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 029 | Total loss: 2.232 | Reg loss: 0.025 | Tree loss: 2.232 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 029 | Total loss: 2.234 | Reg loss: 0.025 | Tree loss: 2.234 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 029 | Total loss: 2.340 | Reg loss: 0.025 | Tree loss: 2.340 | Accuracy: 0.153846 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 47 | Batch: 000 / 029 | Total loss: 2.191 | Reg loss: 0.025 | Tree loss: 2.191 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 029 | Total loss: 2.198 | Reg loss: 0.025 | Tree loss: 2.198 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 029 | Total loss: 2.230 | Reg loss: 0.025 | Tree loss: 2.230 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 029 | Total loss: 2.190 | Reg loss: 0.025 | Tree loss: 2.190 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 029 | Total loss: 2.183 | Reg loss: 0.025 | Tree loss: 2.183 | Accuracy: 0.310547 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 029 | Total loss: 2.251 | Reg loss: 0.025 | Tree loss: 2.251 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 029 | Total loss: 2.182 | Reg loss: 0.025 | Tree loss: 2.182 | Accuracy: 0.304688 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 029 | Total loss: 2.192 | Reg loss: 0.025 | Tree loss: 2.192 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 029 | Total loss: 2.213 | Reg loss: 0.025 | Tree loss: 2.213 | Accuracy: 0.240234 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 029 | Total loss: 2.181 | Reg loss: 0.025 | Tree loss: 2.181 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 029 | Total loss: 2.198 | Reg loss: 0.025 | Tree loss: 2.198 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 029 | Total loss: 2.195 | Reg loss: 0.025 | Tree loss: 2.195 | Accuracy: 0.248047 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 029 | Total loss: 2.225 | Reg loss: 0.025 | Tree loss: 2.225 | Accuracy: 0.244141 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 029 | Total loss: 2.220 | Reg loss: 0.025 | Tree loss: 2.220 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 029 | Total loss: 2.200 | Reg loss: 0.025 | Tree loss: 2.200 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 029 | Total loss: 2.249 | Reg loss: 0.025 | Tree loss: 2.249 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 029 | Total loss: 2.165 | Reg loss: 0.025 | Tree loss: 2.165 | Accuracy: 0.316406 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 029 | Total loss: 2.229 | Reg loss: 0.025 | Tree loss: 2.229 | Accuracy: 0.248047 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 029 | Total loss: 2.181 | Reg loss: 0.025 | Tree loss: 2.181 | Accuracy: 0.312500 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 029 | Total loss: 2.178 | Reg loss: 0.025 | Tree loss: 2.178 | Accuracy: 0.271484 | 0.254 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 020 / 029 | Total loss: 2.164 | Reg loss: 0.025 | Tree loss: 2.164 | Accuracy: 0.320312 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 029 | Total loss: 2.226 | Reg loss: 0.025 | Tree loss: 2.226 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 029 | Total loss: 2.166 | Reg loss: 0.025 | Tree loss: 2.166 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 029 | Total loss: 2.209 | Reg loss: 0.025 | Tree loss: 2.209 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 029 | Total loss: 2.205 | Reg loss: 0.025 | Tree loss: 2.205 | Accuracy: 0.240234 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 029 | Total loss: 2.189 | Reg loss: 0.025 | Tree loss: 2.189 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 029 | Total loss: 2.157 | Reg loss: 0.025 | Tree loss: 2.157 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 029 | Total loss: 2.134 | Reg loss: 0.025 | Tree loss: 2.134 | Accuracy: 0.310547 | 0.254 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 029 | Total loss: 2.078 | Reg loss: 0.025 | Tree loss: 2.078 | Accuracy: 0.307692 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 48 | Batch: 000 / 029 | Total loss: 2.220 | Reg loss: 0.025 | Tree loss: 2.220 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 029 | Total loss: 2.157 | Reg loss: 0.025 | Tree loss: 2.157 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 029 | Total loss: 2.201 | Reg loss: 0.025 | Tree loss: 2.201 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 029 | Total loss: 2.163 | Reg loss: 0.025 | Tree loss: 2.163 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 029 | Total loss: 2.203 | Reg loss: 0.025 | Tree loss: 2.203 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 029 | Total loss: 2.222 | Reg loss: 0.025 | Tree loss: 2.222 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 029 | Total loss: 2.189 | Reg loss: 0.025 | Tree loss: 2.189 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 029 | Total loss: 2.183 | Reg loss: 0.025 | Tree loss: 2.183 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 029 | Total loss: 2.191 | Reg loss: 0.025 | Tree loss: 2.191 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 029 | Total loss: 2.215 | Reg loss: 0.025 | Tree loss: 2.215 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 029 | Total loss: 2.169 | Reg loss: 0.025 | Tree loss: 2.169 | Accuracy: 0.259766 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 029 | Total loss: 2.192 | Reg loss: 0.025 | Tree loss: 2.192 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 029 | Total loss: 2.180 | Reg loss: 0.025 | Tree loss: 2.180 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 029 | Total loss: 2.185 | Reg loss: 0.025 | Tree loss: 2.185 | Accuracy: 0.248047 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 029 | Total loss: 2.208 | Reg loss: 0.025 | Tree loss: 2.208 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 029 | Total loss: 2.155 | Reg loss: 0.025 | Tree loss: 2.155 | Accuracy: 0.312500 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 029 | Total loss: 2.220 | Reg loss: 0.025 | Tree loss: 2.220 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 029 | Total loss: 2.167 | Reg loss: 0.025 | Tree loss: 2.167 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 029 | Total loss: 2.209 | Reg loss: 0.025 | Tree loss: 2.209 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 029 | Total loss: 2.147 | Reg loss: 0.025 | Tree loss: 2.147 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 029 | Total loss: 2.184 | Reg loss: 0.025 | Tree loss: 2.184 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 029 | Total loss: 2.178 | Reg loss: 0.025 | Tree loss: 2.178 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.312500 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 029 | Total loss: 2.149 | Reg loss: 0.025 | Tree loss: 2.149 | Accuracy: 0.333984 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 029 | Total loss: 2.162 | Reg loss: 0.025 | Tree loss: 2.162 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 029 | Total loss: 2.186 | Reg loss: 0.025 | Tree loss: 2.186 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 029 | Total loss: 2.177 | Reg loss: 0.025 | Tree loss: 2.177 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 029 | Total loss: 2.195 | Reg loss: 0.025 | Tree loss: 2.195 | Accuracy: 0.214844 | 0.254 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 029 | Total loss: 2.353 | Reg loss: 0.025 | Tree loss: 2.353 | Accuracy: 0.153846 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 49 | Batch: 000 / 029 | Total loss: 2.159 | Reg loss: 0.025 | Tree loss: 2.159 | Accuracy: 0.322266 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 029 | Total loss: 2.192 | Reg loss: 0.025 | Tree loss: 2.192 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 029 | Total loss: 2.187 | Reg loss: 0.025 | Tree loss: 2.187 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 029 | Total loss: 2.202 | Reg loss: 0.025 | Tree loss: 2.202 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 029 | Total loss: 2.200 | Reg loss: 0.025 | Tree loss: 2.200 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 029 | Total loss: 2.198 | Reg loss: 0.025 | Tree loss: 2.198 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 029 | Total loss: 2.183 | Reg loss: 0.025 | Tree loss: 2.183 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 029 | Total loss: 2.158 | Reg loss: 0.025 | Tree loss: 2.158 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 029 | Total loss: 2.172 | Reg loss: 0.025 | Tree loss: 2.172 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 029 | Total loss: 2.193 | Reg loss: 0.025 | Tree loss: 2.193 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 029 | Total loss: 2.156 | Reg loss: 0.025 | Tree loss: 2.156 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 029 | Total loss: 2.208 | Reg loss: 0.025 | Tree loss: 2.208 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 029 | Total loss: 2.120 | Reg loss: 0.025 | Tree loss: 2.120 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 029 | Total loss: 2.205 | Reg loss: 0.025 | Tree loss: 2.205 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 029 | Total loss: 2.150 | Reg loss: 0.025 | Tree loss: 2.150 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 029 | Total loss: 2.166 | Reg loss: 0.025 | Tree loss: 2.166 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 029 | Total loss: 2.166 | Reg loss: 0.025 | Tree loss: 2.166 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 029 | Total loss: 2.165 | Reg loss: 0.025 | Tree loss: 2.165 | Accuracy: 0.308594 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 029 | Total loss: 2.176 | Reg loss: 0.025 | Tree loss: 2.176 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 029 | Total loss: 2.182 | Reg loss: 0.025 | Tree loss: 2.182 | Accuracy: 0.259766 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 029 | Total loss: 2.187 | Reg loss: 0.025 | Tree loss: 2.187 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 029 | Total loss: 2.187 | Reg loss: 0.025 | Tree loss: 2.187 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 029 | Total loss: 2.130 | Reg loss: 0.025 | Tree loss: 2.130 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 029 | Total loss: 2.153 | Reg loss: 0.025 | Tree loss: 2.153 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 029 | Total loss: 2.195 | Reg loss: 0.025 | Tree loss: 2.195 | Accuracy: 0.238281 | 0.254 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 025 / 029 | Total loss: 2.167 | Reg loss: 0.025 | Tree loss: 2.167 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 029 | Total loss: 2.171 | Reg loss: 0.025 | Tree loss: 2.171 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 029 | Total loss: 2.026 | Reg loss: 0.025 | Tree loss: 2.026 | Accuracy: 0.307692 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 50 | Batch: 000 / 029 | Total loss: 2.176 | Reg loss: 0.025 | Tree loss: 2.176 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 029 | Total loss: 2.174 | Reg loss: 0.025 | Tree loss: 2.174 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 029 | Total loss: 2.168 | Reg loss: 0.025 | Tree loss: 2.168 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 029 | Total loss: 2.179 | Reg loss: 0.025 | Tree loss: 2.179 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 029 | Total loss: 2.134 | Reg loss: 0.025 | Tree loss: 2.134 | Accuracy: 0.304688 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 029 | Total loss: 2.174 | Reg loss: 0.025 | Tree loss: 2.174 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 029 | Total loss: 2.142 | Reg loss: 0.025 | Tree loss: 2.142 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 029 | Total loss: 2.157 | Reg loss: 0.025 | Tree loss: 2.157 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 029 | Total loss: 2.167 | Reg loss: 0.025 | Tree loss: 2.167 | Accuracy: 0.259766 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 029 | Total loss: 2.155 | Reg loss: 0.025 | Tree loss: 2.155 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 029 | Total loss: 2.204 | Reg loss: 0.025 | Tree loss: 2.204 | Accuracy: 0.251953 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 029 | Total loss: 2.141 | Reg loss: 0.025 | Tree loss: 2.141 | Accuracy: 0.320312 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 029 | Total loss: 2.202 | Reg loss: 0.025 | Tree loss: 2.202 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.318359 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 029 | Total loss: 2.160 | Reg loss: 0.025 | Tree loss: 2.160 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 029 | Total loss: 2.158 | Reg loss: 0.025 | Tree loss: 2.158 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 029 | Total loss: 2.121 | Reg loss: 0.025 | Tree loss: 2.121 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 029 | Total loss: 2.147 | Reg loss: 0.025 | Tree loss: 2.147 | Accuracy: 0.248047 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 029 | Total loss: 2.191 | Reg loss: 0.025 | Tree loss: 2.191 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 029 | Total loss: 2.143 | Reg loss: 0.025 | Tree loss: 2.143 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 029 | Total loss: 2.182 | Reg loss: 0.025 | Tree loss: 2.182 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 029 | Total loss: 2.159 | Reg loss: 0.025 | Tree loss: 2.159 | Accuracy: 0.314453 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 029 | Total loss: 2.227 | Reg loss: 0.025 | Tree loss: 2.227 | Accuracy: 0.228516 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 029 | Total loss: 2.162 | Reg loss: 0.025 | Tree loss: 2.162 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 029 | Total loss: 2.198 | Reg loss: 0.025 | Tree loss: 2.198 | Accuracy: 0.240234 | 0.254 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 029 | Total loss: 2.216 | Reg loss: 0.025 | Tree loss: 2.216 | Accuracy: 0.384615 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 51 | Batch: 000 / 029 | Total loss: 2.146 | Reg loss: 0.025 | Tree loss: 2.146 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 029 | Total loss: 2.184 | Reg loss: 0.025 | Tree loss: 2.184 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 029 | Total loss: 2.147 | Reg loss: 0.025 | Tree loss: 2.147 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 029 | Total loss: 2.198 | Reg loss: 0.025 | Tree loss: 2.198 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 029 | Total loss: 2.145 | Reg loss: 0.025 | Tree loss: 2.145 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 029 | Total loss: 2.141 | Reg loss: 0.025 | Tree loss: 2.141 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 029 | Total loss: 2.142 | Reg loss: 0.025 | Tree loss: 2.142 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 029 | Total loss: 2.171 | Reg loss: 0.025 | Tree loss: 2.171 | Accuracy: 0.255859 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 029 | Total loss: 2.137 | Reg loss: 0.025 | Tree loss: 2.137 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 029 | Total loss: 2.175 | Reg loss: 0.025 | Tree loss: 2.175 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 029 | Total loss: 2.129 | Reg loss: 0.025 | Tree loss: 2.129 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 029 | Total loss: 2.136 | Reg loss: 0.025 | Tree loss: 2.136 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 029 | Total loss: 2.219 | Reg loss: 0.025 | Tree loss: 2.219 | Accuracy: 0.248047 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 029 | Total loss: 2.105 | Reg loss: 0.025 | Tree loss: 2.105 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 029 | Total loss: 2.127 | Reg loss: 0.025 | Tree loss: 2.127 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 029 | Total loss: 2.188 | Reg loss: 0.025 | Tree loss: 2.188 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 029 | Total loss: 2.168 | Reg loss: 0.025 | Tree loss: 2.168 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 029 | Total loss: 2.157 | Reg loss: 0.025 | Tree loss: 2.157 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 029 | Total loss: 2.158 | Reg loss: 0.025 | Tree loss: 2.158 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 029 | Total loss: 2.132 | Reg loss: 0.025 | Tree loss: 2.132 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 029 | Total loss: 2.174 | Reg loss: 0.025 | Tree loss: 2.174 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 029 | Total loss: 2.130 | Reg loss: 0.025 | Tree loss: 2.130 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 029 | Total loss: 2.119 | Reg loss: 0.025 | Tree loss: 2.119 | Accuracy: 0.294922 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 029 | Total loss: 2.174 | Reg loss: 0.025 | Tree loss: 2.174 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 029 | Total loss: 2.170 | Reg loss: 0.025 | Tree loss: 2.170 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 029 | Total loss: 2.130 | Reg loss: 0.025 | Tree loss: 2.130 | Accuracy: 0.308594 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 029 | Total loss: 2.196 | Reg loss: 0.025 | Tree loss: 2.196 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 029 | Total loss: 1.912 | Reg loss: 0.025 | Tree loss: 1.912 | Accuracy: 0.538462 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Batch: 000 / 029 | Total loss: 2.146 | Reg loss: 0.025 | Tree loss: 2.146 | Accuracy: 0.312500 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 029 | Total loss: 2.205 | Reg loss: 0.025 | Tree loss: 2.205 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 029 | Total loss: 2.127 | Reg loss: 0.025 | Tree loss: 2.127 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 029 | Total loss: 2.154 | Reg loss: 0.025 | Tree loss: 2.154 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 029 | Total loss: 2.166 | Reg loss: 0.025 | Tree loss: 2.166 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 029 | Total loss: 2.136 | Reg loss: 0.025 | Tree loss: 2.136 | Accuracy: 0.257812 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 029 | Total loss: 2.150 | Reg loss: 0.025 | Tree loss: 2.150 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 029 | Total loss: 2.169 | Reg loss: 0.025 | Tree loss: 2.169 | Accuracy: 0.259766 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.242188 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 029 | Total loss: 2.190 | Reg loss: 0.025 | Tree loss: 2.190 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 029 | Total loss: 2.155 | Reg loss: 0.025 | Tree loss: 2.155 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 029 | Total loss: 2.166 | Reg loss: 0.025 | Tree loss: 2.166 | Accuracy: 0.281250 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 029 | Total loss: 2.129 | Reg loss: 0.025 | Tree loss: 2.129 | Accuracy: 0.277344 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 029 | Total loss: 2.147 | Reg loss: 0.025 | Tree loss: 2.147 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 029 | Total loss: 2.144 | Reg loss: 0.025 | Tree loss: 2.144 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 029 | Total loss: 2.175 | Reg loss: 0.025 | Tree loss: 2.175 | Accuracy: 0.306641 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 029 | Total loss: 2.110 | Reg loss: 0.025 | Tree loss: 2.110 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 029 | Total loss: 2.130 | Reg loss: 0.025 | Tree loss: 2.130 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 029 | Total loss: 2.099 | Reg loss: 0.025 | Tree loss: 2.099 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 029 | Total loss: 2.225 | Reg loss: 0.025 | Tree loss: 2.225 | Accuracy: 0.220703 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 029 | Total loss: 2.172 | Reg loss: 0.025 | Tree loss: 2.172 | Accuracy: 0.253906 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 029 | Total loss: 2.096 | Reg loss: 0.025 | Tree loss: 2.096 | Accuracy: 0.306641 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 029 | Total loss: 2.157 | Reg loss: 0.025 | Tree loss: 2.157 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 029 | Total loss: 2.105 | Reg loss: 0.025 | Tree loss: 2.105 | Accuracy: 0.287109 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 029 | Total loss: 2.098 | Reg loss: 0.025 | Tree loss: 2.098 | Accuracy: 0.310547 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 029 | Total loss: 2.124 | Reg loss: 0.025 | Tree loss: 2.124 | Accuracy: 0.292969 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 029 | Total loss: 2.137 | Reg loss: 0.025 | Tree loss: 2.137 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 029 | Total loss: 2.118 | Reg loss: 0.025 | Tree loss: 2.118 | Accuracy: 0.230769 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 53 | Batch: 000 / 029 | Total loss: 2.160 | Reg loss: 0.025 | Tree loss: 2.160 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.261719 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 029 | Total loss: 2.127 | Reg loss: 0.025 | Tree loss: 2.127 | Accuracy: 0.285156 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.324219 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 029 | Total loss: 2.138 | Reg loss: 0.025 | Tree loss: 2.138 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 029 | Total loss: 2.137 | Reg loss: 0.025 | Tree loss: 2.137 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 029 | Total loss: 2.124 | Reg loss: 0.025 | Tree loss: 2.124 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 029 | Total loss: 2.175 | Reg loss: 0.025 | Tree loss: 2.175 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 029 | Total loss: 2.129 | Reg loss: 0.025 | Tree loss: 2.129 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 029 | Total loss: 2.145 | Reg loss: 0.025 | Tree loss: 2.145 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 029 | Total loss: 2.134 | Reg loss: 0.025 | Tree loss: 2.134 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.300781 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 029 | Total loss: 2.125 | Reg loss: 0.025 | Tree loss: 2.125 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 029 | Total loss: 2.153 | Reg loss: 0.025 | Tree loss: 2.153 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 029 | Total loss: 2.138 | Reg loss: 0.025 | Tree loss: 2.138 | Accuracy: 0.296875 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 029 | Total loss: 2.173 | Reg loss: 0.025 | Tree loss: 2.173 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 029 | Total loss: 2.131 | Reg loss: 0.025 | Tree loss: 2.131 | Accuracy: 0.310547 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 029 | Total loss: 2.126 | Reg loss: 0.025 | Tree loss: 2.126 | Accuracy: 0.279297 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 029 | Total loss: 2.190 | Reg loss: 0.025 | Tree loss: 2.190 | Accuracy: 0.238281 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 029 | Total loss: 2.157 | Reg loss: 0.025 | Tree loss: 2.157 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.240234 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 029 | Total loss: 2.111 | Reg loss: 0.025 | Tree loss: 2.111 | Accuracy: 0.306641 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 029 | Total loss: 2.111 | Reg loss: 0.025 | Tree loss: 2.111 | Accuracy: 0.298828 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 029 | Total loss: 2.111 | Reg loss: 0.025 | Tree loss: 2.111 | Accuracy: 0.267578 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 029 | Total loss: 2.135 | Reg loss: 0.025 | Tree loss: 2.135 | Accuracy: 0.265625 | 0.254 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 029 | Total loss: 2.271 | Reg loss: 0.025 | Tree loss: 2.271 | Accuracy: 0.230769 | 0.254 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 54 | Batch: 000 / 029 | Total loss: 2.132 | Reg loss: 0.025 | Tree loss: 2.132 | Accuracy: 0.283203 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 029 | Total loss: 2.171 | Reg loss: 0.025 | Tree loss: 2.171 | Accuracy: 0.273438 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 029 | Total loss: 2.106 | Reg loss: 0.025 | Tree loss: 2.106 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 029 | Total loss: 2.126 | Reg loss: 0.025 | Tree loss: 2.126 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 029 | Total loss: 2.171 | Reg loss: 0.025 | Tree loss: 2.171 | Accuracy: 0.275391 | 0.254 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 005 / 029 | Total loss: 2.142 | Reg loss: 0.025 | Tree loss: 2.142 | Accuracy: 0.263672 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 029 | Total loss: 2.168 | Reg loss: 0.025 | Tree loss: 2.168 | Accuracy: 0.250000 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 029 | Total loss: 2.142 | Reg loss: 0.025 | Tree loss: 2.142 | Accuracy: 0.289062 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 029 | Total loss: 2.114 | Reg loss: 0.025 | Tree loss: 2.114 | Accuracy: 0.291016 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 029 | Total loss: 2.157 | Reg loss: 0.025 | Tree loss: 2.157 | Accuracy: 0.240234 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 029 | Total loss: 2.178 | Reg loss: 0.025 | Tree loss: 2.178 | Accuracy: 0.275391 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 029 | Total loss: 2.135 | Reg loss: 0.025 | Tree loss: 2.135 | Accuracy: 0.269531 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 029 | Total loss: 2.123 | Reg loss: 0.025 | Tree loss: 2.123 | Accuracy: 0.271484 | 0.254 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 029 | Total loss: 2.088 | Reg loss: 0.025 | Tree loss: 2.088 | Accuracy: 0.316406 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 029 | Total loss: 2.060 | Reg loss: 0.025 | Tree loss: 2.060 | Accuracy: 0.349609 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 029 | Total loss: 2.123 | Reg loss: 0.025 | Tree loss: 2.123 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 029 | Total loss: 2.139 | Reg loss: 0.025 | Tree loss: 2.139 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 029 | Total loss: 2.164 | Reg loss: 0.025 | Tree loss: 2.164 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 029 | Total loss: 2.140 | Reg loss: 0.025 | Tree loss: 2.140 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 029 | Total loss: 2.114 | Reg loss: 0.025 | Tree loss: 2.114 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 029 | Total loss: 2.080 | Reg loss: 0.025 | Tree loss: 2.080 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 029 | Total loss: 2.150 | Reg loss: 0.025 | Tree loss: 2.150 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 029 | Total loss: 2.110 | Reg loss: 0.025 | Tree loss: 2.110 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 029 | Total loss: 2.117 | Reg loss: 0.025 | Tree loss: 2.117 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 029 | Total loss: 2.171 | Reg loss: 0.025 | Tree loss: 2.171 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 029 | Total loss: 2.077 | Reg loss: 0.025 | Tree loss: 2.077 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 029 | Total loss: 2.158 | Reg loss: 0.025 | Tree loss: 2.158 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 029 | Total loss: 1.925 | Reg loss: 0.025 | Tree loss: 1.925 | Accuracy: 0.384615 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 55 | Batch: 000 / 029 | Total loss: 2.127 | Reg loss: 0.025 | Tree loss: 2.127 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 029 | Total loss: 2.108 | Reg loss: 0.025 | Tree loss: 2.108 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 029 | Total loss: 2.106 | Reg loss: 0.025 | Tree loss: 2.106 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 029 | Total loss: 2.104 | Reg loss: 0.025 | Tree loss: 2.104 | Accuracy: 0.320312 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 029 | Total loss: 2.143 | Reg loss: 0.025 | Tree loss: 2.143 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 029 | Total loss: 2.188 | Reg loss: 0.025 | Tree loss: 2.188 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 029 | Total loss: 2.150 | Reg loss: 0.025 | Tree loss: 2.150 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 029 | Total loss: 2.091 | Reg loss: 0.025 | Tree loss: 2.091 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 029 | Total loss: 2.153 | Reg loss: 0.025 | Tree loss: 2.153 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 029 | Total loss: 2.129 | Reg loss: 0.025 | Tree loss: 2.129 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 029 | Total loss: 2.116 | Reg loss: 0.025 | Tree loss: 2.116 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 029 | Total loss: 2.156 | Reg loss: 0.025 | Tree loss: 2.156 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 029 | Total loss: 2.094 | Reg loss: 0.025 | Tree loss: 2.094 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 029 | Total loss: 2.117 | Reg loss: 0.025 | Tree loss: 2.117 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 029 | Total loss: 2.118 | Reg loss: 0.025 | Tree loss: 2.118 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 029 | Total loss: 2.082 | Reg loss: 0.025 | Tree loss: 2.082 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 029 | Total loss: 2.146 | Reg loss: 0.025 | Tree loss: 2.146 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 029 | Total loss: 2.136 | Reg loss: 0.025 | Tree loss: 2.136 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 029 | Total loss: 2.162 | Reg loss: 0.025 | Tree loss: 2.162 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 029 | Total loss: 2.121 | Reg loss: 0.025 | Tree loss: 2.121 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 029 | Total loss: 2.117 | Reg loss: 0.025 | Tree loss: 2.117 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 029 | Total loss: 2.153 | Reg loss: 0.025 | Tree loss: 2.153 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 029 | Total loss: 2.108 | Reg loss: 0.025 | Tree loss: 2.108 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 029 | Total loss: 2.100 | Reg loss: 0.025 | Tree loss: 2.100 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 029 | Total loss: 2.101 | Reg loss: 0.025 | Tree loss: 2.101 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 029 | Total loss: 2.134 | Reg loss: 0.025 | Tree loss: 2.134 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 029 | Total loss: 2.119 | Reg loss: 0.025 | Tree loss: 2.119 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 029 | Total loss: 1.931 | Reg loss: 0.025 | Tree loss: 1.931 | Accuracy: 0.384615 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 56 | Batch: 000 / 029 | Total loss: 2.061 | Reg loss: 0.025 | Tree loss: 2.061 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 029 | Total loss: 2.096 | Reg loss: 0.025 | Tree loss: 2.096 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 029 | Total loss: 2.109 | Reg loss: 0.025 | Tree loss: 2.109 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 029 | Total loss: 2.155 | Reg loss: 0.025 | Tree loss: 2.155 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 029 | Total loss: 2.115 | Reg loss: 0.025 | Tree loss: 2.115 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 029 | Total loss: 2.101 | Reg loss: 0.025 | Tree loss: 2.101 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 029 | Total loss: 2.125 | Reg loss: 0.025 | Tree loss: 2.125 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 029 | Total loss: 2.092 | Reg loss: 0.025 | Tree loss: 2.092 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 029 | Total loss: 2.101 | Reg loss: 0.025 | Tree loss: 2.101 | Accuracy: 0.271484 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 010 / 029 | Total loss: 2.174 | Reg loss: 0.025 | Tree loss: 2.174 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 029 | Total loss: 2.106 | Reg loss: 0.025 | Tree loss: 2.106 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 029 | Total loss: 2.098 | Reg loss: 0.025 | Tree loss: 2.098 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 029 | Total loss: 2.114 | Reg loss: 0.025 | Tree loss: 2.114 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 029 | Total loss: 2.156 | Reg loss: 0.025 | Tree loss: 2.156 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 029 | Total loss: 2.100 | Reg loss: 0.025 | Tree loss: 2.100 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 029 | Total loss: 2.158 | Reg loss: 0.025 | Tree loss: 2.158 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 029 | Total loss: 2.083 | Reg loss: 0.025 | Tree loss: 2.083 | Accuracy: 0.306641 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 029 | Total loss: 2.145 | Reg loss: 0.025 | Tree loss: 2.145 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 029 | Total loss: 2.136 | Reg loss: 0.025 | Tree loss: 2.136 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 029 | Total loss: 2.114 | Reg loss: 0.025 | Tree loss: 2.114 | Accuracy: 0.306641 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 029 | Total loss: 2.107 | Reg loss: 0.025 | Tree loss: 2.107 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 029 | Total loss: 2.117 | Reg loss: 0.025 | Tree loss: 2.117 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 029 | Total loss: 2.143 | Reg loss: 0.025 | Tree loss: 2.143 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 029 | Total loss: 2.091 | Reg loss: 0.025 | Tree loss: 2.091 | Accuracy: 0.306641 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 029 | Total loss: 2.139 | Reg loss: 0.025 | Tree loss: 2.139 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 029 | Total loss: 2.145 | Reg loss: 0.025 | Tree loss: 2.145 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 029 | Total loss: 2.268 | Reg loss: 0.025 | Tree loss: 2.268 | Accuracy: 0.000000 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 57 | Batch: 000 / 029 | Total loss: 2.169 | Reg loss: 0.025 | Tree loss: 2.169 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 029 | Total loss: 2.154 | Reg loss: 0.025 | Tree loss: 2.154 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 029 | Total loss: 2.146 | Reg loss: 0.025 | Tree loss: 2.146 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 029 | Total loss: 2.113 | Reg loss: 0.025 | Tree loss: 2.113 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 029 | Total loss: 2.113 | Reg loss: 0.025 | Tree loss: 2.113 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 029 | Total loss: 2.126 | Reg loss: 0.025 | Tree loss: 2.126 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 029 | Total loss: 2.110 | Reg loss: 0.025 | Tree loss: 2.110 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 029 | Total loss: 2.160 | Reg loss: 0.025 | Tree loss: 2.160 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 029 | Total loss: 2.110 | Reg loss: 0.025 | Tree loss: 2.110 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 029 | Total loss: 2.116 | Reg loss: 0.025 | Tree loss: 2.116 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 029 | Total loss: 2.099 | Reg loss: 0.025 | Tree loss: 2.099 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 029 | Total loss: 2.092 | Reg loss: 0.025 | Tree loss: 2.092 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 029 | Total loss: 2.140 | Reg loss: 0.025 | Tree loss: 2.140 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 029 | Total loss: 2.135 | Reg loss: 0.025 | Tree loss: 2.135 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 029 | Total loss: 2.035 | Reg loss: 0.025 | Tree loss: 2.035 | Accuracy: 0.324219 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 029 | Total loss: 2.076 | Reg loss: 0.025 | Tree loss: 2.076 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 029 | Total loss: 2.108 | Reg loss: 0.025 | Tree loss: 2.108 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 029 | Total loss: 2.079 | Reg loss: 0.025 | Tree loss: 2.079 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 029 | Total loss: 2.113 | Reg loss: 0.025 | Tree loss: 2.113 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 029 | Total loss: 2.073 | Reg loss: 0.025 | Tree loss: 2.073 | Accuracy: 0.320312 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 029 | Total loss: 2.147 | Reg loss: 0.025 | Tree loss: 2.147 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 029 | Total loss: 2.107 | Reg loss: 0.025 | Tree loss: 2.107 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 029 | Total loss: 2.073 | Reg loss: 0.025 | Tree loss: 2.073 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 029 | Total loss: 2.087 | Reg loss: 0.025 | Tree loss: 2.087 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 029 | Total loss: 2.141 | Reg loss: 0.025 | Tree loss: 2.141 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 029 | Total loss: 2.125 | Reg loss: 0.025 | Tree loss: 2.125 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 029 | Total loss: 2.059 | Reg loss: 0.025 | Tree loss: 2.059 | Accuracy: 0.307692 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 58 | Batch: 000 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 029 | Total loss: 2.113 | Reg loss: 0.025 | Tree loss: 2.113 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 029 | Total loss: 2.150 | Reg loss: 0.025 | Tree loss: 2.150 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 029 | Total loss: 2.144 | Reg loss: 0.025 | Tree loss: 2.144 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 029 | Total loss: 2.086 | Reg loss: 0.025 | Tree loss: 2.086 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 029 | Total loss: 2.105 | Reg loss: 0.025 | Tree loss: 2.105 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 029 | Total loss: 2.063 | Reg loss: 0.025 | Tree loss: 2.063 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 029 | Total loss: 2.137 | Reg loss: 0.025 | Tree loss: 2.137 | Accuracy: 0.232422 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 029 | Total loss: 2.088 | Reg loss: 0.025 | Tree loss: 2.088 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 029 | Total loss: 2.135 | Reg loss: 0.025 | Tree loss: 2.135 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 029 | Total loss: 2.122 | Reg loss: 0.025 | Tree loss: 2.122 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 029 | Total loss: 2.090 | Reg loss: 0.025 | Tree loss: 2.090 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 029 | Total loss: 2.143 | Reg loss: 0.025 | Tree loss: 2.143 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 029 | Total loss: 2.065 | Reg loss: 0.025 | Tree loss: 2.065 | Accuracy: 0.296875 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 015 / 029 | Total loss: 2.106 | Reg loss: 0.025 | Tree loss: 2.106 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 029 | Total loss: 2.151 | Reg loss: 0.025 | Tree loss: 2.151 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.306641 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 029 | Total loss: 2.126 | Reg loss: 0.025 | Tree loss: 2.126 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 029 | Total loss: 2.090 | Reg loss: 0.025 | Tree loss: 2.090 | Accuracy: 0.318359 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 029 | Total loss: 2.114 | Reg loss: 0.025 | Tree loss: 2.114 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 029 | Total loss: 2.121 | Reg loss: 0.025 | Tree loss: 2.121 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 029 | Total loss: 2.056 | Reg loss: 0.025 | Tree loss: 2.056 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 029 | Total loss: 2.084 | Reg loss: 0.025 | Tree loss: 2.084 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 029 | Total loss: 2.085 | Reg loss: 0.025 | Tree loss: 2.085 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 029 | Total loss: 2.079 | Reg loss: 0.025 | Tree loss: 2.079 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 029 | Total loss: 2.092 | Reg loss: 0.025 | Tree loss: 2.092 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 029 | Total loss: 2.081 | Reg loss: 0.025 | Tree loss: 2.081 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 029 | Total loss: 2.285 | Reg loss: 0.025 | Tree loss: 2.285 | Accuracy: 0.230769 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 59 | Batch: 000 / 029 | Total loss: 2.078 | Reg loss: 0.025 | Tree loss: 2.078 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 029 | Total loss: 2.056 | Reg loss: 0.025 | Tree loss: 2.056 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 029 | Total loss: 2.067 | Reg loss: 0.025 | Tree loss: 2.067 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 029 | Total loss: 2.073 | Reg loss: 0.025 | Tree loss: 2.073 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 029 | Total loss: 2.142 | Reg loss: 0.025 | Tree loss: 2.142 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 029 | Total loss: 2.184 | Reg loss: 0.025 | Tree loss: 2.184 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 029 | Total loss: 2.116 | Reg loss: 0.025 | Tree loss: 2.116 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 029 | Total loss: 2.143 | Reg loss: 0.025 | Tree loss: 2.143 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 029 | Total loss: 2.107 | Reg loss: 0.025 | Tree loss: 2.107 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 029 | Total loss: 2.094 | Reg loss: 0.025 | Tree loss: 2.094 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 029 | Total loss: 2.108 | Reg loss: 0.025 | Tree loss: 2.108 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 029 | Total loss: 2.075 | Reg loss: 0.025 | Tree loss: 2.075 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 029 | Total loss: 2.129 | Reg loss: 0.025 | Tree loss: 2.129 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 029 | Total loss: 2.126 | Reg loss: 0.025 | Tree loss: 2.126 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 029 | Total loss: 2.130 | Reg loss: 0.025 | Tree loss: 2.130 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 029 | Total loss: 2.063 | Reg loss: 0.025 | Tree loss: 2.063 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 029 | Total loss: 2.104 | Reg loss: 0.025 | Tree loss: 2.104 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 029 | Total loss: 2.107 | Reg loss: 0.025 | Tree loss: 2.107 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 029 | Total loss: 2.147 | Reg loss: 0.025 | Tree loss: 2.147 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 029 | Total loss: 2.114 | Reg loss: 0.025 | Tree loss: 2.114 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 029 | Total loss: 2.075 | Reg loss: 0.025 | Tree loss: 2.075 | Accuracy: 0.322266 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 029 | Total loss: 2.086 | Reg loss: 0.025 | Tree loss: 2.086 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 029 | Total loss: 2.090 | Reg loss: 0.025 | Tree loss: 2.090 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 029 | Total loss: 2.097 | Reg loss: 0.025 | Tree loss: 2.097 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 029 | Total loss: 2.067 | Reg loss: 0.025 | Tree loss: 2.067 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 029 | Total loss: 2.119 | Reg loss: 0.025 | Tree loss: 2.119 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 029 | Total loss: 1.966 | Reg loss: 0.025 | Tree loss: 1.966 | Accuracy: 0.307692 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 60 | Batch: 000 / 029 | Total loss: 2.128 | Reg loss: 0.025 | Tree loss: 2.128 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 029 | Total loss: 2.101 | Reg loss: 0.025 | Tree loss: 2.101 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 029 | Total loss: 2.099 | Reg loss: 0.025 | Tree loss: 2.099 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 029 | Total loss: 2.079 | Reg loss: 0.025 | Tree loss: 2.079 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 029 | Total loss: 2.098 | Reg loss: 0.025 | Tree loss: 2.098 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 029 | Total loss: 2.097 | Reg loss: 0.025 | Tree loss: 2.097 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 029 | Total loss: 2.132 | Reg loss: 0.025 | Tree loss: 2.132 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 029 | Total loss: 2.081 | Reg loss: 0.025 | Tree loss: 2.081 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 029 | Total loss: 2.137 | Reg loss: 0.025 | Tree loss: 2.137 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 029 | Total loss: 2.141 | Reg loss: 0.025 | Tree loss: 2.141 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 029 | Total loss: 2.108 | Reg loss: 0.025 | Tree loss: 2.108 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 029 | Total loss: 2.109 | Reg loss: 0.025 | Tree loss: 2.109 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 029 | Total loss: 2.071 | Reg loss: 0.025 | Tree loss: 2.071 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 029 | Total loss: 2.076 | Reg loss: 0.025 | Tree loss: 2.076 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 029 | Total loss: 2.083 | Reg loss: 0.025 | Tree loss: 2.083 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 029 | Total loss: 2.104 | Reg loss: 0.025 | Tree loss: 2.104 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 029 | Total loss: 2.086 | Reg loss: 0.025 | Tree loss: 2.086 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 029 | Total loss: 2.108 | Reg loss: 0.025 | Tree loss: 2.108 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 029 | Total loss: 2.082 | Reg loss: 0.025 | Tree loss: 2.082 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 029 | Total loss: 2.048 | Reg loss: 0.025 | Tree loss: 2.048 | Accuracy: 0.306641 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 020 / 029 | Total loss: 2.115 | Reg loss: 0.025 | Tree loss: 2.115 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 029 | Total loss: 2.152 | Reg loss: 0.025 | Tree loss: 2.152 | Accuracy: 0.246094 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 029 | Total loss: 2.111 | Reg loss: 0.025 | Tree loss: 2.111 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 029 | Total loss: 2.077 | Reg loss: 0.025 | Tree loss: 2.077 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 029 | Total loss: 2.037 | Reg loss: 0.025 | Tree loss: 2.037 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 029 | Total loss: 2.099 | Reg loss: 0.025 | Tree loss: 2.099 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 029 | Total loss: 2.104 | Reg loss: 0.025 | Tree loss: 2.104 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 029 | Total loss: 2.112 | Reg loss: 0.025 | Tree loss: 2.112 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 029 | Total loss: 2.193 | Reg loss: 0.025 | Tree loss: 2.193 | Accuracy: 0.076923 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 61 | Batch: 000 / 029 | Total loss: 2.117 | Reg loss: 0.025 | Tree loss: 2.117 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 029 | Total loss: 2.056 | Reg loss: 0.025 | Tree loss: 2.056 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 029 | Total loss: 2.137 | Reg loss: 0.025 | Tree loss: 2.137 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 029 | Total loss: 2.100 | Reg loss: 0.025 | Tree loss: 2.100 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 029 | Total loss: 2.113 | Reg loss: 0.025 | Tree loss: 2.113 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 029 | Total loss: 2.100 | Reg loss: 0.025 | Tree loss: 2.100 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 029 | Total loss: 2.082 | Reg loss: 0.025 | Tree loss: 2.082 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 029 | Total loss: 2.107 | Reg loss: 0.025 | Tree loss: 2.107 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 029 | Total loss: 2.093 | Reg loss: 0.025 | Tree loss: 2.093 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 029 | Total loss: 2.102 | Reg loss: 0.025 | Tree loss: 2.102 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 029 | Total loss: 2.057 | Reg loss: 0.025 | Tree loss: 2.057 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 029 | Total loss: 2.107 | Reg loss: 0.025 | Tree loss: 2.107 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 029 | Total loss: 2.069 | Reg loss: 0.025 | Tree loss: 2.069 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 029 | Total loss: 2.055 | Reg loss: 0.025 | Tree loss: 2.055 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 029 | Total loss: 2.129 | Reg loss: 0.025 | Tree loss: 2.129 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 029 | Total loss: 2.082 | Reg loss: 0.025 | Tree loss: 2.082 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 029 | Total loss: 2.100 | Reg loss: 0.025 | Tree loss: 2.100 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 029 | Total loss: 2.107 | Reg loss: 0.025 | Tree loss: 2.107 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 029 | Total loss: 2.093 | Reg loss: 0.025 | Tree loss: 2.093 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 029 | Total loss: 2.109 | Reg loss: 0.025 | Tree loss: 2.109 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 029 | Total loss: 2.134 | Reg loss: 0.025 | Tree loss: 2.134 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 029 | Total loss: 2.083 | Reg loss: 0.025 | Tree loss: 2.083 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 029 | Total loss: 2.121 | Reg loss: 0.025 | Tree loss: 2.121 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 029 | Total loss: 2.075 | Reg loss: 0.025 | Tree loss: 2.075 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 029 | Total loss: 2.037 | Reg loss: 0.025 | Tree loss: 2.037 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 029 | Total loss: 2.077 | Reg loss: 0.025 | Tree loss: 2.077 | Accuracy: 0.242188 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 029 | Total loss: 2.085 | Reg loss: 0.025 | Tree loss: 2.085 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 029 | Total loss: 2.014 | Reg loss: 0.025 | Tree loss: 2.014 | Accuracy: 0.230769 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 62 | Batch: 000 / 029 | Total loss: 2.194 | Reg loss: 0.025 | Tree loss: 2.194 | Accuracy: 0.224609 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 029 | Total loss: 2.095 | Reg loss: 0.025 | Tree loss: 2.095 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 029 | Total loss: 2.046 | Reg loss: 0.025 | Tree loss: 2.046 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 029 | Total loss: 2.049 | Reg loss: 0.025 | Tree loss: 2.049 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 029 | Total loss: 2.084 | Reg loss: 0.025 | Tree loss: 2.084 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 029 | Total loss: 2.110 | Reg loss: 0.025 | Tree loss: 2.110 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 029 | Total loss: 2.085 | Reg loss: 0.025 | Tree loss: 2.085 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 029 | Total loss: 2.082 | Reg loss: 0.025 | Tree loss: 2.082 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 029 | Total loss: 2.084 | Reg loss: 0.025 | Tree loss: 2.084 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 029 | Total loss: 2.122 | Reg loss: 0.025 | Tree loss: 2.122 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 029 | Total loss: 2.038 | Reg loss: 0.025 | Tree loss: 2.038 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 029 | Total loss: 2.065 | Reg loss: 0.025 | Tree loss: 2.065 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 029 | Total loss: 2.074 | Reg loss: 0.025 | Tree loss: 2.074 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 029 | Total loss: 2.114 | Reg loss: 0.025 | Tree loss: 2.114 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 029 | Total loss: 2.100 | Reg loss: 0.025 | Tree loss: 2.100 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 029 | Total loss: 2.097 | Reg loss: 0.025 | Tree loss: 2.097 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 029 | Total loss: 2.056 | Reg loss: 0.025 | Tree loss: 2.056 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 029 | Total loss: 2.142 | Reg loss: 0.025 | Tree loss: 2.142 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 029 | Total loss: 2.082 | Reg loss: 0.025 | Tree loss: 2.082 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 029 | Total loss: 2.084 | Reg loss: 0.025 | Tree loss: 2.084 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 029 | Total loss: 2.095 | Reg loss: 0.025 | Tree loss: 2.095 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 029 | Total loss: 2.081 | Reg loss: 0.025 | Tree loss: 2.081 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 029 | Total loss: 2.112 | Reg loss: 0.025 | Tree loss: 2.112 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.244141 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 025 / 029 | Total loss: 2.087 | Reg loss: 0.025 | Tree loss: 2.087 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 029 | Total loss: 2.097 | Reg loss: 0.025 | Tree loss: 2.097 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 029 | Total loss: 2.073 | Reg loss: 0.025 | Tree loss: 2.073 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 029 | Total loss: 2.018 | Reg loss: 0.025 | Tree loss: 2.018 | Accuracy: 0.230769 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 63 | Batch: 000 / 029 | Total loss: 2.149 | Reg loss: 0.025 | Tree loss: 2.149 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 029 | Total loss: 2.076 | Reg loss: 0.025 | Tree loss: 2.076 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 029 | Total loss: 2.072 | Reg loss: 0.025 | Tree loss: 2.072 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 029 | Total loss: 2.083 | Reg loss: 0.025 | Tree loss: 2.083 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 029 | Total loss: 2.125 | Reg loss: 0.025 | Tree loss: 2.125 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 029 | Total loss: 2.069 | Reg loss: 0.025 | Tree loss: 2.069 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 029 | Total loss: 2.067 | Reg loss: 0.025 | Tree loss: 2.067 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 029 | Total loss: 2.095 | Reg loss: 0.025 | Tree loss: 2.095 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 029 | Total loss: 2.126 | Reg loss: 0.025 | Tree loss: 2.126 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 029 | Total loss: 2.097 | Reg loss: 0.025 | Tree loss: 2.097 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 029 | Total loss: 2.058 | Reg loss: 0.025 | Tree loss: 2.058 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 029 | Total loss: 2.068 | Reg loss: 0.025 | Tree loss: 2.068 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 029 | Total loss: 2.111 | Reg loss: 0.025 | Tree loss: 2.111 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 029 | Total loss: 2.075 | Reg loss: 0.025 | Tree loss: 2.075 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 029 | Total loss: 2.105 | Reg loss: 0.025 | Tree loss: 2.105 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 029 | Total loss: 2.059 | Reg loss: 0.025 | Tree loss: 2.059 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 029 | Total loss: 2.068 | Reg loss: 0.025 | Tree loss: 2.068 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 029 | Total loss: 2.062 | Reg loss: 0.025 | Tree loss: 2.062 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 029 | Total loss: 2.050 | Reg loss: 0.025 | Tree loss: 2.050 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 029 | Total loss: 2.088 | Reg loss: 0.025 | Tree loss: 2.088 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 029 | Total loss: 2.099 | Reg loss: 0.025 | Tree loss: 2.099 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 029 | Total loss: 2.051 | Reg loss: 0.025 | Tree loss: 2.051 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 029 | Total loss: 2.138 | Reg loss: 0.025 | Tree loss: 2.138 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 029 | Total loss: 2.054 | Reg loss: 0.025 | Tree loss: 2.054 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 029 | Total loss: 2.111 | Reg loss: 0.025 | Tree loss: 2.111 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 029 | Total loss: 2.106 | Reg loss: 0.025 | Tree loss: 2.106 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 029 | Total loss: 2.090 | Reg loss: 0.025 | Tree loss: 2.090 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 029 | Total loss: 2.058 | Reg loss: 0.025 | Tree loss: 2.058 | Accuracy: 0.230769 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 64 | Batch: 000 / 029 | Total loss: 2.092 | Reg loss: 0.025 | Tree loss: 2.092 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 029 | Total loss: 2.083 | Reg loss: 0.025 | Tree loss: 2.083 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 029 | Total loss: 2.099 | Reg loss: 0.025 | Tree loss: 2.099 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 029 | Total loss: 2.103 | Reg loss: 0.025 | Tree loss: 2.103 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 029 | Total loss: 2.076 | Reg loss: 0.025 | Tree loss: 2.076 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 029 | Total loss: 2.050 | Reg loss: 0.025 | Tree loss: 2.050 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 029 | Total loss: 2.101 | Reg loss: 0.025 | Tree loss: 2.101 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 029 | Total loss: 2.124 | Reg loss: 0.025 | Tree loss: 2.124 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 029 | Total loss: 2.098 | Reg loss: 0.025 | Tree loss: 2.098 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 029 | Total loss: 2.054 | Reg loss: 0.025 | Tree loss: 2.054 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 029 | Total loss: 2.053 | Reg loss: 0.025 | Tree loss: 2.053 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 029 | Total loss: 2.100 | Reg loss: 0.025 | Tree loss: 2.100 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 029 | Total loss: 2.026 | Reg loss: 0.025 | Tree loss: 2.026 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 029 | Total loss: 2.133 | Reg loss: 0.025 | Tree loss: 2.133 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 029 | Total loss: 2.077 | Reg loss: 0.025 | Tree loss: 2.077 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 029 | Total loss: 2.068 | Reg loss: 0.025 | Tree loss: 2.068 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 029 | Total loss: 2.086 | Reg loss: 0.025 | Tree loss: 2.086 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 029 | Total loss: 2.045 | Reg loss: 0.025 | Tree loss: 2.045 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 029 | Total loss: 2.085 | Reg loss: 0.025 | Tree loss: 2.085 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 029 | Total loss: 2.080 | Reg loss: 0.025 | Tree loss: 2.080 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 029 | Total loss: 2.123 | Reg loss: 0.025 | Tree loss: 2.123 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 029 | Total loss: 2.123 | Reg loss: 0.025 | Tree loss: 2.123 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 029 | Total loss: 2.120 | Reg loss: 0.025 | Tree loss: 2.120 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 029 | Total loss: 2.069 | Reg loss: 0.025 | Tree loss: 2.069 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 029 | Total loss: 2.060 | Reg loss: 0.025 | Tree loss: 2.060 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 029 | Total loss: 2.110 | Reg loss: 0.025 | Tree loss: 2.110 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 029 | Total loss: 2.008 | Reg loss: 0.025 | Tree loss: 2.008 | Accuracy: 0.332031 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 029 | Total loss: 2.116 | Reg loss: 0.025 | Tree loss: 2.116 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 029 | Total loss: 1.870 | Reg loss: 0.025 | Tree loss: 1.870 | Accuracy: 0.384615 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Batch: 000 / 029 | Total loss: 2.108 | Reg loss: 0.024 | Tree loss: 2.108 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 029 | Total loss: 2.063 | Reg loss: 0.025 | Tree loss: 2.063 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 029 | Total loss: 2.121 | Reg loss: 0.025 | Tree loss: 2.121 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 029 | Total loss: 2.050 | Reg loss: 0.025 | Tree loss: 2.050 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 029 | Total loss: 2.089 | Reg loss: 0.025 | Tree loss: 2.089 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 029 | Total loss: 2.185 | Reg loss: 0.025 | Tree loss: 2.185 | Accuracy: 0.224609 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 029 | Total loss: 2.105 | Reg loss: 0.025 | Tree loss: 2.105 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 029 | Total loss: 2.031 | Reg loss: 0.025 | Tree loss: 2.031 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 029 | Total loss: 2.121 | Reg loss: 0.025 | Tree loss: 2.121 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 029 | Total loss: 2.053 | Reg loss: 0.025 | Tree loss: 2.053 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 029 | Total loss: 2.072 | Reg loss: 0.025 | Tree loss: 2.072 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 029 | Total loss: 2.084 | Reg loss: 0.025 | Tree loss: 2.084 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 029 | Total loss: 2.092 | Reg loss: 0.025 | Tree loss: 2.092 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 029 | Total loss: 2.029 | Reg loss: 0.025 | Tree loss: 2.029 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 029 | Total loss: 2.105 | Reg loss: 0.025 | Tree loss: 2.105 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 029 | Total loss: 2.038 | Reg loss: 0.025 | Tree loss: 2.038 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 029 | Total loss: 2.069 | Reg loss: 0.025 | Tree loss: 2.069 | Accuracy: 0.306641 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 029 | Total loss: 2.074 | Reg loss: 0.025 | Tree loss: 2.074 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 029 | Total loss: 2.069 | Reg loss: 0.025 | Tree loss: 2.069 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 029 | Total loss: 2.050 | Reg loss: 0.025 | Tree loss: 2.050 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 029 | Total loss: 2.072 | Reg loss: 0.025 | Tree loss: 2.072 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 029 | Total loss: 2.100 | Reg loss: 0.024 | Tree loss: 2.100 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 029 | Total loss: 2.096 | Reg loss: 0.024 | Tree loss: 2.096 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.318359 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 029 | Total loss: 2.149 | Reg loss: 0.024 | Tree loss: 2.149 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 029 | Total loss: 2.089 | Reg loss: 0.024 | Tree loss: 2.089 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.322266 | 0.253 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 029 | Total loss: 1.930 | Reg loss: 0.024 | Tree loss: 1.930 | Accuracy: 0.307692 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 66 | Batch: 000 / 029 | Total loss: 2.081 | Reg loss: 0.024 | Tree loss: 2.081 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 029 | Total loss: 2.097 | Reg loss: 0.024 | Tree loss: 2.097 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 029 | Total loss: 2.094 | Reg loss: 0.024 | Tree loss: 2.094 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 029 | Total loss: 2.091 | Reg loss: 0.024 | Tree loss: 2.091 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 029 | Total loss: 2.094 | Reg loss: 0.024 | Tree loss: 2.094 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 029 | Total loss: 2.047 | Reg loss: 0.025 | Tree loss: 2.047 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 029 | Total loss: 2.115 | Reg loss: 0.025 | Tree loss: 2.115 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 029 | Total loss: 2.086 | Reg loss: 0.025 | Tree loss: 2.086 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 029 | Total loss: 2.095 | Reg loss: 0.024 | Tree loss: 2.095 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 029 | Total loss: 2.089 | Reg loss: 0.024 | Tree loss: 2.089 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 029 | Total loss: 2.115 | Reg loss: 0.024 | Tree loss: 2.115 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 029 | Total loss: 2.091 | Reg loss: 0.024 | Tree loss: 2.091 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 029 | Total loss: 2.096 | Reg loss: 0.024 | Tree loss: 2.096 | Accuracy: 0.246094 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 029 | Total loss: 2.087 | Reg loss: 0.024 | Tree loss: 2.087 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 029 | Total loss: 2.069 | Reg loss: 0.024 | Tree loss: 2.069 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.230769 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 67 | Batch: 000 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 029 | Total loss: 2.104 | Reg loss: 0.024 | Tree loss: 2.104 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 029 | Total loss: 2.120 | Reg loss: 0.024 | Tree loss: 2.120 | Accuracy: 0.271484 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 005 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 029 | Total loss: 2.100 | Reg loss: 0.024 | Tree loss: 2.100 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 029 | Total loss: 2.075 | Reg loss: 0.024 | Tree loss: 2.075 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 029 | Total loss: 2.104 | Reg loss: 0.024 | Tree loss: 2.104 | Accuracy: 0.240234 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 029 | Total loss: 2.102 | Reg loss: 0.024 | Tree loss: 2.102 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 029 | Total loss: 2.090 | Reg loss: 0.024 | Tree loss: 2.090 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 029 | Total loss: 2.069 | Reg loss: 0.024 | Tree loss: 2.069 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 029 | Total loss: 2.079 | Reg loss: 0.024 | Tree loss: 2.079 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 029 | Total loss: 2.111 | Reg loss: 0.024 | Tree loss: 2.111 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 029 | Total loss: 2.010 | Reg loss: 0.024 | Tree loss: 2.010 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 029 | Total loss: 2.119 | Reg loss: 0.024 | Tree loss: 2.119 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 029 | Total loss: 2.099 | Reg loss: 0.024 | Tree loss: 2.099 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 029 | Total loss: 2.086 | Reg loss: 0.024 | Tree loss: 2.086 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 029 | Total loss: 2.081 | Reg loss: 0.024 | Tree loss: 2.081 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 029 | Total loss: 2.412 | Reg loss: 0.024 | Tree loss: 2.412 | Accuracy: 0.076923 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 68 | Batch: 000 / 029 | Total loss: 2.083 | Reg loss: 0.024 | Tree loss: 2.083 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 029 | Total loss: 2.058 | Reg loss: 0.024 | Tree loss: 2.058 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 029 | Total loss: 2.126 | Reg loss: 0.024 | Tree loss: 2.126 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 029 | Total loss: 2.079 | Reg loss: 0.024 | Tree loss: 2.079 | Accuracy: 0.228516 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 029 | Total loss: 2.104 | Reg loss: 0.024 | Tree loss: 2.104 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 029 | Total loss: 2.030 | Reg loss: 0.024 | Tree loss: 2.030 | Accuracy: 0.316406 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 029 | Total loss: 2.097 | Reg loss: 0.024 | Tree loss: 2.097 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 029 | Total loss: 2.066 | Reg loss: 0.024 | Tree loss: 2.066 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 029 | Total loss: 2.076 | Reg loss: 0.024 | Tree loss: 2.076 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 029 | Total loss: 2.171 | Reg loss: 0.024 | Tree loss: 2.171 | Accuracy: 0.234375 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 029 | Total loss: 2.018 | Reg loss: 0.024 | Tree loss: 2.018 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 029 | Total loss: 1.932 | Reg loss: 0.024 | Tree loss: 1.932 | Accuracy: 0.307692 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 69 | Batch: 000 / 029 | Total loss: 2.088 | Reg loss: 0.024 | Tree loss: 2.088 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 029 | Total loss: 2.093 | Reg loss: 0.024 | Tree loss: 2.093 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 029 | Total loss: 2.098 | Reg loss: 0.024 | Tree loss: 2.098 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 029 | Total loss: 2.112 | Reg loss: 0.024 | Tree loss: 2.112 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 029 | Total loss: 2.105 | Reg loss: 0.024 | Tree loss: 2.105 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 029 | Total loss: 2.022 | Reg loss: 0.024 | Tree loss: 2.022 | Accuracy: 0.326172 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 010 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 029 | Total loss: 2.096 | Reg loss: 0.024 | Tree loss: 2.096 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 029 | Total loss: 2.001 | Reg loss: 0.024 | Tree loss: 2.001 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 029 | Total loss: 2.077 | Reg loss: 0.024 | Tree loss: 2.077 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 029 | Total loss: 2.079 | Reg loss: 0.024 | Tree loss: 2.079 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 029 | Total loss: 2.069 | Reg loss: 0.024 | Tree loss: 2.069 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 029 | Total loss: 2.110 | Reg loss: 0.024 | Tree loss: 2.110 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 029 | Total loss: 2.075 | Reg loss: 0.024 | Tree loss: 2.075 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 029 | Total loss: 2.019 | Reg loss: 0.024 | Tree loss: 2.019 | Accuracy: 0.320312 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 029 | Total loss: 2.117 | Reg loss: 0.024 | Tree loss: 2.117 | Accuracy: 0.246094 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 029 | Total loss: 2.084 | Reg loss: 0.024 | Tree loss: 2.084 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 029 | Total loss: 2.166 | Reg loss: 0.024 | Tree loss: 2.166 | Accuracy: 0.076923 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 70 | Batch: 000 / 029 | Total loss: 2.016 | Reg loss: 0.024 | Tree loss: 2.016 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 029 | Total loss: 2.083 | Reg loss: 0.024 | Tree loss: 2.083 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 029 | Total loss: 2.078 | Reg loss: 0.024 | Tree loss: 2.078 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 029 | Total loss: 2.064 | Reg loss: 0.024 | Tree loss: 2.064 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 029 | Total loss: 2.093 | Reg loss: 0.024 | Tree loss: 2.093 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 029 | Total loss: 2.040 | Reg loss: 0.024 | Tree loss: 2.040 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 029 | Total loss: 2.098 | Reg loss: 0.024 | Tree loss: 2.098 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 029 | Total loss: 2.051 | Reg loss: 0.024 | Tree loss: 2.051 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 029 | Total loss: 2.099 | Reg loss: 0.024 | Tree loss: 2.099 | Accuracy: 0.224609 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 029 | Total loss: 2.093 | Reg loss: 0.024 | Tree loss: 2.093 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 029 | Total loss: 2.088 | Reg loss: 0.024 | Tree loss: 2.088 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 029 | Total loss: 2.034 | Reg loss: 0.024 | Tree loss: 2.034 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 029 | Total loss: 2.094 | Reg loss: 0.024 | Tree loss: 2.094 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 029 | Total loss: 2.098 | Reg loss: 0.024 | Tree loss: 2.098 | Accuracy: 0.312500 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 029 | Total loss: 2.092 | Reg loss: 0.024 | Tree loss: 2.092 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 029 | Total loss: 2.039 | Reg loss: 0.024 | Tree loss: 2.039 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 029 | Total loss: 2.093 | Reg loss: 0.024 | Tree loss: 2.093 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 029 | Total loss: 2.099 | Reg loss: 0.024 | Tree loss: 2.099 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 029 | Total loss: 1.749 | Reg loss: 0.024 | Tree loss: 1.749 | Accuracy: 0.538462 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 71 | Batch: 000 / 029 | Total loss: 2.087 | Reg loss: 0.024 | Tree loss: 2.087 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 029 | Total loss: 2.128 | Reg loss: 0.024 | Tree loss: 2.128 | Accuracy: 0.236328 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 029 | Total loss: 2.097 | Reg loss: 0.024 | Tree loss: 2.097 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 029 | Total loss: 2.086 | Reg loss: 0.024 | Tree loss: 2.086 | Accuracy: 0.232422 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 029 | Total loss: 2.104 | Reg loss: 0.024 | Tree loss: 2.104 | Accuracy: 0.240234 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 029 | Total loss: 2.087 | Reg loss: 0.024 | Tree loss: 2.087 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 029 | Total loss: 2.006 | Reg loss: 0.024 | Tree loss: 2.006 | Accuracy: 0.326172 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 029 | Total loss: 2.079 | Reg loss: 0.024 | Tree loss: 2.079 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.277344 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 015 / 029 | Total loss: 2.023 | Reg loss: 0.024 | Tree loss: 2.023 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 029 | Total loss: 2.051 | Reg loss: 0.024 | Tree loss: 2.051 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 029 | Total loss: 2.061 | Reg loss: 0.024 | Tree loss: 2.061 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 029 | Total loss: 2.039 | Reg loss: 0.024 | Tree loss: 2.039 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 029 | Total loss: 2.247 | Reg loss: 0.024 | Tree loss: 2.247 | Accuracy: 0.230769 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 72 | Batch: 000 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 029 | Total loss: 2.075 | Reg loss: 0.024 | Tree loss: 2.075 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 029 | Total loss: 2.069 | Reg loss: 0.024 | Tree loss: 2.069 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 029 | Total loss: 2.082 | Reg loss: 0.024 | Tree loss: 2.082 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 029 | Total loss: 2.088 | Reg loss: 0.024 | Tree loss: 2.088 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 029 | Total loss: 2.092 | Reg loss: 0.024 | Tree loss: 2.092 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 029 | Total loss: 2.092 | Reg loss: 0.024 | Tree loss: 2.092 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 029 | Total loss: 2.009 | Reg loss: 0.024 | Tree loss: 2.009 | Accuracy: 0.326172 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 029 | Total loss: 2.123 | Reg loss: 0.024 | Tree loss: 2.123 | Accuracy: 0.242188 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 029 | Total loss: 2.066 | Reg loss: 0.024 | Tree loss: 2.066 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 029 | Total loss: 2.076 | Reg loss: 0.024 | Tree loss: 2.076 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 029 | Total loss: 2.000 | Reg loss: 0.024 | Tree loss: 2.000 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 029 | Total loss: 2.019 | Reg loss: 0.024 | Tree loss: 2.019 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 029 | Total loss: 2.096 | Reg loss: 0.024 | Tree loss: 2.096 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 029 | Total loss: 2.398 | Reg loss: 0.024 | Tree loss: 2.398 | Accuracy: 0.076923 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 73 | Batch: 000 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 029 | Total loss: 2.034 | Reg loss: 0.024 | Tree loss: 2.034 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 029 | Total loss: 2.082 | Reg loss: 0.024 | Tree loss: 2.082 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 029 | Total loss: 2.110 | Reg loss: 0.024 | Tree loss: 2.110 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 029 | Total loss: 2.084 | Reg loss: 0.024 | Tree loss: 2.084 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 029 | Total loss: 2.057 | Reg loss: 0.024 | Tree loss: 2.057 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.289062 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 020 / 029 | Total loss: 2.089 | Reg loss: 0.024 | Tree loss: 2.089 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 029 | Total loss: 2.098 | Reg loss: 0.024 | Tree loss: 2.098 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 029 | Total loss: 2.023 | Reg loss: 0.024 | Tree loss: 2.023 | Accuracy: 0.310547 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 029 | Total loss: 2.092 | Reg loss: 0.024 | Tree loss: 2.092 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 029 | Total loss: 2.270 | Reg loss: 0.024 | Tree loss: 2.270 | Accuracy: 0.076923 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 74 | Batch: 000 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 029 | Total loss: 2.077 | Reg loss: 0.024 | Tree loss: 2.077 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 029 | Total loss: 2.057 | Reg loss: 0.024 | Tree loss: 2.057 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 029 | Total loss: 2.034 | Reg loss: 0.024 | Tree loss: 2.034 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.246094 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 029 | Total loss: 2.093 | Reg loss: 0.024 | Tree loss: 2.093 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 029 | Total loss: 2.100 | Reg loss: 0.024 | Tree loss: 2.100 | Accuracy: 0.234375 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 029 | Total loss: 2.086 | Reg loss: 0.024 | Tree loss: 2.086 | Accuracy: 0.242188 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.310547 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 029 | Total loss: 2.087 | Reg loss: 0.024 | Tree loss: 2.087 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 029 | Total loss: 2.022 | Reg loss: 0.024 | Tree loss: 2.022 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 029 | Total loss: 2.004 | Reg loss: 0.024 | Tree loss: 2.004 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.296875 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 029 | Total loss: 1.879 | Reg loss: 0.024 | Tree loss: 1.879 | Accuracy: 0.384615 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 75 | Batch: 000 / 029 | Total loss: 2.061 | Reg loss: 0.024 | Tree loss: 2.061 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 029 | Total loss: 2.099 | Reg loss: 0.024 | Tree loss: 2.099 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 029 | Total loss: 2.069 | Reg loss: 0.024 | Tree loss: 2.069 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 029 | Total loss: 2.075 | Reg loss: 0.024 | Tree loss: 2.075 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 029 | Total loss: 2.092 | Reg loss: 0.024 | Tree loss: 2.092 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 029 | Total loss: 2.012 | Reg loss: 0.024 | Tree loss: 2.012 | Accuracy: 0.322266 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 029 | Total loss: 2.021 | Reg loss: 0.024 | Tree loss: 2.021 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 029 | Total loss: 2.081 | Reg loss: 0.024 | Tree loss: 2.081 | Accuracy: 0.232422 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.310547 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 029 | Total loss: 2.113 | Reg loss: 0.024 | Tree loss: 2.113 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 029 | Total loss: 2.064 | Reg loss: 0.024 | Tree loss: 2.064 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.310547 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 029 | Total loss: 2.103 | Reg loss: 0.024 | Tree loss: 2.103 | Accuracy: 0.275391 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 025 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 029 | Total loss: 2.061 | Reg loss: 0.024 | Tree loss: 2.061 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 029 | Total loss: 1.847 | Reg loss: 0.024 | Tree loss: 1.847 | Accuracy: 0.384615 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 76 | Batch: 000 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 029 | Total loss: 2.028 | Reg loss: 0.024 | Tree loss: 2.028 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 029 | Total loss: 2.098 | Reg loss: 0.024 | Tree loss: 2.098 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 029 | Total loss: 2.023 | Reg loss: 0.024 | Tree loss: 2.023 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 029 | Total loss: 2.089 | Reg loss: 0.024 | Tree loss: 2.089 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 029 | Total loss: 2.022 | Reg loss: 0.024 | Tree loss: 2.022 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 029 | Total loss: 2.014 | Reg loss: 0.024 | Tree loss: 2.014 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 029 | Total loss: 2.112 | Reg loss: 0.024 | Tree loss: 2.112 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.244141 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.232422 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 029 | Total loss: 2.083 | Reg loss: 0.024 | Tree loss: 2.083 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 029 | Total loss: 2.019 | Reg loss: 0.024 | Tree loss: 2.019 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 029 | Total loss: 2.001 | Reg loss: 0.024 | Tree loss: 2.001 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 029 | Total loss: 2.083 | Reg loss: 0.024 | Tree loss: 2.083 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 029 | Total loss: 2.061 | Reg loss: 0.024 | Tree loss: 2.061 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 029 | Total loss: 2.064 | Reg loss: 0.024 | Tree loss: 2.064 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 029 | Total loss: 1.917 | Reg loss: 0.024 | Tree loss: 1.917 | Accuracy: 0.384615 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 77 | Batch: 000 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 029 | Total loss: 2.057 | Reg loss: 0.024 | Tree loss: 2.057 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 029 | Total loss: 2.078 | Reg loss: 0.024 | Tree loss: 2.078 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 029 | Total loss: 2.078 | Reg loss: 0.024 | Tree loss: 2.078 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 029 | Total loss: 2.028 | Reg loss: 0.024 | Tree loss: 2.028 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 029 | Total loss: 2.061 | Reg loss: 0.024 | Tree loss: 2.061 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 029 | Total loss: 2.076 | Reg loss: 0.024 | Tree loss: 2.076 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 029 | Total loss: 2.088 | Reg loss: 0.024 | Tree loss: 2.088 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 029 | Total loss: 2.012 | Reg loss: 0.024 | Tree loss: 2.012 | Accuracy: 0.314453 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 029 | Total loss: 2.076 | Reg loss: 0.024 | Tree loss: 2.076 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 029 | Total loss: 2.099 | Reg loss: 0.024 | Tree loss: 2.099 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 029 | Total loss: 2.092 | Reg loss: 0.024 | Tree loss: 2.092 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 029 | Total loss: 2.025 | Reg loss: 0.024 | Tree loss: 2.025 | Accuracy: 0.318359 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 029 | Total loss: 2.023 | Reg loss: 0.024 | Tree loss: 2.023 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 029 | Total loss: 1.781 | Reg loss: 0.024 | Tree loss: 1.781 | Accuracy: 0.461538 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 000 / 029 | Total loss: 2.117 | Reg loss: 0.024 | Tree loss: 2.117 | Accuracy: 0.230469 | 0.253 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 029 | Total loss: 2.057 | Reg loss: 0.024 | Tree loss: 2.057 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 029 | Total loss: 2.089 | Reg loss: 0.024 | Tree loss: 2.089 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 029 | Total loss: 2.003 | Reg loss: 0.024 | Tree loss: 2.003 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 029 | Total loss: 2.034 | Reg loss: 0.024 | Tree loss: 2.034 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.306641 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 029 | Total loss: 2.077 | Reg loss: 0.024 | Tree loss: 2.077 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.308594 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 029 | Total loss: 2.040 | Reg loss: 0.024 | Tree loss: 2.040 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 029 | Total loss: 2.016 | Reg loss: 0.024 | Tree loss: 2.016 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 029 | Total loss: 2.064 | Reg loss: 0.024 | Tree loss: 2.064 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 029 | Total loss: 2.254 | Reg loss: 0.024 | Tree loss: 2.254 | Accuracy: 0.076923 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 79 | Batch: 000 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 029 | Total loss: 2.040 | Reg loss: 0.024 | Tree loss: 2.040 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 029 | Total loss: 2.101 | Reg loss: 0.024 | Tree loss: 2.101 | Accuracy: 0.224609 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 029 | Total loss: 2.064 | Reg loss: 0.024 | Tree loss: 2.064 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.318359 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 029 | Total loss: 2.086 | Reg loss: 0.024 | Tree loss: 2.086 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 029 | Total loss: 2.066 | Reg loss: 0.024 | Tree loss: 2.066 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 029 | Total loss: 1.999 | Reg loss: 0.024 | Tree loss: 1.999 | Accuracy: 0.308594 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 029 | Total loss: 2.051 | Reg loss: 0.024 | Tree loss: 2.051 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 029 | Total loss: 2.030 | Reg loss: 0.024 | Tree loss: 2.030 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.314453 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 029 | Total loss: 2.080 | Reg loss: 0.024 | Tree loss: 2.080 | Accuracy: 0.248047 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 029 | Total loss: 2.093 | Reg loss: 0.024 | Tree loss: 2.093 | Accuracy: 0.230469 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 029 | Total loss: 2.030 | Reg loss: 0.024 | Tree loss: 2.030 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.307692 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 80 | Batch: 000 / 029 | Total loss: 2.022 | Reg loss: 0.024 | Tree loss: 2.022 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 029 | Total loss: 2.022 | Reg loss: 0.024 | Tree loss: 2.022 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 029 | Total loss: 2.091 | Reg loss: 0.024 | Tree loss: 2.091 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 029 | Total loss: 2.012 | Reg loss: 0.024 | Tree loss: 2.012 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 029 | Total loss: 2.004 | Reg loss: 0.024 | Tree loss: 2.004 | Accuracy: 0.308594 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 005 / 029 | Total loss: 2.004 | Reg loss: 0.024 | Tree loss: 2.004 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 029 | Total loss: 1.995 | Reg loss: 0.024 | Tree loss: 1.995 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 029 | Total loss: 2.057 | Reg loss: 0.024 | Tree loss: 2.057 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 029 | Total loss: 2.123 | Reg loss: 0.024 | Tree loss: 2.123 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 029 | Total loss: 2.007 | Reg loss: 0.024 | Tree loss: 2.007 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 029 | Total loss: 2.039 | Reg loss: 0.024 | Tree loss: 2.039 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 029 | Total loss: 2.117 | Reg loss: 0.024 | Tree loss: 2.117 | Accuracy: 0.242188 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 029 | Total loss: 2.130 | Reg loss: 0.024 | Tree loss: 2.130 | Accuracy: 0.224609 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 029 | Total loss: 2.030 | Reg loss: 0.024 | Tree loss: 2.030 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 029 | Total loss: 2.075 | Reg loss: 0.024 | Tree loss: 2.075 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 029 | Total loss: 2.097 | Reg loss: 0.024 | Tree loss: 2.097 | Accuracy: 0.242188 | 0.252 sec/iter\n",
      "Epoch: 80 | Batch: 028 / 029 | Total loss: 1.912 | Reg loss: 0.024 | Tree loss: 1.912 | Accuracy: 0.384615 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 81 | Batch: 000 / 029 | Total loss: 2.088 | Reg loss: 0.024 | Tree loss: 2.088 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 029 | Total loss: 2.079 | Reg loss: 0.024 | Tree loss: 2.079 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 029 | Total loss: 2.012 | Reg loss: 0.024 | Tree loss: 2.012 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 029 | Total loss: 1.999 | Reg loss: 0.024 | Tree loss: 1.999 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 029 | Total loss: 2.051 | Reg loss: 0.024 | Tree loss: 2.051 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 029 | Total loss: 1.985 | Reg loss: 0.024 | Tree loss: 1.985 | Accuracy: 0.335938 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.251953 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 029 | Total loss: 2.069 | Reg loss: 0.024 | Tree loss: 2.069 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.306641 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 029 | Total loss: 2.051 | Reg loss: 0.024 | Tree loss: 2.051 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.246094 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 029 | Total loss: 2.076 | Reg loss: 0.024 | Tree loss: 2.076 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 029 | Total loss: 2.100 | Reg loss: 0.024 | Tree loss: 2.100 | Accuracy: 0.307692 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 82 | Batch: 000 / 029 | Total loss: 2.100 | Reg loss: 0.024 | Tree loss: 2.100 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 029 | Total loss: 2.028 | Reg loss: 0.024 | Tree loss: 2.028 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 029 | Total loss: 2.051 | Reg loss: 0.024 | Tree loss: 2.051 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 029 | Total loss: 2.082 | Reg loss: 0.024 | Tree loss: 2.082 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 029 | Total loss: 2.010 | Reg loss: 0.024 | Tree loss: 2.010 | Accuracy: 0.324219 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 029 | Total loss: 2.014 | Reg loss: 0.024 | Tree loss: 2.014 | Accuracy: 0.277344 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 010 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 029 | Total loss: 2.006 | Reg loss: 0.024 | Tree loss: 2.006 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 029 | Total loss: 2.057 | Reg loss: 0.024 | Tree loss: 2.057 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 029 | Total loss: 2.058 | Reg loss: 0.024 | Tree loss: 2.058 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 029 | Total loss: 2.007 | Reg loss: 0.024 | Tree loss: 2.007 | Accuracy: 0.310547 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 029 | Total loss: 2.039 | Reg loss: 0.024 | Tree loss: 2.039 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 029 | Total loss: 2.111 | Reg loss: 0.024 | Tree loss: 2.111 | Accuracy: 0.234375 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.296875 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 029 | Total loss: 2.075 | Reg loss: 0.024 | Tree loss: 2.075 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 029 | Total loss: 2.051 | Reg loss: 0.024 | Tree loss: 2.051 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 029 | Total loss: 2.018 | Reg loss: 0.024 | Tree loss: 2.018 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 029 | Total loss: 2.003 | Reg loss: 0.024 | Tree loss: 2.003 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 029 | Total loss: 2.017 | Reg loss: 0.024 | Tree loss: 2.017 | Accuracy: 0.296875 | 0.252 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 029 | Total loss: 2.294 | Reg loss: 0.024 | Tree loss: 2.294 | Accuracy: 0.307692 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 83 | Batch: 000 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 029 | Total loss: 2.019 | Reg loss: 0.024 | Tree loss: 2.019 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 029 | Total loss: 2.039 | Reg loss: 0.024 | Tree loss: 2.039 | Accuracy: 0.306641 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 029 | Total loss: 2.083 | Reg loss: 0.024 | Tree loss: 2.083 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 029 | Total loss: 2.104 | Reg loss: 0.024 | Tree loss: 2.104 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 029 | Total loss: 2.086 | Reg loss: 0.024 | Tree loss: 2.086 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 029 | Total loss: 2.089 | Reg loss: 0.024 | Tree loss: 2.089 | Accuracy: 0.244141 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.251953 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 029 | Total loss: 2.022 | Reg loss: 0.024 | Tree loss: 2.022 | Accuracy: 0.248047 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 029 | Total loss: 2.011 | Reg loss: 0.024 | Tree loss: 2.011 | Accuracy: 0.314453 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 029 | Total loss: 2.017 | Reg loss: 0.024 | Tree loss: 2.017 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 029 | Total loss: 2.099 | Reg loss: 0.024 | Tree loss: 2.099 | Accuracy: 0.248047 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 029 | Total loss: 2.077 | Reg loss: 0.024 | Tree loss: 2.077 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 029 | Total loss: 1.980 | Reg loss: 0.024 | Tree loss: 1.980 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 029 | Total loss: 2.000 | Reg loss: 0.024 | Tree loss: 2.000 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.316406 | 0.252 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 029 | Total loss: 1.727 | Reg loss: 0.024 | Tree loss: 1.727 | Accuracy: 0.538462 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 84 | Batch: 000 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 029 | Total loss: 2.009 | Reg loss: 0.024 | Tree loss: 2.009 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 029 | Total loss: 2.028 | Reg loss: 0.024 | Tree loss: 2.028 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 029 | Total loss: 2.030 | Reg loss: 0.024 | Tree loss: 2.030 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.296875 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 015 / 029 | Total loss: 1.996 | Reg loss: 0.024 | Tree loss: 1.996 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.251953 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.248047 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 029 | Total loss: 2.039 | Reg loss: 0.024 | Tree loss: 2.039 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 029 | Total loss: 2.014 | Reg loss: 0.024 | Tree loss: 2.014 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 029 | Total loss: 1.953 | Reg loss: 0.024 | Tree loss: 1.953 | Accuracy: 0.307692 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 85 | Batch: 000 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.242188 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 029 | Total loss: 2.011 | Reg loss: 0.024 | Tree loss: 2.011 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 029 | Total loss: 2.099 | Reg loss: 0.024 | Tree loss: 2.099 | Accuracy: 0.232422 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 029 | Total loss: 2.001 | Reg loss: 0.024 | Tree loss: 2.001 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 029 | Total loss: 2.112 | Reg loss: 0.024 | Tree loss: 2.112 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 029 | Total loss: 2.025 | Reg loss: 0.024 | Tree loss: 2.025 | Accuracy: 0.308594 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 029 | Total loss: 1.992 | Reg loss: 0.024 | Tree loss: 1.992 | Accuracy: 0.318359 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 029 | Total loss: 2.025 | Reg loss: 0.024 | Tree loss: 2.025 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 029 | Total loss: 2.003 | Reg loss: 0.024 | Tree loss: 2.003 | Accuracy: 0.308594 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.296875 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 029 | Total loss: 2.027 | Reg loss: 0.024 | Tree loss: 2.027 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.242188 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 029 | Total loss: 2.006 | Reg loss: 0.024 | Tree loss: 2.006 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 029 | Total loss: 1.971 | Reg loss: 0.024 | Tree loss: 1.971 | Accuracy: 0.307692 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 86 | Batch: 000 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 029 | Total loss: 2.015 | Reg loss: 0.024 | Tree loss: 2.015 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 029 | Total loss: 2.014 | Reg loss: 0.024 | Tree loss: 2.014 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 029 | Total loss: 2.001 | Reg loss: 0.024 | Tree loss: 2.001 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 029 | Total loss: 2.025 | Reg loss: 0.024 | Tree loss: 2.025 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 029 | Total loss: 2.034 | Reg loss: 0.024 | Tree loss: 2.034 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 029 | Total loss: 1.999 | Reg loss: 0.024 | Tree loss: 1.999 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 029 | Total loss: 2.025 | Reg loss: 0.024 | Tree loss: 2.025 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 029 | Total loss: 2.013 | Reg loss: 0.024 | Tree loss: 2.013 | Accuracy: 0.330078 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 020 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 029 | Total loss: 2.023 | Reg loss: 0.024 | Tree loss: 2.023 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 029 | Total loss: 2.057 | Reg loss: 0.024 | Tree loss: 2.057 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 029 | Total loss: 2.073 | Reg loss: 0.024 | Tree loss: 2.073 | Accuracy: 0.251953 | 0.252 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 029 | Total loss: 1.954 | Reg loss: 0.024 | Tree loss: 1.954 | Accuracy: 0.307692 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 87 | Batch: 000 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 029 | Total loss: 2.068 | Reg loss: 0.024 | Tree loss: 2.068 | Accuracy: 0.246094 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 029 | Total loss: 2.078 | Reg loss: 0.024 | Tree loss: 2.078 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.306641 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 029 | Total loss: 1.983 | Reg loss: 0.024 | Tree loss: 1.983 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 029 | Total loss: 2.014 | Reg loss: 0.024 | Tree loss: 2.014 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 029 | Total loss: 1.980 | Reg loss: 0.024 | Tree loss: 1.980 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 029 | Total loss: 1.995 | Reg loss: 0.024 | Tree loss: 1.995 | Accuracy: 0.296875 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 029 | Total loss: 2.056 | Reg loss: 0.024 | Tree loss: 2.056 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 029 | Total loss: 2.078 | Reg loss: 0.024 | Tree loss: 2.078 | Accuracy: 0.224609 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 029 | Total loss: 2.061 | Reg loss: 0.024 | Tree loss: 2.061 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 029 | Total loss: 2.085 | Reg loss: 0.024 | Tree loss: 2.085 | Accuracy: 0.248047 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 029 | Total loss: 2.034 | Reg loss: 0.024 | Tree loss: 2.034 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 029 | Total loss: 1.989 | Reg loss: 0.024 | Tree loss: 1.989 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 029 | Total loss: 2.016 | Reg loss: 0.024 | Tree loss: 2.016 | Accuracy: 0.296875 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 029 | Total loss: 2.274 | Reg loss: 0.024 | Tree loss: 2.274 | Accuracy: 0.153846 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 88 | Batch: 000 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.306641 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 029 | Total loss: 2.018 | Reg loss: 0.024 | Tree loss: 2.018 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 029 | Total loss: 2.022 | Reg loss: 0.024 | Tree loss: 2.022 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 029 | Total loss: 2.004 | Reg loss: 0.024 | Tree loss: 2.004 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 029 | Total loss: 2.019 | Reg loss: 0.024 | Tree loss: 2.019 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 029 | Total loss: 2.025 | Reg loss: 0.024 | Tree loss: 2.025 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 029 | Total loss: 2.075 | Reg loss: 0.024 | Tree loss: 2.075 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 029 | Total loss: 2.074 | Reg loss: 0.024 | Tree loss: 2.074 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 029 | Total loss: 2.091 | Reg loss: 0.024 | Tree loss: 2.091 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 029 | Total loss: 1.994 | Reg loss: 0.024 | Tree loss: 1.994 | Accuracy: 0.302734 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 029 | Total loss: 2.007 | Reg loss: 0.024 | Tree loss: 2.007 | Accuracy: 0.304688 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 | Batch: 025 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 029 | Total loss: 2.234 | Reg loss: 0.024 | Tree loss: 2.234 | Accuracy: 0.153846 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 89 | Batch: 000 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 029 | Total loss: 2.066 | Reg loss: 0.024 | Tree loss: 2.066 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.306641 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 029 | Total loss: 2.013 | Reg loss: 0.024 | Tree loss: 2.013 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 029 | Total loss: 2.090 | Reg loss: 0.024 | Tree loss: 2.090 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 029 | Total loss: 2.020 | Reg loss: 0.024 | Tree loss: 2.020 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 029 | Total loss: 2.027 | Reg loss: 0.024 | Tree loss: 2.027 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 029 | Total loss: 2.058 | Reg loss: 0.024 | Tree loss: 2.058 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 029 | Total loss: 2.013 | Reg loss: 0.024 | Tree loss: 2.013 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 029 | Total loss: 2.089 | Reg loss: 0.024 | Tree loss: 2.089 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 029 | Total loss: 2.012 | Reg loss: 0.024 | Tree loss: 2.012 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 029 | Total loss: 2.018 | Reg loss: 0.024 | Tree loss: 2.018 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 029 | Total loss: 2.047 | Reg loss: 0.024 | Tree loss: 2.047 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 029 | Total loss: 1.990 | Reg loss: 0.024 | Tree loss: 1.990 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 029 | Total loss: 2.018 | Reg loss: 0.024 | Tree loss: 2.018 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 029 | Total loss: 2.039 | Reg loss: 0.024 | Tree loss: 2.039 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 029 | Total loss: 2.144 | Reg loss: 0.024 | Tree loss: 2.144 | Accuracy: 0.076923 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 90 | Batch: 000 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 029 | Total loss: 2.077 | Reg loss: 0.024 | Tree loss: 2.077 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 029 | Total loss: 1.997 | Reg loss: 0.024 | Tree loss: 1.997 | Accuracy: 0.326172 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 029 | Total loss: 2.038 | Reg loss: 0.024 | Tree loss: 2.038 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 029 | Total loss: 2.015 | Reg loss: 0.024 | Tree loss: 2.015 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 029 | Total loss: 2.041 | Reg loss: 0.024 | Tree loss: 2.041 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.251953 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 029 | Total loss: 2.040 | Reg loss: 0.024 | Tree loss: 2.040 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.255859 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 029 | Total loss: 1.955 | Reg loss: 0.024 | Tree loss: 1.955 | Accuracy: 0.324219 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 029 | Total loss: 1.996 | Reg loss: 0.024 | Tree loss: 1.996 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 029 | Total loss: 2.027 | Reg loss: 0.024 | Tree loss: 2.027 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 029 | Total loss: 2.063 | Reg loss: 0.024 | Tree loss: 2.063 | Accuracy: 0.240234 | 0.253 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 029 | Total loss: 1.994 | Reg loss: 0.024 | Tree loss: 1.994 | Accuracy: 0.153846 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 000 / 029 | Total loss: 2.023 | Reg loss: 0.024 | Tree loss: 2.023 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 029 | Total loss: 2.015 | Reg loss: 0.024 | Tree loss: 2.015 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 029 | Total loss: 1.990 | Reg loss: 0.024 | Tree loss: 1.990 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 029 | Total loss: 2.055 | Reg loss: 0.024 | Tree loss: 2.055 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 029 | Total loss: 2.053 | Reg loss: 0.024 | Tree loss: 2.053 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 029 | Total loss: 2.007 | Reg loss: 0.024 | Tree loss: 2.007 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 029 | Total loss: 1.995 | Reg loss: 0.024 | Tree loss: 1.995 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 029 | Total loss: 2.094 | Reg loss: 0.024 | Tree loss: 2.094 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 029 | Total loss: 2.062 | Reg loss: 0.024 | Tree loss: 2.062 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 029 | Total loss: 2.102 | Reg loss: 0.024 | Tree loss: 2.102 | Accuracy: 0.244141 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 029 | Total loss: 2.006 | Reg loss: 0.024 | Tree loss: 2.006 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 029 | Total loss: 2.042 | Reg loss: 0.024 | Tree loss: 2.042 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 029 | Total loss: 2.007 | Reg loss: 0.024 | Tree loss: 2.007 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 029 | Total loss: 2.104 | Reg loss: 0.024 | Tree loss: 2.104 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 029 | Total loss: 1.989 | Reg loss: 0.024 | Tree loss: 1.989 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 029 | Total loss: 2.026 | Reg loss: 0.024 | Tree loss: 2.026 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 029 | Total loss: 2.031 | Reg loss: 0.024 | Tree loss: 2.031 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 029 | Total loss: 2.035 | Reg loss: 0.024 | Tree loss: 2.035 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 029 | Total loss: 1.900 | Reg loss: 0.024 | Tree loss: 1.900 | Accuracy: 0.384615 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 92 | Batch: 000 / 029 | Total loss: 2.094 | Reg loss: 0.024 | Tree loss: 2.094 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 029 | Total loss: 2.018 | Reg loss: 0.024 | Tree loss: 2.018 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 029 | Total loss: 2.044 | Reg loss: 0.024 | Tree loss: 2.044 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 029 | Total loss: 2.019 | Reg loss: 0.024 | Tree loss: 2.019 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 029 | Total loss: 2.012 | Reg loss: 0.024 | Tree loss: 2.012 | Accuracy: 0.314453 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 029 | Total loss: 2.059 | Reg loss: 0.024 | Tree loss: 2.059 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 029 | Total loss: 2.049 | Reg loss: 0.024 | Tree loss: 2.049 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 029 | Total loss: 2.037 | Reg loss: 0.024 | Tree loss: 2.037 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 029 | Total loss: 2.004 | Reg loss: 0.024 | Tree loss: 2.004 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 029 | Total loss: 2.043 | Reg loss: 0.024 | Tree loss: 2.043 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 029 | Total loss: 2.061 | Reg loss: 0.024 | Tree loss: 2.061 | Accuracy: 0.316406 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 029 | Total loss: 2.029 | Reg loss: 0.024 | Tree loss: 2.029 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 029 | Total loss: 2.071 | Reg loss: 0.024 | Tree loss: 2.071 | Accuracy: 0.240234 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 029 | Total loss: 2.003 | Reg loss: 0.024 | Tree loss: 2.003 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 029 | Total loss: 2.046 | Reg loss: 0.024 | Tree loss: 2.046 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 029 | Total loss: 2.079 | Reg loss: 0.024 | Tree loss: 2.079 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 029 | Total loss: 2.001 | Reg loss: 0.024 | Tree loss: 2.001 | Accuracy: 0.287109 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 029 | Total loss: 2.007 | Reg loss: 0.024 | Tree loss: 2.007 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 029 | Total loss: 2.048 | Reg loss: 0.024 | Tree loss: 2.048 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.228516 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 029 | Total loss: 2.033 | Reg loss: 0.024 | Tree loss: 2.033 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 029 | Total loss: 2.040 | Reg loss: 0.024 | Tree loss: 2.040 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 029 | Total loss: 2.008 | Reg loss: 0.024 | Tree loss: 2.008 | Accuracy: 0.328125 | 0.253 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 029 | Total loss: 2.027 | Reg loss: 0.024 | Tree loss: 2.027 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 029 | Total loss: 2.050 | Reg loss: 0.024 | Tree loss: 2.050 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 029 | Total loss: 2.009 | Reg loss: 0.024 | Tree loss: 2.009 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 029 | Total loss: 2.252 | Reg loss: 0.024 | Tree loss: 2.252 | Accuracy: 0.153846 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 93 | Batch: 000 / 029 | Total loss: 2.022 | Reg loss: 0.023 | Tree loss: 2.022 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 029 | Total loss: 2.032 | Reg loss: 0.024 | Tree loss: 2.032 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 029 | Total loss: 2.067 | Reg loss: 0.024 | Tree loss: 2.067 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.291016 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 005 / 029 | Total loss: 2.065 | Reg loss: 0.024 | Tree loss: 2.065 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 029 | Total loss: 2.024 | Reg loss: 0.024 | Tree loss: 2.024 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 029 | Total loss: 2.014 | Reg loss: 0.024 | Tree loss: 2.014 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 029 | Total loss: 2.005 | Reg loss: 0.024 | Tree loss: 2.005 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 029 | Total loss: 1.999 | Reg loss: 0.024 | Tree loss: 1.999 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 029 | Total loss: 2.023 | Reg loss: 0.024 | Tree loss: 2.023 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 029 | Total loss: 2.045 | Reg loss: 0.024 | Tree loss: 2.045 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 029 | Total loss: 2.034 | Reg loss: 0.024 | Tree loss: 2.034 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 029 | Total loss: 2.007 | Reg loss: 0.024 | Tree loss: 2.007 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 029 | Total loss: 2.021 | Reg loss: 0.024 | Tree loss: 2.021 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 029 | Total loss: 2.079 | Reg loss: 0.024 | Tree loss: 2.079 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 029 | Total loss: 2.052 | Reg loss: 0.024 | Tree loss: 2.052 | Accuracy: 0.246094 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 029 | Total loss: 2.032 | Reg loss: 0.023 | Tree loss: 2.032 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 029 | Total loss: 2.073 | Reg loss: 0.023 | Tree loss: 2.073 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 029 | Total loss: 2.057 | Reg loss: 0.023 | Tree loss: 2.057 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 029 | Total loss: 2.021 | Reg loss: 0.023 | Tree loss: 2.021 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 029 | Total loss: 2.040 | Reg loss: 0.023 | Tree loss: 2.040 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 029 | Total loss: 2.030 | Reg loss: 0.023 | Tree loss: 2.030 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 029 | Total loss: 2.013 | Reg loss: 0.023 | Tree loss: 2.013 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 029 | Total loss: 2.073 | Reg loss: 0.023 | Tree loss: 2.073 | Accuracy: 0.236328 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 029 | Total loss: 1.997 | Reg loss: 0.023 | Tree loss: 1.997 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 029 | Total loss: 2.071 | Reg loss: 0.023 | Tree loss: 2.071 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 029 | Total loss: 2.029 | Reg loss: 0.023 | Tree loss: 2.029 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 029 | Total loss: 1.965 | Reg loss: 0.023 | Tree loss: 1.965 | Accuracy: 0.461538 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 94 | Batch: 000 / 029 | Total loss: 2.026 | Reg loss: 0.023 | Tree loss: 2.026 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 029 | Total loss: 2.038 | Reg loss: 0.023 | Tree loss: 2.038 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 029 | Total loss: 2.047 | Reg loss: 0.023 | Tree loss: 2.047 | Accuracy: 0.275391 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 029 | Total loss: 2.072 | Reg loss: 0.024 | Tree loss: 2.072 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 029 | Total loss: 2.060 | Reg loss: 0.024 | Tree loss: 2.060 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 029 | Total loss: 2.064 | Reg loss: 0.024 | Tree loss: 2.064 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 029 | Total loss: 1.995 | Reg loss: 0.024 | Tree loss: 1.995 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 029 | Total loss: 2.070 | Reg loss: 0.024 | Tree loss: 2.070 | Accuracy: 0.234375 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 029 | Total loss: 2.036 | Reg loss: 0.024 | Tree loss: 2.036 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 029 | Total loss: 2.054 | Reg loss: 0.024 | Tree loss: 2.054 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 029 | Total loss: 2.026 | Reg loss: 0.023 | Tree loss: 2.026 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 029 | Total loss: 2.066 | Reg loss: 0.023 | Tree loss: 2.066 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 029 | Total loss: 2.048 | Reg loss: 0.023 | Tree loss: 2.048 | Accuracy: 0.240234 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 029 | Total loss: 2.004 | Reg loss: 0.023 | Tree loss: 2.004 | Accuracy: 0.318359 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 029 | Total loss: 2.033 | Reg loss: 0.023 | Tree loss: 2.033 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 029 | Total loss: 2.004 | Reg loss: 0.023 | Tree loss: 2.004 | Accuracy: 0.310547 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 029 | Total loss: 2.070 | Reg loss: 0.023 | Tree loss: 2.070 | Accuracy: 0.250000 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 029 | Total loss: 2.059 | Reg loss: 0.023 | Tree loss: 2.059 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 029 | Total loss: 2.001 | Reg loss: 0.023 | Tree loss: 2.001 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 029 | Total loss: 2.046 | Reg loss: 0.023 | Tree loss: 2.046 | Accuracy: 0.261719 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 029 | Total loss: 1.995 | Reg loss: 0.023 | Tree loss: 1.995 | Accuracy: 0.316406 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 029 | Total loss: 2.013 | Reg loss: 0.023 | Tree loss: 2.013 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 029 | Total loss: 2.021 | Reg loss: 0.023 | Tree loss: 2.021 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 029 | Total loss: 2.051 | Reg loss: 0.023 | Tree loss: 2.051 | Accuracy: 0.240234 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 029 | Total loss: 2.046 | Reg loss: 0.023 | Tree loss: 2.046 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 029 | Total loss: 2.018 | Reg loss: 0.023 | Tree loss: 2.018 | Accuracy: 0.300781 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 029 | Total loss: 2.013 | Reg loss: 0.023 | Tree loss: 2.013 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 029 | Total loss: 2.000 | Reg loss: 0.023 | Tree loss: 2.000 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 029 | Total loss: 2.457 | Reg loss: 0.023 | Tree loss: 2.457 | Accuracy: 0.230769 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 95 | Batch: 000 / 029 | Total loss: 2.018 | Reg loss: 0.023 | Tree loss: 2.018 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 029 | Total loss: 2.032 | Reg loss: 0.023 | Tree loss: 2.032 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 029 | Total loss: 2.086 | Reg loss: 0.023 | Tree loss: 2.086 | Accuracy: 0.250000 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 029 | Total loss: 2.004 | Reg loss: 0.023 | Tree loss: 2.004 | Accuracy: 0.330078 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 029 | Total loss: 2.031 | Reg loss: 0.023 | Tree loss: 2.031 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 029 | Total loss: 2.125 | Reg loss: 0.023 | Tree loss: 2.125 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 029 | Total loss: 2.007 | Reg loss: 0.023 | Tree loss: 2.007 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 029 | Total loss: 1.987 | Reg loss: 0.023 | Tree loss: 1.987 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 029 | Total loss: 2.030 | Reg loss: 0.023 | Tree loss: 2.030 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 029 | Total loss: 2.022 | Reg loss: 0.023 | Tree loss: 2.022 | Accuracy: 0.287109 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 010 / 029 | Total loss: 2.074 | Reg loss: 0.023 | Tree loss: 2.074 | Accuracy: 0.242188 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 029 | Total loss: 2.027 | Reg loss: 0.023 | Tree loss: 2.027 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 029 | Total loss: 2.002 | Reg loss: 0.023 | Tree loss: 2.002 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 029 | Total loss: 2.040 | Reg loss: 0.023 | Tree loss: 2.040 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 029 | Total loss: 2.048 | Reg loss: 0.023 | Tree loss: 2.048 | Accuracy: 0.263672 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 029 | Total loss: 2.046 | Reg loss: 0.023 | Tree loss: 2.046 | Accuracy: 0.291016 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 029 | Total loss: 2.046 | Reg loss: 0.023 | Tree loss: 2.046 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 029 | Total loss: 2.031 | Reg loss: 0.023 | Tree loss: 2.031 | Accuracy: 0.281250 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 029 | Total loss: 2.036 | Reg loss: 0.023 | Tree loss: 2.036 | Accuracy: 0.253906 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 029 | Total loss: 2.019 | Reg loss: 0.023 | Tree loss: 2.019 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 029 | Total loss: 2.016 | Reg loss: 0.023 | Tree loss: 2.016 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 029 | Total loss: 2.047 | Reg loss: 0.023 | Tree loss: 2.047 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 029 | Total loss: 2.010 | Reg loss: 0.023 | Tree loss: 2.010 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 029 | Total loss: 2.053 | Reg loss: 0.023 | Tree loss: 2.053 | Accuracy: 0.242188 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 029 | Total loss: 1.997 | Reg loss: 0.023 | Tree loss: 1.997 | Accuracy: 0.310547 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 029 | Total loss: 2.017 | Reg loss: 0.023 | Tree loss: 2.017 | Accuracy: 0.304688 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 029 | Total loss: 2.060 | Reg loss: 0.023 | Tree loss: 2.060 | Accuracy: 0.238281 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 029 | Total loss: 2.047 | Reg loss: 0.023 | Tree loss: 2.047 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 029 | Total loss: 2.279 | Reg loss: 0.023 | Tree loss: 2.279 | Accuracy: 0.230769 | 0.253 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 96 | Batch: 000 / 029 | Total loss: 2.011 | Reg loss: 0.023 | Tree loss: 2.011 | Accuracy: 0.302734 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 029 | Total loss: 2.028 | Reg loss: 0.023 | Tree loss: 2.028 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 029 | Total loss: 2.032 | Reg loss: 0.023 | Tree loss: 2.032 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 029 | Total loss: 2.012 | Reg loss: 0.023 | Tree loss: 2.012 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 029 | Total loss: 2.023 | Reg loss: 0.023 | Tree loss: 2.023 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 029 | Total loss: 2.004 | Reg loss: 0.023 | Tree loss: 2.004 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 029 | Total loss: 2.065 | Reg loss: 0.023 | Tree loss: 2.065 | Accuracy: 0.261719 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 029 | Total loss: 2.000 | Reg loss: 0.023 | Tree loss: 2.000 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 029 | Total loss: 2.050 | Reg loss: 0.023 | Tree loss: 2.050 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 029 | Total loss: 2.043 | Reg loss: 0.023 | Tree loss: 2.043 | Accuracy: 0.271484 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 029 | Total loss: 2.066 | Reg loss: 0.023 | Tree loss: 2.066 | Accuracy: 0.257812 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 029 | Total loss: 2.023 | Reg loss: 0.023 | Tree loss: 2.023 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 029 | Total loss: 2.058 | Reg loss: 0.023 | Tree loss: 2.058 | Accuracy: 0.269531 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 029 | Total loss: 2.040 | Reg loss: 0.023 | Tree loss: 2.040 | Accuracy: 0.298828 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 029 | Total loss: 2.023 | Reg loss: 0.023 | Tree loss: 2.023 | Accuracy: 0.308594 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 029 | Total loss: 2.032 | Reg loss: 0.023 | Tree loss: 2.032 | Accuracy: 0.283203 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 029 | Total loss: 2.009 | Reg loss: 0.023 | Tree loss: 2.009 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 029 | Total loss: 2.052 | Reg loss: 0.023 | Tree loss: 2.052 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 029 | Total loss: 2.063 | Reg loss: 0.023 | Tree loss: 2.063 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 029 | Total loss: 2.028 | Reg loss: 0.023 | Tree loss: 2.028 | Accuracy: 0.273438 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 029 | Total loss: 2.030 | Reg loss: 0.023 | Tree loss: 2.030 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 029 | Total loss: 2.051 | Reg loss: 0.023 | Tree loss: 2.051 | Accuracy: 0.292969 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 029 | Total loss: 2.027 | Reg loss: 0.023 | Tree loss: 2.027 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 029 | Total loss: 2.030 | Reg loss: 0.023 | Tree loss: 2.030 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 029 | Total loss: 2.028 | Reg loss: 0.023 | Tree loss: 2.028 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 029 | Total loss: 2.010 | Reg loss: 0.023 | Tree loss: 2.010 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 029 | Total loss: 2.065 | Reg loss: 0.023 | Tree loss: 2.065 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 029 | Total loss: 2.030 | Reg loss: 0.023 | Tree loss: 2.030 | Accuracy: 0.287109 | 0.252 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 029 | Total loss: 2.308 | Reg loss: 0.023 | Tree loss: 2.308 | Accuracy: 0.307692 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 97 | Batch: 000 / 029 | Total loss: 2.044 | Reg loss: 0.023 | Tree loss: 2.044 | Accuracy: 0.294922 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 029 | Total loss: 2.082 | Reg loss: 0.023 | Tree loss: 2.082 | Accuracy: 0.259766 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 029 | Total loss: 2.004 | Reg loss: 0.023 | Tree loss: 2.004 | Accuracy: 0.285156 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 029 | Total loss: 2.024 | Reg loss: 0.023 | Tree loss: 2.024 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 029 | Total loss: 2.025 | Reg loss: 0.023 | Tree loss: 2.025 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 029 | Total loss: 2.006 | Reg loss: 0.023 | Tree loss: 2.006 | Accuracy: 0.289062 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 029 | Total loss: 2.042 | Reg loss: 0.023 | Tree loss: 2.042 | Accuracy: 0.277344 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 029 | Total loss: 2.090 | Reg loss: 0.023 | Tree loss: 2.090 | Accuracy: 0.248047 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 029 | Total loss: 2.032 | Reg loss: 0.023 | Tree loss: 2.032 | Accuracy: 0.300781 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 029 | Total loss: 2.048 | Reg loss: 0.023 | Tree loss: 2.048 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 029 | Total loss: 2.048 | Reg loss: 0.023 | Tree loss: 2.048 | Accuracy: 0.251953 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 029 | Total loss: 2.047 | Reg loss: 0.023 | Tree loss: 2.047 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 029 | Total loss: 2.006 | Reg loss: 0.023 | Tree loss: 2.006 | Accuracy: 0.296875 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 029 | Total loss: 2.033 | Reg loss: 0.023 | Tree loss: 2.033 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 029 | Total loss: 2.060 | Reg loss: 0.023 | Tree loss: 2.060 | Accuracy: 0.281250 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 015 / 029 | Total loss: 2.021 | Reg loss: 0.023 | Tree loss: 2.021 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 029 | Total loss: 2.035 | Reg loss: 0.023 | Tree loss: 2.035 | Accuracy: 0.267578 | 0.253 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 029 | Total loss: 2.035 | Reg loss: 0.023 | Tree loss: 2.035 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 029 | Total loss: 2.013 | Reg loss: 0.023 | Tree loss: 2.013 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 029 | Total loss: 2.018 | Reg loss: 0.023 | Tree loss: 2.018 | Accuracy: 0.248047 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 029 | Total loss: 2.018 | Reg loss: 0.023 | Tree loss: 2.018 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 029 | Total loss: 2.021 | Reg loss: 0.023 | Tree loss: 2.021 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 029 | Total loss: 2.047 | Reg loss: 0.023 | Tree loss: 2.047 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 029 | Total loss: 2.085 | Reg loss: 0.023 | Tree loss: 2.085 | Accuracy: 0.253906 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 029 | Total loss: 2.008 | Reg loss: 0.023 | Tree loss: 2.008 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 029 | Total loss: 2.023 | Reg loss: 0.023 | Tree loss: 2.023 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 029 | Total loss: 2.026 | Reg loss: 0.023 | Tree loss: 2.026 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 029 | Total loss: 1.978 | Reg loss: 0.023 | Tree loss: 1.978 | Accuracy: 0.314453 | 0.252 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 029 | Total loss: 2.148 | Reg loss: 0.023 | Tree loss: 2.148 | Accuracy: 0.153846 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 98 | Batch: 000 / 029 | Total loss: 2.068 | Reg loss: 0.023 | Tree loss: 2.068 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 029 | Total loss: 2.042 | Reg loss: 0.023 | Tree loss: 2.042 | Accuracy: 0.265625 | 0.253 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 029 | Total loss: 2.044 | Reg loss: 0.023 | Tree loss: 2.044 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 029 | Total loss: 2.036 | Reg loss: 0.023 | Tree loss: 2.036 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 029 | Total loss: 2.015 | Reg loss: 0.023 | Tree loss: 2.015 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 029 | Total loss: 2.033 | Reg loss: 0.023 | Tree loss: 2.033 | Accuracy: 0.255859 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 029 | Total loss: 2.025 | Reg loss: 0.023 | Tree loss: 2.025 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 029 | Total loss: 2.020 | Reg loss: 0.023 | Tree loss: 2.020 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 029 | Total loss: 2.036 | Reg loss: 0.023 | Tree loss: 2.036 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 029 | Total loss: 2.030 | Reg loss: 0.023 | Tree loss: 2.030 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 029 | Total loss: 2.027 | Reg loss: 0.023 | Tree loss: 2.027 | Accuracy: 0.265625 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 029 | Total loss: 2.010 | Reg loss: 0.023 | Tree loss: 2.010 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 029 | Total loss: 2.036 | Reg loss: 0.023 | Tree loss: 2.036 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 029 | Total loss: 1.983 | Reg loss: 0.023 | Tree loss: 1.983 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 029 | Total loss: 2.023 | Reg loss: 0.023 | Tree loss: 2.023 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 029 | Total loss: 2.049 | Reg loss: 0.023 | Tree loss: 2.049 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 029 | Total loss: 2.046 | Reg loss: 0.023 | Tree loss: 2.046 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 029 | Total loss: 2.036 | Reg loss: 0.023 | Tree loss: 2.036 | Accuracy: 0.283203 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 029 | Total loss: 2.035 | Reg loss: 0.023 | Tree loss: 2.035 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 029 | Total loss: 2.082 | Reg loss: 0.023 | Tree loss: 2.082 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 029 | Total loss: 2.021 | Reg loss: 0.023 | Tree loss: 2.021 | Accuracy: 0.273438 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 029 | Total loss: 2.049 | Reg loss: 0.023 | Tree loss: 2.049 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 029 | Total loss: 1.990 | Reg loss: 0.023 | Tree loss: 1.990 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 029 | Total loss: 1.993 | Reg loss: 0.023 | Tree loss: 1.993 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 029 | Total loss: 2.039 | Reg loss: 0.023 | Tree loss: 2.039 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 029 | Total loss: 2.022 | Reg loss: 0.023 | Tree loss: 2.022 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 029 | Total loss: 2.098 | Reg loss: 0.023 | Tree loss: 2.098 | Accuracy: 0.259766 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 029 | Total loss: 2.025 | Reg loss: 0.023 | Tree loss: 2.025 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 029 | Total loss: 1.700 | Reg loss: 0.023 | Tree loss: 1.700 | Accuracy: 0.538462 | 0.252 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 99 | Batch: 000 / 029 | Total loss: 2.038 | Reg loss: 0.023 | Tree loss: 2.038 | Accuracy: 0.279297 | 0.253 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 029 | Total loss: 2.003 | Reg loss: 0.023 | Tree loss: 2.003 | Accuracy: 0.298828 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 029 | Total loss: 2.096 | Reg loss: 0.023 | Tree loss: 2.096 | Accuracy: 0.246094 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 029 | Total loss: 1.974 | Reg loss: 0.023 | Tree loss: 1.974 | Accuracy: 0.294922 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 029 | Total loss: 1.997 | Reg loss: 0.023 | Tree loss: 1.997 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 029 | Total loss: 2.046 | Reg loss: 0.023 | Tree loss: 2.046 | Accuracy: 0.289062 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 029 | Total loss: 2.006 | Reg loss: 0.023 | Tree loss: 2.006 | Accuracy: 0.263672 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 029 | Total loss: 2.054 | Reg loss: 0.023 | Tree loss: 2.054 | Accuracy: 0.257812 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 029 | Total loss: 2.027 | Reg loss: 0.023 | Tree loss: 2.027 | Accuracy: 0.285156 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 029 | Total loss: 2.013 | Reg loss: 0.023 | Tree loss: 2.013 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 029 | Total loss: 2.069 | Reg loss: 0.023 | Tree loss: 2.069 | Accuracy: 0.304688 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 029 | Total loss: 2.030 | Reg loss: 0.023 | Tree loss: 2.030 | Accuracy: 0.281250 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 029 | Total loss: 2.045 | Reg loss: 0.023 | Tree loss: 2.045 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 029 | Total loss: 2.055 | Reg loss: 0.023 | Tree loss: 2.055 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 029 | Total loss: 2.028 | Reg loss: 0.023 | Tree loss: 2.028 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 029 | Total loss: 2.044 | Reg loss: 0.023 | Tree loss: 2.044 | Accuracy: 0.279297 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 029 | Total loss: 2.027 | Reg loss: 0.023 | Tree loss: 2.027 | Accuracy: 0.277344 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 029 | Total loss: 2.068 | Reg loss: 0.023 | Tree loss: 2.068 | Accuracy: 0.242188 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 029 | Total loss: 2.107 | Reg loss: 0.023 | Tree loss: 2.107 | Accuracy: 0.246094 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 029 | Total loss: 2.033 | Reg loss: 0.023 | Tree loss: 2.033 | Accuracy: 0.269531 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 020 / 029 | Total loss: 1.987 | Reg loss: 0.023 | Tree loss: 1.987 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 029 | Total loss: 2.042 | Reg loss: 0.023 | Tree loss: 2.042 | Accuracy: 0.275391 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 029 | Total loss: 2.025 | Reg loss: 0.023 | Tree loss: 2.025 | Accuracy: 0.269531 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 029 | Total loss: 2.006 | Reg loss: 0.023 | Tree loss: 2.006 | Accuracy: 0.292969 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 029 | Total loss: 2.000 | Reg loss: 0.023 | Tree loss: 2.000 | Accuracy: 0.291016 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 029 | Total loss: 1.985 | Reg loss: 0.023 | Tree loss: 1.985 | Accuracy: 0.312500 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 029 | Total loss: 2.071 | Reg loss: 0.023 | Tree loss: 2.071 | Accuracy: 0.267578 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 029 | Total loss: 2.016 | Reg loss: 0.023 | Tree loss: 2.016 | Accuracy: 0.271484 | 0.252 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 029 | Total loss: 1.955 | Reg loss: 0.023 | Tree loss: 1.955 | Accuracy: 0.307692 | 0.252 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b785471518ad4a91a3f1d1a6dd8cfc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423843e1febb48dd83a0df24caab2987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae71c8bacac48e2ab9429311a191556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2297871504b0498b88cccab8aa7a9f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.426229508196721\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 122\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "14349\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "Average comprehensibility: 35.24590163934426\n",
      "std comprehensibility: 5.664545292199045\n",
      "var comprehensibility: 32.08707336737436\n",
      "minimum comprehensibility: 16\n",
      "maximum comprehensibility: 44\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
